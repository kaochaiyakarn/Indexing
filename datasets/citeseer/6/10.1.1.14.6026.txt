automatic word sense discrimination hinrich xerox palo alto research center presents context group discrimination disambiguation algorithm clustering 
senses interpreted groups clusters similar contexts ambiguous word 
words contexts senses represented word space high dimensional real valued space closeness corresponds semantic similarity 
similarity word space second order occurrence tokens contexts ambiguous word assigned sense cluster words occur turn occur similar words training corpus 
algorithm automatic unsupervised training application senses induced corpus labeled training insta external knowledge sources 
demonstrates performance context group discrimination sample natural artificial ambiguous words 

word sense disambiguation task assigning sense labels occurrences ambiguous word 
problem divided subproblems sense discrimination sense labeling 
sense discrimination divides occurrences word number classes determining occurrences belong sense 
sense labeling assigns sense class combination sense discrimination occurrence ambiguous word 
view disambiguation stage process may completely general example may appropriate iterative process lexicographer arrives sense divisions dictionary entry applicable disambiguation computational linguistics 
address problem sense discrimination defined 
concerned sense labeling component word sense disambiguation 
word sense discrimination easier full disambiguation need determine occurrences meaning meaning focusing solely word sense discrimination serious constraint common word sense disambiguation 
sense labeling part task outside source knowledge necessary define senses 
regardless takes form dictionaries lesk guthrie dagan itai karov edelman thesauri yarowsky walker bilingual corpora brown church gale hand labeled training sets hearst leacock towell voorhees niwa nitta bruce wiebe providing information sense definitions considerable burden 
approach unique narrow problem sense discrimination dispense outside source knowledge defining senses 
xerox palo alto research center coyote hill road palo alto ca association computational linguistics computational linguistics volume number call approach automatic word sense discrimination require manually constructed sources knowledge 
applications word sense disambiguation discriminate label occurrences example order find correct translation ambiguous word machine translation right text speech system 
application interest information access making sense finding information large text databases 
problems information access sufficient solve discrimination problem 
study measured document query similarity word senses words achieved considerable improvement ranking relevant documents ahead nonrelevant documents pedersen 
measurement similarity process externally defined senses need 
potentially beneficial application word sense discrimination information access design interfaces take account ambiguity 
user enters query contains ambiguous word system capable discrimination give examples different senses word text database 
user decide sense intended documents intended sense retrieved 
external sense definitions required task 
algorithm propose context group discrimination 
discrimination groups occurrences ambiguous word clusters clusters consist contextually similar occurrences 
words contexts clusters represented high dimensional real valued vector space 
context vectors capture information second order occurrence 
forming context representation words ambiguous word directly occurs particular context order occurrence form context representation words words turn occur training corpus 
second order occurrence information sparse robust order information 
context group discrimination context occurrence ambiguous word training corpus represented context vector formed second order occurrence information 
context vectors clustered coherent groups occurrences judged similar second order occurrence assigned cluster 
clusters represented centroids average elements 
occurrence test text disambiguated computing second order representation relevant context assigning cluster centroid closest representation 
choice representation influences formation clusters experiment representations involving dimensionality reduction singular value decomposition svd 
context group discrimination generalized discrimination task goes notion sense underlies contributions disambiguation literature 
ambiguous word occurrences clustered large number clusters clusters capture fine contextual distinctions 
consider example space 
small number clusters senses outer space limited extent dimensions separated 
word occurrences clustered clusters finer distinctions office space exhibition space discovered 
note differences sense entries dictionaries similarly fine grained 
basic idea algorithm described 
automatic word sense discrimination training text vectors word space xxl lx 
ix xi test context basic design context group discrimination 
contexts ambiguous word training set mapped context vectors word space upper dashed arrow summing vectors words context 
context vectors grouped clusters dotted lines represented sense vectors centroids squares 
context ambiguous word test context disambiguated mapping context vector word space lower dashed arrow circle 
context assigned sense closest sense vector solid arrow 
contextual distinctions captured generalized context group discrimination line perfectly finer distinctions dictionaries help characterize contextual meaning ambiguous word particular instance 
characterization useful information access applications described 
basic idea context group discrimination induce senses contextual similarity 
evidence contextual similarity plays crucial role human semantic categorization 
miller charles evidence experiments humans determine semantic similarity words similarity contexts 
hypothesize extension senses contextual similarity sense group contextually similar occurrences word 
sections describe disambiguation algorithm evaluation results algorithm test set drawn new york times news wire discuss relevance approach context word sense disambiguation 

context group discrimination context group discrimination groups set contextually similar occurrences ambiguous word cluster interpreted sense 
particular implementation idea described high dimensional real valued vector space 
context group discrimination corpus method representa tions derived large text corpus 
basic design context group discrimination shown 
occurrence ambiguous word training set mapped point word space shown example occurrence see dashed line training text word space 
mapping word vectors looked word space described 
training text contexts mapped word space resulting point cloud clustered groups points points close group groups distant computational linguistics volume number possible 
resulting clusters delimited dotted lines 
cluster assumed correspond sense ambiguous word assumption evaluated 
representative group centroid depicted square 
training new occurrence ambiguous word labeled test context disambiguated mapping context word space see lower dashed line context point depicted circle 
context assigned context group centroid closest solid arrow 
context categorized sense corresponding context group 
types entities need represent words contexts senses 
represented word vectors context vectors sense vectors respectively 
word vectors derived neighbors corpus context vectors derived word vectors sense vectors derived way clustering distribution context vectors 
representational medium vector space chosen wide acceptance information retrieval ir see salton mcgill 
vectorspace model arguably common framework ir 
systems ranked best evaluations ir performance harman 
success vector space model motivates representation words 
represent words space dimension corresponds word just documents queries commonly represented space ir 
approach computing word similarity representation words document space dimension corresponds document lesk salton qiu frei 
fewer occurrence document occurrence events word representations tend sparse arguably informative word representations 
word vectors hand encoded features gallant dictionaries sparck jones wilks 
corpus methods proposed advantage manual labor required possible mismatch general dictionary specialized text chemistry avoided 
word similarity computed structural features head modifier relationships grefenstette dagan marcus markovitch pereira tishby lee dagan pereira lee 
document representations structure representations sparser occurrence 
debatable structural features informative associational features grefenstette pedersen 
approaches word representation closely related proposed niwa nitta burgess 
occurrence counts vector entries mutual information scores word represented dimension words niwa nitta approach 
algorithms vector derivation sense discrimination described follows 
word vectors vector word derived close neighbors corpus 
close neighbors words occur sentence larger context 
simplest case vector entry word occurs corpus 
entry word vector records number times word occurs close corpus 
representational vector space refer word space 
gives schematic example words represented twodimensional space 
representation occurrence counts hypo automatic word sense discrimination table occurrence counts words hypothetical corpus 
words legal clothes interpreted dimensions judge robe vectors 
vector dimension judge robe legal clothes legal robe clothes derivation word vectors 
judge robe represented word vectors dimensional space dimensions legal clothes 
occurrence data table 
corpus table 
word judge value dimension legal judge legal occur times see words selected dimensions word dimension word space represented word vector word space time 
vector representation captures typical topic subject matter word 
example words judge law closer legal dimension words robe tailor closer clothes dimension 
looking amount overlap vectors roughly determine closely related semantically 
related meanings expressed similar sets words 
semantically related words occur similar neighbors vectors considerable overlap 
similarity measured cosine vectors 
cosine equivalent normalized correlation coefficient corr ff vectors dimension vector space 
value cosine higher overlap neighbors words vectors compared 
words occur exactly neighbors computational linguistics volume number perfect overlap value cosine 
overlap value cosine 
cosine rough measure semantic relatedness words 
words serve dimensions word space 
experiment strategies global local 
local strategy focuses contexts ambiguous words ignores rest corpus 
global strategy select frequent words corpus features regardless word disambiguated 
see karov edelman different approach selects features combination global frequency local salience 
local selection frequency cutoff 
alternative test selection test 
frequency selection criterion neighbors ambiguous word corpus counted 
neighbor word occurs distance words ambiguous word word window centered ambiguous word 
frequent neighbors chosen dimensions space 
criterion measure dependence applied contingency table containing number contexts ambiguous word candidate word occurs occur number contexts occurrence ambiguous word candidate word occurs occur 
underlying assumption test candidate words occur rence depends ambiguous word occurs indicative senses ambiguous word useful disambiguation 
words selected local selection word vectors formed collecting matrix element cij records number times words occur window size column equivalently row matrix represents word note symmetric words represented word vectors form dimensions dimensional space 
chose window size improvement discrimination performance 
global selection choose frequent words features frequent words dimensions word space 
global occurrence matrix derived corpus 
association data extracted training set consisting months new york times news service june october 
size set megabytes words 
months november may megabytes words set aside test set 
context vectors representation words derived conflates senses 
example senses word suit lawsuit garment summed word vector positioned legal clothes dimensions 
need go back individual contexts corpus acquire information sense distinctions 
contexts represented context vectors word space 
candidate words selected list stopwords removed 
list text data base system cutting pedersen 
automatic word sense discrimination legal centroid suit clothes derivation context vectors 
context vector computed centroid words occurring context 
words example context law judge suit 
context vector centroid sum vectors words occurring context 
shows context vector example context suit containing words law judge suit 
note context vector closer legal clothes dimension capturing context legal suit 
true sum vectors longer shown 
correlation coefficients normalized length vector play role computations 
centroid averages direction set vectors 
words context strong component topics legal average vectors context vector strong component topic 
conversely words represent particular topic context vector weak component 
context vector represents strength different topical semantic components context 
computation context vector weight word vector discriminating potential 
rough measure word wi discriminates different topics log inverse document frequency information retrieval salton buckley ni number documents wi occurs total number documents 
poor discriminators topics words idea help relatively uniformly distributed high document frequency 
content discriminators automobile china bursty distribution occurrences short interval occur church gale low document frequency relative absolute frequency :10.1.1.38.3957
algorithms computing context vectors proposed wilks 
dictionary entries gallant hand encoded semantic features grefenstette light parsing niwa nitta comparison dictionary corpus context vectors 
computational linguistics volume number legal sense clc 
sense clothes derivation sense vectors 
sense vectors derived clustering context vectors ambiguous word cl cs computing sense vectors centroids resulting clusters 
vectors sense sense sense vectors clusters respectively 
sense vectors sense representations computed groups similar contexts 
contexts ambiguous word collected corpus 
context context vector computed 
set context vectors clustered predetermined number coherent clusters context groups buckshot cutting combination em algorithm agglomerative clustering :10.1.1.34.6746
representation sense simply centroid cluster 
marks portion multidimensional space occupied cluster 
chose em algorithm clustering guaranteed converge locally optimal solution clustering problem 
case solution optimal sum squared distances context vectors ad centroids minimal 
words centroids optimal representatives context vectors cluster 
problem em algorithm finds solution locally optimal 
important find starting point bad starting point lead local minimum globally optimal 
experimental evidence shows cluster quality varies considerably depending initial parameters 
order find starting point group average agglomerative clustering sample context vectors 
clustering experiments described choose random sample 
size roughly equal number context vectors clustered 
time complexity guarantees linear time complexity clustering procedure 
training set instances ambiguous word context vectors selected randomly 
centroids resulting clusters parameters iteration em 
compute iterations em algorithm experiments cases context vectors reassigned fifth iteration 
em algorithm group average agglomerative clustering described detail appendix 
automatic word sense discrimination example shown 
clustering step grouped context vectors cl group cs second group 
sense vector group centroid labeled sense sense vector second group centroid labeled sense 
result clustering depends representation context vectors 
reason investigate transformation multidimensional space singular value decomposition svd golub van loan 
svd form dimensionality reduction finds major axes variation word space 
context vectors represented values principal dimensions 
motivation applying svd latent semantic indexing lsi information retrieval deerwester :10.1.1.108.8490
lsi abstracts away surface word representation detects underlying features 
similarity computed features cosine svd reduced context vectors contextual similarity potentially better measured cosine unreduced context vectors 
appendix defines svd gives example matrix decomposition 
word vectors reduced dimensions 
experiments reported give evidence reduction dimensionality decrease accuracy sense discrimination 
space requirements context vectors reduced dimensional dimensional word space respectively 
word vectors sparse context vectors dense sum word vectors 
time efficiency increased order magnitude correlation context vectors sense vectors computed 
computation svd took minutes word local feature set hours global feature set 
application context group discrimination context group discrimination uses word vectors sense vectors follows discriminate occurrences ambiguous word 
occurrence ambiguous word map corresponding context vector word space vectors words context lower dashed line 
retrieve sense vectors points marked squares 
assign sense sense vector closest assignment shown solid arrow 
algorithm selects context group sense vector closest context vector occurrence word disambiguated 
context vectors sense vectors capture semantic characteristics corresponding context sense respectively 
consequently sense vector closest context vector best semantic match context 
context group discrimination categorizes occurrence belonging sense 

evaluation test context group discrimination natural ambiguous words formed test set artificial ambiguous words 
table glosses major senses words 
computational linguistics volume number table number occurrences test words training test set percent rare senses test set baseline performance occurrences assigned frequent sense main senses artificial natural ambiguous words experiment 
word training test 
rare senses baseline frequent senses wide range consulting firm heart disease reserve board urban development cease fire drug administration fernando valley economic development right field national park committee japanese companies city hall drug dealers webber league baseball square feet pete rose nuclear power capital interest motion plant wide range consulting firm heart disease reserve board urban development cease fire drug administration fernando valley economic development right field national park committee japanese companies city hall drug dealers webber league baseball square feet pete rose nuclear power stock goods seat government feeling special attention charge borrowed money movement proposal action factory living automatic word sense discrimination table continued 
word training test rare senses baseline frequent senses ruling authoritative decision exert control influence space area volume outer space suit action process court set tank combat vehicle receptacle liquids train line railroad cars teach vessel ship plane tube canal artery artificial ambiguous words pseudowords convenient means testing disambiguation algorithms gale church yarowsky 
time consuming hand label large number instances ambiguous word evaluating performance disambiguation algorithm 
pseudowords circumvent need words banana door conflated new type banana door 
occurrences word corpus replaced new type 
easy evaluate disambiguation performance pseudowords go back original text decide correct decision 
create pseudowords shown table word pairs extracted corpus pairs words occurred adjacent corpus particular order 
numbers discarded numbers involve sense ambiguity 
pseudowords created randomly drawing pairs frequency corpus 
pseudowords generated pairs simple words pairs words ambiguous 
pair pseudowords examples ambiguous words clearly distinct senses 
table indicates ambiguous word occurred training test sets instances instances rare senses baseline performance achieved assigning occurrences frequent sense 
evaluation senses account occurrences ambiguous word taken account 
rare senses account fewer occurrences 
words table frequent senses 
frequency rare senses ranges average 
rare senses eliminated training set 
training test sets taken new york times news service described training set june october test set november may 
word occurrences test set occurrences included evaluation 
labeling words test corpus performed author 
computational linguistics volume number tions senses table intuitively clear 
example probability context suit time refer set action court low 
consequently fewer instances appropriate sense obvious immediate context 
cases sense plausible author assigned 
important evaluate test set separate training set 
context group discrimination distribution context vectors training set 
distribution training set bad model distribution test set 
practice intended text application time period covered training set example newswire text date training 
word distributions change considerably time 
test set constructed time period different time period training set 
reason cross validation 
cross validation respecting constraint test training sets different time periods required test set times larger available 
clustering evaluating set problematic sampling variation 
consider example 
set context vectors cl dimensional space 
contexts uses sense context sense 
training evaluation set average performance probability get centroids accuracy probability get centroids accuracy 
split training set size test set size get average performance cl lower 
example shows training test set result artificially high performance 
advantage context group discrimination granularity sense distinctions adjustable parameter algorithm 
experiments run directly senses table test algorithm ability discriminate coarse sense distinctions 
test performance fine grained sense distinctions office space vs exhibition space run experiments evaluates performance clustering context vectors word clusters information retrieval experiment number clusters large sufficiently frequent words 
goal cluster experiments induce fine grained sense distinctions cluster experiments 
harder determine ground truth fine sense distinctions 
comes fine distinctions large number occurrences indeterminate compatible finely senses cf 
kilgarriff 
reason experiments large number clusters evaluated indirect measures 
measure accuracy way discriminations degree clusters contained coarse senses 
evaluation indirect cluster contains say limited extent dimensions uses space deemed correct randomly mixed far fine sense distinctions concerned office space vs exhibition space 
author inspected data separation fine grained senses cluster experiments extent evaluation measure indicated performance way discrimination task 
mentioned subjectivity judgements fine sense distinctions hard quantify 
results second evaluation information retrieval task section 
show sense information retrieval relevance documents query determined context group discrimination automatic word sense discrimination improves performance ir system considerably 
success retrieval depends accuracy context group discrimination infer algorithm reliably assigns ambiguous instances induced senses fine grained case 

experiments word sense discrimination table shows experimental results context group discrimination 
conditions varied experiments described section local vs global feature selection feature selection frequency vs term representations vs svd reduced representations number clusters vs local feature selection parameters varied systematically columns table 
global feature selection selection possible test presupposes event occurrence ambiguous word occurrence candidate words depends 
event global feature selection 
larger number dimensions global variant algorithm order get coverage large range topics relevant disambiguation 
apply svd global feature selection case 
word vectors sparse context vectors usually 
clustering dimensional vectors computationally expensive ran experiments svd reduced vectors global variant 
experiments different randomly chosen initial parameters run combinations different levels word representation clustering 
mean percentage correctness standard deviation set experiments shown cells table 
give mean deviation percentage correctly labeled occurrences instances training set total instances sense si instances sense 
bottom row table gives averages total percentage correct numbers words covered 
rightmost column gives averages means experiments 
analyzed results table analysis variance anova see example ott 
anova performed design replicates 
factors word representation local frequency terms local frequency svd local terms local svd global frequency svd clustering coarse clusters fine clusters 
percentages transformed functionf sin recommended 
transformed percentages distribution close normal distribution required application anova 
effects factors interactions significant level 
effects discussed follows 
factor word 
general performance pseudowords better natural words 
explained fact pseudowords focussed senses word pairs composed 
contrast senses natural ambiguous computational linguistics volume number table results disambiguation experiments 
rows give total accuracy word accuracy senses separately si 
average bottom row average total accuracy numbers 
columns describe experimental conditions mean standard deviation replications experiment 
rightmost column contains average mean values experiments 
pseudowords abbreviated words pairs 
local global frequency frequency terms svd terms svd svd cr cr 
wide 
reserve 
urban cease 

drug fern 


con right 

nat 

city 
drug 

square 
ete nuclear 

interest 

motion 



ruling 
pace 
suit 
tank 
train 

vessel 
average ii ii automatic word sense discrimination table tukey test shows significantly different performance representations 
proportions transformed sin 
rightmost column contains accuracy percent correspond average value second column 
significant difference ct average difference corresponding level sin closest accuracy local terms local frequency terms local frequency svd local svd global frequency svd words example space interest composed different hard identify people computers 
pseudoword poor performance wide range consulting firm 
illustrative example weakness particular implementation discrimination chosen 
rely topical information word composed sense wide range occur subject area disambiguated poorly 
area volume sense space teaching sense train similarly topically amorphous hard topical information considered 
poor performance plant cluster experiments probably due way training set clusters assigned senses 
training set clustered clusters cluster sense label 
procedure introduces misclassifications individual instances training set 
contrast performance achieved hand categorizing training set instance instance 
note experimental conditions words performance group clustering baseline 
completely unsupervised setting assumption induced clusters correspond different senses 
worst case get 
clusters identical proportions senses accuracy baseline assigning occurrences sense occurs cases 
example vessel worst case clusters ship instances tube instances 
accuracy 
argued true baseline unsupervised group clustering proportion frequent sense 
factor representation 
tukey test ott performed evaluate factor representation 
tukey test determines significant difference sample means 
yields threshold levels factor differ threshold significantly different 
factor representation case significant difference 
table shows differences significant 
evidence svd representations perform better term representations global representations perform better local representations 
advantage svd representations partly due normality assumption clustering 
poor approximation term computational linguistics volume number table occurrence selected term features test set 
table shows number words occurring test set averaged ambiguous words number words occurring context averaged contexts proportion words representation occurring averaged contexts ambiguous words average selected terms set selected local frequency average number contexts selected term occurred average selected term occurred contexts artificial ambiguous words averaged words context 
local frequency global frequency words occurring test set words context term overlap local frequency global frequency average frequency terms artificial words natural words representations accurate svd reduced representations 
globally selected features perform better 
table presents data occurrence selected terms test set relevant question 
note locally selected features better globally selected ones measures 
locally selected features occur test set words occurring test set vs local features occur individual contexts words context vs global features local features vice versa context basis global features local features local features global features suggesting local features capture information global features 
measures show selected features suffer sparseness 
total number features occur training set number words context small 
evidence explains svd representations address sparseness better term representations 
explain difference performance local global frequency features break average accuracy artificial natural ambiguous words 
average accuracy artificial ambiguous words clus ters clusters local features clusters clusters global features 
average accuracy natural ambiguous words clusters clusters local features clusters clusters global features 
data show clear split 
performance local global features comparable natural ambiguous words 
global features perform clearly better artificial ambiguous words 
rows table explain difference behavior 
numbers correspond average number contexts selected features occur averaged words context contexts context selected terms occurring contexts ambiguous word training set average number contexts 
averages automatic word sense discrimination small local frequency case artificial ambiguous words 
clustering contexts elements common similarity determined robustly 
apparently elements common local frequency case artificial ambiguous word patterns sparse svd effective remedy 
problem artificial ambiguous words frequent training set natural ambiguous words average frequencies vs reliable feature selection harder artificial ambiguous words 
ample information natural ambiguous words available training set features selected occur densely test set 
quality feature selection artificial ambiguous words successful due smaller training set sizes 
analysis reiterates importance clear separation training test sets 
performance numbers artificially high feature selection done training test sets avoiding problems feature coverage demonstrated table 
global feature selection simpler effective local approaches global feature selection preferred implementation context group discrimination general case 
note different words may different optimal representations 
example local features best vessel 
similar individual differences frequency vs selection 
frequency selection best suit selection better vessel svd reduced representations 
factor clustering 
fine clustering generally better coarse clustering 
case coarse clustering comes close performance fine clustering global feature selection 
small difference entirely due bad performance fine clustering plant due insufficient training set explained 
fine clustering performs better coarse clustering surprising information evaluation fine clustering labeling clusters training set 
coarse clustering evaluated strictly unsupervised disambiguation evaluation set fine sense distinctions 
variance 
general variance discrimination accuracy higher coarse clustering fine clustering 
surprising fact evaluate types clustering way distinction 
may quite different ways dividing set context vectors groups 
cluster groups assign groups senses resulting way partitions resemble initial group clusterings similar 
experiments indicate context group discrimination globally selected features best implementation general case 
algorithm achieves baseline performance small number exceptions certain parameter settings 
average performance svd representations satisfactory inferior disambiguation minimal manual intervention yarowsky 
manually supplied priming information senses difference context group discrimination disambiguation algorithms 
differences responsible difference performance 
fact error rate doubles seeds yarowsky experiments reduced sense best collocations just word sense suggests error rate increase seeds provided 
computational linguistics volume number application information retrieval principal motivation concentrating discrimination subtask apply disambiguation information retrieval 
evidence ambiguity resolution improves performance ir systems krovetz croft researchers failed achieve consistent experimental improvements practically realistic rates disambiguation accuracy 
voorhees compared term expansion methods information retrieval queries term expanded related terms expanded terms related sense query 
disambiguation improve performance term expansion 
study disambiguation eliminate document query matches due sense mismatches word question different types context query document 
approach decreases number documents query matches term expansion increases 
important difference study longer queries 
long queries may arise ir system relevance feedback provide context short queries voorhees worked experiments 
sanderson modified test collection creating pseudowords similar ones study 
unrealistically high rates disambiguation accuracy little effect retrieval performance 
analysis pedersen suggests main reason minor effect disambiguation pseudowords created study major sense accounted occurrences pseudoword 
creating type pseudoword amounts adding small amount noise unambiguous word expected large effect retrieval performance 
extent actual dictionary senses property sense accounts large proportion occurrences 
necessarily true rare senses taken account high frequency senses broken smaller groups example office space vs exhibition space 
large dictionaries tend break high frequency senses narrowly defined 
successful disambiguation study may due fact rare senses useful ir taken account frequent senses subdivided 
evidence potential utility disambiguation information retrieval provided krovetz croft 
showed considerable amount ambiguity technical text assumed writing 
technical terms meanings addition specialized senses technical text window application computer magazines convertible automobile magazines krovetz 
krovetz croft showed sense mismatches spurious matching words different senses query document occurred significantly nonrelevant relevant documents 
suggests eliminating spurious matches improve separation nonrelevant relevant documents quality retrieval results 
order show context group discrimination approach disambiguation beneficial information retrieval summarize experiment pedersen 
experiment evaluates sense retrieval modification standard vector space model information retrieval 
refer standard vector space model word retrieval 
word retrieval documents queries represented vectors multidimensional space dimension corresponds word similar way repre automatic word sense discrimination sent word vectors word space 
sense retrieval documents queries represented multidimensional space dimensions senses words 
words disambiguated context group discrimination 
documents queries contain word assigned particular sense nonzero value corresponding dimension 
test corpus pedersen category trec collection documents wall street journal conjunction queries harman 
sense retrieval improved average precision compared word retrieval 
combination word sense retrieval increased performance 
greater improvement combination probably due discrimination errors fact discrimination correct partially undone combining sense word evidence 
improvement particularly high small sets documents requested example sense word sense combined recall level relevant documents 
experiment suggests high utility sense discrimination information retrieval 
sight sense retrieval may related term expansion 
sense retrieval term expansion take individual terms starting point modifying similarity measure determines documents deemed closely related query 
approaches opposites sense 
term expansion increases number matching documents query 
example query contains expansion adds astronaut documents containing astronaut additional nonzero matches 
sense retrieval decreases number matches 
example word suit occurs query disambiguated legal sense documents contain suit different sense longer match query 

discussion distinguishes context group discrimination disambiguation outside source information need supplied input algorithm 
disambiguation algorithms employ various sources information 
kelly stone consider hand constructed disambiguation rules lesk krovetz croft guthrie 
karov edelman line dictionaries hirst constructs knowledge bases cottrell uses syntactic semantic structure encoded connectionist net brown 
church gale exploit bilingual corpora dagan itai bilingual dictionary hearst leacock towell voorhees niwa nitta bruce wiebe exploit hand labeled training set yarowsky walker perform computations hand constructed semantic categorization words roget thesaurus longman subject codes respectively algorithms expense supplying information disambiguation algorithm relatively small 
example methods hand labeled training sets hearst relatively small number training examples sufficient 
yarowsky proposed algorithm requires little user input seed word sense start training process yarowsky 
minimal user input negligible burden users situations 
consider interactive information access application described 
asked improve initial ambiguous information request users reluctant computational linguistics volume number give seed word set features sense word 
satisfy request system choose correct sense mouse click example contexts corresponding different senses requirement additional user interaction 
application great advantage context group discrimination require manual intervention induce senses 
body related literature word clustering computational linguistics brown finch pereira tishby lee grefenstette document clustering information retrieval van rijsbergen willett sparck jones cutting :10.1.1.13.9919:10.1.1.34.6746
contrast earlier cluster contexts equivalently word tokens words precisely word types documents 
straightforward extension word type clustering document clustering word token clustering represent token words context cluster representations 
approach order occurrence example hearst plaunt representation tiles document subunits similar notion context 
second order occurrence represent tokens ambiguous words words occur token turn looked training corpus words occur represent token 
secondorder representations sparse robust order representations 
cluster approach subdivision universe elements clusters depends representation 
representation capture information crucial distinguishing senses context group discrimination performs poorly 
clearest example experiments pseudoword wide range consulting firm 
algorithm better baseline choosing frequent sense 
reason representation captures topic information 
cluster contain group contexts topic 
unfortunately pair wide range come text topic 
clear topical characterization sense pseudoword context group discrimination performs poorly 
reliance topical similarity may reason performance pseudowords generally better performance natural ambiguous words 
pseudowords wide range composed pairs different topics 
example heart disease reserve board pertain biology finance respectively clearly distinct topics 
hand senses ambiguous words clear associations particular topics 
example trained perform wide variety activities teaching 
sense train invoked different topics 
part superior performance pseudowords due different topic sensitivity natural artificial ambiguous words 
limitation topical distinctions flaw context group discrimination flaw particular implementation 
possible integrate information context vectors reflect syntactic subcategorization behavior different senses output shallow parser pereira tishby lee 
example indicator senses word interest preposition occurring right 
phrase interest invokes feeling attention sense phrase interest sense charge borrowed money 
plausible performance improved words senses sensitive topical distinctions proximity information integrated 
experiments pedersen bruce proximity features tags close words presence absence close functions words automatic word sense discrimination content words promising results 
suggests combination topical features proximity features may give optimal performance context group discrimination 
source information topical features interest simplicity see inherent advantage topical features compared combination multiple sources evidence 
justification basic idea context group discrimination inducing senses contextual similarity results align ground truth senses defined dictionaries 
evidence contextual similarity plays crucial role human semantic categorization 
study miller charles evidence human subjects determine semantic similarity words similarity contexts 
summarized result hypothesis strong contextual hypothesis words semantically similar extent contextual representations similar 
contextual representation word knowledge word 
hypothesis states semantic similarity determined degree similarity sets contexts words 
hypothesis underlies context group discrimination extension strong contextual hypothesis senses contextual hypothesis senses occurrences ambiguous word belong sense extent contextual representations similar 
sense simply group occurrence tokens similar contexts 
analogy contextual hypotheses words senses word types word tokens semantically similar extent contexts semantically similar 
group contextually similar word tokens sense 
miller charles provides justification framework induction senses contextual similarity 
issues need addressed context group discrimination 
experiments considered words major senses 
algorithm needs tested words frequent senses infrequent senses 
second test set consisted relatively small number natural ambiguous words 
flaw contemporary word sense disambiguation extensive test sets required establish general applicability disambiguation algorithms 
implementation context group discrimination proposed topical similarity 
necessary incorporate structural constraints interest vs interest case discussed achieve adequate performance wide variety ambiguous words 
appendix singular value decomposition singular value decomposition factors matrix product matrices dia see leacock discussion proximity topical features supervised disambiguation 
computational linguistics volume number table occurrence counts words dimensional word space 
judge suit robe criminal police gun bail legal clothes cop fashion pants table svd reduction dimensions matrix table 
judge suit robe criminal police gun bail diml dim table correlation coefficients words svd dimensionality reduction 
criminal robe word space svd space word space svd space criminal min left matrix orthonormal matrix right matrix orthonormal matrix diag ri 
crp matrix diagonal elements rl 
rp value zero elements golub van loan 
dimensionality reduction svd keeping singular values cr setting remaining ones zero 
shown product diag ai ak closest approximation dimensional space matrix rank smaller square distance 
see golub van loan berry detailed description svd efficient algorithms compute 
benefits dimensionality reduction purposes best explained example 
table shows occurrence counts hypothetical corpus legal robe occur times 
note semantically similar words criminal low correlation words occur belong different registers reasons topically similar words neighbors common 
table shows columns right matrix svd matrix table 
table dimensionality reduction table dimensions 
advantage reduced space directly represents similar topicality criminal vectors close space shown 
hand words vectors automatic word sense discrimination dimension criminal robe dimension vectors robe criminal reduced svd space 
words criminal represented semantically similar 
represented semantically dissimilar robe 
correlated topically dissimilar word robe reduced space 
correlation coefficients words shown table unreduced reduced space 
correlation topically related words criminal increases correlation words robe decreases 
example demonstrates effect svd dimensionality reduction topically similar words projected closer reduced space topically dissimilar words projected distant locations 
part motivation svd word vectors success latent semantic indexing lsi information retrieval deerwester :10.1.1.108.8490
lsi projects topically similar documents close locations reduced space just project topically similar words close locations 
appendix em algorithm clustering algorithm em algorithm 
observed data context vectors case interpreted generated hidden causes clusters 
em algorithm iterative procedure starting initial hypothesis cluster parameters improves estimates parameters iteration 
follow discussion notation dempster laird rubin ghahramani 
assumption cluster gaussian source density wj exp re mean covariance matrix wj 
write oj fij parameters cluster computational linguistics volume number assume dimensional context vectors 
tc gen erated gaussians 
wa 
em algorithm iteratively applies expectation step step maximization step step 
step estimation pa rameters hij hij probability event event cluster generated context vector 
hij zij xi wt iteration step computes parameters distribution cluster membership probabilities ij maximum likelihood estimates mean variance gaussian 
recomputed means variances parameters iteration 
reasons computational efficiency chose implementation em clustering means hard duda hart 
iteration context vectors cluster closest mean cluster means recomputed centroid members cluster 
small fixed variance clusters re means step 
cluster parameters computed applying group average agglomerative sample size appendix agglomerative clustering agglomerative clustering clustering technique starts assigning element different cluster iteratively merges clusters goodness criterion desired number clusters reached 
goodness measures give rise single link clustering complete link clustering 
single link clustering step merges clusters elements smallest distance clusters 
complete link clustering step executes merger resulting cluster smallest diameter possible mergers 
single link clustering practice produce elongated clusters parallel lines correspond intuitive notion cluster mass points center 
complete link clustering strongly affected outliers time complexity cubic number points merged efficient single link clustering computed quadratic time 
chose group average agglomerative clustering clustering algorithm hybrid single link complete link clustering 
iteration executes merger gives rise cluster largest average correlation ifl corr ff gef ep automatic word sense discrimination implemented run time quadratic number vectors clustered cutting :10.1.1.34.6746
low time complexity single link clustering 
goodness measure average elements cluster behavior influenced outliers elements closest clusters single link clustering elements distant clusters clustering generally gives better clustering results 
see van rijsbergen jain dubes overview clustering algorithms cutting 
cutting karger pedersen constant time linear time clustering algorithms 
acknowledgments greg grefenstette michael inman lauri karttunen adam kilgarriff jan pedersen ted pedersen john tukey tom helpful comments dan murphy creative support illustrations 
am indebted anonymous reviewers valuable suggestions 
mike berry provided package compute svd article 
berry michael 
large scale sparse singular value computations 
international journal ter applications 
brown peter stephen della pietra vincent della pietra robert mercer 

word sense statistical methods 
proceedings th annual meeting pages berkeley ca 
association computational linguistics 
brown peter vincent della pietra peter desouza lai robert mercer 

class gram models natural language 
computational linguistics 
bruce rebecca janyce wiebe 

word sense decomposable models 
proceedings nd annual meeting pages las cruces nm 
association computational linguistics 
burgess kevin lund 

modelling parsing constraints high dimensional context space 
language cognitive processes 
appear 
church kenneth william gale 

concordances parallel text 
proceedings seventh annual conference uw centre new oed text research pages oxford england 
church kenneth william gale 

poisson mixtures 
journal natural language engineering 
cottrell garrison 
connectionist approach word sense disambiguation 
pitman london 
cutting douglas david karger jan pedersen 

constant scatter gather browsing large document collections 
proceedings sigir pittsburgh pa cutting douglass jan pedersen kristian 

object oriented architecture text retrieval 
proceedings riao pages barcelona spain 
cutting douglas jan pedersen david karger john tukey 

scatter gather cluster approach browsing large document collections 
proceedings sigir pages copenhagen denmark 
dagan ido alon itai ulrike 

languages informative 
proceedings th annual meeting pages berkeley ca 
association computational linguistics 
dagan ido shaul marcus shaul markovitch 

contextual word similarity estimation sparse data 
proceedings st annual meeting pages columbus oh 
association computational linguistics 
dagan ido fernando pereira lillian lee 

similarity estimation word cooccurrence probabilities 
proceedings nd annual meeting pages las cruces nm 
association computational linguistics 
deerwester scott susan dumais george furnas thomas landauer richard harshman 

indexing latent semantic analysis 
journal american society information science 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
computational linguistics volume number journal royal statistical society series 
duda richard peter hart 

pattern cation scene analysis 
john wiley sons new york 
finch steven paul 

finding structure language 
ph thesis university edinburgh 
gale william kenneth church david yarowsky 

statistical methods word sense disambiguation 
robert goldman peter norvig eugene charniak bill gale editors working notes aaai fall symposium probabilistic approaches natural language pages aaai press menlo park ca 
gallant stephen 
practical approach representing context performing word sense disambiguation neural networks 
neural computation 
ghahramani zoubin 

solving inverse problems em approach density estimation 
michael mozer paul smolensky david touretzky andreas weigend editors proceedings connectionist models summer school erlbaum associates hillsdale nj 
golub gene charles van loan 

matrix computations 
johns hopkins university press baltimore london 
grefenstette gregory 

syntactic context produce term association lists text retrieval 
proceedings sigir pages 
grefenstette gregory 

corpus derived second third order word affinities 
proceedings sixth international congress amsterdam 
grefenstette gregory 

explorations automatic thesaurus discovery 
kluwer academic press boston 
grefenstette gregory 

evaluation techniques automatic semantic extraction comparing syntactic window approaches 
boguraev james pustejovsky editors corpus processing lexical acquisition 
mit press cambridge ma 
guthrie joe louise guthrie yorick wilks homa 

subject dependent occurrence word sense disambiguation 
proceedings th annual meeting pages berkeley ca 
association computational linguistics 
harman editor 

text retrieval conference trec 
department commerce washington dc 
nist special publication 
hearst marti 
noun homograph disambiguation local context large text corpora 
proceedings seventh annual conference uw centre new oed text research corpora pages oxford 
hearst marti christian plaunt 

subtopic structuring document access 
proceedings sigir pages 
hirst graeme 

semantic interpretation resolution ambiguity 
cambridge university press cambridge 
jain anil richard dubes 

algorithms clustering data 
prentice hall englewood cliffs nj 
karov yael shimon edelman 

learning similarity word sense disambiguation sparse data 
proceedings fourth workshop large corpora 
kelly edward phillip stone 

computer recognition english word senses 
north holland amsterdam 
kilgarriff adam 

dictionary word sense distinctions enquiry nature 
computers humanities 
krovetz robert 

homonymy polysemy information retrieval 
proceedings th annual meeting eacl pages morgan kaufmann san francisco ca 
association computational linguistics 
krovetz robert bruce croft 

word sense disambiguation machine readable dictionaries 
proceedings sigir pages cambridge ma 
krovetz robert bruce croft 

lexical ambiguity information retrieval 
acm transactions information systems 
leacock claudia geoffrey towell ellen voorhees 

building contextual representations word senses statistical models 
boguraev james pustejovsky editors acquisition lexical knowledge text workshop proceedings pages ohio 
leacock claudia geoffrey towell ellen voorhees 

corpus statistical sense resolution 
proceedings arpa workshop human language technology morgan kaufman san mateo ca 
lesk 
word word association document retrieval systems 
american documentation 
automatic word sense discrimination lesk michael 

automatic sense disambiguation tell pine cone ice cream cone 
proceedings conference pages new york 
association computing machinery 
miller george walter charles 

contextual correlates semantic similarity 
language cognitive processes 
niwa nitta 

occurrence vectors corpora vs distance vectors dictionaries 
proceedings coling pages 
ott lyman 

statistical methods data analysis 
wadsworth belmont ca 
pedersen ted rebecca bruce 

distinguishing word senses untagged text 
proceedings second conference empirical methods natural language processing pages providence ri 
pereira fernando naftali tishby lillian lee 

distributional clustering english words 
proceedings st annual meeting pages columbus oh 
association computational linguistics 
qiu frei 

concept query expansion 
proceedings sigir pages 


experiments linguistically term associations 
information processing management 
salton gerard 

experiments automatic thesaurus construction information retrieval 
proceedings ifip congress pages 
salton gerard chris buckley 

improving retrieval performance relevance feedback 
journal american society information science 
salton gerard michael mcgill 

modern information retrieval 
mcgraw hill new york 
sanderson mark 

word sense disambiguation information retrieval 
proceedings sigir pages 
hinrich 

context space 
robert goldman peter norvig eugene charniak bill gale editors working notes aaai fall symposium probabilistic approaches natural language pages aaai press menlo park ca 
hinrich 

dimensions meaning 
proceedings supercomputing pages minneapolis mn 
hinrich 

ambiguity resolution language learning 
csli publications stanford ca 
hinrich jan pedersen 

information retrieval word senses 
proceedings fourth annual symposium document analysis information retrieval pages las vegas nv 
hinrich jan pedersen 

cooccurrence thesaurus applications information retrieval 
information processing management 
sparck jones karen 

synonymy semantic classification 
edinburgh university press edinburgh 
publication ph thesis university cambridge 
sparck jones karen 

notes early classification 
acm sigir forum 
van rijsbergen 
information retrieval 
second edition 
butterworths london 
voorhees ellen 
wordnet disambiguate word senses text retrieval 
proceedings sigir pages 
walker donald robert 

machine readable dictionaries sublanguage analysis 
ralph grishman richard kittredge editors analyzing language restricted domains sublanguage description processing 
erlbaum associates hillsdale nj pages 
wilks yorick dan fass cheng ming guo james mcdonald tony plate brian slator 

providing machine tractable dictionary tools 
journal computers translation 
willett peter 

trends hierarchic document clustering critical review 
information processing management 

statistical principles experimental design 
second edition 
mcgraw hill new york ny 
yarowsky david 

word sense disambiguation statistical models roget categories trained large corpora 
proceedings coling pages nantes france 
yarowsky david 

unsupervised word sense disambiguation rivaling supervised methods 
proceedings rd annual meeting cambridge ma 
association computational linguistics 

