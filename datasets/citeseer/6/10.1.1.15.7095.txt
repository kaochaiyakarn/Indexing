metacost general method making classifiers cost sensitive pedro domingos artificial intelligence group instituto superior lisbon portugal gia ist utl pt www gia ist utl pt research machine learning statistics related fields produced wide variety algorithms classification 
algorithms assume errors cost seldom case kdd prob lems 
individually making classification learner laborious non trivial 
propose principled method making arbitrary classifier cost sensitive wrapping cost minimizing procedure 
procedure called metacost treats underlying classifier black box requiring knowledge functioning change 
stratification metacost applicable number classes arbitrary cost matrices 
empirical trials large suite benchmark databases show metacost produces large cost reductions compared cost blind classifier rules forms stratification 
tests identify key components metacost varied substantial loss 
experiments larger database indicate metacost scales 
classification primary tasks data mining 
subject research machine learning statistics pattern recognition networks areas decades 
result developed approaches exist including rule induction decision tree induction instance learning linear neural classifiers bayesian learning 
classification problems goal correctly assign examples typically described vectors attributes finite number classes 
currently available algorithms classification designed minimize zero loss error rate number incorrect predictions equivalently probability making incorrect prediction 
implicitly assumes errors costly kdd applications far case 
example database marketing cost mailing non respondent small cost mailing respond entire profit lost 
general misclassification costs may described arbitrary cost matrix cost predicting example belongs class fact belongs class realization real world applications non uniform costs rule exception led years increased interest algorithms cost sensitive classification 
discussed section related turney provides online bibliography topic 
gone making individual algorithms cost sensitive 
doing ll available literature time consuming enterprise far obvious best perform conversion 
potentially better solution procedure converted broad variety error classifiers cost sensitive ones 
knowledge currently available procedure type stratification changing frequency classes training data proportion cost 
approach shortcomings 
distorts distribution examples may seriously affect performance algorithms 
reduces data available learning stratification carried undersampling 
increases learning time done oversampling 
seriously applicable class problems multiclass problems particular type cost matrix cost misclassifying example independent predicted class 
propose new procedure cost sensitive classification attenuates eliminates disadvantages stratification 
procedure metacost wrapping metalearning stage error classifier way classifier effectively minimizes cost seeking minimize zero loss 
section describes detail metacost reasoning leading 
section describes extensive empirical evaluation metacost compared stratification properties particular scalability investigated 
concludes discussion related research directions summary results 
metacost algorithm order obtain procedure making error classifiers cost sensitive start basic notions 
example know probability class bayes optimal prediction class minimizes conditional risk conditional risk expected cost predicting belongs class bayes optimal prediction guaranteed achieve lowest possible cost lowest expected cost oil possible examples weighted probabilities 
rule imply partition example space possibly nonconvex regions class optimal cost prediction region cost sensitive classification find frontiers regions explicitly implicitly 
complicated dependence cost matrix misclassifying examples class expensive relative misclassifying region predicted expand expense regions classes class probabilities remain unchanged 
fact know optimal predictions pre classified examples training set depending cost matrix may may coincide classes examples labeled 
examples training set relabeled classes cost matrix error classifier applied learn optimal frontiers examples labeled frontiers 
limit consistent error learner learn cost minimizing frontiers 
finite sample learner principle worse finding frontiers finding zero loss frontiers training set 
metacost procedure idea 
order training examples optimal classes need find way estimate class probabilities 
note different finding class probabilities unseen examples estimates important insofar influences final frontiers produced probability estimates quite poor lead optimal classification long class minimizes risk estimated probabilities minimizes true ones 
possibility standard probability estimation techniques kernel density estimation 
successful learning cost sensitive classifier approach require machine learning bias implicit assumptions classifier probability estimator valid application domain 
strictly speaking impossible classifier density estimator mismatch probability estimation classification stages hurt performance context similar see related section 
example decision tree rule inducers effective learners high domains kdd precisely domains commonly probability estimation techniques kernel densities mixture models effective 
assumption user chosen particular classifier characteristics suited domain classifier 
classifiers yield class probability estimates product learning poor 
example decision tree rule learners attempting drive class probabilities zero lea rule resulting estimates correspondingly 
classifiers may produce class probabilities metacost allows require 
robust generally applicable method obtaining class probability estimates classifier suggested research model ensembles 
authors breiman modern learners highly unstable applying slightly different training sets tends produce different models correspondingly different predictions examples accuracy remains broadly unchanged :10.1.1.32.9399
accuracy improved learning models way variations combining predictions example voting 
metacost estimates class probabilities learning multiple classifiers example class fraction total vote estimate probability example 
learners unstable fashion described methods applying metacost learners discussed section 
specifically metacost uses variant breiman bagging ensemble method :10.1.1.32.9399
bagging procedure training set size bootstrap resample constructed samples replacement training set 
new training set size produced original examples may appear 
procedure repeated times resulting models aggregated uniform voting unclassified example ensemble labels class predicted greatest number models 
metacost differs bagging number examples resample may smaller training set size allows efficient 
classifier produces class probabilities class vote estimated unweighted average probabilities models example 
estimating class probabilities training example metacost allows models generated consideration learned resamples example included 
type estimate lower variance larger number samples second lower statistical bias influenced example class training set 
short metacost works forming multiple bootstrap replicates training set learning classifier estimating class probability example fraction votes receives ensemble equation training example estimated optimal class reapplying classifier relabeled training set 
pseudo code metacost procedure shown table 
note cost matrix changes final learning stage needs repeated equivalent single run error classifier 
empirical evaluation question metacost reduces cost compared error classifier stratification studied empirically benchmark es uci repository 
decision tree learner error classifier cause de facto role standard empirical comparisons 
rules post processor converts decision trees sets 
rules tends improve accuracy produces simpler comprehensible results 
follows combina tion referred abbreviation 
noted experiments carried randomly selecting examples database table metacost algorithm 
inputs training set classification learning algorithm cost matrix number resamples generate number examples resample true iff produces class probabilities true iff resamples example 
procedure metacost fori resample examples 
mi model produced applying si 
example class ifp mi produced mi class predicted 
ranges ranges class 
model produced applying 
return training classifiers remaining measuring cost predictions 
results reported average runs 
report results experiments multiclass databases followed experiments class databases 
lesion studies scaling study larger database complete section 
multiclass problems experiments conducted different types cost model 
chosen random uniform distribution interval chosen random fixed interval 
different costs generated runs conducted database standard deviations reported incorporate effects varying cost matrix 
second experiment chosen table average costs standard deviations multiclass problems 
database costs fixed interval costs class prob dependent interval metacost metacost annealing audiology glass iris led lenses lung cancer 
post oper 
pr 
tumor solar flare soybean splice wine zoology chosen uniform probability interval probabilities occurrence classes training set 
expected value 
means highest costs misclassifying rare class frequent inversely lowest 
mimics situation practice database marketing domains mentioned rarest classes ones important identify correctly 
case low error rate achieved simply ignoring minority classes cost high 
cost sensitive learning particularly important problems 
different cost matrix generated run 
stratification directly applied arbitrary multiclass cost matrices followed breiman suggestion making ic cost misclassifying example class irrespective class predicted 
training set resampled class probability equal 
see pp 
detailed justification procedure 
done different ways undersampling oversampling 
undersampling procedure examples class highest retained fraction examples class chosen random inclusion resampled training set 
probably frequently type cost stratification disadvantage reducing data available learning may increase cost 
oversampling 
alternative examples class lowest retained examples class duplicated approximately times training set 
avoids loss training data may significantly increase learning time particularly superlinear algorithms 
metacost applied considering example models train rule class probabilities generating resamples size equal original training set false true table 
results obtained shown table graphically 
results obtained variants metacost reported section lesion studies 
fixed interval case form stratification effective reducing costs surprising approximations order apply far true 
contrast metacost reduces costs compared undersampling database compared oversampling 
probability dependent case closely matches assumptions apply stratification undersampling oversampling reduce cost compared databases 
metacost better achieving lower costs forms stratification databases 
globally average cost reduction obtained metacost compared approximately twice large obtained undersampling times oversampling 
sets experiments costs obtained metacost lower algorithms confidences exceeding sign wilcoxon tests 
results support metacost cost oo metacost multiclass class metacost multiclass class multiclass class metacost comparison metacost costs axis undersampling oversampling axes 
point corresponds database type cost matrix 
points line metacost outperformed alternative classifier 
reduction method choice multiclass problems 
class problems class problems stratification applied approximation making proceeding 
letting minority class majority class experiments class databases conducted cost model set alternately 
note absolute values irrelevant algorithm comparison purposes ratio significant 
results obtained settings metacost shown table graphically 
oversampling effective reducing cost cost ratios 
undersampling effective 
metacost reduces costs compared undersampling oversampling databases cost ratios 
cases costs obtained metacost lower algorithms confidences exceeding sign wilcoxon tests sign test undersampling confidence 
results support metacost cost reduction method choice class problems stratification applied approximation 
lesion studies questions arise connection metacost results 
sensitive number resamples 
simply class probabilities produced single run error classifier full training set 
metacost perform better models relabeling example irrespective example learn 
metacost class probabilities produced ignored probability class estimated simply fraction models predicted 
section answers questions carrying relevant experiments 
sake space results class databases results multiclass databases broadly similar 
table reports results obtained variations metacost resamples labeled relabeling training examples class probabilities produced single run data labeled probs ignoring class probabilities produced labeled votes models relabeling example labeled ms 
case table average costs standard deviations class problems 
cost ratio database metacost breast cancer credit diabetes echocardiogram heart disease hepatitis horse colic labor liver disease promoters sonar voting breast cancer credit diabetes echocardiogram heart disease hepatitis horse colic labor liver disease promoters sonar voting breast cancer credit diabetes echocardiogram heart disease hepatitis horse colic labor liver disease promoters sonar voting settings previous sections 
main observations comparing table table range cost increases number resamples decreases gradually 
particular significant difference costs obtained costs ratios metacost reduces costs compared forms stratification datasets cost ratios 
data produces worse results metacost undersampling datasets cost ratios performs similarly undersampling 
outperforms oversampling 
ignoring class probabilities increases cost majority datasets relative differences generally minor 
metacost form outperforms types stratification large majority datasets cost ratios 
multiple runs estimate class probabilities essential 
single run models decreases cost increases 
table average costs standard deviations different versions metacost 
cost ratio database probs votes ms breast cancer credit diabetes echocardiogram heart disease hepatitis horse colic labor liver disease promoters sonar voting breast cancer credit diabetes echocardiogram heart disease hepatitis horse colic labor liver disease promoters sonar voting breast cancer credit diabetes echocardiogram heart disease hepatitis horse colic labor liver disease promoters sonar voting cases relative differences generally minor performance vs stratification generally similar 
summarize crucial element metacost multiple runs estimate class probabilities number runs low sufficient excellent performance 
class probabilities beneficial critical estimating example class probabilities excluding models learn 
scaling obvious potential disadvantage metacost far increases learning time compared error classifier 
databases previous sections learning times order seconds fractions second arguably immaterial 
larger databases serious limitation 
metacost increases time fixed factor number resamples approximately asymptotic time complexity order classifier increased constant may critical large scale applications 
solution lies fact multiple runs error classifier required form probability estimates completely independent trivially parallelized reducing time increase factor table costs cpu times minutes seconds oversampling variants metacost shuttle database 
algorithm noise noise cost time cost time undersampling lo 
potential solution require parallel processing resamples smaller original training set addition relatively small number resamples example 
smaller resamples may result higher costs conceivably lower obtained error learner stratification worthwhile 
increase cost caused learning class probabilities smaller samples may offset exceeded reduction obtained multiple models 
idea breiman successful pasting method scaling learners 
time reducing resample sizes reduce running time factor particularly significant learner superlinear running time 
example classifier running time quadratic number examples tenth examples reduce running time resample factor 
resamples cpu time probability estimation phase order magnitude smaller single run error classifier data insignificant 
test approach feasible experiments carried largest database available uci repository shuttle 
goal database diagnose state space shuttle set sensor readings 
problem suited testing cost sensitive algorithms large majority class normal state easy obtain low error rates presumably cost missing rare anomalous states potentially higher false alarm making error rate inappropriate measure performance 
possible states numeric readings 
database contains examples shuttle flight 
flight training second testing 
runs carried mhz pentium computer 
costs set possible obtain results oversampling running expanded training set exceeded available memory 
judging results previous sections high probability worse undersampling 
table shows costs running times oversampling metacost combinations number resamples resample size function training set size 
metacost true false see table 
undersampling fast reduce cost 
metacost resamples tenth size original training set reduces cost order magnitude running time similar 
increasing number resamples resample sizes predictably increases running time reduce cost 
reducing resample size training set size increases cost 
poor results obtained undersampling suggest metacost excellent performance just due problem easy requiring small number examples achieve low costs 
check repeated experiment class noise added training test sets probability example class changed different classes having probability new 
results shown table 
costs running times higher algorithms undersampling effective reducing cost metacost far best performer 
increasing produces significant improvements 
exceeded available memory 
remarkably metacost order magnitude faster reducing cost order magnitude 
result may appear surprising due fact estimating class probabilities averaging models learned small subsamples effect filtering noise producing cleaner dataset runs faster original 
examining algorithms output shows metacost induces single short rule anomalous state normal state default output order magnitude larger 
known inefficient large noisy databases results may generalize error learners 
preliminary strong indication metacost applied effectively large databases reducing cost significantly increasing running time 
suggest noisy databases metacost may additionally useful method speeding learning 
related cost sensitive learning subject burgeoning literature space allow review 
point reader brief review online bibliography 
section discusses elements previous research closely related metacost 
chan stolfo proposed variation stratification involves learning multiple classifiers stratified subsamples database combined classifier uses outputs inputs 
method produce single model output hard understand metacost produces single model similar complexity error classifier 
compared stratification undersampling chan stolfo method avoids loss training data applicable form class problems 
ad hoc stratification lacking large sample guarantees clear foundations resampling scheme 
metacost requires repeating runs time cost matrix changes 
tested single domain credit card fraud detection 
ting zheng proposed cost sensitive variant boosting ensemble method decision trees 
significant easily adaptable error learners metacost creates cost sensitive learner multiple runs error 
chan stolfo method produce single comprehensible model 
compared regular boosting ad hoc lacking guarantees nearoptimal performance training data 
requires repeating runs time cost matrix changes 
published results appears produce substantially smaller cost reductions metacost 
metacost architecture respects similar cmm meta learner combines multiple models single 
metacost goal reduce costs cmm goal increase comprehensibility retaining accuracy gains multiple models 
metacost uses model ensemble relabel training examples cmm uses label additional artificially generated examples 
combination may conceivably bring advantages 
item carry experiments additional large databases learners 
particular interest apply metacost algorithms unstable respect variations training set nearest neighbor naive bayes 
form metacost may effective algorithms alternative readily suggested results 
method consists learning multiple models different subsets attributes different subsets examples 
nearest neighbor naive bayes unstable respect changes attributes method reduce algorithms errors extent similar bagging learners 
readily incorporated metacost 
application metacost large databases may improved subsamples partitioning bagging 
similar phase chan stolfo method avoid losing useful information resamples 
necessary correct probability estimates obtained effects stratification 
interesting comparison performed shelf probability estimator kernel densities phase metacost multiple runs error classifier 
generally useful previous results may successful domains combinations probability estimator classifier 
generally effect metacost performance quality probability estimates needs investigated 
best done synthetic data know true class probabilities example 
interesting roc analysis metacost varying probability thresholds example relabeling changes class 
current version metacost bagging 
alternative method constructing model boosting 
boosting achieves lower er ror rates bagging metacost produce corresponding reductions cost 
kdd applications hindered lack powerful cost sensitive learners 
converting individual error learners cost sensitive ones tedious difficult process general purpose alternative stratification limited applicability number disadvantages 
proposed evaluated metacost new procedure making error classifiers 
metacost relabeling training examples estimated minimal cost classes applying error learner new training set 
experiments show metacost systematically reduces cost compared error classification stratification large amounts metacost efficiently applied large databases 
research partly funded praxis xxi program 
author grateful provided datasets empirical study 
aha kibler albert 
instance earning algorithms 
machine learning 
bay 
combining nearest neighbor classifiers multiple feature subsets 
proc 
th intl 
conf 
machine learning pp 
madison wi 
bishop 
neural networks pattern recognition 
oxford university press oxford uk 
blake keogh metz 
uci repository machine learning databases 
dept information computer science university nia irvine ca 
www ics uci edu mlearn mlrepository 
ht ml 
breiman :10.1.1.32.9399
bagging predictors 
machine learn ing 
breiman 
pasting bites prediction large data sets line 
technical report statistics dept university california berkeley ca 
breiman 
bag estimation 
technical report statistics dept university california berkeley ca 
breiman friedman olshen stone 
classification regression trees 
wadsworth belmont ca 
chan stolfo 
scalable learning non uniform class cost distributions 
proc 
jth intl 
conf 
knowledge discovery data mining pp 
new york ny 
chan stolfo wolpert editors 
proc 
wkshp 
integrating multiple learned models improving scaling machine learning algorithms 
aaai press portland 
dasarathy editor 
nearest neighbor nn norms nn pattern classification techniques 
ieee computer society press los alamitos ca 
domingos 
linear time rule induction 
proc 
nd intl 
conf 
knowledge discovery data mining pp 
portland 
domingos 
knowledge acquisition exam ples multiple models 
proc 
jth intl 
conf 
machine learning pp 
nashville tn 
domingos 
bagging 
bayesian account implications 
proc 
rd intl 
conf 
knowledge discovery data mining pp 
newport beach ca 
domingos 
get flee lunch simple cost model machine learning applications 
proc 
icml wkshp 
methodology applying machine learning pp 
madison wi 
domingos pazzani 
optimality simple bayesian classifier zero loss 
machine learning 
duda hart 
pattern classification scene analysis 
wiley new york ny 
fayyad piatetsky shapiro smyth 
data mining knowledge discovery overview 
fayyad piatetsky shapiro smyth uthurusamy editors advances knowledge discovery data mining pp 

aaai press menlo park ca 
freund schapire 
experiments new boosting algorithm 
proc 
th intl 
conf 
machine learning pp 
bari italy 
michalski 
theory methodology inductive learning 
artificial intelligence 
provost fawcett 
analysis visual ization classifier performance 
proc 
rd intl 
conf 
knowledge discovery data mining pp 
newport beach ca 
provost fawcett kohavi 
case accuracy estimation comparing induction algorithms 
proc 
th intl 
conf 
machine learning pp 
madison wi 
quinlan 
cj programs machine learning 
morgan kaufmann san mateo ca 
ting zheng 
boosting trees cost sensitive classifications 
proc 
loth european conf 
machine learning pp 
chemnitz germany 
turney 
cost sensitive learning phy 
online bibliography institute information technology national research coun cil canada ottawa canada 
aj iit nrc ca bibliographies cost sensitive html 
zheng 
naive classifier committees 
proc 
loth european conf 
machine learning pp 
chemnitz germany 
