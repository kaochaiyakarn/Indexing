advances neural information systems mit press cambridge ma 
neural information systems conference denver colorado dec 
revolution belief propagation graphs cycles brendan frey www 
cs 
utoronto 
ca frey department computer science university toronto david mackay wol 
ra 
phy 
cam 
ac 
uk mackay department physics cavendish laboratory cambridge university artificial intelligence researchers application probability propagation bayesian belief networks cycles 
probability propagation algorithm exact networks cycle free 
discovered best error correcting decoding algorithms performing probability propagation belief networks cycles 
communicating noisy channel increasingly wired world demands efficient methods communicating bits information physical channels introduce errors 
examples real world channels include twisted pair telephone wires shielded cable tv wire fiber optic cable deep space radio terrestrial radio indoor radio 
engineers attempt correct errors introduced noise channels channel coding adds protection information source channel errors corrected 
popular model physical channel shown fig 

vector information bits 
uk uk encoded vector codeword bits xn transmitted channel 
independent gaussian noise variance added codeword bit brendan frey currently beckman fellow beckman institute advanced science technology university illinois urbana champaign gaussian noise variance encoder decoder communication system channel adds gaussian noise transmitted discrete time sequence 
producing real valued channel output vector yn 
decoder received vector guess fi original information vector 
probability pb bit error minimized choosing uk maximizes uk rate code number information bits communicated codeword bit 
consider rate systems 
simplest rate encoder duplicates information bit optimal decoder repetition code simply averages pairs noisy channel outputs applies threshold 
clearly procedure effect reducing noise variance factor 
resulting probability pb information bit erroneously decoded area tail noise gaussian cumulative standard normal distribution 
plot versus repetition code shown fig 
thumbnail picture shows distribution noisy received signals noise level repetition code gives 
sophisticated channel encoders decoders increase tolerable noise level increasing probability bit error 
approach principle improve performance bound determined shannon 
probability bit error limit gives maximum noise level tolerated matter channel code 
shannon proof nonconstructive meaning showed exist channel codes achieve limit practical encoders decoders 
curve shannon limit shown fig 

curves described define region interest practical channel coding systems 
pb system requires lower noise level repetition code interesting 
extreme impossible system tolerate higher noise level shannon limit 
decoding hamming codes probability propagation way detect errors string bits add parity check bit chosen sum modulo bits 
channel flips bit receiver find sum modulo detect error occurred 
simple hamming code codeword consists original vector pcc le le standard deviation gaussian noise probability bit error versus noise level codes rates near signalling 
impossible obtain shannon limit shown far right rate 
pp hamming code rate decoded probability propagation iterations exact hamming code decoded exactly pp low density parity check coded decoded probability propagation tc pp decoded probability propagation 
thumbnail pictures show distribution noisy received signals noise levels repetition code shannon limit give 
addition parity check bits depends different subset information bits 
way hamming code detect errors correct 
code cast form conditional probabilities specify bayesian network 
bayesian network rate hamming code shown fig 

assuming information bits uniformly random uk uk 
codeword bits direct copies information bits 
codeword bits parity check bits xs lu indicates addition modulo xor 
conditional channel probability densities 
probabilities computed exactly belief network lauritzen spiegelhalter algorithm just brute force computation 
powerful codes discussed exact computations intractable 
way decoder approximate probabilities ly applying probability propagation algorithm pearl bayesian network 
probability propagation approximate case ui bayesian network hamming code 
bayesian network low density parity check code 
block diagram linear feedback shift register 
bayesian network 
network contains cycles ignoring edge directions ut xs ut channel output vector observed propagation begins sending message yn xn 
message sent xk uk 
iteration begins sending messages information variables ut parity check variables xs 
parallel 
iteration finishes sending messages parity check variables back information variables parallel 
time iteration completed new estimates obtained 
pb curve optimal decoding curve probability propagation decoder iterations shown fig 

quite surprisingly performance iterative decoder quite close optimal decoder 
expectation short cycles confound probability propagation decoder 
performance obtained short cycles code network 
simple hamming code complexities probability propagation decoder exact decoder comparable 
similarity performance decoders prompts question probability propagation decoders give performances comparable exact decoding cases exact decoding computationally intractable 
leap limit low density parity check codes explosion interest channel coding community new coding systems brought leap closer shannon limit 
codes described bayesian networks cycles turns corresponding iterative decoders performing probability propagation networks 
fig 
shows bayesian network simple low density paucity check code gallager 
network information bits represented explicitly 
network defines set allowed configurations codewords 
parity check vertex qi requires codeword bits xn neq qi con nected parity neq qi xn clamped ensure parity 
qi set indices codeword bits parity check vertex qi connected 
conditional probability densities channel outputs eq 

way view code binary codeword variables set linear modulo equations 
want degrees freedom number linearly independent parity check equations example codeword bits leaving degrees 
degrees freedom represent information vector code linear dimensional vector mapped valid simply multiplying matrix modulo addition 
encoder produce low density parity check codeword input vector 
channel output vector observed iterative probability propagation decoder begins sending messages iteration begins sending messages codeword variables parity check constraint variables iteration finishes sending messages parity check constraint variables back codeword variables 
time iteration completed new estimates obtained 
valid necessarily correct codeword prespecified limit number iterations reached decoding stops 
estimate codeword mapped back estimate fi information vector 
fig 
shows performance low density parity check code decoded described 
see mackay neal details 
impressively close shannon limit significantly closer concatenated code described lin costello considered best practical code 
leap codeword berrou consists original informa tion vector plus sets bits protect information 
sets produced feeding information bits linear feedback shift register type finite state machine 
sets differ set produced set information bits order bits scrambled fixed way bits fed 
fig 
shows block diagram bayesian network experiments 
box represents delay memory element circle performs addition modulo 
kth information bit arrives machine state sk written binary string state bits bsb shown 
state sk determined current input previous state 
bits just shifted versions bits previous state 
fig 
shows bayesian network simple 
notice state variable constituent chains depends previous state information bit 
chain second output transmitted 
way rate code information bits codeword bits 
conditional probabilities states chain sls state follows input 
conditional probabilities states chain similar inputs permuted 
probabilities information bits uniform conditional probability densities channel outputs eq 

decoding proceeds messages passed channel output variables constituent chains information bits 
messages passed information variables constituent chain messages passed forward backward chain manner forward backward algorithm smyth 
messages passed chain second chain second chain processed forwardbackward algorithm 
complete iteration messages passed information bits 
fig 
shows performance decoded described fixed number iterations 
see frey details 
performance significantly closer shannon limit performances low density parity check code textbook standard concatenated code 
open questions certainly claiming np hard problem cooper proba inference general bayesian networks solved polynomial time probability propagation 
results show practical problems solved approximate inference graphs cycles 
iterative decoding algorithms probability propagation graphs cycles understood decoders 
compared approximate inference techniques variational methods probability propagation graphs cycles 
principled decoders 
mackay neal variational de coder maximized lower bound ii low density parity check codes 
performance variational decoder performance probability propagation decoder 
difficult design small bayesian networks cycles probability propagation unstable 
way easily distinguish graphs propagation graphs propagation unstable 
belief uncommon graphical models community short cycles particularly apt lead probability propagation astray 
possible design networks variety interesting networks hamming code network described propagation works despite short cycles 
probability distributions deal decoding special distributions true posterior probability mass concentrated microstate space size large 
decoding problem find probable microstate may iterative probability propagation decoders true probability distribution concentrated microstate 
believe interesting contentious issues area remain resolved 
frank kschischang bob radford neat discussions related zoubin ghahramani comments draft 
research supported part foundation information technology research council natural sciences engineering research council 
berrou glavieux 
near optimum error correcting coding decoding turbo codes 
ieee transactions communications 
cooper 
computational complexity probabilistic inference bayesian belief networks 
artificial intelligence 
frey 
bayesian networks classification data compression channel coding mit press cambridge ma 
see www 
cs 
utoronto 
ca frey 

low density parity check codes mit press cambridge ma 
lin costello jr 
control coding fundamentals applications prentice hall englewood cliffs nj 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
journal royal statistical society 
mackay neat 
codes sparse matrices 
cryptography coding 
th ima conference number lecture notes computer science springer berlin germany 
mackay neat 
near shannon limit performance low density parity check codes 
letters 
due editing errors reprinted electronics letters 
pearl 
probabilistic reasoning intelligent systems morgan kaufmann san mateo ca 
shannon 
mathematical theory communication 
bell system technical journal 
smyth heckerman jordan 
probabilistic independence networks hidden markov probability models 
neural computation 
