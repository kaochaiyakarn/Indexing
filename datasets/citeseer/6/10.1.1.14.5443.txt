submitted th international conference machine learning icml 
improving text classification shrinkage hierarchy classes andrew mccallum mccallum com ronald rosenfeld roni cs cmu edu tom mitchell mitchell cs cmu edu andrew ng ai mit edu just research henry street pittsburgh pa school computer science carnegie mellon university pittsburgh pa mit ai lab tech square cambridge ma documents organized large number topic categories categories arranged hierarchy 
patent database yahoo examples 
shows accuracy naive bayes text classifier significantly improved advantage hierarchy classes 
adopt established statistical technique called shrinkage smoothes parameter estimates data sparse child parent order obtain robust parameter estimates 
approach employed deleted interpolation technique smoothing grams language modeling speech recognition 
method scales large data sets numerous categories large hierarchies 
experimental results real world data sets usenet yahoo corporate web pages show improved performance reduction error traditional flat classifier 
keywords text classification information retrieval hierarchical modeling statistical language modeling bayesian learning dramatic expansion world wide web continues amount line text grows development methods automatically categorizing text important 
variety demonstrated success statistical approaches learning classify text documents joachims koller sahami yang pederson nigam approaches tfidf salton naive bayes lewis ringuette typically represent documents vectors words learn gathering statistics observed frequencies words documents belonging different classes 
rely learned word statistics approaches data intensive require large numbers hand labeled training documents class achieve high classification accuracy 
considers question scale statistical learning algorithms tasks large number classes sparse training data class 
humans organize extensive data sets fine grained categories topic hierarchies employed large collection categories manageable 
yahoo patent database medline dewey decimal system examples hierarchies 
technique leverages commonly available topic hierarchies order significantly improve classification accuracy especially hierarchy large training data class sparse 
method exponentially reducing amount computation necessary classification sacrificing small amount accuracy 
approach applies understood technique statistics called shrinkage provides improved estimates parameters uncertain due limited amounts training data stein james stein technique exploits hierarchy shrinking parameter estimates data sparse children estimates ancestors ways provably optimal appropriate conditions 
employ simple form shrinkage creates new parameter estimates child linear interpolation hierarchy nodes child root 
interpolation weights learned form expectation maximization dempster form shrinkage applied deleted interpolation technique smoothing grams language modeling speech recognition jelinek mercer note approach text classification hierarchy quite different koller sahami koller sahami machine employs hierarchy learning separate classifiers internal node tree labeling document classifiers greedily select sub branches reaches leaf 
approach shown helpful documents represented small subset 
words available vocabulary different subset vocabulary selected node hierarchy 
approach show improvement larger vocabularies domains established large vocabulary sizes perform best joachims nigam somewhat surprisingly shown pure form machine classification maximum likelihood estimates constant vocabulary fact equivalent performing non hierarchical classification mitchell remainder structured follows explain probabilistic approach text classification shrinkage context 
show experimental results real world data sets related close discussion 
probabilistic framework naive bayes approach task text classification bayesian learning framework 
assume text data generated parametric model training data calculate estimates model parameters 
armed estimates classify new test documents bayes rule turn generative model calculate posterior probability class generated test document question 
classification simple matter selecting probable class 
assume data generated mixture model parameterized correspondence mixture model components classes fcg 
dictates document created selecting class class priors having corresponding mixture component generate document parameters distribution jc 
marginal probability generating document sum total probability mixture components jcj jc document comprised ordered sequence word events drawn vocabulary naive bayes assumption probability word event document independent word context class furthermore independent position document 
document drawn multinomial distribution independent trials number words write ik word position document subscript indicates index vocabulary 
probability document class jc jd ik jc assumption correspondence mixture model components classes naive bayes assumption position independence assumption mixture model composed disjoint sets parameters class parameter set class composed probabilities word jt jc jt 
parameters model class prior probabilities written 
set labeled training documents calculate estimates parameters model generated documents 
estimates consist straightforward counting events supplemented standard laplace smoothing primes estimate count avoid probabilities zero 
define count number times word occurs document define jd document class label 
estimate probability word class jt jc jdj jd jv jv jdj jd class prior parameters set maximum likelihood estimate jc jdj jd jdj 
estimates parameters calculated training documents classification performed test documents calculating posterior probability class evidence test document selecting class highest probability 
formulate applying bayes rule substituting jc equations 
jd jc jd ik jc jcj jd ik jc despite fact mixture model word independence assumptions strongly violated real world data naive bayes performs classification 
domingos pazzani discuss violation word independence assumption little damage classification accuracy domingos pazzani hierarchical classification section presents method improving estimates model parameters advantage hierarchy 
briefly describe shrinkage general sense discuss application text classification hierarchy mechanics algorithm 
background shrinkage wish estimate parameters jcj class probability distribution words parameter estimate estimators improved shrinking common value 
see carlin louis summary shrinkage 
justifications shrinkage 
quantities jcj thought similar regarded draws common distribution 
case shrinkage estimator just bayes estimate 
surprisingly quantities completely unrelated data estimator independent shrinkage estimators reduce risk estimators 
deep counterintuitive fact discovered stein james stein 
shrinkage text classification shrinkage better estimate probability word class jt node tree construct maximum likelihood ml estimate data associated node equation laplace smoothing 
improved estimate leaf node derived shrinking ml estimate ml estimates ancestors estimates path leaf root 
statistical language modeling terms build unigram model node tree smooth leaf model linearly interpolating models path root 
estimates path leaf root represent tradeoff specificity reliability 
estimate leaf specific pertinent biased data topic 
reliable usually amount data 
estimator root reliable specific 
root contains finite amount data may estimate rare words 
extend tree adding root uniform estimate 
longer need smooth individual ml estimates 
ensure ml estimates path independent subtract child data parent calculating parent ml estimate 
estimate data belongs siblings said child child 
note way path leaf root datum tree exactly ml estimates providing independence estimates efficient training data 
determining mixture weights set ml estimates path leaf root uniform estimate decide weights interpolating mixing 
estimates estimate leaf gamma depth class tree 
interpolation weights ancestors class written 
write new estimate class conditioned word probabilities shrinkage 
jc gamma gamma jv derive empirically optimal weights ancestors finding weights maximize likelihood hitherto unseen heldout data 
fact likelihood data mixture model convex function weights falls jensen inequality attains single global maximum 
find maximum leaf class iterative procedure initialize set initial values say normalized initial values 
iterate calculate words held set fi generate derive new guaranteed improved weights normalizing fi fi fi terminate convergence likelihood function usually achieved dozen iterations 
algorithm viewed particularly simple form em dempster datum assumed generated choosing tree nodes path root say probability estimate generate datum 
em maximizes total likelihood choices estimates various data unknown 
step iterative part step second step 
conceptually simple method inefficient available training data carving held set 
overcome problem modify algorithm follows available data construct ml estimates optimize weights 
document algorithm ml estimates modified exclude data independent 
method known leave commonly statistical estimation 
technique finding optimal weights routinely statistical language modeling interpolate different models trigram bigram unigram uniform known deleted interpolation jelinek mercer experimental results section provides empirical evidence shrinkage reduces text classification error 
show shrinkage helps training class data sparse number classes large 
demonstrate dynamically pruning tree exponentially reduces computation time minimal loss accuracy 
experiments different real world data sets consisting usenet articles web pages 
results averages cross validation trials 
industry sector hierarchy available market guide 
www com data collected mark craven consists web pages classified hierarchy industry sectors 
implementation currently hardcoded handle hierarchies depth remove classes depth different 
result web pages partitioned classes 
tokenizing data skip mime headers html tags stoplist stem 
removing tokens occur corpus contains words vocabulary size 
newsgroups data set collected ken lang contains articles evenly divided usenet discussion groups joachims topic classes quite confusable computers discuss religion 
data set build level hierarchy classes fit top level categories vehicles computers politics religion sports 
tokenize data way 
resulting data set removing words occur contains words vocabulary size 
data sets available line 
see www cs cmu edu 
gathered entirety yahoo science hierarchy july 
web pages divided disjoint classes containing pages result coalescing classes hierarchy depth greater removing classes fewer documents 
tokenizing removing stopwords words occur corpus contains words vocabulary size 
gathered yahoo health hierarchy resulting classes documents words 
feature selection performed selecting words highest mutual information class 
previous study method best text common methods yang pederson addition selecting features traditional flat mutual information hierarchy feature selection 
hierarchical feature selection selects top words mutual information internal node tree node immediate children classes 
corresponds koller sahami hierarchical feature selection zero dependencies koller sahami define total vocabulary union vocabularies chosen internal nodes 
hierarchical classification improves accuracy 
shows classification accuracy industry sector data set train test splits varying vocabulary size 
partial credit classification neighbors true class 
note larger vocabulary sizes generally perform better consistent previous results naive bayes data sets joachims nigam second note hierarchical feature selection somewhat improves performance flat naive bayes mid range feature selection words traditional flat feature selection obtains accuracy hierarchical feature selection reaches 
third importantly observe shrinkage improves classification accuracy board making largest improvement full unpruned vocabulary size achieves accuracy 
comparison flat classifier reaches best performance words 
difference represents reduction classification error 
maintain low frequency words contribute significantly correct classifications shrinkage helps reduce variance estimates larger parameter space results larger vocabulary 
large vocabularies need computational concern 
experiments vocabulary feature set size hierarchical naive bayes hierarchical feature selection flat naive bayes hierarchical feature selection flat naive bayes classification accuracy industry sector data set varying vocabulary size horizontal axis 
tiny vertical bars data point indicate standard error 
performance best full vocabulary shrinkage reduces error 
shrinkage helps training data sparse 
shows accuracy newsgroups data set full vocabulary varying amount training data 
experiments indicate accuracy domain highest feature selection flat hierarchical classifiers small amounts training data 
interesting see hierarchical modeling provides improvement data set industry sector corpus 
expect due significantly reduced branch factor smaller hierarchy 
industry sector hierarchy mean number siblings mean number siblings 
child data borrow strength 
second expected result confirmed graph shrinkage provides improvement amount training data small shrinkage reduces variance classifications notice larger error bars flat classification curve 
class infinite amount training data accurate parameter estimates obtained class largest vocabulary takes seconds classify industry sector documents write results disk 
comparison smallest vocabulary takes seconds difference seconds document average 
number training documents hierarchical naive bayes flat naive bayes classification accuracy newsgroups data set varying amounts training data 
vertical axis zoomed magnification error bars 
hierarchical modeling provides improvement industry sector data set hierarchy smaller 
notice expected shrinkage helps training data 
independently training data sparse estimates improved shrinkage smooth class parameters ancestors 
finding confirmed experiments yahoo data set 
shows classification accuracy science hierarchy function vocabulary size 
best performance hierarchical flat classification equal 
classes training documents shrinkage provides improvement accuracy flat hierarchical best performing vocabulary size 
classes large amount training data shrinkage effect hurts slightly 
probably due fact shrinkage performed linear interpolation optimal certain conditions satisfied believe performance shrinkage improved data heavy classes complex standard bayes empirical bayes techniques discussed carlin louis plan investigate 
majority training testing documents fall classes accuracy averaged testing documents improved shrinkage 
users document classification may care accuracy small vocabulary feature set size hierarchical naive bayes hier vocab flat naive bayes hier vocab vocabulary feature set size hierarchical naive bayes hierarchical feature selection flat naive bayes hierarchical feature selection classification accuracy yahoo data set varying vocabulary sizes 
yahoo science 
yahoo science accuracy averaged classes documents 
classes accuracy large classes 
plots accuracy averaged classes documents 
see presence large classes shrinkage increases accuracy 
note shrinkage implementation handle hierarchies depth greater able represent yahoo deepest leaf classes 
classes small amount training data nature yahoo hierarchy creators split categories large 
simulate shallow thin version effect testing shrinkage yahoo health hierarchy coalesced depth classes documents removed 
measured traditional accuracy averaged test documents shrinkage increases classification accuracy points 
experiments full deeper wider tree expect effect increase 
pruning tree increased computational efficiency addition improving accuracy class hierarchy leveraged improve computational efficiency 
classifier avoid calculating jd majority classes leaves tree pruning tree dynamically classification document 
machine koller sahami classify document internal nodes tree choose calculate probabilities classes underneath branches selected higher level coarse grained classifiers 
note pruning classification interior tree opportunity error deeper hierarchy opportunities error compound 
expected experimental results show performing pruning reduce classification accuracy 
may willing accept reduction exchange exponential reduction amount computation necessary classification 
industry sector data set averaged runs pruning removes consideration single branch interior node reaches accuracy points lower pruning 
machine paradigm allows comparison classification scores leaves share parent 
prune aggressively 
pruning keeps branches attains 
pruning branches achieves 
result half percent obtained full evaluation tree pruning 
related shrinkage estimation considered standard methodology statistics 
routinely vast array problems theoretical properties studied bayesian frequentist points view 
discussion ample examples contained carlin louis shrinkage leave techniques derive language model jelinek mercer known deleted interpolation 
interpolation language models path tree described bahl seymore rosenfeld classified speech recognizer output multiple topics automatically derived topic tree interpolate models associated appropriate nodes tree 
variety information retrieval machine learning communities demonstrated success statistical approaches learning classify text documents 
naive bayes text classification due probabilistic foundations applied extensions lewis ringuette joachims nigam earlier approach hierarchical document classification machine proposed koller sahami koller sahami method differs significantly shrinkage 
machine classifies documents internal nodes tree greedily selects sub branches reaches leaf 
classification errors internal nodes compound accuracy internal nodes high order accuracy higher flat classifier especially deeper hierarchies 
experimented schemes allow lower node reject document send back tree re classification find 
koller sahami results small vocabularies words results literature indicate large vocabulary sizes higher accuracy joachims nigam possible explanation discrepancy koller sahami binomial model multinomial model sahami personal communication 
experiments multinomials outperform 
shrinkage allowed robustly keep large vocabulary sizes believe necessary classifying large data sets large numbers diverse classes 
learning method uses em set mixture weights ancestors hierarchy adaptive mixtures probabilistic transducers singer node hierarchy represents history window linearly mixed parent turn mixed parent 
model applied success noun phrase recognition 
examined class hierarchies improving text classification 
amount line text increases number topic categories organized grows hierarchies increasingly prevalent way collection categories manageable 
need text classification algorithms take advantage hierarchies important 
demonstrate shrinkage class hierarchy improves parameter estimation reduce text classification error 
shrinkage helps especially sparse training data shrinkage beneficial scale larger higher resolution deeper hierarchies classes require larger vocabularies 
improvements due shrinkage increasingly strong move away simple naive bayes classifiers complex probabilistic models parameters sparser training data 
show class hierarchy exponentially reduce amount computation required classify documents sacrificing significant classification accuracy 
explore alternative methods shrinkage including improved methods setting mixture weights 
em cluster data parent allow child mix em clusters independently 
addition plan explore ways learn class hierarchy investigating methods specifically aim increase classification accuracy 
acknowledgments larry wasserman contributed valuable discussions pointers statistics literature comments 
kamal nigam provided numerous helpful suggestions earlier drafts 
market guide permission industry sector hierarchy mark craven gathering data web 
yahoo 
permission data 
research supported part darpa hpkb program contract 
bahl bahl peter brown peter desouza robert mercer 
tree statistical language model natural language speech recognition 
ieee transactions acoustics speech signal processing 
carlin louis carlin louis 
bayes empirical bayes methods data analysis 
chapman hall 
dempster dempster laird rubin 
maximum likelihood incomplete data em 
algorithm 
journal royal statistical society series 
domingos pazzani domingos pazzani 
independence conditions optimality simple bayesian classifier 
machine learning 
james stein james stein 
estimation quadratic loss 
proceedings fourth berkeley symposium mathematical statistics probability pages 
university california press 
jelinek mercer fred jelinek robert mercer 
interpolated estimation markov source parameters sparse data 
gelsema kanal editors pattern recognition practice pages 
joachims thorsten joachims 
probabilistic analysis rocchio algorithm tfidf text categorization 
international conference machine learning icml 
koller sahami koller sahami 
hierarchically classifying documents words 
icml proceedings fourteenth international conference machine learning pages 
morgan kaufmann 
lewis ringuette david lewis marc ringuette 
comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pages 
mitchell tom mitchell 
conditions equivalence hierarchical flat bayesian classifiers 
www cs cmu edu tom ps 
nigam kamal nigam andrew mccallum sebastian thrun tom mitchell 
learning classify text labeled unlabeled documents 
submitted aaai 
www cs cmu edu mccallum 
salton salton 
developments automatic text retrieval 
science 
seymore rosenfeld seymore ronald rosenfeld 
story topics language model adaptation 
eurospeech september 
singer yoram singer 
adaptive mixtures probabilistic transducers 
neural computation 
stein stein 
usual estimator mean multivariate normal distribution 
proceedings third berkeley symposium mathematical statistics probability pages 
university california press 
yang pederson yiming yang jan pederson 
feature selection statistical learning text categorization 
icml pages 
yang yiming yang 
evaluation statistical approaches text categorization 
technical report cmu cs computer science department carnegie mellon university 

