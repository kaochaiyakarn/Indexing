feature reduction hierarchy classifiers fast object detection video images bernd heisele thomas serre mukherjee tomaso poggio center biological computational learning cambridge ma usa honda americas boston ma usa heisele serre tp ai mit edu step method speed object detection systems computer vision support vector machines svms classifiers 
step perform feature reduction choosing relevant image features measure derived statistical learning theory 
second step build hierarchy classifiers 
bottom level simple fast classifier analyzes image rejects large parts background 
top level slower accurate classifier performs final detection 
experiments face detection system show combining feature reduction hierarchical classification leads speed factor similar classification performance 
object detection tasks computer vision computationally expensive large amount input data processed complex classifiers robust pose illumination changes 
speeding classification major concern developing systems real world applications 
investigate methods speed ups feature reduction hierarchical classification 
system detecting frontal near frontal views faces gray images 
system achieved high detection accuracy classifying gray patterns non linear svm 
searching image faces different scales took minutes pc far long real world applications 
way speed reduce number features 
basically types feature selection methods literature filter wrapper methods 
filter methods preprocessing steps performed independently classification algorithm error criteria pca example filter method 
wrapper methods attempt search space feature subsets criterion classification algorithm select optimal feature subset 
wrapper methods provide accurate solutions filter methods general computationally expensive 
new wrapper method reduce dimensions input feature space svm 
alternative approach speeding svm classification proposed reducing number support vectors 
feature reduction generic tool applied classification problem 
dealing specific classification task prior knowledge type data speed classification 
assumptions hold vision object detection tasks vast majority analyzed patterns image belongs background class background patterns easily distinguished objects 
assumptions sensible apply hierarchy classifiers 
fast classifiers remove large parts background bottom middle levels hierarchy accurate slower classifier performs final detection top level 
idea falls framework coarse fine template matching related biologically motivated attention vision 
cascade linear classifiers trained adaboost proposed frontal face detection 
idea related sense combines hierarchical classification feature selection 
approach complexity classifiers hierarchy controlled number features image resolution class decision functions class svm kernel functions 
bottom level hierarchy consists linear classifier operates low resolution patterns top level consists non linear classifier operating higher resolution patterns 
section give brief overview svm theory describe training test data experiments 
section rank select features input space 
feature selection feature space classifier described section 
section hierarchical structure classifiers 
concluded section 
background support vector machine theory svm performs pattern recognition problem determining separating hyperplane maximum distance closest points training set 
closest points called support vectors 
perform non linear separation input space nonlinear transformation maps data points input space high dimensional space called feature space 
mapping represented svm classifier kernel function defines inner product examples decision function svm linear feature space written optimal hyperplane maximal distance space closest points training data 
determining hyperplane leads maximizing functional respect constraints 
upper bound expected error probability svm classifier distance support vec tors separating hyperplane radius smallest sphere including points training data feature space 
bound expected error probability rank select features 
computational issues non linear kernel investigated second degree polynomial kernel successfully applied various object detection tasks 
eq 
shows ways computing decision function 
kernel representation right side eq 
number multiplications required calculate decision function second degree polynomial kernel dimension input space number support vectors 
number independent dimensionality feature space 
depends number support vectors linear size training data 
hand computation decision function feature space independent size training samples depends dimensionality feature space 
second degree polynomial kernel feature space dimension number multiplications required projecting input vector feature space computing decision function eq 
see computation svm second degree polynomial efficiently done feature space number support vectors bigger 
case experiments number support vectors times larger 
investigated methods reducing number input features methods feature reduction feature space 
training test sets experiments training test sets 
positive training set contained faces 
negative training set contained randomly selected non face patterns 
relatively small size training set affects classification performance 
experiments show classifier false positive rate reduced factor increasing training set bootstrapping methods 
main goal speed classifier loss classification performance 
opted small training set order save time training classifiers numerous experiments 
test set extracted cmu test set extracted faces non face patterns 
nonface patterns selected linear svm classifier test set subset cmu test set consists images faces 
excluded images containing line drawn faces non frontal faces 
non face patterns similar faces 
final evaluation system performed entire cmu test set containing images 
processing images different scales resulted analyzed windows 
dimension reduction input space ranking features input space gradient descent method proposed rank input features minimizing bound expectation leave error classifier 
basic idea re scale dimensional input space diagonal matrix minimized 
new mapping function kernel function decision function eq 
maximization problem eq 
subject constraints 
solve iterative procedure initialize vector ones solve eq 
margin radius 
values equations bound eq 
compute minimizing gradient descent procedure 
start new iteration algorithm current iteration initialization 
applied ranking method gray features generated preprocessing image patterns described 
additionally performed tests pca gray features obtained projecting data points dimensional eigenvector space 
pca computed combined set positive negative sets 
computed iteration algorithm experiments 
tests performed small test set ranked features 
roc curves second degree polynomial svms shown fig 

features difference gray pca gray features 
features pca gray features gave clearly better results 
reason focused remainder pca features 
interesting observation ranking pca features obtained described gradient descent method similar ranking decreasing eigenvalues 

roc curves gray top pca gray bottom features best ranked features 
selecting features input space section ranked features scaling factors 
problem determine subset ranked features problem formulated finding optimal subset ranked features possible subsets number selected features 
measure classification performance svm subset ranked features bound expected error probability 
simplify computation algorithm avoid solving quadratic optimization problem order compute radius approximated previously normalized data range 
result points lay dimensional cube length smallest sphere including data points upper 
approximation estimated bound expected error versus number principal components 
values axis normalized number training samples 
dimension feature space second degree polynomial kernel type get number selected features estimated bound expected error shown fig 

training error selected features 
estimated bound expected error shows plateau features increases steadily 
bound eq 
considered loose bound expected error 
check bound practical selecting number features performed tests cmu test set 
fig 
compare roc curves obtained different numbers selected features 
results show features improve performance system 
observation coincides run curve fig 

error test set change significantly features estimated bound expected error shown fig 
increases 
probably bound gets looser increasing number features approximation dimensionality feature space 
feature reduction feature space previous section described reduce number features input space 
consider bound 
second degree polynomial svm dimension feature space 
roc curves different number pca gray features 
problem reducing number features feature space 
new method contribution features feature space decision function svm 
second degree polynomial kernel feature space dimension contribution feature decision function eq 
depends 
straightforward way order features ranking 
alternatively weighted support vectors account different distributions features training data 
features ordered ranking denotes th component support vector feature space methods trained svm second degree polynomial kernel pca gray features input space corresponds features calculated support vectors decision function features ranking 
note contrast previously described selection features input space method require retraining svms different feature sets 
results fig 
show ranking weighted components lead faster convergence eq 
fig 
shows roc curves 
classifying support vectors reduced number features 
axis shows number features axis mean absolute difference output svm features svm features 
features ranked components weighted components normal vector separating hyperplane 
features 
added roc curve second degree svm trained original gray features 
corresponds components feature space 
combining methods feature reduction reduce dimensionality factor loss performance 
hierarchy classifiers system overview object detection problems majority analyzed image patches belongs background class 
small percentage patches look similar objects require highly accurate classifier avoid false classifications 
reason developed level hierarchy classifiers computational complexity classifiers increases level 
propagating patterns classified faces quickly decrease amount data going hierarchy 
bottom level hierarchy consisted linear svm trained face images 
second level increased image resolution factor face patterns kept linear kernel 
third level best classifier nonlinear svm second degree polynomial kernel trained images 
classifier highly sensitive translation 
face centered classi 
roc curves different dimension feature space 
fication window classified non face 
order faces search faces small neighborhood detection location propagated second level 
means analyze patterns level pattern classified face level 
fig 
gives impression performance individual classifiers shown detection results images cmu test set 
experiments classifiers hierarchical system trained training set faces randomly selected non face patterns 
train classifier layer images pixels 
classifiers layers trained gray value features described section 
third classifier trained pca gray features determined feature selection techniques described sections features feature space determined pca gray features input space 
roc curves individual classifiers shown fig 
cmu test set 
fig 
show data flow hierarchy cmu test set 
classifier removes background 
final classifier selective classifies input patterns non faces 
fig 
compare roc curve level system roc curve original single svm classifier second degree polynomial kernel 
performances similar 
average computing time image shown table 
achieved speed factor compared original system 

roc curves classifiers hierarchical system cmu test set 
speed methods object detection systems feature reduction hierarchical classification 
feature reduction done ranking selecting pca gray features classification criterion derived learning theory 
applied face detection system remove original features loss classification performance 
quickly remove large background parts image arranged classifiers increasing computational complexity hierarchical structure 
experiments face detection system show combination feature selection hierarchical classification speeds system factor maintaining classification accuracy 
run experiments larger training sets apply feature reduction levels hierarchical classifier explore ways finding optimal number levels 
perform experiments hierarchical training classifier trained outputs classifier previous level 
authors prentice helping experiments research partially sponsored darpa contract 
national science foundation contract 
iis 
additional support provided dfg eastman kodak compaq 
blum langley 
selection relevant features examples machine learning 
artificial intelligence 
roc curve level hierarchical system original system cmu test set 

burt 
smart sensing pyramid vision machine 
proc 
ieee 
heisele poggio pontil 
face detection gray images 
memo center biological computational learning mit cambridge ma 
itti koch niebur 
model saliency visual attention rapid scene analysis 
ieee trans 
pattern analysis machine vision 
kohavi 
wrappers feature subset selection 
artificial intelligence special issue relevance 
oren papageorgiou sinha osuna poggio 
pedestrian detection wavelet templates 
ieee conference computer vision pattern recognition pages san juan 
torr blake 
computationally efficient face detection 
proc 
iccv ii 
rosenfeld 
coarse fine template matching 
ieee transactions systems man cybernetics 
rowley baluja kanade 
rotation invariant neural network face detection 
computer technical report cmu cs cmu pittsburgh 

sung 
learning example selection object pattern recognition 
phd thesis mit artificial intelligence laboratory center biological computational learning cambridge ma 
vapnik 
statistical learning theory 
john wiley sons new york 
viola jones 
robust real time face detection 
proc 
iccv 
weston mukherjee chapelle pontil poggio vapnik 
feature selection support vector machines 
advances neural information processing systems 

data flow level hierarchy classifiers determined cmu test set 

detections level hierarchy 
system typical speed detection time factor single degree polynomial svm single degree polynomial svm feature reduction level hierarchy feature reduction table 
computing time image processed dual pentium iii mhz 
original image rescaled steps detect faces resolutions pixels 
