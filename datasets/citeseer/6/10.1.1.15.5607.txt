latent maximum entropy principle wang dale schuurmans zhao department statistics university toronto canada school computer science university waterloo canada department computer engineering computer science university missouri columbia usa extension jaynes maximum entropy principle handles latent variables 
principle latent maximum entropy propose di erent jaynes maximum entropy principle maximum likelihood estimation yields better estimates presence hidden variables limited training data 
show solving latent maximum entropy model poses hard nonlinear constrained optimization problem general 
show feasible solutions problem obtained ciently special case log linear models forms basis cient approximation latent maximum entropy principle 
derive algorithm combines expectation maximization iterative scaling produce feasible log linear solutions 
algorithm interpreted alternating minimization algorithm information divergence reveals intimate connection latent maximum entropy maximum likelihood principles 
select final model generate series feasible candidates calculate entropy choose model attains highest entropy 
experimental results show estimation latent maximum entropy principle generally gives better results maximum likelihood estimating latent variable models small observed data samples 
index terms maximum entropy iterative scaling expectation maximization latent variable models information geometry alternating minimization graphical models random fields probabilistic inference statistical machine learning learning world requires system extract useful sensory features form model interact concepts 
maximum entropy principle ective method combining sources evidence complex structured natural systems wide application science engineering economics 
ectiveness principle arises ability model distributions random variables combining critical features functions random variables log linear form 
yield succinct representation complex joint distribution allow ective generalization practical inference realized standard graphical models bayesian networks markov random fields 
standard graphical models making direct conditional independence assumptions domain principle requires specification certain properties data model respect example marginal means model match marginal means data 
applications specifying constraints model form easier proposing conditional independence properties :10.1.1.43.7345
weakness standard approach handles constraints observed data directly model latent variable structure 
standard principle allow missing data constraints infers existence hidden variables 
weakness problematic practice natural patterns wish classify result causal processes hidden hierarchical structure yielding data report value latent variables 
example natural language data rarely reports value hidden semantic variables syntactic structure 
propose latent maximum entropy principle lme explicitly handles latent variables extends jaynes original principle case data components missing 
formulate problem latent variables explicitly encoded model 
constrained optimization problem results complex introduce log linear assumption allows derive practical algorithm em obtaining feasible solutions 
em algorithm iterative technique combines expectation maximization em iterative scaling yield convergent procedure guaranteed produce log linear models satisfying desired feature expectations 
develop em show intimate connection latent maximum entropy principle maximum likelihood estimation mle 
latent maximum entropy maximum likelihood principles remain distinct sense feasible solutions lme chooses model maximizes entropy mle selects model maximizes likelihood 
compare di erent approaches estimating hidden variable models main estimation algorithm em repeatedly solves di erent feasible log linear models calculates entropy selects model attains highest entropy 
order implement algorithm exploit fact entropy ciently determined feasible log linear models produced em experimental results show lme principle implemented em algorithm consistently achieves better estimates maximum likelihood estimation estimating hidden variable models small samples observed data 
motivation jaynes proposed maximum entropy principle statistical inference states data summarized model maximally respect missing information 
infer probability distribution data distribution satisfy known constraints distributions consistent constraints choose distribution maximum entropy 
principle understood clearly considering case modeling single real variable simple example assume observe random variable reports people heights population 
sample data trust simple statistics sample mean sample mean square represented data 
jaynes principle suggests infer distribution maximum entropy subject constraints mean mean square values match sample values ey ey respectively 
case known maximum entropy solution gaussian density mean variance consequence known fact gaussian random variable largest di erential entropy random variable specified mean variance 
assume observing data histogram find peaks empirical data 
obviously standard solution appropriate model bi modal data continue postulate uni modal distribution 
existence peaks data accidental 
example sub populations represented data male female di erent height distributions 
case height measurement accompanying hidden gender label indicates sub population measurement taken 
additional knowledge incorporated framework 
way explicitly add missing label data 
denotes person height gender label obtain labeled measurements 
case formulate problem follows 
indicator function 
nk nk denote set observed heights 
definitions formulate problem max subject dx dx dx problem find joint model maximizes entropy matching expectations 
fully observed data case witness gender label obtain separable optimization problem unique solution 
case maximum entropy solution mixture gaussian distributions specified 
unfortunately obtaining fully labeled data tedious impossible realistic situations 
cases variables unobserved jaynes principle maximally respect missing information insu cient 
example gender label unobserved reduced inferring single uni modal gaussian 
cope missing non arbitrary hidden structure extend principle account underlying causal structure data model 
lme principle formulate latent maximum entropy lme principle random variable denoting complete data observed incomplete data missing data 

example observed natural language form text text missing syntactic semantic information denote densities respectively denote conditional density dz 
notation propose latent maximum entropy principle follows 
lme principle features specifying properties match data select joint probability model space probability distributions maximize entropy log dx subject constraints dx dz 
empirical distribution observed data denotes set observed values conditional distribution latent variables observed data 
intuitively constraints specify require expectations joint model match empirical expectations incomplete data account structure implied dependence unobserved component note conditional distribution implicitly encodes latent structure nonlinear mapping 
dz dx denotes finite measure finite countably infinite counting measure integrals reduce sums 
subset finite dimensional space lebesgue measure 
combination cases combination measures 
definition 
clearly nonlinear function division 
missing data problem reduced jaynes model constraints dy 
requirement framework sense lme principle general unfortunately find straightforward formulation lme yield simple closed form solution optimal distribution 
constraining distribution exponential log linear form able show equivalence satisfying constraints achieving feasibility locally maximizing likelihood 
equivalence allow derive practical algorithm finding feasible solutions section 
finding lme solutions consider problem finding joint distribution satisfies lme principle set features data example features specify su cient statistics desired exponential model 
problem amounts solving constrained optimization problem 
unfortunately due mapping constraints nonlinear feasible set longer convex 
objective function concave unique maximum guaranteed exist 
fact minima saddle points may exist 
attempt derive iterative training procedure finds approximate local solutions lme problem 
define lagrangian dx dz natural way proceed optimization iteratively hold fixed compute unconstrained maximum lagrangian arg max refer dual function 
note weak duality dual function provides upper bounds optimal value original lme problem max strong duality holds min min min max obtain closed form solution terms plug reduce constrained optimization unconstrained minimization respect 
attempting solve run di culty 
attempt solve take derivative respect try set log dz dz log dz 
unfortunately resulting system nonlinear simple closed form solution approximating lme solutions restriction log linear form original lme principle yield simple closed form solution look approximate solution 
ignoring term equation setting remainder zero find exp exp dx normalizing constant ensures dx 
hope approximately log linear 
note impose additional constraint log linear plug back definition lagrangian obtain closed form approximation dual function log dz assumption log linear model approximately reduce original constrained optimization simpler unconstrained minimization problem arg min 
assuming easily recover normalization constant attempt solve take derivative respect obtain dx dz dz dz dz unfortunately system equations nonlinear due terms yield simple closed form solution log linear assumption easy satisfy lme principle 
valuable progress formulating practical algorithm approximately satisfying lme principle assumption log linearity 
fact point show intimate connection lme principle maximum likelihood estimation mle principle log linear models 
theorem log linear assumption maximizing likelihood log linear models incomplete data equivalent satisfying feasibility constraints lme principle 
distinction mle lme log linear models local maxima feasible solutions lme selects model maximum entropy mle selects model maximum likelihood 
proof assuming log linear model prove satisfying constraints lme principle equivalent achieving local maxima log likelihood 
restrict complete model log linear form exp 
dz log likelihood function observed incomplete data log log derivative respect yields dx dz dz dx dz setting obtain original constraints 
feasible solutions satisfy conditions stationary points log likelihood function 
establishes part theorem 
remains show mle lme principles remain distinct log linear models 
prove proving log likelihood function entropy related equation non constant function maxima generally coincide 
fact proved theorem section 
result conclude feasible log linear solutions mle lme maximize objective produce di erent solutions 
problem maximum likelihood estimation log linear models missing data previously studied lauritzen riezler previously observed locally maximizing likelihood log linear model equivalent satisfying feasibility constraints latent maximum entropy problem 
example revisited illustrate relationship mle lme principles concretely consider simple example introduced section 
circumstance gender labels unobserved jaynes principle fails incorporate ect latent variables 
lme principle capture influence latent gender information considering joint model includes hidden valued variable 
denotes hidden gender index 
case observed data latent maximum entropy principle lme formulated max subject dx dx dx trying maximize joint entropy matching expectations features denotes indicator function event comparing constraints complete data case see di erence conditional probability complete model empirical conditional probability 
due nonlinear mapping imposed simple closed form solution longer exists 
common log linear model gives convenient approximation 
imagine attempting satisfy lme principle directly interested finding maximum likelihood model observed data 
consider distribution mixture gaussians parameters means variances respective classes 
distribution marginal density case joint distribution written exp natural canonical parameters corresponding features re write distribution log linear form exp canonical parameters related standard parameters log 
normalization constant exp 
model log likelihood function written log log exp solve maximum likelihood solution calculate derivatives obtain dy dy dy key result setting quantities zero results precisely constraints 
locally maximum likelihood gaussian mixture feasible solution lme principle conversely feasible log linear solution lme principle critical point log likelihood function form gaussian mixture 
example provides concrete demonstration log linear model parameterized stationary points incomplete data likelihood function give feasible solution original lme principle 
general algorithm finding feasible log linear solutions exploit observation theorem derive practical training algorithm obtaining feasible solutions lme principle log linear assumption 
obviously theorem shows locally maximizing likelihood observed incomplete data satisfy constraints lme principle natural strategy derive em algorithm log linear models 
doing able guarantee recover feasible solutions original constrained optimization problem theorem 
derivation em iterative algorithm recall log linear model determined parameter vector 
derive em algorithm typically decomposes log likelihood function log log dz log dz conditional expected complete data log likelihood conditional expected missing data log likelihood measures uncertainty due missing data 
note case empirical conditional entropy latent variables 
em algorithm maximizes iteratively maximizing 
jth iteration em defined expectation step computes function followed maximization step finds maximize 
iteration em monotonically non decreases generally em converges fixed point stationary point usually local maximum 
log linear models particular log dz log dz log dz plugging log linear form recalling 
crucially turns maximizing function fixed step equivalent solving constrained optimization problem corresponding maximum entropy principle simpler 
theorem maximizing function fixed equivalent solving max log dx subject dx dz 
usually possible check stationary point fact local maximum 
proof define lagrangian dx dz holding fixed compute unconstrained maximum lagrangian get arg max exp result obtained derivative respect setting zero 
plugging obtain dual function log dz exactly negative 
denote optimal value subject conditions strong duality holds max min min min max important realize new constrained optimization problem theorem easier maximizing subject log linear models right hand side constraints longer depend previous fixed means maximizing subject convex optimization problem linear constraints unfortunately closed form solution general means iterative algorithms usually necessary 
maximizer unique exists 
problems large number iterative algorithms available including bregman balancing method multiplicative algebraic reconstruction technique mart newton method conjugate gradient interior point methods 
case feature functions non negative generalized iterative scaling algorithm gis improved iterative scaling algorithm iis maximize ciently :10.1.1.103.7637:10.1.1.43.7345
usually gis iis iterations needed step 
observations propose maximizing entropy log linear models latent variables algorithm combines em nested iterative scaling iis gis calculate step see 
em algorithm initialization randomly choose initial guesses parameters step current model feature calculate current expectation respect dz step gis iis step em em procedure embedding iterative scaling loop auxiliary function gis iis denotes index cycle full parallel update denotes number cycles full parallel updates 
quantities form right hand side constraints 
step 
attempt solve equivalently maximize respect initialize perform iterations full parallel update parameter values gis iis follows 
update satisfies dx special case constant explicitly log dx constant value computed numerically example solving nonlinear equation newton raphson new old old dx old dx possible bisection method purpose 
repeat note implementing algorithm em algorithm able calculate various expectations respect underlying log linear model particular need calculate expectations form dz dx 
structured models gaussian mixtures simple log linear models expectations calculated directly ciently time polynomial number features number observations 
log linear models cient algorithms calculating expectations exist resort monte carlo methods approximation methods cases :10.1.1.43.7345
demonstrate kinds models section 
natural interpretation iterative em procedure right hand side equation constant optimal solution log linear model parameters provided gis iis algorithm 
obtain calculate value right hand side equation 
value matches constant assigned previously optimality condition reached stationary point likelihood function feasible solution maximizing entropy complete model subject required nonlinear constraints 
iterate constraints met 
note lauritzen suggested similar algorithm maximum likelihood estimation loglinear models incomplete data 
supply proof convergence provide 
riezler proposed similar algorithm doubly iterative approach nesting iterative scaling inside em loop 
riezler proposed single loop procedure repeatedly applying auxiliary function obtain closed form solution parameter estimates 
turns riezler algorithm special case em algorithm setting 
nested iteration em appear unnecessary complication see section setting important obtaining rapid convergence 
example demonstrate em applied consider simple example sections 
joint model representing heights gender labels observe height measurements lme principle formulated shown 
solve feasible log linear model apply em follows start initial guess parameters canonical parameterization features specified 
execute step calculate feature expectations 
execute step formulate simpler maximum entropy problem linear constraints obtaining max subject dx dx dx 
similarly section solve problem analytically avoid gis iis performing step 
problem directly obtain unique log linear solution 
set repeat convergence 
em produces model form gaussian mixture 
case lme general jaynes principle postulate bi modal distribution observed component standard reduced producing uni modal gaussian situation 
interestingly update formula obtain equivalent standard em update estimating gaussian mixture distributions 
fact find natural situations em recovers standard em updates special case 
turns situations em yields new iterative update procedures converge faster standard parameter estimation formulas 
demonstrate cases section 
establish key result em guaranteed converge feasible lme solution log linear models 
proof correctness prove em converges log linear models feasible solutions lme principle theorem exploited reduce question showing em converges critical point log likelihood function 
convergence proof em similar gem algorithm 
theorem em algorithm monotonically increases likelihood function limit points em sequence belong set em asymptotically yields feasible solutions lme principle log linear models 
proof discussed previous section obvious em algorithm converges local maximum likelihood yields feasible solution lme principle theorem 
prove convergence show em generalized em procedure 
define auxiliary function way :10.1.1.103.7637:10.1.1.43.7345
specifically parameter settings bound change objective functions auxiliary function 
dz log dz dz dx dz dx inequalities follow convexity log exp index cycle full parallel update assume perform cycles full parallel updates equation true inspection concave 
new update stationary point result step procedure increases em algorithm monotonically increases likelihood function 
show convergence stationary points likelihood function show convergence just consider successive phases stage 
theorem show mapping defined gis iis closed mapping ii 
compactness condition continuity condition assertion verified directly second establish assertion ii shown 
implies 
maximum maximizes required 
show convergence cases respectively argue similarly 
conclude limit points em sequence belong set 
appendix gives detailed characterization information geometry em provides insight behavior behavior em algorithms generally 
finding high entropy solutions exploit em algorithm develop practical approximation lme principle 
noted section di cult solve optimal latent maximum entropy model general 
fact section points hard solve optimal lme model restrict attention log linear models 
em algorithm section provides ective technique finding feasible necessarily optimal solutions lme principle 
appendix illustrates multiple distinct feasible solutions general 
approach em approximate lme principle simple generate candidate feasible solutions running em convergence di erent initial points evaluate entropy candidate model select model highest entropy 
em algorithm initialization randomly choose initial guesses parameters 
em run em convergence obtain feasible solution entropy calculation calculate entropy model selection repeat steps times produce set distinct feasible candidates 
choose final estimate candidate achieves highest entropy 
apparent di culty implementing algorithm needs calculate entropies candidate models produced em suppose entropy calculated explicitly candidate model evaluating expectation log dx log dx turns need perform calculation explicitly 
fact easily recover entropy feasible log linear model merely byproduct running em convergence 
recall decomposition respectively 
case feasible solution obtain relationship 
theorem set feasible solutions defined proof know 
arg max 
obtain max 
argument proof theorem show limit points sequence belong set 

theorem provides needed result establishing half theorem section 
interestingly provides simplification entropy calculation feasible solution em convergence relationship 
calculate feasible solution combining log entropy easily determined values calculated step em normalization constant needs determined part step solving 
observations follow theorem 
note special case missing data theorem shows feasible solution known result standard maximum entropy theory :10.1.1.103.7637:10.1.1.43.7345
draw clear distinction lme mle principles 
assume term constant di erent feasible solutions 
case mle maximizes likelihood choose model lowest entropy lme maximizes entropy choose model likelihood 
course constant di erent feasible practice comparison mle lme straightforward example highlight di erence 
di erence principles raises question method ective inferring model sample data 
address question turn brief experimental comparison lme mle 
experimental comparison conducted simple experiment ascertain lme mle yields better estimates inferring models sample data missing components 
considered simple component mixture model case study mixing component unobserved dimensional vector observed 
features su cient statistics try match data sections case mixture components observed data dimensional dimensional 
sample data idea infer log linear model 
basis comparison lme mle realize discussion section feasible solution lme principle corresponds locally maximum likelihood gaussian mixture specified 
implement em outlined section generate feasible candidates lme mle principles simultaneously noted section em reduces standard em algorithm estimating gaussian mixtures case 
theorem know lme mle consider set feasible candidates feasible solutions lme selects model highest entropy mle selects model highest likelihood 
theorem shows equivalent 
interested determining method yields better estimates various underlying models generate data 
measure quality estimate calculating cross entropy correct marginal distribution estimated marginal distribution observed data component log dy goal minimize cross entropy marginal distribution estimated model correct marginal cross entropy zero obtained matches 
consider series experiments di erent models di erent sample sizes test robustness lme mle sparse training data high variance data deviations log linearity underlying model 
particular experimental design 

fix generative model 

generate sample observed data 

run em generate multiple feasible solutions restarting random initial vectors 
generated initial vectors generating mixture weights uniform prior independently generating component mean vectors covariance matrices choosing numbers uniformly see section relation parameters 

calculate entropy likelihood feasible candidate 

select maximum entropy candidate lme lme estimate maximum likelihood candidate mle mle estimate 

calculate cross entropy marginals lme mle respectively 

repeat steps times compute average respective cross entropies 
average cross entropy repeated trials sample size method experiment 

repeat steps di erent sample sizes 
repeat steps di erent generative models 
scenario experiment generated data component gaussian mixture model form expected estimators 
specifically uniform mixture distribution component gaussians specified mean vectors covariance matrices respectively 
figures show average log likelihoods average entropies models produced lme mle respectively behave expected 
mle clearly achieves higher log likelihood lme lme clearly produces models significantly higher entropy mle 
interesting outcome estimation strategies obtain significantly di erent cross entropies 
reports average cross entropy obtained mle lme function sample size shows somewhat surprising result lme achieves substantially lower cross entropy mle 
lme advantage lme mle sample size average log likelihood mle estimates versus lme estimates experiment 
lme mle sample size average entropy mle estimates versus lme estimates experiment 
lme mle sample size sample size mle lme average cross entropy true distribution mle estimates versus lme estimates experiment 
especially pronounced small sample sizes persists sample sizes large considered 
expected advantage lme regularization ect completely explain lme superior performance large sample sizes 
discussing regularization properties lme detail consider alternative scenarios observed relationship mle lme di erent 
experiment considered favorable scenario underlying generative model form distributional assumptions estimators 
consider situations structural assumptions violated 
scenario second experiment generative model mixture gaussian distributions specifically generated data sampling uniform distribution mixture components generated observed data sampling corresponding gaussian distribution distributions means covariances respectively 
lme mle estimators inferred component mixtures case making incorrect assumption underlying model 
shows lme obtained significantly lower cross entropy mle small sample sizes lost advantage larger sample sizes 
crossover point data points mle began produce slightly better estimates lme marginally 
lme appears safer estimator problem uniformly dominant 
scenario third experiment attempted test robust estimators high variance data generated heavy tailed distribution 
experiment yielded dramatic results 
generated data component mixture correctly assumed estimators laplacian distribution gaussian distribution generate observations 
model generated data variable data generated gaussian mixture challenged estimators significantly 
specific parameters experiment means covariances laplacians 
shows lme produces significantly better estimates mle case improved advantage larger sample sizes 
clearly mle stable estimator subjected heavy tailed data expected 
lme proves far robust circumstances clearly dominates mle 
scenario situations mle appears slightly better estimator lme su cient data available 
shows results subjecting estimators data generated component gaussian mixture means covariances respectively 
case lme retains sizeable advantage small sample sizes sample size mle begins demonstrate persistent modest advantage 
results suggest maximum likelihood estimation mle ective large sample sizes long presumed model close underlying data source 
mismatch assumption reality limited training data lme appears er significantly lme mle sample size sample size mle lme average cross entropy true distribution mle estimates versus lme estimates experiment 
lme mle sample size sample size mle lme average cross entropy true distribution mle estimates versus lme estimates experiment 
lme mle sample size sample size mle lme average cross entropy true distribution mle estimates versus lme estimates experiment 
safer ective alternative 
course results far definitive experimental theoretical analysis required give completely authoritative answers 
comments order 
appears lme adds just fixed regularization ect mle 
fact demonstrate section add regularization term lme principle way add regularization term mle principle 
lme behaves adaptive fixed regularizer see real fitting lme large data samples lme chooses far smoother models mle smaller sample sizes 
fact lme demonstrate far stronger regularization ect standard penalization method known case em converges degenerate solution determinant covariance matrix goes zero finite penalty counteract resulting unbounded likelihood 
lme principle automatically filter degenerate models models zero entropy non degenerate model preferred 
eliminating degenerate models lme principle solves main practical problems gaussian mixture estimation 
observation experiments show mle lme reduce cross entropy error sample size increased 
proved lme principle statistically consistent guaranteed converge zero cross entropy limit large samples underlying model log linear form features considered estimator 
interested stronger form consistency requires estimator converge best representable log linear model minimum cross entropy error underlying distribution minimum achievable cross entropy nonzero 
determining statistical consistency lme sense remains important topic research 
application models clearly lme principle general gaussian mixture models 
section demonstrate lme applied important estimation problems involving latent variables 
aim section full fledged study problem merely illustrate lme principle applied case 
specifically focus application em algorithm finding feasible solutions point cases yields faster converging algorithms standard maximum likelihood training algorithms 
mixtures dirichlet distributions model consider mixture dirichlet distributions applications natural language modeling areas 
problem observed data form dimensional probability vector 
observed variable random vector ym happens normalized 
underlying class variable unobservable 

observed sequence dimensional probability vectors attempt infer latent maximum entropy model matches expectations features log 
case lme principle formulated max subject dx dx log dx log dx indicates due nonlinear mapping caused closed form solution 
gaussian mixtures apply em obtain feasible log linear model problem 
perform step calculate feature expectations log 
note expectations calculated ciently gaussian mixture case 
execute step formulate simpler maximum entropy problem linear constraints obtain max subject dx log dx problem obtain log linear solution form class conditional model dirichlet distribution parameters need solve lagrange multipliers iterative scaling 
gaussian mixture case solve lagrange multipliers directly 
specified update parameters added iterative scaling phase step obtained solving equations log dy 
equivalent solving digamma function 
solution obtained iterating fixed point equations 
iteration corresponds known technique locally maximizing likelihood dirichlet mixture 
em recovers classical training algorithm special case 
hidden markov models assume observed data sequence takes value finite set integer values 
assume observed sequence related hidden state sequence takes value finite set integer values 
complete data random sequences respectively 
yt hidden markov model lme approach estimation conditional independence assumptions directly specifies set features expectations force model satisfy 
case assume features defined contiguous pairs hidden state variables concurrent observed hidden state variables 
consider features kv values concurrent hidden state observed variables respectively values consecutive hidden state variables 
note features binary represent structure log linear model graph nodes depict variables edges connect pair variables occur feature function 
resulting graphical structure illustrated corresponds standard graphical representation hidden markov model hmm 
sequence observed data formulate lme principle max subject 
find feasible log linear solutions problem apply em algorithm 
step performed calculating feature expectations kv prove log linear model equivalent standard hmm terms ciently calculated forward backward algorithm 
execute step formulate simpler maximum entropy problem linear constraints max subject kv problem solved considering equivalent problem finding parameters maximize likelihood observed data theorem 
observation state trellis calculate normalization factor feature expectation weight link exponential corresponding lagrangian multiplier computational problems easier prove marginal distribution hidden state sequence structure markov chain 
fact log linear model defines decomposable graphical model 
rewrite full joint probability product maximal clique potentials divided potentials nonempty intersection maximal cliques 
variables thought nodes graph edge variables share feature function 
accordingly obtain equation shows sequence order markov chain 
shown lme principle infers distribution form hidden markov model problem 
perform step em note case equation directly calculate update contrary assertion normalization factor exp kv calculated ciently sum product algorithm 
summing links time slice passing trellis nodes product weight ongoing nodes feature expectations calculated explicitly right hand sides calculated ciently summing links corresponding feature time slice passing trellis nodes dividing normalization factor 
see illustration 
computational complexity em problem order baum welch algorithm 
un boltzmann machine model nodes observable nodes unobservable 
boltzmann machines consider graphical model binary nodes values 
assume nodes observable nodes unobservable nodes ul 

problem observed data form dimensional vector observed sequence dimensional vectors attempt infer latent maximum entropy model matches expectations features defined pair variables model 
specifically consider features km um mn ul 
note features binary represent structure log linear model graph shown 
sequence observed data formulate lme principle max subject um um ul 
apply em find feasible log linear model 
execute step calculate feature expectations um execute step formulate simpler maximum entropy problem linear constraints max subject um ul 
case probability distribution complete data model written yu symmetric matrix parameters corresponding features variable pairs diagonal elements equal zero normalization factor 
graphical model corresponds boltzmann machine 
solve optimal lagrange multipliers step need iterative scaling 
iteratively improve adding update parameters satisfy 
calculated newton method bisection method solve exp um exp exp dimensional vector elements identity matrix 
required expectations calculated direct enumeration small approximated generalized belief propagation monte carlo estimation large 
byrne sequential update algorithm step boltzmann machine parameter estimation algorithm 
maintain monotonic convergence byrne algorithm requires large number iterations step ensure maximum achieved monotonic convergence property violated sequential updates proposes 
case em uses parallel update avoids di culty 
sequential algorithm maintains monotonic convergence property adapted described 
compare em standard boltzmann machine estimation techniques consider derivation direct em approach 
standard em previous parameters solves new parameters maximizing auxiliary function respect log log derivatives respect gives xx xx emis emis gradient ascent convergence evaluation log likelihood versus iteration solid curve denotes em dotted curve denotes em dashed curve denotes gradient ascent 
apparently closed form solution step generalized em algorithm case 
standard approach gradient ascent approximately solve step 
step size needs controlled ensure monotonic improvement comparison em distinct advantages standard gradient ascent em approach 
em completely avoids tuning parameters guaranteeing monotonic improvement 
em converges faster gradient ascent em 
shows result simple experiment compares rate convergence step optimization techniques small boltzmann machine visible nodes hidden nodes 
comparing em gradient ascent em algorithm proposed find em obtains substantially faster convergence 
shows iterations inner loop yields faster convergence single step corresponds riezler proposed algorithm 
extensions briefly outline useful extensions relaxations basic lme principle 
bayesian extension statistical modeling situations constraints subject error due small sample size ects particularly domains large number features 
way mitigate sensitivity constraint errors relax lme principle introducing slack variables 
augment lme principle max subject constraints dx dz slack variables allow errors constraints convex function minimum 
regularization term penalizes violations reliably observed constraints greater degree deviations reliably observed constraints 
establishes bayesian framework exponential models prior distribution feature parameters naturally incorporated 
solve reformulated lme problem restrict log linear model develop iterative algorithm finding feasible solutions 
key developing algorithm note stationary points penalized log likelihood observed data log feasible set relaxed constraints convex conjugate example quadratic penalty obtain gaussian prior 
case em algorithm remains parameter update step needs modified dx automated feature selection model construction lme principle possible extend incremental feature induction paradigm reveal hidden structure :10.1.1.43.7345
assume pool candidate features nth step induction algorithm added features weights model 
st step consider adding single feature weight largest information gain arg min arg max arg max log dz value obtained iterative maximization equivalent maximizing dual function complete data kullback leibler divergence problem max log dx subject dx dz solved newton method :10.1.1.103.7637:10.1.1.43.7345
general bregman divergences lme principle easily extended terms minimizing bregman distances class generalized entropy measures associated convex functions discrete state distribution 
strictly convex function defined closed convex set typically assumed set probability distributions positive measures finite set points xm assume di erentiable points int interior bregman divergence int defined measures discrepancy distributions di erence evaluated order taylor expansion evaluated finite set data points xm set features denote matrix feature values point 

columns correspond finite set points xm rows correspond features notation denote column values corresponding features case correspond weak learners boosting su cient statistics log linear model 
set data points propose generalization lme principle minimizing bregman divergence 
latent minimum bregman divergence principle default distribution chosen 
features specifying properties match data choose distribution minimize min subject constraints decomposes find feasible solutions principle analogous alternating minimization procedure nested iterative scaling 
proposed latent maximum entropy principle lme incorporates latent variables inferred models obtain expressive power maximum entropy principle jaynes 
em algorithm incorporates nested iterative scaling em solve problem finding feasible solutions lme principle 
em retains main virtues em algorithm guarantee monotonic improvement likelihood function absence tuning parameters 
shown familiar models recovered lme principle em recovers standard iterative training procedures models 
case seen em leads new training procedure superior convergence properties standard methods 
em develop em algorithm approximately realizing lme principle 
algorithm exploits em generate feasible solutions evaluates entropy candidates selects highest entropy feasible solution 
experiments show advantage lme standard maximum likelihood estimation mle estimating data source hidden variables 
applications lme principle highlight generality show useful extensions 
planning investigate theoretical questions including statistical consistency lme bounds generalization error techniques automatic model complexity control relationship lme graphical models 
investigating ideas relaxing log linear assumption lme principle unsupervised boosting 
begun lme build models complex natural phenomenon 
applied method build sophisticated mixed chain tree table graphical model statistical language modeling various aspects natural language local word interaction syntactic structure semantic document information modeled mixtures exponential families rich expressive power 
lme allows combine models ectively unified framework 
planning investigate practical applications lme principle including problems machine translation text classification information retrieval image analysis computer vision bioinformatics 
information geometry em give information geometric interpretation em algorithm information divergence technique alternating minimization probability manifolds 
interpretation provide clear illustration em algorithm converges stationary point likelihood function 
analysis clarifies properties em algorithms generally 
define kullback leibler divergence log dx log log log measure distance non negative equals non symmetric satisfy triangle inequality 
space probability distribution complete data curve denotes set satisfies nonlinear lme constraints curve denotes set exponential models intersection set stationary points log likelihood function observed data 
understand relationship maximum likelihood lme models note complete data case missing data components 
stationary points log likelihood function approximate solution log linear assumption ignoring terms illustrate relationship maximum likelihood models lme models consider manifolds stationary points loglikelihood incomplete data general model feasible solutions lme principle log linear assumption respectively dx dz exp exp dx restriction guarantee maximum likelihood estimate interior point set defined 
illustrates manifolds intersect set log linear models stationary points log likelihood function incomplete data 
define manifolds dz dx vector constants 
lemma linear submanifold proof assume 
dz dz dz 
linear manifold 
dx dz conclude linear submanifold alternating minimization step starts distribution finds backward projection arg min 
fixing find forward projection arg min 
possible establish known result alternating backward projection forward projection step leads em update auxiliary function 
include proof self contained 
lemma alternating minimization step equivalent em update arg max proof log dx log dz log independent implies min log achieved setting 
fixing seek minimizes log dx log dz log log dz independent log dz term exactly auxiliary function 
arg min equivalent finding 
equivalence enables establish information geometric interpretation em algorithm follows see illustration space probability distributions complete data curve denotes set satisfies nonlinear lme constraints curve denotes set exponential models intersection set stationary points log likelihood function observed data 
line denotes set distributions marginal matches empirical distribution 
information geometry alternating minimization procedures 
straight line denotes set distributions marginal distribution matches empirical distribution nonlinear operator denotes marginalization maps entire space singleton 
intersection set distributions alternating minimization procedure reaches fixed point 
starting line denotes set feature expectations match constant intersection point dz backward projection arg min 
step determines value step finds intersection achieved forward projection arg min equivalent projection uniform distribution arg min ga 
alternating procedure halt point manifolds common intersection reach stationary point case 
due nonlinearity manifold intersection unique 
note em algorithm update iterative scaling phase increases decreases divergence finding final forward projection step em finds approximation solution iterations iterative scaling procedure 
note case unobserved training data manifold shrinks singleton stretches match case manifolds intersect unique point 
previously amari byrne csiszar information geometric interpretations em algorithm log linear models 
explicitly consider constraints imposed nonlinear manifold subsequently explanations em converge di erent solutions depending initial point unclear hampered omission 
gain insight considering known pythagorean theorem log linear models complete data case states exists incomplete data case theorem needs modified reflect ect latent variables :10.1.1.43.7345
theorem pythagorean property exists proof pick 
obviously show establishing equivalent showing log dz log dz log dx second terms right hand side cancel theorem 
plugging exponential form remaining terms yields log dz log dx dz dx term inside brackets incomplete data case point unique point forms right triangle complete data case incomplete data case multiple points acknowledgments performed author university waterloo 
roni rosenfeld peng ali kind assistance 
research supported nserc 
ackley hinton sejnowski learning algorithm boltzmann machines cognitive science vol 
pp 
amari information geometry em em algorithms neural networks neural networks vol pp 
amari methods information geometry american mathematical society berger della pietra della pietra maximum entropy approach natural language processing computational linguistics vol 
pp 
bertsekas nonlinear programming athena scientific blei ng jordan latent dirichlet allocation advances neural information processing systems nips bregman relaxation method finding common point convex sets applications solution problems convex programming ussr computational mathematics mathematical physics vol 
pp 
byrne alternating minimization boltzmann machine learning ieee trans 
neural networks vol 
pp 
july censor zenios parallel optimization theory algorithms applications oxford university press chen rosenfeld survey smoothing techniques models ieee trans 
speech audio processing vol 
pp 
collins schapire singer logistic regression adaboost bregman distances machine learning vol 
pp 
cover thomas elements information theory john wiley sons csiszar divergence geometry probability distributions minimization problems annals probability vol pp 
csiszar information geometry alternating minimization procedures statistics decisions supplement issue pp 
csiszar geometric interpretation darroch generalized iterative scaling annals vol 
pp 
csiszar squares maximum entropy 
axiomatic approach inference linear inverse problems annals vol 
pp 
csiszar maxent mathematics information theory maximum entropy bayesian methods edited hanson silver pp 
kluwer academic publishers darroch generalized iterative scaling log linear models annals mathematical statistics vol 
pp 
della pietra della pietra la erty inducing features random fields ieee transactions pattern analysis machine intelligence vol :10.1.1.43.7345
pp 
april della pietra della pietra la erty duality auxiliary functions bregman distances technical report cmu cs school computer science cmu dempster laird rubin maximum likelihood estimation incomplete data em algorithm journal royal statistical society series vol 
pp friedman schuurmans data perturbation escaping local maxima learning proceedings national conference artificial intelligence aaai 
fang tsao entropy optimization mathematical programming kluwer academic publishers miller judge maximum entropy econometrics robust estimation limited data john wiley son jaynes papers probability statistics statistical physics edited rosenkrantz reidel publishing la erty della pietra della pietra statistical learning algorithms bregman distances proceedings canadian workshop information theory pp 
la erty pereira mccallum conditional random fields probabilistic models segmenting labeling sequence data proceedings international conference machine learning lauritzen em algorithm graphical association models missing data computational statistics data analysis vol 
pp 
lauritzen graphical models clarendon press lebanon la erty boosting maximum likelihood exponential models advances neural information processing systems nips luenberger optimization vector space methods john wiley sons mackay peto hierarchical dirichlet language model natural language engineering vol 
pp 
meng rate convergence ecm algorithm annals statistics vol 
pp 
minka estimating dirichlet distribution manuscript sullivan alternating minimization algorithms blahut expectationmaximization codes curves signals common threads communications vardy ed kluwer riezler probabilistic constraint logic programming ph dissertation university stuttgart germany rubinstein hastie discriminative vs informative learning proceedings third international conference knowledge discovery data mining aaai press small wang yang eliminating multiple root problems estimation statistical science vol 
pp 
vapnik natural statistical learning theory springer wainwright jaakkola willsky tree reparameterization framework analysis belief propagation related algorithms appear ieee trans 
information theory wang statistical recursive estimation algorithms speaker adaptation ph dissertation university illinois urbana champaign wang rosenfeld zhao latent maximum entropy principle statistical language modeling ieee workshop automatic speech recognition understanding december wu convergence properties em algorithm annals statistics vol 
pp 
yedidia freeman weiss understanding belief propagation generalizations exploring artificial intelligence new millenium lakemeyer nebel editors morgan kaufmann 

