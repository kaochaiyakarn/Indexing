effective reinforcement learning mobile robots programming mobile robots long time consuming process 
specifying low level mapping sensors actuators prone programmer misconceptions debugging mapping tedious 
idea having robot learn accomplish task told explicitly appealing 
easier intuitive programmer specify robot doing learn fine details 
introduce framework reinforcement learning mobile robots describe experiments learn simple tasks 
keywords mobile robots machine learning reinforcement learning learning demonstration 
programming mobile robots timeconsuming process 
takes iterations fine tune low level mapping sensors actuators 
difficult programmer translate knowledge complete task terms useful robot 
robot sensors actuators different humans misconceptions operate cause control code fail 
idea providing high level specification task machine learning techniques fill details appealing 
take time write highlevel specification learned control policy uses empirical observations world prone programmer bias hand written control code 
briefly survey reinforcement learning machine learning paradigm especially suited learning control policies mobile robots 
discuss shortcomings introduce framework effectively reinforcement learning mobile robots 
go give experimental results applying framework mobile robot control tasks 
department computer science washington university st louis st louis mo cs wustl edu 
artificial intelligence laboratory mit technology square cambridge ma ai mit edu 
supported darpa contract dabt 
william smart leslie pack kaelbling ii 
reinforcement learning reinforcement learning rl machine learning paradigm particularly suited mobile robots 
assumes world described set states agent robot case take fixed number actions time divided discrete steps 
time step agent observes state world st include internal state robot chooses action take 
tak ing action agent reward rt reflecting short term sense action observes new state world st 
goal rl take experience tuples st rt st learn mapping states states actions depending particular algorithm measure long term value state known optimal value function 
particular reinforcement learning algorithm learning 
qlearning optimal value function defined max represents expected value reward action state state acting optimally 
parameter known discount factor measure attention pay possible rewards get 
optimal function easy calculate optimal policy simply looking possible actions state selecting largest value arg max function typically stored table indexed state action 
starting arbitrary values iteratively approximate optimal function observations world 
time robot takes action experience tuple st rt st generated 
table entry state action updated st st rt maxa st reasonable conditions guaranteed converge optimal function 
important features learning known policy algorithm 
means distribution training samples drawn effect limit policy learned 
prove important algorithm discussed 
description learning necessarily brief 
comprehensive coverage see book sutton barto survey kaelbling littman moore 
iii 
reinforcement learning robots reinforcement learning learning particular natural choice learning control policies mobile robots 
designing lowlevel control policy design higher level task description form reward function 
designing sparse reward function generally easier designing low level mapping observations actions 
robot tasks rewards correspond physical events world 
easy come simple reward functions tasks 
example obstacle avoidance task robot get reward reaching goal hitting obstacle 
theory necessary robot learn optimal policy 
number problems simply standard learning techniques 
call type reward function sparse 
sparse reward functions zero places obstacles goal example 
contrast dense reward functions give non zero rewards time 
dense reward function obstacle avoidance example sum distances obstacles divided distance goal state 
dense reward functions give information action difficult construct sparse functions 
interested sparse reward functions generally simpler design 
problem rl mobile robots state space description mobile robots best expressed terms vectors real values learning requires discrete states actions 
approach overcome problem known value function approximation replaces tabular representation general purpose function approximator 
shown general function approximators seemingly benign situations 
previous algorithm hedger addresses problems associated value function approximation 
algorithm observation gordon function approximator safely replace tabular value function representation training data 
showed locally weighted averaging function approximator 
hedger uses powerful function approximator locally weighted regression lwr supplemented extrapolation checks value function approximation 
query checks sure query point training data seen predicted value reasonable limits rewards observed 
query outside training data prediction reasonable returns prediction guaranteed safe 
prediction lwr returned 
algorithm shown learn faster robust number test domains 
hedger part learning implementation 
previous generally solved problem domain knowledge create discretization state space hierarchically decomposing problem hand learning task easier 
main problem incorporating prior knowledge learning system 
way learning find information environment take actions observe effects 
early stages learning system knows environment act 
forced choose arbitrary actions 
information form rewards arrives learning iteratively improve approximation value function 
rewards observed value function approximation change 
problem compounded sparse reward functions 
rewards state action space large chances finding reward chance small 
section iv introduce framework rl robots addresses problem 
iv 
learning framework major hurdles implementing rl systems real robots inclusion prior knowledge learning system 
sparse reward function prior knowledge environment learning system certainly doomed fail 
solution problem supply example trajectories learning system split learning phases 
environment supplied control policy learning system environment supplied control policy learning system phase phase fig 

learning phases 
phase learning shown robot controlled supplied control policy 
actual control code human directly controlling robot joystick 
learning phase rl system passively watching states actions rewards supplied policy generating 
uses rewards bootstrap information value function approximation 
key element phase rl system control robot 
point assume value function approximation complete adequately control robot 
le supplied control policy expose rl system interesting parts state space parts reward non zero 
important note trying learn trajectories generated supplied policy simply generate experiences bootstrap value function approximation 
value function approximation complete control robot effectively second learning phase starts 
phase shown learned policy control robot standard rl implementation 
supplied control policy piece software opposed direct human control kept running background offer advice control decisions needed 
splitting learning phases gain ability bootstrap information approximation committing learned policy control robot 
allows sure move second learning phase robot capable finding states learning stall 
observing example trajectories remove need know robot sensor actuator systems detail 
phase learning done direct control robot human controller need know sensor systems inverse reinforcement learning 
lin similar method bootstrap information value function relied hand coded sequences experiences 
requires detailed knowledge robot moves sensors 
knowledge incorrect faulty assumptions learning system fail produce desired results 
observing actual trajectories state action space learning empirical data subject biases 
sort learning demonstration quite popular robots generally trying learn inverse kinematics task observation 
closest reported lin example trajectories accelerate learning 
trajectories assembled hand required detailed knowledge robot dynamics sensor systems 
approach differs fact robot human guidance create example trajectories rl system learns 
experimental results section results framework learn control policies simple robot tasks corridor obstacle avoidance 
experiments carried real world interface mobile robot 
robot synchronous drive locomotion system rotate axis translate forwards backwards 
experiments reported translation speed controlled fixed policy corridor constant obstacle avoidance 
goal learn policy setting rotation velocity robot 
robot uses scanning laser range finder identify walls obstacles world 
higher level features described computed raw laser information inputs control system 
training runs learning temporarily disabled performance learned policy evaluated 
evaluation runs started pre specified starting poses 
evaluation metric number steps taken reach goal state 
performance graphs show average number steps goal confidence bound average 
training runs started variety poses similar evaluation 
experiments rl learning rate set 
corridor experiments discount factor set obstacle avoidance experiments set 
position corridor corridor center line distance fig 

corridor task 
robot reward area fig 

calculating angle corridor task 
corridor target point corridor task shown 
translation speed vt robot controlled fixed policy causes move quickly middle corridor slowly gets closer wall 
state space problem dimensions distance corridor distance left hand wall fraction total corridor width angle target point shown 
angle target point tan angle moving target point resulted smoother behavior angle robot heading corridor 
robot gets reward reaching corridor reward zero situations 
shows performance framework simple control policy phase learning 
supplied policy simple proportional controller vr 
value gain varied run run order generate wider range experiences learning system 
seen performance learned policy improves experience 
phase training runs performance significantly better level shortest phase training run labelled best example 
steps optimal steps goal phase phase training runs training runs best example fig 

corridor performance simple policy examples 
phase training runs learned policy deemed competent take control robot 
evaluation phase training runs worse phase evaluation 
performance learned policy continues improve 
noticed behavior domains just change learning phases 
phase learning experiences generated example trajectories fairly predictable 
number novel experiences run learning system cope relatively small 
phase transition learned policy control greater number novel experiences generated 
causes temporary decrease performance learning system time integrate new data properly 
phase training runs performance learned policy indistinguishable level best authors achieve directly controlling robot joystick labelled optimal 
confidence bounds average performance narrow training 
corresponds performance robot predictable different starting poses 
shows performance framework corridor task 
time phase training done directly controlling robot joystick hand coded example policy 
corridor experiment longer previous best possible performance different 
graph shows average shortest training runs phase best authors 
phase training runs longer optimal value attempt examples 
fact robot driven quite phase steps optimal steps goal phase phase training runs training runs average example best example fig 

corridor performance direct control examples 
eventually reached goal path far shortest possible 
basic profile performance similar shown notable exceptions 
fewer phase training runs performance improvement phase rapid phase 
caused nature training data generated 
experiences generated human driving robot varied generated example control policy 
variety training data means framework able generalize effectively tends lead better performance new situations 
previous set experiments slight performance decrease just learning phase change evident 
final performance indistinguishable best authors directly controlling robot 
see effective phase training terms time spent ran simulations corridor task 
dealing sparse reward function learning time dominated time taken reach reward giving state time 
simulated robot corridor long approximately length corridor set experiments described timed long took get reward states corridor 
simulated robot translation speed policy real experiments idealized forward model systems 
set experiments started robot middle corridor pointing random direction half circle goal 
set experiments limited magnitude rotation velocity vr real robot experiments vr 
actions chosen uniform distribution appropriate range 
shows results simulations 
fastest simulation able reach goal time hours time reach goal max rotation speed rad fig 

performance simulated corridor task 
starting point obstacles fig 

obstacle avoidance task 
reward region target point slightly hours 
argued informed action selection policy better assuming standard rl approach domain specific knowledge 
case forced take arbitrary actions 
real robot experiments reported training accomplished approximately hours 
shows time reward clear phase learning offers huge time saving task 
obstacle avoidance obstacle avoidance task shown 
translation speed vt robot fixed trying learn policy rotation speed vr 
goal robot drive goal state avoiding obstacles environment 
reward reaching goal reward colliding obstacle zero reward situations 
experimental run terminates robot reaches goal hits obstacle 
inputs learning system direction distance goal state obstacles shown 
robot started random pose approximately robot successful runs obstacle target point fig 

features obstacle avoidance task 
phase training runs phase training runs fig 

successful runs obstacle avoidance task 
meters goal point experimental run 
performance metric number time steps needed reach goal state 
experiments reported section obstacle example trajectories generated direct joystick control robot 
obstacle avoidance task somewhat difficult corridor 
robot contained walls corridor constrained dynamics robot 
relatively difficult get seriously lost start driving corridor wrong direction 
means eventually reach reward giving state corridor experiments 
obstacle avoidance task help environment 
fact policies perfect just goal state unable recover 
reflected shows number evaluation runs manage reach goal state 
notice th framework see fifteen phase example trajectories capable reaching goal steps optimal steps goal phase phase training runs training runs best example fig 

performance obstacle avoidance task 
starting distance successful time hours table performance simulated obstacle avoidance task 

find goal state evaluation starting position quickly learns reach 
notice slight performance decrease just change learning phases 
evaluation runs able reach goal shows average number steps taken 
performance increase learning phase dramatic second phase 
similar previous task direct control phase training 
final performance significantly different level best authors achieve direct control robot 
corridor task total time reach final performance level shown approximately hours 
ran simulation task see long take robot reach goal state benefit phase training 
robot started various distances goal state pointing straight 
obstacles rotation velocity vt chosen normal distribution mean zero variance 
table shows number runs reached goal week simulated time average time successful runs took 
starting point real robot run able reach goal state 
took average approximately half hours 
longer battery life robot 
demonstrates necessity example trajectories phase training reinforcement learning real robot 
vi 
framework reinforcement learning mobile robots 
main feature system described example trajectories bootstrap approximation splitting learning phases 
learning phase robot control example solution task controlled directly human 
reinforcement learning system passively observes states actions rewards encountered robot uses information bootstrap value function approximation 
learned policy control robot second phase learning begins 
rl system control robot learning progresses standard learning framework 
example trajectories space allows easily incorporate human knowledge perform task learning system 
human guiding robot need know sensor effector systems reinforcement learning 
show robot best solution task 
experiments final performance levels tasks significantly better example trajectories phase training 
underlines point learning trajectories shown simply generate experience reinforcement learning system 
framework capable learning control policies quickly moderately experienced programmers hand code tasks looked 
anecdotally observed novice programmers take week write wall program 
experiments managed learn policy approximately hours 
believe framework shows promise rl techniques real robots 
open questions 
complex task learned sparse reward functions 
balance bad phase trajectories affect speed learning 
automatically determine change learning phases 
addressing questions current 
christopher watkins peter dayan qlearning machine learning vol 
pp 

richard sutton andrew barto reinforcement learning adaptive computations machine learning 
mit press cambridge ma 
leslie pack kaelbling michael littman andrew moore reinforcement learning survey journal artificial intelligence research vol 
pp 

justin boyan andrew moore generalization reinforcement learning safely approximating value function advances neural information processing systems tesauro touretzky leen eds 
vol 
pp 
mit press 
william smart leslie pack kaelbling practical reinforcement learning continuous spaces proceedings seventeenth international conference machine learning icml pp 

william smart making reinforcement learning real robots ph thesis department computer science brown university 
geoffrey gordon approximate solutions markov decision processes ph thesis school computer science carnegie mellon university june available technical report cmu cs 
christopher atkeson andrew moore stefan schaal locally weighted learning artificial intelligence review vol 
pp 

minoru asada noda koh hosoda purposive behavior acquisition real robot vision reinforcement learning machine learning vol 
pp 

sridhar mahadevan jonathan connell automatic programming behavior robots reinforcement learning machine learning vol 
pp 
june 
long ji lin self improving reactive agents reinforcement learning planning teaching machine learning vol 
pp 

paul bakker robot see robot overview robot imitation proceedings aisb workshop learning robots animals pp 

stefan schaal christopher atkeson robot learning nonparametric regression proceedings intelligent robots systems iros graefe ed pp 

