efficient algorithms parsing dop model excellent results reported data oriented parsing dop natural language texts bod 
unfortunately existing algorithms computationally intensive difficult implement 
previous algorithms sive due factors exponential number rules generated monte carlo parsing algorithm 
solve problem novel reduction dop model small equivalent probabilistic context free grammar 
solve second prob lem novel deterministic parsing strategy maximizes expected number correct con probability correct parse tree 
ithe optimizations experiments yield crossing brackets rate zero crossing brackets rate 
differs significantly results reported bod ble results duplication pereira schabes experiment data 
show bod results partially due extremely fortuitous choice test data partially due cleaner data researchers 
data oriented parsing dop model short interesting controversial history 
introduced scha studied bod 
unfortunately bod able find efficient exact acknowledge support na tional science foundation iri national science foundation graduate student fel 
bod stan chen andrew kehler david magerman wheeler rural stuart shieber khalil help ful discussions comments earlier drafts comments anonymous reviewers 
joshua goodman harvard university oxford st cambridge ma goodman das harvard edu algorithm parsing model discover implement monte carlo approxi mations 
tested algorithms cleaned version atis corpus achieved exciting results reportedly getting test set exactly correct huge improvement previous results 
instance bod com results schabes short sentences sentences crossing brackets easier measure ex act match 
bod achieves extraordinary fold error rate reduction 
surprisingly researchers attempted duplicate results due lack de tails parsing algorithm publications researchers able confirm results magerman personal commu nication 
bod thesis bod contain information replicate re sults 
parsing dop model especially dif 
model summarized spe cial kind stochastic tree substitution grammar stsg bracketed labelled training cor pus subtree corpus tree probability proportional number occurrences subtree train ing corpus 
unfortunately number trees general exponential size training corpus trees producing unwieldy grammar 
introduce reduction dop model exactly equivalent probabilistic context free grammar pcfg linear number nodes training data 
algorithm parsing returns parse expected largest num ber correct constituents 
reduction algorithm parse held test data com results replication pereira np vp pn pn np det training corpus tree dop example schabes data 
results disappointing pcfg implementation dop model performs pereira schabes method 
anal ysis runtime algorithm bod 
analyze bod data showing difference performance due fortuitous choice test data 
contains published repli cation full dop model parser sums derivations 
contains algo rithms implementing model significantly fewer resources previously needed 
time dop model com pared data competing model 
previous research dop model extremely simple described follows sentence parsed training corpus extract subtree 
general number subtrees large typically exponential sentence length 
trees form stochastic tree tion grammar stsg 
ways de fine stsg stochastic tree adjoin ing grammar schabes restricted sub operations extended pcfg entire trees may occur right hand side just strings terminals non terminals 
tree dop model convert stsg 
numbers parentheses represent prob abilities 
trees combined various ways parse sentences 
theory dop model advan models 
pcfg trees allows capturing large contexts making model sensitive 
subtree included trivial ones corresponding rules pcfg novel sentences unseen contexts simple example stsg parsed 
unfortunately number subtrees huge bod randomly samples sub trees throwing away rest 
significantly speeds parsing 
existing ways parse dop model 
find proba ble derivation 
ways sentence derived stsg 
probable derivation criterion simply finds probable way sentence produced 
shows simple ex ample stsg 
string xx probable derivation 
parse tree probability generated trivial derivation containing single tree 
tree cor responds probable derivation xx 
try find probable parse tree 
sentence parse tree different derivations lead parse tree 
probability parse tree sum probabilities derivations 
example dif ferent ways generate parse tree probability parse tree probability 
parse tree probable 
bod shows approximate probable parse monte carlo algo rithm 
algorithm randomly samples possible derivations finds tree sam derivations 
bod shows proba ble parse yields better performance probable derivation exact match criterion 
np det np vp vp np np 
det vp np det np pn pn np vp np vp pn pn pn pn np khalil implemented version dop model parses efficiently lim number trees efficient probable derivation model 
ex periments differed bod ways including version atis corpus word strings part speech strings fact parse sentences containing unknown words effec tively throwing difficult sentences 
furthermore sim limited number sub sites trees effectively sub set dop model 
reduction dop pcfg unfortunately bod reduction stsg ex expensive throwing away grammar 
fortunately possible find equivalent pcfg contains exactly pcfg rules node training data 
reduction smaller discard grammar 
pcfg equivalent senses generates strings probabilities second isomorphism defined generates trees probabilities sum pcfg trees stsg tree 
show reduction equivalence define terminology 
assign ev ery node tree unique number call address 
denote node address non terminal labeling node 
need create new non terminal node training data 
call non terminal ak 
call non terminals form interior non terminals original non terminals parse trees np vp sample stsg produced dop model exterior 
np vp np vp pn pn np det np aj represent number subtrees headed node represent num ber subtrees headed nodes non terminal aj 
consider node form subtrees 
consider possibilities left branch 
bk non trivial subtrees headed trivial case left node sim ply bk different ities left branch 
similarly right branch cl possibilities 
cre ate subtree choosing possible left subtree possible right subtree 
aj bk possible subtrees headed example tree noun phrases exactly subtree np nl verb phrase subtrees vp sentence sl 
numbers correspond number subtrees 
call pcfg subderivation phic stsg tree subderivation begins external non terminal uses internal non terminals intermediate steps ends external non terminals 
instance consider tree np vp pn pn np taken 
pcfg sub derivation isomorphic np vp pn pn vp pn pn np 
say pcfg derivation isomorphic stsg derivation corresponding pcfg sub derivation step stsg derivation 
give simple small pcfg surprising property subtree training corpus headed grammar generate isomorphic subderivation proba bility words large explicit stsg small pcfg generates isomorphic derivations iden tical probabilities 
construction follows 
node generate pcfg rules number parentheses rule probability 
aj sc aj bc aj bh aj bk aj bci ci aj cja aj ci aj show subderivations headed external non terminals roots leaves internal non terminals probability subderivations headed aj external non terminals leaves non terminals probability aj 
proof induction depth trees 
trees depth cases trivially trees required ties 
assume theorem true trees depth 
show holds trees depth 
cases rules 
show 
represent tree depth external leaves headed internal intermediate non terminals 
trees pcfg derivation productions np vp pn pn np stsg derivation subtrees np vp det pn pn np np det example isomorphic derivation bhc probability tree ai 
simi case trees headed probability tree cases follow trivially similar reasoning 
call pcfg derivation isomorphic stsg derivation substitution stsg corresponding subderivation pcfg 
contains example iso morphic derivations subtrees stsg productions pcfg 
call pcfg tree isomorphic stsg tree identical internal non terminals changed external non terminals 
main theorem construction pro duces pcfg trees isomorphic stsg trees equal probability 
subtree training corpus occurred exactly trivial prove 
stsg tion isomorphic pcfg sub derivation equal probability 
stsg derivation isomorphic pcfg derivation equal probability 
stsg tree produced pcfg equal probability 
extremely sub trees especially trivial ones occur repeatedly 
np vp stsg formalism modified slightly trees occur multiple times relationship 
consider modified form dop model subtrees occurred multiple times training corpus counts merged iden tical trees added grammar 
trees lower probability counts merged 
change probabilities derivations prob abilities parse trees change correspondingly derivations tree 
desired relation ship holds derivation new stsg isomorphic derivation pcfg equal probability 
summing derivations tree stsg yields probability summing isomorphic derivations pcfg 
stsg tree produced pcfg equal prob ability 
follows trivially extra trees produced pcfg 
total prob ability trees produced stsg pcfg produces trees probability probability left trees 
parsing algorithm different evaluation metrics finding best parse 
sec tion covering previous research considered probable derivation probable parse tree 
metric con sider 
performance evaluation number constituents correct measures similar crossing brackets measure want parse tree largest number correct constituents 
criterion example grammar best parse tree probability constituent correct probability constituent correct probability con correct 
tree average constituents correct 
trees fewer constituents correct average 
call best parse tree criterion maximum constituents parse 
notice parse tree produced gram mar constituents necessarily considered full tree 
bod shows prob able derivation perform probable parse dop model getting exact match probable deriva tion versus correct probable parse 
surprising parse tree derived different deriva tions probable parse criterion takes possible derivations account 
similarly maximum constituents parse derived sum different derivations 
fur thermore maximum constituents parse exact match criterion perform better percent constituents correct criterion 
previously performed detailed comparison tween parse maximum constituents parse probabilistic context free grammars goodman showed performance broad range measures difference error rate change error rate error rate 
think rea maximum constituents parser parse dop model 
parsing algorithm variation inside outside algorithm developed baker discussed detail lari young 
inside outside algo rithm grammar re estimation algorithm algorithm just parsing algo rithm 
closely related similar algorithm hidden markov models rabiner finding state time 
hmm case algorithm produces simple state sequence pcfg case parse tree produced resulting addi length length length non terminals sum loop addresses non terminal sum sum loop non terminals max art max sum ix loop best split max maxc maxc maxc sum max best split maximum constituents data oriented parsing algorithm tional constraints 
formal derivation similar algorithm goodman tuition 
algorithm sum follows 
potential con constituent non terminal start position position find prob ability constituent parse 
put constituents form parse tree dynamic programming 
probability potential constituent occurs correct parse tree ws wl wn called 
words probability sentence wl symbol generates ws wt 
compute probability elements inside outside algorithm 
compute inside probabilities wt 
second compute outside probabil ities wl wn 
third compute matrix wi wt wl wn matrix computed dy namic programming algorithm de termine best parse sense maximizing number constituents expected correct 
fig ure shows pseudocode simplified form algorithm 
grammar nonterminals train ing data size run time algorithm tn gn layers outer loops run time inner loops addresses training data non terminals dominated computation inside outside prob abilities takes time rna grammar rules 
rules node training data tn 
modifying algorithm slightly record actual split node recover best parse 
entry maxc contains expected number correct constituents model 
experimental results discussion grateful bod supplying data experiments bod bod bod 
original atis data penn tree bank version noisy difficult automatically read data due inconsistencies files 
re searchers left difficult decision clean data 
conducted sets experiments minimally cleaned set data making results comparable previous results atis data prepared bod contained significant revisions 
data sets constructed randomly splitting minimally edited atis sentences sentence training set sentence test set discarding sentences length 
sets dop algorithm outlined grammar induction experiment pereira schabes run 
crossing brackets zero crossing brackets paired differences table 
sentences output parser bi nary branching see section covering analysis bod data crossing brack ets measures meaningless magerman 
diff file original atis data cleaned version form usable eft program available anonymous ftp ftp ftp das harvard edu pub goodman atis ed par ed ti tb pos ed 
note number changes small 
diff files sum bytes versus bytes original files 
criteria cross brack dop cross brack cross brack dop zero cross brack dop zero cross brack min max range mean stddev zero cross brack dop table dop versus pereira schabes minimally edited atis criteria min max range mean stddev cross brack dop cross brack cross brack dop zero cross brack dop zero cross brack zero cross brack dop exact match dop sentences parsable signed right branching period high structure heuristic brill 
ran experiments bod data sentence test sets limit sentence length 
bod provided data provide split test training random splits 
results disappointing shown table 
noticeably worse bod comparable pereira schabes 
bod reported exact match got ing restrictive zero crossing brackets cri 
clear exactly accounts differences 
noteworthy results better bod data minimally edited data crossing brackets rates bod data versus min edited data 
appears part bod extraordinary performance explained fact data cleaner data researchers 
dop slightly better mea sures 
performed statistical analysis test paired differences dop pereira schabes performance run 
table dop versus pereira schabes bod data ideally exactly reproduce exper iments bod algorithm 
unfortunately possible get full specification algorithm 
minimally edited atis data differences statistically insignificant bod data differences statistically significant th percentile 
technique finding sta tistical significance assume test sentences parsed training data results sin gle run correlated 
compare paired differences entire runs sentences constituents 
harder achieve statistical significance 
notice minimum maximum columns dop lines constructed finding paired runs difference dop pereira schabes algorithms 
notice minimum usually negative maximum usually positive meaning tests dop worse pereira schabes better 
important run multiple tests especially small test sets order avoid mis leading results 
timing analysis section examine empirical runtime algorithm analyze bod 
note bod algorithm probably particularly inefficient longer sentences 
takes seconds sentence run algorithm hp versus hours run bod algorithm sparc bod 
factoring hp roughly times faster sparc new algorithm times faster 
course difference may due differences implementation estimate fairly rough 
furthermore believe bod analysis parsing algorithm flawed 
letting represent grammar size represent maximum estima tion error bod correctly analyzes runtime gn 
bod neglects analy sis term assuming constant 
concludes algorithm runs poly nomial time 
algorithm reasonable chance finding proba ble parse number times sample data inversely proportional con ditional probability parse 
instance maximum probability parse probability need sample times reasonably sure finding parse 
note conditional probabil ity probable parse tree general decline exponentially sentence length 
sume number ambiguities sentence increase linearly sentence length word sentence average ambiguity word sentence linear increase ambiguity lead exponential decrease probability probable parse 
probability probable parse decreases exponentially sentence length number random samples needed find probable parse increases exponentially sentence length 
monte carlo algorithm left uncomfortable choice exponentially decreasing probability finding probable parse tially increasing runtime 
admit somewhat informal argument 
monte carlo algorithm tested sentences longer atis corpus reason believe algorithm longer sen tences 
note algorithm true runtime tn shown previously 
analysis bod data dop model sentence exactly correct parse productions correct parse occur training set 
get upper bound performance ex test corpus finding parse trees generated produc tions training corpus 
unfortunately bod provided data specify sentences test training 
find upper bound average case performance upper bound probability particular level perfor mance achieved 
bod randomly split corpus test training 
thesis bod page test sentences correct parse generated training data 
turns sur 
analysis bod data shows difference performance tween results due extraordinarily fortuitous choice test data 
interesting see algorithm performed bod split test training provided split 
bod examine versions dop smoothed allowing productions occur training set coverage re spect version smoothing 
order perform analysis de termine certain details bod parser af fect probability having sentences cor rectly parsable 
chart parser bod problematic cases han productions unary productions ary productions 
kinds pro handled probabilistic chart parser large difficult matrix tions required stolcke lations especially difficult size bod grammar 
examining bod data find removed productions 
assume bod choice eliminated unary productions difficulty correctly parsing 
bod know technique ary productions chart parser written third party bod personal communication 
ary productions parsed straightforward manner converting bi nary branching form different ways convert illus table 
method correct ary branching productions converted way overgeneration introduced 
set special non terminals added partial right hand side 
method continued single original correct continued simple cde de table transformations ary binary branching structures correct continued simple unary unary table probabilities sentences unique productions test data sentences new non terminal introduced original non terminal 
non terminals occur multiple contexts overgeneration 
overgeneration con strained elements tend occur middle right hand side production occur 
simple method new non terminals introduced method possible recover ary branching struc ture resulting parse tree significant overgeneration occurs 
table shows ties possible techniques han unary productions ary productions 
number column probabil ity sentence training data production occurs 
sec ond number probability test set sentences drawn database sentence table arranged generous generous upper left hand corner technique bod reasonably case probability getting test set described lessthan 
aa perl script analyzing bod data available anonymous ftp ftp ftp das harvard edu pub goodman analyze perl slight overestimate reasons including fact sentences drawn replacement 
consider sentence production occurs sen tence corpus probability sentences fin test data causing 
lower right corner give bod absolute max imum benefit doubt assume parser capable parsing unary branching pro gram mar loose definition exact match 
case chance getting test set bod describes 
efficient techniques parsing dop model 
results significant dop model best reported parsing accuracy previously full dop model replicated due difficulty computa tional complexity existing algorithms 
shown previous results par tially due choice test data partially due heavy cleaning data reduced difficulty task 
course research raises ques tions answers 
previous results due choice test data differences implementation partly responsible 
case significant required stand differences account bod tional performance 
complicated fact sufficient details bod implemen tation available 
research shows importance testing small test set importance making cross corpus com new corpus required previous algorithms duplicated comparison 
baker baker 

trainable gram mars speech recognition 
proceedings spring conference acoustical society america pages boston ma june 
bod bod 

mathematical prop erties data oriented parsing model 
third meeting mathematics language mol austin texas 
bod bod 

data oriented parsing general framework tic language processing 
sikkel nijholt editors parsing natural language 
twente netherlands 
bod bod 

monte carlo parsing 
proceedings third inter national workshop parsing technologies tilburg 
bod bod 

anno corpus stochastic grammar 
pro ceedings sixth conference european chapter acl pages 
bod bod 

enriching lin statistics performance models natural language 
university amsterdam illc dissertation series 
pers amsterdam 
bod bod 

problem computing probable tree data oriented parsing stochastic tree grammars 
proceedings seventh conference european chapter acl 
brill eric brill 

corpus ap proach language learning 
ph thesis uni versity pennsylvania 
goodman joshua goodman 

pars ing algorithms metrics 
proceedings th annual meeting acl 
ap pear 
charles john godfrey george doddington 

atis spoken language systems pilot corpus 
darpa speech natural lan guage workshop hidden valley pennsylvania june 
morgan kaufmann 
lari young lari young 

estimation stochastic context free grammars inside outside algorithm 
computer speech language 
magerman david magerman 

nat ural language parsing statistical pattern recognition 
ph thesis stanford university university february 
pereira schabes fernando pereira yves schabes 

inside outside rees partially bracketed corpora 
proceedings th annual meeting acl pages newark delaware 
rabiner rabiner 

tutorial hidden markov models selected applica tions speech recognition 
proceedings ieee february 
scha scha 

language theory language technology competence perfor mance 
de editors de 
van 
dutch 
schabes yves schabes michal roth randy osborne 

parsing wall street journal inside outside algo rithm 
proceedings sixth conference european chapter acl pages 
schabes schabes 

stochastic lexi tree adjoining grammars 
proceedings th international conference compu tational linguistics 
khalil 

efficient disambiguation means stochastic tree sub grammars 
ni editors advances nlp volume current issues linguistic ory 
john benjamins amsterdam 
stolcke andreas stolcke 

ef ficient probabilistic context free parsing algo rithm computes prefix probabilities 
tech nical report tr international com puter science institute berkeley ca 
