analysis switching dynamics competing support vector machines ming wei chang chih jen lin department computer science information engineering national taiwan university taipei taiwan mail ntu edu tw ruby weng department statistics national university taipei taiwan framework unsupervised segmentation time series support vector regression 
applies non stationary time series alter time 
follow architecture pawelzik consists competing predictors :10.1.1.4.9877:10.1.1.4.9877
competing neural networks exploit support vector machines new learning technique :10.1.1.4.9877
results indicate proposed approach :10.1.1.4.9877
differences approaches discussed 
support vector machines svm promising method data classification regression 
types problems exploited 
apply unsupervised segmentation time series 
consider case pawelzik different samples xt yt generated number unknown functions frt rt :10.1.1.4.9877:10.1.1.4.9877
alternate rt yt frt xt 
determine functions fr respective rt time series xt yt 
points different function surfaces task separate points different groups corresponds points surface 
practical applications time series segmentation include example speech recognition signal classification brain data 
training information problem considered unsupervised manner 
order correctly separate points count information xt yt 
previous approaches usually need additional properties 
authors assumed time series low switching rate :10.1.1.4.9877:10.1.1.4.9877
general data time point time series 
addition spatial relation xt 
assumption provides connections xt 
example consider xt yt slowly changing time series 
assume example binary sequence xt yt associated categories 
xt yt class xt yt 
additional information typically available different applications 
focus time series condition slow changing rate assumed 
papers considered issue neural networks 
see :10.1.1.4.9877
important approach hidden markov models examples :10.1.1.53.917
basically proposed competing neural networks weighted relative performance :10.1.1.4.9877
weights different networks adjusted annealed manner gradually increase degree competition 
neural network radius basis function network moody darken type 
follow similar framework discuss point view solving global minimization problem 
addition rbf networks svm differences discussed 
organized follows 
section ii discuss approach svm incorporated 
important parameter algorithm calculation discussed section iii 
section iv demonstrates experimental results data sets 
discussions section ii 
annealed competition support vector machines considering noise assign yt frt xt 
rt yt fi xt 
fi optimal solution non convex optimization problem min yt fi xt 

course find single function fits data objective value zero optimal 
need avoid overfitting adjust values pt obtained 
pt zero find group point xt yt belongs 
authors consider iterative process iteration fixed radial basis function rbf networks minimize quadratic functions min fi yt fi xt :10.1.1.4.9877:10.1.1.4.9877
fi te ith approximate predictor 

centers constructing functions 
new fi obtained update exp exp yt fi xt parameter controls degree competition 
updating rule pt bayes rule assumption xt yt xt yt time series 
put subsequent time series data group 
addition large pt called hard competition winner takes 
consider min solve yt fi xt 

fixed considering min wi bi fi wt wi yt xt 
modification standard support vector regression 
note term wt wi pt fixed equivalent 
original idea support vector regression find function approximates hidden relationships data 
data xt mapped higher dimensional space function 
svm uses insensitive loss function data points tube width considered correctly approximated 
note different weights cpt term objective function 
original svm usually considers uniform penalty parameter data 
essential called regularization term wt wi 
kernel matrix xt xo positive definite solution yt xt bi 
overfitting occurs trapped local minimum want 
adding wt wi remedies problem happen early iterations 
calculate error updating pt 
optimal solution wi xt obtained dual formulation described 
ith predictor fi ik xi xt xt usually called kernel function 
compare rbf networks svm 
rbf kernel svm xt choose ct xt quadratic loss function fact 
difference regularization term 
usually rbf implemented additional regularization term different wt svr rewritten xt xo 
possible advantage svm linear insensitive loss function automatically decides number nonzero xt construct fi 
contrary rbf networks decide ct advance 
example comparing rbf networks svm classification problems 
course set appropriate issue 
discussions relation rbf networks svm 
implementations rbf networks svm different 
unconstrained minimization derivative linear system direct method gaussian elimination 
iterative methods steepest descent direction considered 
modified form svr usually consider dual min yt cp 
square matrix kt xt xo 

main difficulty solving large dense matrix 
issue occurred case classification methods decomposition method proposed :10.1.1.9.6021
decomposition method starts zero vector avoid memory problem percentage support vectors small 
extended regression need modifications handling different upper bounds cpt consider decomposition method software libsvm easily modified purpose :10.1.1.114.4532
modified code libsvm available request 
rbf function parameter affects data fitted 
adjusts smoothness predictors 
initially small machine fit data saturated 
trapped local minimum 
start small gradually increase 
final steps data correctly separated large predictors try fit different groups data 
experience shows fixed sense small large algorithm 
course choice depends smoothness functions fi 
example random sample points functions high degree nonlinearity safe large fixed 
stopping criterion algorithm follows obj obj obj obj obj objective values consecutive iterations 
objective function defined yt fi xt 
fig 

iterations data noise iii 
adjustment section describe method adjusting important parameter controls update yt fi xt 

assume 
define ai percentage data ith group ai data rt ai rt 
log likelihood function logp yt xt yt xt yt rt xt yt xt pi yt xt yt rt xt exp yt fi xt exp 
yt rt xt art prt yt xt 
fig 

iterations data noise estimate 
estimate rt xt yt yt rt xt yt rt xt ai exp ak exp 
comparing suggest choose 
measure variation intuitively clear decrease fi iteration better fit data 
new increase corresponding fact temperature decreasing 
rt xt yt infor mation previous iteration 
shall show obtain 
xl 
yl 
rl 
denote conditional log likelihood function logp log yt rt xt log yt rt xt log yt xt log yt xt rt log ai rt log pi yt xt rt log ai log pi yt xt follow independence observation 
note log pi yt xt log log eti argmax 
simple calculations show maximum occurs eti pt eti rt xt yt ai exp ak exp obtained similarly 
st iteration implementation replace 
practically really calculate 
worry ai unknown advance 
linear loss function support vector regression feel formulations linear quadratic terms 
replaced 
words derivation section assumes normal distribution consider laplace double exponential distribution get results 
iv 
experiments chaotic time series test extreme case completely overlapping input manifold :10.1.1.4.9877
xt yt yt frt xt 
consider xt different functions 
illustration functions 
easily seen functions map 
randomly assign pt keeping condition pti set rbf kernel 
updating consider 
series activated consecutively time steps giving time steps 
periods totally steps 
case algorithm stops iterations 
seen points separated 
assume number functions unknown start competing svms case 
experience indicates exactly svms may fall local minima 
svms may necessary 
consider cases groups possess different number data 
implementation able handle data different ratios 
test implementation add noise function 
algorithm stops iterations iterations 
mackey glass time series similar earlier results check time series obtained mackey glass delay differential equation dx dt td td earlier experiments points selected time steps 
sequentially generate points segment order td 
totally point testing 
embedding dimension 
yt step ahead value consecutive xt 
problem set 
settings implementation section iv 
results seen different segments separated 
discussion svm number support vectors directly affects training testing time 
zero pt means necessary corresponding variables dual problem removed 
theory pt zero due 
threshold removing points small pt computational time largely reduced 
difference svr neural networks width insensitive tube 
svr smoother tolerate noise appropriate 
case noise setting results fewer support vectors running time 
fig 


time point cacciatore nowlan :10.1.1.53.917
mixtures controllers jump linear non linear plants 
cowan tesauro alspector editors advances neural information processing systems volume pages 
morgan kaufmann publishers 

chang 
lin 
libsvm library support vector machines 
software available www csie ntu edu tw cjlin libsvm 
pontil poggio 
regularization networks support vector machines 
advances computational mathematics 

approach adaptive classification 
haykin kosko editors intelligent signal processing 
haykin cong 
classification radar clutter neural networks 
ieee transactions neural networks 
kehagias petridis 
time series segmentation predictive modular neural networks 
neural computation 
pawelzik kohlmorgen muller 
hidden markov mixtures experts application eeg recordings sleep 
theory biosci 
mackey glass 
oscillation chaos physiological control systems 
science 
moody darken 
fast learning networks locally tuned processing units 
neural computation 

ller kohlmorgen pawelzik 
analysis switching dynamics competing neural networks 
ieice transactions fundamentals electronics communications computer sciences 
osuna freund girosi 
training support vector machines application face detection 
proceedings cvpr 
pawelzik 
detecting coherence neuronal data 
van hemmen schulten editors physics neural networks 
springer 
pawelzik kohlmorgen :10.1.1.4.9877
muller 
annealed competition experts segmentation classification switching dynamics 
neural computation 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
sch lkopf 
sung burges girosi niyogi poggio vapnik 
comparing support vector machines gaussian kernels radial basis function 
ieee transactions signal processing 
shi weigend 
time seriously hidden markov experts applied financial engineering 
proc 
conf 
computational intelligence financial engineering pages 
ieee 
vapnik 
statistical learning theory 
wiley new york ny 
