mining concept drifting data streams ensemble classifiers wang wei fan philip yu jiawei han ibm watson research hawthorne ny ibm com dept computer science univ illinois urbana il cs uiuc edu mining data streams concept drifts actionable insights important challenging task wide range applications including credit card fraud protection target marketing network intrusion detection conventional knowledge discovery tools facing challenges overwhelming volume streaming data concept drifts 
propose general framework mining concept drifting data streams weighted ensemble classifiers 
train ensemble classification models ripper naive bayesian sequential chunks data stream 
classifiers ensemble judiciously weighted expected classification accuracy test data time evolving environment 
ensemble approach improves efficiency learning model accuracy performing classification 
empirical study shows proposed methods substantial advantage single classifier approaches prediction accuracy ensemble framework effective variety classification models 

scalability data mining methods constantly challenged real time production systems generate tremendous amount data unprecedented rates 
examples data streams include network event logs telephone call records credit card transactional flows surveillance video streams huge data volume streaming data characterized drifting concepts 
words underlying data generating mechanism concept try learn data constantly evolving 
knowledge discovery streaming data research topic growing interest :10.1.1.119.3124:10.1.1.106.9846:10.1.1.146.51
fundamental problem need solve infinite amount continuous measurements model order capture time evolving trends patterns stream time critical predictions 
huge data volume drifting concepts unfamiliar data mining community 
goals traditional data mining algorithms mine models large databases bounded permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigkdd august washington dc usa copyright acm 
memory 
goal achieved number classification methods including sprint boat fact algorithms require multiple scans training data inappropriate streaming data environment examples coming higher rate repeatedly analyzed 
incremental online data mining methods option mining data streams 
methods continuously revise refine model incorporating new data arrive 
order guarantee model trained incrementally identical model trained batch mode online algorithms rely costly model updating procedure learning slower batch mode 
efficient incremental decision tree algorithm called vfdt introduced domingos :10.1.1.119.3124
streams discrete type data hoeffding bounds guarantee output model vfdt asymptotically nearly identical batch learner 
mentioned algorithms including incremental online methods vfdt produce single model represents entire data stream 
suffers prediction accuracy presence concept drifts 
streaming data generated stationary stochastic process examples need classify may different distribution historical data 
order time critical predictions model learned streaming data able capture date trends transient patterns stream 
revise model incorporating new examples eliminate effects examples representing outdated concepts 
nontrivial task 
challenge maintaining accurate todate classifier infinite data streams concept drifts including accuracy 
difficult decide examples represent outdated concepts effects excluded current model 
commonly approach forget old examples constant rate 
higher rate lower accuracy upto date model supported amount training data lower rate model sensitive current trend prevent discovering transient patterns 
efficiency 
decision trees constructed greedy conquer manner non stable 
slight drift underlying concepts may trigger substantial changes replacing old branches new branches re growing building alternative tree severely compromise learning efficiency 
ease 
substantial implementation efforts required adapt classification methods decision trees handle data streams drifting concepts incremental manner 
usability approach limited state art learning methods applied directly 
light challenges propose weighted classifier ensembles mine streaming data concept drifts 
continuously revising single model train ensemble classifiers sequential data chunks stream 
maintaining date classifier necessarily ideal choice potentially valuable information may wasted discarding results previously trained accurate classifiers 
show order avoid overfitting problems conflicting concepts expiration old data rely data distribution arrival time 
ensemble approach offers capability giving classifier weight expected prediction accuracy current test examples 
benefit ensemble approach efficiency ease 
consider issues cost sensitive learning instance ensemble pruning method shows cost sensitive scenario pruned ensemble delivers level benefits entire set classifiers 
organization 
rest organized follows 
section discuss data expiration problem mining concept drifting data streams 
section prove error reduction property classifier ensemble presence concept drifts 
section outlines algorithm framework solving problem 
section method allows greatly reduce number classifiers ensemble little loss 
experiments related shown section section 
data expiration problem fundamental problem learning drifting concepts identify timely manner data training set longer consistent current concepts 
data discarded 
straightforward solution current approaches discards data indiscriminately old fixed period time passed arrival 
solution conceptually simple tends complicate logic learning algorithm 
importantly creates dilemma vulnerable unpredictable conceptual changes data large training set contain outdated concepts reduces classification accuracy small training set may data result learned model carry large variance due overfitting 
simple example illustrate problem 
assume stream dimensional data partitioned sequential chunks arrival time 
si data came time ti ti 
shows distribution data optimum decision boundary time interval 
problem arrival time part training data remain influential current model data arriving right accurately classified 
hand order reduce influence old data may represent different concept shall data stream training set 
instance training set consisting data discarded 
shown learned model optimum boundary overfitting arrived arrived positive negative arrived data distributions optimum boundaries may carry significant variance insufficient amount data overfitted 
optimum boundary training dataset 
inclusion historical data training hand may reduce classification accuracy 
training set see discrepancy underlying concepts cause problem 
training set consisting solve problem 
may exists optimum avoid problems arising overfitting conflicting concepts 
discard data may provide useful information classify current test examples 
shows combination creates classifier overfitting conflicting concept concerns 
reason similar class distribution 
discarding data criteria solely arrival time shall decisions class distribution 
historical data class distributions similar current data reduce variance current model increase classification accuracy 
non trivial task select training examples class distribution 
show weighted classifier ensemble enables achieve goal 
prove section carefully weighted classifier ensemble built set data partitions sn accurate single classifier built sn 
discuss classifiers weighted 

error reduction analysis test example classifier outputs fc probability instance class classifier ensemble pools outputs classifiers decision 
popular way combining multiple classifiers averaging case probability output ensemble probability output th classifier ensemble 
outputs trained classifier expected approximate posterior class distribution 
addition bayes error remaining error classifier decomposed parts bias variance 
specifically test example probability output classifier ci expressed added error posterior probability distribution class input bias ci variance ci input discussion assume error consists variance major goal reduce error caused discrepancies classifiers trained different data chunks 
assume incoming data stream partitioned sequential chunks fixed size sn sn chunk 
ci gk ek denote models 
ci classifier learned training set si gk classifier learned training set consisting chunks sn sn ek classifier ensemble consisting classifiers cn cn 
classification error sn sn 
sn sn stream test example models classification error test example concept drifting environment models learned stream may carry significant variances applied current test cases 
averaging outputs classifiers ensemble weighted approach 
assign classifier ci weight wi wi reversely proportional ci expected error applied current test cases 
section introduce method generating weights estimated classification errors 
assuming classifier weighted prove property 
ek produces smaller classification error gk classifiers ek weighted expected classification accuracy test data 
error regions associated approximating posteriori probabilities 
prove property bias variance decomposition tumer 
bayes optimum decision assigns class ci ck shown bayes optimum boundary loci points ci cj argmax ck 
decision boundary classifier may vary optimum boundary 
xb denotes amount boundary classifier differs optimum boundary 
words patterns corresponding shaded region erroneously classified classifier 
classifier introduces expected error err addition error bayes optimum decision err fb db area shaded region fb density function tumer proves expected added error expressed err cj ci independent trained model denotes variances 
test example probability output single classifier gk expressed assuming partition si size study variance 
si identical class distribution conceptual drift single classifier gk learned partitions reduce average variance factor presence conceptual drifts ensemble approach weighted averaging combine outputs classifiers ensemble 
probability output ensemble wif cj denotes derivative cj 
wi wi weight th classifier assumed reversely proportional constant wi probability output ek expressed wi assuming variances different classifiers independent derive variance wi wi reverse proportional assumption simplify easy prove equivalently means proved err err means compared single classifier gk learned examples entire window chunks classifier ensemble approach capable reducing classification error weighting scheme classifier weight reversely proportional expected error 
note property guarantee ek higher accuracy classifier gj instance concept drifts partitions dramatic sn sn represent totally conflicting concepts adding cn decision making raise classification error 
weighting scheme assign classifiers representing totally conflicting concepts near zero weights 
discuss tune weights detail section 
classifier ensemble drift ing concepts proof error reduction property section showed classifier ensemble outperform single classifier presence concept drifts 
apply real world problems need assign actual weight classifier reflects predictive accuracy current testing data 
accuracy weighted ensembles incoming data stream partitioned sequential chunks sn sn date chunk chunk size 
learn classifier ci si 
error reduction property test examples give classifier ci weight reversely proportional expected error ci classifying need know actual function learned unavailable 
derive weight classifier ci estimating expected prediction error test examples 
assume class distribution sn training data closest class distribution current test data 
weights classifiers approximated computing classification error sn 
specifically assume sn consists records form true label record 
ci classification error example probability ci instance class mean square error classifier ci expressed sn sn weight classifier ci reversely proportional 
hand classifier predicts randomly probability classified class equals class distributions mean square error mser instance class distribution uniform mser 
random model contain useful knowledge data mser error rate random classifier threshold weighting classifiers 
discard classifiers error equal larger mser 
furthermore computation easy weight wi classifier ci wi mser cost sensitive applications credit card fraud detection benefits total fraud amount detected achieved classifier ci training data sn weight 
predict fraud predict fraud actual fraud cost actual fraud cost table benefit matrix bc assume benefit classifying transaction actual class case class bc 
benefit matrix shown table transaction amount cost fraud investigation cost total benefits achieved ci bi bc fc sn assign weight ci wi bi br br benefits achieved classifier predicts randomly 
discard classifiers negative weights 
handling infinite incoming data flows learn infinite number classifiers time 
impossible unnecessary keep classifiers prediction 
keep top classifiers highest prediction accuracy current training data 
section discuss ensemble pruning detail technique instance pruning 
algorithm gives outline classifier ensemble approach mining concept drifting data streams 
new chunk data arrived build classifier data data tune weights previous classifiers 
usually small experiments chunks size ranging records entire chunk held memory ease 
algorithm classification straightforward omitted 
basically test case classifiers applied outputs combined weighted averaging 
input dataset incoming stream total number classifiers set previously trained classifiers output set classifiers updated weights train classifier compute error rate benefits cross validation derive weight classifier ci apply ci derive bi compute wi top weighted classifiers return algorithm classifier ensemble approach mining concept drifting data streams complexity assume complexity building classifier data set size 
complexity classify test data set order tune weight linear size test data set 
suppose entire data stream divided set partitions complexity algorithm ks hand building single classifier requires 
classifier algorithms super linear means ensemble approach efficient 

ensemble pruning classifier ensemble combines probability benefit output set classifiers 
test example need consult classifier ensemble time consuming online streaming environment 
overview applications combined result classifiers usually converges final value classifiers consulted 
goal pruning identify subset classifiers achieves level total benefits entire ensemble 
traditional pruning classifiers performances average error rate average benefits 
criteria pruning 
criterion mean square error 
goal find set classifiers minimum mean square error 
second approach favors classifier diversity diversity major factor error reduction 
kl distance relative entropy widely measure difference distributions 
kl distance distributions defined log 
case class distributions classifiers 
goal find set classifiers maximizes mutual 
impractical search optimal set classifiers mse criterion kl distance criterion 
greedy methods time consuming complexities greedy methods approaches respectively total number available classifiers 
complexity issue approaches apply cost sensitive applications 
applicability kl distance criterion limited streams mild concept drifting concept drifts enlarge kl distance 
apply instance pruning technique data streams conceptual drifts 
instance pruning cost sensitive applications usually provide higher error tolerance 
instance credit card fraud detection decision threshold launch investigation fraud cost amount transaction words long fraud cost transaction classified fraud matter exact value fraud example assuming cost fraud fraud result prediction 
property helps reduce expected number classifiers needed prediction 
approach instance ensemble pruning 
ensemble consisting classifiers order classifiers decreasing weight pipeline 
weights tuned training set 
classify test example classifier highest weight consulted followed classifiers pipeline 
pipeline procedure stops soon confident prediction classifiers pipeline 
specifically assume ck classifiers pipeline having highest weight 
consulting classifiers ck derive current weighted probability wi pi fraud fk wi final weighted probability derived classifiers consulted fk 
fk fk error stage question ignore fk decide launch fraud investigation confidence fk reached decision 
algorithm estimates confidence 
compute mean variance assuming follows normal distribution 
mean variance statistics obtained evaluating fk 
current training set classifier ck 
reduce possible error range study distribution finer grain 
divide range fk bins :10.1.1.106.9846
example put bin fk 
com pute mean variance error training examples bin stage algorithm outlines procedure classifying unknown instance decision rules applied classifiers pipeline instance fk cost fraud fk cost non fraud uncertain bin belongs confidence interval parameter 
assumption normal distribution delivers confidence confidence 
prediction uncertain instance falls sigma region classifier pipeline ck employed rules applied 
classifiers left pipeline current prediction returned 
result example need classifiers pipeline compute confident prediction 
expected number classifiers reduced 
input dataset incoming stream total number classifiers number bins set previously trained classifiers output set classifiers updated weights mean variance stage bin train classifier compute error rate benefits cross validation derive weight classifier ck apply ck derive bk compute wk top weighted classifiers compute fk belongs bin fk incrementally updates bin algorithm obtaining ensemble construction complexity algorithm outlines procedure instance pruning 
classify dataset size worst case complexity ks 
experiments show actual number classifiers reduced dramatically affecting classification performance 
cost instance pruning mainly comes updating statistics obtained training time 
procedure shown algorithm improved version algorithm 
complexity algorithm remains ks updating statistics costs ks size data stream number partitions data stream 

experiments conducted extensive experiments synthetic real life data streams 
goals demonstrate error reduction effects weighted classifier ensembles evaluate impact input test example confidence level set previously trained classifiers output prediction class cn wi wj fk fk wk pk fraud wk wk bin belongs apply rules check region return fraud non fraud confidence reached fk cost return fraud return non fraud algorithm classification instance pruning frequency magnitude concept drifts prediction accuracy analyze advantage approach alternative methods incremental learning 
base models tests ripper rule learner naive bayesian method 
tests conducted linux machine mhz cpu mb main memory 
algorithms comparison denote classifier ensemble capacity classifiers ek 
classifier trained data set 
compare algorithms rely single classifier mining streaming data 
assume classifier continuously revised data just arrived data faded 
call window classifier data window influence model 
denote classifier gk number data chunks window total number records window 
ensemble ek gk trained amount data 
particularly 
denote classifier built entire historical data starting data stream 
instance boat vfdt classifiers gk classifier :10.1.1.119.3124
streaming data synthetic real life data streams 
synthetic data 
create synthetic data drifting concepts moving hyperplane 
hyperplane dimensional space denoted equation aixi label examples satisfying aixi positive ex amples satisfying aixi negative 
hyperplanes simulate time changing concepts orientation position hyperplane changed smooth manner changing magnitude weights 
generate random examples uniformly distributed multidimensional space weights ai initialized random values range :10.1.1.106.9846
choose value hyperplane cuts multi dimensional space parts volume ai 
roughly half examples positive half negative 
noise introduced randomly switching labels examples 
experiments noise level set 
simulate concept drifts series parameters 
parameter specifies total number dimensions weights involved changing 
parameter specifies magnitude change examples weights ak si specifies direction change weight ai weights change continuously ai adjusted si example generated 
furthermore possibility change reverse direction examples generated si replaced si probability 
time weights updated recompute disturbed 
ai class distribution credit card fraud data 
real life credit card transaction flows cost sensitive mining 
data set sampled credit card transaction records year period contains total transactions 
features data include time transaction merchant type merchant location past payments summary transaction history detailed description data set 
benefit matrix shown table cost investigating fraud transaction fixed cost 
total benefit sum recovered amount fraudulent transactions investigation cost 
study impact concept drifts benefits derive streams dataset 
records st stream ordered transaction time records nd stream transaction amount 
experimental results time analysis 
study time complexity ensemble approach 
generate synthetic data streams train single decision tree classifiers ensembles 
consider window chunks data stream 
shows ensemble approach ek efficient corresponding single classifier gk training 
offers better training performance 
affects classification error 
shows relationship error rate 
dataset generated certain concept drifts weights dimensions change records large chunks produce higher error rates ensemble detect concept drifts occurring inside chunk 
small chunks drive error rates number classifiers ensemble large 
small individual classifier ensemble supported amount training data 
pruning effects 
pruning improves classification efficiency 
examine effects instance pruning credit card fraud data 
show total benefits achieved ensemble classifiers instance pruning 
axis represents number classifiers ensemble ranges 
instance pruning ef training time training time training time ensemble error rate training time error rate fect actual number classifiers consulted reduced 
overload meaning axis represent average number classifiers instance pruning 
pruning reduces average number classifiers reduction 
achieves benefit just drop benefit achieved uses classifiers 
studies phenomena classifiers 
dynamic pruning top classifiers axis shows benefits improvement ratio 
top ranked classifiers pipeline outperform cases st classifier pipeline 
error analysis 
base model compare error rates single classifier approach ensemble approach 
results shown table 
synthetic datasets study dimensions 
shows averaged outcome tests data streams generated varied concept drifts number dimensions changing weights ranges magnitude change ranges records 
study impact ensemble size total number classifiers ensemble classification accuracy 
classifier trained dataset size ranging records records averaged error rates shown 
apparently number classifiers increases due increase diversity ensemble error rate ek drops significantly 
single classifier gk trained amount data higher error rate due changing concepts data stream 
vary chunk size average error rates different ranging 
shows error rate ensemble approach lower single classifier approach cases 
detailed comparison single ensemble classifiers table represents global classifier trained entire history data bold font indicate better result gk ek 
tested naive bayesian ripper classifier setting 
results shown table table 
naive bayesian ripper deliver different accuracy rates confirmed reasonable amount classifiers ensemble ensemble approach outperforms single classifier approach 
error rate benefits error rate instance pruning classifier ensemble classifiers ensemble improvement ratio top classifiers classifier ensemble classifiers reduction ensemble size benefit improvement pruned instance pruning ensemble credit card dataset effects instance pruning single ensemble error rate varying window size ensemble size average error rate single ensemble decision tree classifiers single ensemble table error rate single ensemble decision tree classifiers table error rate single ensemble naive bayesian classifiers table error rate single ensemble ripper classifiers total benefits error rate single ensemble dimension error rate single ensemble dimension error rate single ensemble weight change monotonic changing dimensions total dimensionality monotonic weight change dimension ensemble single total benefits magnitude concept drifts ensemble single total benefits ensemble single total benefits ensemble single varying varying original stream original stream simulated stream simulated stream averaged benefits single classifiers classifier ensembles concept drifts 
studies impact magnitude concept drifts classification error 
concept drifts controlled parameters synthetic data number dimensions weights changing ii magnitude weight change dimension 
shows ensemble approach outperform single classifier approach circumstances 
shows classification error gk ek averaged different dimensions weights changing change dimension fixed 
shows increase classification error dimensionality dataset increases 
datasets dimensions weights changing records 
interesting phenomenon arises weights change monotonically weights dimensions constantly increasing constantly decreasing 
classification error drops change rate increases 

initially weights range 
monotonic changes cause attributes important model easier learn 
cost sensitive learning 
cost sensitive applications aim maximizing benefits 
compare single classifier approach ensemble approach credit card transaction stream 
benefits averaged multiple runs different chunk size ranging transactions chunk 
starting advantage ensemble approach obvious 
average benefits ek gk fixed chunk size 
benefits increase chunk size fraudulent transactions discovered chunk 
ensemble approach outperforms single classifier approach 
study impact concept drifts different magnitude derive data streams credit card transactions 
simulated stream obtained sorting original transactions transaction amount 
perform test simulated stream results shown 
detailed results tests table 
discussion related data stream processing important research domain 
done modeling querying mining data streams instance papers published classification regression analysis clustering 
traditional data mining algorithms challenged characteristic features data streams infinite data flow drifting concepts 
methods require multiple scans datasets handle infinite data flows incremental algorithms refine models continuously incorporating new data stream proposed 
order handle drifting concepts methods revised achieve goal effects old examples eliminated certain rate 
terms incremental decision tree classifier means discard re grow sub trees build alternative subtrees node 
resulting algorithm complicated indicates substantial efforts required adapt state art learning methods infinite concept drifting streaming environment 
aside undesirable aspect incremental methods hindered prediction accuracy 
old examples discarded fixed rate matter represent changed concept learned model supported current snapshot relatively small amount data 
usually results larger prediction variances 
classifier ensembles increasingly gaining acceptance data mining community 
popular approaches creating en table benefits single classifiers classifier ensembles simulated stream table benefits single classifiers classifier ensembles original stream include changing instances training techniques bagging boosting 
classifier ensembles advantages single model classifiers 
classifier ensembles offer significant improvement prediction accuracy 
second building classifier ensemble efficient building single model model construction algorithms super linear complexity 
third nature classifier ensembles lend scalable parallelization line classification large databases 
previously averaging ensemble scalable learning large datasets 
show model performance estimated completely learned 
weighted ensemble classifiers concept drifting data streams 
combines multiple classifiers weighted expected prediction accuracy current test data 
compared incremental models trained data window approach combines set experts credibility adjusts nicely underlying concept drifts 
introduced dynamic classification technique streaming environment results show enables dynamically select subset classifiers ensemble prediction loss accuracy 

babcock babu datar widom 
models issues data stream systems 
acm symposium principles database systems pods 
babu widom 
continuous queries data streams 
sigmod record 
eric bauer ron kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
chen dong han wah wang 
multi dimensional regression analysis time series data streams 
proc 
large database vldb hongkong china 
william cohen 
fast effective rule induction 
int conf 
machine learning icml pages 
domingos 
unified bias variance decomposition applications 
int conf 
machine learning icml pages 
domingos hulten :10.1.1.119.3124
mining high speed data streams 
int conf 
knowledge discovery data mining sigkdd pages boston ma 
acm press 
fan wang yu lo 
progressive modeling 
int conf 
data mining icdm 
fan wang yu lo 
inductive learning sequential scan 
int joint conf 
artificial intelligence 
fan wang yu stolfo 
framework scalable cost sensitive learning combining probabilities benefits 
siam int conf 
data mining sdm 
wei fan fang chu wang philip yu 
pruning dynamic scheduling cost sensitive ensembles 
proceedings th national conference artificial intelligence aaai 
yoav freund robert schapire 
experiments new boosting algorithm 
int conf 
machine learning icml pages 
gao wang 
continually evaluating similarity pattern queries streaming time series 
int conf 
management data sigmod madison wisconsin june 
gehrke ganti ramakrishnan loh 
boat optimistic decision tree construction 
int conf 
management data sigmod 
geman bienenstock doursat 
neural networks bias variance dilemma 
neural computation 
greenwald khanna 
space efficient online computation quantile summaries 
int conf 
management data sigmod pages santa barbara ca may 
guha motwani callaghan 
clustering data streams 
ieee symposium foundations computer science focs pages 
hall bowyer kegelmeyer moore chao 
distributed learning large data sets 
workshop distributed parallel knowledge discover 
hulten spencer domingos 
mining time changing data streams 
int conf 
knowledge discovery data mining sigkdd pages san francisco ca 
acm press 
ross quinlan 
programs machine learning 
morgan kaufmann 
shafer agrawal mehta 
sprint scalable parallel classifier data mining 
proc 
large database vldb 
stolfo fan lee prodromidis chan 
credit card fraud detection meta learning issues initial results 
aaai workshop fraud detection risk management 
nick street kim 
streaming ensemble algorithm sea large scale classification 
int conf 
knowledge discovery data mining sigkdd 
tumer joydeep ghosh 
error correlation error reduction ensemble classifiers 
connection science 
utgoff 
incremental induction decision trees 
machine learning 
