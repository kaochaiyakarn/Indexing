level paths generation kevin knight usc information sciences institute admiralty way marina del rey ca knight isi edu large scale natural language generation re quires integration vast mounts knowledge lexical grammatical tual 
robust generator able operate pieces knowledge axe missing 
robust incomplete inaccurate inputs 
attack problems built hybrid gen erator gaps symbolic knowledge filled statistical methods 
describe algorithms show experimental results 
discuss hybrid generation model simplify current generators enhance portability perfect knowledge principle obtainable 
large scale natural language generation nlg system unrestricted text able op erate environment conceptual terms words phrases 
turning conceptual expressions english requires integration large knowledge bases kbs including grammar ontology lexicon collocations mappings tween 
quality nlg system depends quality inputs knowledge bases 
perfect kbs exist impor tant question arises build high quality nlg systems robust incomplete kbs inputs 
robustness heavily stud ied natural language understanding weischedel black hayes lavie received attention nlg robin 
describe hybrid model natural language generation offers improved performance presence knowledge gaps generator grammar lexicon errors se mantic input 
model comes practi cal experience building large japanese english newspaper machine translation system japan gloss knight knight 
system translates japanese representations terms drawn sensus vasileios hatzivassiloglou department computer science columbia university new york ny vh cs columbia edu ogy knight luk node knowl edge base skeleton derived resources word net miller longman dictionary procter penman upper model bateman 
representations turned en glish generation 
processing unrestricted newspaper text modules japan gloss robust 
addition show model useful simplifying design generator knowl edge bases perfect knowledge available 
accomplished aspects lexical choice preposition selection non compositional constraints cal component 
generator simpler rules combine freely price simplicity output may invalid 
point statistical component intervenes filters output fluent expres sions 
advantage level approach knowledge bases generator simpler easier develop portable mains accurate robust presence knowledge gaps 
knowledge gaps machine translation experiences traced generation disfluencies sources incom plete inaccurate conceptual interlingua struc tures caused knowledge gaps source lan guage analyzer knowledge gaps gen erator 
categories gaps include interlingual analysis include ac representations number definiteness time 
unmarked japanese require exceedingly difficult inferences recover 
generation lexicon mark rare words generally distinguish tween near synonyms finger vs 
see kukich discussion fluency problems nlg systems 
generation lexicon contain collocational knowledge field vs zone 
lexico syntactic constraints tell hi vs say hi syntax semantics mappings vase broke vs food ate selectional restrictions available accurate 
generation system penman pen man robust supplies appropriate defaults knowledge missing 
default choices frequently ones hy model describe provides satisfactory solutions 
issues lexical choice process selecting words semantic concept intrinsically linked syntactic semantic discourse structure issues 
multiple constraints apply lexical decision highly interdependent manner 
lexical decisions affect past lexical decisions purely local sense affect semantic roles 
consider case time adjuncts express single point time sume generator decided prepositional phrase 
forms adjuncts 
left monday 
february 
terms interactions rest sentence manifestations adjunct identical 
different prepositions constraint semantic syntactic heads pp propagate outside pp 
consequently selection preposition postponed 
existing generation models select preposition defaults randomly 
possible alternatives explicitly ing lexical constraints 
penman gener ation system penman defaults tion choice point time adjuncts commonly preposition cases 
fuf surge elhadad generation system example prepositional lexical restrictions time adjuncts encoded hand producing fluent expressions cost larger gram mar collocational restrictions example lexical constraints 
phrases straight consider lexical choice general problem open closed class words limiting done generation literature 
frequently sports reports express historical information decomposed semantically head noun plus modifiers 
ellipsis head noun consid ered detailed corpus analysis actual basketball game reports robin shows forms straight won lost hree consecutive won lost straight regularly form won lost consecutive 
achieve fluent output knowledge generation paradigm lexical constraints type explicitly identified represented 
examples indicate presence domain dependent lexical constraints explainable semantic grounds 
case prepositions time adjuncts constraints institutionalized language concept month relates preposition month names say herskovits 
furthermore lexical constraints limited syntagmatic con straints discussed 
generator able produce sufficiently varied text multiple tions concept accessible 
generator faced paradigmatic choices alternatives sufficient informa tion may look equivalent 
choices include choices synonyms near synonyms choices alternative syntactic realizations semantic role 
possible alternatives share level fluency currency domain rough paraphrases 
short knowledge generators faced multiple complex interacting lexical con straints integration constraints difficult problem extent need different specialized architecture lexical choice domain suggested 
compositional approaches lexical choice successful detailed representa tions lexical constraints collected en tered lexicon elhadad ku 
unfortunately constraints identified manually automatic methods acquisition types lexical knowledge exist smadja mckeown extracted constraints transformed generator representa tion language hand 
narrows scope lexicon specific domain approach fails scale unrestricted language 
goal domain independent generation need methods producing reasonable output absence large part information tradi including constraints discussed ing example discourse structure user models speaker hearer pragmatic needs 
available lexical chooser 
current solutions strategies lexical choice knowledge gaps exist selection default random choice alternatives 
default choices advantage carefully chosen mask knowledge gaps extent 
exam ple penman defaults article selection tense produce dog chases cat absence definiteness information 
choosing tactic works mass count singular plural occasionally proper nouns 
side outnumber knight guess ing frequently wrong 
give preference nominalizations clauses 
generates sentences plan statement filing bankruptcy avoiding disasters plan said file bankruptcy 
course renditions plan say file bankruptcy 
alternative randomized decisions offers increased paraphrasing power risk producing non fluent expressions generate sen tences dog chased cat dog chase cat earth circles sun 
sum defaults help knowledge gaps take time construct limit para phrasing power return mediocre level quality 
seek methods better 
statistical methods approach problem incomplete knowledge 
suppose knowledge bases input may rendered sentence sentence device invoke new easily obtainable knowledge score input output pair choose vice versa 
alter native forget simply score basis fluency 
essentially assumes generator produces valid mappings may unsure correct rendition 
point approximation modeling fluency likelihood 
words seen past 
oc times choose ifa long sentences probably seen 
case approxi mations required 
example contain frequent word sequences 

reasoning led cal language modeling 
built language model see thorough dis defaulting nlg systems 
english language estimating bigram trigram probabilities large collection words wall street journal material 
smoothed estimates class mem proper names numbers accord ing extended version enhanced turing method church gale re words 
smoothing operation optimally regresses probabilities seen grams assigns non zero probability unseen grams depends component grams words bi grams 
resulting conditional probabilities converted log likelihoods reasons nu accuracy estimate probability english sentence accord ing markov assumption log log wi bigrams log log wi trigrams equations assign lower lower probabilities longer sentences need compare sentences different lengths heuristic strictly increasing function sentence length added log likelihood estimates 
experiment goal integrate symbolic knowl edge penman system statistical knowledge language model 
took se mantic representation generated automatically short japanese sentence 
pen man generate english sentences corre sponding 
possible com values binary ternary features unspecified se mantic input 
features relevant semantic representation values extractable japanese sentence combinations corresponded par ticular interpretation possible presence incompleteness semantic put 
specifying feature forced penman particular linguistic decision 
example adding identifiability forces choice de lex feature offers explicit con trol selection open class words 
literal translation input sentence new plan establish february 
randomly selected transla tions note object establishing ac tion unspecified japanese input pen man supplies placeholder necessary ensure grammaticality acl data collection initiative cd rom 
new mind establishing february 
new plans launching february 
new companies goal launching february 
ranked sentences bigram version statistical language model hope renditions come top 
abridged list outputs log likelihood scores heuristically corrected length rankings new plans launch february 
new plans foundation february 
new plans establishment february 
new plans establish february 



new companies plan establishment february 
new companies plan launching february 


new companies goal foundation february 
new companies mind establish february 


experiment shows statistical mod els help choices generation fails computational strategy 
running penman times expensive compared cost exhaustively exploring combinations larger input representations corresponding sen tences typically newspaper text 
choice points typically multiply millions billions potential sentences infeasible generate independently 
leads consider algorithms 
paths generation explicitly constructing possible tions semantic input running penman efficient data structure control algorithm express possible ties 
data structure word acyclic state transition network start state fi nal state transitions labeled words 
word lattices commonly model uncertainty speech recognition waibel lee adapted gram models 
discussed section number gen eration difficulties traced existence constraints words phrases 
genera tor operates lexical islands interact words concepts 
identify islands important problem nlg ical rules agreement may help group words collocational knowledge mark boundaries lexical islands nal compounds 
explicit information resort treating single words lex ical islands essentially adopting view maximum compositionality 
rely statistical model correct approximation identifying violations compositionality principle fly actual text generation 
type lexical islands manner identified affect way generator processes 
island cor responds independent component final sentence 
individual word island specifies choice point search causes creation state lattice continuations alterna tive island paths leave state 
choices alternative lexi cal islands concept states lattice arcs leading sub lattices corresponding island 
semantic input generator transformed word lattice search com ponent identifies highest scoring paths start final state cal language model 
version best algorithm chow schwartz viterbi style beam search algorithm allows extraction just best scoring path 
knight details search algorithm method applied es parameters statistical model 
approach differs traditional top generation way top bottom parsing differ 
top parsing backtracking employed exhaustively examine space possible alternatives 
similarly tra ditional control mechanisms generation operate top deterministically meteer tomita nyberg penman backtracking previous choice points elhadad 
mode operation unnecessarily du lot run time sophisticated control directives included search engine elhadad robin 
contrast bottom parsing generation model special data structure chart lattice respectively efficiently encode multiple analyses allow structure sharing alternatives eliminating repeated search 
word lattices produced gen erator look 
generator complete far generator knows 
knowledge word lattice degenerate string large federal deficit fell suppose uncertain definiteness number 
generate lattice paths deficit stands empty string 
run risk gram model pick non grammatical path large federal deficits fell 
pro duce lattice large federal deficits case knowledge agreement constrain choices offered statistical model paths 
notice path lattice states complex path 
gram length critical 
long distance features control gram rely statistical model 
fortunately long distance features agreement go symbolic gen erator 
example symbolic statistical knowledge sources contain information sig advantage combining 
need algorithm converting gener ator inputs word lattices 
approach assign word lattices fragment input bottom compositional fashion 
example consider semantic input writ penman style sentence plan language spl penman concepts drawn sensus ontology knight luk may rendered english easy cans obtain guns quality domain agent american patient gun arm range process semantic subexpressions bottom order grammar assigns call structure subex pression 
structure consists list dis syntactic categories paired english word lattices syn lat syn lat 
climb input expression grammar various word lattices 
grammar organized semantic feature patterns english syntax having np vp rule semantic triggers agent patient rule english renderings 
sample rule xl agent patient rest seq xl rip tensed np seq xl np tensed wrd seq xl np tensed inf raise seq np passive xl rip seq wrd xl np wrd np inf raise seq xl np seq wrd np np seq wrd rip rip seq rip np wrd xl np input semantic pattern locate grammar rule matches rule left hand side features rest contained input pattern 
feature rest mech anism allowing partial matchings rules semantic inputs 
input features matched selected rule collected rest recursively matched gram mar rules 
remaining features compute new structures rule right hand side 
example rule gives ways syntactic ways infinitive way np 
corresponding word lattices built elements include seq 
create lattice sequentially gluing lattices 
create lattice branching wrd create smallest lattice single arc labeled word xn syn structure se mantic material xn feature contains syn lat return word lattice lat oth fail 
failure inside alternative right hand side rule causes alternative fail ignored 
alternatives processed results collected new structure 
word lattices created rule merged final 
grammar organized seman tic patterns nicely concentrates mate required build word lattices 
unfortunately forces restate syntactic constraint places 
second problem sequential composition allow insert new words inside old lattices needed generate sentences john looked 
extended tation allow constructions full lution move unification framework structures replaced arbitrary fea ture structures syn lat fields 
course requires extremely efficient handling disjunctions inherent large word lattices 
results implemented medium sized grammar en glish ideas previous section experiments ma chine translation system 
system converts se mantic input word lattice sending result sentence extraction programs random follows random path lattice 
default follows topmost path lat tice 
alternatives ordered gram mar writer topmost lattice path cor responds various defaults 
grammar defaults include singular noun phrases def article nominal direct objects versus active voice versus alphabet ically synonym open class words statistical sentence extractor word bigram probabilities described sec tions 
evaluation compare english outputs sources 
look lattice prop erties execution speed 
space limitations pre vent tracing generation long sentences show short ones 
note sample sentences shown random ex traction model quality mally expected knowledge genera tor high degree ambiguity un specified features semantic input 
completeness turn attributed part lack information japanese source text part desire find ambiguity automatically resolved statistical model 
input agent patient lattice created agent patient nodes arcs paths distinct unigrams distinct bigrams 
random extraction automobiles 
am steal autos 
auto 
default extraction steals auto 
statistical bigram extraction charged stole car 
charged stole cars 
charged stole cars 
charged stole car 
charges stole car 
total execution time cpu seconds 
input quality domain agent american patient gun arml range easy effortless lattice created nodes arcs paths distinct unigrams distinct bigrams 
random extraction guns americans easiness 
guns americans 
easy americans gun 
default extraction gun american easy 
statistical bigram extraction easy americans obtain gun 
easy americans obtain gun 
easy americans obtain gun 
easy american obtain gun 
easy americans obtain gun 
total execution time cpu seconds 
input quality domain quality domain take inl agent patient range range possible potential lattice created nodes arcs paths distinct unigrams distinct bigrams 
random extraction may obliged eat 
consumptions may requirements 
requirement chicken eaten 
default extraction consumption chicken obligatory possible 
statistical bigram extraction may eat chicken 
eat chicken 
may required eat chicken 
required eat chicken 
may obliged eat chicken 
total execution time cpu seconds 
final abbreviated example comes expressions produced semantic analyzer involving long sentences charac newspaper text 
note lattice larger previous ex amples encodes paths 
lattice created nodes arcs paths distinct unigrams distinct 
random extraction subsidiary japan hold stocks majority beginnings stepper dry etching devices applied microchip 
statistical bigram extraction japanese subsidiary holds majority stocks production dry etching devices construct chips planned 
total execution time cpu seconds 
strengths weaknesses paths generation leads new style incre mental grammar building 
dealing new construction erate providing grammar ways ex press thing 
watch statistical component selections 
selections correct need refine grammar 
example grammar lexical grammatical case distinctions 
lattices included paths saw saw statistical model stu avoided bad paths fact see incorrect case usage genera tor 
likewise grammar proposes box box statistically 
special rule prohibit articles appearing noun phrase bigram awful null article selected presence possessive pronoun 
get away treating possessive pronouns regular adjectives greatly simplifying grammar 
able simplify genera tion morphological variants 
true irregular forms child children kept small exception table problem multiple regular patterns usually increases size table dra matically 
example ways plu noun cor rect noun potatoes photos 
inflectional derivational patterns 
approach apply patterns insert results word lattice 
fortunately statis tical model steers clear sentences containing non words 
get small exception table furthermore spelling habits automatically adapt training corpus 
importantly level generation model allows indirectly apply lexical constraints selection open class words constraints explicitly represented gen erator lexicon 
example selection word pair frequently occurring cent words automatically create strong bias selection member pair compatible semantic concept lexicalized 
type collocational knowl edge additional collocational information long variable distance dependencies successfully past increase fluency generated text smadja 
collocational infor mation extracted automatically manually reformulated generator represen tational framework addi tional constraint pure knowledge gen eration 
contrast level model provides automatic collection implicit represen tation collocational constraints adjacent words 
addition absence external lexical con straints language model prefers words typ ical common domain generic overly specialized formal alternatives 
result text fluent closely sim style training corpus respect 
note example choice obtain second example previous section favor formal 
times statistical model finish job 
bigram model happily se lect sentence hires men pilots 
see plenty output grammatical agreement needed 
con sider planned increase production model drops article planned crease frequent bigram 
subtle interaction planned main verb adjective 
model prefers short sentences long ones semantic content favors con selects bad grams avoid longer clearer rendition 
interesting problem encountered similar speech recognition models 
currently investigating solutions problems highly exper setting 
statistical methods give way address wide variety knowledge gaps generation 
possible load non traditional duties generator word sense disambiguation machine translation 
example bei japanese may mean american rice sha may mean 
reason analysis fails resolve ambiguities generator pass lattice builds american case statistical model strong preference american nearly correct translation 
furthermore level generation model implicitly handle paradigmatic matic lexical constraints leading tion generator grammar lexicon enhancing portability 
retraining cal component different domain au pick peculiarities guage preferences particular words collocations 
time take advantage strength knowledge approach guarantees grammatical inputs cal component reduces amount language structure retrieved statistics 
approach addresses problematic aspects pure knowledge generation incomplete knowledge inevitable pure statistical bag gen eration brown statistical system linguistic guidance 
course results perfect 
im prove enhancing statistical model incorporating knowledge constraints lattices possibly automatic knowledge acquisition methods 
direction intend pursue rescoring top generated sen tences expensive extensive methods incorporating example stylistic features ex plicit knowledge flexible collocations 
acknowledgments yolanda gil eduard hovy kathleen mckeown jacques robin bill swartout acl reviewers helpful comments ear versions 
supported part advanced research projects agency order contract mda department defense 
john bateman 

upper modeling level semantics natural language processing 
see dagan itai study lexical occurrences choose open class word translations 
proc 
fifth international workshop natural language generation pages dawson pa peter brown della pietra della pietra mercer 

mathematics statistical machine translation parameter esti mation 
computational linguistics june 
yen lu chow richard schwartz 

best algorithm efficient search procedure finding top sentence hypotheses 
proc 
darpa speech natural language workshop pages 
kenneth church william gale 

comparison enhanced turing deleted estimation methods estimating proba bilities english bigrams 
computer speech language 
ido dagan alon itai 

word sense dis second language monolingual corpus 
computational linguistics 
laurence 

linguistic basis text generation 
studies natural language processing 
cambridge university press 
michael elhadad jacques robin 

con content realization functional cation grammars 
robert dale eduard hovy dietmar stock editors automated natural language generation pages 
verlag 
michael elhadad 

argumentation control lexical choice unification im plementation 
ph thesis computer science department columbia university new york 
karin gen anne kil ger 

default handling incremental gener ation 
proc 
coling pages ky japan 
vasileios hatzivassiloglou kevin knight 

unification glossing 
proc 
philip hayes 

construction specific ap proach focused interaction flexible parsing 
proc 
acl pages 
annette herskovits 

language spatial cognition interdisciplinary study sitions english 
studies natural language processing 
cambridge university press 
kevin knight 

auto mated documents 
proc 
kevin knight steve luk 

building large scale knowledge base machine transla tion 
proc 
kevin knight matthew haines vasileios hatzivassiloglou eduard hovy iida steve luk okumura richard whitney kenji yamada 

integrating knowledge bases statistics mt proc 
conference association machine trans lation americas 
kevin knight matthew haines vasileios hatzivassiloglou eduard hovy iida steve luk richard whitney kenji yamada 

filling knowledge gaps broad coverage mt system 
proc 
ijcai 
karen kukich mckeown shaw robin morgan phillips 

user needs analysis design methodology auto mated document generator 
palmer editors current computational linguistics honour don walker 
kluwer academic press boston 
karen kukich 

fluency natural language reports 
david mcdonald leonard editors natural language generation sys tems 
springer verlag berlin 
alon lavie 

integrated heuristic scheme partial parse evaluation 
proc 
cl stu dent session 
marie meteer mcdonald anderson forster gay st bun 

design implementa tion 
technical report coins university massachussets amherst ma 
george miller 

wordnet line lexical database 
international journal lexicography 
special issue 
penman 

penman documentation 
tech nical report usc information sciences institute 
paul procter editor 

longman dictionary contemporary english 
longman essex uk 
jacques robin 

revision generation natural language summaries providing historical background corpus analysis design im plementation evaluation 
ph thesis com puter science department columbia university new york ny 
technical report cu cs 
frank smadja kathleen mckeown 

collocations language generation 
com putational intelligence december 
tomita nyberg 

transformation kit user guide 
technical re port cmu cmt memo center machine translation carnegie mellon university 
waibel lee editors 

readings speech recognition 
morgan kaufmann san mateo ca 
weischedel black 

responding potentially sentences 
am 
com putational linguistics 
