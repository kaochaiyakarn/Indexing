northwestern university department electrical engineering computer science limited memory algorithm bound constrained optimization richard byrd lu jorge nocedal zhu technical report nam revised may computer science department university colorado boulder boulder colorado 
author supported nsf ccr aro daal afosr afosr 
department electrical engineering computer science northwestern university evanston il nocedal eecs nwu edu 
authors supported national science foundation ccr asc department energy de fg er 
limited memory algorithm bound constrained optimization richard byrd lu jorge nocedal zhu algorithm solving large nonlinear optimization problems simple bounds described 
gradient projection method uses limited memory bfgs matrix approximate hessian objective function 
shown form limited memory approximation implement algorithm ciently 
results numerical tests set large problems reported 
key words bound constrained optimization limited memory method nonlinear optimization quasi newton method large scale optimization 
abbreviated title limited memory method 

describe limited memory quasi newton algorithm solving large nonlinear optimization problems simple bounds variables 
write problem min subject nonlinear function gradient available vectors represent lower upper bounds variables number variables assumed large 
algorithm require second derivatives knowledge structure objective function applied hessian matrix practical compute 
limited memory quasi newton update approximate hessian matrix away storage required linear algorithm described similar algorithms proposed conn gould toint gradient projection method determine set active constraints iteration 
algorithm distinguished methods line searches opposed trust regions mainly limited memory bfgs matrices approximate hessian objective function 
properties limited memory matrices far reaching consequences implementation method discussed 
nd making compact representations limited memory matrices described byrd nocedal schnabel computational cost iteration algorithm 
gradient projection approach determine active set studies indicate possess theoretical properties appears cient large problems 
main components algorithm useful frameworks long limited memory matrices approximate hessian objective function 

outline algorithm 
iteration current iterate xk function value fk gradient gk positive de nite limited memory approximation bk 
allows form quadratic model xk mk xk xk xk bk xk just method studied conn gould toint algorithm approximately minimizes mk subject bounds 
done rst gradient projection method nd set active bounds followed minimization mk treating bounds equality constraints 
rst consider piece wise linear path xk obtained projecting steepest descent direction feasible region li xi li xi xi li ui ui xi ui 
compute generalized cauchy point de ned rst local minimizer univariate piece wise quadratic qk mk variables value lower upper bound comprising active held xed 
consider quadratic problem subspace free variables min fmk xi subject li xi ui rst solve approximately solve ignoring bounds free variables accomplished direct iterative methods subspace free variables dual approach handling active bounds lagrange multipliers 
iterative method employed starting point iteration 
truncate path solution satisfy bounds 
approximate solution xk problem obtained compute new iterate xk line search dk xk xk satis es su cient decrease condition attempts enforce curvature condition xk xk kg dk jg jg parameters values respectively code 
line search ensures iterates remain feasible region described 
gradient compute new limited memory hessian approximation bk new iteration 
algorithm hessian approximation bk positive de nite approximate solution xk quadratic problem de nes descent direction dk xk xk objective function see rst note generalized cauchy point isa minimizer mk projected steepest descent direction satis es mk xk mk ifthe projected gradient nonzero 
point xk path minimizer mk decreases value mk xk larger value xk mk xk mk mk xk xk dk dt inequality implies dk positive de nite dk zero 
analysis study possibility 
gradient projection step computation believe analyses similar possible problem degenerate case 
hessian approximations bk algorithm limited memory bfgs matrices nocedal byrd nocedal schnabel 
matrices take advantage structure problem require small amount storage show allow computation generalized cauchy point subspace minimization performed operations 
new algorithm similar computational demands limited memory algorithm bfgs unconstrained problems described liu nocedal gilbert 
sections describe detail limited memory matrices computation cauchy point minimization quadratic problem subspace 

limited memory bfgs matrices 
algorithm limited memory bfgs matrices represented compact form described byrd nocedal schnabel 
iterate xk algorithm stores small number say correction pairs fsi sk xk xk yk gk gk correction pairs contain information curvature function conjunction bfgs formula de ne limited memory iteration matrix bk 
question best represent matrices explicitly forming 
proposed compact outer product form de ne limited memory matrix bk terms correction matrices yk yk yk sk sk sk speci cally shown positive scaling parameter correction pairs fsi satisfy sti yi matrix obtained updating bfgs formula pairs fsi times written sk lk bk wk yk sk dk mk lk lk dk matrices lt st sk yk dk yk point slight rearrangement equation 
note mk matrix chosen small integer cost computing inverse negligible 
shown compact representation various computations involving bk inexpensive 
particular product bk times vector occurs algorithm performed ciently 
similar representation inverse limited memory bfgs matrix hk approximates inverse hessian matrix hk mk sk rk wk yk sk dk yk yk 
note slight rearrangement equation 
representations limited memory matrices analogous exist quasi newton update formulae sr dfp see principle bk algorithm 
consider bfgs considerable computational experience unconstrained case indicating limited memory bfgs performs 
bounds problem may prevent line search satisfying see guarantee condition yk holds cf 
dennis schnabel 
maintain positive de niteness limited memory bfgs matrix discard correction pair curvature condition yk eps kyk satis ed small positive constant eps 
happens delete oldest correction pair normally done limited memory updating 
means directions sk yk may include indices 
generalized cauchy point 
objective procedure described section nd rst local minimizer quadratic model piece wise linear path obtained projecting points steepest descent direction xk feasible region 
de ne xk section drop index outer iteration stand gk xk bk 
subscripts denote components vector example gi denotes th component ofg 
superscripts represent iterates piece wise search cauchy point 
de ne breakpoints coordinate direction compute ti ui gi gi li gi gi sort ng increasing order obtain ordered set tj tj ng 
done means heapsort algorithm 
search tg piecewise linear path expressed xi xi ti 
suppose examining interval 
de ne th breakpoint td gi tj ti notation write quadratic line segment td zj td td line segment written quadratic zj bz bz dj bd fj parameters dimensional quadratic fj zj bz gt bz dj bd di erentiating equating zero obtain positive de nite de nes minimizer provided tj lies tj tj 
generalized cauchy point lies tj iff tj iff 
generalized cauchy point exploring interval set update directional derivatives search moves interval 
assume moment variable active att denote index tb zero corresponding component search direction eb th unit vector 
de nitions obtain bz bz bd bz bz bd bd bd eb beb expensive computations bd eb beb eb bz eb bd eb beb require operations dense limited memory matrix 
appear computation generalized cauchy point require operations worst case segments piece wise linear path examined 
cost prohibitive large problems 
limited memory bfgs formula de nition updating formulae tj mwt zj mw wt stands th row matrix operations remaining zj dj zj dj updated iteration simple computation 
maintain vectors updating expressions mc fj mpj wt require operations 
variable active att situation repeat updating process just described examining new interval 
able achieve signi cant reduction cost computing generalized cauchy point 

examination rst segment projected steepest descent path computation generalized cauchy point requires operations 
subsequent segments require operations number correction vectors stored limited memory matrix 
usually small say cost examining segments rst negligible 
algorithm describes detail toachieve savings computation 
note necessary keep track vector component corresponding bound active needed update fj fj algorithm cp computation generalized cauchy point 
wmw initialize compute ti di xi ui gi gi xi li gi gi ti gi fi ti mn operations operations operations dt dt pt mp operations tmin told fg heapsort algorithm ti remove 
examination subsequent segments tmin cp ub zb cp db db lb xb tp operations tf mc operations mp wt operations operations db tmin told fg heapsort algorithm ti remove told tmin maxf tmin told told tmin cp xi ti ti remove step algorithm updates vector termination xk vector initialize subspace minimization primal direct method conjugate gradient method discussed section 
operation counts take account multiplications divisions 
note computations inside loop 
denotes total number segments explored total cost algorithm cp operations plus log operations approximate cost heapsort algorithm 

methods subspace minimization 
cauchy point proceed approximately minimize quadratic model mk space free variables impose bounds problem 
consider approaches minimize model direct primal method sherman morrison woodbury formula primal iterative method conjugate gradient method direct dual method lagrange 
appropriate problem dependent experimented numerically 
approaches rst minimizing mk ignoring bounds appropriate point truncate move satisfy bound constraints 
notation section 
integer denotes number free variables cauchy point words variables bound previous section denotes set indices corresponding free variables note set de ned completion cauchy point computation 
de ne zk matrix columns unit vectors columns identity matrix span subspace free variables similarly ak denotes matrix active constraint gradients consists unit vectors 
note zk 
direct primal method 
aka primal approach variables bound generalized cauchy point solve quadratic problem subspace remaining free variables starting imposing free variable bounds 
consider points form zk vector dimension notation points form write quadratic mk fk xk xc xk bk xk gk bk xk xc bk constant reduced hessian mk bk bk gk bk xk reduced gradient express reduced gradient gk xk vector saved cauchy point computation costs extra operations 
subspace problem formulated min mk bk subject li xc di ui xc subscript denotes th component 
minimization solved direct method discuss iterative method discussed subsection constraints imposed backtracking 
reduced limited memory matrix bk small rank correction diagonal matrix formally compute inverse means sherman morrison woodbury formula obtain unconstrained solution subspace problem rc backtrack feasible region necessary obtain positive scalar de ned maxf li ui fg approximate solution subproblem xi xi zk 
remains consider perform computation 
bk zk reduced matrix mw dropped subscripts simplicity 
applying sherman morrison woodbury formula see example obtain mw zz mw unconstrained subspace newton direction zt mw zz mw set free variables determines matrix limited memory bfgs matrix de ned terms procedure implements approach just described 
note columns unit vectors operation zv amounts selecting appropriate elements operation counts include multiplications divisions 
recall denotes number free variables number corrections stored limited memory matrix 
direct primal method 

compute operations 
mt operations 
mv operations 
form mw zz zz mt operations mn operations 
operations 
wv mt operations 
find satisfying operations 
compute xi operations total cost subspace minimization step sherman morrison woodbury formula mt operations 
quite acceptable small free variables 
problems opposite true constraints active andt large 
case cost direct primal method quite large mechanism provide signi cant savings 
note large computation matrix zz zz zz zz th zz zz step requires operations drives cost 
fortunately reduce cost variables enter leave active set iteration saving matrices zz zz zz matrices updated account parts inner products corresponding variables changed status add rows columns corresponding new step 
computational experiments wehave implemented device 
addition larger cient relationship zz aa follows compute zz similar relationships matrices zz zz expression relationships described 

primal conjugate gradient method 
approach approximately solving subspace problem apply conjugate gradient method positive de nite linear system bk iteration boundary encountered residual small 
note accuracy solution controls rate convergence algorithm correct active set identi ed chosen care 
follow conn gould toint conjugate gradient iteration residual satis es rk min iteration bound conjugate gradient step violate bound guaranteeing satis ed 
conjugate gradient method appropriate eigenvalues bk identical 
describe conjugate gradient method give operation counts 
note ective number variables 
bk procedure computes approximate solution 
conjugate gradient method 

computed operations 
operations 
rk min 
maxf ug operations 
bkp mt operations 
operations 
set compute operations operations operations operations go matrix vector multiplication step performed described 
total operation count conjugate gradient procedure approximately number conjugate gradient iterations 
comparing cost primal direct method direct method cient 
note costs methods increase number free variables larger 
limited memory matrix bk rank correction identity matrix termination properties conjugate gradient method guarantee subspace problem solved conjugate gradient iterations 
point conjugate gradient iteration boundary unconstrained solution subspace problem inside box 
consider example case unconstrained solution lies near corner starting point conjugate gradient iteration lies near corner edge box 
iterates soon fall outside feasible region 
example illustrates di culties conjugate gradient approach nearly degenerate problems 

dual method subspace minimization 
happens number active bounds small relative size problem cient handle bounds explicitly lagrange multipliers 
approach referred dual range space method see 
write xk restrict xk lie subspace free variables imposing condition xk bk recall ak matrix constraint gradients 
notation formulate subspace problem min gk dt subject bk xk rst solve problem bound constraint 
optimality conditions gk ak bk pre multiplying hk hk inverse bk obtain obtain bk columns ak unit vectors ak full column rank principal submatrix hk 
determines ak gk special case active constraints simply obtain gk ifthe vector xk violates bounds backtrack feasible region line joining infeasible point generalized cauchy point linear system solved sherman morrison woodbury formula 
inverse limited memory bfgs matrix recalling identity ak obtain mw omitted subscripts simplicity 
applying sherman morrison woodbury formula obtain aka mw ak gk set active variables determines matrix constraint gradients ak inverse limited memory bfgs matrix hk procedure implements dual approach just described 
recall denotes number free variables de ne ta ta denotes number active constraints operation counts include multiplications divisions denotes number corrections stored limited memory matrix 
dual method ta compute gk mw gk follows gk operations mw operations gk ww operations ta compute 
gk mw gk xc xk operations gk operations mw operations wv mta operations gk ta operations 
mw aka mw mta operations form ta operations operations mw operations ww mta operations ta operations 
hk ak gk ak gk mw ak ak mta operations mw operations ak gk ww operations backtrack necessary compute max li xc xk ui fg operations set xc xk 
operations vectors gk gk computed updating hk saved product gk requires computation 
total number operations procedure bounds active ta 
ta bounds active mta ta operations required compute unconstrained subspace solution 
comparison cost primal computation implemented described indicates dual method expensive number bound variables 
comparison take account devices saving costs computation inner products discussed 
fact primal dual approaches brought closer noting matrix mw zz appearing shown identical matrix mw aka 
second expression primal approach cost primal computation ta mt making competitive dual approach 
expression primal method computational experiments 
dual approach primal reuse inner products relevant forthe iteration 

numerical experiments tested limited memory algorithm options subspace minimization direct primal primal conjugate gradient dual methods compared results obtained subroutine lancelot 
code lancelot terminated kp xk gk note xk gk xk projected gradient 
algorithm follows 
bfgs algorithm choose starting point integer determines number limited memory corrections stored 
de ne initial limited memory matrix identity set 
convergence test satis ed 

compute cauchy algorithm cp 

compute search direction dk direct primal method conjugate gradient method dual method 

perform line search dk subject bounds problem compute set xk xk 
line search starts unit satis es attempts satisfy 
compute rf xk 

yk satis es eps add sk yk sk yk 
updates stored delete oldest column sk yk 

update sk yk lk rk set yk sk 
set go 
line search performed means routine attempts enforce wolfe conditions sequence polynomial interpolations 
greater routine generating infeasible points de ning maximum routine step closest bound current search direction 
approach implies objective function bounded line search generate point xk satis es satis es hits bound active 
wehave observed line search performs practice 
rst implementation algorithm backtracking line search approach drawback 
problems backtracking line search generated updating condition hold bfgs update skipped 
resulted poor performance problems 
contrast new line search update rarely skipped tests performance code markedly better 
explore compared early version code backtracking line search bfgs code unconstrained optimization uses routine unconstrained problems bfgs superior signi cant number problems 
results suggest backtracking line searches signi cantly degrade performance bfgs satisfying wolfe conditions possible important practice 
code written double precision fortran 
details limited memory matrices step see 
testing routine lancelot tried options bfgs sr exact hessians 
cases default settings lancelot 
test problems selected cute collection contains problems collection 
bound constrained problems cute tested problems discarded reasons number variables various algorithms converged di erent solution point instances essentially problem cute case selected representative sample 
results numerical tests tables 
computations performed sun sparcstation mhz cpu mb memory 
record number variables number function evaluations total run time 
limited memory code computes function gradient denotes total number function gradient evaluations 
lancelot number function evaluations necessarily number gradient evaluations tables record number nf function evaluations 
notation indicates solution obtained function evaluations indicates run terminated iterate generated line search close current iterate recognized distinct 
occurred cases limited memory method quite near solution unable meet stopping criterion 
table denotes number active bounds solution number zero mean bounds encountered solution process 
bfgs bfgs lancelot lancelot problem bfgs sr time time nf time nf time biggs hs hs hs palmer palmer palmer palmer torsion torsion torsion torsion torsion table 
test results new limited memory method bfgs primal method subspace minimization results lancelot bfgs sr options 
bfgs bfgs bfgs bfgs problem time time time time biggs hs hs hs palmer palmer palmer palmer torsion torsion torsion torsion torsion table 
results new limited memory method primal method subspace minimization various values memory parameter bfgs bfgs bfgs lancelot problem primal dual cg hessian time time time nf time biggs hs hs hs palmer palmer palmer palmer torsion torsion torsion torsion torsion table 
results new limited memory method methods subspace minimization primal dual cg results lancelot exact hessian 
di erences number function evaluations required direct primal dual methods due rounding errors relatively small 
computing time quite similar due form described 
computational experience suggests conjugate gradient method subspace minimization ective approach tends take time function evaluations 
table appears indicate cg option results fewer failures tests di erent values resulted failures cg method primal method 
limited memory method unable locate solution accurately result excessive number function evaluations failure progress 
reasons clear investigated 
tests described intended establish superiority lancelot new limited memory algorithm methods designed solving di erent types problems 
lancelot tailored sparse partially separable problems limited memory method suited unstructured dense problems 
lancelot simply benchmark reason ran default settings experiments various options nd give best results problems 
observations methods 
results table indicate limited memory method usually requires function evaluations sr option lancelot 
bfgs option lancelot clearly inferior sr option 
terms computing time limited memory method cient seen examining problems variables ones times meaningful 
surprise observe exact hessians lancelot requires time limited memory method large problems 
spite fact objective function problems signi cant degree kind partial separability lancelot designed exploit 
hand lancelot exact hessians requires smaller number function evaluations limited memory method 
new algorithm bfgs ciency unconstrained limited memory algorithm bfgs capability handling bounds cost signi cantly complex code 
unconstrained method bound limited memory algorithm main advantages low computational cost iteration modest storage requirements ability solve problems hessian matrices large unstructured dense unavailable 
competitive terms function evaluations exact hessian available signi cant advantage taken structure 
study appears indicate terms computing time new limited memory code competitive versions lancelot problems 
code implementing new algorithm described obtained authors nocedal eecs nwu edu 

aho hopcroft ullman design analysis computer algorithms 
reading mass addison wesley pub 

user guide test problem collection argonne national laboratory mathematics computer science division report anl mcs tm argonne il 
bertsekas projected newton methods optimization problems simple constraints siam control optimization 
conn gould ph toint 
cute constrained unconstrained testing environment research report ibm watson research center yorktown usa 
burke andj identi cation active constraints siam numer 
anal 
vol 

byrd nocedal schnabel representation quasi newton matrices limited memory methods mathematical programming pp 
projected gradient methods linearly constrained problems mathematical programming conn gould ph 
toint testing class methods solving minimization problems simple bounds variables mathematics computation 
vol 

conn gould ph 
toint global convergence class trust region algorithms optimization simple bounds siam numer 
anal vol 

conn gould ph toint 
lancelot package large scale nonlinear optimization release number springer series computational mathematics springer verlag new york 
conn private communication 
dennis jr schnabel numerical methods unconstrained optimization nonlinear equations englewood cli prentice hall 
subroutine library release 
advanced computing department aea industrial technology laboratory united kingdom 
gilbert numerical experiments variable storage quasi newton algorithms mathematical programming 
gill murray wright practical optimization london academic press 
goldstein convex programming hilbert space bull 
amer 
math 
soc 

constrained minimization problems ussr comput 
math 
math 
phys 

liu nocedal limited memory bfgs method large scale optimization methods mathematical programming 
line search algorithms guaranteed su cient decrease mathematics computer science division preprint mcs argonne national laboratory argonne il 
algorithms bound constrained quadratic programming problems numer 
math 

nocedal updating quasi newton matrices limited storage mathematics computation 
ortega iterative solution nonlinear equations variables academic press zhu byrd lu nocedal fortran subroutines large scale bound constrained optimization report nam eecs department northwestern university 

