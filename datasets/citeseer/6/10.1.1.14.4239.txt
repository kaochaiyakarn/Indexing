ubicrawler scalable fully distributed web crawler paolo boldi bruno massimo santini sebastiano vigna th january report experience implementing ubicrawler scalable distributed web crawler java programming language 
main features crawler platform independence fault tolerance effective assignment function partitioning domain crawl general complete decentralization task 
necessity handling large sets data highlighted limitation java apis prompted authors partially reimplement 
design implementation ubicrawler scalable faulttolerant fully distributed web crawler evaluate performance priori posteriori 
structure ubicrawler design described :10.1.1.162.194:10.1.1.14.4239
interest distributed web crawlers lies possibility gathering large data set study structure web 
goes statistical analysis specific web domains estimates distribution classical parameters page rank 
provided main tools redesign largest italian search engine 
stages project realized centralized crawlers longer sufficient crawl meaningful portions web 
recognized size web grows imperative parallelize crawling process order finish downloading pages reasonable amount time :10.1.1.162.194
commercial research institution run web crawlers gather data web 
code available cases basic design public case instance mercator altavista crawler original google crawler research crawlers developed academic community :10.1.1.109.4049
little published investigates fundamental issues underlying parallelization different tasks involved crawling process 
time name crawler changed ubicrawler authors learned existence italian search engine named 
particular approaches aware employ kind centralized manager decides urls visited stores urls crawled 
best components replicated statically 
contrast designing ubicrawler decided decentralize task obvious advantages terms scalability fault tolerance 
essential features ubicrawler platform independence full distribution task single point failure centralized coordination tolerance failures permanent transient failures dealt gracefully scalability 
outlined section features offspring defined design goal fault tolerance full distribution lack centralized control assumptions guided architectural choices 
instance reasonable ways partition domain crawled assume presence central server harder find assignment urls different agents fully distributed require coordination allows cope failures 
design assumptions requirements goals section give brief presentation important design choices guided implementation ubicrawler 
precisely sketch general design goals requirements assumptions type faults tolerated 
full distribution 
order achieve significant advantages terms programming deployment debugging parallel distributed crawler composed identically programmed agents distinguished unique identifier 
fundamental consequence task performed fully distributed fashion central coordinator exist 
want rely assumption concerning location agents implies latency issue minimize communication reduce 
balanced locally computable assignment 
distribution urls agents important issue crucially related efficiency distributed crawling process 
identify goals time url assigned specific agent solely responsible 
url knowledge responsible agent locally available 
words agent capability compute identifier agent responsible url communicating 
distribution urls balanced agent responsible approximately number urls 
scalability 
number pages crawled second agent independent number agents 
words expect throughput grow linearly number agents 

parallel crawler try fetch page time host 
fault tolerance 
distributed crawler continue crash faults agents abruptly die 
behaviour assumed presence kind crash faulty agent stops communicating particular prescribe action crashing agent recover state agent crashes remaining agents continue satisfy balanced locally computable assignment requirement means particular urls crashed agent redistributed 
important consequences possible assume urls statically distributed 
balanced locally computable assignment requirement satisfied time reasonable rely distributed reassignment protocol crash 
protocol requirement violated 
software architecture ubicrawler composed agents autonomously coordinate behaviour way scans share web 
agent performs task running threads dedicated visit single host 
precisely thread scans single host breadth visit 
sure different threads visit different hosts time host overloaded requests 
outlinks local host dispatched right agent puts queue pages visited 
visit web breadth soon new host met note radically different milder assumptions instance saying state faulty agent recovered 
case try crawler global state analyzing state crashed agent 
entirely visited possibly bounds depth reached number pages breadth fashion 
sophisticated approaches take account suitable priorities related urls instance rank easily implemented 
worth noting authors see argued breadth visits tends find high quality pages early crawl 
deeper discussion page quality section 
important advantage host breadth visits dns requests infrequent 
web crawlers global breadth strategy high latency dns servers usually obtained buffering requests multithreaded cache 
similarly caching needed robots txt file required robot exclusion standard file downloaded time host breadth visit begins 
assignment hosts agents takes account mass storage resources bandwidth available agent 
currently done means single indicator called capacity acts weight assignment function distribute hosts 
certain circumstances agent gets fraction hosts proportional capacity ca see section precise description works 
note number urls host varies wildly distribution urls agents tends large crawls 
empirical statistical reasons motivations usage policies bounding maximum number pages crawled host maximum depth visit 
policies necessary avoid possibly malicious web traps 
essential component ubicrawler reliable failure detector uses timeouts detect crashed agents reliability refers fact crashed agent eventually distrusted active agent property usually referred strong completeness theory failure detectors 
failure detector synchronous component ubicrawler component timings functioning components interact completely asynchronous way 
assignment function section describe assignment function ubicrawler explain possible decentralize task achieve fault tolerance goals 
set agent identifiers potential agent names set alive agents assign hosts agents precisely set function nonempty set alive agents host delegates responsibility fetching agent properties desirable assignment function 
balancing 
agent get approximately number hosts words total number hosts want 
contravariance 
set hosts assigned agent change contravariant manner respect set alive agents deactivation reactivation 
precisely say number agents grows portion web crawled agent shrink 
contravariance fundamental consequence new set agents added old agent lose assignment favour old agent precisely guarantees time set agents enlarged minimal interference current host assignment 
note satisfying partially requirement difficult instance typical approach non fault tolerant distributed crawlers compute hash function host name 
balancing properties agent gets approximately number hosts certainly computed locally agent knowing just set alive agents 
happens agent crashes 
assignment function computed giving different result hosts 
size sets hosts assigned agent grow shrink contravariantly content sets change completely chaotic way 
consequence crash pages stored agent fetched mistakenly re fetched times clearly central coordinator available agents engage kind resynchronization phase gather information mechanisms redistribute hosts crawl 
just shifted fault tolerance problem resynchronization phase faults fatal 
background completely obvious difficult show contravariance implies possible host induces total order permutation precisely contravariant assignment equivalent function assigns element symmetric group set permutations elements equivalently set total orderings elements host computed permutation associated agent belongs set simple technique obtain balanced contravariant assignment function consists trying generate permutations instance bits extracted host name seed pseudo random generator permuting randomly set possible agents 
solution big disadvantage running time space proportional set possible agents wants keep large feasible 
need sophisticated approach 
reason modulo hash function difficult increase number agents crawl 
consistent hashing new type hashing called consistent hashing proposed implementation system distributed web caches different approach problem 
idea consistent hashing simple profound 
noted typical hash function adding bucket new place hash table catastrophic event 
consistent hashing bucket replicated fixed number times copy shall call replica mapped randomly unit circle 
want hash key compute way key point unit circle find nearest replica corresponding bucket hash 
reader referred detailed report powerful features consistent hashing particular give balancing free 
contravariance easily verified 
case buckets agents keys hosts 
careful want contravariance hold mapping randomly replicas unit circle time agent started depend choice replicas 
agents compute set replicas corresponding agent host turned point unit circle agents agree responsible host 
identifier seeded consistent hashing method fix set replicas associated agent try maintain randomness properties consistent hashing derive set replicas random number generator seeded agent identifier call approach identifier seeded consistent hashing 
opted mersenne twister fast random generator extremely long cycle passes strong statistical tests 
solution imposes constraints replicas overlap discretization unit circle incur birthday paradox large number points probability replicas overlap nonnegligible 
new agent started identifier generate replicas agent 
process generate replica assigned agent force new agent choose identifier 
solution source problems agent goes discovers conflict restarted 
standard probability arguments show bit representation elements unit circle room agents conflict probability theoretical analysis balancing produced identifier seeded consistent hashing difficult impossible course uses working assumption replicas behave randomly distributed 
report experimental data see substantial number hosts crawled deviation perfect balancing small large sets agents replicas bucket thin maximum relative error number urls replicas replicas log number agents experimental data identifier seeded consistent hashing 
deviation perfect balancing replicas thin lines replicas thick lines 
lines deviation decreases thick lines 
implemented consistent hashing follows unit interval mapped set representable integers replicas kept balanced tree keys integers 
allows hash host logarithmic time number alive agents 
keeping leaves tree doubly linked chain easily implement search nearest replica 
mentioned important feature ubicrawler run heterogeneous hardware different amount available space 
purpose number replicas generated agent multiplied capacity guarantees assignment function distributes hosts evenly respect mass storage available agent 
number threads execution agent tuned suit network bandwidth cpu limitations 
note excessive number threads lead contention shared data structures store excessive cpu load corresponding performance degradation 
implementation issues mentioned key ideas web crawling public seminal papers 
ubicrawler builds knowledge uses ideas previous crawlers rabin fingerprinting 
decided write ubicrawler pure java application 
choice java tm implementation language mainly motivated need achieve platform independence necessity especially urgent fully distributed application 
currently ubicrawler consists java classes interfaces organized fifteen packages methods lines code 
course java imposes certain system overhead respect implementation 
tests show speed ubicrawler limited network bandwidth cpu power 
fact performance penalty java smaller usually believed instance implementors cern colt package claim linear performance penalty hand crafted assembler 
realistic user perception intrinsic java slowness mainly due bad performance swing window toolkit 
java possible adopt remote method invocation technology enables create distributed applications methods remote java objects invoked java virtual machines possibly different hosts object serialization implicitly marshal parameters 
freed need implementing communication agents 
components agent interact semi independent modules running possibly thread 
bound amount information exchanged network agent confined live single machine 
different agents may typically run different machines interact rmi 
intensive java apis highly distributed time space critical project highlighted limitations issues led devise new ad hoc solutions turned interesting se 
space time efficient type specific collections 
hierarchies java util package basic tool useful code development 
unfortunately awkward management primitive types stored collection need wrapped suitable objects hierarchies suitable handling primitive types situation happens practice 
need set integers wrap single integer object 
apart space inefficiency object creation highly time consuming task creation small objects garbage collection problematic fact dramatic distributed setting responsiveness critical 
issues derive way collections maps implemented standard api 
example realized closed entry table additional caches hash codes entry pair int minimally require bytes requires allocation objects integer objects wrapping integers entry object entry contains key value field additional integer field keep hash code cached 
implemented single 
ubicrawler agent keeps track urls visited obtained hash table stores bit fingerprints urls 
table turns responsible memory occupancy 
storing table standard apis reduce factor number urls worse number objects memory generate garbage collections long generate timeouts inter process communications 
considerations led development alternative sets maps defined util 
package contains classes offer type specific mapping int 
implement standard java interfaces offer polymorphism methods easier access reduced object creation 
algorithmic techniques implementation different standard api open addressing threaded balanced trees bidirectional iterators provided kind performance controlled object creations needed 
classes released gnu lesser general public license 
robust fast error tolerant html parsing 
crawling thread fetching page needs parse storing parsing required extract hyperlinks necessary crawling proceed obtain relevant information charset page case differs specified headers set words contained page information needed indexing wants analysis line parsing step 
current version ubicrawler uses highly optimized html xhtml parser able common errors 
standard pc performance page includes url parsing word occurrence extraction 
string stringbuffer 
java string classes known cause inefficiency 
particular stringbuffer synchronized implies huge performance hit multithreaded application 
worse stringbuffer equality defined buffers content equal trivial task extracting word occurrences store data structure poses nontrivial problems 
rewrote string class lying halfway 
experience reported authors mercator 
performance evaluation goal section discuss ubicrawler framework classification analyze scalability fault tolerance features :10.1.1.162.194
particular consider important features identified degree distribution coordination partitioning techniques coverage overlap communication overhead contrast ubicrawler :10.1.1.162.194
degree distribution 
parallel crawler intra site distributed agents communicate lan wan 
ubicrawler distributed crawler run kind network 
coordination 
classification agents different amount coordination extreme agents crawl network independently hopes overlap small due careful choice starting urls extreme central coordinator divides network statically agents start dynamically crawl :10.1.1.162.194:10.1.1.162.194
ubicrawler assignment function gives rise kind coordination fit models options suggested 
coordination dynamic central authority handles 
sense agents run independently time tightly coordinated 
call feature distributed dynamic coordination 
partitioning techniques 
web partitioned ways particular partition obtained url hash host hash hierarchically instance internet domains 
currently ubicrawler uses host hash note consider consistent hashing arguments shortcomings hashing functions longer true ubicrawler :10.1.1.162.194
coverage 
defined number crawled pages number pages crawler visit 
faults occur ubicrawler achieves coverage optimal 
principle possible urls stored locally crashed agent crawled 
urls reached paths crash clearly fetched new agent responsible 
overlap 
defined total number crawled pages number unique pages page erroneously fetched times 
presence faults ubicrawler achieves overlap optimal 
agent crashes restarted part state recovered crawled pages guarantee absence duplications overlap greater zero 
note event ubicrawler autonomously tries converge state overlap see section 
property usually known self stabilization technique protocol design introduced dijkstra 
communication overhead 
defined number urls exchanged agents crawl number crawled pages 
assuming reported average page contains just link site crawled pages give rise urls potentially communicated agents due balancing property note principle urls necessarily communicated agents just rely choice seed guarantee pages lost :10.1.1.162.194
worst case scenario obtain coverage urls crawled communicated agent 
assignment function ca ca messages sent network ranges set alive agents agent fetched page recall ca capacity agent 
definition communication overhead fact confirmed experimental analysis :10.1.1.162.194:10.1.1.162.194
interesting feature number messages independent number agents depends number crawled pages 
words large number agents generate network traffic due fact fetching pages design bottleneck 
quality 
complex measure importance relevance crawled pages ranking techniques important challenge build crawler tends collect high quality pages 
mentioned currently ubicrawler uses parallel host breadthfirst visit dealing ranking quality page issues 
immediate goal focus scalability crawler analysis portions web opposed building search engine 
breadth single process visit tends visit high quality pages natural ask strategy works 
rough reasonable quality measure number incoming links computed average quality visited pages changes time 
shows example average indegree crawl domain eu int surprisingly ubicrawler performance strategy far plain breadth visit see section 
fault tolerance best knowledge commonly accepted metrics exist estimating fault tolerance distributed crawlers issue faults taken serious consideration 
interesting open problem define set measures test robustness parallel crawlers presence faults 
give overview reaction ubicrawler agents faults 
ubicrawler agents die unreachable instance maintenance unexpectedly instance network problem 
time agent view agents alive reachable views necessarily coincide 
agent dies abruptly failure detector discovers bad happened timeouts 
properties assignment function fact different agents different views set alive agents disturb crawling process 
suppose instance knows dead 
contravariance difference course possible order pages ranking function instance backlink information stage project 
quality crawled pages average indegree function number crawled urls 
assignments host agents set hosts pertaining agent correctly dispatches hosts agents agent soon realizes dead happen worst case tries dispatch url point believed dead host dispatched correctly 
dispatch hosts different agents 
consequence design choice agents dynamically added crawl pages responsible removed stores agents fetched new agent birth 
words making ubicrawler self stabilizing design gives fault tolerance greater adaptivity dynamical configuration changes 
page recovery interesting feature contravariant assignment functions allow guess easily fetched previously page agent responsible configuration 
responsible host agent responsible started associated nearest replica 
allows implement page recovery protocol simple way 
certain conditions protocol allows avoid re fetching times page presence faults 
system parametrized integer time agent going fetch page host currently responsible checks agents fetched page 
difficult prove guarantees page recovery long agents started page crawled 
note number agents crashed completely irrelevant 
approach implies want accept possibly transient faults generating overlap increase linear factor network traffic fetched page generate communications 
unreasonable distributed system typical number rounds linearly related maximum number faults required solve instance consensus 
scalability highly scalable system guarantee performed thread constant number threads changes system communication overhead reduce performance thread 
figures section show amount network communication grows linearly number downloaded pages fact implies performance crawler thread essentially independent number agents 
measured average number pages stored second thread changes number agents number threads agent changes 
plots resulting data african domain 
graph shows thread changes number agents increases 
perfectly scalable system lines horizontal mean increasing number agents arbitrarily accelerate crawling process 
slight drop second part graph significant threads 
drop case artifact test caused current limitations terms hardware resources run experiments agents start agents machine existence active processes raised cpu load led hard disk thrashing 
decided include anyway data show constant smaller number threads agents 
graph shows thread changes number threads agent increases 
case data contention cpu load disk thrashing serious issues performed single thread reduces 
drop strongly dependent hardware architecture reader take grain salt lower lines manifest artifact seen graph 
just graphs actual figures note system sixteen agents running threads fetch pages day expect figures scale linearly number agents sufficient network bandwidth available 
tests run network simulation essentially provided infinite bandwidth 
network latency involved number threads raised higher figures thread contention greatly reduced replaced waiting network provide data 
real crawls threads ubicrawler download pages day ghz pcs point link saturated try increase parallelism 
includes precomputation list word occurrences page 
average number pages crawled second thread thread 
graph shows changes number agents changes different patterns represent number threads solid line long dashes short dashes dots 
graph shows changes number threads changes different lines represent different number agents higher lower 
related works mentioned details design implementation issues commercial crawlers public highly scalable crawling systems structure described discussed authors distributed crawlers compared ubicrawler mercator altavista spider discussed spider discussed 
mercator high performance web crawler components loosely coupled distributed computing units 
central element frontier keeps track urls crawled filters new requests 
original description mercator component unique centralised 
authors added possibility structuring crawler hive hosts statically partitioned finite number independent crawling analysis components 
address main problem information set urls crawled centralised frontier component 
mercator uses ingenious mix rabin fingerprinting compressed hash tables access sets efficiently 
contrary ubicrawler spreads evenly agents information 
hand mercator complete content handling providing protocol modules ftp importantly content seen module filters urls content urls crawled noted authors explain implement content seen module 
spider discussed developed python various components interact socket connections small message exchanges nfs network file system large messages 
downloading components communicate central components called crawl manager crawl application 
crawl application responsible parsing downloaded pages compressing storing application charge deciding visit policy 
crawl application communicates url crawled manager dispatches urls downloaders manager takes care issues robot exclusion speed rate control dns resolution described architecture scaled arbitrary number downloaders presence centralized parser dispatcher bottleneck 
authors solve problem partitioning set urls statically classes crawl applications responsible urls classes technique adopted similar internet archive crawler 
downloaders communicate page application responsible url 
number crawl manager reduced connecting applications manager 
worth mentioning assignment urls applications fixed statically number structure crawl applications changed runtime may change set downloaders 
set visited urls maintained crawl application kept partly main memory balanced tree partly disk 
polite crawling implemented domain throttling technique urls random order course need technique thread allowed issue requests host currently visited thread 
notable exception previous cases described authors propose solutions completely dynamic distribution urls means url distribution process urls mapped large array containing agent identifiers agent obtained array url 
entries array act replicas consistent hashing 
major drawbacks authors explain manage births deaths agent 
technique array renumbering guaranteed give balanced assignment renumbering guarantee agent dies short time gets alive get url assignment contravariance main features consistent hashing 
ubicrawler fully distributed scalable fault tolerant web crawler 
believe ubicrawler introduces new ideas parallel crawling particular consistent hashing mean completely decentralize coordination logic graceful degradation presence faults linear scalability 
development ubicrawler highlighted weaknesses java api able solve better algorithms 
plan continue development deployment ubicrawler test performance large web domains 
arvind arasu junghoo cho hector garcia molina andreas paepcke sriram raghavan 
searching web 
acm transactions internet technology 
paolo boldi bruno massimo santini sebastiano vigna 
highly scalable distributed web crawler 
poster proc 
tenth international world wide web conference pages hong kong china 
winner best poster award 
paolo boldi bruno massimo santini sebastiano vigna 
structural properties african web 
poster proc 
eleventh international world wide web conference honolulu usa 
paolo boldi bruno massimo santini sebastiano vigna :10.1.1.14.4239
ubicrawler scalable fully distributed web crawler 
proc 

eighth australian world wide web conference 
paolo boldi bruno massimo santini sebastiano vigna :10.1.1.162.194
ubicrawler scalability fault tolerance issues 
poster proc 
eleventh international world wide web conference honolulu usa 
sergey brin lawrence page :10.1.1.109.4049
anatomy large scale hypertextual web search engine 
computer networks 
burner 
crawling eternity building archive world wide web 
web techniques 
tushar deepak chandra sam toueg 
unreliable failure detectors reliable distributed systems 
journal acm 
junghoo cho hector garcia molina :10.1.1.162.194
parallel crawlers 
proc 
th international world wide web conference 
robert devine 
design implementation ddh distributed dynamic hashing algorithm 
david lomet editor proc 
foundations data organization algorithms th international conference fodo volume lecture notes computer science pages chicago illinois usa 
springer verlag 
edsger dijkstra 
self stabilizing systems spite distributed control 
communications acm 
wolfgang hoschek 
colt distribution 
tilde hoschek home cern ch hoschek colt 
david karger eric lehman tom leighton matthew levine daniel lewin rina panigrahy 
consistent hashing random trees distributed caching protocols relieving hot spots world wide web 
proc 
th annual acm symposium theory computing pages el paso texas 
david karger tom leighton danny lewin alex sherman 
web caching consistent hashing 
proc 
th international world wide web conference toronto canada 
martijn koster 
robot exclusion standard 
www org 
matsumoto nishimura 
mersenne twister dimensionally uniform pseudo random number generator 
acm transactions modeling computer simulation 
marc najork allan heydon 
high performance web crawling 
abello pardalos resende editors handbook massive data sets 
kluwer academic publishers 
marc najork janet wiener 
breadth search crawling yields highquality pages 
proc 
th international world wide web conference hong kong china 
lawrence page sergey brin rajeev motwani terry winograd 
pagerank citation ranking bringing order web 
technical report stanford digital library technologies project stanford university stanford ca usa 
java tm remote method invocation rmi java sun com products jdk rmi 
shkapenyuk torsten suel 
design implementation highperformance distributed web crawler 
ieee international conference data engineering icde 
yan wang xiaoming li lin guo 
architectural design evaluation efficient web crawling system 
journal systems software 

design implementation distributed crawler filtering processor 
proc 
volume lecture notes computer science pages 

