learning semantic similarity kandola john shawe taylor royal holloway university london cs rhul ac uk nello cristianini university california berkeley nello support vector net standard representation text documents bags words su ers known limitations due inability exploit semantic similarity terms 
attempts incorporate notion term similarity include latent semantic indexing semantic networks probabilistic methods 
propose methods inferring similarity corpus 
rst de nes word similarity document similarity viceversa giving rise system equations equilibrium point obtain semantic similarity measure 
second method models semantic relations means di usion process graph de ned lexicon occurrence information 
approaches produce valid kernel functions parametrised real number 
shows alignment measure successfully perform model selection parameter 
combined support vector machines obtain positive results 
kernel algorithms exploit information encoded inner products pairs data items see example 
matches naturally standard representation text retrieval known vector space model similarity documents inner product high dimensional vectors indexed terms corpus 
combination methods pioneered successively explored produces powerful methods text categorization :10.1.1.11.6124
approach su ers known limitations due inability exploit semantic similarity terms documents sharing terms di erent semantically related considered unrelated 
number attempts incorporate semantic knowledge vector space representation 
semantic networks considered whilst occurrence analysis semantic relation assumed terms occurrence patterns documents corpus correlated 
methods limited exibility question infer semantic relations terms documents corpus remains open issue 
propose methods model relations unsupervised way 
structure follows 
section provides semantic similarity introduced vector space model :10.1.1.23.6757
section derives parametrised class semantic proximity matrices recursive de nition similarity terms documents 
parametrised class kernels alternative similarity measures inspired considering di usion weighted graph documents section 
section show introduced alignment measure perform model selection classes kernels de ned :10.1.1.23.6757
positive experimental results methods reported section draw section 
representing semantic proximity kernel methods attractive choice inferring relations textual data enable document document setting term term :10.1.1.11.6124:10.1.1.23.6757
vector space model document represented vector indexed terms corpus 
vector typically sparse non zero entries terms occurring document 
documents semantically related distinct words show similarity 
aim semantic proximity matrix correct indicating strength relationship terms distinct semantically related 
semantic proximity matrix indexed pairs terms entry ab ba giving strength semantic similarity 
vectors corresponding documents inner product evaluated kernel pd denotes transpose vector matrix symmetry ensures kernel symmetric 
require positive semide nite order satisfy mercer conditions 
case decompose matrix view semantic similarity projection semantic space 
rd pd rd purpose infer re ne similarity measure examples account higher order correlations performing unsupervised learning proximity matrix corpus 
propose methods di erent observations 
rst method exploits fact standard representation text documents bags words gives rise interesting duality documents seen bags words simultaneously terms viewed bags documents documents contain 
model documents highly correlated term vectors considered having similar content 
similarly terms correlated document vector semantic relation 
course rst order approximation knock ect similarities needs considered 
show possible de ne term similarity document similarity vice versa obtain system equations solved order obtain semantic proximity matrix second method exploits representation lexicon set words corpus graph nodes indexed words cooccurrence establish links nodes 
representation studied giving rise number topological properties 
consider idea higher order correlations terms ect semantic relations di usion process graph 
exponentially paths connecting nodes graph di usion kernels enables obtain level semantic relation nodes eciently inferring semantic proximity matrix data 
equilibrium equations semantic similarity section consider rst methods outlined previous section 
aim create recursive equations relations documents terms 
feature example term document case text data matrix possibly kernel de ned feature space gives kernel matrix xx gives correlations di erent features training set 
denote matrix consider similarity matrices de ned recursively gx kx interpret augmenting similarity indirect similarities measured vice versa 
factor kkk ensures longer range ects decay exponentially 
rst result characterizes solution recurrences 
proposition provided kkk kgk kernels solve recurrences proof observe substitute second recurrence rst obtain kx xx showing expression satisfy recurrence :10.1.1.23.6757
clearly symmetry de nition expression satis es recurrence 
view form solution introduce de nition de nition von neumann kernel kernel derived kernel referred von neumann kernel :10.1.1.23.6757
note view kernel semantic proximity matrix px gx solution de nes re ned similarity terms features 
section consider second method introducing semantic similarity derived viewing terms documents vertices weighted graph 
semantic similarity di usion process graph structures data occur frequently diverse settings 
case language topological structure lexicon graph analyzed 
graph nodes indexed terms corpus edges occurrence terms documents corpus 
terms connected related meaning terms higher degree separation considered related 
di usion process graph considered model semantic relations existing indirectly connected terms 
number possible paths nodes grow exponentially results spectral graph theory show possible compute similarity nodes eciently examining possible paths 
possible show similarity measure obtained way valid kernel function 
exponentiation operation de nition naturally yields mercer conditions required valid kernel functions 
alternative insight semantic similarity section orded multiply expression entries matrix ij mg sum products weights paths length start vertex nish vertex weighted graph examples :10.1.1.23.6757
view connection strengths channel capacities entry ij seen measure sum routes products capacities 
entries satisfy positive vertex sum connections view entry probability random walk vertex vertex steps 
reasons kernels de ned combinations powers kernel matrix termed di usion kernels 
similar equation holds examples lie cluster similar examples strongly related similar features occur cluster related features drawn semantic proximity matrix stress emphasis di usion connections relation semantic proximity 
link motivates alternative decay factors considered 
kernel combines indirect link kernels exponentially decaying weight 
suggests alternative weighting scheme shows faster decay increasing path length 
exp proposition gives semantic proximity matrix corresponding 
proposition exp 
corresponds semantic proximity matrix exp 
proof singular value decomposition eigenvalue decomposition 
write exp exp exp exp required 
leads de nition second kernel consider 
de nition kernel derived kernels exp referred exponential kernels 
experimental methods previous sections introduced new kernel adaptations cases parameterized positive real parameter 
order apply kernels real text data need develop method choosing parameter 
course possibility just cross validation considered 
adopt expensive methodology quantitative measure agreement di usion kernels learning task known alignment measures degree agreement kernel target :10.1.1.23.6757
de nition alignment empirical alignment kernel kernel respect sample quantity hk hk hk kernel matrix sample kernel de nition inner products gram matrices hk corresponding frobenius inner product :10.1.1.23.6757
text categorization perspective viewed cosine angle bi dimensional vectors representing gram matrices :10.1.1.23.6757
consider yy vector outputs sample yy hk yy hk kif hyy yy ky alignment shown possess convenient properties :10.1.1.23.6757
notably eciently computed training kernel machine takes place training data information sharply concentrated expected value empirical value stable respect di erent splits data 
developed method choosing optimize alignment resulting matrix target labels training set 
method follows similar results parameterization non linear solve optimal value :10.1.1.23.6757
seek optimal value line search range possible values value derivative alignment respect zero 
propositions give equations satis ed point 
proposition solution argmax yy eigenvector eigenvalue pairs kernel matrix exp hv yi exp exp hv yi exp proof observe mv ii exp :10.1.1.23.6757
express alignment yy hv yi pp function di erentiable function maximal value derivative zero :10.1.1.23.6757
derivative expression setting equal zero gives condition proposition statement 
proposition solution argmax kkk yy eigenvector eigenvalue pairs kernel matrix hv yi hv yi proof proof identical proposition ii de nition line search optimization alignment take place line search values nd maximum point alignment seeking points equations proposition hold :10.1.1.23.6757
results demonstrate performance proposed algorithm text data medline dataset commonly text processing 
dataset contains documents queries obtained national library medicine 
focus query :10.1.1.23.6757
bag words kernel :10.1.1.11.6124
words punctuation removed documents porter stemmer applied words 
terms documents weighted variant tfidf scheme 
log tf log df tf represents term frequency df document frequency total number documents 
support vector classi er svc assess performance derived kernels medline dataset 
fold cross validation procedure nd optimal value capacity control parameter 
having selected optimal parameter svc re trained times random training test dataset splits 
error results di erent algorithms values 
measure popular statistic information retrieval community comparing performance train align svc error table medline dataset mean associated standard deviation alignment svc error values svc trained bag words kernel exponential kernel :10.1.1.23.6757
index represents percentage training points 
train align svc error table medline dataset mean associated standard deviation alignment svc error values svc trained bag words kernel von neumann :10.1.1.23.6757
index represents percentage training points 
algorithms typically uneven data 
computed pr represents precision measure proportion selected items system classi ed correctly represents recall proportion target items system selected :10.1.1.23.6757
applying line search procedure nd optimal value di usion kernels 
results averaged random splits standard deviation brackets 
table shows results bag words kernel matrix exponential kernel matrix 
table presents results von neumann kernel matrix bag words kernel matrix di erent sizes training data :10.1.1.23.6757
index represents percentage training points 
rst column table shows alignments gram matrices rank labels matrix di erent sizes training data :10.1.1.23.6757
cases results indicate alignment di usion kernels labels greater bag words kernel matrix sum standard deviations sizes training data 
second column tables represents support vector classi er svc error obtained di usion gram matrices bag words gram matrix 
svc error di usion kernels shows decrease increasing alignment value 
values shown instances show improvement di usion kernel matrices 
interesting observation regarding value von neumann kernel matrix trained training data :10.1.1.23.6757
despite increase alignment value reduction svc error value increase exponential kernel trained proportion data observation implies di usion kernel needs data ective :10.1.1.23.6757
investigated 
proposed compared di erent methods model notion semantic similarity documents implicitly de ning proximity matrix way exploits high order correlations terms 
methods di er way matrix constructed 
view propose recursive definition document similarity depends term similarity vice versa 
solving resulting system kernel equations ectively learn parameters model construct kernel function kernel learning methods 
approach model semantic relations di usion process graph nodes documents edges incorporate rst order similarity 
di usion eciently takes account possible paths connecting nodes propagates similarity remote documents share similar terms 
kernel resulting model known literature di usion kernel 
experimentally demonstrated validity approach text data novel approach set adjustable parameter kernels optimising alignment target training set 
dataset partitions substantial improvements performance traditional bag words kernel matrix obtained di usion kernels line search method 
despite success large imbalanced datasets encountered text classi cation tasks computational complexity constructing di usion kernels may prohibitive 
faster kernel construction methods investigated regime 
cristianini shawe taylor 
support vector machines 
cambridge university press cambridge uk :10.1.1.23.6757
nello cristianini john shawe taylor kandola :10.1.1.23.6757
kernel target alignment 
proceedings neural information processing systems nips :10.1.1.23.6757
nello cristianini john shawe taylor lodhi 
latent semantic kernels 
journal intelligent information systems :10.1.1.23.6757
ferrer sole 
small world human language 
proceedings royal society london series biological sciences pages :10.1.1.23.6757
thomas hofmann 
probabilistic latent semantic indexing 
research development information retrieval pages 
joachims :10.1.1.11.6124
text categorization support vector machines 
proceedings european conference machine learning ecml 
kondor la erty 
di usion kernels graphs discrete structures 
proceedings conference machine learning icml :10.1.1.23.6757
todd michael berry 
large scale information retrieval latent semantic indexing 
information sciences 
buc 
support vector machines semantic kernel text categorization 
ieee ijcnn :10.1.1.23.6757
