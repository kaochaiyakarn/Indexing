technical report idsia january optimality universal bayesian sequence prediction marcus hutter idsia ch lugano switzerland marcus idsia ch www idsia ch marcus keywords bayesian sequence prediction mixture distributions solomono induction kolmogorov complexity learning universal probability tight loss error bounds pareto optimality 
various optimality properties universal sequence prediction general solomono prediction scheme particular studied 
probability observing time past observations computed bayes rule true generating distribution sequences known 
unknown known belong class base ones prediction bayes mix de ned weighted sum distributions 
number additional prediction errors optimal universal prediction scheme minus number errors optimal informed prediction scheme bounded 
show bound tight predictor lead smaller bounds 
furthermore various performance measures show pareto optimality sense predictor performs better equal environments strictly better 
optimal predictors performance measures expectation mixture 
give occam razor argument solomono choice weights optimal length shortest program describing 
marcus hutter technical report universal prior inductive inference problem brought form string take guess continuation assume strings continued drawn probability distribution 
maximal prior information prediction algorithm possess exact knowledge cases true distribution known 
prediction guess 
expect predictor performs close converges sense 
nite countable set candidate probability distributions strings 
de ne weighted average 
easy see probability distribution weights positive normalized probabilities 
call universal relative multiplicatively dominates distributions 
assume known contains true distribution 
show predictors bayes mix general solomono prediction scheme particular optimal various criteria 
furthermore show error loss bounds derived hut hut hut tight 
contents apart introducing notation brie discuss section convergence representation universal posterior jx solomono choice case previous bounds total expected number errors universal predictor terms total expected number errors optimal informed scheme slightly improved section 
section show improved bounds optimal sense weights derived error bounds tight 
predictor lead better bounds predictor larger error smaller error 
section show predictor proving optimal pareto sense 
optimal predictors performance measures expectation mixture distributions 
leaves open choose weights 
section give occam razor argument solomono choice optimal length shortest program describing 
includes deterministic environments case probability distribution sequence 
call probability distributions kind deterministic 
weight may interpreted initial belief degree belief existence true randomness rejected philosophical grounds may consider containing deterministic environments 
represents belief probabilities 
universal prediction general loss alphabet new 
theorem lower error bounds theorems theorem optimal choice weights new 
error bound theorem slightly improves hut 
convergence theorem known result sol lv hut 
introductory reviews papers books solomono sequence prediction lv inductive inference sol general mdl reasoning uncertainty gr worst case wm approaches ces bayesian prediction approaches hb competitive online statistics contain 
setup convergence notation denote strings nite alphabet abbreviate xm greek letters probability distributions measures especially arbitrary ones true generating arbitrary ones mixture 
probability nite sequence starts conditional probability jx string continued obtained bayes rule 
prediction schemes posteriors 
abbreviate expectations jx expectations taken true distribution 
bayes rule 
abbreviate probability 
relative entropy euclidian distance measure instantaneous total distances ln jx jx ln jx jx theorem convergence sequences nite alphabet drawn probability rst symbols 
universal conditional probability jx symbol related true conditional probability jx way jx jx ln relative entropies weight 
marcus hutter technical report proof binary alphabet sol lv general nite alphabet hut 
niteness implies jx jx convergence motivates belief predictions known asymptotically predictions unknown rapid convergence 
universal posterior probability distribution prediction schemes conditional probabilities jx 
possible express conditional probability jx weighted average conditional jx similarly time dependent weights jx jx representation proven dividing applying bayes rule 
expressions give intuitive non rigorous argument jx converges jx weight increases decreases assigns high low probability new symbol random sequence signi cantly di ers 
expect total weight consistent converges weights converge 
expect jx converge jx random strings form sees learned 
case discuss cases parts apply 
theorems remain valid nite linear combination dominance 
ensured min min general nite linear combination dominance ensured dominate sense 
possibly interesting situation true generating distribution nearby distribution weight measure distance kullback leibler divergence jj ln assume bounded constant ln ln ln ln lnw remains valid de ne 
see gr detailed discussion case related context 
probability classes nite possible choice give equal weight jmj 
assume known contains true distribution serious constraint include computable probability distributions high weight assigned simple 
solomono universal prediction general loss alphabet universal semi measure obtained take multi set enumerated turing machine enumerates enumerable semi measures take weights length shortest program sol sol lv 
discussion various general purpose choices hut 
constructive point view set containing nitely asymptotically semi computable suciently rich 
section give occam razor argument choice optimal 
called universal element lv 
need property may nite countable set distributions 
consider generic continuous classes considered companion hut 
error loss bounds start simple performance measure making wrong prediction counts error making correct prediction counts error 
strategy predicts maximizes posterior probability jx minimizes expected number errors 
generally prediction scheme predicting argmax jx distribution 
probability making wrong prediction th symbol total expected number errors rst predictions predictor jx known obviously best prediction scheme sense making number expected errors prediction scheme 
special interest universal predictor error bound shown 
theorem error bound sequences nite alphabet drawn probability rst symbols 
system predicts de nition maximizes jx 
universal prediction scheme universal prior 
optimal informed prediction scheme 
total expected number prediction errors de ned bounded way lnw squared distance relative entropy weight 
marcus hutter technical report rst bound contains particularly useful major bound prove follow easily 
furthermore somewhat nicer symmetric structure second bound 
section show second bound optimal 
bound discuss asymptotics second bound 
observe number errors universal predictor nite number errors informed predictor nite 
complicated probabilistic environments ideal informed system nite number errors theorem ensures regret order lnw const 
theorem shows ratio converges gives speed convergence 
see hut detailed discussion 
bounds slight improvement bounds obtained hut 
proof somewhat nicer signi cance improvement stems fact new second bound tight improved general nite non asymptotically 
proof rst inequality theorem proven 
second inequality start modestly try nd constants satisfy linear inequality bs show bs follow immediately summation de nition abbreviations ng jx jx jx various error functions expressed inserting get de nition prove sequence inequalities show positive suitable proves 
obviously positive 
assume 
square keep contributions universal prediction general loss alphabet de nition constraints zm 
easy see square terms function minimized 
furthermore de ne ym replace 
bx quadratic minimized inserting gives ab inequality holds provided insert minimize leading upper bound rst bound theorem 
second bound prove square sides expressions simplify just get 
implies 
inequality theorem simple triangle inequality 
completes proof theorem ut note second bound implies rst bounds equal 
general loss function setup result generalized arbitrary loss functions 
ir received loss predicting actual outcome 
error function previous subsection special case assigns unit loss erroneous prediction loss correct prediction 
true probability symbol jx 
expected loss predicting goal minimize expected loss 
generally de ne prediction scheme arg min jx marcus hutter technical report minimizes expected loss 
true distribution actual expected loss predicts th symbol total expected loss rst predictions causal prediction scheme deterministic probabilistic matter constraint predicting losses similarly de ned 
known obviously best prediction scheme sense achieving minimal expected loss loss bound universal predictor proven companion hut 
loss bounds form error bounds substituting theorem 
show replaced gives invalid bound general bound slightly weaker 
example loss functions including absolute square logarithmic hellinger loss discussed hut 
instantaneous error loss bounds proven lower error bound want show exists class distributions predictor knowing distribution observed sequence sampled minimal additional number errors compared best informed predictor deterministic environments lower bound easily obtained combinatoric argument 
consider class containing binary sequences pre length occurs exactly 
assume deterministic predictor knowing sequence advance prediction times exists sequence opposite symbol log jmj lower worst case bound predictor includes course 
upper bound theorem obtained inserting sharp easily improved log jmj deterministic environments matching lower bound 
general probabilistic case show similar argument upper bound theorem sharp 
argmin 
de ned minimizes argument 
tie broken arbitrarily 
general prediction space allowed di er nite exists 
nite action space assume minimizing exists assumption may removed 
universal prediction general loss alphabet theorem lower error bound deterministic predictor knowing distribution observed sequence sampled 
knows depends time access previous outcomes weights total expected number errors de ned 
equalities especially hold universal predictor proof proof parallels generalizes deterministic case 
consider class distributions binary alphabet indexed want distribution posterior probability posterior probability independent past determined 

interested predictions time completeness may de ne assign probability informed scheme predicts bit highest probability seek maximize predictor 
assume predicts possibly depending history 
want lower bounds seek worst case 
success lowest possible probability 
need eliminate favor assume uniform weights get unbiased bernoulli sequence jx 
proves instantaneous regret formula inserting solving get nally get marcus hutter technical report proves total regret formula theorem 
ut 

error bound theorem replaced asymptotically tight implies 
shows restrictions loss function exclude error loss loss bounds asymptotically tight 
independent set leading tight lower bound jx minf lnw 
show lnn pareto optimality subsection want establish di erent kind optimality property 
performance measures relative considered previous sections 
easy nd tailored probably expense increasing 
know advance may ask exists better equal performance strictly better performance 
clearly render suboptimal show performance measures studied 
de nition pareto optimality performance measure relative 
universal prior called pareto optimal strict inequality 
theorem pareto optimality universal prior pareto optimal instantaneous total squared distances entropy distances errors losses 
proof rst proof theorem instantaneous expected loss need general expected instantaneous losses jx predictor 
want arrive contradiction assuming assuming existence predictor de nition look deterministic predictor exists universal prediction general loss alphabet strict inequality 
implicit assumption assumption exist 
exists jx exists 
equalities follow inserting 
strict inequality follows assumption 
inequality follows fact minimizes de nition expected loss similarly 
contradiction proves pareto optimality way prove pareto optimality total loss de ning expected total losses predictor assuming strict inequality get contradiction help 
instantaneous total expected errors considered special loss functions 
pareto optimality understood geometrical insight 
formal proof goes follows abbreviations jx jx jx temporarily ask vector 
implies 
implies proving unique pareto optimality similarly assumption ln ln implies ln ln ln ln implies proving unique pareto optimality proofs similar 
ut proven uniquely pareto optimal case actions predictions invoke unique ties argmax broken consistent way counts 
measures relevant decision theoretic point view loss functions welcomed property pareto optimal pareto optimal performance measures 
marcus hutter technical report theorem non pareto optimality pareto optimal norm jj 
jj positive linear combinations norms power pareto optimal esp jj 
jj general pareto optimal norm jj 
jj positive linear combinations jj 
jj 
positive linear combinations pareto optimal intuition problem gained considering probability vectors 
ir 
probability triangle wx mixture consider sets fr analogously empty contains interior pareto optimal 
visualize boundaries areas qualitatively various performance measures gives intuition prove pareto optimality construct counter examples 
proofs 
pareto optimality regarded necessary condition prediction scheme aiming optimal 
practical point view signi cant decrease may desirable causes small increase 
impossibility balanced improvement demanding condition pure pareto optimality 
theorem shows balanced pareto optimal 
consider performance measure suppress index convenience 
theorem balanced pareto optimality 

implies assume larger loss environments total weighted amount 

smaller loss mnl improvement bounded 


especially 
max 
means weighted loss decrease 
compensated large weighted increase 
environments 
increase small decrease small 
special case single environment increased loss 
decrease bound 

increase amount 
cause decrease amount times factor increase cause smaller decrease simpler environments scaled decrease complex environments 
note pure pareto optimality follows balanced pareto optimality special case increase 

proof 
follows 
linearity remainder theorem obvious 

bounding weighted average 
maximum ut universal prediction general loss alphabet optimal choice weights indicate dependency explicitly writing shown prediction schemes balanced pareto optimal prediction scheme bayes mix uniformly better 
assumptions environment large possible 
section discussed set enumerable regarded suciently large see sch larger sets computational realm 
agreeing leaves open question choose weights prior believes pareto optimal leads asymptotically optimal predictions 
derived bounds mean squared sum lnw loss excess lnw lnw bounds monotonically decrease increasing desirable assign high weights 
due semi probability constraint nd compromise 
argue class enumerable weight functions short program optimal compromise gives solomono prior 
consider class enumerable weight function fv 
ir 

corollary lv says log enumerable discrete 
identifying program index describing get ln ln means bounds depending lnw larger bounds depending lose additive constant order bounds solomono prior safe side getting best bounds environments 
theorem optimality universal weights set enumerable weight functions short program universal weights lead smallest performance bounds additive lnw constant enumerable environments 
justi es solomono prior solomono prior assigns high probability environment low kolmogorov complexity may results stated proven probability measures 
hand class considered class enumerable semi elements semi 
combinations sense 
convergence error bound theorem loss bound statements hold holds maximal semi probability fails semi probability 
marcus hutter technical report interpret result justi cation occam razor note bootstrap argument implicitly occam razor justify restriction enumerable 
considered weight functions low complexity 
enter assumption came result speci universal weights optimal 
want conclude chapter occam razor versus free 
regard balanced pareto optimality result free lunch nfl theorem wm 
environments completely random small concession loss completely uninteresting environments provides margin 
yield distinguished performance non random interesting environments 
interpret nfl theorems optimization search wm balanced pareto optimality results 
interestingly prediction bayes mixes pareto optimal search optimization algorithm pareto optimal 
strong religious war believers occam razor believers free go sto 
various optimality properties universal prediction bayes mixtures general solomono prediction scheme particular studied 
shown weights error loss bounds derived hut hut improved general making extra assumptions true independent predictor 
shown pareto optimality sense predictor performs better equal environments strictly better 
optimal predictors cases mixture distributions 
gave occam razor argument solomono choice weights length shortest program describing optimal 
course optimality depends setup assumptions chosen criteria 
instance universal predictor pareto optimal popular decision theoretic performance measures 
bayes predictors necessarily optimal worst case criteria cbl 
see hut discussions duality bayes worst case wm approaches results classi cation tasks games chances nite alphabet active systems uencing environment 
supported snf urgen schmidhuber 
direction shown easy direct argument sch 
universal prediction general loss alphabet angluin smith 
inductive inference theory methods 
acm computing surveys 
cbl cesa bianchi lugosi 
worst case bounds logarithmic loss predictors 
machine learning 
ces cesa bianchi expert advice 
journal acm 
gr gr 
minimum length principle reasoning uncertainty 
phd thesis universiteit van amsterdam 
hb haussler barron 
bayes methods line prediction values 
proc 
rd nec symposium computation cognition siam pages 
hut hutter 
convergence error bounds universal prediction general alphabet 
proceedings th conference machine learning ecml pages 
hut hutter 
general loss bounds universal sequence prediction 
proceedings th international conference machine learning icml pages 
hut hutter 
finite loss bounds universal bayesian sequence prediction 
submitted 
www idsia ch marcus ai ps 
lv li vit anyi 
kolmogorov complexity applications 
springer nd edition 
sch schmidhuber 
hierarchies generalized kolmogorov complexities universal measures computable limit 
international journal foundations computer science 
press 
sol solomono formal theory inductive inference part 
inform 
control 
sol solomono complexity induction systems comparisons convergence theorems 
ieee trans 
inform 
theory 
sol solomono discovery algorithmic probability 
journal computer system sciences 
sto stork 
foundations occam razor parsimony learning 
nips workshop 
www rii com stork html 
vovk 
competitive line statistics 
technical report docs university london 
wm david wolpert william macready 
free lunch theorems optimization 
ieee transactions evolutionary computation 
