fast robust logistic regression large sparse datasets binary outputs paul dept mathematical sciences carnegie mellon university pittsburgh pa andrew moore school computer science carnegie mellon university pittsburgh pa popular extremely established mainstream statistical data analysis logistic regression strangely absent field data mining 
possible explanations phenomenon 
assumption tool produce linear classification boundaries modern nonlinear tools 
second legitimate fear logistic regression practically scale massive dataset sizes modern data mining tools applied 
consists empirical examination assumption surveys implements compares techniques logistic regression scaled data millions attributes records 
results large life sciences dataset indicate logistic regression perform surprisingly statistically computationally compared array classification algorithms 
asks simple question wide variety classification algorithms support vector machines oldest logistic regression measure 
investigate logistic regression lr performance statistical method concern accurate modeling monomial pdf binary valued output conditioned set inputs 
investigate lr data mining method concern computational tractability large datasets millions records hundreds thousands attributes 
concludes circumstances lr equals exceeds predictive performance algorithms 
show application wellestablished sparse conjugate gradient approaches inner loop allows scaling lr sparse datasets aforementioned dimensions large dense datasets thousands dimensions 
terminology discussing datasets record belongs predicted belong positive negative class 
positive row row belonging predicted belong positive class 
similar definition holds negative row 
number rows dataset denoted number attributes sparsity factor dataset proportion nonzero entries written total number nonzero elements dataset product mrf logistic regression logistic regression gets name logistic curve uses model expected value zero outputs 
suppose data rows attributes 
denote row dataset denote model parameters model expected value output associated row logistic curve may written exp 
bm im exp 
bm im single variable appears 
notice strictly zero 
vector outputs 
maximum likelihood equations model exp exp logistic model common iteratively re weighted squares irls algorithm solve maximum likelihood equations 
algorithm outer loop evaluates current expected values prediction errors creates linear models approximate logistic model 
linear models diagonal weight matrix diagonal entries adjusted dependent variables previous guess parameters 
model new guess computed weighted linear regression shown equation 
wx 
wh new parameter vector update expected values prediction errors 
outer loop 
residual errors logistic regression model binomially distributed variance changing domain 
may seen 
suppose model assigns point domain probability belonging positive class 
observed class point positive negative yielding residual error respectively 
residual error depends outer loop algorithm described may terminated conditions depending numerical characteristics data personal preference 
common convergence criterion stabilization error measurement known deviance 
deviance dev may computed dev ln ln err mu err mu mu mu logistic model residuals vary may desirable check stabilization expected values datasets possible excessive iterations create numerical problems mcintosh mccullagh 
reason create customized termination criteria logistic regression outer loop 
important note difference logistic regression squares methods sigmoid logistic model 
logistic regression type generalized linear model error function chosen reflect changing variance binomially distributed residual errors 
applications squares similar criterion models ignore nonuniform error variance domain 
may may matter performance ad hoc function estimator neural networks watrous kramer 
approach statistical foundations give credibility logistic regression 
logistic regression studied supported rich body literature 
details wish read mccullagh 
theoretical numerical issues covered works 
fast robust logistic regression applying logistic regression large sparse realworld data encountered principal problems 
problem numerical instability second scalability speed 
simple approaches problems 
simple algorithm solving weighted linear model irls algorithm discussed section requires direct indirect evaluation wx wh 
weight matrix nonnegative diagonal full column rank wx symmetric positive definite spd 
suggests obvious method computing parameters called simple algorithm 
remove linear dependencies data 
rewrite equation wh 
outer loop iteration compute cholesky factorization ll wx 
solve efficient back substitution 
approach attractive stability cholesky decomposition wide variety matrices 
assumption structure sparsity data exploited prevent fill course algorithm gaussian elimination 
reason preprocessing require approximately min notation section 
suppose data linear dependencies dimensions data sparse complexity approach rf second term represents standard cholesky decomposition wx 
data dense complexity simple algorithm standard cholesky decomposition input data full column rank rf dense datasets 
simple optimizations improve speed complexity 
instance data sparse construction wx lr iteration efficient pre caching intersections columns data intersection cache require large amounts storage attribute dataset average common values columns intersection cache gigabyte 
cache effectively eliminates cost constructing wx 
typically iterations lr outer loop fold doing fold cross validation 
clear simple algorithm scale dataset size grows 
fully exploit sparseness cholesky decomposition 
limited fill cholesky decomposition require construction wx matrix reduce time complexity simple algorithm simultaneously making approximate algorithm 
experiments numerical issues wx matrix appear indefinite cholesky decomposition preventing solution equation 
robust cholesky algorithm possible combine linear dependency removal cholesky decomposition 
just cholesky decomposition test matrix spd small modification allows find spd submatrix symmetric input matrix 
suppose computing cholesky decomposition symmetric matrix produce lower triangular cholesky factor logistic regression wx 
diagonal entries cholesky factor may written closed form diagonal entries may written input matrix spd diagonal entries defined positive 
modification standard cholesky decomposition nearly simple discarding rows columns input matrix associated diagonal entry zero defined 
note discarding row column equivalent discarding attribute data suppose spd numerical problems cause diagonal entry nonpositive 
indicates numerator right hand side equation negative summation term numerator large 
element errant summation successfully previous rows values row 
recompute row row column exist 
newly computed diagonal entry positive mark row column input matrix bad continue 
newly computed diagonal entry negative assume row bad discard 
developed robust cholesky decomposition removing linear dependencies columns datasets prevent numerical problems standard cholesky decomposition wx 
observed robust cholesky decomposition removed nearly set attributes gaussian elimination resulting predictions better 
robust cholesky implementation inside lr outer loop original unprocessed data called robust cholesky algorithm 
time complexity method similar simple algorithm section 
difference need preprocessing 
cost removal linear dependencies shifted iteration lr robust cholesky decomposition run 
fact occurs lr iteration fold 
robust cholesky algorithm overcomes numerical issues fails exploit sparseness 
stepwise logistic regression approach handling problematic datasets stepwise logistic regression 
procedure wraps lr outer loop basis choosing loop 
basis choosing step creates subset data runs lr outer loop subset uses result choose new data subset repeats basis choosing loop met termination criterion 
algorithm called stepwise algorithm 
stepwise algorithm attractive preconditioning data needed numerical problems handled dynamically loop 
basis choosing step prior knowledge attributes generic measures information gain 
complexity algorithm depends step number basis choosing iterations 
basis chosen solution logistic regression problem basis 
believe attractiveness logistic regression large sparse datasets materially affected stepwise algorithm 
conjugate gradient inner loop nearly time spent lr iterations solving linear regression 
particular time spent constructing wx finding cholesky factors 
conjugate gradient cg iterative linear solver equation avoid construction wx altogether 
algorithm technique called cg algorithm 
thorough explanation conjugate gradient related methods greenbaum nash bishop 
briefly solving linear system ax spd conjugate gradient similar minimizing quadratic expression ax newton method specially chosen set direction vectors 
cg thought specialization newton method quadratic functions spd matrices ignores features important application 
features conjugate gradient require explicit computation hessian expression newton method cg perform matrix rank deficient spd typically cg iterations necessary 
perfect arithmetic exact solution linear system spd matrix having dimensions exactly iterations 
practice termination parameter control quality approximation 
refer parameter epsilon 
cg algorithm runs error measurement smaller epsilon 
error measurement distance current solution optimal solution assumption finding minima expression 
error measurement appropriate equivalent problem solving linear systems spd matrix newton method cg studied 
modified cg method known biconjugate gradient method designed matrices expression spd 
reduce number iterations needed convergence cg method speed iteration preconditioning matrix applied sides linear system ax significant literature choosing preconditioners cg method name preconditioned conjugate gradient 
application iterative methods linear systems greenbaum 
significant success basic cg algorithm hope improved speed predictive performance greater sophistication inner loop 
time complexity cg algorithm mrf 
upperbound placed number cg iterations approximate solution linear system 
experiments fewer cg iterations 
fix independent number attributes cg algorithm scales linearly dimensions data 
tremendous improvement robust algorithm rf complexity 
data report results large significant datasets life sciences 
dataset referred ds ds 
ds approximately binary valued attributes rows ds approximately attributes rows 
datasets sparse 
time submitting permission disclose details datasets 
qualitatively may think datasets containing information similar combination publicly available open compound database associated biological test data provided national cancer institute 
results additional datasets derived ds ds 
datasets ds pca ds pca linear projections ds dimensions principle component analysis pca 
expense pca projection ds dimensions uses approximate linear projection algorithm known anchors moore 
resulting dataset called ds anchors 
note pca anchors reducing number attributes number rows remains 
datasets allow comparison predictive performance original large sparse datasets smaller dense datasets 
datasets allow comparison lr computationally expensive algorithms neighbor nn reasonably run ds ds 
dataset ds sparsity factor ds sparsity factor 
pca datasets dense imagined sparsity factor 
assume sparsity input data exploitable structure 
analysis run experiments algorithms mentioned section 
simple algorithm numerical problems produced results 
stepwise algorithm early experiments ds 
basis choosing step implementation added attribute time basis slow accumulate attributes 
robust cholesky algorithm hand able run approximately attributes ds faster stepwise algorithm accumulate 
reasons section focus robust cholesky algorithm abbreviated robust lr cg algorithm abbreviated cg lr 
evaluate compare predictive performance lr implementations traditional machine learning algorithms fold cross validation 
visualize predictive performance receiver operating characteristic roc curves duda 
construct curves sort dataset rows probability row positive class logistic model 
starting graph origin step rows order decreasing probability moving unit row positive right unit 
point roc curve represents learner favorite rows dataset 
favorite rows positive negative 
suppose dataset positive rows negative rows 
perfect learner dataset roc curve starting origin moving straight straight right 
random guessing produce average roc curve started origin moved directly termination point 
note roc curves start origin steps right taken row 
crude summary roc curve measure area curve relative area perfect learner curve 
result denoted auc 
perfect learner auc random guessing produces auc 
order compute table computer specifications name cpu ram liver alpha ev mhz gb mb cache athlon xp mb confidence intervals compute auc score fold fold cross validation 
report mean scores confidence interval 
comparing learning algorithms set fold partitions 
allows pairwise test determine algorithm significantly better worse auc measure 
results include machine learning algorithms 
algorithms comparison due computational complexity 
cg lr bayes classifier bc decision tree learner dtree reasonably run largest datasets ds 
tree utilizes chi squared pruning threshold 
implementation details tree projected datasets 
special effort required nearest neighbor nn fast ds pca ds anchors 
support vector machine results linear linear svm radial basis function rbf svm kernel implemented svm light joachims 
data input format svm light sparse format believe sparse instance vectors exploited 
results obtained different machines 
experiments ds ds pca ds pca performed somewhat typical modern desktop 
experiments ds ds anchors liver old alpha server 
believe experiment allocated gigabytes memory 
specifications machines table 
analysis begins comparing learners ds dataset 
roc curves bc tree robust lr cg lr linear svm rbf svm seen crowded 
summarizing graph table 
see lr methods predictive performance competitive learning algorithms 
rbf svm performed par cg lr cg lr fifteen times faster 
curious cg lr performed better robust lr 
experiment hints happen 
pca reduce dimensionality ds dataset attributes changes situation somewhat shown table 
robust lr doing better despite reduction available information 
interpret sign robust lr overfitting experiment ds 
approximate solutions equation cg table predictive performance learners fold ds machine learner time sec 
auc bc tree robust lr cg lr linear svm rbf svm false positives comparing learners ds fold bc tree robust lr cg lr linear svm rbf svm predictive performance learners fold ds machine lr reduce overfitting 
explanation similar spirit support early stopping training neural networks bishop 
linear svm suffer significantly projection bc rbf svm lr methods affected 
conclude ds pca contains useful information 
ds pca dense small fast nn algorithm run 
note cg lr required seconds dataset 
complexity section expect cg lr scale dense datasets thousands attributes hundreds thousands rows 
project approximately attributes ds table predictive performance learners fold ds pca machine learner time sec 
auc bc nn robust lr cg lr linear svm rbf svm false positives comparing methods ds pca fold bc nn robust lr cg lr linear svm rbf svm predictive performance learners fold ds pca machine table predictive performance learners fold ds pca machine learner time sec 
auc bc nn robust lr cg lr linear svm rbf svm attributes ds pca predictive performance algorithms spreads seen table 
lr methods rbf svm fall significantly linear svm 
bc nn able find useful information dataset 
certainly special property dataset 
experiments ds ds anchors support hypothesis 
pca project best dimensions inexpensive attractive conclude ds pca contains little information useful 
final comparison uses ds ds anchors datasets 
data proved problematic algorithm 
sparse positive rows course noisy 
ds attributes bc tree cg lr run full dataset 
addition algorithms nn rbf svm linear svm run dimensional anchors projection ds anchors 
results seen table 
general performance better random guessing cg lr tree full dataset manage step auc algorithms cg lr doing significantly better dtree 
bc benefit exposure full dataset nn distinguish steep ascent figures 
supports hypothesis false positives comparing methods ds pca fold bc nn robust lr cg lr linear svm rbf svm predictive performance learners fold ds pca machine table predictive performance learners fold ds anchors cg ds bc ds run full ds dataset machine liver 
learner time sec 
auc bc nn cg lr linear svm rbf svm bc ds tree ds cg ds bc nn exploiting peculiarities ds pca ds pca manner may generalize datasets 
call attention concept scalable 
cg lr required approximately hours cpu time old mhz alpha guessed hours athlon xp believe superior predictive performance justifies expense 
established potential lr particular cg lr machine learning tool 
note roc curves shown cg lr initially fairly steep indicating lr favorite rows large members positive class 
property lr attractive data mining scenarios goal discovering promising rows dataset low false positive rate 
summarize performance robust cg lr ds ds pca ds pca provide 
new material 
seen clearly cg lr second class lr implementation especially superior performance robust lr 
satisfactory clearly competitive results false positives comparing methods ds ds anchors fold bc nn cg lr linear svm rbf svm bc ds tree ds cg lr ds predictive performance learners fold ds ds anchors machine liver false positives applying lr ds derivatives fold ds cg lr ds robust lr ds pca cg lr ds pca robust lr ds pca cg lr ds pca robust lr comparing robust lr cg lr ds ds pca ds pca 
obtained projected datasets cg lr run original sparse datasets 
cg lr speed resistance overfitting expense reducing dataset dimensionality reason run cg lr original sparse datasets 
interesting explore effects conjugate gradient epsilon parameter terminate conjugate gradient iterations 
earlier hypothesized finding approximate solutions equation done cg lr helped prevent overfitting 
clear best value epsilon necessarily smallest 
conclude epsilon causes overfitting epsilon acknowledge looser approximations solving equation result better lr predictions 
false positives changing cg conv 
epsilon ds fold epsilon epsilon epsilon epsilon epsilon epsilon varying cg epsilon ds 
demonstrated logistic regression accelerated conjugate gradient approximate linear solver surpass known modern machine learning algorithms large sparse modern life sciences datasets 
implementation provided superior predictive performance sparse dataset attributes approximately rows appears able scale millions attributes millions rows 
demonstrated conjugate gradient logistic regression suitable dense datasets having thousands attributes tens thousands rows 
explore combination logistic regression conjugate gradient algorithms 
mcintosh mcintosh clearly articulated demonstrated promise combination years ago 
independently rediscovered combination demonstrated value surprised logistic regression prominent place modern machine learning data mining toolbox 
chief aim larger empirical study techniques including exploration cg related methods 
particular explore cg preconditioners performance enhancing techniques mcintosh 
investigate semidefinite iterative methods biconjugate gradient discussed greenbaum 
intend compare performance cg irls common technique applying cg directly maximum likelihood equations 
authors wish ting liu school computer science carnegie mellon university implementing debugging benchmarking non lr algorithms 
bishop 
neural pattern recognition 
oxford university press 
duda hart 
pattern classification scene analysis 
john wiley sons 
greenbaum 
iterative methods solving linear systems volume frontiers applied mathematics 
siam 

applied logistic regression 
wiley interscience edition 
joachims 
making large scale svm learning practical 
advances kernel methods support vector learning 
mit press 
kramer sangiovanni vincentelli 
efficient parallel learning algorithms neural networks 
touretzky editor neural information processing systems volume pages 
morgan kaufmann san mateo 
mccullagh nelder 
generalized linear models volume monographs statistics applied probability 
chapman hall edition 
mcintosh 
fitting linear models application conjugate gradient algorithms volume lecture notes statistics 
springer verlag new york 
moore 
anchors hierarchy triangle inequality survive high dimensional data 
proceedings uai sixteenth conference uncertainty artificial intelligence 
nash 
linear nonlinear programming 
mcgraw hill 
national cancer institute open compound database 
cactus nci nih gov 
svm light 
svmlight joachims org 
watrous 
learning algorithms connectionist networks applied gradient methods nonlinear optimization 
technical report ms cis linc lab university pennsylvania june 
