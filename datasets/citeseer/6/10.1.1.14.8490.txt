information bottleneck em algorithm gal nir friedman school computer science engineering hebrew university nir cs huji ac il learning hidden variables central challenge probabilistic graphical models important implications real life problems 
classical approach expectation maximization em algorithm 
algorithm get trapped local maxima 
explore new approach information bottleneck principle 
approach view learning problem tradeoff information theoretic objectives 
hidden variables uninformative identity specific instances 
second hidden variables informative observed attributes 
exploring different tradeoffs objectives gradually converge high scoring solution 
show resulting information bottleneck expectation maximization ib em algorithm manages find solutions superior standard em methods 
years great deal research learning graphical models general bayesian networks particular data 
central challenge learning graphical models learning hidden latent variables 
hidden variables typically serve summarizing mechanism captures information observed variables passes information part network 
hidden variables simplify network structure consequently lead better generalization 
classical approach learning hidden variables expectation maximization em algorithm 
algorithm performs greedy search likelihood surface proven converge local stationary point usually local maximum 
unfortunately hard real life learning problems local maxima trap em poor solution 
attempts address problem variety strategies :10.1.1.33.3047
introduce new approach learning bayesian networks hidden variables 
approach view learning problem tradeoff information theoretic objectives 
objective compress information training data 
second objective hidden variables informative observed attributes ensure preserve relevant information 
exploring different tradeoffs objectives gradually converge high scoring solution 
approach builds information bottleneck framework tishby multivariate extension :10.1.1.39.9882
framework provides methods constructing new variables stochastic functions set variables time provide information set variables intuition new variables capture relevant aspects informative show pose learning problem multivariate information bottleneck framework derive target lagrangian hidden variables 
show lagrangian extension lagrangian formulation em neal hinton additional regularization term 
controlling strength regularization term scale parameter explore range target functions 
spectrum trivial target compression data total relevant information lost 
target function em 
continuity target functions allows learn procedure motivated deterministic annealing approach :10.1.1.33.3047
start optimum trivial target function slowly change scale parameter tracking solution step way 
alternative view optimization problem joint space model parameters scaling factor 
provides appealing method scanning range solutions homotopy continuation 
procedure automatically increases effective cardinality hidden variables progresses 
stopping procedure earlier stage reach solution better generalization properties 
extend information bottleneck em ib em framework multiple hidden variables bayesian network structure 
show relation information bottleneck em holds case variational em 
show ib em algorithm effective learning models hard real life problems superior em methods 
bayesian networks consider finite set 
xn random variables 
bayesian network bn annotated directed acyclic graph encodes joint probability distribution nodes graphs correspond random variables 
node annotated conditional probability distribution cpd random variable parents pa graph joint distribution written 
xn pa 
graph represents independence properties assumed hold underlying distribution independent non descendants parents pa suppose network structure training set 
consists instances want learn parameters network 
maximum likelihood setting want maximize log likelihood function log log 
function equivalently written log empirical distribution frequencies practice add prior parameters try maximize log log results map estimation term compensate rescaling likelihood expectation 
priors thought adding imaginary instances distributed certain distribution uniform training data 
consequently point view priors modifying empirical distribution additional instances apply maximum likelihood principle 
multivariate information bottleneck information bottleneck method general information theoretic clustering framework :10.1.1.39.9882
joint distribution variables attempts finds bottleneck variable defined stochastic function 
variable compresses information preserving information relevant example variable define soft clustering words appearing documents preserving information relevant topic documents 
multivariate extension framework allows definition cluster variables desired interactions observed variables 
desirable interactions represented bayesian networks called representing required compression called representing independencies striving bottleneck variables target variables 
specifies want minimize information compresses specifies want variables independent 
objectives conflicting tradeoff information contains 
definition multivariate information bottleneck framework formally framework friedman attempts minimize objective function ii id id leibler divergence joint probabilities represented networks respectively 
minimization possible parameterizations marginal fixed possible parameterizations represented words want compress way distribution defined close possible desired distribution scale parameter balances factors 
zero interested compressing variable resort trivial solution single cluster equivalent parameterization 
high concentrate choosing close distribution satisfying independencies encoded learning hidden variable main focus information bottleneck learned distribution 
distribution thought soft clustering original data 
emphasis somewhat different 
dataset 
interested learning better generative model describing distribution observed attributes want give high probability new data instances source 
learned network hidden variables serve summarize part data retaining relevant information observed variables start extending multivariate information bottleneck framework task generalization addition task clustering interested learning generative model consider case single hidden variable extend framework hidden variables section 
information bottleneck em lagrangian intuitively task generalization requires forget specific training examples hand preserve general form distribution observed variables 
want compress identity specific instances 
hand observed variables deterministically known identity specific instance expect loose information observed variables performing compression 
defines tradeoff compression identity specific instances preservation information relevant observed variables 
formalize idea 
define new variable denotes instance identity 
takes values 
define empirical distribution attributes data augmented distribution new variable instance value take specific instance 
apply information bottleneck graph 
choice graph depends network want learn 
take target bayesian network additional variable parent 
simplicity consider simple clustering model parent 
xn practice seen section choice 
apply information bottleneck framework pair graphs 
attempt define conditional probability approximated distribution factorizes construction aim find captures relevant information instance identity observed attributes 
start determining objective function particular choice dealing 
proposition minimizing information bottleneck objective function eq 
equivalent minimizing lagrangian lem ii log log function 
proof chain rule structure write 
similarly independent write 
id log id log log log setting term reaches zero minimal value 
second term constant change 
need minimize terms result follows immediately 
immediate question target function relates standard maximum likelihood learning 
explore connection formulation em introduced neal hinton 
em usually thought terms changing parameters target function neal hinton show view dual optimization auxiliary distribution notation write functional defined neal hinton log ih ih log fixed observed empirical distribution 
theorem stationary point stationary point log likelihood function log 
neal hinton show em iteration corresponds maximizing choice holding fixed maximizing holding fixed 
form quite similar ib em lagrangian relate 
proposition lem ii proof identity ih log ii write log log ii result follows immediately 
consequence minimizing ib em lagrangian equivalent maximizing em functional combined information theoretic regularization term 
lagrangian em functional equivalent finding local minima lem equivalent finding local maxima likelihood function 
ib em algorithm specific value information bottleneck em ib em algorithm described iterations similar em iterations neal hinton 
step minimize lem optimizing holding fixed 
step minimize lem optimizing holding fixed 
step essentially standard maximum likelihood optimization bayesian networks 
see note term involves log 
term form log likelihood function empirical distribution distribution variables sufficient statistics efficient estimates 
step consists computing expected sufficient statistics plug formula choosing parameters step bit involved 
need optimize 
results follow results friedman 
proposition stationary point lem respect fixed choice values respectively exp ep ep log normalizing constant 
proposition stationary point lem achieved iteratively applying self consistent equations proposition 
cases stationary convergence point reached applying self consistent equations local maxima 
combining result result neal hinton show optimization increases conclude step step decrease lem reach stationary point 
bypassing local maxima continuation discussed parameter balances desire compress data desire fit parameters close objective compressing data 
effective dimensionality leading trivial solution 
larger values pay attention distribution expect additional states utilized 
ultimately expect sample assigned different cluster dimensionality allows 
proposition tells limit solution converge standard em possible solutions 
naively allow high cardinality hidden variable set high value find bottleneck solution point 
drawbacks approach 
typically converge sub optimal solution cardinality maxima 
second know correct cardinality assigned hidden variable 
cardinality large learning robust intractable 
low dimensionality fully utilize potential hidden variable 
identify beneficial number clusters having simply try options 
cope task adopt deterministic annealing strategy :10.1.1.33.3047
strategy start single cluster solution high entropy optimal compression total 
progress higher values 
gradually introduces additional structure learned model 
ways executing general strategy 
common approach simply increase fixed steps increment apply iterative algorithm re attain local minima new value 
problems examine section naive approach prove successful 
refined approach utilizes continuation methods executing strategy 
approach provides means automatically tuning appropriate change size ensures keep track solution iteration 
perform continuation view optimization problem joint space parameters 
space want follow smooth path trivial solution solution 
furthermore require fix point equations hold points path 
continuation theory guarantees excluding degenerate cases path free discontinuities exists 
start characterizing paths 
note fix parameter parameters optimal choice determined function take free parameters problem 
shown proposition gradient lagrangian zero eq 
holds value want consider paths equations hold 
define log ep log clearly exactly eq 
holds goal follow equi potential path functions zero starting small value upto desired em solution 
suppose point functions equal 
want move direction dq dq satisfies fix point equations 
want find direction computing derivatives respect parameters results derivative matrix 
rows matrix correspond functions eq 
columns correspond parameters 
entries correspond partial derivative function associated row respect parameter associated column 
find direction satisfies eq 
need satisfy matrix equation words trying find vector nullspace 
note ma trix direction vector length 
null space dimension rank 
numerically excluding measure zero cases expect rank full unique upto scaling solution eq 

finding direction costly 
notice size 
number quadratic training set size just computing matrix prohibitively expensive small datasets 
resort approximating matrix contains diagonal entries column case significant values matrix diagonal derivatives terms set zero 
approximation solve eq 
time linear note find vector satisfies eq 
need decide size step want take direction 
various standard approaches normalizing direction vector predetermined size 
problem natural measure progress 
recall ii increases captures information samples 
runs ii starts upper bounded log cardinality interesting steps learning process occur ii grows 
exactly points balance terms lagrangian changes second term grows sufficiently allow term increase ii 
intuition hand want normalize step size expected change ii 
region changes parameters influencing ii big step 
hand change strong influence ii want carefully track solution 
formally compute ii rescale direction vector ii predetermined step size 
bound minimal maximal change get trapped steps alternatively overlook regions change 
continuation method takes correct direction approximation inherent numerical instability lead suboptimal path 
cope situation adopt commonly heuristic deterministic annealing 
value slightly perturb current solution re solve equations 
perturbation leads higher lagrangian take current solution 
summarize procedure works follows start trivial solution exists 
stage compute direction leave fix point equations intact 
take small step direction apply ib em iterations attain fix point equilibrium new value 
repeat iterations reach 
regularization generalization discussion far concerned reaching value solution 
experimental results show continuation methods algorithm manages reach solutions superior running standard em 
domains maximizing likelihood lead overfitting poor generalization 
common machine learning regularization tendency overfit training examples 
priors reduce effect 
guarantee best generalization performance 
discussed want learn hidden variables large cardinality regularization determine effective cardinality information bottleneck formalization provides form regularization arise naturally definition learning problem 
learn compression term counteracts tendency overfit data 
get better generalization parameters estimate intermediate values 
examine models learned different continuation iterations see iterations closer degrade generalization performance 
observation suggests learning purposes prefer learn model usually smaller 
technical challenge select relatively straightforward somewhat costly approach cross validation cv test 
approach perform runs algorithm learning th training data 
run evaluate intermediate models remaining th data 
estimate generalization different values averaging log likelihood held data value folds 
evaluation estimate perform continuation process training data critical important stress approach utilizes training data order predict value generalization best 
multiple hidden variables framework described previous section easily accommodate learning networks multiple hidden variables simply treating vector hidden variables 
case distribution describes joint distribution hidden variables value describes joint distribution attributes desired network 
unfortunately number variables large representation grows exponentially approach definition networks multivariate information bottleneck framework multiple hidden variables 
shows mean field assumption 
shows possible hierarchy infeasible 
strategy alleviate problem force factorized form 
reduces cost representing cost performing inference example require factored product 
assumption similar mean field variational approximation 
multivariate information bottleneck framework different factorizations correspond different choices networks example mean field factorization achieved parent 
general consider choices introduce edges different 
choice get exactly lagrangian case single hidden variable 
main difference factorized form decompose ii 
example mean field factorization get ii ii 
similarly decompose log sum terms family factorization lead tractable computation terms lagrangian written proposition 
unfortunately term log evaluated efficiently 
approximate term log 
mean field factorization resulting lagrangian approximation form em ii log log case single hidden variable characterize fix point equations hold stationary points lagrangian 
proposition assuming mean field approximation local maximum em achieved iteratively solving self consistent equations hidden variable independently 
exp ep ep log normalizing constant 
proof follows lines theorem 
difference computation fix point equations derivative log respect resulting different form ep computed efficiently 
easy see single hidden variable considered forms coincide 
interesting consequence discussion maximizing em equivalent performing mean field em 
modified lagrangian generalize variational learning principle show manage reach better solutions 
easily extend idea describe correspondence different choices matching structural approximation applied standard em 
lack space go details fairly straightforward 
summarize ib em algorithm section easily generalized handle multiple hidden variables simply altering form ep fix point equations 
details continuation method remain unchanged 
experimental validation evaluate ib em method examine generalization performance types models real life datasets 
architecture consider networks hidden variables different cardinality simplicity cardinality hidden variables network 
briefly describe datasets model architectures 
stock dataset records daily changes major technology stocks period years 
training set includes samples test set includes instances 
trained naive bayes hidden variable model hidden variable parent observations 
digits dataset contains instances sampled usps postal service dataset handwritten digits see www kernel machines org data html 
data includes images digit 
image represented variables denotes gray level pixel matrix 
discretized pixel values equal bins 
images training set test set 
data tried network architectures 
naive bayes model single hidden variable 
addition examined complex hierarchical models 
models introduce hidden parents quadrant image recursively 
level hierarchy hidden parent quadrant hidden variable parent hidden variables 
level hierarchy starts blocks additional intermediate level hidden variables total hidden variables 
yeast dataset contains expression measurement baker yeast genes experiments 
experi table comparison ib em algorithm runs em random starting points runs mean field em random starting points 
shown train test log likelihood instance best th percentile random runs 
shown percentile runs worse ib em results 
datasets shown include naive bayes model stock dataset digit dataset level hierarchical model digit dataset hierarchical model yeast dataset 
model show cardinalities hidden variables shown column 
train log likelihood test log loss model ib em restarts em mean field em ib em restarts em mean field em stock digit yeast random runs ib em em mean field em comparison test performance ib em algorithm cumulative performance random em mean field em runs level hierarchy model binary variables digit domain 
ments measure yeast response changes environmental conditions 
experiment expression genes measured 
discretized expression genes ranges threshold standard deviation gene mean expression experiments 
data treat gene instance described behavior different experiments 
randomly partitioned data training instances genes test instances 
model learned data hierarchical structure hidden variables level hierarchy determined nature different experiments 
middle level hidden parent similar conditions different types heat shock top level contains single variable parent variables middle level 
sanity check model cardinality hidden variables performed runs em random starting points 
resulted parameters wide range likelihoods training set test set 
results elaborate indicate learning problems challenging sense em runs trapped different local maxima 
considered application ib em problems 
performed single ib em run problem compared random em runs random mean field em runs 
example compares test set performance log likelihood instance runs digit dataset level hierarchy binary hidden variables 
solid lines shows performance ib em solution 
dotted lines show accumulative performance random runs 
see ib em model superior mean field em runs em runs 
important note time required runs pentium iv ghz machine 
single mean field em run requires approximately minute exact em random run requires roughly minutes single ib em run took minutes 
ib em takes time mean field em runs reaches solutions runs achieve 
ib em run time roughly similar exact em terms time need runs get comparable solution 
proportions hold datasets harder problems standard em runs somewhat digit naive bayes continuation performance runs 
top panel log likelihood instance vs 
dotted lines show best th percentile random em runs 
middle panel log likelihood unseen test data 
bottom panel predicted test performance cross validation training data 
vertical line denotes cv estimate circles mark values lagrangian evaluated continuation process 
expensive times ib em runs 
table summarizes results comparisons different learning problems 
shows training set test performance different methods 
show best run group random starting points relative percentile ib em solution runs 
see ib em runs outperforms random restarts naive bayes models cases 
harder problems involve hierarchy hidden variables situation complex 
compare ib em mean field em see consistently better non trivial margin 
compare ib em exact em see simpler level digit models ib em better em runs 
complex models performance ib em worse em runs 
hypothesis drop performance due inherent limitations mean field approximation models 
approximation loses information interactions hidden variables 
stress selected models perform exact inference hidden variables compare exact em 
applications exact inference infeasible approximations needed 
clearly mean field approximation quite crude 
discussed framework allows refined variational approximations expect improve performance variational em ib em 
compared ib em method perturbation method 
briefly method alters landscape likelihood perturbing relative weight samples progressively diminishing perturbation factor temperature parameter 
stock dataset perturbation method initialized starting temperature cooling factor performance similar ib em 
running time perturbation method order magnitude larger 
examples considered running perturbation method parameters proved prohibitively expensive 
run efficient parameter settings perturbation method performance inferior ib em 
results consistent showed improvement case parameter learning mainly focused structure learning hidden variables 
turn examine continuation process closely 
illustrates progression ib em 
top panel shows training loglikelihood instance parameters intermediate points process 
panel shows values evaluated continuation process circles 
see continuation procedure focuses region significant changes likelihood 
middle panel shows likelihood test data 
see training set likelihood increases increase test set performance deteriorates stage due overfitting data 
example suggests improve performance model 
section suggested cv test training data estimate critical value lower panel shows cv estimate test set likelihood training data 
estimates biased see allow learning algorithm pinpoint value demonstrate effectiveness approach clearly examine situation excessive number parameters 
naive bayes model digit dataset 
scenario expect learned model overfit training data 
expect lower 
see early stages learning model clearly overfits data 
cv estimate procedure learns model test set performance better naive bayes models learned dataset 
discussion set learn models hidden variables 
described method reaching high scoring solution starting simple solution trajectory high scoring 
contribution threefold 
formal connection information bottleneck principle maximum likelihood learning graphical models 
information bottleneck extensions originally viewed methods understand structure distribution 
showed sense information bottleneck maximum likelihood estimation sides coin 
information bottleneck focuses distribution variables instance maximum likelihood focuses projection distribution estimated model 
understanding extends general bayesian networks results slonim weiss relate original information bottleneck maximum likelihood estimation univariate mixture distributions 
second ib em principle allowed approach starts solution progresses solution complex landscape 
general scheme common deterministic annealing approaches 
approaches flatten landscape raising likelihood power 
main technical difference approach regularization term derived structure approximation probability latent variables instance 
third applied continuation methods traversing path trivial solution solution 
standard approaches deterministic annealing information bottleneck procedure automatically detect important regions solution changes drastically ensure tracked closely 
preliminary experiment results shown continuation method clearly superior standard annealing strategies 
show experimental results applying ib em hard learning problems leads solutions superior equivalent version em 
early stopping rule find solutions intermediate values better generalization 
methods extended directions 
relaxing mean field variational approximation explore better tradeoff tractability quality learned model 
second approximate solution performing continuation 
better approximations lead accurate results fewer steps 
third showed cross validation detect values generalization better 
deriving principled method understanding generalization theoretical implications lead faster accurate learning 
principle generalized problems structure learning replacing parameter optimization step structure learning procedure 
results natural extension structural em framework learning structure parameters bayesian network interest 
bachrach chechik koller tishby weiss discussions comments earlier drafts 
supported part israeli ministry science 
supported horowitz fellowship 
friedman supported alon fellowship harry abe sherman senior computer science 
boyen friedman koller 
learning structure complex dynamic systems 
uai 
cover thomas 
elements information theory 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
roy 
stat 
soc 
friedman schuurmans 
data perturbation escaping local maxima learning 
aaai 
friedman 
bayesian structural em algorithm 
uai 
friedman slonim tishby 
multivariate bottleneck 
uai 
genomic expression program response yeast cells environmental changes 
mol 
bio 
cell 
glover laguna 
tabu search 
modern heuristic techniques combinatorial problems 
heckerman 
tutorial learning bayesian networks 
learning graphical models 
jordan ghahramani jaakkola saul 
variational approximations methods graphical models 
learning graphical models 
gelatt jr vecchi 
optimization simulated annealing 
science may 
lauritzen 
em algorithm graphical association models missing data 
computational statistics data analysis 
neal hinton 
new view em algorithm justifies incremental variants 
learning graphical models 
pearl 
probabilistic reasoning intelligent systems 
rose :10.1.1.33.3047
deterministic annealing clustering compression classification regression related optimization problems 
ieee 
slonim weiss 
maximum likelihood information bottleneck 
nips 
tishby pereira bialek :10.1.1.39.9882
information bottleneck method 
proc 
th allerton conf 
communication computation 
ueda nakano 
deterministic annealing em algorithm 
neural networks 
watson 
theory globally convergent probability homotopies non linear programming 
tech 
report tr cs virginia tech 
