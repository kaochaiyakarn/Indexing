polynomial time algorithms network information flow peter sanders mpi informatik saarbr germany sanders mpi sb mpg de sebastian philips research laboratories prof aa eindhoven netherlands sebastian philips com philips research laboratories prof aa eindhoven netherlands famous max flow min cut theorem states source node send information network sink node data rate determined min cut separating shown rate achieved multicasting sinks provided intermediate nodes allowed information receive 
contrast graphs coding rate factor omega jv smaller 
far fast algorithms constructing appropriate coding schemes known 
main result polynomial time algorithms constructing coding schemes multicasting maximal data rate 
categories subject descriptors coding information theory formal models communication network protocols routing protocols algorithms problems routing layout general terms algorithms theory keywords coding communication derandomization finite field linear algebra multicasting network information theory randomized algorithm 
partially supported dfg sa permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
spaa june san diego california usa 
copyright acm 
example coding helps 
study problem multicasting consider directed graph source node set sink nodes task send information source sinks maximum data rate bandwidth 
edges transport symbols alphabet 
sink known max flow problem maximum data rate corresponds maximum flow equals minimum cut separating maximum flows polynomial time 
furthermore flow magnitude decomposed edge disjoint paths multicasting simply send input symbol paths 
things get complicated multiple sinks 
example consider graph 
flows magnitude sink fy zg 
reader verify way assign input symbols flow paths sink gets symbols 
ahlswede shown coding network solve problem 
example assume want multicast bits node forwards exclusive phi bits receives 
sink find computing phi phi sink get phi phi 
turns works networks upper bound obtainable data rate imposed minimum cut separating sink achieved coding 
area network information flow conceptually interesting brings seemingly unrelated concepts coding network flows 
seen example data rate smaller coding allowed nodes forward copies symbols receive 
section give simple examples rate achievable coding factor omega log jv smaller 
addition maximizing multicast data rate coding hard minimum steiner tree problem undirected graphs ffl approximation hard ffl intensive research algorithm achieving approximation factor better known 
main result coding allows higher data rates finding optimal multicast coding schemes possible polynomial time 
overview continue basic notation section review related section 
main result section develops polynomial time algorithms unit capacity directed acyclic graphs 
section gives adaptations capacitated cyclic graphs achieve optimal rate 
logarithmic gap ting coding demonstrated section 
section gives alternative algorithm multicasting coding trades faster coding scheme construction higher requirements size finite field 
algorithm illustrates relations different approaches reasoning multicasting coding 
section discusses open questions 
appendix summarizes notation 
basic notation consider acyclic unit capacity network parallel edges allowed 
node source node set sink nodes size smallest min cut separating notation gamma gamma gamma set edges reaching leaving node respectively start denotes node edge starts 
linear coding finite field turn jfj jt sufficient 
source node gets input symbols symbol carried edge linear combination symbols carried edges entering start 
local coding vector gamma gamma start determines coefficients linear combination gamma gamma start formula holds edges leaving source node introduce dummy node parallel input edges ek connecting th input symbol 
task determine coefficients sinks reconstruct original information symbols reaching 
note definition assumes single batch symbols sent 
nodes wait incoming data arrived 
section see multicasting scheme type easily adapted independently chou jain obtained similar result 
pipelined scheme symbols timestep communicated 
symbol ffi stands 
previous ahlswede shown source multicast information rate approaching sinks symbol size approaches infinity 
give simple example shows coding rate achievable 
li show linear coding multicasting rate finite symbol size 
algorithms viewed fast implementations approach li main difference li check exponential number edge sets verify coding coefficients chosen particular edge correct 
reduce single edge set sink node making explicit precomputed flows sink 
koetter give elegant algebraic characterization linear coding schemes achieve maximal data rate 
show finite fields size jt delta sufficient give polynomial time algorithm verify linear network coding scheme 
give algorithm constructing coding schemes 
algorithm involves checking polynomial identity multivariate polynomial exponential number coefficients 
chou jain independently concurrently developed algorithm similar deterministic algorithm section runs time gamma jej jt delta plus time jt maximum flow computations 
number results multicasting coding undirected networks 
single source problem amounts fractionally packing steiner trees connecting fsg fractional steiner tree packing problem equivalent minimum weight steiner tree problem mwst duality corresponding linear programs 
particular approximation algorithms mwst yield approximation algorithms packing steiner trees approximation ratio ellipsoid method 
coding multiple multicast requests decomposed corresponding number steiner tree packing problems linear programming 
randomized rounding obtains approximate integral solutions multiple multicast requests 
approximation quality constant factor plus logarithmic additive term 
jansen zhang avoid ellipsoid method cost slightly worse approximation ratio 
approach running times low jej jej achieved number unit data rate multicast requests 

polynomial time coding section concerned establishing result 
theorem 
consider acyclic unit capacity multigraph denote minimum cut size source sink linear information flow algorithm constructs linear codes nodes gamma jej delta jt delta delta expected time randomized variant gamma jej delta jt delta jt delta worst case time deterministic variant 
codes properties ffl finite field size jfj delta jt represent symbols sent edges 
deterministic case jfj jt sufficient 
ffl source gets information symbols input 
ffl node needs time gamma min jt gamma gamma start delta compute symbol sent edge source needs time edge 
ffl sink reconstruct information symbols time gamma delta building notation section explain effects linear coding terms linear algebra 
linear coding information carried edge linear combination input symbols characterize effect local coding vectors edge independently concrete input global coding vectors input edges unit vectors gamma gammai global coding vectors inductively defined gamma gamma start vectors defined network acyclic 
elementary linear algebra seen linear coding scheme multicasting vectors phi gamma gamma psi span vector space reconstructing original information achieved solving linear system equations polynomial size 
intuition linear code mixes information received different edges loose essential information long bijective mapping input data reaching sink 
challenge find local coding vectors efficiently possibly small finite field allows fast arithmetics 
algorithm achieves goal making explicit maximum flow algorithm 
initially computes flows magnitude decomposes flows edge disjoint paths single sink node task simple 
route th input symbol th edge disjoint path 
edge flow path denote predecessor edge edge path single sink example choose nonzero coefficient zero coefficients 
multiple sinks approach superimpose multiple flows 
algorithm steps nodes topological order 
ensures global coding vectors edges reaching known local coding vectors edges leaving determined 
algorithm defines coefficients edge gamma 
multiple flow paths different sinks edge denote set sinks example multicasting linear coding fa gg 

assume flows decomposed topmost path bottommost path 
thin lines give nonzero coefficients local coding vectors 
vectors give resulting global coding vectors 
assume gf edges leaving considered top bottom 
feasible linear combinations 
examples notation gamma gamma start fb dg fe fixed fp correspondingly phi gamma delta gamma delta psi flow phi psi denote set predecessors edges flow path 
nonzero coefficients chosen edges 
ensure sinks reconstruct input algorithm li verifies global coding vector linearly independent exponential number sets global coding vectors 
algorithm dramatically simplify task exploiting flows 
turns jt edge sets need checked maintain invariant sink set edges global coding vectors fb basis original input reconstructed information carried edges contains edge path edge global coding vector defined 
computation completes gamma gamma invariant ensures sink gets information 
establish invariant assigning artificial input edges fe gamma gammai linear combination new edge defined exchange 
maintain invariant suffices check spans gives example algorithm notation 
remains explain find coefficients maintain invariant 
argue random coefficients edges job delta jt lemma shows fixed sink failure probability jfj 
summing sinks see failure probability jt jf 
straight forward construction yields monte carlo algorithm finding single local coding vector construction fail 
find coding vectors fast small field sufficient 
knowledge flows encoded invariant come allow convert monte carlo algorithm las vegas algorithm constant expected number trials followed jt independence tests suffices find feasible local coding vector 
lemma 
assume basis probability jfj random local coding vector fulfills property phi psi fb basis corresponding global coding vector 
proof 
fix coefficients phi psi exactly choice linearly dependent phi psi exactly jfj jp gamma local coding vectors violate property sink jfj jp choices local coding vectors probability random choice violates property jfj jp gamma jfj jp jfj 
said far yields algorithm running polynomial expected time 
follows refine algorithm obtain fast concrete implementation deterministic way choosing linear combinations testing linear independence fast mathematical basis refinement lif algorithm lemma states problem testing linear independence gamma dimensional vector space reduced single scalar product 
lemma 
consider basis vectors delta ffi vector linearly independent fbg delta 
proof 
unique representation basis get delta delta ffi linearly independent fbg delta 
lif algorithm maintains vectors sink edge test linear independence fb invariant jc delta ffi invariant implies linear independence desired property algorithm implements outline lif algorithm 
prove correctness verify loop invariant 
lemma 
loop invariant holds 
proof 
induction loop vertices loop invariant trivially satisfied 
assume induction hypothesis invariant holds 
show holds 
replace edge edge size size algorithm chosen linearly independent deltaa lemma defined 
verify delta ffi short calculation delta delta gamma delta delta gamma delta delta gamma delta delta delta gamma delta delta gamma delta delta delta gamma gamma delta delta delta gamma delta delta delta ffi 
vectors arranged rows matrix columns correspondingly arranged matrix invariant equivalent gamma notation method updating inverse vectors lif algorithm special case sherman morrison formula section 
said far suffices establish complexity randomized variant lif lemma 
line implemented choosing random condition linearly independent fb satisfied algorithm implemented run time gamma jej delta jt delta delta returned information allows decoding time gamma delta sink 
proof 
single graph traversal find flow augmenting path time jej 
apply routine round robin sink sink augmenting paths left 
find augmenting paths sink time jt delta jej delta 
algorithm constructs finite field elements chosen jfj jt involves creation lookup table jfj entries incrementing elements conway logarithm algorithm find paths time jej 
large yields improved asymptotic time bounds flow computation part :10.1.1.135.6797
function lif min min min jmax flow tj insert new source help establish invariant insert parallel edges fe denote set edge disjoint paths chosen flow notation access flows field size blog forall gamma gammai th unit vector forall fe supplied fb eh coding vectors span forall inverse vectors foreach vertex fs topological order forall outgoing edges invariant jc delta ffi choose linear combination linearly independent fb forall ff feg advance set edges fb fb update correspondingly gamma delta delta gamma update correspondingly forall ff gamma delta return fme tg 
linear information flow lif coding fast testing linear independence 
network source set sinks algorithm constructs linear codes intermediate nodes rate maximal 

table arithmetic operation computed constant time 
initializing takes time gamma jt delta delta main loops collectively iterate edges total number jej iterations 
computing takes time jt flows maintain pointers predecessors edges path decomposition finding random local coding vector takes time jp jt 
computing testing linear independence vectors takes time hjt 
success probability constant expected cost finding linearly independent delta delta jt delta jt 
computing takes time gamma jt delta delta combining parts get claimed expected time bound gamma jej delta jt delta delta sink reconstruct vector input symbols computing denotes symbol received edge takes time gamma delta deterministic implementation explain algorithm lif table considered large resort polynomial representation field elements 
case table needed cost additional factors running time polylogarithmic jt implemented deterministically 
develop deterministic method choosing local coding vectors step lemmas formulate pure linear algebra problems graph theoretic concepts 
lemma 
consider pairs theta delta jfj 
linear combination xn delta 
proof 
denote vector space spanned xn denote dimension 
fx delta gamma dimensional subspace clearly satisfies delta iff null vector common number linear combinations required lemma fi fi fi fi fi fi fi fi fi fi jv gamma jk gj jfj gamma jfj gamma gamma jj gamma jfj jfj gamma jfj jfj gamma 
jfj jfj gamma field elements 
linear combination lemma fixing coefficient linear combination greedy fashion 
lemma 
linear combination proven exist lemma worst case time gamma jfj delta proof 
proof induction simply set induction step gamma apply induction hypothesis obtain vector delta delta yn happens nonzero done 
apply lemma yn gamma xn yn 
jfj linear combination fiu delta 
particular delta yn delta yn 
fl fl form ffu xn delta 
coefficient ff trying field elements 
jfj trials time jfj precomputing scalar products delta xn delta get total time gamma delta iterations 
lemma find linear combination lif algorithm apply lemma phi psi denote vector delta 
value obtained setting sum deterministic part theorem proven analogously proof lemma 
just substitute expected time jt delta needed randomized routine choosing time gamma jt delta jfj delta gamma jt jt delta needed apply lemma 
obtain total execution time gamma jej jt jh jt jt delta gamma jej delta jt jt delta 
refinements pipelining far analysis covers sending symbols sinks 
reality want send symbols time step pipelined fashion 
nodes naively forward linear combinations symbols received previous time step get mix information sent different time steps difficult decode 
naive algorithm works fine layered graphs node mapped layer edges exist adjacent layers 
case information received node layer time step linear combination information sent time step gamma acyclic graphs layered easy convert layered graphs replacing edges spanning layers chain new nodes 
gives example 
new nodes indegree outdegree detailed analysis shows trials needed constant expected number trials needed coefficients tried random order jfj 
transformation layered graph 
additional coding necessary 
emulated original network simply introducing time delays data sent edge spanning layers delayed steps 
edge capacities consider acyclic graphs integer edge capacities 
replacing capacitated edge parallel unit capacity edges section immediately yields pseudopolynomial time algorithms algorithms running time polynomial jv jej 
exponential number bits needed define input graph 
question arises handle graphs large edge capacities 
section suffices solve problem theorem 
denote maximum flow acyclic network edge capacities ffl linear codes network information flow time polynomial jv jej ffl gamma ffl symbols time step communicated 
proof 
preprocessing step find maximum flow sink reduce max 
particular edge capacity exceeds 
jej denote maximum number edge disjoint paths needed realize flows build network unit capacity edges edge corresponds bw delta parallel unit capacity edges find multicast coding scheme possible polynomial time delta jej ffl edges unit capacity problem 
coding decoding capacitated instance edge split edges capacity hffl edge transmits hffl symbols time steps encoding prescribed corresponding unit capacity edge 
path flow unused capacity hffl hffl paths flow 
total capacity gamma hffl gamma ffl cyclic graphs ahlswede explain convert cyclic graph maximal obtainable rate acyclic graph jv delta jv je delta jej rate gamma jv explain communication graph emulated time steps cyclic graph requirement avoids trivial rounding issues 
appropriately choosing unit time quite flexible choosing ffl 
example symbols time step delivered 
coding best send symbols time steps 
transformation polynomial time algorithm finding coding schemes 
choosing jv ffl obtain coding scheme achieves rate gamma ffl 
gap theorem 
acyclic networks unit capacity edges multicasting coding allows factor omega jv larger rate multicasting coding 
proof 
consider network vertices fsg hg ft jtj hg edges ug tg network layers source constitutes layer second layer nodes sink nodes constitute third layer node element subset note theta log jv 
figures give examples respectively 
min cut size rate coding coding rate 
see suppose source attempts send symbols sinks consecutive uncoded transmissions 
denote subset intermediate nodes receiving edges unit capacity source send hn symbols total intermediate nodes ju hn 
implies ju element subset sink receive symbol network suffices source encodes information code length conveys information symbols allows reconstruction information received symbols 
called mds code 

faster construction outline alternative algorithm constructing linear network coding schemes 
algorithm faster cost larger finite fields possibly expensive coding decoding 
importantly approach illustrates interesting connections previous results 
theorem 
linear network coding schemes finite fields size jfj delta jej delta jt achieving rate expected time jej delta jt jh jt jm gamma delta denotes time required performing theta matrix multiplications 
proof outline find flows decomposed paths time jej delta jt delta 
pick independent random local coding vectors edges simultaneously 
compute global coding vectors time gamma jp delta delta jej delta jt delta 
sink denote set global coding vectors corresponding edges path check span time jt delta matrix inversion fast matrix multiplication 
tests fails repeat 
lemma analysis careful algorithm section seen success probability careful algorithm performs jt independence tests going wrong probability jfj 
tests failure probability jt jfj expected number repetitions constant 
algorithm quite similar ahlswede 
main difference choose random linear local coding functions completely arbitrary random functions 
analysis ahlswede adapted 
analysis goes arbitrary random functions replaced random choice universal family hash functions 
known random linear mappings form universal family 
exploiting special structure linear functions idea developed polynomial time randomized algorithm achieving rate gamma ffl constant ffl 
analysis yields stronger results rate exponentially smaller finite fields flow arguments reduce exponential number cuts considered polynomial number edge sets need carry information 
interesting observation koetter arrive similar requirement jfj theorem quite different algebraic arguments 

discussion polynomial time algorithms multicasting acyclic networks quite simple practice rate large family functions called universal pr jaj randomly chosen decoding expensive 
addition algorithms limited maximizing rate 
particular application streaming multimedia content large computer network rate required fixed 
situation find cheap favorable subgraph minimum cut algorithms construct codes achieve multicast rate server client 
number open problems graphs cycles 
polynomial time algorithm finds coding scheme rate exactly 
practical point view approximate algorithm interesting allows coding schemes faster decode 
currently rate gamma ffl requires decoding time omega delta jv ffl decoded symbol looks forbidding large graphs 
little known problem multiple sources 
find coding optimal solutions easier find efficient solutions coding 
rudolf ahlswede ning cai philip chou irit piotr anand berthold fruitful discussions 

ahlswede cai 
li yeung 
network information flow 
ieee transactions information theory 
ahuja magnanti orlin 
network flows 
prentice hall 

approximate implementation minimum multicast congestion implementation versus theory 
manuscript 
carr vempala 
randomized meta rounding 
nd acm symposium theory computing pages 
random structures algorithms 

approximation hardness steiner tree problem graphs 
th scandinavian workshop algorithm theory number lncs pages 
coppersmith winograd 
matrix multiplication arithmetic progressions 
symbolic computation 
dinic 
algorithm solution problem maximum flow 
soviet math 
dokl 
spelled 
dumas gautier 
finite field linear algebra subroutines 
issac pages lille 
acm 
tarjan :10.1.1.135.6797
network flow testing graph connectivity 
siam comput 

method computing addition tables gf 
ieee transactions information theory 
chou jain 
low complexity algebraic multicast network codes 
international symposium information theory 
ieee july 
page appear 
jain 
packing steiner trees 
th acm siam symposium discrete algorithms 
jansen zhang 
approximation algorithm multicast congestion problem minimum steiner trees 
rd international workshop approximation randomized algorithms communication networks rome italy 
koetter 
algebraic approach network coding 
proceedings infocom 

li yeung cai 
linear network coding 
ieee transactions information theory 
press teukolsky vetterling flannery 
numerical recipes cambridge university press nd edition 
robins zelikovsky 
improved steiner tree approximation graphs 
th acm symposium parallel architectures algorithms pages 
vempala 
approximating multicast congestion 
th int 
symp 
algorithms computation number lncs pages 
springer 
appendix notation vector property delta linearly independent fb global coding vector edge set global coding vectors fb edges edge disjoint paths capacity edge ffi 
set edges edge eh input edges connecting ffl small constant finite field flow magnitude represented edge disjoint paths predecessor edge path gamma gamma set edges entering node gamma set edges leaving node graph smallest maximum flow sink start node edge departs gamma gamma start local coding vector edge coefficient multiplied contribute predecessor edges flow path phi psi source node dummy source node set sink nodes sinks supplied edge phi psi sink node set nodes node symbol carried edge 
