appear kearns solla cohn editors advances neural information processing systems mit press cambridge ma 
gradient descent general reinforcement learning baird andrew moore cs cmu edu cs cmu edu www cs cmu edu www cs cmu edu computer science department computer science department forbes avenue forbes avenue carnegie mellon university carnegie mellon university pittsburgh pa pittsburgh pa simple learning rule derived vaps algorithm instantiated generate wide range new algorithms 
algorithms solve number open problems define new approaches reinforcement learning unify different approaches reinforcement learning single theory 
algorithms guaranteed convergence include modifications existing algorithms known fail converge simple mdps 
include learning sarsa advantage learning 
addition value algorithms generates pure policy search reinforcement learning algorithms learn optimal policies learning value function 
addition allows value algorithms combined unifying different approaches reinforcement learning single value policy search vaps algorithm 
algorithms converge pomdps requiring proper belief state 
simulations results areas research discussed 
convergence greedy exploration reinforcement learning algorithms known parameterized function approximator represent value function adjust weights incrementally learning 
examples include learning sarsa advantage learning 
simple mdps original form algorithms fails converge summarized table 
cases algorithms guaranteed converge reasonable assumptions decaying learning rates 
cases known counterexamples diverge oscillate best worst possible policies different values 
happen infinite training time slowly decreasing learning rates baird gordon 
columns changed converge modified form algorithm residual form baird 
possible learning fixed training distribution rarely practical 
large problems useful explore policy usually greedy respect current value function changes value function changes 
case rightmost column chart current convergence guarantees 
way guarantee convergence columns modify algorithm performing stochastic gradient descent average error function average weighted state visitation frequencies current usually greedy policy 
weighting changes policy changes 
appear gradient difficult compute 
consider learning exploring boltzman distribution usually greedy respect learned function 
difficult calculate gradients changing single weight change values changing single value change action choice probabilities state changing single action choice probability may affect frequency state mdp visited 
difficult 
surprisingly unbiased estimates gradients visitation distributions respect weights calculated quickly resulting algorithms put case table 
derivation vaps equation consider sequence transitions observed particular stochastic policy mdp 
sequence states actions reinforcements time performing action state yields reinforcement transition state table 
current convergence results incremental value rl algorithms 
residual algorithms changed columns new algorithms proposed change fixed distribution policy fixed distribution distribution lookup table markov chain linear nonlinear lookup table mdp linear nonlinear lookup table pomdp linear nonlinear convergence guaranteed counterexample known diverges oscillates best worst possible policies 
stochastic policy may function vector weights assume mdp single start state named mdp terminal states terminal state set possible sequences time error function calculates error time step squared bellman residual time error occurring time function weights smooth function weights 
consider period time starting time probability sequence occurs 
probabilities expected squared period length finite 
expected total error period expectation weighted state visitation frequencies generated policy traj time ends period note line particular error added sequence starts terms weighted probability complete trajectory starts sum probabilities trajectories start simply probability observed period assumed eventually probability 
second line equals 
third line probability sequence factor function probability smooth function weights nonzero 
partial derivative respect particular element weight vector ln space limited may clear short sketch derivation summing entire period give unbiased estimate expected total error period 
incremental algorithm perform stochastic gradient descent weight update left side table summation previous time steps replaced trace weight 
algorithm general previously published algorithms form function previous states actions reinforcements just current reinforcement 
allows vaps value policy search 
algorithm proposed special case vaps equation left side table 
note model needed algorithm 
probability needed algorithm policy transition probability mdp 
stochastic gradient descent update rule correct observed transitions sampled trajectories current stochastic policy 
smooth functions vector bounded 
algorithm simple generates large class different algorithms depending choice trace reset zero 
single sequence sampled current policy sum dw sequence give unbiased estimate true gradient finite variance 
learning weight updates trial weights stay bounded region learning rate approaches zero converge probability 
adding weight decay term constant times norm weight vector prevent weight divergence small initial learning rates 
guarantee global minimum general function approximators converge 
true backprop 
instantiating vaps algorithm reinforcement learning algorithms value try learn value function satisfies bellman equation 
examples learning learns value function actor critic algorithms learn value function policy greedy respect td learns value function rewards 
algorithms pure policy search algorithms directly learn policy returns high rewards 
include reinforce williams backprop time learning automata genetic algorithms 
algorithms proposed combine approaches perform value policy search vaps 
general vaps equation instantiated choosing expression bellman residual yielding value reinforcement yielding policy search linear combination yielding value policy search 
single vaps update rule left side table generates variety different types algorithms described sections 
reducing mean squared residual trial mdp terminal states trial time start terminal state reached possible minimize expected total error trial resetting trace zero start trial 
convergent form sarsa learning incremental value iteration advantage learning generated choosing squared bellman residual shown right side table 
case expected value taken possible table 
general vaps algorithm left instantiations right 
single algorithm includes value policy search approaches combination gives guaranteed convergence case 
sarsa max learning max max advantage ln max iteration value sarsa policy sarsa triplets policy smooth nonzero function weights 
greedy policy chooses greedy action probability chooses uniformly 
cause discontinuity gradient values state equal 
policy approaches greedy positive temperature approaches zero number possible actions state 
instance table value iteration gradient estimated independent unbiased estimates expected value 
example sarsa sarsa gf estimate true gradient 
algorithm described baird retains guaranteed convergence may learn quickly pure gradient descent values note gradient time uses primed variables 
means new state action time generated independently state action time 
course mdp deterministic primed variables unprimed 
mdp nondeterministic model known model evaluated additional time get state 
model known choices 
model learned past data evaluated give independent sample 
second issue ignored simply reusing unprimed variables place primed variables 
may affect quality learned function depending random mdp doesn convergence acceptable approximation practice 
third past transitions recorded primed variables searching times seen randomly choosing transitions successor state action primed variables 
equivalent learning certainty equivalence model sampling special case choice 
extremely large state action spaces starting states give result practice simply reusing unprimed variables primed variables 
note weights effect policy algorithms reduce standard residual algorithms baird 
possible reduce mean squared residual step trial 
done making period lengths independent policy minimizing error period minimize error step 
example period defined steps traces reset state returned start state 
note state action pair positive chance seen steps just solving finite horizon problem 
solving discounted infinite horizon problem reducing bellman residual state 
weighting residuals determined happens steps 
different problems solved vaps algorithm instantiating definition period different ways 
policy search value learning possible add term tries maximize reinforcement directly 
example defined sarsa policy sarsa table trace reset zero terminal state reached 
constant affect expected gradient affect noise distribution discussed williams 
algorithm try learn function satisfies bellman equation just 
directly learns policy minimize expected total discounted reinforcement 
resulting function may close containing true values satisfying bellman equation just give policy 
algorithm tries satisfy bellman equation give greedy policies 
similar modification algorithms table 
special case algorithm reduces reinforce algorithm williams 
reinforce special case gaussian action distributions tresp extensions appear 
case pure policy search particularly interesting need kind model generating independent successors 
algorithms proposed finding policies directly gullapalli various algorithms learning automata theory summarized narendra 
vaps algorithms proposed appears unifying approaches reinforcement learning finding value function approximates bellman equation solution directly optimizes greedy policy 
shows simulation results combined algorithm 
run said learned greedy policy optimal consecutive trials 
graph shows average plot runs different initial random weights learning rate optimized separately value 
leaving state leaving state entering 

algorithm modified learning table exploration equation 
states share parameters ordinary sarsa greedy learning converge shown gordon 
pure value new algorithm converges course learn optimal policy start state values learn equal 
pure policy search learning converges optimality slowly value function caching results long sequence states near 
combining approaches new algorithm learns quickly 
interesting vaps algorithms described sections applied directly partially observable markov decision process pomdp true state hidden available time step start beta trials 
pomdp number trials needed learn vs combination policy search value rl outperforms 
ambiguous observation function true state 
normally algorithm sarsa guaranteed convergence applied mdp 
vaps algorithms converge cases 
new algorithm 
special cases give new algorithms similar learning sarsa advantage learning guaranteed convergence wider range problems previously possible including pomdps 
time guaranteed converge exploration policy changes learning 
special cases allow new approaches reinforcement learning tradeoff satisfying bellman equation improving greedy policy 
mdp simulation showed combined algorithm learned quickly approach 
unified theory unifying time value reinforcement learning theoretical interest practical value simulations performed 
research unified framework may able empirically analytically address old question better learn value functions better learn policy directly 
may shed light new question best 
acknowledgments research sponsored part air force 
baird 

residual algorithms reinforcement learning function approximation 
armand prieditis stuart russell eds 
machine learning proceedings twelfth international conference july morgan kaufman publishers san francisco ca 
gordon 

stable fitted reinforcement learning 
tesauro mozer hasselmo eds advances neural information processing systems pp 

mit press cambridge ma 
gullapalli 

reinforcement learning application control 
dissertation coins technical report university massachusetts amherst ma 
kaelbling littman cassandra planning acting partially observable stochastic domains 
artificial intelligence appear 
available www cs brown edu people 


simulation optimization markov decision processes 
thesis lids th massachusetts institute technology 
mccallum reinforcement learning selective perception hidden state 
dissertation department computer science university rochester rochester ny 
narendra 

learning automata 
prentice hall englewood cliffs nj 
tresp 
missing noisy data nonlinear time series prediction 
proceedings neural networks signal processing girosi makhoul wilson eds ieee signal processing society new york new york pp 

williams 

theory reinforcement learning connectionist systems 
technical report nu ccs northeastern university boston ma 
