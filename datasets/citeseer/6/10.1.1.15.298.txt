comparative study discretization methods naive bayes classifiers ying yang geoffrey webb school computing mathematics deakin university vic australia deakin edu au school computer science software engineering monash university vic australia geoff webb mail monash edu au 
discretization popular approach handling numeric attributes machine learning 
argue requirements effective discretization differ naive bayes learning learning algorithms 
evaluate effectiveness naive bayes classifiers discretization methods equal width discretization equal frequency discretization efd fuzzy discretization fd entropy minimization discretization emd iterative discretization id proportional interval discretization lazy discretization ld discretization ndd weighted proportional interval discretization 
general naive bayes classifiers trained data preprocessed ld ndd achieve lower classification error trained data preprocessed discretization methods 
ld scale large data 
study leads new discretization method weighted non disjoint discretization combines ndd advantages 
experiments show rival discretization methods best helps naive bayes classifiers reduce average classification error 
classification tasks involve numeric attributes 
naive bayes classifiers numeric attributes preprocessed discretization classification performance tends better numeric attributes discretized assumed follow normal distribution 
numeric attribute categorical attribute created 
value corresponds interval values training classifier 
proceedings pacific rim knowledge acquisition workshop tokyo japan pp 

discretization methods employed naive bayes classifiers thorough comparisons methods rarely carried 
furthermore methods tested small datasets hundreds instances 
large datasets high dimensional attribute spaces huge numbers instances increasingly real world applications study methods performance large datasets necessary desirable 
discretization methods included comparative study designed especially naive bayes classifiers practice naive bayes classifiers 
seek answers questions experimental evidence strengths weaknesses existing discretization methods applied naive bayes classifiers 
extent discretization method help reduce naive bayes classifiers classification error 
improving performance naive bayes classifiers particular significance efficiency accuracy led widespread deployment 
follows section gives overview naive bayes classifiers 
section introduces discretization naive bayes classifiers 
section describes discretization method discusses suitability naive bayes classifiers 
section compares complexities methods 
section presents experimental results 
section proposes weighted non disjoint discretization inspired comparative study 
section provides 
naive bayes classifiers classification learning instance described vector attribute values class take value predefined set values 
training data set instances known classes provided 
test instance 
learner asked predict class test instance evidence provided training data 
define random variable denoting class instance xk vector random variables denoting observed attribute values instance particular class xk particular observed attribute value vector particular instance shorthand xk xk 
expected classification error minimized choosing bayes theorem calculate probability 
denominator invariant classes affect final choice dropped 
need estimated training data 
unfortunately usually unseen instance appear training data may possible directly estimate 
simplification attributes xk conditionally independent class xi xi xi xi 
combining estimate probability xi xi 
classifiers called naive bayes classifiers 
naive bayes classifiers simple efficient robust noisy data 
limitation attribute independence assumption violated real world 
domingos pazzani suggest limitation impact expected classification zero loss function sign probability estimation classification accuracy remain high probability estimation poor 
discretization naive bayes classifiers attribute categorical numeric 
values categorical attribute discrete 
values numeric attribute discrete continuous 
xi xi modelled real number denoting probability attribute xi take particular value xi class assumes attribute values discrete finite number may possible assign probability single value attribute infinite number values 
attributes finite large number values training instances value advisable aggregate range values single value purpose estimating probabilities 
keeping normal terminology research area call conversion numeric attribute categorical discretization irrespective numeric attribute discrete continuous 
categorical attribute takes small number values 
class label 
accordingly xi xi estimated reasonable accuracy frequency instances frequency instances xi xi training data 
experiment laplace estimate estimate nc nc number instances satisfying number training instances number classes 
estimate estimate xi xi nci nc nci number instances satisfying xi xi nc number instances satisfying prior probability xi xi estimated laplace estimate 
continuous numeric attribute xi large infinite number values discrete numeric attributes suppose si value space xi particular xi si probability xi xi arbitrarily close 
probability distribution xi completely determined density function satisfies 
xi xi si 
si xi dxi 
bi ai xi dxi ai xi bi ai bi si 
xi xi estimated 
real world data usually unknown 
discretization categorical attribute formed xi 
value corresponds interval ai bi xi 
xi ai bi xi xi estimated xi xi xi 
xi training classifiers estimated categorical attributes probability estimation xi bounded specific distribution assumption 
difference may cause information loss 
xi xi discretization methods developed learning forms classifiers requirements effective discretization differ naive bayes learning learning contexts 
naive bayes classifiers probabilistic selecting class highest probability instance 
plausible important form intervals dominated single class naive bayes classifiers decision trees decision rules 
discretization methods pursue pure intervals containing instances class suit naive bayes classifiers 
naive bayes classifiers deem attributes conditionally independent attribute combinations predictors 
need calculate joint probabilities multiple attribute values 
discretization methods seek capture inter dependencies attributes applicable naive bayes classifiers 
naive bayes classifiers required discretization result accurate estimation substituting categorical numeric xi 
required discretization efficient order maintain naive bayes classifiers desirable computational efficiency 
discretization methods discretizing numeric attribute xi suppose training instances value xi known minimum maximum value vmin vmax respectively discretization method sorts values ascending order 
methods differ follows 
equal width discretization divides number line vmin vmax intervals equal width :10.1.1.47.6141
intervals width vmax vmin cut points vmin vmin vmin user predefined parameter set experiments 
equal frequency discretization efd efd divides sorted values intervals interval contains approximately number training instances :10.1.1.47.6141
interval contains possibly duplicated adjacent values 
user predefined parameter set experiments 
efd potentially suffer attribute information loss determined properties training data 
may deemed simplistic methods surprisingly naive bayes classifiers 
reason suggested discretization usually assumes discretized attributes dirichlet priors perfect aggregation ensure naive bayes discretization appropriately approximates distribution numeric attribute 
fuzzy discretization fd versions fuzzy discretization proposed kononenko naive bayes classifiers 
differ estimation ai xi bi obtained 
space limits version experiments best reduces classification error 
fd initially forms equal width intervals ai bi 
fd estimates ai xi bi training instances instances value xi ai bi 
influence training instance value xi ai bi assumed normally distributed mean value equal proportional bi ai dx 
parameter algorithm control fuzziness interval bounds 
set equal vmax vmin suppose nc training setting chosen shown achieve best results kononenko experiments 
instances known value xi class influence vj ai bi nc ai xi bi ai xi bi nc vj 
idea fuzzy discretization small variation value numeric attribute small effects attribute probabilities non fuzzy discretization slight difference values cut point drastic effects estimated probabilities 
training instances influence interval follow normal distribution fd performance degrade 
number initial intervals predefined parameter set experiments 
entropy minimization discretization emd emd evaluates candidate cut point midpoint successive pair sorted values 
evaluating candidate cut point data discretized intervals resulting class information entropy calculated 
binary discretization determined selecting cut point entropy minimal candidates 
binary discretization applied recursively selecting best cut point 
minimum description length criterion mdl applied decide discretization 
naive bayes classifiers emd developed context top induction decision trees 
uses mdl termination condition form categorical attributes values 
decision tree learning important minimize number values attribute avoid fragmentation problem 
attribute values split attribute result branches receives relatively training instances making difficult select appropriate subsequent tests 
naive bayes learning considers attributes independent class subject fragmentation problem values attribute 
emd bias forming small number intervals may justified naive bayes classifiers decision trees 
emd naive bayes classifiers emd discretizes numeric attribute calculating class information entropy naive bayes classifiers single attribute discretization 
emd form attribute independence assumption discretizing 
risk reinforce attribute independence assumption inherent naive bayes classifiers reducing capability accurately classify context violation attribute independence assumption 
iterative discretization id id initially forms set intervals med iteratively adjusts intervals minimize naive bayes classifiers classification error training data 
defines operators merge contiguous intervals split interval intervals introducing new cut point midway pair contiguous values interval 
loop iteration numeric attribute id applies operators possible ways current set intervals estimates classification error adjustment leave cross validation 
adjustment lowest error retained 
loop stops adjustment reduces error 
disadvantage id results iterative nature 
id discretizes numeric attribute respect error training data 
training data large possible adjustments applying operators tend immense 
consequently repetitions leave cross validation prohibitive id infeasible terms time consumption 
experiments include large datasets introduce id integrality study implementing 
proportional interval discretization adjusts discretization bias variance tuning interval size number adjusts naive bayes probability estimation bias variance achieve lower classification error 
idea discretization bias variance relate interval size interval number 
larger interval size smaller interval number lower variance higher bias 
converse smaller interval size larger interval number lower bias higher variance 
lower learning error achieved tuning interval size number find trade bias variance 
suppose desired interval size desired interval number employs calculate 
discretizes sorted values intervals size gives equal weight discretization bias reduction variance reduction setting interval size equal interval number 
furthermore sets interval size number proportional training data size 
quantity training data increases discretization bias variance decrease 
means greater capacity take advantage additional information inherent large volumes training data 
previous experiments showed significantly reduced classification error larger datasets 
sub optimal smaller datasets small tends produce number intervals small size offer data reliable probability estimation resulting high variance inferior performance naive bayes classifiers 
strict definition smaller larger datasets 
research deems datasets size larger smaller datasets larger datasets 
lazy discretization ld ld defers discretization classification time estimating xi xi attribute test instance :10.1.1.34.7334
waits test instance determine cut points xi 
value xi test instance selects pair cut points value middle corresponding interval size created efd 
arbitrary number justified superior value 
ld tends high memory computational requirements lazy methodology 
eager approaches carry discretization prior naive bayes learning 
keep training instances estimate probabilities training time discard classification time 
contrast ld needs keep training instances classification time 
demands high memory training data large 
large number instances need classified ld incur large computational overheads estimate probabilities classification time 
ld achieves comparable performance efd emd high memory computational overheads impede feasible implementation classification tasks large training test data 
experiments implement ld interval size equal created shown outperform original implementation 
non disjoint discretization ndd ndd forms overlapping intervals xi locating value xi middle corresponding interval ai bi 
idea ndd substituting ai bi xi distinguishing information xi probability estimation reliable xi middle ai bi xi close boundary ai bi 
ld embodies idea context lazy discretization 
ndd employs eager approach performing discretization prior naive bayes learning 
ndd bases interval size strategy 
illustrates procedure 
calculated ndd identifies sorted values atomic intervals size equal 
interval formed set consecutive atomic intervals kth interval ak bk satisfies ak bk theoretically odd number acceptable long number atomic intervals grouped probability estimation 
simplicity take demonstration 
value assigned interval index atomic interval case assigned interval case assigned interval 
result case falling atomic interval xi middle ai bi 
atomic interval interval fig 

atomic intervals compose actual intervals weighted proportional interval discretization improved version 
credible smaller datasets variance reduction contribute lower naive bayes learning error bias reduction 
fewer intervals containing instances greater utility 
accordingly weighs discretization variance reduction bias reduction setting minimum interval size reliable probability estimation 
training data increase interval size minimum interval number increase 
definitions suppose minimum interval size employs calculate 
set commonly held minimum sample draw statistical inferences 
strategy mitigate disadvantage smaller datasets establishing suitable bias variance trade retains advantage larger datasets allowing additional training data reduce bias variance 
algorithm complexity comparison suppose number training instances classes respectively 
consider instances known value numeric attribute discretized 
efd fd ndd dominated sorting 
complexities order log 
emd sorting operation complexity log 
goes training instances maximum log times recursively applying binary division find cut points 
time estimate candidate cut points 
candidate point probabilities classes estimated 
complexity operation mn log dominates complexity sorting resulting complexity order mn log 
id operators possible ways adjust iteration 
adjustment leave cross validation complexity order number attributes 
iteration repeats times error reduction leave crossvalidation complexity id order mvu 
ld performs discretization separately test instance complexity nl number test instances 
efd fd ndd low complexity 
emd complexity higher 
ld tends high complexity testing data large 
id complexity prohibitively high training data large 
experimental validation experimental setting run experiments natural datasets uci machine learning repository kdd archive 
datasets vary extensively number instances dimension attribute space 
table describes dataset including number instances size numeric attributes num categorical attributes cat 
classes class 
dataset implement naive bayes learning conducting trial fold cross validation 
fold training data separately discretized efd fd med ld ndd 
intervals formed separately applied test data 
experimental results recorded average classification error listed table percentage incorrect predictions naive bayes classifiers test trials 
experimental statistics statistics employed evaluate experimental results 
mean error 
mean errors datasets 
provides gross indication relative performance 
debatable errors different datasets averaging errors datasets meaningful 
low average error indicative tendency low errors individual datasets 
table 
experimental datasets classification error classification error dataset size num 
cat 
class efd fd emd ld ndd labor negotiations echocardiogram pittsburgh bridges material iris hepatitis wine recognition flag sonar glass identification heart disease cleveland haberman ecoli liver disorders ionosphere horse colic credit screening australia breast cancer wisconsin pima indians diabetes vehicle annealing vowel context german multiple features hypothyroid satimage musk pioneer handwritten digits australian sign language letter recognition adult la census income forest gm geometric mean error ratio 
method explained webb 
allows relative difficulty error reduction different datasets reliable mean ratio errors datasets 
win lose tie record 
values respectively number datasets method obtains lower higher equal classification error compared alternative method 
tailed sign test applied record 
test result significantly low critical level reasonable conclude outcome obtained chance record wins losses represents systematic underlying advantage winning method respect type datasets studied 
experimental results analysis latest discretization technique proposed naive bayes classifiers 
claimed outperform previous techniques efd fd emd 
comparison done ld ndd wp kid 
analysis take benchmark study performance methods 
dataset forest excluded calculations mean error geometric mean error ratio win lose tie records involving ld completed ld tolerable time 
row table presents mean error method 
ld achieves lowest mean error achieving similar score 
ge row table presents geometric mean ratio method 
results ld larger 
suggests ld enjoy advantage terms error reduction type datasets studied research 
respect win lose tie records methods wp kid results listed table 
sign test shows efd fd emd worse error reduction frequency significant level 
ld ndd competitive performance compared 
table 
win lose tie records alternative methods efd fd emd ld ndd win lose tie sign test ndd ld similar error performance similar terms locating attribute value middle discretized interval 
win lose tie record ndd ld resulting sign test equal 
view computation time ndd overwhelmingly superior ld 
table lists computation time training testing naive bayes classifier data preprocessed ndd ld respectively fold fold cross validation largest datasets ndd faster ld 
forest ld able obtain classification result running seconds 
allowing feasibility real world classification tasks meaningless keep ld running 
stopped process resulting precise record running time 
table 
computation time fold seconds adult la census income forest ndd ld interesting comparison efd 
said vulnerable outliers may drastically skew value range 
experiments win lose tie record efd means performs efd better 
naive bayes classifiers take attributes account simultaneously 
impact wrong discretization attribute absorbed attributes zero loss 
observation advantage efd apparent training data increasing 
reason training data available impact outlier 
discussion experiments show ld ndd better alternatives reducing naive bayes classifiers classification error 
ld infeasible context large datasets 
ndd performance attributed fact focus accurately estimating naive bayes probabilities 
ndd retain distinguishing information value discretized properly adjust learning variance bias 
consequent interesting question happen combine ndd 
possible obtain discretization method better reducing naive bayes classifiers classification error 
name new method weighted non disjoint discretization 
follows ndd terms combining atomic intervals interval 
interval size equal produced ndd interval size equal produced 
implement way discretization methods record resulting classification error column table 
experiments show achieves lowest gm 
win lose tie records previous methods listed table 
achieves lower classification error methods ld ndd frequency significant level 
statistically significant delivers lower classification error comparison ld ndd 
overcomes computational limitation ld complexity low ndd 
table 
win lose tie records alternative methods efd fd emd ld ndd win lose tie sign test evaluation comparison discretization methods naive bayes classifiers 
discovered terms reducing classification error ld ndd perform best 
ld lazy methodology impedes scaling large data 
analysis leads new discretization method combines ndd advantages 
experiments demonstrate performs best existing techniques minimizing classification error 
outstanding performance achieved computational overheads hamper application lazy discretization 

cercone discretization continuous attributes learning classification rules 
proceedings third pacific asia conference methodologies knowledge discovery data mining pp 


bay uci kdd archive kdd ics uci edu 
department information computer science university california irvine 

bay multivariate discretization continuous variables set mining 
proceedings sixth acm sigkdd international conference knowledge discovery data mining pp 


blake merz uci repository machine learning databases www ics uci edu mlearn mlrepository html 
department information computer science university california irvine 

catlett changing continuous attributes ordered discrete attributes 
proceedings european working session learning pp 


cestnik estimating probabilities crucial task machine learning 
proceedings european conference artificial intelligence pp 


busse global discretization continuous attributes preprocessing machine learning 
international journal approximate reasoning 

domingos pazzani optimality simple bayesian classifier zero loss 
machine learning 

dougherty kohavi sahami supervised unsupervised discretization continuous features 
proceedings twelfth international conference machine learning pp 


fayyad irani multi interval discretization attributes classification learning 
proceedings thirteenth international joint conference artificial intelligence pp 


freitas speeding knowledge discovery large relational databases means new discretization algorithm 
advances databases proceedings fourteenth british national databases pp 


friedman bias variance loss curse dimensionality 
data mining knowledge discovery 

gama soares dynamic discretization continuous attributes 
proceedings sixth american conference ai pp 


ho scott zeta global method discretization continuous variables 
proceedings third international conference knowledge discovery data mining pp 


holte simple classification rules perform commonly datasets 
machine learning 

hsu huang wong discretization works naive bayesian classifiers 
proceedings seventeenth international conference machine learning pp 


john langley estimating continuous distributions bayesian classifiers 
proceedings eleventh conference uncertainty artificial intelligence pp 


johnson bhattacharyya statistics principles methods 
john wiley sons publisher 

kerber chimerge discretization numeric attributes 
national conference artificial intelligence aaai press pp 


kononenko naive bayesian classifier continuous attributes 
informatica 

kononenko inductive bayesian learning medical diagnosis 
applied artificial intelligence 

evolutionary algorithm multivariate discretization decision rule induction 
proceedings european conference principles data mining knowledge discovery pp 


widmer relative unsupervised discretization association rule mining 
proceedings fourth european conference principles practice knowledge discovery databases 

monti cooper multivariate discretization method learning bayesian networks mixed data 
proceedings fourteenth conference uncertainty ai pp 


pazzani iterative improvement approach discretization numeric attributes bayesian classifiers 
proceedings international conference knowledge discovery data mining 

multi interval discretization methods decision tree learning 
advances pattern recognition joint iapr international workshops spr pp 


provost aronis scaling machine learning massive parallelism 
machine learning 

quinlan programs machine learning 
morgan kaufmann publishers 

class driven statistical discretization continuous attributes extended 
european conference machine learning springer 

probability statistics engineers fourth ed 
duxbury press 

wang liu concurrent discretization multiple attributes 
pacific rim international conference artificial intelligence pp 


webb technique combining boosting 
machine learning 

yang webb proportional interval discretization naive bayes classifiers 
proceedings twelfth european conference machine learning pp 


yang webb non disjoint discretization naive bayes classifiers 
proceedings nineteenth international conference machine learning pp 


yang webb weighted proportional interval discretization naive bayes classifiers 
submitted ieee international conference data mining 
