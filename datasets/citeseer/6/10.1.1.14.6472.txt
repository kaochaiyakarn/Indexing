tagging english text probabilistic model bernard merialdo institut eurecom experiments probabilistic model tag english text assign word correct tag part speech context sentence 
main novelty experiments untagged text training model 
simple model looking best way estimate parameters model depending kind amount training data provided 
approaches particular compared combined text tagged hand computing relative frequency counts text tags training model hidden markov process maximum likelihood principle 
experiments show best training obtained tagged text possible 
show maximum likelihood training procedure routinely estimate hidden markov models parameters training data necessarily improve tagging accuracy 
fact generally degrade accuracy limited amount hand tagged text available 

lot effort devoted past problem tagging text assigning word correct tag part speech context sentence 
main approaches generally considered rule klein simmons martin brill probabilistic bahl mercer tannenbaum carstensen marshall leech garside atwell merialdo derose church beale marcken merialdo cutting 
proposed neural networks mackie anderson nakamura shikano 
multimedia communications department institut eurecom route des cedex france merialdo eurecom fr 
carried author visitor continuous speech recognition group ibm watson research center yorktown heights ny usa 
part material included ieee international conference acoustics speech signal processing toronto canada may 
association computational linguistics computational linguistics volume number different approaches common points emerged word tags possible list dictionary morphological analysis word 
word possible tags correct tag generally chosen local context contextual rules define valid sequences tags 
rules may priorities selection rules apply 
kinds considerations fit nicely inside probabilistic formulation problem beale garside leech offers advantages sound theoretical framework provided approximations clear probabilities provide straightforward way disambiguate probabilities estimated automatically data 
particular probabilistic model model results experiments involving different ways estimate parameters intention maximizing ability model tag text accurately 
particular interested way best untagged text training model 

problem tagging suppose user defined set tags attached words 
consider sentence sequence tags ht length 
call pair alignment 
say word wi assigned tag ti alignment 
assume tags linguistic meaning user possible alignments sentence single correct grammatical point view 
tagging procedure procedure selects sequence tags defines alignment sentence 
measures quality tagging procedure sentence level word level percentage sentences correctly tagged percentage words correctly tagged bernard merialdo tagging english text probabilistic model practice performance sentence level generally lower performance word level words tagged correctly sentence tagged correctly 
standard measure literature performance word level considered 

probabilistic formulation probabilistic formulation tagging problem assume alignments generated probabilistic model probability distribution case depending criterion choose evaluation optimal tagging procedure follows evaluation sentence level choose probable sequence tags sentence argmax argmax call procedure viterbi tagging 
achieved dynamic programming scheme 
evaluation word level choose probable tag word sentence argmax argmax ti ti tag assigned word wi tagging procedure context sentence call procedure maximum likelihood ml tagging 
interesting note commonly method viterbi tagging see derose church optimal method evaluation word level 
reasons preference presumably viterbi tagging simpler implement ml tagging requires computation asymptotic complexity viterbi tagging provides best interpretation sentence linguistically appealing ml tagging may produce sequences tags linguistically impossible choice tag depends contexts taken 
experiments show viterbi ml tagging result similar performance 
computational linguistics volume number course real tags generated probabilistic model able determine model exactly cause practical limitations 
models construct approximations ideal model exist 
happens despite assumptions approximations models able perform reasonably 

model mathematical expression ii wi 
wi lti lti ti 
wi lti tri pos tri hk model approximations probability tag past depends tags ti 
wi lti ti ti ati probability word past depends tag wi 
wi lti lti wi ti name hk model comes notation chosen probabilities 
order define model completely specify values probabilities 
nw size vocabulary nt number different tags nt nt nt values probabilities nw nt values probabilities 
probability distributions sum nt nt equations constrain values probabilities nt equations constrain values probabilities 
total number free parameters nw nt nt nt nt 
note number grows linearly respect size vocabulary model attractive vocabularies large size 
model allows word tag 
dictionary specifies list possible tags word information constrain model valid tag word sure nonzero values probabilities possible pairs word tag allowed dictionary 
bernard merialdo tagging english text probabilistic model 
training model consider different types training relative frequency rf training maximum likelihood ml training done forward backward fb algorithm 
relative frequency training tagged text available compute number times word appears tag number times se quence tl appears text 
estimate probabilities computing relative frequencies corresponding events data tb tl tg tl tl estimates assign probability zero sequence tags occur training data 
sequences may occur consider texts 
probability zero sequence creates problems alignment contains sequence get probability zero 
may happen sequences words alignments get probability zero model useless sentences 
avoid interpolate distributions uniform distributions consider interpolated model defined hinter tl tl tl tt ku number words tag interpolation coefficient computed deleted interpolation algorithm jelinek mercer possible coefficients interpolation interpolation 
value coefficient expected increase increase size training text rela tive frequencies reliable 
interpolation procedure called smoothing 
smoothing performed follows quantity tagged text training data computation relative frequencies 
called held data 
coefficient chosen maximize probability emission held data interpolated model 
computational linguistics volume number maximization performed standard forward backward fb baum welch algorithm baum jelinek bahl jelinek mercer considering transition probabilities markov model 
noted complicated interpolation schemes possible 
example different coefficients depending count intuition relative frequencies trusted count high 
interpolate models different orders 
smoothing achieved procedures interpolation 
example backing strategy proposed katz 
maximum likelihood training model possible compute probability sequence words model zp sum taken possible alignments 
maximum likelihood ml training finds model maximizes probability training text max ii pm product taken sentences training text 
problem training hidden markov model hidden sequence tags hidden 
known solution problem forward backward fb baum welch algorithm baum jelinek bahl jelinek mercer iteratively constructs sequence models improve probability training data 
advantage approach require tagging text assumption correct model tags best predict word sequence 

tagging algorithms viterbi algorithm easily implemented dynamic programming scheme bellman 
maximum likelihood algorithm appears complex glance involves computing sum probabilities large number alignments 
case hidden markov model computations arranged way similar fb algorithm amount computation needed linear length sentence baum 

experiments main objective compare rf ml training 
done section 
take advantage environment set perform experiments described section theoretical interest bernard merialdo tagging english text probabilistic model table rf training sentences viterbi tagging 
training data interpolation nb errors correct sentences coefficient words tags bring improvement practice 
concerns difference viterbi ml tagging concerns constraints training 
shall describing textual data presenting different tagging experiments various training tagging methods 
text data treebank data described beale 
contains sentences words associated press 
sentences tagged manually unit computer research english language university lancaster collaboration ibm 
ibm speech recognition group yorktown heights usa 
fact sentences tagged parsed 
information contained parse 
treebank different tags 
tags projected smaller system tags designed peter brown see appendix 
results quoted refer smaller system 
built dictionary indicates list possible tags word words occur text word tags assigned text 
sense optimal dictionary data word possible tags language tags text 
separated data parts set tagged sentences training data build models set tagged sentences words test data test quality models 
basic experiments rf training viterbi tagging experiment extracted tagged sentences training data 
computed relative frequencies sentences built smoothed model procedure previously described 
model tag test sentences 
experimented different values indicate value interpolation coefficient number percentage correctly tagged words 
results indicated table 
computational linguistics volume number expected size training increases interpolation coefficient creases quality tagging improves 
model uniform distributions 
case alignments sentence equally probable choice correct tag just choice random 
percentage correct tags relatively high half words text single possible tag mistake words quarter words text possible tags average random choice correct time 
note behavior obviously dependent system tags 
noted reasonable results obtained quite rapidly 
tagged sentences words tagging error rate 
times data tagged sentences provides improvement 
ml training viterbi tagging ml training take training data available sentences word sequences associated tags compute initial model described 
possible fb algorithm able train model word sequence 
experiment took model uniform distributions initial 
constraints model came values set zero tag possible word dictionary 
ran fb algorithm evaluated quality tagging 
results shown 
perplexity measure average branching factor probabilistic models 
shows ml training improves perplexity model reduces tagging error rate 
error rate remains relatively high level higher obtained rf training tagged sentences 
having shown ml training able improve uniform model wanted know able improve accurate models 
took initial model models obtained previously rf training performed ml training training word sequences 
results shown graphically numerically table 
results show tagged data model obtained relative frequency maximum likelihood training able im prove 
amount tagged data increases models obtained relative frequency accurate maximum likelihood training improves initial iterations deteriorates 
tagged sentences iteration ml training degrades tagging 
number course dependent particular system tags kind text experiment 
results call comments 
ml training theoretically sound pro cedure routinely successfully speech recognition es parameters hidden markov models describe relations sequences phonemes speech signal 
ml training guaranteed improve perplexity perplexity necessarily related tagging accuracy possible improve degrading 
case tagging bernard merialdo tagging english text probabilistic model oo ii iterations ml training uniform distributions 
table ml training various initial points 
iterations number tagged sentences initial model iter correct tags words ml words computational linguistics volume number 



zz 
iterations ml training various initial points top line corresponds io bottom line 
relations words tags precise relations tween phonemes speech signals correct correspondence harder define precisely 
characteristics ml training effect smoothing probabilities probably suited speech tagging 
extra experiments viterbi versus ml tagging experiment considered initial model built rf training training data successive models created iterations ml training 
models performed viterbi tagging ml tagging test data evaluated compared number tagging errors produced methods 
results shown table 
models obtained different iterations related draw strong definite superiority tagging procedure 
difference error rate small shows choice tagging procedure critical kind training material 
constrained ml training suggestion jelinek investigated effect constraining ml training imposing constraints probabilities 
idea comes observation amount training data needed properly estimate model increases number free parameters model 
case little training data adding reasonable constraints shape models looked reduces number free parameters improve quality estimates 
bernard merialdo tagging english text probabilistic model table viterbi vs ml tagging 
tagging errors words iter 
viterbi ml vit 
ml nb nb nb table standard ml vs tw constrained ml training 
tagging errors words iter 
ml tw 
ml nb nb tried different constraints keeps fixed frequent word case frequent words 
call tw constraint 
rationale frequent relative frequency provides estimate training change 
second keeps marginal distribution constant similar reasoning 
call constraint 
tw constraint tw constrained ml training similar standard ml training probabilities changed iteration 
results table show number tagging errors model trained standard tw constrained ml training 
show tw constrained ml training degrades rf training quickly standard ml 
computational linguistics volume number table standard ml vs constrained ml training 
tagging errors words model iter 
ml 
ml nb nb tested happens smaller training data build initial model 
constraint constraint difficult implement previous prob abilities parameters model combination parame ters 
help designed iterative procedure allows likelihood improved preserving values 
sufficient space describe procedure 
greater computational complexity applied model model ti 
wi lti ti ti 
initial model estimated relative frequency training data viterbi tagging 
previous experiment results table show number tagging errors model trained standard constrained ml training 
show constrained ml training degrades rf training quickly standard ml 
tested happens smaller training data build initial model 

results show estimating parameters model counting relative frequencies large amount hand tagged text lead best tagging accuracy 
maximum likelihood training guaranteed improve perplexity necessarily improve tagging accuracy 
experiments ml training degrades performance initial model bad 
preceding results suggest optimal strategy build best possible model tagging get tagged hand text afford bernard merialdo tagging english text probabilistic model compute relative frequencies data build initial model get untagged text afford starting perform forward backward iterations 
iteration evaluate tagging quality new model mi held tagged text 
reached preset number iterations model mi performs worse mi whichever occurs 
acknowledgments peter brown fred jelinek john lafferty robert mercer salim roukos members continuous speech recognition group fruitful discussions 
want referees judicious comments 
bahl mercer robert 

part speech assignment statistical decision algorithm 
ieee international symposium information theory 
ronneby 
bahl jelinek frederick mercer robert 

maximum likelihood approach continuous speech recognition ieee transactions 
baum 

inequality application statistical estimation probabilistic functions markov processes model ecology 
bulletin american mathematicians society 
beale 

probabilistic approach grammatical analysis written english computer 
proceedings second conference european chapter acl geneva switzerland 
beale 

lexicon grammar probabilistic tagging written english 
proceedings th annual meeting association computational linguistics buffalo ny 
bellman 

dynamic programming 
princeton university press 
mackie anderson 

syntactic category disambiguation neural networks 
computer speech language 
brill magerman marcus santorini 

deducing linguistic structure statistics large corpora 
proceedings darpa speech natural language workshop hidden valley pa 
benny 
problems tagging solution 
nordic journal linguistics 
church kenneth 

stochastic parts program noun phrase parser unrestricted text 
ieee proceedings icassp glasgow 
martelli 

experimental evaluation italian language models large dictionary speech recognition 
proceedings european conference speech technology edinburgh 
cutting kupiec pedersen sibun 

practical part speech tagger 
proceedings third conference applied language processing trento italy 

des matrices de precedence par apprentissage 
doctoral dissertation engineering department universite paris france 
derose 

grammatical category disambiguation statistical optimization 
computational linguistics 
anne marie merialdo bernard 
natural language modeling phoneme text transcription 
ieee transactions pattern analysis machine intelligence 
garside leech 

probabilistic parser 
proceedings second conference european chapter acl geneva switzerland 
jelinek frederick 
continuous speech recognition statistical methods 
proceedings ieee 
jelinek frederick mercer robert computational linguistics volume number 
interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice amsterdam 
katz 

estimation probabilities sparse data language model component speech recognizer 
ieee transactions assp 
klein simmons 
grammatical approach grammatical coding english words 
jacm 
leech garside atwell 

automatic grammatical tagging lob corpus 
newsletter international computer archive modern english 
de marcken 

parsing lob corpus 
proceedings acl annual meeting pittsburg pa 
marshall ian 
choice grammatical word class global syntactic analysis tagging words lob corpus 
computers humanities 
merialdo bernard 
tagging text probabilistic model 
ieee proceedings icassp toronto 
nakamura shikano 

study english word category prediction neural networks 
ieee proceedings icassp glasgow 
martin 

dilemma lemmatizer tagger medical abstracts 
proceedings third conference applied language processing trento italy 
alan 

hidden markov models guided tour 
ieee proceedings icassp new york 
tannenbaum carstensen 

stochastic approach grammatical coding english 
communications acm 
bernard merialdo tagging english text probabilistic model appendix list tags possessive marker app possessive adjectives article boundary tag sentence marker ccf coordinating conjunction cs subordinating conjunction ct subordinating conjunctions determiner wh determiner comparative plural determiner determiner singular little determiner plural dat superlative determiner ex existential fw foreign words facto preposition general ics preposition conjunction preposition io preposition adjective small pretty comparative adjective smaller superlative adjective le leading coordinator cardinal number md ordinal number second noun number english singular noun cat man plural noun cats men npr proper noun paris fred nr noun adverb direction south west time tomorrow tuesday non nominative pronoun oneself personal pronoun reflexive computational linguistics volume number pni indefinite pronoun anybody pp possessive pronoun mine pp personal pronoun object pp personal pronoun subject pp personal subject rd person singular sentence 
non terminal punctuation quot quote adverb slowly wh adverb comparative adverb better longer rg degree adverb wh degree adverb comparative degree adverb worse rp adverb serve preposition sign sign ct pre infinitive uh interjection gee vbg infinitive form imperative am vbn vbr vbz vdg doing past participial form past form vd conjugated form infinitive conjugated form having past participial form past form bernard merialdo tagging english text model vh conjugated form conjugated form vm modals ought non aux verb ing past participial form non aux verb non aux verb vv non third person singular form non aux verb infinitive third person singular form non aux verb xx 
