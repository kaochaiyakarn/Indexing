ieee transactions computer aided design integrated circuits systems vol 
february shared buffer implementations signal processing systems lifetime analysis techniques praveen murthy member ieee bhattacharyya member ieee proliferation block diagram environments specifying prototyping digital signal processing dsp systems 
include tools academia ptolemy commercial tools angeles design systems signal processing system spw cadence synopsys 
block diagram languages environments usually dataflow semantics various subsets dataflow proven matches expressing modeling signal processing systems 
particular synchronous dataflow sdf particularly match expressing multirate signal processing systems 
key problems arises synthesis sdf specification scheduling 
past scheduling sdf focused optimization program memory buffer memory model exploit sharing opportunities 
build previously developed analysis optimization framework looped schedules formally tackle problem generating optimally compact schedules sdf graphs 
develop techniques computing optimally compact schedules manner attempt minimize buffering memory assumption buffers shared 
results schedules data memory usage drastically lower methods past achieved 
method lifetime analysis develop model buffer lifetimes sdf graphs develop scheduling algorithms attempt generate schedules minimize maximum number live tokens particular buffer lifetime model 
develop efficient algorithms extracting relevant lifetimes sdf schedule 
known firstfit heuristic packing arrays efficiently memory 
report extensive experimental results applying techniques practical sdf systems show improvements average previous techniques systems exhibiting improvement previous techniques 
index terms block diagram compiler dsp software synthesis dynamic programming dynamic storage allocation lifetime analysis loop fusion memory allocation static scheduling synchronous dataflow weighted interval graph coloring 
block diagram environments proving increasingly popular developing digital signal processing dsp 
reasons popularity block diagram languages visual intuitive engineers naturally conceptualizing systems block manuscript received may revised october 
bhattacharyya supported part national science foundation contract 
recommended associate editor 
murthy angeles design systems san jose ca usa mail angeles com 
bhattacharyya university maryland college park md usa mail ssb eng umd edu 
publisher item identifier 
ieee diagrams block diagram languages promote software reuse encapsulating designs modular reusable components languages models computation strong formal properties enabling easier faster development bug free programs 
block diagram specifications desirable property systems enable synthesis tool exploit concurrency parallelism available system level 
block diagram environment user connects various blocks drawn library form system interest 
simulation blocks typically written high level language hll software synthesis technique typically inline code generation schedule generated code generator steps schedule substitutes code actor encounters schedule 
code actor may types 
may hll code obtained actor simulation library 
code may compiled appropriate target code may hand optimized code targeted particular target implementation 
programmable dsps means actors implement functionality hand optimized assembly language segments 
code generator stitching code entire system simply assembles resulting machine code run dsp 
technique generally efficient programmable dsps lack efficient hll dsp compilers 
hardware synthesis similar approach taken blocks implementing functionality hardware description language behavioral hardware language vhdl 
generated vhdl description behavioral synthesis tools generate register transfer level rtl description system compiled hardware logic synthesis layout tools 
hll compilers dsps inadequate past 
highly irregular architecture dsps specialized addressing modes modulo addressing bit reversed addressing small number special purpose registers 
traditional compilers unable generate efficient code processors 
situation change dsp architectures converge general purpose architectures example dsp texas instruments incorporated newest dsp architecture large instruction vliw architecture fairly compiler 
low power requirements cost constraints fixed point dsp irregular architecture dominate embedded applications ieee transactions computer aided design integrated circuits systems vol 
february fig 

flowchart showing sequence various algorithms applied 
foreseeable 
shortcomings existing compilers dsps considerable research effort undertaken design better compilers fixed point dsps see 
synthesis block diagrams useful necessary block diagram specification code 
block diagrams enable coarse grain optimizations knowledge restricted underlying models computation optimizations frequently difficult perform traditional compiler 
step block diagram synthesis flows scheduling block diagram consider scheduling strategies minimizing memory usage 
scheduling techniques develop operate coarse grain system level description techniques somewhat orthogonal optimizations employed tools lower flow 
example behavioral synthesis tool limited view code confined basic blocks block optimizing global control dataflow scheduler exploit 
similarly compiler general purpose hll typically global information application structure scheduler 
techniques develop complementary done developing better hll compilers dsps 
particular techniques develop operate graphs high level particular architectural features target processor largely irrelevant 
assume actor library code generator access consists hand optimized assembly code specifications hll invoke compiler performing dataflow optimizations threading code 
seemingly defeat purpose producing efficient code compiler dsp compiler mentioned studies shown larger systems code produced way compiles better hand written entire system 
ii 
problem statement organization describe technique reducing buffering requirements synchronous dataflow sdf graphs lifetime analysis memory allocation heuristics looped schedules sas 
mentioned step compiling sdf graphs determining schedule 
schedule determined memory allocated buffers graph 
steps algorithmic challenges tackle steps 
concentrate class sass framework sdf graphs exponentially long lead large code size 
class sas algorithmic challenges determine order actors appear schedule subject precedence constraints imposed graph topological ordering order loops organized order determined 
solutions problems depend optimization metric interest 
metric buffer memory algorithms try minimize amount buffer memory needed 
previous techniques buffer minimization techniques buffer allocated independently memory refer nonshared model try share buffers efficiently lifetime analysis techniques referred shared model 
memory allocation steps challenges efficiently extract buffer lifetimes schedule pack buffers memory efficiently 
algorithms provably polynomial time algorithms important sdf compilers rapid prototyping environments fast compile times necessary desirable 
section iii review relevant past subject 
sections iv establish notation definitions 
fig 
summarizes various algorithms develop part sdf compiler framework 
box apgan fig 
finds topological ordering reviewed section vii briefly algorithms developed previously 
box solves loop ordering problem described section vii 
step theoretical idea amount buffer memory required shown actual memory allocation performed know exact requirements 
memory allocation steps take schedule produced steps attempt determine efficient allocation 
order build tree representation schedule covered section representation parameters needed lifetime analysis start times buffers periodicities durations computed algorithms doing efficiently murthy bhattacharyya shared buffer implementations signal processing systems section viii 
lifetime parameters determined structure called intersection graph built 
graph allocation heuristics applied order get final memory allocation steps covered section ix 
developed efficient heuristics generating topological orderings scheduling step said clearly superior developed heuristics memory allocation step experiments section examine possible combinations determine efficient combination test example 
section xi discuss possibilities conclude 
iii 
related lifetime analysis techniques sharing memory known number contexts 
register allocation traditional compilers scheduled dataflow graph register allocation techniques determine variables graph shared looking lifetimes 
simplest form problem formulated interval graph coloring problem elegant polynomial time solution 
problem scheduling graph register requirement minimized np hard problem 
register allocation problems somewhat simpler variables question size 
allocation problem np complete variables differing sizes example allocating arrays different sizes memory 
fabri studies general problem overlaying arrays strings imperative languages 
fabri models array lifetimes weighted interval graphs uses coloring heuristics generating memory allocations 
studies transformation techniques lowering memory cost techniques attempt minimize lower upper bounds extended chromatic number weighted interval graph 
transformation techniques effective reducing storage include renaming transformation judicious renaming aggregate variables lifetimes fragmented allows greater opportunities overlaying technique recalculation variables recalculated needed holding storage code motion techniques reorder program statements semantics preserving manner loop splitting 
important differences fabri 
fabri considers general imperative language code solve allocation problems general class interval graphs 
apply techniques sdf graphs sdf model computation restricted interval graphs problem restricted structure enabling simpler allocation heuristics effectively 
instance liveness profile array framework periodic certain technical sense periods deduced sdf graph specific class schedules general setting liveness profiles may periodic deducing profiles expensive algorithmically 
sdf model sdf sched ules unique problems deducing liveness profiles interval graphs efficient manner techniques studied previous 
show important class sass deductions polynomial time size sdf graph 
optimization technique reducing extended chromatic number performing loop fusion systematic manner 
loop fusion technique applicable general setting opportunities doing general setting arise frequently naturally sdf setting effective technique 
example determining applicability loop fusion undecidable procedural languages exact analysis decidable tractable context 
loop fusion effective sdf graphs exploits increased effectiveness 
previous addressed relationship loop fusion extended chromatic number 
certain subsets techniques studied compilers community date block diagram compilers 
additional contribution show techniques traditional compilers specialized applied fruitfully block diagram dsp programming environments 
observed general full address space array contain live data 
define address window maximum distance live data elements lifetime array fold multiple array elements single window element modulo operation address calculation 
concept similar maximum number live tokens size individual sdf buffer 
number logically distinct memory elements buffer edge equal larger maximum number live tokens reside simultaneously 
synthesis tool called de developed lifetime analysis memory allocation techniques single assignment static control flow specifications involve explicit looping constructs loops 
contrast sdf iteration specified implicitly looping left entirely compiler 
single appearance schedule specified set nested loops 
relationships observed lifetime analysis techniques develop sass 
particular class specifications addressed exhibits general predictable array accessing behavior buffer access patterns emerge sdf sass 
exploit increased predictability sass novel lifetime analysis formulations derived tree schedule representation 
results thorough optimization significantly efficient lower complexity algorithms 
furthermore depth focus restricted useful class sdf sass expose fundamental relationships scheduling buffer sharing multirate signal processing systems 
ritz give approach minimizing buffer memory operates flat sass buffer memory reduction ieee transactions computer aided design integrated circuits systems vol 
february tertiary goal reducing code size context switch overhead defined roughly rate schedule switches various actors 
take context switch account scheduling techniques primary concern memory minimization chip memory bottleneck embedded systems implementations better avoided 
flat sass smaller context switch overhead nested schedules especially code generation strategy procedure calls 
ritz formulate problem minimizing buffer memory flat sass nonlinear integer programming problem chooses appropriate topological sort proceeds allocate schedule 
formulation lead polynomial time algorithms lead expensive memory allocations obtainable nested schedules 
example section show satellite receiver example ritz technique yields allocation larger allocation achieved techniques developed 
techniques take context switch overhead account assume inline code generation effect context switches arguably significant able operate larger class sass class flat sass 
techniques provably polynomial time algorithms 
goddard jeffay dynamic scheduling strategy reducing memory requirements sdf graphs develop earliest deadline edf type dynamic scheduler 
experiments ptolemy system shown dynamic scheduling twice slow static schedules 
embedded applications penalty throughput intolerable 
sung consider expanding sas allow appearances actors buffering memory reduced 
give heuristic techniques performing expansion show buffering reduced significantly allowing actor appear 
technique useful allows tradeoff buffering memory versus code size systematic way 
sass give code size actor schedule distinct distinct implements functionality 
reality actors graph different instantiations basic actor different parameters 
case inline code generated sas necessarily code size optimal different instantiations single actor share code 
profitable implement procedure calls inline code various instantiations code shared 
procedure call pass appropriate parameters 
study optimization done authors formulate precise metrics determine gain loss implementing code sharing compared overhead procedure calls 
clearly scheduling techniques mentioned code sharing technique complementary optimization 
ade developed lower bounds memory requirements sdf specifications assuming buffer fig 

example sdf graph 
valid schedules 
signed separate storage 
exploring incorporation buffer sharing opportunities analysis useful direction investigation 
mentioned dataflow natural model computation underlying model block diagram language designing dsp systems 
blocks language correspond actors dataflow graph connections correspond directed edges actors 
edges represent communication channels conceptually implemented fifo queues establish precedence constraints 
actor fires dataflow graph removing tokens input edges producing tokens output edges 
stream tokens produced way corresponds naturally discrete time signal dsp system 
consider subset dataflow called sdf 
sdf actor produces consumes fixed number tokens numbers known compile time 
addition edge fixed initial number tokens called delays 
iv 
notation background fig 
shows simple sdf graph 
edge annotated number tokens produced consumed source sink actor edge actor actor specifies delays 
unit delay implemented initial token edge 
sdf edge denote source actor writes tokens edge sink actor reads tokens edge delay denote number tokens produced consumed sdf graph called homogenous edges schedule sequence actor firings 
compile sdf graph constructing valid schedule finite schedule fires actor deadlock produces net change number tokens queued edge 
corresponding actor schedule instantiate code block procedure call obtained library predefined actors 
resulting sequence code blocks encapsulated infinite loop generate software implementation sdf graph 
sdf graphs valid schedules exist called consistent sdf graphs 
efficient algorithms determine sdf graph consistent determine minimum number times actor fired valid schedule 
represent minimum numbers firings vector indexed actors suppress subscript understood 
min murthy bhattacharyya shared buffer implementations signal processing systems imum numbers firings derived finding minimum positive integer solution balance equations specify satisfy edge vector exists called repetitions vector schedule sequence actor firings actor fired times firing sequence obeys precedence constraints imposed sdf graph 
graph fig 
actors schedules define total number samples exchanged edge actor constructing memory efficient loop structures concept motivation sas defined shown yield optimally compact inline implementation sdf graph regard code size neglecting code size overhead associated loop control 
sas actor appears loop notation 
sas restriction removed significant increase code size occur 
increase code size manifest inline code generation subroutine calls 
length non sas exponential size graph exponentially subroutine calls 
fig 
shows valid schedules graph fig 

notation represents firing sequence similarly represents schedule loop firing sequence say iteration count loop body loop schedules fig 
sass actors appear 
sas third fig 
called flat nested loops 
general exponentially ways nesting loops flat sas 
scheduling significant impact amount memory required implement buffers edges sdf graph 
example fig 
buffering requirements schedules assuming separate buffer implemented edge respectively 
seen sass significantly higher buffer requirements schedule optimized purely buffer memory 
example non sas sdf graph fig 
buffer requirement possible sass graph requirements respectively 
give priority code size minimization buffer memory minimization justification may 
problem tackle finding buffer memory optimal sass give best schedule terms buffer memory consumption schedules minimum code size 
fig 

schedule trees schedules fig 

schedules schedule tree order extract buffer lifetimes efficiently develop useful representation nested sas called schedule tree 
lifetime extraction algorithms section viii formulated tree traversing algorithms determining various required parameters 
shown possible represent sas acyclic graph sass subgraph consisting actors iteration counts iterating schedules 
words graph partitioned left subset right subset schedule graph represented 
sass having form conjunction additional technical restrictions loop iteration counts levels loop hierarchy called schedules 
schedule represent naturally binary tree call schedule tree 
internal nodes tree contain iteration count subschedule rooted node 
leaf nodes nodes children contain actors residual iteration counts 
node children refer left child right child node parent referred fig 
shows schedule trees sass fig 

note schedule tree unique iteration counts split left right subgraphs multiple places 
fig 
schedule tree flat sas fig 
split take split split affect computations perform tree 
node schedule tree sub tree rooted node subtree define root node function set nodes tree set positive integers returns nonleaf node iteration count nesting level returns leaf node 
vi 
generating single appearance schedules shown arbitrary acyclic graph sas derived topological sort graph 
precise class sass acyclic graph generated enumerating topological sorts graph 
lexical ordering topological sort derive flat sas schedule form actors repetitions lexical order order topological sort graph 
lexical ordering leads set nesting ieee transactions computer aided design integrated circuits systems vol 
february fig 

fine grained coarse grained models buffer sharing 
hierarchies complete set lexical orders lexical order set nesting hierarchies constitutes entire set sass graph 
need method generating topological sort 
shown general problem constructing buffer optimal sass models buffering coarse shared buffer model nonshared model np complete 
methods generating topological sorts necessarily heuristic optimal general 
developed methods generating sas optimized nonshared buffer memory acyclic graphs bottom method clustering called acyclic pairwise grouping adjacent nodes apgan top method graph partitioning called recursive partitioning minimum cuts 
heuristic rule thumb find cut graph edges cross direction enabling recursively schedule half introducing deadlock size buffers crossing cut minimized 
rule intuitively attractive nonshared buffer model attractive shared model shown 
apgan technique clustering adjacent nodes communicate heavily nodes innermost loops loop hierarchy 
broad subclass sdf systems apgan shown construct sas provably minimize nonshared buffer memory metric sas 
arbitrary sdf graph may necessarily sas bhattacharyya developed necessary sufficient conditions existence sas sdf graphs 
developed algorithm generating sass hierarchically decomposes sdf graph strongly connected components scc recursively schedules scc 
stage scc decomposition results acyclic component graph sas mentioned scheduled algorithm generating sas acyclic graphs 
techniques develop incorporated framework handle arbitrary sdf graphs 
vii 
efficient loop fusion minimizing buffer memory topological order generated apgan flat sas corresponding topological order 
step perform loop fusion flat sas reduce buffering memory 
define shared buffer model 
shared buffer model interested sharing buffers determine appropriate model buffer lifetimes manner shared 
need definition describing token traffic edges sdf graph valid schedule edge denote maximum number tokens queued execution example fig 
buffer sharing looped schedules done different levels granularity finest level granularity model buffer edge grows execution loop falls sink actor edge consumes data 
maximum number live tokens give lower bound memory required 
alternative model coarsest level assume source actor edge starts writing tokens tokens immediately live stay live number tokens edge zero schedule 
words live token edge assume array size allocated maintained live tokens 
fig 
shows extremes pictorially buffer edge fine grained case firing results buffer expanding firing results buffer contracting 
coarse grained case buffer expands immediately firings treated composite firing shrinks zero firings occurred 
course number granularities extremes levels loop nests consider fig 
shows alternative example outer loop iteration count considered meaning firings inner loop treated composite firing 
buffer case expands tokens composite firing contracts composite firing consisting firings 
assume coarsest level buffer modeling 
finer levels requiring memory theo murthy bhattacharyya shared buffer implementations signal processing systems may practically infeasible achieve due increased complexity algorithms 
see complexity significantly increase notice finest level requires modeling done granularity single firing actor schedule 
number firings periodic sdf schedule set edges sdf graph course may clever ways representing growth shrinkage presently known ways equivalent stepping schedule size clearly exponential function size sdf graph grow quickly 
contrast show coarsest level model generated time polynomial number nodes edges sdf graph 
weakness coarse buffer sharing model assumption output buffers actor live actor begins execution input buffers live actor finishes execution 
means output buffer actor share input buffer actor model 
reality may overly restrictive assumption instance addition actor adds quantities produce output consumed inputs 
output result occupy space occupied inputs 
formalized idea devised technique called buffer merging merges input output buffers algebraically determining precisely output tokens simultaneously live input tokens formalism called consumed produced cbp parameter 
buffer merging technique similar spirit array merging technique de efficient ways exploits distinguishing characteristics sdf schedules novel way 
shown buffer merging technique highly complementary approach taken effect dual lifetime analysis approach 
buffer merging works level single input output edge pair lifetime analysis approach works global level buffering efficiency results topology graph structure schedule 
initial tokens edges handled naturally coarse shared buffer model 
edge initial token buffer live right schedule 
may live entire duration schedule buffer zero tokens 
buffer zero tokens point buffer live portion schedule buffer zero tokens 
order reason start time time buffer lifetime notion time invocation leaf node schedule tree considered schedule step corresponds unit time 
example looped schedule considered take time steps 
firing sequence schedule loop leaf node schedule tree considered take schedule step 
invocation take place time zero invocation begins time ends time 
note notion time judge run time fig 

anatomy buffer lifetime 
formance schedule terms throughput simply define lifetimes purposes lifetime analysis 
fig 
shows anatomy buffer lifetime 
notice buffer live times 
start time defined time buffer live contrived example time 
time defined time buffer stops live time example 
duration simply difference start times 
periodicity modeled tuple shown described greater detail section viii 
briefly modeled diophantine equation start time th occurrence buffer computed algebraically 
width buffer defined mentioned coarse grained model 
loop fusion nonshared buffer model topological ordering nodes sas determined determine order loops nested shown section fig 
loop hierarchy significant impact buffer memory usage 
developed postprocessing technique dynamic programming called dynamic programming post optimization generates optimal loop hierarchy sas 
cost metric approach edge implemented separate buffer 
briefly review technique technique describe generating loop hierarchies shared model similar 
nonshared model define nonshared buffer memory requirement schedule summation edges fig 

lexical ordering sas denoted sequence actors preceded lexically sdf graph order optimal schedule sas minimum nonshared buffer memory requirement valid sass lexical ordering 
central observations allows development efficient algorithm optimizing buffer memory ieee transactions computer aided design integrated circuits systems vol 
february nonshared model stated intuitively way fusing adjacent loops common factor creating outer loop common factor iteration count gives valid schedule gives schedule nonshared buffering memory requirement equal smaller nonshared buffering memory requirement schedule set separate loops 
formally state fact suppose valid schedule sdf graph suppose schedule loop nesting depth suppose positive integer divides denote schedule loop denote schedule results replacing valid schedule defined 
schedule loop called factored loop 
act factoring common factor factoring schedule 
called loop fusion informally algorithm uses divide conquer approach looks chain actors schedule examines point chain determine buffer cost breaking chain fusing left right parts 
picks best point split chain records considers bigger chains 
problem shown optimal subproblem property meaning optimal solutions left right parts lead optimal solution chain optimal algorithm nonshared buffer model 
formally suppose connected acyclic sdf graph valid sas order optimal schedule contains actors shown exists valid schedule form furthermore order optimality order optimal show replace order equivalent versions affecting split costs 
observation efficiently compute order optimal schedule order optimal schedule subgraph corresponding proper subsequence schedules order optimal schedule derived value minimizes set edges cross split schedule split repeatedly applying idea bottom fashion lexical ordering actor subsequences examined minimum buffer memory requirements edges contained subsequence recorded 
information determine optimal split minimum buffer memory requirement actor subsequence minimum requirements actor subsequences determine optimal split minimum buffer memory requirement actor subsequence optimal split derived original actor sequence order optimal schedule easily constructed recursive top traversal optimal splits 
loop fusion shared buffer model model sharing buffers develop algorithm organizing loops efficiently sas shared buffer cost minimized 
algorithm similar algorithm described 
consider dynamic programming formulation term sum sum buffer costs crossing cut 
term shared buffer memory requirement serves definition 
course actor 
maximum left right costs taken intuition shown fig 

buffers subgraph right side cut live time buffers left side cut live vice versa need maximum buffers crossing cut live simultaneously left right set buffers 
right buffers overlayed left ones 
formulation optimal worst case assumption buffers crossing cut simultaneously live buffers left buffers right buffers preventing sharing 
example consider example fig 

schedule top level partition occurs edge cost 
formulation total cost murthy bhattacharyya shared buffer implementations signal processing systems fig 

intuition revised dynamic programming formulation 
fig 

example illustrate shared suboptimal 
buffering cost subschedule consisting actors partition occurs edge cost wehave total cost gets computed actual cost shown fig 

fact says loop fusion increases buffer memory usage nonshared model unfortunately true shared model 
consider example fig 

fig 
edge actors perform loop fusion shown fig 
see buffer profiles buffers output edges actor disjoint buffers input edges actor perform loop fusion shown fig 
buffers longer disjoint preventing sharing 
advantage gained fusion sizes input output buffers decrease loop fusion reduces size buffers edges actors merged 
hand edge actors shown fig 
loop fusion reduce buffering requirement reduction size buffer edge increase due overlap input output buffers 
depends actual parameters graph follow simple heuristic formulation perform loop fusion internal edges edges terminal points actors merged 
fig 

example illustrate factoring increase buffering requirement shared model 
perform loop fusion internal edges suboptimal reduction buffer sizes internal edges increase due overlap input output buffers 
course attempt compute increase decrease increase complexity algorithm 
choose simpler approach heuristic approach determine fuse leave explore complex approaches 
define formulation heuristic deciding ieee transactions computer aided design integrated circuits systems vol 
february fig 

example show shared buffer optimal schedule nonshared buffer optimal schedule 
perform loop fusion shared dynamic programming post optimization 
note best schedule nonshared buffering model necessarily best schedule shared model shown fig 

see attractive heuristic shared buffer model cut crossing buffers disjoint shared 
intuitive sense drive partitioning process minimizing size buffers attempts 
viii 
creating interval instances sas schedule loop hierarchy determined step compilation process perform memory allocation 
algorithm gives number memory requirement estimate algorithm determine estimate achieved 
main difficulty packing number arrays different sizes optimally np complete problem optimal amount memory required packing determined packing performed 
main steps memory allocation extract buffer lifetimes perform allocation lifetimes 
extracting lifetimes efficiently requires algorithms determining durations start times 
lifetimes periodic desirable represent periodicity implicitly having physically create interval occurrence 
lifetime extraction algorithms model periodicity efficiently 
lifetimes allocation step packing arrays words determines physical location memory buffer reside 
compute parameters buffers schedule tree 
note parameter computed buffer function pair actors constitute edge buffer 
schedule tree represent edges directly actors structure nested loops 
compute parameters nodes schedule tree parameters represent start time fig 

sdf graph 
binary tree representation sas graph duration values nodes schedule tree 
time durations various nested loops 
deduce buffer lifetimes computed quantities nested loops 
running example show various algorithms example sdf graph depicted fig 
sas 
computing duration times loop nests function loop defined section computation duration times nodes loop nests schedule tree depth search tree right left child node leaf nodes numbers nodes murthy bhattacharyya shared buffer implementations signal processing systems fig 

pseudocode depth search algorithm computing start times nested loops 
fig 

start times node computed depth search 
fig 
show result depth search computation duration values 
depth search takes time computing start times loop nests task compute start times nested loop 
times defined left child right child times computed depth search time shown pseudocode fig 

sas fig 
shown computed start times fig 

start time nested loop represents time loop nest starts execution 
time represents time loop nest finishes execution 
example consider edge fig 

time computed leaf node schedule tree 
corresponds time finishes execution firings correspond steps measuring scheme described section vii time computed node marked aef fig 
fig 
firing giving time execution shown fig 

computing start durations buffer lifetimes compute lifetimes buffers parameters computed loop nests 
introduce notation nodes schedule tree 
definition common ancestor pair nodes node contains nodes leaf nodes subtree rooted definition common ancestor pair nodes node measured leaf nodes contains nodes leaf nodes subtree rooted definition greatest common ancestor pair nodes node contains nodes leaf nodes subtree rooted ancestors loop value unity 
definition common ancestor set pair nodes set common ancestors path common ancestor greatest common ancestor 
greatest common ancestors pair leaf nodes correspond innermost outermost loops contain actors corresponding leaf nodes 
fig 
common ancestor set leaf node pair start time lifetime buffer edge clearly start time computed leaf node schedule tree corresponding actor firing actor buffer edge live 
time buffer interval time stops live 
note interval periodic live 
interested time stops live quantity periodicity parameters completely characterize interval 
time simply time leaf node corresponding sink actor time computed leaf node represents time corresponding sink actor finishes execution 
time sink actor finishes execution necessarily time tokens buffer consumed 
instance consider edge fig 

time computed leaf node schedule tree 
notice buffer live firings occurred 
really need compute time execution sink actor buffer takes place loop nest interest 
loop nest interest smallest loop nest containing total number tokens consumed loop nest equal number produced loop nest 
time computed node representing smallest loop nest common ancestor schedule tree includes execution loop nests contained 
want exclude contribution time nests follow sink actor execution loop nest interest 
example fig 
loop nest interest buffer root node common ancestor 
time computed root node includes execution final execution loop nest corresponding root node 
want subtract execution times order get time finishes execution ieee transactions computer aided design integrated circuits systems vol 
february fig 

procedure computing earliest time interval 
time 
idea formalized algorithm shown fig 
takes time 
algorithm finds common ancestor schedule tree procedure done computing ancestor sets leaf node determining common ancestor sets takes time 
time buffer interval set time computed common ancestor 
algorithm moves common ancestor leaf node adjusts time move move left duration right node representing execution loop nests subtracted move right adjustment necessary 
algorithm stops reaches right child common ancestor 
computing periodicity parameters buffer lifetimes schedule buffer edge sdf graph particular lifetime profile 
profile periodic shown fig 

periodicity arises due nested loops 
periodic mean lifetime fragmented deterministic predictable manner 
precisely times buffer live described succinctly simply enumerating occurrences live portions 
useful keep track periodicity certain cases buffers disjoint lifetimes shared shown fig 
buffers edges buffer edge sdf graph common ancestor set denoted nodes common ancestor ancestor 
example buffer edge fig 
greatest common ancestors nodes marked leaf node pair node common ancestor set path represent periodic lifetime buffer tuples integer form triple consisting fig 

periodic intervals corresponding schedule fig 

constants nodes common ancestor set values unity contribute periodicity shown example components eliminated 
size common ancestor set minus number common ancestors loop values unity 
triple allows represent buffer lifetime way buffer live time intervals combinations unity common ancestor common ancestor contribute periodicity zero value take 
example buffer fig 
denote corresponding common ancestor set remove component loop value unity 
buffer live intervals combinations live intervals :10.1.1.21.2539
murthy bhattacharyya shared buffer implementations signal processing systems storage allocation determine time buffer live 
essence determine equation variables range solution solution exists easily property sorted increasing order 
hold 
lemma intuitively lemma states start time buffer due th outer loop occurrences start times buffer taken place due th inner loop loops contained th loop 
intuitively true loops nested outer loop count increments inner loops counted entire range 
proof induction 
nodes schedule tree parent assumed loss generality need show assume claim holds prove corresponding sorted tuple define tuple th component note allowed range define ordering relationship way 
largest index th components differ 
iff th component th component define start time occurrence periodic buffer function denotes vector transpose 
lemma 
lemma tuples proof direction largest tuples differ components lemma 
direction letting highest index differ need show suppose 
suppose wehave lemma contradicting assumption algorithm computing buffer liveness particular time lemma solved algorithm fig 

algorithm subtracts start time buffer computations relative start time 
simply determines maximum factor determine closest starting point occurrence periodic interval time claim stage algorithm 
proof note claim solution computed algorithm gives starting point interval closest starting proof tuple consisting means tuple gives interval starting time greater suppose tuple largest index tuples differ 
th step value computed algorithm identical computation values algorithm computes suppose clearly larger mean giving start time greater largest value th component allowed take definition 
contradicting assumption step algorithm checks determine interval closest starting time equal alive 
live time need determine instance periodic interval occur 
computation needed determine interval particular duration completely disjoint set ieee transactions computer aided design integrated circuits systems vol 
february fig 

algorithm determine periodic buffer live particular time 
depiction right shows possibilities computed start time algorithm relation 
fig 

nearest intervals time 
condition periodic intervals lemma depicted graphically 
intervals corresponding determine interval fitted location assigned 
starting time instance periodic interval obtained simply incrementing number formed basis example 
number represents time buffer live incrementing number basis gives starting time 
ideas formalized lemma 
periodic interval start times th occurrence interval 
th increment number time interval nearest start time equal similarly defined nearest interval start time greater fig 
illustrates definitions 
lemma periodic intervals intersect words intervals intersect closest interval starts start time finishes start time closest interval starts start time starts interval finishes fig 

notice lemma says consider occurrences periodic interval determine overlap occurrence 
proof forward direction trivially true 
reverse direction established case analysis 
edges buffers reside ordering actors schedule clearly condition satisfied third order live entire time orders consider different ways loops nested 
order distinct ways nesting loops 
order note consider part schedule contains actors subtree schedule tree rooted common ancestor nodes ignore actors appear order nesting affect properties particular buffers interested 
fig 
shows buffer profiles cases 
verified holds intervals intersect 
similarly verify lemma true nestings order 
method testing periodic buffer live time easily test periodic buffers disjoint intersect 
test take time murthy bhattacharyya shared buffer implementations signal processing systems fig 

buffer profiles possible nesting hierarchies lemma 
fig 

memory allocation properties 
dsa terminology 
worst case set actors sdf graph 
reason sas schedule tree linear depth depth common ancestor set nodes buffer actors innermost loop 
procedure fig 
test takes time average schedule tree logarithmic depth cases running time testing procedure step allocate various buffers memory 
ix 
dynamic storage allocation lifetimes actual assignment memory locations buffers 
assignment problem called dynamic storage allocation dsa problem arrange different sized arrays memory total memory required time minimized 
assignment memory assumed properties array assigned contiguous block memory assigned array may moved occurrences array periodic lifetime profile assigned location memory 
fig 
depicts properties 
course relax restrictions get smaller memory requirements come expense overheads moving arrays relaxed 
leave investigate models allocation 
formally dsa defined 
definition set buffers 
number elements time live time dies size buffer note duration buffer values integer allocation buffers requires total storage units 
allocation mean function intervals intersect intersection test periodic buffer lifetimes described earlier dynamic dsa refers fact times problem line nature allocation performed intervals come go 
sdf scheduling problem really dynamic lifetimes sizes arrays need allocated known compile time ieee transactions computer aided design integrated circuits systems vol 
february problem called static storage allocation 
term dsa consistent literature 
theorem dsa np complete sizes 
notation instance set buffers 
enumerated instance instance ordering buffers 
instance associated weighted intersection graph wig set buffers set edges 
edge buffers iff lifetimes overlap time 
graph node weighted sizes buffers 
subset nodes define weight sum sizes clique subset nodes edge pair nodes 
clique weight cw weight clique 
maximum clique weight wig clique largest weight denoted corresponds maximum number values live point 
chromatic number cn denoted minimum feasible allocation definition 
fig 
shows definitions example 
heuristic dsa fit ff known algorithm performs allocation enumerated instance assigning smallest feasible location interval order appear enumerated instance 
reallocate intervals allocated consider intervals assigned 
pseudocode algorithm shown fig 

refer reader technical report detailed treatment interesting dsa problem 
briefly algorithm takes input enumerated instance 
tested types orderings generating enumerated instances ordering start times ordering durations 
builds wig routine 
wig built general test developed determining intersection possibly periodic buffers 
ff algorithm examines wig buffer examines nodes adjacent wig buffers intersect 
collects memory allocations adjacent nodes appear enumeration 
sorting allocations sees allocated worst case allocated allocations regions big accommodate allocation determined buffer examined enumeration allocated 
study shows practice ff heuristic compiler framework 
empirical study random shows ordering buffers durations gives better results 
experiments section apply ff ordering start times abbreviated ordering durations 
order analyze running time observe sparse sdf graphs node edge sets sdf graph 
building weighted intersection graph takes time worst case buffers overlap schedule tree linear depth time schedule tree logarithmic depth 
foreach loop procedure takes time worst case buffer overlaps buffer procedure running time dominated step 
computing maximum clique weight clear maximum clique weight lower bound chromatic number weighted interval graph 
known chromatic number times maximum clique weight particular instances known tight upper bound 
maximum clique weight lower bound compare performance allocation strategy particular set lifetimes 
experiments random instances show comes average maximum clique weight practice chromatic number bigger maximum clique weight certainly times big 
maximum clique weight comparison purposes experiments section 
maximum clique weight computed easily exactly instance fragmented lifetimes computing instances fragmented periodic buffer lifetimes difficult 
consider case intervals continuous fragmented 
set times schedule steps maximum overlap intervals overlap amount equal maximum cw 
easy see contain start time interval 
maximum clique weight computed easily sorting intervals starting times determining overlap starting time 
suppose intervals periodic 
case contain start time interval need earliest start time 
start time periodic occurrence greater earliest start time interval see fig 

compute scenario consider start times occurrences periodic interval time algorithm potentially take long time periodic occurrences 
experiments heuristics compute values 
heuristic gives optimistic estimate considers earliest start time interval determines overlap intervals time algorithm fig 

optimistic estimate occur time earliest start time interval 
second heuristic gives pessimistic estimate simply ignores periodicity periodic intervals assumes periodic interval live entire time earliest start time time time occurrence interval 
murthy bhattacharyya shared buffer implementations signal processing systems fig 

pseudocode definition ff heuristic 
experimental results tested algorithms practical benchmark examples random graphs 
mentioned earlier crux experiment study memory requirement result best combination possibilities perform scheduling apgan generate topological ordering perform loop fusion schedule 
perform memory allocation ff buffers ordered starting times ff buffers ordered durations 
compare best memory requirement obtained way best memory requirement nonshared techniques applying apgan loop fusion note buffers shared memory allocation step trivial buffer gets separate block memory 
fig 
shows percentage improvement systems tested 
seen average improvement compiler framework compared previous techniques 
examples improvement high 
details experiments 
practical multirate systems practical multirate examples number sided filter bank structures shown fig 
sided filter banks shown fig 
satellite receiver example shown fig 

type variation occurs frequently practical signal processing systems vari ieee transactions computer aided design integrated circuits systems vol 
february fig 

example shows occur time earliest start time interval 
numbers rectangles denote width intervals 
fig 

improvement percentage best shared implementation versus best nonshared implementation 
ation sample rate change ratios 
example fig 
shows filter bank rate changes changes occur actors instance 
ratios include 
similarly fig 
shows filter bank rate changes example actors changes instance 
experimental data summarized various parameters filter banks different depths table leftmost column contains name example 
filter bank qmf example filter bank depth rate changes 
similarly qmf filter bank depth rate changes 
depth filter banks nodes respectively 
sided filter bank fig 

satellite receiver example 
examples included implementation qam modem transmitter receiver pair pam signal implementation system modulates synthesized music signal vocal parameters implementation overlap add fast fourier transform fft fft applied successive blocks samples overlapped implementation phased array system detecting signals 
examples taken ptolemy system demonstrations 
second column contains results running systems assuming nonshared model buffering 
column gives basis determining improvement shared model 
general refers refers apgan 
third column results applying new dynamic programming heuristic shared buffers generated topological order 
fourth fifth columns contain optimistic pessimistic estimates maximum clique weight schedule generated generated topological order 
sixth seventh columns contain actual allocations achieved applying firstfit ordered durations firstfit ordered start times heuristics 
eighth column contains buffer memory lower bound values system 
briefly lower bound total buffering memory required valid sass assuming nonshared model buffering 
rest columns contain results applying heuristics apgan generated topological orders 
row numbers shown bold better result apgan best shared implementation 
column percentage improvement nonshared implementation computed shown equation bottom page 
seen improvements average dramatic cases improvement depth filter bank rate changes common type filter bank 
interesting note methods ritz shared buffer scheduling achieve allocation units contrast methods achieve improvement 
interesting note possible combinations combination gives best results 
best results murthy bhattacharyya shared buffer implementations signal processing systems fig 

sdf graph sided filter bank depth 
produced consumed numbers specified unity 
fig 

sdf graph sided filter bank 
depth filter bank 
depth filter bank 
produced consumed numbers specified unity 
fig 

sdf abstraction satellite receiver application 
fairly regular qmf filterbanks irregular systems apparently better suited combination 
experiment conducted determine applying schedule gives better re sults applying schedule 
maximum improvement observed examples 
better new heuristic shared buffers improvement dramatic 
order determine apgan generating topological sorts tested results best allocation get generating random topological sorts 
applied technique ff heuristics random topological sort determine best allocation 
small graphs nodes took random trials beat best result generated better schedules 
trials best random schedule resulted allocation example allocation example 
best apgan allocations respectively 
generate better results just random search improve apgan lot time spent doing 
relative improvement random schedules increases larger graphs examined qmf examples nodes 
trials best allocations qmf qmf compared apgan allocations respectively 
running time trials minutes long pentium ii pc conclude bigger graphs require large amounts time compute power equal beat apgan schedules 
conclude compact shared buffer implementation apgan generating topological sorts intelligently easily beaten strategies generating random schedules 
homogenous graphs previous loop scheduling techniques buffer memory reduction techniques described effective homogenous sdf graphs 
allocation techniques sharing strategy greatly reduce buffer memory requirement cases 
example consider class homogenous graphs parametrized shown fig 

type graph close variants arises frequently practice 
clear matter schedule live tokens 
running complete suite ieee transactions computer aided design integrated circuits systems vol 
february fig 

homogenous graph shared allocation techniques highly beneficial 
techniques graph results allocation units 
nonshared implementation table buffer sizes practical examples require units 
savings dramatic horizontal chains vectors matrices exchanged numerical tokens 
xi 
developed powerful sdf compiler framework improves previous efforts demonstrably 
incorporating lifetime analysis aspects scheduling allocation framework able generate schedules allocations reuse buffer memory reducing memory usage dramatically 
order produce code competitive hand coded implementations ways additional optimization problems formulated 
murthy bhattacharyya shared buffer implementations signal processing systems particular problem addressed issue recognizing regularity occur graphical specifications instance fine grained description finite impulse response fir filter 
regularity extraction applied past high level synthesis applied pattern matching algorithms compiler design silicon compilers techniques applied context sdf compilers 
addition useful study techniques regularity implied hierarchy graphical higher order functions dataflow specifications 
acknowledgment authors edwards anonymous referees helpful remarks improving readability presentation 
ade data memory minimization synchronous data flow graphs emulated dsp fpga targets proc 
design automation conf anaheim ca june pp 

bhattacharyya murthy lee optimal lexical orderings dsp block diagrams proc 
ieee workshop vlsi signal processing osaka japan oct pp 

bhattacharyya murthy cbp parameter useful annotation aid block diagram compilers dsp proc 
int 
symp 
circuits systems geneva switzerland may pp 

bhattacharyya murthy lee software synthesis dataflow graphs 
norwell ma kluwer 
bhattacharyya ha buck lee generating compact code dataflow specifications multirate signal processing algorithms ieee trans 
circuits systems fundamental theory applications vol 
pp 
mar 
buck ha messerschmitt lee multirate signal processing ptolemy proc 
int 
conf 
acoustics speech signal processing toronto canada apr pp 

buck ha lee messerschmitt ptolemy framework simulating prototyping heterogeneous systems int 
comput 
jan 
fabri automatic storage optimization 
ann arbor mi 
garey johnson computers intractability guide theory np completeness 
san francisco ca freeman 
goddard jeffay managing memory requirements synthesis real time systems processing graphs proc 
ieee real time technology applications symp denver june pp 

de de man array placement storage size reduction embedded multimedia systems proc 
int 
conf 
applications specific systems architectures processors zurich switzerland july pp 

meyr mapping multirate dataflow complex rt level hardware models proc 
int 
conf 
application specific systems architectures processors zurich switzerland july pp 

polynomial time approximation algorithm dynamic storage allocation discrete math vol 
pp 

technology binding local optimization dag matching proc 
design automation conf miami beach fl june pp 

kulkarni dube evans benchmarking code generation methodologies programmable digital signal processors dept elect 
comput 
eng univ texas austin tech 
rep 
ade geometric parallelism cyclo static data flow grape ii proc 
ieee workshop rapid system prototyping grenoble france june pp 

lee messerschmitt static scheduling synchronous dataflow programs digital signal processing ieee trans 
comput vol 
pp 
feb 
lee parks dataflow process networks proc 
ieee vol 
pp 
may 
leupers marwedel retargetable code generation structural processor descriptions design automat 
embedded syst vol 
pp 
jan 
liao devadas tjiang wang code optimization techniques embedded dsp microprocessors design automat 
embedded syst vol 
pp 
jan 
marwedel goossens eds code generation embedded processors 
norwell ma kluwer 
murthy bhattacharyya systematic consolidation input output buffers synchronous dataflow specifications proc 
ieee workshop signal processing systems lafayette la oct pp 

buffer merging powerful technique reducing memory requirements synchronous dataflow specifications inst 
adv 
comput 
studies univ maryland college park md tr apr 
murthy bhattacharyya lee joint minimization code data synchronous dataflow programs formal methods syst 
design vol 
pp 
july 
murthy bhattacharyya approximation algorithms heuristics dynamic storage allocation problem univ maryland inst 
adv 
comput 
studies college park md www cs umd edu trs html may 
rao approach scheduling allocation regularity extraction proc 
eur 
conf 
design automation paris france feb pp 

clustering maximal regularity extraction ieee trans 
comput aided design integrated circuits syst vol 
pp 
aug 
ritz meyr optimum vectorization scalable synchronous dataflow graphs proc 
int 
conf 
applications specific array processors venice italy oct pp 

ritz willems meyr scheduling optimum data memory compaction block diagram oriented software synthesis proc 
icassp detroit mi may pp 

sethi complete register allocation problems siam computing vol 
pp 
sept 
sung kim ha memory efficient synthesis dataflow graphs int 
symp 
system synthesis taiwan dec pp 

multirate systems filter banks 
englewood cliffs nj prentice hall 
de man compiling multi dimensional data streams distributed dsp asic memory proc 
int 
conf 
computer aided design santa clara ca nov pp 

williamson synthesis parallel hardware implementations synchronous dataflow graph specifications ph dissertation electron 
res 
lab univ california berkeley ca june 
meyr dsp oriented benchmarking methodology int 
conf 
signal processing application technology dallas tx oct pp 

ptolemy ptolemy eecs berkeley edu 
murthy bhattacharyya buffer merging powerful technique reducing memory requirements synchronous dataflow specifications proc 
int 
symp 
system synthesis san jose ca nov pp 

praveen murthy received degree georgia institute technology atlanta ph degrees electrical engineering computer science university california berkeley respectively 
angeles design systems san jose ca holds position distinguished member technical staff 
prior joining angeles cadence design systems member consulting staff 
consulted berkeley ca area digital signal processor dsp architectures 
coauthor software synthesis dataflow graphs norwell ma kluwer 
research interests span areas system level design synthesis including simulation techniques producing optimized software implementations semantics different models computation multidimensional dataflow software tools rapid prototyping 
ieee transactions computer aided design integrated circuits systems vol 
february bhattacharyya received degree university wisconsin madison ph degree university california berkeley respectively 
assistant professor department electrical computer engineering institute advanced computer studies university maryland college park 
held industrial positions researcher hitachi san jose ca compiler developer kuck associates urbana il 
consulted industry areas compiler techniques multiprocessor architectures embedded systems 
coauthor books author coauthor refereed technical articles 
research interests center architectures computer aided design embedded systems 
dr bhattacharyya recipient nsf career award 
