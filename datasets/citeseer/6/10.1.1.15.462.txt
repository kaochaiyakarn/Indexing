periodic motions mapping ordered sequences training dynamic neural networks generate continuous discontinuous trajectories pablo electrical computer engineering department university arizona tucson az usa ece arizona edu ece arizona edu designing efficient methods training dynamic neural networks learning spatio temporal patterns great interest 
particular trajectory generation problem involves training network learn replicate autonomously specified time varying periodic motion attracted considerable attention 
novel systematic approach solve problem decomposing task sub tasks spatio temporal sequence assignment mapping ordered sequences 
decomposition permits dynamic neural network realized cascade simple recurrent net followed non recurrent yields considerable reduction training complexity 
detailed performance evaluation scheme considering trajectory generation experiments highlight strong points approach include simplicity accuracy training flexibility include control parameters order modify line shape trajectory learned speed repetition cyclic trajectory possibility learning continuous discontinuous trajectory patterns 

neural networks recurrent connections dynamic processing elements finding increasing applications diverse areas 
feedforward networks recognized perform excellent pattern recognition function approximation limited processing stationary patterns patterns invariant respect time approximating stationary functions input output maps change time 
requires power dynamic neural networks networks recurrent feedback connections handle challenges posed storage processing spatio temporal patterns sequences 
notwithstanding importance networks major problem deployment practice complexity training due presence recurrent feedback connections 
problem exacerbated gradient descent learning algorithms require computation error gradients necessary updating forcing resort approximations may turn reduce training efficiency 
efforts tailor alternate training schemes reinforcement learning require explicit evaluation gradients 
training complexity poses concerns particularly size network large 
particularly challenging problem popularly benchmark problem scoring efficiency learning scheme training dynamic neural network produce spatio temporal motion specified form called trajectory generation problem ultimately relax desired limit cycle behavior resulting terminal attractor dynamics ensures periodic repetition desired motion 
practical applications problem cited 
repetition trajectory defined certain feature space task robot industrial manufacturing settings accurate generation trajectory typically required satisfactory control robots 
due importance problem number works focused designing efficient schemes training recurrent neural nets produce periodic trajectories desired forms 
specific benchmark trajectories received wide attention performance evaluations learning schemes circle trajectory trajectory 
novel approach synthesizing training dynamic neural network dnn learn replicate specified spatio temporal behavior proposed 
shown efficient synthesis procedure results separating task sub tasks spatio temporal sequence assignment mapping ordered sequences permits dynamic network realized cascade simple recurrent neural network rnn followed non recurrent neural net nrnn feedforward connections reduces training complexity considerably 
shown training network produce periodic trajectories simple special case procedure generation specific benchmark trajectories reduced trivial problem 
approach gives utmost flexibility trajectory generation available existing schemes 
additional input parameters control line precise shape trajectory keeping period repetition alternately control speed repetition trajectory 
importance ensuring capability line adjustment tracking speed order control tracking error robot manipulators quite known 

synthesis dynamic neural net trajectory generation fundamental idea approach trajectory generation dynamic neural networks dnn involves decomposition task sub tasks spatio temporal assignment labels mapping ordered sequences explained follows 
trajectory defined terms set features arbitrary dimensional feature space specified ordered sequence numbers terms parameter amplitude vector 
fn denoting dimension feature space 
physical system evolve real time initial state produce trajectory need mapping function real time parameter general nonlinear function 
provides ability control trajectory equivalently ordered sequence relates real time specifies trajectory representative point time instants provides required spatio temporal assignment 
selected scalar vector achieve desired degree convenience assignment task 
problem interest generation periodic trajectory function periodic attain desired replication problem interest ultimately converge periodic trajectory arbitrary starting point training limit cycle behavior function selected appropriately reflect dynamic evolution 
consequently realization dynamic properties requires recurrent neural network rnn 
architecture network kept simple irrespective complexity desired trajectory spatiotemporal assignment maintained simple limited basic periodic function neural network simplest architecture capable producing 
required spatio temporal assignment handled recurrent neural net remainder trajectory generation problem involves learning mapping simply static input output map relating ordered sequences complexities associated dynamical evolution trajectory eliminated simple non recurrent neural net nrnn feedforward connections multi layer perceptron radial basis function network support vector machine suffice executing task 
due powerful approximation capabilities feedforward networks trained produce mapping desired degree accuracy 
tasks spatio temporal assignment labels approximating ordered sequence maps handled distinct neural nets recurrent connections arbitrarily simple architecture non recurrent feedforward connections approach provides efficient solution trajectory generation problem 
architecture dnn shown 
rnn dynamic neural network nrnn 
architecture dnn 
evident discussion performance evaluation proposed synthesis procedure division task enables precisely control error production trajectory successive cycles 
comparison noted previous attempts solving problem generation simple trajectory circle handled difficulties generation complex trajectory due presence point trajectory intersects resulted unstable performance successive cycles trajectory perfectly overlap 

selection training rnn nrnn section details structure component networks corresponding training schemes 
rnn indicated earlier simple architecture 
model popular generation continuous time model described set coupled nonlinear differential equations dxi tanh ai dt xi denotes state th neuron time constant referred relaxation time ai parameter controls slope threshold function ij denotes interconnection weight th neuron th 
inputs network come initial conditions xi outputs may selected states specific neurons 
training network learn produce simple periodic trajectories mapped plane circle ellipse output nodes provide oscillatory response sinusoidal form specified frequency needed 
instance requiring neurons network produce outputs oscillate relations asin arbitrary frequency generate circle center point bb radius alternate schemes developed satisfactorily training recurrent network produce simple trajectories circle 
reported include generation significantly complex trajectories intersecting patterns discontinuous trajectories rnn output producing neurons form describing circle pattern turn generate sequence consequently task training network significant 
mentioned earlier nrnn selected feedforward network 
experiments discussed section network selected multilayer perceptron architecture number hidden layers nodes appropriately adjusted address complexity specific trajectory desired generated 
training multilayer perceptrons vast attention past powerful results available guide selection number hidden nodes mode processing training data batch processing real time processing result problem regarded significant 
furthermore major emphasis study demonstrate performance new approach decomposing training dynamic network training component networks 
consequently effort optimize learning nrnn experiments size network allowed grow order bound error learning mapping ordered sequences experiment tolerable levels 
nonlinear activation functions neurons network selected form hz 
illustrative experiments trajectory generation experiments conducted training produce various types trajectories 
involved spatio temporal patterns significantly complicated considered earlier researchers including overlapping segments discontinuous motions 
experiments included line control trajectory shape considered previous works 
section experiments order highlight strong points approach training briefly 
experiment consists generating shown 
problem benchmark solutions trajectory generation problem tested 
difficulty encompassed problem lies crossover point corresponds mapping case 
noted output rnn followed trajectory depicted 
nrnn multilayer perceptron inputs hidden layer neurons output layer neurons 
time needed train system negligible 
cycles correspond cycles trajectory defined sequence shown 
error desired trajectory resulting negligible 
second experiment consists generating trajectory points correspond crossover points differentiable see 
demanding problem mapping case previous example taken extreme 
points trajectory correspond crossover points 
points trajectory abrupt reversal direction travel required 
noted output rnn followed trajectory portrayed 
nrnn multilayer perceptron inputs hidden layer neurons hidden layer neurons output layer neurons 
time needed train system significant noticeably slower previous 
cycles trajectory correspond cycles trajectory defined sequence shown 
error desired trajectory resulting negligible 
second dimension dimension 
output rnn 
second dimension dimension second dimension dimension 
generated trajectories 

simulated cycles trajectory experiment 
third experiment consists producing trajectory generated previous example divided sections see 
idea example show important spatial discontinuities constitute obstacle architecture 
nrnn multilayer perceptron inputs hidden layers neurons output layer neurons 
time needed train system small larger previous cases 
cycles trajectory correspond cycles trajectory defined sequence shown 
error desired trajectory resulting negligible 
fourth experiment consists generating trajectory previous example amount counterclockwise rotation point controlled external input dnn 
case nrnn accepted sequence external signal allowed control amount rotation trajectory radians inputs 
idea example show dynamics trajectory controlled desired 
nrnn multilayer perceptron inputs hidden layer neurons second hidden layer neurons output layer neurons 
cycles base trajectory rotated multiples degrees starting degrees training purposes 
trajectory shown 
contrary previous examples training measured seconds dnn required hours order correctly trained 
error desired trajectory resulting negligible 
test generalization ability dnn external signal set degrees increased degrees time cycle trajectory defined sequence completed 
process followed cycle corresponding rotation degrees completed see 
clearly seen dnn correctly generalized problem 
fifth experiment designed show dnn previous experiment generate trajectories considered training 
previous experiment external input nrnn value continuously varied degrees trajectory completed revolution see 
resulting trajectory definitely belong set trajectories trained system 
example internal dynamics dnn mixed dynamics represented sequence external input 
possibility suggests cascaded order obtain complex dynamic behaviors 
second dimension dimension second dimension dimension 
simulated cycles 
plot left shows points trajectory connected 
plot right shows points unconnected 
second dimension dimension second dimension dimension 
trajectory system rotation angle varies zero degrees step size degrees 
plot left shows points trajectory connected 
plot right shows points unconnected 
second dimension dimension second dimension dimension 
trajectory system rotation angle varies degrees step size eighteen degrees 
plot left shows points trajectory connected 
plot right shows points unconnected 
second dimension dimension second dimension dimension 
cycle system angle varying continuously degrees 
plot left shows points trajectory connected 
plot right shows points unconnected 

expected approach effectively allowed reduction training complexity generation complex trajectories 


werbos backpropagation time proc 
ieee vol 
pp 


williams zipser learning algorithm continually running fully recurrent neural networks neural computation vol 
pp 


supervised training dynamical neural networks associative memory design identification nonlinear maps intl 
journal neural systems vol 
pp 
september 

recurrent neural network training learning automaton approach trajectory learning control system design ieee trans 
neural networks vol 
pp 
may 

pearlmutter gradient calculations dynamic recurrent neural networks survey ieee trans 
neural networks vol 
pp 
september 

learning trajectory adjoint functions teacher forcing neural networks vol 
pp 


lin trajectory production adaptive time delay neural network neural networks vol 
pp 


boe line improvement speed tracking performance repetitive paths ieee trans 
control system technology vol 
pp 

