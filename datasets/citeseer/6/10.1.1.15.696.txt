discovering solutions low kolmogorov complexity high generalization capability 
russell eds machine proceedings th international conference morgan kaufmann san francisco ca 
schmidhuber idsia corso lugano switzerland 
ch machine learning algorithms aim finding simple rules explain training data 
expectation simpler rules better generalization test data occam razor 
practi cal implementations measures simplicity lack power universality elegance kolmogorov complexity solomonoff algorithmic probability 
likewise pre vious approaches especially bayesian kind suffer problem choosing appropriate priors 
ad dresses issues 
reviews ba sic concepts algorithmic complexity theory relevant machine learning solomonoff levin distribution universal prior deals prior problem 
uni prior leads probabilistic method finding algorithmically simple problem solutions high generalization capability 
method levin com plexity time bounded extension kolmogorov complexity inspired levin optimal universal search algorithm 
problem solution candidates computed efficient self sizing programs influence runtime storage size 
probabilistic search algorithm finds programs ones quickly comput ing algorithmically probable solutions fitting training data 
experiments focus task discovering algorithmically simple neural networks low kolmogorov complexity high generalization capability 
experiments demonstrate alternatively tum germany 
method certain toy problems computationally feasible lead generalization results previous neural net algorithms 
number 
second number 
third number 
fourth number 
fifth number 
answer 
reason law 
nth number 
iq test requires answer 

reasons sim ple solutions preferred complex ones 
idea referred occam razor 
assumed simpler rules better generalization test data 
makers iq test assume everybody agrees simple means 
similarly researchers agree learning algorithms ought extract simple rules explain training data 
exactly simple mean 
theory providing convincing objective criterion simplicity theory kolmogorov complex ity algorithmic complexity 
contrary popular myth kolmogorov complexity due halting problem prevent ma chine learning applications tractable general extensions kolmogorov complexity 
machine learning researchers powerful tools provided theory see li vitanyi excellent overview see schmidhuber application fine arts 
purpose 
experiments intended demonstrate basic concepts theory kolmogorov complexity interest machine learn ing purposes encourage machine learning researchers study theory point problems concerning incremental learning mention initial steps solving 
outline 
section briefly reviews basic concepts algorithmic complexity theory relevant machine learning including levin complexity extension kolmogorov complexity levin universal optimal search algorithm 
broad class problems universal search shown optimal respect total expected search time leaving aside constant factor depend problem 
knowledge section presents general working implementation probabilistic variant uni search 
experiments section focus task finding simple neural nets excellent generalization capability 
section goes sections addressing incremental learning situations 
basic concepts turing machine tm mapping bitstrings bitstrings loss generality computes partial function fc fc undefined halt 
kolmogorov complexity kv finite string length short est program computes universal turing machine halts set possible halting programs forms prefix code halting program prefix ku min fu denotes length invariance theorem solomonoff kolmogorov chaitin states ku ku uni machines may drop index write ku 
machine learning prior problem 
machine learning applications concerned problem training data select probable hypothesis generating data 
bayes formula yields select maxi mal 
log equation leads prin minimum description length wallace rissanen 
prior come 
define priori probability distribution set possible hypotheses introducing arbitrariness 
perceived prior problem bayesian approaches 
theory algorithmic probability provides solution seen 
universal prior levin solomonoff chaitin 
define pv priori probability bitstring probability guessing halting program computes universal tm way guessing defined procedure scanning head input tape initially blank shifts right field scanned probability fill probability fill 
obtain fv different universal priors different universal machines probabilities string dif fer constant factor independent string size due invariance theorem 
may drop index write pa justifies name universal prior known solomonoff levin distribution 
univer sal priors appear convincing method assigning priori probabilities hypotheses computable objects 
dominance shortest programs 
shown levin chaitin probability string dominated probabilities shortest programs 
justifies oc cain razor equation priori proba ble minimal kolmogorov complexity 
dominance universal prior 
suppose infinitely enumerable solution candidates strings amazingly pu dominates discrete enumerable including probability distributions see li vitanyi details sense constant pv cp strings kolmogorov complexity general universal prior 
popular myth states fact renders useless concepts kolmogorov complexity far practical machine learning concerned 
seen 
focus natural computable general extension kolmogorov complexity 
levin complexity 
follows necessarily halting program string input tape scanned completely scan program finishes printing tape 
number steps taken printed 
kt min log 
invariance theorem similar holds kt 
levin universal optimal search algorithm levin 
suppose looking solution string problem 
levin univer sal search algorithm generates evaluates strings solution candidates order kt complexity solution 
essentially equivalent enumerating programs order decreasing probabilities divided runtimes 
program computes string tested see solution problem 
search stopped 
amazingly broad class problems including inversion problems time limited optimization problems universal search shown optimal respect total expected search time leaving aside constant factor independent problem size string computed time steps program probability guessing time steps systematic enumeration levin generate run time steps output experiments probabilistic algorithm strongly inspired universal search 
probabilistic search levin algorithm considered interest theo purposes see allender li vitanyi 
im plemented experimental applications fear constant factor may large 
knowledge general universal search implemented time project led 
see heil diploma thesis tum 
solomonoff apparently implemented restricted versions 
follows focus working implementation slightly different probabilistic algorithm levin complexity strongly inspired universal search 
tal results obtained probabilistic algorithm see section similar obtained original universal search procedure 
overview 
method described section searches finds algorithms compute solutions problem specified possibly limited training data 
goal discover solutions high generalization performance test data unavailable search phase 
purpose probabilistic search algorithm randomly generates programs written general assembler programming language sequences integers 
programs may influence storage size runtime 
program computes solution candidate tested training data 
probability generating program upper bound tmax runtime essentially equals quotient probability guessing tma implies candidates low levin complexity preferred candidates high levin complexity 
measure generalization performance candidates fitting training data evaluated test data 
experiments section solution candi dates weight matrices neural net supposed solve certain generalization tasks difficult impossible solve conventional neural net algo rithms 
universal programming language 
programs sequences integers 
stored storage consisting single array cells 
cell integer address interval sw sp 
sw sp positive integers 
program tape set cells addresses sp 
tape set cells addresses 
cells non negative addresses belong program tape 
cells negative addresses belong tape 
contents cell address denoted ci maxint maxint type integer implemented version maxint equals 
execution program portion program tape may increase 
portion tape may increase decrease 
time step variable max denotes topmost address storage 
variable min address 
time legal addresses dynamic range min max definition 
time integer sequence written pro gram tape address max called current program 
max implies empty program 
instructions 
time variable may equal address cells contents may interpretable instruction 
hops different possible instruc tions implemented version hops 
instruction uniquely represented instruction number set nops 
instruction may arguments type integer 
arguments stored addresses address instruction 
argument instruction legal argument range set integer values argument allowed take 
certain limits legal argument ranges dynamically modified programs seen shortly 
initialization time limits time probability 
execution program run variables min set zero 
variable define upper bound runtime current program 
obtain probabilistic variant universal search chosen randomly follows ele ments set drawn equal prob ability drawn 
nt denote number trials 
set equals time steps program allowed execute instructions may choose halt earlier 
exceeds replaced 
time probability current program defined short runtimes max long runtimes 
instruction cycle oracles 
single step program interpreter works follows equals max interpreted request oracle 
primitive corresponding arguments chosen randomly set legal options described 
sequentially written program tape starting 
max increased accordingly reflect growth program tape 
new primitive gets executed growth sp halts program 
oracle request equals con tent ci nops corresponding number arguments ni corresponding legal argument ranges looked checked contents ni addresses current address 
instruction syntactically correct gets executed 
current program halted 
executed primitive change value causing jump set point address follow ing address argument current instruction 
instruction executed current runtime incremented 
reached program halted 
runs programs space probability 
initialization instruction cycle repeated halt situation encountered 
space probability program defined product probabilities arguments primitives requested executed runtime 
essentially space probability probability guessing executed content program tape 
probabilistic search 
programs generated randomly executed described results evaluated problem specific performance criterion met 
obviously results low levin complexity preferred results high levin complexity 
similar alternative imple mentation original universal search algorithm systematically generate solution candidates order levin complexities 
see heil diploma thesis tum 
primitives 
instruction numbers semantics primitives experiments listed 
expression form address denotes value interpreted address ith cell containing current instruction indirect addressing 
list assumes syntactical correctness instructions 
rules legal argument ranges syntactical correctness shortly 
address address 
con tents address equal contents address set equal address 
output 
primitive interaction environment 
corresponds tm action writing output tape see section 
ex periments output called write weight 
generate weights neural network 
variants specified needed 
jump address 
set equal address 

halt current program 
add address address address 
contents address added contents address re suit written address 
address address 
primitive interaction external environment 
requires separate input fields may modified environment experiments equal 
reads current value ith put field address value address 
conjunction primitives changing environmental state provides opportunity exploiting computing resources outside world 
applications described pretty useless input fields remain zero time 
move address address 
contents address copied address 
allocate address 
size tape increased value address new cells initialized zeros 
min updated accordingly growth halts program 
variable written space allocated tape 
explained allocate essential self sizing programs 
increment address 
contents address 
decrement address 
contents address decremented 
subtract address address address 
contents address subtracted contents ad dress result written address multiply address address address 
contents address multiplied contents address result written address free address 
size tape decreased value address 
min updated accordingly 
primitive complements allocate 
rules legal argument ranges syntactical correctness 
jumps may lead ad dress dynamic range min max recall max equals current value 
operations read contents certain cells add move may read addresses min max 
operations change contents certain cells may write tape addresses min 
program tape read execute random writes requested moves 
easy 
tape read write execute 
results arithmetic operations leading underflow overflow replaced maxint maxint respectively 
tape cells may allocated freed time 
comments 
universality 
difficult show primitives form universal set sense composed form programs writing computable integer sequence tape size range limitations 

self sizing programs 
programs ence size 
keep small avoiding requests new oracles avoiding jumps current locate free balanced way 
oracle requests allocate free provide ways influencing number visible legal addresses available storage 
oracle requests source randomness 
current program request oracles space probability tend remain low program may perform extensive computations 
bigger storage smaller probability guessing particular visible address arguments instructions generated oracle requests 
small beautiful probable 

probabilistic setting 
probabilistic search algorithm original universal search procedure 
reason avoid unintended bias 
instance unintended bias may intro duced imposing systematic say alphabetic order programs equal quotients probability runtime 
drawback probabilistic version programs low levin com plexity general tested 
speed issue prefer systematic enumeration slightly complicated probabilistic variant expected search time equals systematic enumeration variants systematic universal search primitives implemented collaboration norbert stefan hell 
examples low total search time main issue simulations section probabilistic search intended highlight generalization performance speed 
similar results obtained systematic search 
simple neural nets previous relationship neural network complexity generalization ity baum haussler hinton nowlan mackay hassibi vapnik maass general sense solomonoff kolmogorov levin 
likewise previous implementations measures simplicity lack universality elegance kolmogorov complexity algorithmic information theory 
previous approaches ad hoc usually gaussian priors 
reasons remainder devoted simulations general method uni prior self sizing programs probabilistic search algorithm preferring candidates low levin complexity candidates high levin complexity 
certain toy problems demonstrated approach lead generalization results traditional neural net algorithms 
experiments average program runs time steps halting halted 
programs running millions time steps course 
task counting inputs pattern association task may triv ial difficult traditional approaches providing training exam pies 
task 
linear perceptron network input units output unit weights fed dimensional binary input vectors 
de denotes ith com notes th input vector 
ponent ranges 
input vector exactly bits set bits set zero 
obviously possible inputs 
network put response yp wi th weight 
weight may take integer values 
task find weights yp equals number bits possible number solution candidates search space possible weight vectors huge exhaustive search 
solution 
solution problem wi equal 
kolmogorov complexity solution small short program computes 
levin complexity small logical depth runtime shortest program bennett time steps 
training data 
illustrate generalization capability search solution candidates low levin complexity training examples 
randomly chosen possible inputs 
training example bi nary vector bits positions bits 
second bits positions 
third bits positions 
cases desired output target 
conventional neural net algorithms fail solve problem 
training set small conventional perceptron algorithms solve apparently simple problem 
achieve generalization unseen test data 
reason connections units ways won changed gradient descent algorithms 
note scaling inputs differently going improve matters 
weight decay 
weight decay encourages weight matrices zero entries 
current task bad strategy 
probabilistic search 
search procedure fol lows probabilistic search algorithm described section lists executes programs computing solution candidates weight vectors 
primitive write weight replacing output see section writing network weights 
argument uses variable values set 
run weights initial ized 
instruction number semantics write weight follows compare list primitives section write weight address 
set equal contents address variable weight pointer incremented 
halt range 
solution candidate fits training data exactly solution tested test data 
note reward goal task measure success binary network fits training data doesn 
teacher providing informative error signal distance desired outputs 
results 
programs generating networks fitting training exemplars runs 
led perfect generalization oo unseen test examples 
typical programs conditional jumps generate correct solutions 
see schmidhuber details 
comment 
task probabilistic search self sizing programs leads excellent generalization performance 
theory possible appropriate variant nowlan hinton approach achieve generalization performance task 
nowlan hinton encourage groups weights allow real valued weights set equal contents address divided say 
equal values strategy case 
reason task requires weights equal values 
kolmogorov complexity solution low 
task adding input positions task 
perceptron network input data 
goal different harder achieve 
task find weights yp equals sum positions bits oo possible task difficult providing limited training data 
solution 
solution problem wi equal example short fast programs computing solution 
training data 
training inputs previous task 
target values different 
obviously target input vector 
target input vector 
target input vector 
success binary solution candidate fits training examples exactly solution evaluated test data 
conventional neural net algorithms fail solve problem reasons fail solve previous problem see section 
training set small obtain reasonable general ization test set 
results 
programs generating networks fitting training data runs total search time time steps 
successful runs led perfect generalization unseen test examples 
typical programs allocate increment write weight conditional jumps generate correct solutions 
solution example 
weight vector fitting training data runs 
corresponding program pretty wild 
led perfect generalization test data 
halting program allocated time steps 
time probability 
space probability 
program statements marked addresses allocate cells tape 
initialize zero 
set min min 
get contents input field see list instructions section position write address 
write contents address weight pointed increment 
halt range 
contents address equal contents address goto address 
goto address 
increment contents address 
goto address 
contents address equal contents address true goto address 
instructions addresses useless 
trophic 
essentially program allocates space variable initially zero tape recall program tape read execute variables 
executes loop incrementing writing variable contents network weight vector 
see schmidhuber additional details elegant solutions system 
writes arguments keep task section 
primitive write weight redefined gets additional argument 
primitive redefined gets new name 
separate automatic increment mechanism position 
new primitives may directly address read write network weights 
primitives remain 
new ones instruction numbers compare section address address 
wi set equal contents address value address 
address address 
wi written address location address value address 
appropriate syntax checks halt programs attempt impossible writing non existent weight 
new write weight primitive additional argument guessed correctly successful programs tend 
results 
runs total search time time steps runs generated weight vectors fitting training data 
allowed perfect generalization test data 
execution filled storage seen table 
program ran allocated time steps 
space probability allocate cell tape 
initialize zero 
set min min 
increment contents address 
wc equal contents address 
jump address repeated execution instruction address unnecessarily allocates cells tape damage slightly slowing program 
different sets training exam ples obtained randomly permuting input units led similar generalization results 
numerous additional experiments including complicated maze tasks performed norbert stefan hell tum 
described heil diploma thesis 
general remarks 
bias algorithmic simplicity general 
weaker kinds problem specific inductive bias 
utgoff haussler 
solution simple bias justified require know way solution simple 
solution simple bias algorithmic simplicity won damage case algorithmically complex solutions lose focus search simple candidates looking complex candidates 
general complex candidates greatly outnumber simple ones 
simple ones don significantly affect total search time optimal search algorithm 
incremental learning typical incremental learning situations real world informative feedback tasks 
original universal search procedure formulated levin designed optimal error feedback incremental learning 
appears reasonable way appropriately extending universal search 
ideas solomonoff paul theoretical 
schmidhuber mutations previously useful pro grams listed order levin complexities additional improvements 
schmidhuber presents experimental results 
show incremental extensions allow faster learning tend find elegant pro grams 
ongoing research 
time incremental learning general environments put basis appears theoretically sound schmidhuber goes current pa presenting novel machine learning paradigm called incremental self improvement paradigm 
principle probabilistic system paradigm able previous experience improve improve way improves essentially system uses previous experience learn modify context dependent primitive probabilities way leads success time interval learning better better computational resources 
basic ideas briefly described concluding tion 
incremental self improvement paradigm novel machine learning paradigm schmidhuber goes non incremental approach 
maximize cumulative payoff reinforcement reward obtained en tire span life environment system incremental self improvement continually attempts compute action subsequences leading faster faster payoff intake 
system designed action subsequences may fact represent ex ecution arbitrary algorithms including learning algorithms approach general theoretically sound 
central novel aspects follows schmidhuber exemplifies describing concrete implementation 
computing self modifications 
sections initially highly random actions system primitive instructions turing machine equivalent programming language allows implementing arbitrary learning algorithms 
action subsequences represent normal interactions environment self modification sequences 
self modification sequences compute arbitrary modifications probabilities action subsequences including self modification addresses 
contents 
table storage execution successful program adding write weight primitive arguments 
sequences learning system able modify universal way 
explicit difference learning meta learning kinds information processing 

life way 
action learning system including probability modifying actions executed self modification sequences viewed singular event history system life 
unrealistic concepts exactly repeatable training iterations boundaries trials epochs thrown 
general environment reset 
life way 
lifelong training episode 
learning inductive inference non repeatable experiences 

evaluations self modification sequences 
system time varying utility value average payoff time system start 
completed self modification sequence time varying utility value 
value average amount payoff time measured sequence began execution 
previous systems evaluations utility take account computation time required learning including time required evaluating utility 

recursive definition useful self modification sequences 
system keeps track probability modifications computed completed sequences considers useful 
usefulness defined recursively 
previ ous useful self modification sequences system start completed self modification sequence considered useful long utility value exceeds system utility value 
completed self modification sequences considered useful long higher utility values preceding self modification sequences currently considered useful 
essentially system keeps modifications probability values originated useful sequences 

acceleration payoff intake cal soundness 
shown time system tends better better computational resources 
fact shown accelerates payoff reinforcement intake long run sense time step life system execution self modification sequences self computed valid modifications strategy followed faster average payoff intake previous valid modifications system start 
somewhat surprisingly nature environment matter instance interface pos non deterministic environment 
non incremental system described sections incremental self improvement appears promising way dealing lifelong incremental learning 
system incremental self improvement implemented tested simple toy tasks schmidhuber 
expected experimental results consistent theoretical predictions 
ray solomonoff peter dayan martin sepp hochreiter daniel mark ring jan erik gerhard valuable comments schmidhuber 
allender 
application time bounded kolmogorov complexity complexity theory 
watanabe editor kolmogorov complexity computational complexity pages 
eatcs monographs theoretical computer science springer 
baum haussler 
size net gives valid generalization 
neural computation 
bennett 
logical depth physical complexity 
universal turing machine half century survey volume pages 
oxford university press oxford gc hamburg 
chaitin 
length programs computing finite binary sequences statistical considerations 
journal acm 

theory program size formally iden tical information theory 
journal acm 
cs 
symmetry algorithmic information 
soviet math 
dokl 
hassibi stork 
second order derivatives network pruning optimal brain surgeon 
moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
haussler 
quantifying inductive bias ai learning algorithms valiant learning framework 
artificial intelligence 
heil 
und ler nen diploma thesis 
fakult informatik lehrstuhl prof brauer technische universit 
hinton van camp 
keeping neural net works simple 
proceedings international conference artificial neural networks amsterdam pages 
springer 
kolmogorov 
approaches tive definition information 
problems informa tion transmission 
levin 
universal sequential search problems 
problems information transmission 
levin 
laws information foundation probability theory 
problems information transmission 
levin 
randomness conservation inequalities information independence mathematical theories 
information control 
schmidhuber 
discovering problem solutions low kolmogorov complexity high generalization capability 
technical report fakult informatik technische universit 
short version russell eds machine learning proceedings twelfth international conference morgan kaufmann publishers pages san francisco ca 
schmidhuber 
low complexity art 
technical re port fakult informatik technische universit 
schmidhuber 
learning learn learning strategies 
technical report fakult informatik technische universit november 
revised january 
solomonoff 
formal theory inductive inference 
part information control 
solomonoff 
application algorithmic probability problems artificial intelligence 
kanal editors uncertainty artificial intelligence pages 
elsevier science publishers 
utgoff 
shift bias inductive concept learning 
michalski carbonell mitchell editors machine learning volume pages 
morgan kaufmann los altos ca 
vapnik 
principles risk minimization learning theory 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
wallace boulton 
information theoretic measure classification 
computer jour nal 
li vit 
kolmogorov complexity applications 
springer 
maass 
perspectives current research complexity learning neural nets 
roychowdhury siu editors theoretical advances neural computation learning 
kluwer academic publishers 
mackay 
practical bayesian framework backprop networks 
neural computation 
nowlan hinton 
simplifying neural networks soft weight sharing 
neural computation 
paul solomonoff 
autonomous theory building systems 
manuscript revised 
rissanen 
modeling shortest data description 
automatica 
