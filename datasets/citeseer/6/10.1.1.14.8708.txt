trajectory generation modulation dynamic neural networks pablo generation desired trajectory behavior neural networks involves particularly challenging temporal learning problem due need employing architectures recurrent connections 
introduces novel solution trajectory generation problem designing dynamic system terminal behavior emulates prespecified spatio temporal pattern independently initial conditions 
proposed solution uses dynamic neural network dnn hybrid architecture employs recurrent neural network rnn cascade non recurrent neural network nrnn 
rnn charge generating simple limit cycle nrnn devoted reshaping limit cycle desired trajectory 
main advantage architecture simplicity training results simplification training task due decomposition independent spatial temporal learning subtasks turn permits reduce training complexity training feedforward neural network 
solution trajectory generation problem involves synthesis simple rnn eliminates need type training network enables required training focused nrnn 
part design dynamic neural network architecture systematic synthesis procedure design relay control systems developed configuring rnn produce limit cycle elementary complexity 
shown cascade arrangement rnn appropriately trained nrnn emulate desired trajectory behavior irrespective complexity 
interesting solution trajectory modulation problem line modulation generated trajectories external inputs 
results experiments included demonstrate capabilities performance dnn handling trajectory generation modulation problems 
pablo facultad de universidad de los andes santiago chile 
electrical computer engineering department university arizona tucson arizona usa 
mails cl edu respectively 
classical problem nonlinear dynamics field design system produces pre specified set attractors order ensure convergence desired limit cycle behavior irrespective initial conditions 
problem consists finding set nonlinear differential equations solution converges certain desired trajectory independently starting point 
exact opposite classical differential equations analysis set equations starting point known solution sought 
design systems behavior converges pre specified trajectory independently system initial conditions referred trajectory generation problem 
generalization occurs system uses external inputs control trajectory generation process 
control signals allow system produce certain trajectory independent starting point system shape pattern real time input excitation 
design systems control trajectory generation process means external inputs referred trajectory modulation problem 
problems roots deeply embedded mathematics arena appli cations extend areas engineering biology 
classic engineering example adaptive control industrial manipulators trajectory generation problem arises trajectory manipulator needs 
example trajectory modulation problem design robot learn control limbs order catch ball thrown different places 
case machine needs generate trajectory describes movements robot modulate real time changing trajectory ball 
biological arena examples readily learning self sustained oscillatory patterns walking swimming correspond motions converge desired periodic trajectory behavior 
particular aspect trajectory modulation problem deserves emphasis clearly understand challenges posed problem 
note problem reduces new trajectory generation problem dynamic system allowed re designed neural network implementation system permitted re trained produce new attractor pattern 
system parameters need maintained unchanged robot example discussed trajectory behavior modified external inputs suit new tasks problem interesting quite challenging 
relation problem known concepts adaptive control deserves particular note 
capability dynamic tuning system produces specified trajectory pattern trajectory continuously modulated order implement desired transformations basic pattern rotations translations stretching shifts scale changes attractive real world applications 
instance appropriate variation signals input neural network trained produce basic periodic trajectory pattern may develop ability control line precise shape trajectory keeping repetition period alternately control speed repetition trajectory 
importance ensuring capability line adjustment tracking speed order control tracking error robot manipulators quite known 
various applications need arises modulating trajectory pattern desired ways literature control systems 
needs emphasized requirements precise line adjustment trajectory output nonlinear system termed trajectory modulation quite challenging meet 
despite importance trajectory generation modulation problems noted satisfactory solutions problems general neural networks particular 
explained inherent complexity problems involve temporal learning tasks 
true static problems pattern memorization function approximation spatial requirements need taken account time plays role 
problems satisfactorily addressed existing feed forward neural network theory techniques feed forward neural networks called non recurrent neural networks 
area existing theory helps understand generalization properties architectures ensures convergence training processes desired outputs tt 
hand temporal objectives inherent dynamical problems require neural networks recurrent connections referred recurrent neural networks rnns 
considerable effort spent past developing new rnn architectures training techniques possible solve dynamical problems degree success neural networks static problems 
proven rnn approximate known dynamical system techniques training rnns developed 
subset works directed trajectory generation problem shown rnn trained produce desired trajectory behavior demonstrated success training algorithms generating certain benchmark trajectories circle quadrangle pattern 
techniques solve trajectory generation problem arbitrarily complex trajectories acceptable computational complexity 
inherent difficulty explained fact solution spaces plagued local minima possible find global solutions 
gradient descent approaches gradients tend vanish dynamics neural networks evolve 
circumvent problem approaches gradient descent proposed 
provide satisfactory solutions cases convergence useful results guaranteed 
techniques monotone systems theory quite simple trajectories scale higher dimensionality problems complex dynamic behaviors known 
production asymptotically static behavior rnns fixed point training understood point possible build practical applications 
sum general practically feasible neural network solutions trajectory generation problem due inherent complexity 
trajectory generation problem attracted considerable attention times evidenced trajectory modulation problem hardly addressed 
generalization trajectory generation problem difficult solve 
known techniques cope trajectory generation problem practically acceptable manner surprising available techniques offer general solutions trajectory modulation problem 
emphasize complexity problems needs noted recurrent networks dynamic systems quickly move desired mode dynamic activity production bounded oscillation stable limit cycle undesirable modes unstable responses unbounded outputs multiple attractor patterns chaotic behavior network parameters external inputs carefully regulated 
simple trajectory desired trajectory yv rnn nrnn fig 

shaping trajectory 
fig 

dnn architecture 
novel computationally attractive solutions trajectory generation modulation problems 
core idea underlying approach possible design simple dynamic system produces simple stable robust trajectory system deform trajectory desired behavior produced see fig 

analogy task system generate elastic rope shaped simple geometry task second system stretch warp rope desired final geometry 
implementation neural networks idea suggests bounded continuous tra arbitrarily complex shape generated dynamic neural network dnn architecture comprised rnn produce simple trajectory cascaded nrnn deform desired trajectory see fig 

furthermore rnn required generate trajectory elementary complexity problems associated training considerably reduced employing synthesis procedure building rnn described limiting required training nrnn 
simplification training task fundamental enables solution developed complex trajectory modulation problems 
structure follows 
section mathematical result formalizes central ideas facilitates development neural network solution trajectory gener ation modulation problems 
section describes details systematic synthesis procedure inspired advances relay feedback control configuring rnn part architecture 
dnn implementations performance evaluations handling diverse trajectory generation trajectory modulation problems section 
concludes final remarks section 
ii 
mathematical principles underlying solutions trajectory generation modulation problems 
trajectory generation dnn theorem presents core idea precise manner theorem set data pairs ti ti obtained mapping trajectory yl generated process gl generated process integer arbitrarily close provided implemented nrnn appropriate vc dimension implemented complies condition ti tj ti tj ti tj 
proof mapping reformulated composition mappings trajectory generated process integer provided functions exist 
process chosen complies condition expressed eq 

requirements existence mapping correspond ensure existence function 
previous conditions met function implementable nrnn 
set data pairs ti ti output mapping gl arbitrarily close provided implemented nrnn appropriate vc dimension suggested theorem bounded input bounded output trajectory generation process reformulated cascaded processes maps time intermediate trajectory second maps intermediate trajectory sequence approximates 
division process control temporal aspects output trajectory uses control spatial ones 
important advantage dnn architecture shown fig 
long conditions specified theorem ii satisfied process chosen simple possible 
dnn implementation requires low complexity process irrespective complexity trajectory needs learned 
evident discussion especially important comes implementing process neural networks 
characteristic dnn architecture allows simple rnn 
rnn model extensively employed trajectory generation tasks continuous time model described set coupled differential equations xi xj ii xi denotes state tn neuron instant time constant referred relaxation time wij denotes weight interconnection neuron tn fj defines nonlinear threshold function typically saturating function denotes number neurons 
different forms threshold functions employed typically function form fi ri parameter controls slope threshold function 
simple check function satisfies basic properties required threshold functions monotonicity saturation bounded 
inputs network initial conditions xi outputs observations state trajectories xi 
task designing network serve useful computational device involves selection parameters wij ti ri implementing appropriate training algorithm usual approach carefully designed synthesis procedure described network trajectories xi starting xi behave prescribed manner perform desired computations 
illustration order generate simple dimensional trajectory circle center origin plane radius node rnn produces outputs oscillating designed 
may noted network periodic repetition circle trajectory terminal behavior starting initial condition xi inside outside circle 
context dnn architecture shown fig 
network designed implement rnn block enable generation circular trajectory output 
trajectory ultimately needed displayed circle center origin nrnn set simulate identity function nrnn block rnn nrnn ell eta modulating fig 

modified dnn external inputs 
eliminated altogether 
desired output corresponds complex trajectory behavior considered appropriate nrnn approximates input output mapping function implemented details relegated section 
addition external inputs trajectory modulation handle trajectory modulation problems basic dnn architecture modified adding external input signals alter activation dynamics neurons 
second input addition excitation provided initial conditions xi allows control output trajectory making possible generate trajectory families trajectories 
introducing external input clearly options apply neurons rnn apply neurons nrnn apply 
options implies different degree complexity 
evidently applying input neurons dynamic network viz 
rnn third options results output produced convolution applied input signal impulse response rnn 
consequently designing appropriate input re shaping basic trajectory pattern produced dnn cases difficult 
simpler approach altering shape final trajectory keeping track perturbed shape second option apply external inputs directly nrnn bypassing rnn block see fig 

emphasized application external inputs nrnn powerful achieving spatial mapping simplest implement 
option explored 
theorem formally presents feasibility dnn external inputs introduced shown fig 
solving trajectory modulation problem theorem set data triplets ti ei ti ei ei obtained mapping trajectory yl generated process gl generated process integer arbitrarily close provided implemented nrnn appropriate vc dimension implemented complies condition expressed eq 

proof analogous proof previous theorem iii 
synthesis rnn theorem ii dnn architecture configured implement process deal temporal aspects trajectory generation problem implement function deal spatial ones 
described earlier function approximated nrnn efficient way implementing process involves rnn 
temporal requirements generation trajectory principle possible solve trajectory generation problems rnn 
significant importance fact rnn need complicated 
long condition stated theorem ii satisfied rnn chosen simple possible 
considerations suggest training new rnn problem efficient priori synthesize trajectory generation problems 
training differs synthesis process requires data samples guide adaptation process 
hand synthesis procedure uses knowledge problem analytically design rnn behavior satisfies required constraints 
synthesis procedure need learning process dnn architecture allows solving trajectory generation problems rnn synthesis rnn preferred 
results simplification training dnn computational requirements focused training nrnn 
sum dnn architecture permits simple previously synthesized rnn solve trajectory generation problem 
stated theorem ii trajectory generated rnn existence function possible 
words process comply condition stated eq 

synthesis rnn guided requirements convergence desired behavior independently initial conditions robustness noise 
delay fig 

monotone trajectory 
fig 

dimensional periodic trajectory 
corresponds classical problem automatic control theory design dynamic system converges certain desired behavior irrespective initial states noise conditions operates 
sections procedure synthesizing rnn solution problem 
general requirements approach consists synthesizing rnn generates dimensional output 
simple solution rnn generates dimensional monotone trajectory see fig 

rnn naturally comply condition stated eq 
output values different defined time interval 
solution inappropriate implementation practice 
cases dnn replicate final desired trajectory times case producing limit cycle behavior necessarily delay repetitions caused time required rnn return starting point monotone trajectory see fig 

reset time nrnn receiving arbitrary input generating spurious output 
solution satisfactory implement slightly increased complex ity search solution set rnns produce dimensional outputs 
convenient way solving problem rnn produces dimensional periodic trajectory pattern cross new cycle starts see fig 

period cycle rnn able continuously drive nrnn delays arbitrary inputs produce spurious outputs 
rnn generates trajectory characteristics naturally complies condition expressed eq 
cause points plane corresponding output values distinct time interval 
section introduces design approach produces systems outputs fig 

relay feedback 
naturally converge type limit cycles started arbitrary initial conditions 
generating limit cycles relay feedback linear system described transfer function relay feedback output multiplied passed hard threshold defined output control signal drive system see fig 

configuration systems naturally stabilize sinusoidal limit cycles 
fact synthesize rnn generates stable robust limit cycle 
threshold function defined eq 
called symmetric relay 
asymmetric relay output input input produce complex limit cycle behaviors 
purposes simpler symmetric relay suffice 
typical variation signals appear various points system relay feedback shown fig 

wl denote frequency limit cycle oscillation desired set steady state conditions 
discussion section evident output system approximates sin second output derivative approximates cos possible produce oscillatory behavior approximates circle type trajectory 
theorem prescribes conditions satisfied generating output desired form 
theorem set sufficient conditions system shown fig 
produce oscillatory output amplitude frequency steady state conditions 
low pass filter cutoff frequency wc bounds oz ii fig 

signals relay feedback control system 

zg 
denotes relay threshold specified eq 

proof output relay square wave amplitude period fourier series expansion consists fundamental frequency cot odd ordered configured harmonics wl wl 
furthermore amplitude fundamental function low pass filter cutoff frequency wc keeps fundamental frequency filters higher order harmonics output system steady state conditions approximately sinusoidal function frequency wt amplitude 
order oscillate ol go zero instants relay switches positive negative vice versa 
fundamental component opposite phase system introduce phase equal 
synthesis appropriate provides approximate low pass filtering char practice realization pole transfer function sought 
condition theorem iii 
satisfied simplest transfer function considered important note order transfer function satisfy condi tion second order system jw attain asymptotically 
eq 
poles need selected condition needs satisfied 
offers degrees freedom arbitrarily denotes state vector tx cx system values parameters selected relay feedback system shown fig 
produces steady state oscillatory output period approximately particular importance fact periodic trajectory attained terminal conditions attractor trajectories starting arbitrary initial states 
robustness attractor pattern variations initial selecting third parameter evaluated cv yield cot gain adjusted satisfy third condition theorem iii obtain completing design 
design evident steady state conditions relay feedback system shown fig 
eq 
produces oscillatory output approximates 
period trajectory close important note design approximate analysis considering behavior fundamental component fourier series expansion output relay ignoring higher harmonics 
illustrated development appendix exact expression period system output exact shape obtained desired 
objective employ relay feedback system generate arbitrary periodic trajectory cross exact value period trajectory shape particular significance 
section rnn implements relay feedback system shown fig 
eq 
synthesized 
facilitate may noted state space realization transfer function obtained state significant importance controlling accuracy generated trajectory property readily follows stability limit cycle generated relay feedback system 
proof stability limit cycle relegated appendix directly relevant steps design process 
effect attractor pattern varying values parameters discussed section 
simple see output oscillation selection ca enables controlling period speed repetition final trajectory controlled desired appropriate selection parameter 
synthesizing rnn emulates relay feedback system rnn behaves relay feedback system synthesized method ing rnn emulates third order system transfer function eq 

continuous time rnn model described eq 
expanded permit external input ici xi xj bij np gains multiplying input signals ej 
model threshold function fi described eq 
synthesizing rnn 
simplicity development parameter values ri node network considered 
selection ri ensures slope fi near origin unity 
external input needed chosen 
states neurons linear region threshold function network dynamics approximated wu xl equating eqs 
weights rnn emulates third order system obtained wu 
small values states neurons threshold function approximated linear characteristic rnn synthesized emulate 

fig 

architecture rnn synthesized produce limit cycles 
problem remains solved third order neural network realization described synthesize rnn stabilizes limit cycle 
attractive approach synthesize rnn operates relay feedback system described earlier 
case neural network needs implement relay third order system feedback connection configuration shown fig 

relay function realized memoryless nonlinear function static neuron threshold function parameter value rz order realize high slope near origin 
output relay neuron input third order rnn output rnn fed input relay neuron weight 
output relay feedback rnn thresholded state xa third neuron 
neural network realization entire system set values shown fig 

output generated rnn parameter values shown fig 

seen steady state conditions stabilizes sinusoidal trajectory period close analytically computed value la seconds 
small deviation actual period value due fact analysis considering fundamental fourier component output relay synthesized rnn approximates behavior third order system 
actual nonlinearities threshold functions neurons introduce small deviations desired behavior 
deviation concern objective realize periodic trajectory output system obtained neural network realization successfully meets objective 
fig 
shows outputs produced rnn started different initial conditions 
expected proof stability limit cycle various initial conditions rnn fig 

output rnn fig 

output rnn different initial conditions 
settles periodic trajectory pattern albeit different phase shifts 
influence value parameter shape rnn output seen fig 
trajectories different values shown 
expected smaller values produce trajectories small amplitudes desirable keep states neurons linear region threshold functions 
may noted periods trajectories shown fig 
differ trajectories viz 
corresponds operation may extend non linear region neurons 
simulations value 
fig 
shows influence parameter period trajectory comparing outputs different values 
expected periods close seconds 
analytically computed values la 
seconds rnn dnn architecture shown fig 
produces trajectory 
order produce dimensional limit cycle complies condition stated eq 
second component needed 
simple way generating component thresholded state second neuron signal 
reason choice corresponds synthesis rnn 
fig 

output rnn different values different initial conditions 
smaller ampli tude curve corresponds 
fig 

output rnn different values different initial conditions 
curve larger period oscillations corresponds 
figures show convergence limit cycle trajectory different initial conditions 
expected proof stability limit cycle terminal attractor pattern attained independently initial conditions 
particular interest trajectory meets condition expressed eq 

synthesized rnn drive nrnn dnn architecture shown fig 
generating desired trajectory 
need adjusting period limit cycle match length trajectory reproduced 
iv 
trajectory generation modulation performance dnn issues interest training dnn far synthesis rnn training nrnn completely independent building blocks 
true operational point view necessarily training point view 
part priori knowledge trajectory generation problem time span trajectory learned period limit cycle generated rnn frequency oscillation cl fig 

dimensional limit cycle fig 

fig 
showing evolu different initial conditions 
tion time 
just matter value starting point manually trim value desired period obtained 
human intervention allowed period signal determined standard signal processing techniques parameter adjusted accordingly 
output rnn evolve converges desired limit cycle 
training nrnn data pairs oi yi oi output rnn time ti yi desired output dnn instant 
training stage completed simply matter letting rnn feed output input nrnn system operate 
mentioned earlier nrnn feedforward network 
experiments architecture choice multilayer perceptron 
training multilayer perceptrons received considerable attention past techniques available literature 
emphasis providing feasible solution necessarily best trajectory generation problem standard back propagation algorithm train nrnn specific efforts optimize training process 
reasons particular attention paid nrnn appropriate vc dimension long ultimate problem trajectory generation solved reasonably 
threshold function neurons nrnn defined eq 
value ri selected 
evaluation trajectory generation performance evaluate performance dnn architecture handling generation different types trajectories number experiments conducted 
experiments included producing patterns attempted earlier researchers circles quadrangle 
addition challenging patterns considered earlier researchers generated 
expected dnn handled trajectories remarkable ease able reproduce patterns closely earlier works 
sake conciseness experiments summarized section choosing challenging ones 
experiment consists generating pattern 
problem benchmark solutions trajectory generation problem traditionally tested 
difficulty encountered problem lies crossover point corresponds mapping 
nrnn experiment multilayer perceptron inputs hidden layer neurons output layer neurons 
superimposed cycles depicted fig 
shows error associated resulting trajectories negligible 
expected crossover point pose trouble dnn 
comparing quality trajectory generated experiment produced previous researchers cited possible highlight strong points approach trajectory generation problem 
note hardly mismatch successive cycles trajectory trajectories shown fig 

level accuracy production benchmark trajectory demonstrated earlier works see instance clearly noticeable mismatch successive cycles pattern shown 
secondly importantly computations needed implement training scheme learn trajectory consideration comparing different approaches 
methods error backpropagation require gradient calculations known generate significant computational complexities implemented recurrent neural nets 
fact noted considerable research training recurrent neural nets directed designing training algorithms eliminate need computation gradients see example 
sophisticated methods developed uses constrained optimization approach maximum principle requires computation derivatives implementing training algorithm incurs lot computational overhead correspondingly increasing time train 
contrast approach computational problems entirely avoiding training rnn requiring nrnn trained simple due non dynamic nature network 
fact able produce desired trajectory patterns accurately time requiring computations clearly underscore superiority approach trajectory generation demonstrated experiment 
advantages resulting capability generate complex trajectories tried earlier researchers capability modulate trajectories line considered earlier researchers described 
second example consists generating outward expanding spiral endpoint trajectory instantaneously jumping starting point cycle produce periodically replicating trajectory 
initial point spiral coordinate cor responds starting point trajectory farthest point spiral close coordinate trajectory 
challenge experiment lies change speed needed return trajectory start commencing new cycle 
nrnn multilayer perceptron inputs hidden layer neurons hidden layer neurons output layer neurons 
fig 
shows superimposed trajectories 
previous experiment error production trajectories negligible 
expected point spiral produced output dnn immediately jumped center starting point cycle 
clearly appreciated owing stability rnn output nrnn dnn stable 
spiral perfectly reproduced trajectories 
odd behavior observed system jumps final point trajectory initial cycle 
artifact caused digital simulation continuous system 
may spiral continuous trajectory discontinuity point trajectory starting point 
true output system continuous fig 

trajectories 
super imposed trajectories 
fig 

outward expanding spiral 
trajectories 
su merely exhibits fast spatial transition going trajectory starting point 
third experiment consists generating snake trajectory looks stretched letter starts point traces trajectory path endpoint reached reverses direction movement returns path starting point new cycle 
nrnn multilayer perceptron inputs hidden layer neurons output layer neurons 
error desired trajectory resulting small 
results shown fig 

complexity trajectory needs emphasized point trajectory crossover point contrast pattern discussed experiment crossover 
fourth experiment consists producing trajectory generated previous example fast spatial transition middle 
achieve effect trajectory third experiment divided symmetrical halves 
idea example show spatial overlap trajectory reversal spatial discontinuities handled dnn 
nrnn multilayer perceptron inputs hidden layers neurons neurons output layer neurons 
error desired trajectory resulting negligible 
results depicted fig 
fig 

snake trajectory 
posed trajectories 
fig 

modified snake trajectory intermediate fast spatial transition 
su trajectories 
superimposed trajectories shown 
evaluation trajectory modulation performance case rnn nrnn combined operation follow exactly directives trajectory generation case 
difference nrnn external inputs taken account training network 
demonstrating trajectory modulation ability dnn experiment attempted generate appropriately varying external input signal rotated versions pattern considered earlier 
single input signal fed nrnn control rotation angle discrete steps permitting input take values ranging effect degrees rotation starting pattern discrete steps degrees 
words input nrnn discrete values tm input maintained constant values tracing rotated version pattern 
rotated versions generated input values shown fig 
modulating input value shown inset 
noted trajectory generated corresponding external input equal basic trajectory shown fig 
trajectories values identical due fig 

modulation pattern fig 

test generalization performance 
external input 
symmetry halves pattern 
rnn synthesized trajectory generation experiments nrnn experiment multilayer perceptron inputs corresponding outputs rnn third external modulating signal hidden layer neurons second hidden layer neurons output layer neurons 
larger nrnn compared ones trajectory generation experiments needed case complexity problem attractor patterns rotated versions basic trajectory pattern crossover significantly higher 
cycles shown fig 
demonstrate dnn capable reproducing rotated trajectory pattern perfectly 
generalization ability trained dnn tested setting value external input values training performed 
fig 
shows set patterns generated input values ra ra 
seen rotations starting trajectory angles degrees resulted modulating input values confirming dnn correctly interpolated input values yield modulated trajectory patterns network seen training phase 
ease basic trajectory pattern continuously modulated varying single input parameter deserves particular emphasis 
noted need line adjustment trajectory patterns generated nonlinear dynamic system arises applications particularly adaptive control robots 
fact problem recognized complexity challenges solved training scheme aptly underscores fundamental strengths approach problem 
order appreciate significance solution needs note existing schemes neural network trajectory production struggle producing single basic pattern noticeable mismatch successive cycles address problem controlling line shape speed repetition trajectory 
contrast shown experiment continuous rotation basic pattern mismatch successive cycles single constant input 
appropriately selecting vector inputs possible modulate basic pattern various ways desired 
second experiment consisted generating snake trajectory considered earlier trajectory generation experiment controlling amount counterclockwise rotation point external input 
external input permitted take discrete values starting corresponds degrees rotation corresponds degrees rotation held constant cycle rnn unfolding 
words external input changed instant rnn starting new cycle discrete jump input value corresponds rotation trajectory degrees 
idea example show shape trajectory generated dnn controlled desired 
nrnn multilayer perceptron inputs hidden layer neurons second hidden layer neurons output layer neurons 
cycles basic trajectory rotated counterclockwise steps degrees training purposes 
training completed error desired trajectory resulting small 
resulting behavior shown fig 

test generalization ability dnn external input signal set equivalent degrees kept constant rnn completed cycle 
process repeated basic trajectory increasing rotation steps degrees trajectory rotated degrees 
emphasized rotation values part training data dnn trained 
clearly seen dnn correctly fig 

trajectory system rotation angle varies degrees step size degrees 
fig 

trajectory system rotation angle varies degrees step size degrees 
interpolated input values modulated trajectory intermediate values see fig 

major contributions novel neural network solution procedures trajectory generation trajectory modulation problems possess attractive contradictory features training simplicity accuracy pattern generation 
problems noted complexity due underlying temporal learning tasks need employing neural networks recurrent feedback connections forced earlier attempts solving types problems sacrifice simplicity training favor trajectory generation accuracy vice versa 
solution procedure offered trajectory generation problem employs decomposition learning task temporal learning part spatial learning part turn enables parts handled distinct networks recurrent neural net rnn address temporal aspects non recurrent neural net nrnn address spatial aspects 
cascade arrangement rnn nrnn constitutes dynamic neural network dnn architecture offered procedure accurate generation trajectories arbitrary complexity independent starting states noise conditions 
reduction complexity training accomplished new synthesis procedure designing rnn reducing training demands simpler task training nrnn turn enables offering new solution procedure trajectory modulation problem selection external inputs nrnn 
results trajectory generation trajectory modulation experiments reported confirm efficacy new solution procedures feasibility implementing practice complex trajectories 
significant property dnn architecture emphasized earlier ability scale gracefully generation modulation higher dimensional trajectories 
sake simplicity representation planar trajectories considered approach directly extends trajectory problems higher dimensional spaces 
noting extension involves modification spatial requirements separation temporal spatial requirements trajectory production process underlies approach enables retain synthesized rnn changes 
nrnn needs scale complexity map dimensional trajectory generated rnn final desired trajectory higher dimensional space demanding part temporal learning task 
appendix stability properties relay feedback system purpose appendix briefly outline useful stability properties limit cycle generated rnn synthesized relay control system procedure described sections iii iii 
precise procedure compute exact period limit cycle relay control system described ax bu cx error signal defined mathematical property satisfied limit cycle stated lemma followed demonstration stability limit cycle stated theorem 
derivations appendix inspired developments theory digital control 
lemma system described eqs 
limit cycle period exists ih 
bd fo bd 
proof see 
timing limit cycle tk tk tk tk tk tk tk transition times relay conditions eq 
reduces pulse transfer function sampling period zi ar fo transform operator inverse laplace transform operator 
conditions eq 
true valid 
conditions 

example third order system sampled period pulse transfer function kr ar solving eq 
gives exact period limit cycle function due complexity expression equation solved numerically 
far obtained series conditions relate parameters system relay feedback period cyclic trajectory remains proven cycle stable astrom state theorem theorem system described eqs 
limit cycle period exists eigenvalues cw ih inside unit circle ih bd aa bd rd ihi limit cycle locally stable 
proof initial condition system described eqs 
perturbed tk tk fo sr taylor expansion ea 

ar la stands higher order terms 
replacing eq 
eq 
sr ih la ra rd 

sr 


ra rd bdd bdd dt ra fd bd rd aa bd 
value control signal changes jr cx jr replacing eq 
eq 
jr ja fd 
cw proven tk fd see 
tk corresponds transition point 
substituting eq 
get ih jr 
cw eq 
eq 
ih jr ih ja fd cw cw ih ja 
oc jr ja wc ja ih ja 
analysis time interval relay switches jt gives jt cw ja 
replacing eq 
eq 
jt ja 
eq 
describes dynamics discrete time system realizes relay control system 
amplitude initial perturbation ja cycle magnitude jal 
amplitude perturbation converges exponentially zero poles inside unit circle 
checked matrices define eqs 
replaced equations stated previous theorem poles resulting expression inside unit circle 
selected system generates limit cycle locally stable 
identification decentralized adaptive control dynamical neural networks application robotic manipulators ieee trans 
neural networks vol 
pp 
november 
stable adaptive systems prentice hall 
boe line improvement speed tracking performance repetitive paths ieee trans 
control systems technology vol 
pp 

park choi lee optimal tracking nonlinear dynamic systems ieee trans 
neural networks vol 
pp 

kolmogorov interpolation extrapolation stationary random sequences april translated rand 
cybenko approximation superpositions sigmoidal function mathematics control signals systems vol 
pp 

hornik stinchcombe white multilayer feedforward networks universal function approxima tors neural networks vol 
pp 

approximate realization continuous mappings neural networks neural networks vol 
pp 

poggio girosi regularization algorithms learning equivalent multilayer networks science vol 
pp 

vapnik nature statistical learning theory springer 
rumelhart hinton williams learning representations backpropagation errors nature vol 
pp 

kramer sangiovanni vincentelli efficient parallel learning algorithms neural networks advances neural information processing systems california pp 
morgan kaufmann 
verma fast training multilayer ieee trans 
neural networks vol 
pp 
january 
schraudolph fast compact approximation exponential function neural computation vol 
pp 

baum neural net algorithms learn polynomial time examples queries ieee trans 
neural networks vol 
pp 
january 
nakamura approximation dynamical systems continuous time recurrent neural networks neural networks vol 
pp 

pineda generalization backpropagation recurrent neural networks physical review letters vol 
pp 
november 
pearlmutter learning state space trajectories recurrent neural networks neural computation vol 
pp 

william zipser learning algorithm continually running fully recurrent neural networks neural computation vol 
pp 

werbos backpropagation time proceedings ieee vol 
pp 

williams peng efficient gradient algorithm line training recurrent network trajectories neural computation vol 
pp 

pearlmutter gradient calculations dynamic recurrent neural networks survey ieee trans 
neural networks vol 
pp 
september 
fang sejnowski faster learning dynamic recurrent backpropagation neural computation vol 
pp 

learning trajectory adjoint functions teacher forcing neural networks vol 
pp 

method improving real time recurrent learning algorithm neural networks vol 
pp 

lin trajectory production adaptive time delay neural network neural networks vol 
pp 

recurrent neural network training feedforward complexity ieee trans 
neural networks vol 
march 
cohen saad efficient training recurrent neural networks time delays neural networks vol 
pp 

white learning continuous trajectories recurrent neural networks time dependent weights ieee trans 
neural networks vol 
pp 
july 
gori problem local minima recurrent neural networks ieee trans 
neural networks vol 
pp 
march 
bengio simard frasconi learning long term dependencies gradient descent difficult ieee trans 
neural networks vol 
pp 
march 
angeline saunders pollack evolutionary algorithm constructs recurrent neural networks ieee trans 
neural networks vol 
pp 
january 
recurrent neural network training learning automaton approach trajectory learning control system design ieee trans 
neural networks vol 
pp 
may 
richards lin patterns dynamic activity timing neural network processing neural networks pattern recognition 
eds 
academic press 
wong training algorithms recurrent neural nets eliminate need computation error gradients applications trajectory production problems recurrent neural networks design applications jain eds 
crc press 
ruiz owens existence learning replication periodic motions recurrent neural networks ieee trans 
neural networks vol 
pp 
july 
weiss ruiz owens ex learning oscillations recurrent neural networks ieee trans 
neural networks vol 
pp 
january 
hopfield neurons graded response collective computational properties state neurons proceedings national academy sciences usa vol 
pp 

supervised training dynamical neural networks associative memory design identification nonlinear maps international journal neural systems vol 
pp 
september 
periodic motions mapping ordered sequences training dynamic neural networks generate continuous discontinuous trajectories proceedings ijcnn como italy 
relay control systems cambridge university press 
astrom automatic tuning simple regulators proceedings ifac th world congress pp 

astrom automatic tuning simple regulators specifications phase amplitude margins automatica vol 
pp 

astrom wittenmark adaptive control addison wesley publishing 
kailath linear systems prentice hall 
cherkassky learning data concepts theory methods john wiley sons 
comparison recurrent neural networks trajectory gener ation recurrent neural networks design applications jain eds 
crc press 
ogata discrete time control systems prentice hall 
new results architecture training process estimation error bounds learning machines ph thesis university arizona 
