unlabeled data degrade classification performance generative classifiers fabio cozman polit cnica universidade de paulo av 
prof mello paulo sp brazil analyzes effect unlabeled training data generative classifiers 
interested classification performance unlabeled data added existing pool labeled data 
show unlabeled data degrade performance classifier discrepancies modeling assumptions build classifier actual model generates data analysis situation explains seemingly disparate results literature 
purpose discuss performance generative classifiers built labeled unlabeled records 
classifiers received attention machine learning literature due potential reducing need expensive labeled data nigam seeger :10.1.1.1.5684
applications web search text classification genetic research machine vision examples find abundance cheap unlabeled data addition pool expensive labeled data 
show cases unlabeled data degrade performance classifier 
show degradation happen common classification problems consequence numerical instabilities outliers serious differences assumed actual models data 
minor modeling inaccuracies lead degradation unlabeled data 
analysis labeled unlabeled data problem demonstrate unlabeled data improve degrade classification performance 
analysis clarifies seemingly disparate results reported literature explains existing unpublished experiments field 
results offer insights handle unlabeled data classification learning 
conducted author internet systems storage laboratory hewlett packard laboratories palo alto 
mailing address beckman institute mathews ave urbana il 
copyright american association artificial intelligence www aaai org 
rights reserved 
ira cohen hewlett packard laboratories page mill road palo alto ca labeled unlabeled data goal label incoming vector features instantiation record 
assume exists class variable values labels 
want build classifiers receive record generate label notation follows friedman 
classifiers built combination existing labeled unlabeled records 
knew exactly joint distribution design optimal classification rule label incoming record friedman 
storing joint distribution simply store posterior distribution strategy usually termed diagnostic example diagnostic procedures train neural networks 
statistical setting diagnostic procedures may cumbersome require great number parameters essentially number probability values required specify joint distribution alternative strategy store class distribution conditional distributions observe compute bayes rule 
strategy usually called generative 
advantage generative methods unlabeled data relate portions model marginal distribution 
focus solely obvious principled way handle unlabeled data cohen cozman seeger zhang oles 
reason employ generative schemes leave approaches 
interested estimation note possible classifier small estimation error large classification error vice versa friedman 
build classifier normally adopt set modeling assumptions 
example assume fixed number labels 
assume set independence relations variables call set assumptions concerning independence relations structure classifier 
fix modeling assumptions estimate parameters classifier 
assume variables class features known fixed number values structure generates data fixed known 
assumed structure matches structure gen data say structure correct labeled unlabeled data problem combination supervised unsupervised problems duda hart 
suppose classifier modeling assumptions exactly match model generating data 
early proved unlabeled data lead improved maximum likelihood estimates finite sample cases castelli cover 
shahshahani landgrebe emphasize variance reduction caused unlabeled data assumption bias zero unlabeled data help classification similar zhang oles 
general unlabeled data help providing information marginal distribution cohen cozman 
castelli cover investigated value unlabeled data asymptotic sense assumption number unlabeled records goes infinity faster number labeled records castelli castelli cover 
prove assuming modeling assumptions correct classifier classification error decreases exponentially number labeled records linearly number unlabeled records similar analysis venkatesh 
message previous unlabeled data help long modeling assumptions correct 
top theoretical just described empirical investigations suggested unlabeled training data improve classification performance 
shahshahani landgrebe describe classification improvements spectral data mitchell workers report number approaches extract valuable information unlabeled data variations maximum likelihood estimation nigam training algorithms mitchell :10.1.1.1.5684:10.1.1.1.5684
publications report algorithms baluja bruce miller uyar training approaches collins singer comit goldman zhou 
workshops labeled unlabeled data problem nips nips nips ijcai 
publications meetings advance optimistic view labeled unlabeled data problem unlabeled data profitably available 
detailed analysis current results reveal puzzling aspects unlabeled data 
fact workshop held ijcai witnessed great deal discussion unlabeled data really useful 
results particularly interesting 
shahshahani landgrebe describe experiments demonstrate unlabeled data help mitigate hughes phenomenon degradation performance features added report situations castelli cover assume identifiability property may fail features discrete machine learning applications duda hart 
lack identifiability crucial matter labeled unlabeled problem extensive tests discrete models observed behavior consistent gaussian identifiable models 
fact communicated george forman 
unlabeled data degrade performance 
attribute cases deviations modeling assumptions example outliers 
samples unknown classes suggest unlabeled records care labeled data produce poor classifier 

baluja naive bayes friedman tan classifiers friedman geiger goldszmidt obtain excellent classification results cases unlabeled data degraded performance 

aimed classification documents nigam naive bayes classifiers fixed structure large number features :10.1.1.1.5684:10.1.1.1.5684
nigam discuss situations unlabeled data degrade performance propose techniques reduce observed degradation 
nigam attempt completely explain reasons degradation suggest problem mismatch natural clusters feature space actual labels 
understood natural sequence nigam investigation verify explain conditions lead degradation unlabeled data 
fact intrigued existing results conducted series experiments aimed value unlabeled data cohen cozman 
short experiments indicate unlabeled data deleterious effect situations 
consider shows typical results 
estimated parameters naive bayes classifiers features em algorithm dempster laird rubin 
shows classification performance underlying model naive bayes structure left underlying model follows tan model right 
result clear estimate naive bayes classifier data naive bayes model unlabeled data help estimate naive bayes classifier data come corresponding structure unlabeled data degrade performance 
previous discussion raises questions unlabeled data degrade performance 
effect unlabeled data section discuss effect unlabeled data classification error 
visualize effect unlabeled data propose new strategy graphing performance labeled unlabeled data problem 
fixing number labeled records varying number unlabeled records propose fix percentage unlabeled records training records 
plot classification error number training records 
call graph lu graph 
introduce lu graphs example 
consider situation class variable labels probability real valued features distributions probability error labeled labeled labeled number unlabeled records probability error labeled labeled labeled number unlabeled records naive bayes classifier data generated naive bayes model left tan model right 
point summarizes runs classifier testing data bars cover percentiles 
denotes gaussian distribution mean variance problem identifiable dependency depends 
suppose build naive bayes classifier problem 
shows lu graphs unlabeled records unlabeled records unlabeled records 
lu graphs unlabeled data interesting property asymptotes converge value different asymptote labeled data 
suppose started labeled records training data 
classification error see lu graph unlabeled data 
suppose added labeled records reduced classification error 
suppose added unlabeled records 
move lu graph lu graph 
classification error increase 
added unlabeled records move lu graph classification error twice error just labeled records 
fact classification error different asymptotes different levels unlabeled data lead degradation performance addition unlabeled data 
moving lu graph see increase decrease classification error depending slopes particular lu graphs 
noted difficult classification problems lu graphs decrease slowly unlabeled data improve classification performance large amounts labeled data available 
problems large number features parameters require training data expect problems benefit consistently unlabeled data 
examples discussed nigam fit description exactly suggest adding features worsen effect unlabeled data opposite expected :10.1.1.1.5684:10.1.1.1.5684
observation agrees empirical findings shahshahani landgrebe unlabeled data useful features classifiers 
understand unlabeled data produce asymptotic differences lu graphs visualize geometry estimation adopt assumptions classification error log classification error unlabeled records number records log unlabeled unlabeled unlabeled lu graphs example gaussian features 
point graph average trials classification error obtained testing labeled records drawn correct model 
regarding problem 
simplify analysis concentrating extreme lu graphs fully labeled data fully unlabeled data expect asymptotic gap performance graphs filled continuum lu graphs just middle graph 
general correct modeling assumptions classifier obtain model approximates regardless estimation procedure employ 
imagine set probability distributions represents models compatible modeling assumptions assuming easy imagine set distributions polytope high dimensional space modeling assumptions may induce complex set 
shows set correct distribution outside imagine different set obtained pointwise marginalization shows set distribution obtained marginalization assumption closest distribution sets distributions induced modeling assumptions distributions generated estimation 
denote correspond closest distribution denote 
closeness distributions induced estimation procedure need true norm 
illustrate assumption take maximum likelihood estimation discrete models 
consider maximum likelihood estimate number training records grows bound empirical distribution converges asymptotically choose distribution closest respect kullback leibler divergence friedman geiger goldszmidt 
suppose compute maximum likelihood esti mate subject choices maximum likelihood solution unlabeled data 
obtain closest distribution respect kullback leibler divergence 
note induces estimates kullback leibler divergence marginal ization commute need equal focus exactly situation different 
additional argument needed understand effect unlabeled data appreciate difference classification estimation 
adding data labeled unlabeled leads better estimation respect various global measures likelihood squared error variance fisher information improvement may uneven estimated parameters 
note classification matters friedman bias larger bias asymptotic classification performance note correct modeling assumptions estimates equal identifiability 
unlabeled data smaller labeled data 
performance gap unlabeled data degrade performance lu graphs capture phenomenon 
basically fact estimation error guiding factor building classifier leads estimates optimal respect classification error 
preceeding discussion indicates missing labels different missing feature values 
forms missing data degrade estimation performance unlabeled data affects classification performance directly introducing bias critical parameters insight clarifies questions missing unlabeled data raised seeger 
message unlabeled training data degrade classification performance modeling assumptions classifier 
reports unlabeled data produce degradation explanations offered far suggest degradation occur somewhat extreme 
main point type degradation produced unlabeled data occur common assumptions explained fundamental differences classification estimation errors 
propose lu graphs excellent visualization phenomenon 
important point investigation asymptotic classification performance various percentages unlabeled data 
point search possible sources performance degradation particularly severe mismatches actual assumed models 
certainly creativity exercised dealing unlabeled data 
discussed literature seeger currently coherent strategy handling unlabeled data diagnostic classifiers generative classifiers suffer effects described 
general simply different approaches welcome jaakkola meila jebara 
investigate unlabeled data degrade performance different classification approaches decision trees training 
regardless approaches unlabeled data affected modeling assumptions unlabeled data help search correct modeling assumptions 
helpful step understanding unlabeled data peculiarities machine learning 
alex proposing research labeled unlabeled data suggestions comments course restricted maximum likelihood estimation depend identifiability 
omit examples squares estimation identifiable models due lack space 
help critical described 
castelli sending copy phd dissertation charles elkan sending bnb software george forman telling ijcai workshop unlabeled data 
kevin murphy freely available bnt system generate examples data 
coded naive bayes tan classifiers java language libraries system freely available www cs cmu edu 
marina meila read preliminary version sent useful comments 
baluja 
probabilistic modeling face orientation discrimination learning labeled unlabeled data 
neural information processing systems nips 
bruce 
semi supervised learning prior probabilities em 
ijcai workshop text learning supervision 
castelli cover 
exponential value labeled samples 
pattern recognition letters 
castelli cover 
relative value labeled unlabeled samples pattern recognition unknown mixing parameter 
ieee transactions information theory 
castelli 
relative value labeled unlabeled samples pattern recognition 
ph dissertation stanford university 
cohen cozman 
effect unlabeled data generative classifiers application model selection 
technical report hp labs 
collins singer 
models named entity classification 
proc 
th international conf 
machine learning 
morgan kaufmann san francisco ca 
comit denis gilleron 
positive unlabeled examples help learning 
watanabe yokomori eds proc 
th international conference algorithmic learning theory 
springer verlag 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
duda hart 
pattern classification scene analysis 
new york john wiley sons 
friedman geiger goldszmidt 
bayesian network classifiers 
machine learning 
friedman 
bias variance loss curse dimensionality 
data mining knowledge discovery 
goldman zhou 
enhancing supervised learning unlabeled data 
international joint conference machine learning 
jaakkola meila jebara 
maximum entropy discrimination 
neural information processing systems 
miller uyar 
mixture experts classifier learning labelled unlabelled data 
advances neural information processing systems 

mitchell 
role unlabeled data supervised learning 
proc 
sixth international colloquium cognitive science 
nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning 
venkatesh 
learning mixture labeled unlabeled examples parametric side information 
colt 
seeger 
learning labeled unlabeled data 
technical report institute adaptive neural computation university edinburgh edinburgh united kingdom 
shahshahani landgrebe 
effect unlabeled samples reducing small sample size problem mitigating hughes phenomenon 
ieee transactions geoscience remote sensing 
zhang oles 
probability analysis value unlabeled data classification problems 
international joint conference machine learning 
