bayesian classifiers accurate probabilities charles ling zhang department computer science university western ontario london ontario canada email ling csd ca 
data mining applications accurate ranking probability estimation essential 
traditional classifiers aim high classification accuracy low error rate produce probability estimates 
high predictive accuracy imply better ranking probability estimation 
better evaluation method classifiers classification accuracy purpose data mining applications 
answer area roc receiver operating characteristics curve simply auc 
show auc provides discriminating evaluation ranking probability estimation accuracy 
show classifiers constructed maximise auc score produce higher auc values higher classification accuracies 
results experimental comparison error auc learning algorithms tan tree augmented naive bayes 
classification important task machine learning 
classification classifier built set training examples class labels 
key formance measure classifier predictive accuracy error rate minus accuracy training testing examples 
predictive er ror rate simply percentage number incorrectly classified examples versus total number examples 
classifiers produce probability estimation confidence prediction 
error rate consider far prediction example target class largest probability estimation 
classifiers optimal assumptions care classification results second different types errors false positive false negative treated equivalent 
data mining applications assumptions true 
example direct marketing need promote top customers gradual roll deploy different promotion strategies customers different likelihoods buying products 
charles ling 
accomplish tasks need classification buyers 
need ranking customers terms likelihoods buying 
ranking desirable just classification 
true classifier smaller error rate better ranking 
addition costs errors false positive false negative quite different 
direct marketing example cost missing valuable buyer higher cost promoting non buyer 
case classifiers produce small error rates optimise business goal customers shall promoted maximum profit 
probabilities prediction crucial optimal decision making 
example example cost predicting class true class bayes optimal prediction class minimises expected loss il 
ile expected cost predicting class clearly bayes opti mal prediction requires accurate probability estimation ranking classification 
aiming accurate ranking probability estimation classifier naturally think need true ranking true probabilities training examples 
scenarios possible 
dataset examples class labels 
classification labels training testing sets better methods error rate evaluate classifiers produce probability estimates ranking purpose data mining applications 
answer roc curve 
roc receiver operating characteristics curve compares classifiers performance cross entire range class distributions error costs 
details roc curve related calculations appendix give intuitive explanation 
shows plot roc curves representing classifiers roc curve said dominate roc curve left means classifier lower expected cost possible error costs class distributions 
example dominate clear dominating relation roc curves 
example curves dominating range 
situations area roc curve simply auc summary comparing roc curves 
clearly roc curve dominates auc larger 
intuitively auc roc curve larger auc roc curve probability estimation better 
bradley shows auc proper metric quality classifiers averaged possible probability thresholds 
believe auc discriminating evaluation method error rate classifiers produce probabilities 
bayesian classifiers accurate probabilities os false positive rate fig 

example roc curves 
example classifiers producing probabilities set testing examples 
assume classifiers classify examples positive negative 
rank testing examples increasing probabilities get rank lists table 
table 
example classifiers classification accuracy different auc values 
classifier classifier 
clearly classifiers produce error rate false positive false negative classifiers equivalent terms error rate 
intuition tells classifier better classifier positive examples ranked higher classifier 
calculate aucs obtain auc classifier auc classifier clearly auc tells classifier better 
perfect classifier ranks positive examples higher proba bility negative examples auc 
worst case reverse auc equal 
ranking random auc 
note auc global measure classifier better error rate 
counter examples section 
auc better evaluation method error rate data mining algorithms produce probabilities natural question construct classifiers auc directly error matrix entropy error rate cross validation 
clearly auc charles ling 
constructing popular learning algorithms bayesian networks decision trees 
example decision tree learning information gain ratio choose best attribute top sub tree choose attribute produces maximum auc current tree construction 
decision trees maximise auc training data hopefully testing overfitting prevented 
study auc construct bayesian classifiers compare strategy previous algorithms error rate 
choose bayesian networks essentially approximating underlying probability distribution auc constructed bayesian networks produce larger auc values 
rest organized follows section reviews necessary background bayesian networks particular tan new auc learning algorithm construct 
section proposes new learning algorithm auc tan learning compares learning algorithms empirical experiments 
review learning simple bayesian networks bayesian networks bns probabilistic models combine probability theory graph theory 
represent causal probabilistic relations arcs conditional probability tables random variables nodes governed probability theory 
probabilistic inference directly bayesian networks 
bayesian networks widely applications provide intuitive causal representations real world applications supported rigorous theoretical foundation 
bayesian networks classification 
assume attributes 
example represented vector ai value ai 
represent classification variable corresponds class represent value takes 
bayes rule probability example aa class ic cl classified probable class maxp called bayesian classifier 
term lal difficult estimate 
assume attributes independent value class variable alc ffi alc bayesian classifiers accurate probabilities resulting maxp called naive bayesian classifier simply naive bayes nb 
shows example naive bayes 
fig 

example naive bayes example tan values ai estimated easily training examples naive bayes easy construct 
surprisingly effective classification 
unfortunately independence assumption rarely true realworld applications 
naive bayes poorly regression problems produces poor probability estimates 
researchers extended structure naive bayes represent dependencies attributes 
tree augmented naive bayes tan extension classification node points directly attributes naive bayes attribute parent attribute 
shows example tan 
tan specific case general augmented naive bayesian networks simply anb classification node points directly attributes limitation arcs attributes form directed cycle 
general anb powerful general bayesian networks 
tan nice trade complexity learning representational power 
past number learning algorithms published learning tan 
algorithms conditional independencies attributes ci :10.1.1.30.9978
minimising error rate error 
previous learns tan extensions aim improving predictive accuracy better probability estimations 
charles ling 
auc 
questions answer error learning algorithms produce accurate probability estimates 
learning tan maximising directly auc 
compare learning algorithms ci error auc terms er ror rate auc 
learning tan accurate probability estimates error rate vs auc seen auc viewed better measure error rate classifiers rank produce probabilities larger auc imply lower error rate 
table shows counter example 
easy obtain auc classifier classifier 
error rate classifier classifier 
table 
counter example classifier higher auc lower classification accuracy 
classifier classifier 
compare different tan learning algorithms terms error rate auc values ci second error third auc 
discuss algorithms 
ci tan learning algorithms ci conditional independence learning algorithms studied researchers 
basic idea detect conditional dependence tween attributes performing conditional independence tests data search network detected dependencies 
essentially ci learning algorithms attempt directly approximate underlying probability distribution 
intuitively may tend produce accurate probability estimates 
chow liu tan learning algorithm typical ci algorithm 
idea estimate dependencies pair attributes find maximum spanning tree estimation building tan 
friedman extend chow liu algorithm conditional mutual information attributes class variable 
function defined ip log bayesian classifiers accurate probabilities friedman algorithm described com parison 
refer algorithm ci tan 

compute ai aj ic pair attributes 
build complete undirected graph nodes attributes annotate weight edge connecting ai aj aj ic 

build maximum weighted spanning tree 

transform resulting undirected tree directed choosing root variable setting direction edges outward 

construct tan model adding node labelled adding arc ai 
error tan learning algorithms error algorithms learning bayesian networks search network minimises classification error maximises accuracy 
belong scored algorithms accuracy score maximise 
learning tan algorithm algorithm searches arcs maximising accuracy cross validation 
node called extend arcs orphan nodes parent 
algorithm depicted 

initialise network naive bayes 

evaluate current classifier classification accuracy 

consider making node 
ap increases accuracy 

consider arc ap orphan 
best arc improves accuracy keep go return current classifier 
error algorithms aim directly higher classification accuracy intuitively may tend produce model higher classification accuracy ci algorithms 
verified results experiment see section 
intriguing question auc algorithm produce better probability estimation measured auc compared error algorithm 
auc algorithm auc tan learning algorithms simply auc score maximise cross validation search arcs learning tan 
intuitively search best tan structure higher auc values resulting tan tend produce higher auc accurate probability estimates 
extend algorithm auc evaluating current network classification accuracy 
algorithm called 
algorithm described 

initialise network naive bayes 
charles ling 

evaluate current classifier terms auc 

consider making node 
ap increases auc 

consider arc ap orphan 
best arc improves auc keep go return current classifier 
empirical comparisons questions interested auc algorithms really result classifiers higher auc 
auc better discriminating measure error rate auc learning algorithms produce lower error rate 
answer questions empirical experiments 
twelve datasets uci repository conduct experiments 
table lists properties datasets experiments 
datasets comparing tan published keogh pazzani 
table 
descriptions datasets experiments 
dataset attributes class instances australia breast cars ecoli hepatitis import iris pima segment vehicle vote included naive bayes experiments algorithms derived 
experiments follow procedure 
continuous attributes dataset discretized 

dataset run naive bayes ci tan auc fold cross validation obtain auc classification accuracy testing set unused training 

repeat times obtain average auc classification accuracy testing data 
bayesian classifiers accurate probabilities table shows experimental results auc values learning algorithms naive bayes ci tan auc 
represented nb ci tan sp auc sp respectively table 
table 
experimental results aucs naive bayes ci tan auc 
dataset nb ci tan sp auc sp australia breast cars ecoli hepatitis import iris pima segment vehicle vote simple threshold average auc values judge algorithm outperforms 
comparison algorithms datasets table 
table means algorithm corresponding row wins datasets loses datasets ties datasets compared algorithm corresponding column 
see average auc auc slightly better wins loss ties error ci tan significantly better naive bayes 
datasets higher naive bayes percent dataset reverse happens 
auc best regarding auc score 
confirms auc directly build bayesian networks result network accurate ranking probability estimation 
tables show ci tan perform table 
comparison algorithms terms auc 
algorithms nb ci tan sp auc sp nb ci tan sp auc sp charles ling 
slightly worse naive bayes 
ci tan constructed goal fit conditional independencies attributes data necessarily fit classification accuracy probability estimation care measure 
shows indirectly want learn bayesian network certain goal best bet search network maximises score goal 
ranking probability estimation important data mining data mining models constructed maximise auc value predictive accuracy 
aug discriminating evaluation method compared accuracy may expect auc higher predictive accuracy compared error 
table shows experimental results learning algorithms classification accuracy testing set 
table 
experimental results accuracies naive bayes ci tan auc 
dataset nb ci tan sp auc sp australia breast cars ecoli hepatitis import iris pima segment vehicle vote set threshold comparison algorithms terms classification accuracies shown table 
indicates auc better terms predictive accuracy naive bayes wins loss ties ci tan wins loss ties wins losses ties 
discussed earlier auc discriminating evaluation criterion accuracy build tan purpose high classification accuracy probably maximise auc search network structures 
bayesian classifiers accurate probabilities table 
comparison algorithms terms classification accuracy 
algorithms nb ci tan sp auc sp nb ci tan sp auc sp investigate tan learning algorithms purpose accurate probability estimation required applications data mining 
show auc discriminating measure quality ranking probability estimation 
propose new algorithm auc learning tan directly auc search criterion 
empirical experiments obtained interesting results auc bayesian network learning algorithms tend produce accurate ranking probability estimation error algorithms 
auc bayesian network learning algorithms tend produce higher classification accuracy error algorithms 
conclude auc evaluation criterion scoring function data mining algorithms 
research study methods improving probability estimation smoothing binning bagging 
direction working study learning algorithms auc auc decision tree learning algorithms 

bradley area roc curve evaluation machine learning algorithms 
recognition vol 


chow liu approximating discrete probability distributions dependence trees 
ieee trans 
information theory vol 


frank trigg holmes witten naive bayes regression 
machine learning vol 


friedman geiger goldszmidt bayesian network classifiers 
machine learning vol 

hand till simple generalisation area roc curve multiple class classification problems 
machine learning vol 


keogh pazzani learning augmented naive bayes classifiers 
proceedings seventh international workshop ai statistics ft laud 
charles ling zhang 

merz murphy aha uci repository machine learn ing databases 
dept ics university california irvine 
www www ics uci edu mlearn mlrepository html 

monti cooper bayesian network classifier combines finite mixture model naive bayes model 
proceedings th conference uncertainty artificial intelligence 
morgan kaufmann 

provost fawcett analysis visualization classifier performance comparison imprecise class cost distribution 
proceedings third international conference knowledge discovery data mining 
aaai press 

provost fawcett kohavi case accuracy estimation comparing induction algorithms 
proceedings fifteenth international conference machine learning 
morgan kaufmann 

swets measuring accuracy diagnostic systems 
science vol 

appendix review basics roc auc 
see details 
positive negative instance classes classifications produced classifier 
pi posterior proba bility instance positive 
true positive rate tp classifier tp plp positives correctly classified total positives false positive rate fp classifier fp pl negatives incorrectly classified total negatives roc graph tp plotted axis fp plotted axis 
roc space classifier class distribution cost matrix represented point fp tp 
model produces continuous output tp fp vary threshold output varies extremes 
resulting curve called roc curve 
roc curve illustrates tradeoff available model point recorded different cost class distribution 
roc dominates classifier lower expected cost possible error costs class distributions 
dominate auc rough measure expected cost 
hand till showed auc equivalent probability randomly chosen negative example smaller estimated probability belonging positive class randomly chosen positive example 
larger auc negative example misclassified 
give simple formula calculating auc 
