temporal binding hierarchical recruitment conjunctive concepts delayed lines anthony center advanced computer studies institute cognitive science university louisiana lafayette lafayette la email cacs louisiana edu temporal correlation hypothesis proposes distributed synchrony binding different stimulus features 
synchronized spikes travel cortical circuits varying length pathways leading mismatched arrival times 
raises question initial stimulus dependent synchrony preserved destination binding site 
earlier proposed constraints tolerance segregation parameters phase coding approach cortical circuits address question 
purpose twofold 
conduct simulation experiments test proposed constraints 
second explore practicality temporal binding drive process long term memory formation recruitment learning method 
network valiant architecture demonstrate coalition temporal binding recruitment 
complementing similar approaches implement continuous time learning procedure allowing computation spiking neurons 
viability proposed binding scheme investigated conducting simulation studies examine binding errors 
simulation binding errors cause perception illusory conjunctions features belonging separate objects 
results indicate tolerance segregation parameters obey proposed constraints assemblies correct bindings dominant assemblies spurious bindings reasonable operating conditions 
improve stability recruitment method deep hierarchies limited size structures suitable computer simulations 
key words synchrony temporal correlation binding recruitment learning tolerance window phase segregation preprint submitted elsevier science april brain functionally physically separate areas sensory cortex visual auditory somatosensory analyze different stimulus features environment 
raises question physically distributed feature representations combined form coherent unitary percepts 
question identified binding problem neural representations rosenblatt 
approach solving problem uses temporal correlation hypothesis posits binding disparate feature representations accomplished synchronized firing cortical areas 
viability temporal binding model depends maintaining synchrony spikes coding features stimulus spikes coding features separate stimuli 
temporal binding model von der malsburg suggested highly connected brain structure synchronously active units may employ fast synaptic modification mechanism form dynamic ensembles 
ensembles represent combinations features unitary percept 
approach eliminates problem combinatorial explosion associated static binding mechanisms new unit needed represent possible binding 
temporal binding time coding space requires elementary feature units allows combinations formed dynamically transient potentials interconnecting synapses 
terms number units needed represent entities cognitive system exposed magnitude lowered exponential quadratic respect number features 
temporal binding proposal opened way theoretical simulation studies 
noted previous cortical connection topologies appear pose obstacles maintaining synchrony 
topologies consist variablelength pathways converge destination area 
sources synchronized expect destination means detect synchrony 
proposed constraints timing integration properties circuits order address problems 
purposes conduct simulation studies explore effectiveness proposed constraints place studies larger framework incorporates form long term memory acquisition method known recruitment learning complements temporal binding 
hope ultimately models help establish biological grounding structured connectionist models capable cognitive functions reasoning 
fig 

connection topology showing possible direct indirect pathways initially synchronized source activity converging destination 
dashed boxes indicate hypothesized stages processing individual solid box indicates localized processing single feature 
recruitment learning 
temporal binding proposes temporary representations formed criticized cognitive incompleteness regard formation permanent representations 
complement transient nature temporal binding recruitment learning allocate long term permanent memories 
recruitment may serve readout process results temporal binding processes forming binding detectors 
idea consistent storage retrieval criterion requirements von der malsburg binding mechanism 
results behavioral fmri studies visual attention support view 
studies suggest new mechanisms recruited feature bindings explicitly remembered 
recruitment learning originally proposed feldman motivated algorithms valiant 
recruitment provides feasible way allocate representations concepts randomly connected static structures brain 
connections brain completely random recruitment model captures statistical properties 
particular model motivated ratio connections cortical pyramidal cells brain number projections single neuron roughly proportional square root total number neurons 
spike timing tolerance segregation viability recruitment learning induced temporal binding depends accuracy neuron spike timing 
turns spike timing difficult maintain network topologies seen 
initially direct indirect msec fig 

tolerance window required integrate inputs shown 
initially left spikes corresponding primitive object properties synchronized 
traveling separate pathways different delays synchrony degraded seen right side 
defining tolerance window spikes treated synchronous 
timings chosen arbitrarily illustration 
synchronous activity may disrupted passing separate direct indirect converging paths 
addresses related aspects spike timing direct indirect connection topologies tolerating delays preventing crosstalk 
hypothetical circuit formed recruitment learning recognizing yellow object illustrated 
leftmost part neural firings representing primitive object properties occur synchronously subject focuses attention 
consistent stimulus dependent synchrony behavior 
reasonable require additional level processing potentially complex shape properties color property object 
allows circuit features small car curved roof elements form concept independent color 
temporal binding employed signals destination need synchronous represent object 
signal transmission times considered degree synchrony destination degraded due signals crossing varying length pathways varying delays 
window tolerance seen defined integrating signals variable delays response 
window corresponds maximum time allowed incoming spikes contribute cause action potential 
signals corresponding different objects need remain separate desynchronized order avoid crosstalk features objects 
framework notion defined assign object different tolerance window 
yields notion segregation object representations phase windows 
phase coding approach possible method responses 
method puts neural activity pertaining object separate phase window large oscillatory period 
prevents interference activity propa similar examples formed multi modal sensory stimuli 
regarding formation object representations reaction time auditory stimuli faster visual stimuli signals account sound object processed faster signals representing image object 
square triangle upper lower fig 

activity separate phase windows object scene 
scene contains triangle upper part visual field square lower part 
gates deeper structures preserving integrity respect temporal binding 
example objects shown 
theoretical simulation studies segregation object representations obtained inhibitory projections 
propose global inhibitory projections propose lateral recurrent inhibitory connections 
extends study phase segregation investigating maintain synchrony delays taken account 
relevance recurrent inhibitory circuits secondary interested effect delays synchronous activity coming upstream areas converging single destination site 
goal study constraints tolerance segregation measures required maintain phase coding destination read site 
chosen connection topology exemplar observing effects delays 
significance exemplar theory complex connection topologies transformed case hierarchical reduction 
tolerance segregation measures able calculate lower bound minimal value satisfies conditions temporal binding exemplar topology 
need define measures tolerance segregation addressed 
forming hierarchical learning structures valiant suggested higher levels processing need operate slower time scales lower levels 
slower operation higher level units may integrate information faster lower levels processing 
instance high level units representing scene may need integrate information contents scene sequentially giving response 
implies scene unit tolerance duration times longer lower level 
consistent view newell recognized time scale hierarchy ranging milliseconds years modeling cognitive behavior 
line study discusses phase windows problem multiple instantiation connectionist models 
studies offer biologically plausible way represent multiple instances objects placing apart time 
approach contrast symbolic systems models closely defects performance observed psychological studies 
symbolic systems possible instantiate arbitrary number representations object 
lgn pathway layers ca thick stripes fig 

simple example direct indirect connections visual cortex possibly leading mismatched arrival times spikes 
stands axonal propagation delay stands synaptic transmission delay integration time 
hand neural models object representation usually associated units 
problem object needs appear relation 
time dimension possible solution problem 
psychological theory evidence previously discussed evidence type scenario depicted 
give brief overview theory evidence 
direct indirect connection topologies frequently neocortical connections considered see 
psychology neuroscience studies support view temporal binding may employed cortical circuits 
primarily psychological studies identify classes visual search tasks attentional mechanisms serially scan multiple objects visual scene 
evidence attending objects separate times initiates stimulus dependent synchronous activity primary visual cortex 
temporal binding employed reasonable assume synchronous activity representing object sustained propagates various functional areas computation terminates 
support hypothesis synchrony experimentally observed different cortical areas 
complementing observations analytical simulation studies show spike propagating neuronal pools synchronized 
neurons favor synchrony allowing synchronous propagate protected filtering uncorrelated noise 
observation holds certain degree connectivity employed neurons 
phase segregation issue desynchronization segregation signals pertaining different objects phase windows explored 
fairness phase coding approach temporal binding criticized 
palm suggested results electromagnetic recording experi area earliest mean mt table response latencies msec visual areas taken 
early response mt due connection received superior colliculus sc disregard study 
ments inconsistent phase coding 
particular recordings activity pertaining multiple separate stimuli bars moving opposite directions cause flat correlogram phase coding predict non zero time lag central peak 
direct indirect connection topologies assuming phase coding approach timing crucial preserving integrity signals respect temporal binding especially synchronized spike meet alternate cortical paths 
similar hypothetical example discussed 
potentially candidates cortical connection topologies exhibit types asymmetrical connections 
illustrate shows pathway connection visual areas 
propagation pathways alternatively verified observing visual response latencies seen table 
take synchronous response originates area propagates area 
area direct connections receive synchronized spike caused stimulus directly 
aware direct evidence supporting meeting pathways converge cellular level reasonable believe arriving signals interact local cortical circuits highly interconnected 
computational basis simulation environment simulation environment introduced interesting characteristics 
motivation simulator goes back architecture valiant unified independent recruitment learning temporal binding 
interested model proposes solutions problems fundamental ai 
problems brittleness variable binding problem predicate calculus non monotonic reasoning learning mechanisms close human cognitive functioning 
due recruitment learning capability network 
valiant illustrates phenomenon giving example 
person exposed juxtaposition familiar words constitutes title new book adjustments take place cognitive machinery 
title encountered person recognize effortlessly 
kind learning runs allocation space 
recruitment learning paradigm attempts explain possible cognitive mechanisms involved represent production novel concept 
knowledge representations model exhibit significant features 
knowledge represented redundantly number units 
temporal binding implied definition recruitment learning 
suggest solution variable binding problem predicate calculus 
valiant suggests order prevent brittleness way imitate human behavior learning mechanisms acquire skills opposed approach traditional ai systems preprogrammed knowledge 
valiant proposes model tractable learning procedures operate network 
purpose learning probably approximately correct pac sense employed positive examples concepts selected random distribution train system 
pac theorem guarantees small probability error case system recognize positive instance system fail positive inputs recognized earlier 
instability hierarchical recruitment instability issue recruitment expected number neuroids recruited represent concept depends statistical properties network 
chain concepts recruited cascade deep hierarchies variance number recruited neuroids needs vanishingly small 
progressing deeper hierarchy set recruited neuroids may grow disappear completely 
valiant theoretical calculations suggest expected number recruited units predicted variance parameter close expectation 
initial simulations low number total neuroids area compared valiant showed units recruited crossing fields machine learning computational learning theory concerned subject pp 
levels hierarchy 
valiant claims neuroids employed replication factor stable recruitment levels achieved 
result additional asymptotic conditions number units taken infinite 
issue instability graver smaller sizes networks 
propose implement stabilizing mechanism recruitment low number neuroids 
mechanism applicable larger networks studied valiant optimizing performance 
mechanism may appropriate employed small networks localized patches cortical circuits 
biological plausibility needs investigation 
overview main objective summarized follows 
investigate tolerance segregation parameter constraints performing temporal binding pathways topologies shown 
describes simulation study order verify timing hypotheses proposed earlier 
hope ultimately models help establish biological grounding structured connectionist models capable cognitive functions reasoning 
consistent previous recruitment learning 
augment model continuous time spike response model srm gerstner :10.1.1.36.4685
complementing studies recruitment mainly provided analytical calculations statistical simulations extends implementing actual simulator employs recruitment learning investigates practical applicability 
uncovered issues experiment instability recruitment method 
result verifies tolerance window keeping coherent representations amount phase segregation prevent crosstalk topology calculated 
improve stability recruitment method deep hierarchies allows recruitment limited size structures suitable computer simulations 
organization rest follows 
recruitment learning described 
timing issues recruitment certain problematic conditions explored 
measures tolerance segregation maintaining coherence temporal binding direct indirect connection topologies defined 
problem instability recruitment discussed solutions proposed 
methods testing proposed hypotheses followed simulation results 

recruitment learning section describes recruitment learning procedure 
subsections progressively build context recruitment learning simulation sections 
starts giving brief summary key points recruitment 
recruitment learning 
briefly recruitment learning scheme allocating demand representations new concepts 
key feature recruitment learning operates static random graph 
vertices graph correspond neural units participate representing concepts 
recruitment learning method addresses question localist concepts allocated graph structure brain 
method allows novel concepts allocated synchronous stimulation existing concepts 
existing concepts stimulation coincidentally activate units signals converge due random interconnections 
points emphasize recruitment learning random connections synchronous activity biological support 
recruitment unsupervised learning method 
concepts acquired recruitment supervised learning methods associate related concepts 
recruitment accomplished single example allowing shot learning 
novel concepts added system necessary similar art model 
important feature distinguishes recruitment monolithic nature standard artificial neural networks 
summary intended stand starting point illustrate procedure subsections 
introduces valiant architecture employing recruitment learning 
describes organize network differently valiant simple random network test segregation 
describes type neural representation employed 
relation temporal binding explicit 
gives simple example illustrate recruitment 
example revisited analyze inner workings network recruitment 
random interconnection network neuroids store information 
known tabula rasa 
inputs weights output state available ltu neuroid 
architecture firings initial state fig 

valiant network 
threshold memorized state potential net input state machine neuroid 
valiant describes recruitment learning framework architecture 
simplest form network formed simple random interconnection network seen 
nodes represent simplest building block neuroid 
apart linear threshold unit ltu neuroid finite state machine fsm controls parameters ltu see 
network fsm detects coincidences neuroid inputs 
starts initial available state changes memorized state membrane potential 
random connections 
structural organization projection set neuroids representing concept 
temporal binding activates intersection set 
fig 

network structure 
order construct models varying length pathways organized network subparts grouping neuroids single random interconnection network 
consistent valiant group neuroids sets influenced cortical areas form graph structure 
random interconnections enable recruitment located areas seen 
redundant localist representations neural representations employ described redundant localist representations 
concepts represented small redundant number randomly distributed units network 
number known replication factor 
assembly represents single concept representation essence localist 
conventional localist representations redundancy ensures robustness case units lost 
type representation called modularly distributed distributed localist 
neuroids projects large number neuroids connected areas randomly seen 
recruitment depends temporal binding projections separate concept assemblies target area common units included projection sets due statistical properties network 
units receive input sources recruited coincident firing concept assemblies seen 
theory temporal binding suggests brain features objects bound synchronous activity 
synchronized activity represents features single object desynchronized activity indicates separate objects 
said recruitment depends principle temporal binding 
coincident synchronous activity triggers memorization mechanism recruitment caused activity preset threshold units intersection set projections 
units change internal states weights permanently memorize feature combination 
assembly recruited units turn represents high level concept conjunction primitive concepts 
premise recruitment learning suitable connection density chosen recruited set expected contain units initial concepts 
recruitment works simple example illustrates recruitment giving details state machine section 
shows target neuroid binary inputs 
neuroid initially available state connection weights set cf 
fig 

consider time profile spikes inputs seen 
time input active affect state target neuroid 
net input insufficient cause state transition see fig 

coincidence occurred 
inputs active simultaneously 
net input transition threshold target neuroid changes memorized state 
seen weight changes indicated fig 
weights retain original value weight decreases 
furthermore threshold set current net input 
result target neuroid emits spike 
neuroid target neuroid recruitment example 
time profile activity inputs target 
fig 

simple recruitment example 
specialized memorized recognize ignoring rest inputs 
inputs active 
event ignored weight pruned memorization 
neuroid specialized particular coincidence counted spurious event causing target neuroid change states fire 
inputs active memorized event triggers spike target neuroid 
note state machine explained far simplistic built illustration fast computer simulations 
minimally consists states allowing permanent memorization 
may coarse model cognitive memory functions 
human memory dynamic forgetting unnecessary items accessed low frequency 
state machine improved model realistic memory behavior 
instance third state introduced case neuroid enters forgetting phase 
inputs weights output state available initial state 
tracing state transitions firings inputs weights output state memorized state memorization 
fig 

fsm states memorization 
fire section describes details state transitions caused behavior briefly explained previous section 
recall state machine consists states initial available state final memorized state see state machine 
initial state seen ltu left inputs available ii weights wi firing threshold 
neuroid stay state indicates cumulative sum simultaneous input 
ltu right shows triggering situation inputs simultaneously active causing reset active connection weights threshold set net input ensure neuroid respond inputs just memorized 
robust hierarchical recruitment instability issue recruitment expected number neuroids recruited represent concept depends statistical properties network 
chain concepts recruited cascade deep hierarchies variance number recruited neuroids needs vanishingly small 
progressing deeper hierarchy set recruited neuroids may grow disappear completely 
valiant theoretical calculations suggest expected number recruited units predicted variance parameter close expectation 
initial simulations low number total neuroids area compared valiant showed units recruited crossing levels hierarchy 
valiant claims neuroids employed replication factor stable recruitment fig 

number neuroids intersections vary unstable manner recruitment repetitively 
levels achieved 
result additional asymptotic conditions number units taken infinite 
issue instability graver smaller sizes networks 
propose implement stabilizing mechanism recruitment low number neuroids 
mechanism applicable larger networks studied valiant optimizing performance 
mechanism may appropriate employed small networks localized patches cortical circuits 
biological plausibility needs investigation 
solution propose may resemble model localized cortical circuit stabilizes size recruited set desired level 
random connection density calculated feldman valiant prescriptions amplified helps obtain sufficient recruitment 
happens expectation larger required value recruitment 
time instability introduced due uncontrolled increase recruitment 
instability viewed steady increase size set recruited neuroids 
behavior result state global synchrony similar seizure prevented 
suggest negative feedback element control amplification applied place keep recruitment desired level 
call mechanism boost limit 
seen winner take wta circuit allows units fire allocated target concept called wta 
limit imposed recruited set replication factor discussed earlier 
wta maintaining recruitment stability network proposed earlier shastri 
similar wta mechanism separating assemblies representing items spiking associative memories proposed palm 
notice increase proposed connection density determines properties static network density need change dynamically 
timing issues recruitment far explained simplest recruitment learning algorithm 
showed algorithm depends implicitly temporal binding 
problems timing arise certain learning conditions considered temporal binding hierarchical learning learning direct indirect connection topologies delays 
timing hierarchical learning phase coding approach temporal binding suggests activity pertaining different objects occurs separate phase windows 
aspect hierarchical learning evaluating higher level concept composed conjunction lower level objects occur different phases requires detecting conjunction temporally separate events 
forces extend meaning temporal binding 
consider example shown 
left hypothetical connection circuit brain shown 
right hand side shows timing activity circuit nodes 
circuit represents substrate integrates features visual scene 
objects scene blue square green circle 
assume temporal binding objects attended brain separate phases 
object attended phase blue square active simultaneously 
see blue square detector unit active phase time profile right 
phase happens green circle object 
having constructed lower level primitives perceived scene assume concept represents scene needed 
node represent scene blue square green circle notice node depend detecting simultaneous activity inputs units active separate phases overlapping activity 
intuitively solution suggested scene detector unit observes activity adjacent phases interval gives decision 
justified claiming different levels cognitive activity different speeds 
case primitive feature binding happens fast scene perceived objects scanned 
fig 

hierarchical learning example 
left structure concepts 
right time profile activity 
idea motivated consistent proposals ch 

scenario solved valiant architecture detecting timed conjunctions 
conjunctions inputs appear different times 
valiant examined forming hierarchical structures recruitment style learning algorithm employed 
particular level hierarchy shown slower previous factor number objects allowed 
case objects imposes scene recognition unit waits objects 
general operation scene unit accommodate objects need represented 
valiant showed possible objects base time window synchrony lowest level new level needs time duration integrate results previous level 
general level newell proposed time scales explain different cognitive phenomena levels 
delays converging direct indirect pathways proceed explore cases require timed conjunctions 
far ignored delays examples simplicity 
realistic system consider depend unavoidable delays components transmission lines 
source destination intermediate fig 

direct indirect pathways source converging destination 
treat kinds delays similarly study focusing exemplar case containing varying length converging pathways shown 
case tolerant way conjoining needed integrating signals covering different length pathways coming final destination due varying delays 
specific situation addressed ch 
approaches discussed ignoring case assuming paths converging source destination equal length having peripheral systems provide persistent firings computations terminate 
follows second proposal defining peripheral systems help computations structures 
detecting temporally separate coincidences hierarchical learning scenario shows difficulty passing information time scales 
difficulty detecting activities appear duration shorter higher level time scale overlap time 
overlap problem appears case direct indirect connection topologies delays 
simultaneous activity net input neuroid provide information detecting desired timed conjunctions 
memory past state machine proposed recruitment learning handle timed conjunctions 
possible solutions proposed problem tolerant conjoining temporally separate related activity timed conjunctions 
seen increasing spread activation membrane caused inputs unit representing high level concept 
activity membrane long overlap time unit detect simultaneous activity learn features scene 
second method seen persistent repetitive firing units representing low level concepts 
increasing spread activity spike 
persistent firing fixed spread activity 
fig 

possible solutions tolerant conjoining changing behavior high level concept representations 
case inputs need fire repetitively overlapping effect caused destination higher level concept 
solutions principle causing overlapping activity detected target unit differing implementation 
defining peripherals timing delays delays considered tolerant conjoining needed simple conjunctions time scale 
consider example shown objects similar previous example 
object features shape color movement type 
assume shape information needs additional level processing movement needs 
lacks direct biological evidence known movement perceived faster pathways visual system 
increase transmission speed pathway due larger axons 
direct connection line may model increased speed transmitting movement information 
subject attends shaking blue square object inputs available simultaneously 
blue square detector active delay time unit delay simplicity requiring tolerant conjoining shaking input 
main aim example introduce constraint phase segregation parameter 
multiple object scenes tolerance limited avoid crosstalk object attended 
hand tolerance long successively attended objects crosstalk 
hand tolerance short system suffer limited speed processing due inefficient fig 

recruitment example delays direct indirect connections 

optimal setting possibly maintained dynamically brain 
shaking blue square sh 
bl 
sq 
tolerate signals arriving result spurious detections 
define constraints formal tolerance segregation parameters overcoming problems described 
defining measures tolerance segregation section extends constraints tolerance phase segregation parameters earlier maintaining object coherency respect temporal binding 
connection topology converging inputs pathways varying delays requirements studied duration tolerance window second duration phase segregation parameter 
consider tolerance constraints 
definition tolerance window neuronal unit xi longest time duration interval unit integrate train incoming spikes contribute emit single action potential ap 
unit said conjoin inputs received interval 
change different units simply 
definition neuronal unit said cover set distributed sources case synchronized spikes sources arrive interval 
lemma neuronal unit receive incoming signals set distributed sources varying distances 
covers set sources dmax dmin dmax dmin longest shortest transmission delays sources respectively 
start building model capable delay tolerant conjoining 
neuroid neuron linear threshold unit ltu features finite state machine 
employ simple model effect incoming spike membrane potential discrete pulse constant magnitude 
theorem delay tolerant conjoining disparate spikes sources covered unit xi possible spike causes long constant potential magnitude membrane pi 
proof 
spike reach xi time second spike time def 

potentials caused disparate spikes membrane pi overlap interval 
xi ltu net input pi reach interval threshold set detect sum overlap value ignore non overlapping spikes choosing crossing threshold cause ap trigger recruitment concept representing conjunction satisfying def 

implementing continuous time model far discrete time model consistent original proposal recruitment learning 
case previous examples illustration employed unit delays 
considering timing dynamics brain unit delays discrete models provide coarse approximation 
simple integrate fire spiking neuron model appropriate 
computational reasons employ spike response model srm gerstner :10.1.1.36.4685
model naturally allows delay tolerant conjoining disparate signals generalizing thm 
relying increasing spread higher level concept activation discussed see figures 
direct indirect msec initially synchronous signals cross direct indirect pathways arrive early late respectively 
long shaded window duration needs tolerated 
potential threshold direct spike temporal summation indirect spike time msec msec tolerance shaded window demonstrated srm neurons 
early late arriving spikes cause overlapping excitatory post synaptic potentials sum exceed detection threshold 
obviously non overlapping reach threshold 
fig 

srm helps tolerate delayed signals 
srm tolerance window direct indirect connection topologies implemented adjusting length epsp destination neuroid membrane 
allows lagged spikes overlapping effect destination membrane detected threshold device depicted 
attempt find constraints srm epsp order asses parts significant tolerance segregation 
similar study shastri discretize epsp regions rising part plateau decaying part 
allows making formal claims 
srm epsp caused single spike axonal delay ax kernel ax exp ax exp ax heaviside step function synaptic rise time constant membrane time constant 
rise decay behavior depend respectively parameters :10.1.1.36.4685:10.1.1.36.4685
propose parameters implementing tolerance window 
conjoining neuroid emit ap effective parts caused separate incoming spikes overlap epsp caused single spike cause ap 
definition intended restrict srm satisfy necessary conditions 
definition effective window time range epsp parameter range effective window maxt maxt fig 

numerical analysis boundaries region ax ax 
interested finding appropriate parameter selection satisfies condition definition effective window 
solid line showing time ax indicates condition satisfied independent parameter ratio dashed line showing ax effective window condition guaranteed 
shown value boundaries greater values boundaries 
maxt 
maximum value depends parameter selection write max theorem delay tolerant conjoining disparate spikes possible unit xi employing srm long effective window chosen 
proof 
proof thm 
applies effective windows spikes overlap interval threshold chosen detect overlap value part srm epsp effective tolerant conjoining time range defined pair lower upper bounds 
theorem region ax ax effective window 
proof 
numerical analysis region satisfies def 
cf 
fig 

theorem interpreted needs long include rising time effective magnitude epsp reached 
assuming rise time constant membrane time constant varied biological processes modify membrane conductance offer corollary 
corollary delay tolerant conjoining achieved membrane time constant chosen 
phase segregation second requirement temporal binding concerns phase segregation measure separating activity pertaining different objects 
definition phase segregation time separation synchronized activity pertaining different objects represented successively 
employ discrete model thm 
delay tolerant conjoining asserting tolerance windows exclusive object 
theorem segregation obey prevent crosstalk elementary features different objects neural unit covering sensory sites features 
proof 
sets spike timings pertaining successively objects respectively 
assume earliest spike arrives latest spike arrives 
thm 
effect spike arriving cause constant potential 
earliest arrive 
def 
segregation difference originating times spikes pertaining object yielding informally spike long spread destination membrane latest arriving spike tolerance window effective amount 
tolerance window start effect previous spike ended 
defining requirements segregation discrete model generalize spiking model 
tolerant conjoining apply spikes emitted tolerance window spikes emitted different tolerance windows 
restrict srm epsp satisfy conditions give reciprocal thm 

refinement earlier proposed sufficient 
parameter range effective window maxt fig 

numerical analysis ax region ax ax 
interested finding appropriate selection parameters satisfy condition definition effective window 
data shows condition fails 
shown value fails satisfy effective window condition ax potential threshold time msec fig 

phase segregation srm 
sets spikes pertaining separate objects depicted 
shaded areas show effective windows form tolerance windows 
lemma delay tolerant conjoining separate groups spikes pertaining different objects possible unit xi employing srm effective windows spikes pertaining different objects overlap time 
part srm epsp excluded tolerant conjoining time range 
theorem effective window region outside time range ax ax 
proof 
ax numerical analysis shows cf 
fig 

notice theorem optimized 
give segregation measure adopted srm achieve effect depicted 
theorem segregation srm obey srm 
input presynaptic neuron synapse cs rs connected soma postsynaptic neuron input currents synapses soma outputs current postsynaptic synapses spike produces rm cm fig 

circuit equivalents srm components 
left synapse right membrane 
stands voltage controlled current source 
proof 
proof thm 
arrival time earliest spike outside effective windows pertaining 
thm 
satisfied order avoid overlaps 
yields srm 
corollary amount segregation predicts maximal firing frequency destination neuroid srm phase segregation desynchronization implemented having globally inhibitory projection suppresses source units duration inhibitory time constant srm spike response model srm employ integrate fire model 
srm equivalent standard model appropriate parameter selections :10.1.1.36.4685
model employed synapse modeled low pass filter membrane rc couple seen 
approximate time response membrane potential pi fj wji ji refractory kernel excitatory synaptic kernel eq 

notice differential equations integrate srm kernels 
time response system frequency dynamics name response model time response membrane potential obtained exact solution differential equations effects single spikes approximating interaction inside neurons 
disadvantages spiking neuron models 
spiking models complex require sophisticated simulation environments computationally expensive 
srm saves computation time allowing larger step sizes 
integrated time responses differential equations need fine step sizes maintaining accuracy 
state machine continuous time neuroids recruitment learning algorithms discussed earlier discrete sampling times 
turns finding simple way upgrade existing discretetime algorithm difficult 
continuous time system fixed sampling time changing continuous value 
decided state machine operates continuous parameters seen determines sampling time existing discrete time recruitment algorithm 
proves elegant simple way upgrade existing discrete time model obscuring continuous time parameters 
state machine detects peaks local maxima neuroid membrane fig 

new continuous time state machine working conjunction discrete time state machine recruitment 
states quiescent activity potential rising potential increasing plateau local maximum reached 
sampling time set transition potential derivative providing sampling time discrete time learning algorithm see 
way simple addition system previously defined discrete time machine modification 
summary transition continuous state machine fig 
triggers discrete time state machine fig 

system discrete continuous parts termed hybrid approach 
methods results hypotheses previous section prediction lower bounds calculated tolerance segregation parameters give acceptable performance 
calculation parameters depends previously discussed connection topology circuit 
simulations conducted test prediction 
furthermore order assess correctness recruitment method independent timing hypotheses test network correct storage multiple concepts 
recruitment learning predictions maximal capacity network 
tests conducted presenting sequence hypothetical perceptual objects system simulation cf 
fig 

number objects tolerance segregation parameters varied different simulation runs 
multiple simulations run collect statistical data connections network feature combinations objects chosen uniform random distribution simulation 
details methods subsequent sections 
performance simulation evaluated observing network internal organization 
expect find final layer units recruited represent hypothetical objects input layers combination features 
concepts elementary features represented redundantly replication factor neuroids discussed 
turn recruited concepts intermediate representations objects occur final layer expected roughly neuroids 
representing concepts final layer allocated neuroids expected predicted maximal capacity network defined follows 
definition maximal capacity network recruitment learning total number units available replication factor 
simulations number units final destination area network 
area paths converge final concept assemblies appear simulation testbed described 
theoretically maximal capacity network achieved easily 
network populated concept assemblies fewer neuroids available recruitment new concepts 
probability finding random connections available neuroids left lower 
simulations indicate assumption recruiting neuroids final concept may hold causes network capacity higher expected 
successful recruitment depends timings neural circuits statistical properties random graph projecting areas 
simulations explore proposed parameter values yield desired performance 
simulations successful recruitment provides evidence subsystems timing recruitment working correctly 
test success binding scheme employed artifacts binding errors investigated 
binding errors cause perception illusory conjunctions features separate objects result appearance spurious concepts recruited network 
quantitative performance measure network quality correct spurious concepts formed simulation providing comparison 
relative magnitudes quality indicate threshold set distinguish correct concepts spurious ones 
instance threshold form winner take mechanism 
assume necessary condition network perform correctly 
define quantitative measures follows 
assess quality single object give definition 
definition quality measure object represented network qi ci ci number units allocated object 
quality qi object maximally units allocated represent minimally units represent 
order evaluate set correct spurious concepts simulation define aggregated quality set objects follows 
definition aggregated quality object set qi 
calculate aggregated qualities qc qs object sets oc os correct spurious objects respectively 
testbeds observing timings framework simulation testbed consists choosing number input ii middle mi areas seen 

signals appear synchronously input areas system attends sensory stimulus corresponding perceptual object 
synchronized signals representing object network successively segregated interval srm fig 

type testbed measure tolerance segregation required coherent representations 
middle areas serve create indirect pathways synapses direct pathways compare path 
number input middle areas varied testing 
calculated eq 

indirect pathways created signals crossing middle areas 
information structure input representations 
areas assumed synapse cross area 
axonal delays designed conform topological organization testbed increasing linearly distance 
difference pathway length cf 
lem 
caused synaptic delays 
vary number input middle areas create larger differences delays 
layer topology creates synapse indirect pathway compared single synapse direct pathway destination 
topology tolerance window chosen shortest path contains single synapse 
chose model parameters estimated timing data visual cortex 
time takes cross area assumed msec 
turn axonal delays estimated msec diameter physical distance 
leaves msec synaptic rise time including dendritic delays slower process axonal transmission 
employing parameters segregation values calculated layer topology similar circuit formed cortical areas get msec segregation 
segregation value predicts maximum oscillation frequency circuit hz falls gamma band hz 
activity gamma frequency band suggested object representations 
area contains neuroids replication factor representing concepts 
connection probability neuroids connected areas rn 
probability calculated extending methodology described simple random graphs 
parameter stands amplification factor 
employ increasing expectation set recruited neuroids stability reasons discussed 
value determined empirically yield satisfactory recruitment deep hierarchies 
increased results creating interference objects causing spurious concepts 
learning algorithm multiplicative weight adjustment method consistent hebbian learning inspired winnow algorithm 
state machine parameters designed give simplified model permanent recruitment shot drastic weight modification 
recruitment allow creating non permanent long term memories gradual weight adjustment mechanisms 
parameters simulation include refractory reset spike msec time constant 
spike threshold middle areas chosen proof thm 

behavior inputs concepts inputs network formed pre allocated sensory concepts represented assemblies neuroids 
sensory concepts located input areas network 
input area provides representation primitive sensory feature type 
turn sensory concepts area represent different values dimension specific feature type 
instance feature value square versus circle represented sensory concepts shape input area 
sensory concepts named numerically concept number area ii 
model system attends particular object shape say circular attentional controllers cause circle sensory concept activated 
sensory concept activated neuroids assembly representing concept emits single synchronous spike 
attention multi object scenes modeled activating sensory concept input area synchronously perceptual object scene 
exception rule concepts need chosen input area middle area connected recruitment mechanism requires simultaneous activation separate concept assemblies 
separate objects attended different times separated amount segregation parameter srm eq 

total number unique objects represented scheme calculated number sensory concepts input area total number input areas 
simulations sensory concepts allocated input area ii 

possible objects choose layer topologies respectively 
briefly active inputs inactive inputs weight update rules applied time recruitment 
assemblies representing concepts recruited middle areas activation sensory concepts 
new concept labeled sensory concepts caused recruitment 
instance concept recruited middle area simultaneous activation sensory concepts labeled 
simulation consists presenting sequence multiple perceptual objects network segregated time 
simulation set concepts created network analyzed 
correct concepts conjunctions originally sensory concepts perceptual object 
hand spurious concepts concepts created correct concepts anticipated intermediate concepts recruited middle areas 
simulation environment java simulator conducting experiments 
choose java flexible object oriented low level language contrasted imperative high level languages matlab www mathworks 
com 
java platform independent standard library reasons important sharing ideas results academic research environment 
distinctive features simulator uses java scripting environment source level user interaction 
allows flexible debugging simulations 
allows distributed processing simulations java rmi library 
introduces grapher independent plotting library java allows graphs visualized matlab gnuplot www gnuplot 
info 
helps researcher concentrate efforts simulator providing visualization 
visualization left capabilities external programs seamlessly launched simulator application 
tools research include limited matlab prototyping transfer function synapse effect membrane potential input current complex frequency domain scm linearly summed finding final membrane potential profile simulink 
gnuplot matlab plotting results java grapher library 
hspice validation electrical circuit models 
intuitions tolerance window parameter simulations describe simulation run illustrate delay tolerant conjoining neuroids network 
simulation successful tolerance parameter selected proposed measure eq 
accordingly selecting membrane time constant eq 

membrane potential area membrane potential area membrane potential area fig 

membrane potentials selected neuroid middle area shown top bottom respectively 
resets membrane potential show time spikes emitted 
action potentials depicted ideal dirac delta functions srm 
profiles show membrane potential time profiles selected recruited neuroid middle area 
gives graphical explanation progress signals sources ii 
destination initially neuroid profile top receives signal input area 
msec onset delay due axon transmission msec synaptic rise dendritic delay neuroid fires msec 
recruitment candidate profile middle receives signal input area msec onset delay signal recruited neuroids msec transmission delay msec 
cumulative effects signals neuroids fire msec 
notice spike effect sufficient recruit candidates triggering local maximum reached way see state machine recruitment 
effect similar obtained profile bottom 
time effect signals input area arrive msec delay 
hand signals arrive msec neuroids recruited fired msec 
anomaly worth mentioning 
close examination membrane potential plot see signal coming raise potential value higher reached input 
may contradictory tolerant conjoining described far distinguished effect multiple inputs effect individual input potential level discriminate 
case may mean input signal sufficient cause recruitment waiting results computation coming 
reason recruitment spiking observed recruitment limit simulation maintaining stability recruitment 
neuroids representing active inputs neuroids allowed join recruitment 
artificial recruitment limit imposed boost limit mechanism discussed 
quantitative results subsections results observing effects different parameter values number objects tolerance window segregation parameters 
parameters figures simulations varying size network architectures 
networks constructed varying levels indirect pathways described 
architecture level indirect pathway depicted 
performance measure aggregated quality definition simply referred quality 
qualities correct spurious concepts plotted parameter varied 
graph plots average quality value number trials indicated 
variation quality values displayed showing maximum minimum value trials error bar limits 
figures axis value parameter varied may marked vertical limit bar noteworthy value 
network parameters employed included legends 
defined earlier network successful correct concept quality distinguished spurious concept quality threshold value 
network object capacity quality concepts network number objects network varied simulation testbeds indirect layers 
maximal capacity topology cf 
definition total number units final destination area 
quality quality quality performance layers trials correct concepts spurious concepts half capacity number objects layer testbed 
performance layers trials correct concepts spurious concepts half capacity number objects layer testbed 
performance layers trials correct concepts spurious concepts half capacity number objects layer testbed 
fig 

concept quality function number objects network 
plots show robustness expected capacity network 
see reading plots 
seen approach scale number indirect pathways maximal capacity achieved topologies low number indirect pathways ones shown 
surprisingly network behaves gracefully maximal capacity approached 
quality correct concepts distinguishable spurious ones capacities maximal limit see 
tolerance window parameter variation quality concepts network tolerance parameter varied simulation testbeds indirect layers 
tolerance parameter topology calculated eq 

results network capacity method successful architectures employed correctness performance scale number indirect pathways increased 
phase segregation parameter variation quality concepts network tolerance parameter varied simulation testbeds indirect layers 
segregation parameter topology calculated eq 

expect degree stability network correctness increase segregation 
quality correct concepts initially increase decrease 
decrease undesirable effect due deficiency model possible due selection srm parameters calculated segregation values optimal operating range architectures tested 
decrease quality possibly occurs tolerance value kept fixed segregation increased 
reduces interference successive phases 
apparently interference causes activation effects previous phase help activate stage causing disruption 
intention report kind interference useful disruptive 
discussion spurious concepts results far indicated amount spurious activity network increases number indirect layers 
may raise suspicion calculations tolerance segregation parameters scale 
show increase number spurious con quality quality quality performance layers trials objects correct concepts spurious concepts calculated membrane time constant layer testbed 
performance layers trials objects correct concepts spurious concepts calculated membrane time constant layer testbed 
performance layers trials objects correct concepts spurious concepts calculated membrane time constant layer testbed 
fig 

concept quality function membrane time constant simulations tolerance window varied implying segregation activity srm index dropped simplicity calculated eqs 
respectively 
predicted operating value eq 

see reading plots 
quality quality quality performance layers trials objects correct concepts spurious concepts calculated segregation layer testbed 
performance layers trials objects correct concepts spurious concepts calculated segregation layer testbed 
performance layers trials objects correct concepts spurious concepts calculated segregation layer testbed 
fig 

concept quality function segregation amount 
simulations segregation varied tolerance kept constant value calculated eqs 
respectively 
predicted operating value eq 

see reading plots 
synapse activities area concept weighted potential synapse area area concept weighted potential synapse area area concept weighted potential synapse area area concept membrane potential area fig 

presynaptic activities top plots total membrane potential bottom plot neuroid representing correct concept area 
sensory concept written si 
neuroid represents intermediate concept cepts due tolerance segregation parameters artifact recruitment learning method 
briefly recruiting concepts cascade spurious concepts appear stage cause recruitment spurious concepts stages conjunction spurious concept legitimate concepts 
consider simulation layer topology 
hypothetical perceptual objects network separate times 
object represented sensory concept conjunction second 
gives presynaptic activities total membrane potential neuroid belong assembly correct concept area 
shows incoming synapses neuroids areas 
presynaptic neuroids belong assemblies concepts 
note synapses assembly single synapse assembly 
neuroid recruited synchronous spikes received synapses approach maxima 
sudden increase synaptic potentials reflects change weight values 
sudden decrease membrane potential bottom plot time time hand due reset neuroid fires 
second object network expect neuroid stay silent 
neuroid produces action potential seen reset 
reason erroneous action combined effect strong synapses assembly produces activation cross threshold calculated recruited concept 
culprit learning algorithm weakening synapses uneven distribution synapses different concept assemblies 
issues synapse activities area concept weighted potential synapse area area concept weighted potential synapse area area concept weighted potential synapse area area concept membrane potential area fig 

presynaptic activities top plots total membrane potential bottom plot neuroid representing spurious concept area 
concept caused correct concept fig 
firing wrong phase 
sensory written si 
concept resolved artifacts theory recruitment learning tolerance segregation parameters 
working making network noise tolerant tweak parameters suppress kind natural outcome 
important consequence erroneous activity spike caused neuroid going cause spurious effects downstream areas see fig 

postsynaptic neuroid assume received spike synapse representing concept second object represents concept 
simulator recruited concepts labeled time recruitment incoming neuroids 
may point requires revision 
simulator reads previously assigned label observing neuroid activity 
solution may argued neuroid fired phase second object represent intermediate concept second object 
possibility dynamically change concept neuroid belongs 
implies advanced learning algorithm allows gradual adjustment weights initial memorization 
implementation tolerance window suggested lower bounds tolerance segregation parameters calculated direct indirect connection topology 
results degree stability network correctness increases tolerance expected 
excessive stability especially desirable results trade speed performance 
prefer lowest tolerance value achieve fastest speed compromising network correctness 
purpose values chosen tolerance appropriate correct concept quality values distinguished spurious ones 
respect room optimization tolerance parameter 
proposed membrane time constant dynamically adjusted possibly biological processes vary membrane resistance accommodate calculated tolerance window 
alternative varying membrane time constant may persistent firing inputs occurring separate times creating overlapping effect destination read site see discussion 
alternative adjusting threshold destination unit 
views solve problem variable delays 
particular proposed introducing synapse specific delays integration times adopted development accommodate differences delays 
cortical circuits adapt varying delays may solve problem direct indirect connection topologies 
implementation segregation calculated segregation needs applied initial source possibly attentional mechanisms 
feedback connections destination site inhibit source areas desired segregation amount 
instance dense feedback connections visual area back lateral geniculate nucleus lgn may responsible kind modulation see direct indirect connection topology fig 

difficult assume direct feedback connection initial source topologies 
complex attentional mechanism may responsible segregating signals 
segregation predicts maximum firing frequency local circuit 
field signals systems contributed theory application timing issues interconnected circuits 
particular industry integrated circuits ics nowadays gives high importance timing properties circuits need produce faster computers 
theory field may apply issues discuss 
problem synchronizing varying length varying delayed paths especially important ics 
mainstream approaches identified current literature solution problem ch 

solution personal communication benjamin comments anonymous referee 
achieved central global clock signal synchronize events different parts circuit 
clock signal governs computational units start processing inputs 
buffering devices required keep inputs arriving various times unit 
structures varying delays source destination stages synchronization achieved clock period large tolerate maximally delayed signals 
solution equivalent approach take calculate tolerance window eq 

major disadvantage approach faster computations need wait longer duration 
second approach proposes circuits global clock signal 
circuits called asynchronous unit produces output completes computation 
special effort ensuring pathways appear 
achieve paths different stages computations shortened unified 
major disadvantage approach type fine tuning expensive susceptible errors caused noise slight variations component properties due fabrication artifacts 
third approach attempts combine strengths previous approaches 
interconnected processing stage consists interacting components 
results computation stage transmitted stage receiving release signal 
approach interesting purposes easier model biological circuits 
method require global clock nature connections localized 
modeling approach system left 
previous proposed lower bounds tolerance window phase segregation parameters 
improve hypotheses show viability simulations 
ran simulations networks varying size direct indirect connection topologies 
tested binding errors multiple hypothetical objects tolerance segregation parameters swept range including predicted values 
conclude appropriately chosen tolerance segregation parameters enable temporal binding recruitment learning direct indirect connection topologies 
furthermore clock assumed control source destination stages computation 
intermediate stages computation source destination need controlled independent faster clock signal function asynchronously 
ing spiking neuron model appropriate recruitment learning originally proposed simpler discrete time neuron models 
study consistent view 
improved stability recruitment aid stabilizing mechanism proposed 
result simulations indicate roughly half predicted capacity achieved reasonable performance 
statistical variance inherent recruitment method prevents recruiting chain concepts cascade 
problem especially apparent smaller network sizes employ low number neuroids area 
earlier stability recruitment method larger network sizes asymptotic conditions indicate recruitment levels deep 
stabilizing method potentially applied larger networks 
need design neural circuits adaptively adjust tolerance segregation parameters calculating setting fixed values topology 
cortical circuits known change tolerance segregation managed dynamically changing conditions 
managing tolerance shown membrane resistance externally manipulated vary membrane time constant desired effect achieved 
mechanism deserves neural circuits may responsible managing proposed stabilizing machinery hierarchical recruitment 
number neural circuits proposed realizing boost limit function 
instance proposed global inhibition local lateral inhibition noisy delays trigger inhibitory circuit shut activity sufficient recruitment reached control recruitment 
may improve robustness recruitment 
intend propose implement circuit increases complexity system need observe parameters closely 
boost limit mechanism solely implemented software techniques simulator 
advantage mechanism result discussed 
mark propose neural circuits conform results obtained 
authors joshi ben anonymous referees valuable comments suggestions 
research partially funded university doctoral fellowship university louisiana awarded author 
abeles 
firing rates timed events cerebral cortex 
domany pages 
ungerleider desimone 
pathways motion analysis cortical connections medial superior temporal superior temporal visual areas macaque 
journal comparative neurology june 
anthony browne ron sun 
connectionist inference models 
neural networks 
shannon campbell wang 
synchrony integrate fire oscillators 
neural computation 
cannon hasselmo 
biophysics behavior design biologically plausible models spatial navigation 

carpenter grossberg 
massively parallel architecture selforganizing neural pattern recognition machine 
computer vision graphics image processing 
chandrakasan william frank fox editors 
design high performance microprocessor circuits 
ieee press 
cleeremans editor 
unity consciousness binding integration dissociation 
oxford university press 
press 
joachim diederich 
steps knowledge intensive connectionist learning 
pollack editors advances connectionist neural computation theory volume 
ablex norwood nj 


stable propagation synchronous spiking cortical neural networks 
nature 
domany van hemmen schulten editors 
models neural networks volume physics neural networks 
springer verlag new york 
troy bryan downing 
developing distributed java applications java rmi remote method invocation 
idg books worldwide 
andreas engel pascal peter nig michael wolf singer 
temporal binding binocular consciousness 
consciousness cognition 
mark alan 
learning structured connectionist networks 
technical report computer science department university rochester rochester new york april 
feldman 
dynamic connections neural networks 
biological cybernetics 
jerome feldman david bailey 
layered hybrid connectionist models cognitive science 
wermter sun pages 
isbn 

topics parallel distributed computation 
phd thesis division applied sciences harvard university cambridge massachusetts january 
gerstner :10.1.1.36.4685
spiking neurons 
maass bishop editors pulsed neural networks chapter pages 
mit press cambridge ma 
gerstner 
framework spiking neuron models spike response model 
frank moss stan editors handbook biological physics volume chapter pages 

james gosling bill joy guy steele gilad bracha 
java language specification 
sun microsystems second edition 
url java sun com docs books second edition html title doc html 
charles gray peter nig andreas engel wolf singer 
oscillatory responses cat visual cortex exhibit inter columnar synchronization reflects global stimulus properties 
nature march 

required measures phase segregation distributed cortical processing 
proceedings international joint conference neural networks volume pages washington july 

temporal binding robust connectionist recruitment learning delayed lines 
technical report tr center advanced computer studies university louisiana lafayette lafayette la 
hummel biederman 
dynamic binding neural network shape recognition 
psychological review 
jensen 
novel lists known items reliably stored oscillatory short term memory network interaction longterm memory 
learning memory 
andreas nther palm 
pattern separation synchronization spiking associative memories visual areas 
neural networks 
christof koch moshe rapp segev 
brief history time constants 
cerebral cortex march 
nig engel 
correlated firing sensory motor systems 
current opinion neurobiology 

distinct modes vision offered feedforward recurrent processing 
trends neuroscience 
elsevier netherlands 

storage short term memories oscillatory subcycles 
science march 
littlestone 
redundant noisy attributes attribute errors linear threshold learning winnow 
proc 
th annu 
workshop comput 
learning theory pages san ca 
morgan kaufmann 
livingstone hubel 
segregation form color movement depth anatomy physiology perception 
science 
mani shastri 
connectionist solution multiple instantiation problem temporal synchrony 
proceedings fourteenth conference cognitive science society pages bloomington indiana july 
alan newell 
unified theories cognition 
harvard university press 
pat 
user manual 
url www 
org manual contents html 
nowak 
timing information transfer visual system 
kathleen jon alan peters editors cerebral cortex volume pages 
kluwer new york 
randall reilly richard soto 
forms binding neural substrates alternatives temporal synchrony 
cleeremans 
press 
ritz sejnowski 
synchronous oscillatory activity sensory systems new mechanisms 
current opinion neurobiology 
rosenblatt 
principles neurodynamics perceptrons theory brain mechanisms 
spartan books washington 
stuart russell peter norvig 
artificial intelligence modern approach 
prentice hall new jersey 
nig 
binding temporal structure multiple feature domains oscillatory neuronal network 
biological cybernetics 
walter martin schneider berthold ruf 
activity dependent development axonal dendritic delays synaptic transmission unreliable 
neural computation 
shastri 
computational model tractable reasoning inspiration cognition 
proceedings ijcai thirteenth international joint conference artificial intelligence pages france august 
shastri 
advances neurally motivated model relational knowledge representation rapid inference temporal synchrony 
applied intelligence 
shastri 
recruitment binding binding error detector circuits long term potentiation 
neurocomputing 
shastri ajjanagadde 
simple associations systematic reasoning connectionist representation rules variables dynamic bindings temporal synchrony 
behavioral brain sciences 
shastri 
semantic networks evidential formalization connectionist realization 
research notes artificial intelligence 
morgan kaufmann publishers san california 
shastri 
types quantifiers connectionist model rapid reasoning relational processing 
wermter sun pages 
isbn 
shastri 
biological grounding recruitment learning algorithms long term potentiation 
stefan wermter jim austin david willshaw editors emergent neural computational architectures neuroscience volume lecture notes computer science pages 
springer 
shiffrin schneider 
controlled automatic human information processing ii 
perceptual learning automatic attending general theory 
psychological review 
american psychological association washington dc 
singer gray 
visual feature integration temporal correlation hypothesis 
annual review neuroscience 
wolf singer 
time coding space neocortical processing hypothesis 
michael editor cognitive neurosciences chapter pages 
mit press cambridge massachusetts 

connectionism problem multiple instantiation 
trends cognitive sciences 

period doubling means representing multiply instantiated entities 
proceedings twentieth annual conference cognitive science society pages 
mahwah nj lawrence ass 
french 
inspired model working memory neuronal synchrony 
houghton editors proceedings fourth neural computation psychology workshop connectionist representations pages 
london springer verlag 
david wang 
global competition local cooperation network neural oscillators 
physica 
treisman 
binding problem 
current opinion neurobiology 
elsevier netherlands 
anne treisman 
consciousness attention binding 
cleeremans 
talk th annual meeting association scientific study consciousness 
leslie valiant 
theory learnable 
communications acm november 
leslie valiant 
functionality neural nets 
proceedings th national conference artificial intelligence pages san ca 
aaai morgan kaufmann 
leslie valiant 
circuits mind 
oxford university press 
leslie valiant 
architecture cognitive computation 
kim larsen sven glynn winskel editors icalp volume lecture notes computer science pages 
springer 
isbn 
originally appeared technical report tr center research computing technology harvard university cambridge ma 
leslie valiant 
architecture cognitive computation 
journal acm september 
von der malsburg schneider 
neural cocktail party processor 
biological cybernetics 
christoph von der malsburg 
correlation theory brain function 
domany chapter pages 
originally appeared technical report max planck institute biophysical chemistry 
christoph von der malsburg 
binding models perception brain function 
current opinion neurobiology 
elsevier netherlands 
stefan wermter ron sun editors 
hybrid neural systems revised papers workshop held december denver usa volume lecture notes computer science 
springer 
isbn 
wickelgren 
chunking consolidation theoretical synthesis semantic networks 
configuring conditioning versus cognitive learning normal forgetting syndrome hippocampal arousal system 
psychological review 

