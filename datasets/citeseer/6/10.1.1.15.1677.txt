structure performance direct access file system kostas alexandra margo seltzer division engineering applied sciences harvard university jeffrey chase andrew richard rajiv department computer science duke university eran gabber lucent technologies bell laboratories direct access file system dafs emerging industrial standard network attached storage 
dafs takes advantage new user level net interface standards 
enables user level file system structure client side functionality remote data access resides library kernel 
structure addresses longstanding performance problems stemming weak integration buffering layers network transport kernel file systems applica tions 
benefits architecture include lightweight portable asynchronous access network storage improved application control data movement caching prefetching 
explores fundamental performance characteristics user level file system structure dafs 
presents experimental results open source dafs prototype compares performance kernel nfs implementation optimized zero copy data transfer 
results show systems deliver file access throughput excess mb sat network links similar raw bandwidth 
lower client overhead dafs configuration improve application performance optimized nfs application processing demands balanced 
performance high speed network storage systems limited client overhead memory copying network access costs protocol overhead 
related source inefficiency stems poor integration applications file system services lack control kernel policies leads problems double caching false prefetching poor concur management 
result databases performance critical applications bypass file systems favor raw block storage access 
sacrifices benefits file system model including ease administration safe sharing resources data 
problems motivated design radical operating system structures allow application control resource management :10.1.1.130.1539
emergence commercial transport networks creates opportunity address issues changing operating systems common 
networks incorporate defining features user level networking remote direct memory access rdma 
user level networking allows safe network communication directly user mode applications removing kernel critical path 
rdma allows net adapter reduce copy overhead accessing application buffers directly 
direct access file system dafs new standard network attached storage direct access transport networks 
dafs protocol network file system version protocol added protocol features direct data transfer rdma scatter gather list reliable locking command flow control session recovery 
dafs designed enable user level file system client dafs client may run application library operating system kernel kernel role limited basic network device support memory management 
structure improve performance portability ity offer applications fully asynchronous direct control data movement caching 
network appliance storage vendors planning dafs interfaces products 
explores fundamental structural performance characteristics network file access user level file system structure direct access transport network rdma 
dafs basis exploring features fully specified file system protocol support 
describe dafs client server implementations open source unix system freebsd report experimental results comparing dafs zero copy nfs implementation 
purpose illustrate benefits tradeoffs techniques provide basis informed choices deployment dafs systems similar extensions network file protocols nfs 
experiments explore application prop erties determine rdma user level file systems affect performance 
example workload balanced application simultaneously saturates cpu network link dafs delivers benefit compared traditional architectures 
workloads limited disk dafs traditional network file systems behave comparably 
workload fac tors metadata intensity sizes file sizes access pattern influence performance 
important property user level file system structure applications longer bound kernel policies file system buffering caching prefetching 
user level file system structure dafs api allow applications full control file system access application longer benefit shared kernel facilities caching prefetching 
secondary goal show adaptation libraries specific classes applications enable applications benefit improved control tighter integration file system reducing eliminating burden application developers 
experiments adap tation libraries dafs clients berkeley db tpie external memory toolkit 
adaptation libraries provide benefits user level file system requiring applications modified dafs api 
layout follows 
section summarizes trends motivated dafs user level file systems sets study context previous 
section gives overview salient features dafs specifications section describes dafs implementation experiments 
section presents example adaptation libraries section describes zero copy kernel nfs alternative dafs 
section presents experimental re suits 
conclude section 
background related section discuss previous lays foundation dafs provides context experimental results 
discussion issues limit performance network storage systems discuss critical architectural features examine tack performance bottlenecks direct access transports user level file systems 
network storage performance network storage solutions categorized storage area network san solutions provide block abstraction clients network attached storage nas solutions export network file system interface 
san storage volume appears local disk client full control volume data layout client side file systems database software run unmodified 
precludes concurrent access shared volume clients client software extended ordinate accesses clients 
con trast nas file service control sharing access individual files shared volume 
approach allows safe data sharing diverse clients applications 
communication overhead key factor driving acceptance fibre channel high performance san 
fibre channel leverages network interface controller nic support offload transport processing host access blocks host memory directly copying 
nics supporting emerging iscsi block storage standard entered market ip san alternative 
contrast nas solutions typically ip protocols conventional nics paid performance penalty 
cited causes poor performance net file systems protocol processing network stacks memory copies kernel overhead system calls context switches 
data copying particular incurs substantial byte overhead cpu memory system masked advancing processor technology 
way reduce network storage access head offload transport protocol processing nic 
network adapters compute internet checksums data moves host memory approach relatively simple delivers substantial benefit 
increasing number adapters offload tcp udp protocol processing substantial kernel revisions needed 
approach avoids fundamental overheads data copying 
known techniques remove copies transport data path 
previous explored copy avoidance tcp ip communication chase provide summary 
introduced copy scheme avoids copying network preserving copy semantics 
io lite adds scatter gather features api relies support nic handle multiple client processes safely copying 
approach implement critical applications web servers kernel 
advantages obtained cleanly combined data movement primitives move data storage directly network connection user space transfer useful file transfer common server applications 
dafs introduced combine low head flexibility san products generality nas file services 
dafs approach removing overheads direct access transport read write application buffers directly 
dafs enables implementation file system client user level improved efficiency portability application control 
sections discuss aspects dafs detail 
section discuss alternative approach reduces nfs overhead eliminating data copying 
direct access transports direct access transports characterized nic support remote direct memory access rdma user level networking minimal ker nel overhead reliable messaging transport connections connection buffering efficient asynchronous event notification 
virtual inter face vi architecture defines host interface api nics supporting features 
direct access transports enable user level networking user mode process interacts directly nic send receive messages dafs client user applications library dafs client network driver user space nfs user applications library vm vfs buf cache nfs tcp ip stack network driver os kernel network adapter nic network adapter user level vs kernel client file sys tem structure 
minimal intervention operating system kernel 
nic device exposes array connection descriptors system physical address space 
connection setup time kernel network driver maps free connection descriptor user process virtual address space giving process direct safe access nic control registers buffer queues descriptor 
enables rdma allows network adapter reduce copy overhead accessing application buffers directly 
combination user level network access copy avoidance lengthy heritage research systems spanning decades 
experiments section quantify improvement access overhead dafs gains rdma transport offload direct access nics 
user level file systems addition overhead reduction dafs protocol leverages user level networking enable network file system structure depicted left hand side 
contrast traditional kernel network file system implementations shown right side dafs file clients may run user mode libraries linked directly applications 
dafs supports kernel clients focuses primarily properties user level file system structure 
user level client yields additional modest reductions overhead removing system call costs 
importantly run operating system special kernel support needed nic driver 
client may evolve independently operating system multiple client implementations may run system 
importantly structure offers opportunity improve integration file system func tions intensive applications 
particu lar enables fully asynchronous pipelined file system access systems inadequate kernel support asynchronous offers full ap plication control caching data movement prefetching 
long recognized kernel policies file system caching prefetching poorly matched needs important ap plications 
migrating os functions libraries allow improved application control specialization similar spirit library oper ating systems exokernel protocol service decomposition high speed networking re lated approaches 
user level file systems con shrimp project secure disks nasd project 
nfs network file system protocols support user level clients rpc layer rating relevant features dafs believe results apply system 
earlier arguing user level file sys tems assumed form kernel mediation critical path take ac count primary sources overhead outlined section 
user level structure considered potential disadvantages 
depends direct access network hardware widely deployed 
application control caching prefetching benefit common policies shared caching prefetching kernel 
simplest form structure places burden application manage data movement may necessary extend applications new file system api 
section shows power complexity encapsulated adaptation libraries depicted implementing apis policies appro priate particular class applications 
adaptation api syntax semantics pre existing api unnecessary modify applications operating system 
dafs architecture standards dafs specification grew dafs collaborative industry academic consortium led network appliance intel presently undergoing standardization storage networking industry association 
draft standard defines dafs proto col set request response formats semantics recommended proce daf api access dafs service client program 
library level components may replaced client programs may ac cess dafs service convenient interface 
dafs api specified recommended interface promote portability dafs client programs 
dafs api richer complex common file system apis including standard unix system call interface 
section gives overview dafs architecture standards emphasis transport related aspects sections focus dafs support rdma asynchronous file respectively 
dafs protocol summary dafs protocol derives nfs version diverges sig ways 
dafs assumes reliable network transport offers server directed command manner similar block storage protocols iscsi 
contrast dafs operation separate request dafs supports request chaining allow pipelining dependent requests name lookup open followed file read 
dafs protocol headers organized preserve alignment fixed size fields 
dafs defines features reliable session re enhanced locking primitives 
enable application adaptation layer support file caching dafs adopts mechanism consistent caching open tions 
dafs specification independent underlying transport features depend direct access nics 
addition transport level features message flow control defined dafs protocol viewed separate layer file service protocol 
direct access data transfer benefit rdma dafs supports direct variants key data transfer operations read write readdir getattr setattr 
direct operations transfer directly client provided memory regions rdma read write operations described section 
client register memory region local kernel requesting direct region 
dafs api defines primitives reg unregister memory regions direct register primitive returns region descriptor designate region direct operations 
current implementations registration issues system call pin buffer regions physical memory loads page translations region lookup table nic may interpret incoming rdma directives 
control buffer pin ning process direct operating sys tem impose resource limit similar applied case bsd api 
buffer registration may encapsulated adaptation library 
rdma operations direct dafs protocol initiated server client 
example request dafs direct write client write request server includes region token buffer containing data 
server issues rdma read fetch data client responds dafs write request rdma completes 
allows server manage buffers control order rate data transfer 
asynchronous prefetching dafs api supports fully asynchronous interface enabling clients pipeline operations overlap application processing 
flexible event notification mechanism delivers chronous completions client may create arbitrary number completion groups specify arbitrary completion group dafs operation poll wait events completion group 
asynchronous primitives enable event driven application architectures alternative multithreading 
event driven application structures efficient portable threads 
asynchronous apis allow better application control concurrency lower overhead synchronous threads 
nfs implementations support limited form asynchrony beneath synchronous kernel apis 
typically multiple processes called daemons issue blocking requests tial block read ahead write 
frequent context switching adds head 
kernel policies prefetch run sequential reads may prefetch erroneously reads sequential 
dafs implementation built prototypes user level dafs client kernel dafs server implementation freebsd 
sides implementation protocol stubs dafs sdk provided network appliance 
implementa tion currently uses gb clan vi interconnect 
user level client user level dafs client module design separating transport functions protocol handling 
implements asynchronous event driven control core dafs request response channel protocol 
sub set dafs api supported includes direct asynchronous variants basic file access data transfer operations 
client design allows full asynchrony single threaded applications 
requests library non blocking caller explicitly requests wait pending completion 
client polls event completion context application threads explicit polling requests standard preamble epilogue executed ev ery entry exit library 
points checks received responses may initiate pending sends permitted request window 
thread entry library advances client 
drawback structure pending completions build client receive queues application enter library 
deferring response processing case interfere activity client collecting completions initiating new general approach proposed asynchronous application level networking exokernel 
kernel server kernel dafs server kernel loadable module freebsd release implements complete dafs specification 
ing vfs vnode interface server may export local file system dafs 
server event driven design takes advantage efficient hardware support event notification delivery asynchronous net server multithreaded order deal blocking conditions disk buffer cache locking 
adaptation libraries adaptation libraries user level li implement high level abstractions caching prefetching insulate applications complexity handling dafs adap tation libraries interpose application file system interface dafs api 
introducing versions library file system api applications written library api run user level kernel file systems depicted 
dafs adaptation libraries offer opportunity specialize file system functions classes applications 
example file caching application level offers potential benefits 
application access user level cache lower overhead kernel cache accessed system call interface 
second client efficiently applicationspecific fetch replacement policies 
third cases caching essential function adaptation library user level file system avoids problem double caching data cached redundantly kernel level user level caches 
problem user level caching kernel virtual memory system may evict cached pages client cache consumes memory kernel allocates 
reason adaptation libraries pre registers cache described section configures safe size 
second problem user level caches easily shared multiple applications need sharing common high performance domains 
illustrate role adaptation libraries consider examples enhanced dafs tpie berkeley db 
tpie tpie transparent parallel environment toolkit external memory em gorithms 
em algorithms structured handle dafs client user applications tpie dafs client clan library dma user space clan vi adapter nic nfs client user applications iu vm vfs buf cache nfs os kernel tcp ip stack ethernet driver 
alteon ethernet copy dma adaptation libraries benefit user level clients modifying applications 
massive data problems efficiently minimizing number operations 
enhance locality data accesses performing large blocks maximizing useful computation block memory 
tpie toolkit supports range applications including geographic information system gis analysis programs massive terrain grids 
support em algorithms tpie implements dataflow streaming paradigm collections fixed size records 
provides stream types high level interfaces push streams data records application defined record operators 
pluggable block transfer engine manages transfer buffering provides interface underlying storage system 
introduced new dafs interface allowing run tpie applications dafs modification 
read ahead write data streams dafs asynchronous primitives handles details block cluster ing memory registration direct berkeley db berkeley db db open source embed ded database system provides library support concurrent storage retrieval key value pairs 
db manages buffering caching independent caching underlying file system buffer cache 
db configured spe cific page size unit locking usually kb buffer pool size 
running db dafs avoids double caching bypasses standard file system prefetching heuristics may degrade performance common db access patterns 
section shows importance effects db performance 
low overhead kernel nfs dafs approach alternatives improving access performance network storage 
section consider prominent competing structure basis empirical comparisons section 
approach enhances kernel nfs client reduce overhead protocol processing data movement 
reduce system call costs need modify existing applications relink kernel api preserved 
require new kernel support barrier fast reliable deployment 
dafs meaningful nfs enhancements sort rely new support nic 
general form copy avoidance file services uses header splitting page flipping variants tcp ip protocols decade 
illustrate briefly describe freebsd enhancements extend copy avoidance read write operations nfs 
nfs implementations send data directly kernel file cache making copy client initiating write server responding read avoid copies 
focus case client receiving read response containing block data placed file cache 
key challenge arrange nic deposit data payload file block page aligned physical page frames 
pages may inserted file cache copying 
straightforward deliver data user process remapping pages physical copy user buffers page grained suitably aligned 
assumes file system block size integral multiple page size 
nic strips transport headers nfs header message places data separate page aligned buffer header splitting 
note network mtu smaller hardware page size transfer page data spread multiple packets arrive receiver order interspersed packets flows 
order pack data contiguously pages nic significant protocol processing nfs transport decode incoming packets 
nfs complicates processing variable length headers 
modified firmware alteon gigabit ethernet adapters perform header splitting nfs read response messages 
sufficient implement zero copy nfs client 
modifications apply transport udp ip network configured frames allow nfs exchange data units pages 
allow larger block sizes altered ip fragmentation code kernel avoid splitting page buffers fragments large udp packets 
associated kernel support file cache vm system allows zero copy data exchange nfs block transfer sizes kb 
large nfs transfer sizes reduce overhead bulk data transfer limiting number trips nfs protocol stack reduces transport overheads networks allow large packets 
general solution allows assess performance potential optimizing kernel file system adopting direct access user level file system architecture dafs 
approximates performance achievable kernel dafs client nfs implementation vi network interface 
practical matter rdma approach embraced dafs promising alternative low overhead nfs 
note page flipping nfs difficult tcp nic buffer reassemble tcp stream locate nfs headers appearing arbitrary offsets stream 
possible nics implementing tcp offload engine impractical conventional nics ii 
experimental results section presents performance results range benchmarks dafs implementation kernel nfs configurations 
goal analysis quantify effects various architectural features discussed understand interact properties workload 
system configuration consists iii mhz clients servers server works le chipset equipped mb gb sdram mhz memory bus 
disks gb rpm seagate bit mhz pci bus 
systems run patched versions freebsd release 
dafs uses vi gi clan adapters 
nfs uses udp ip gigabit ethernet alteon ii adapters 
table baseline network performance 
vi clan udp ii latency bandwidth mb mb table shows raw byte roundtrip latency bandwidth characteristics networks 
ii higher latency partly due datapath crossing kernel udp ip stack 
bandwidths comparable identical 
order best compare systems formance results sections normalized maximum bandwidth achievable particular technology 
nfs clients servers exchange data units kb kb kb kb nfs block transfer size set mount time sent frag mented udp packets byte mtu ethernet frames minimize data transfer heads 
checksum offloading ii enabled minimizing checksum overheads 
interrupt coalescing ii set high possible degrading minimum way latency byte messages 
nfs client server concurrency tuned best performance cases 
nfs experiments copy avoidance nfs modified ii firmware ip fragmentation code file cache code vm system ii driver nfs udp header splitting page remapping described section 
configuration state art low overhead data transfer nfs 
experiments stan dard nfs implementation nfs standard ii driver vendor firmware 
bandwidth overhead explore bandwidth client cpu overhead reads read ahead 
experiments gb server cache mb dataset 
experiment factor client caching nfs client cache small effective dafs client cache 
experiments designed stress network data transfer 
results representative workloads sequential large disk arrays chronous random access loads servers cient disk arms deliver data client net speed 
architectural features discussed yield benefit workloads servers disk bound effect remote file system disk system performance shown section 
application request size file request denoted block size figures 
ideal block size depends nature application large blocks preferable applications long sequential reads streaming multimedia smaller blocks useful nonsequential applications databases 
read ahead sequential configurations dafs client uses asynchronous api section nfs kernel tial read ahead enabled 
experiments read ahead random access tuned nfs best case performance request size block size 
request sizes nfs configured matching nfs transfer size readahead disabled 
avoids unnecessary data transfer false prefetching 
larger request sizes nfs transfer size implicit readahead block size 
benefit userlevel file system structure allows clients select transfer size read ahead policy application basis 
dafs experiments transfer size equal request block size 
random access reads read ahead disabled 
left graphs reports bandwidth cpu utilization respectively random block reads read ahead disabled 
configurations achieve progressively higher bandwidths increasing block size wire idle requests 
key observation small request sizes dafs configuration outperforms nfs implementations factor 
due lower operation latency stemming primarily lower network latency see table lower overhead 
lower head results transport protocol offload direct access nic including reduced system call interrupt overhead possible user level networking 
large request sizes transfer time dominates 
nfs peaks half link bandwidth mb limited memory copying overhead saturates client cpu 
dafs achieves wire speed rdma low client cpu utilization cpu involved rdma transfers 
nfs eliminates copy overhead page flipping dafs nfs block size kb block size kb read bandwidth left right read ahead 
loo nfs dafs block size kb nfs dafs block size kb read client cpu utilization left right read ahead 
approaches wire speed large block sizes overhead page flipping kernel protocol code consumes client cpu peak band width 
sequential reads read ahead 
right graphs report bandwidths cpu utilization similar experiment sequential reads read ahead enabled 
case configurations reach peak band width small block sizes 
band widths roughly constant cpu utilization figures highlight differences protocols 
nfs peaks client cpu saturated overhead relatively insensitive block size dominated copying overhead 
nfs dafs avoid byte copy overhead exhibit lower cpu utilization nfs 
cpu utilization drops increasing block size drop significant dafs noticeably 
nfs overhead dominated page flipping transport protocol costs insensitive block size changes page size network mtu size 
contrast dafs overhead dominated request initiation response handling costs file system client code nic handles data transport rdma 
number requests drops increasing block size client cpu utilization drops 
experiments illustrate importance factors application perfor mance 
tpie merge experiment combines raw sequential performance including writes varying amounts application processing 
previous experiment configure system stress client data transfer overheads domi nate server adequate cpu bandwidth application 
case load spread servers mb memory file systems 
performance limited client cpu server cpu net disk record size bytes tpie merge throughput 
benchmark tpie sequential record merge program combines sorted input files byte records fixed size key integer sorted output file 
performance reported total throughput 
bytes sec experiment shows effect low overhead network real application perfor mance merge processing competes overhead client cpu cycles 
varying merge order record size allows control amount cpu application performs block data 
cpu cost record key comparisons increases logarithmically cpu cost byte decreases linearly record size 
larger records amortize comparison cost larger number bytes fewer records block 
ran merge program variants tpie library described section 
variant tpie dafs linked user level dafs client accesses servers dafs 
variant tpie manages stream ing asynchronous zero copy reads rdma zero copy writes inline dafs writes clan scatter gather messaging clan support dafs writes rdma 
second variant tpie nfs config standard kernel file system interface access servers nfs 
tpie nfs ran experiments standard nfs configurations previous section 
nfs configurations tuned read ahead write concurrency best performance cases 
tpie request size fixed kb 
shows normalized merge throughput results averages runs variance average 
merge der 
record size varies axis showing effect changing cpu demands application 
results show lower overhead dafs client noted previous section leaves cpu memory cycles free application rate result ing higher merge throughputs 
note presence application processing gap relative raw bandwidth tests previous subsection 
easy see client cpu saturated merge throughput inversely proportional total processing time block sum total block overhead application processing time 
example right side graph application highest demand due larger records highest overhead dafs outperforms nfs nfs client consumes cpu kernel executing protocol code managing performance nfs configuration limited memory copying system call interface writes 
important point relative benefit low overhead dafs con figuration insignificant application compute bound 
application processing time block diminishes left right reducing overhead yields progressively higher returns overhead progressively larger share total cpu time 
postmark postmark synthetic benchmark aimed measuring file system performance workload composed short lived relatively small files 
workload typical mail netnews servers internet service providers 
postmark workloads characterized mix metadata intensive operations 
benchmark begins creating pool files random sizes specified range 
number files upper lower bounds file sizes configurable 
creating files sequence transactions performed 
transactions chosen randomly file creation deletion operation paired file read write 
file creation operation creates writes random text file 
file deletion removes random file ac tive set 
file read reads random file entirety file write appends random amount data randomly chosen file 
section consider dafs high performance alternative nfs deployment mail netnews servers 
compare postmark performance dafs nfs 
tune nfs exhibit best case performance average file size time 
file sizes equal nfs uses block size equal file size avoid read ahead 
larger file sizes nfs uses blocks read ahead adjusted file size 
read buffers page aligned enable page remapping delivering data user process 
cases ffs file system exported ing soft updates eliminate synchronous disk metadata updates 
nfs implementation nfs version 
uses write file data writes delayed asynchronous depending size data written 
close nfs client flushes dirty buffers server memory waits flushing complete commit server disk 
nfs version close consistency dictates cached file blocks re validated server time file opened dafs client cache requests go server 
writes syn committed disk offering data reliability similar provided nfs 
dafs inlines data write rpc request clan support server initiated rdma read opera tions required direct file writes uses direct file reads 
cases client waits rpc response 
key factor determines postmark performance cost metadata operations open close create delete 
enabling soft updates server exported filesystem decouples meta data updates server disk system 
client metadata operation cost sensitive client server rpc metadata update server filesystem 
number files directory increases update time server dominates ffs performs linear time lookup 
factors affecting performance client caching network order better understand postmark nfs implementation writing file fore closing originally resulted invalidation cached blocks open file client tell responsible write 
modified implementation avoid problem 
dafs 
nfs file size kb postmark 
effect boundedness 
performance measured latency involved postmark transaction 
saw table ii significantly higher roundtrip latency clan 
similar difference null rpc cost clan rpc times faster ii rpc rs 

addition dafs file operations translate single rpc server 
nfs file create remove rpc preceded second rpc get attributes directory contains file 
similarly file open requires rpc validate file cached blocks 
combination ex frequent rpcs introduces significant performance differential systems 
experiment measures effect increas ing boundedness performance 
increase average file size kb kb maintaining small number files mini server lookup time 
run performs transactions create delete paired read operation 
report average runs variance average 
client server mb gb memory respectively 
small file sizes increased cost metadata operations nfs dominates performance 
boundedness workload increases dafs performance dominated network transfers drops linearly 
large file sizes reads nfs benefit caching writes go server retain open close semantics 
experiment shows dafs outperforms nfs factor small files due largely lower latency metadata operations clan network 
part difference alleviated improvements networking technology 
difference ol dafs nfs dataset size mb log scale berkeley db 
effect double caching remote memory access performance 
number rpcs dafs nfs fundamental protocols 
larger files nfs benefit client caching limited consistency model requires waiting outstanding asynchronous writes file close 
berkeley db experiment synthetic workload composed read transactions accessing small record uniformly random tree compare db performance dafs nfs 
workload single threaded read logging locking 
experiments warming db cache performed sequence read transactions long ensure record database touched twice average 
results report throughput transactions second 
unit kb block 
vary size db working set order change bottleneck local memory remote memory remote disk com pare dafs db client nfs db client running machine mb physical memory 
cases server runs machine gb memory 
expect readahead help random access pattern considered disable read ahead nfs transfer size 
db user level cache size set amount physical memory expected available allocation user process 
dafs client uses mb communication buffers statically sized structures leaving mb db cache 
comparison systems configure cache identically nfs 
reports throughput warm db server cache 
nfs reading file system cache creates competition physical memory user level file system caches section 
database sizes size db cache user level cache able progressively physical memory warming period network diminishes 
perfor mance determined local memory access db eventually satisfies requests entirely local cache 
database size exceeds client cache performance degrades systems start accessing remote memory 
nfs performance degrades sharply due effects 
double caching effect creating competition physical memory user level file system caches persistent due increased network demands 
result filesystem cache grows user level cache memory paged disk causing page faults 
second db api issues unaligned page reads file system nfs page remapping deliver data user process 
dafs client avoids effects maintaining single client cache doing direct block reads db buffers 
database sizes larger gb fit server cache systems disk bound server 
explores key architectural features direct access file system dafs new architecture protocol storage direct access transport networks 
dafs approaches exploit networks user level file systems potential close performance gap network file services network storage block access models 
contribution characterize issues determine effects dafs application performance quantify effects experimental results public dafs implementation open source unix system freebsd 
comparison report results experimental zero copy nfs implementa tion 
allows evaluate sources performance improvements dafs copy overhead rs 
protocol overhead alternatives achieving benefits dafs 
dafs offers significant overhead reductions high speed data transfer 
improvements primarily direct access rdma cornerstones dafs design secondarily effect transport offload network adapter 
benefits yield significant application improvements 
particular dafs delivers strongest benefit balanced workloads application processing saturates cpu occurs network speed 
sce dafs improves application performance nfs tpie merge 
workload factors undermine benefits 
direct access transfer yields little benefit servers disk limited workloads heavily compute bound 
results show adaptation li obtain benefits dafs user level client architecture need port applications new dafs api 
impor adaptation libraries leverage addi tional control concurrency asynchrony data movement buffering prefetching caching application specific ways burdening applications 
creates opportunity address longstanding problems related integration application file system high performance applications 
acknowledgments research supported national science foundation ccr eia eia network appliance ibm 
shepherd greg ganger anonymous reviewers valu able comments 
software availability dafs nfs implementations available om www eecs harvard edu vin fs perf dafs www cs duke edu ari dafs 

user level client side caching dafs 
technical report harvard university tr march 
anderson chase gadde 
cheating bottleneck network storage trapeze myrinet 
proc 
usenix technical conference new orleans la june 
arge chase toma vitter halpin urban 
flow computation massive grids 
proc 
acm gis acm symposium advances geographic information systems november 
li felten 
virtual memory mapped network interface shrimp multicomputer 
proc 
st annual symposium computer architecture pages april 

interoperation copy avoidance network file proc 
th ieee conference computer communications infocom 
new york ny march 
jacobson wilkes 
implementation sender managed interface architecture 
proc 
second symposium operating systems design implementation october 
callaghan 
nfs rdma 
progress presentation usenix file access storage symposium monterey ca january 
chase 
system optimizations high speed tcp 
ieee communications special issue tcp performance networking environments april 
baker 
scalable news architecture single pool 
login december 

highly scalable electronic mail service open systems 
proc 
usenix symposium internet technologies systems de 
chu 
zero copy tcp solaris 
proc 
usenix technical conference san diego ca january 
compaq intel microsoft 
virtual interface archi tecture specification version december 
dafs collaborative 
dafs api version november 
dafs collaborative 
direct access file system protocol version september 
www 

org 
dalton watson ed wards 
card provides architectural support high performance protocols 
ieee network pages july 
engler kaashoek hunt 
fast flexible application level networking exokernel systems 
acm transactions computer systems february 
ganger part 
metadata update formance file systems 
proc 
usenix symposium operating system design implementation pages november 
gibson nagle amiri chang gobioff hardin riedel zelenka 
cost effective high bandwidth storage architecture 
proc 
th conference architectural support programming languages operating systems oc 
king russinovich tracey 
high performance memory web servers kernel user space performance 
proc 
usenix technical conference boston ma july 

fibre channel connection 
ieee computer august 
kaashoek engler ganger hunt mazieres grimm jannotti mackenzie 
application performance flexibility exokernel systems 
proc 
th symposium operating system principles october 

postmark new file system bench mark 
technical report network appliance tr october 
lee thekkath 
petal distributed virtual disks 
proc 
th international conference architectural support programming languages operating systems cambridge mass oct 
acm press new york pages october 
maeda bershad 
protocol service decomposition high performance networking 
proc 
symposium operating systems principles pages 

design implementation direct access file system dafs kernel server freebsd 
proc 
usenix conference san francisco ca february 
mckusick bostic karels quarterman 
design implementation operating system 
addison wesley 
nagle ganger butler goodson 
network support network attached storage 
proc 
hot interconnects stanford ca august 
olson bostic seltzer 
berkeley db 
proc 
usenix technical conference freenix track monterey ca june 
pai druschel zwaenepoel 
io lite unified buffering caching scheme 
proc 
third usenix symposium operating system design implementation new orleans la february 
pawlowski smith hitz 
nfs version design implementation 
proc 
usenix technical conference boston ma june 
seltzer endo small smith 
dealing disaster surviving misbehaved kernel extensions 
proc 
symposium operating system design implementation seattle wa october 
callaghan robinson beame 
nfs version protocol 
rfc december 
spector 
performing remote operations effi ciently local computer network 
communications acm pages april 
stonebraker 
operating system support database management 
communications acm july 
khalidi 
efficient zero copy framework unix 
technical report tr sun microsystems lab may 
thekkath mann lee 
frangipani scalable distributed file system 
proc 
th cm symposium operating systems principles pages october 
vitter 
efficient computation tpie approach 
proc 
goddard conference mass storage systems technologies nasa conference publication volume ii pages college park md september 
yon eicken basu vogels 
net user level network interface paral lel distributed computing 
fifteenth acm symposium operating systems principles de 
welch 
file system belongs ker nel 
proc 
second usenix mach symposium november 
