technical report idsia february optimality universal bayesian sequence prediction general loss alphabet marcus hutter idsia ch lugano switzerland marcus idsia ch www idsia ch marcus keywords bayesian sequence prediction mixture distributions solomono induction kolmogorov complexity learning universal probability tight loss error bounds pareto optimality games chance classi cation 
bayesian framework ideally suited induction problems 
probability observing time past observations computed bayes rule true generating distribution sequences known 
problem cases reasonable guess true distribution 
order overcome problem universal mixture distribution de ned weighted sum integral distributions countable continuous set distributions including 
generalization solomono induction set enumerable semi measures 
shown performance measures universal prior nearly unknown true distribution 
sense solves problem unknown prior universal way 
results obtained general nite alphabet 
convergence conditional mean squared sense probability proven 
number additional errors optimal universal prediction scheme minus number errors optimal informed prediction scheme proven bounded 
prediction framework generalized arbitrary loss functions 
system allowed take action receives loss symbol sequence 
assumptions necessary boundedness 
optimal universal optimal informed prediction schemes de ned total loss bounded terms total loss similar error bounds 
show bounds tight predictor lead smaller bounds 
furthermore various performance measures show pareto optimality sense predictor performs better equal environments strictly better 
optimal predictors performance measures expectation mixture 
give occam razor argument solomono choice weights optimal length shortest program describing 
furthermore games chance de ned sequence bets observations rewards studied 
average pro achieved scheme rapidly converges best possible pro time needed reach winning zone proportional relative entropy 
prediction schemes compared weighted majority algorithm 
algorithms settings proofs quite di erent bounds schemes similar structure 
extensions nite alphabets partial delayed probabilistic prediction classi cation active systems brie discussed 
marcus hutter technical report contents induction 
universal sequence prediction 
contents 
introductory 
setup convergence random sequences 
universal prior probability distribution 
universal posterior probability distribution 
distance measures probability distributions 
lemma entropy inequalities 
convergence 
theorem convergence 
case 
probability classes 
error bounds deterministic predictors 
total expected numbers errors 
theorem error bound 
proof theorem 
loss bounds unit loss function 
theorem unit loss bound 
corollary unit loss bound 
loss bound merhav feder 
example loss functions 
proof theorem 
convergence instantaneous losses 
theorem instantaneous loss bound 
general loss 
theorem general loss bound 
application games chance 
games chance 
theorem time win 
example 
information theoretic interpretation 
optimality properties lower error bound 
universal bayesian sequence prediction theorem lower error bound 
pareto optimality 
de nition pareto optimality 
theorem pareto optimality 
theorem non pareto optimality 
balanced pareto optimality 
theorem balanced pareto optimality 
optimal choice weights 
theorem optimality universal weights 
occam razor versus free 
continuous probability classes theorem continuous entropy bound 
applications partial sequence prediction 
independent experiments classi cation 
comparison weighted majority outlook nite alphabet 
delayed probabilistic prediction 
active systems 
miscellaneous 
summary entropy inequalities lemma binary loss inequality binary loss inequality general loss inequality marcus hutter technical report induction problems induction type statements past observations 
probability rain tomorrow weather observations days 
dow jones rise tomorrow chart years possibly additional newspaper information 
reasonably doubt sun rise tomorrow 
de nition science predict intermediate step tries understand past developing theories consequence prediction tries manipulate 
induction problems may studied bayesian framework 
probability observing time observations computed bayes rule know true probability distribution generates observed sequence 
problem cases reasonable guess true distribution 
true probability weather sequences stock charts 
universal sequence prediction order overcome problem unknown true distribution de ne mixture distribution weighted sum integral distributions discrete continuous hypothesis set including 
assumed known contain true distribution 
probability shown converge rapidly true probability conditional sense making decisions nearly infeasible optimal decision unknown mf :10.1.1.8.1845
solomono sol idea de ne universal prior weighted average semi computable probability distributions 
lower weights assigned complex distributions 
uni ed principle multiple explanations razor simplicity principle bayes rule elegant formal theory 
environment possesses ective structure solomono posterior nds structure allows prediction 
sense solves induction problem universal way making problem speci assumptions 
contents main new contributions generalize convergence sol lv section derive general error loss bounds measuring performance relative section improving previous results hut mf apply results games chance section universal bayesian sequence prediction show error loss bounds tight solomono universal prior optimal section generalize bound cb relative entropy continuous probability classes non case section compare universal prediction scheme loss bounds weighted majority scheme loss bounds ces section 
section explains notation de nes universal mixture distribution weighted sum probability distributions set includes true distribution 
structural assumptions 
multiplicatively dominates relative entropy bounded lnw convergence mean squared sense shown theorem 
furthermore elementary proof semi martingales including convergence rate 
representation universal posterior distribution case brie discussed 
various standard sets probability measures discussed including computable enumerable cumulatively enumerable approximable nite state semi measures 
section essentially generalization deterministic error bounds hut binary alphabet general nite alphabet theorem bounds number additional errors optimal universal predictor compared optimal informed prediction scheme 
non binary setting reduced binary case 
think binary coding symbols sequence 
necessary predict block bits receives true block bits di ers bit bit prediction scheme considered sol hut 
section generalizes prediction framework case action results loss symbol sequence 
optimal universal optimal informed prediction schemes de ned case loss bounds similar error bounds section proved 
assumptions boundedness 
unit loss loss bounds theorem essentially error bounds theorem error replaced loss proofs involved 
bounds compared loss bound obtained mf :10.1.1.8.1845
theorem generalizes bounds non unit non static loss functions 
convergence instantaneous losses studied 
popular loss functions including absolute square logarithmic hellinger loss discussed 
section applies theorem games chance de ned sequence bets observations rewards 
average pro achieved scheme rapidly converges best possible average pro achieved scheme 
pro table scheme asymptotically universal scheme pro table 
theorem bounds time needed reach winning zone 
proportional relative entropy factor depending pro range attempt give information theoretic interpretation result 
marcus hutter technical report section discusses quality universal predictor bounds 
show weights derived error bounds tight 
shows error bounds improved general 
show pareto optimality sense predictor performs better equal environments strictly better 
optimal predictors mixture distributions 
leaves open choose weights 
give occam razor argument solomono choice length shortest program describing optimal 
section generalizes setup continuous probability classes consisting continuously parameterized distributions parameter ir certain smoothness regularity conditions bound relative entropy central results derived 
bound depends fisher information grows logarithmically intuitive reason necessity describe accuracy 
section discusses applications 
ways prediction schemes partial sequence prediction symbol needs predicted described 
performing predicting sequence independent experiments online learning classi cation tasks special cases 
section compares universal prediction scheme studied weighted majority wm algorithm lw lw ces kw 
wm combines forecasts experts form prediction 
number prediction errors wm compared best expert assumption distribution strings bounds worst case bounds 
algorithms settings proofs quite di erent wm bounds bound theorem structure 
section outlines possible extensions theory results 
include nite alphabets delayed probabilistic prediction active systems uencing environment learning aspects uni cation wm 
section summarizes results 
appendices contain technical proofs 
introductory introductions surveys solomono sequence prediction lv lv inductive inference general sol mf reasoning uncertainty gr competitive online statistics interesting relations :10.1.1.103.9745
see section details 
self contained 
exceptions subsections solomono mixtures section continuous classes section wm 
universal bayesian sequence prediction setup convergence random sequences denote strings nite alphabet abbreviations xm greek letters probability distributions measures 
probability nite sequence starts xn empty string 
need conditional probabilities derived bayes rule jx 
jx 
jx rst equation states probability string followed equal probability string starts divided probability string starts convenience de ne jx 
second equation rst applied times 
probability distribution denotes true unknown generating distribution sequences 
denote probabilities expectations abbreviate jx probabilities expectations true distribution 
bayes rule argument independent 
abbreviate probability 
say converges mean sum convergence probability 
allows stronger gives speed convergence sense expected number times deviates nitely bounded statistical language sample space elements 
nite sequences nite alphabet cylinder sets events 
de ne algebra set generated cylinder sets countable union 
probability measure uniquely de ned giving values cylinder sets abbreviate 
see lv statistics book thorough treatment 
expressions conditional inverse probabilities unde ned gets zero 
case restrict analysis set strings non zero probability 
convergence mean implies convergence probability weaker convergence probability 
marcus hutter technical report de ne critical set 

countable discrete alphabet union cylinder sets measure zero measurable measure zero 
theorems proven probability nz hold probability 
critical situations sums restricted exclude measures especially de ned paragraph deteriorate nz 
order keep presentation simple usually simply ignore subtleties proceed non zero 
critical cases indicate sum restricted nz exploit jx probability universal prior probability distribution inductive inference problem brought form string take guess continuation assume strings continued drawn probability distribution 
maximal prior information prediction algorithm possess exact knowledge cases sunrise example true distribution known 
prediction guess 
expect predictor performs close converges sense 
nite countable set candidate probability distributions strings 
results generalized continuous sets section 
de ne weighted average 
easy see probability distribution weights positive normalized probabilities 
nite possible choice give equal weight jmj 
call universal relative multiplicatively dominates distributions 
assume known contains true distribution 
chosen suciently large serious constraint 
generic classes especially contains computable probability distributions discussed subsection 
generalizations case contain brie discussed subsection 
subsection motivate subsection show important property converging true distribution sense useful substitute true general unknown distribution 
includes deterministic environments case probability distribution sequence 
call probability distributions kind deterministic 
weight may interpreted initial degree belief degree belief existence true randomness rejected philosophical grounds may consider containing deterministic environments 
represents belief probabilities 
universal bayesian sequence prediction universal posterior probability distribution prediction schemes conditional probabilities jx 
possible express conditional probability jx weighted average conditional jx time dependent weights jx jx jx jx denominator just ensures correct normalization 
induction bayes rule see 
inserting jx gives jx proves equivalence 
expressions give intuitive non rigorous argument jx converges jx weight increases decreases assigns high low probability new symbol random sequence signi cantly di ers 
expect total weight consistent converges weights converge 
expect jx converge jx random strings expressions suitable studying convergence loss bounds universal predictor turn need sole exception proof theorem 
probably useful tries understand learning aspect 
distance measures probability distributions need distance measures vectors general probability distributions particular ng 
absolute distance quadratic euclidian distance hellinger distance relative entropy kullback leibler divergence de ned follows jy ln relative entropy true distance measure probability distributions de ned non negative zero bounds prove heavily rely inequalities lemma entropy inequalities fy fz probability distributions convex function inequalities hold ln ln jy ln ln marcus hutter technical report proofs appendix inequality lemma generalization binary case sol hut lv 
insert ng jx jx jx get various instantaneous distances time 
take expectation sum get various total distances jx jx jx jx jx jx jx ln jx jx shown sol lv ln jx jx ln jx jx ln ln rst line inserted bayes rule 
jx 
due replace argument logarithm independent sum exchanged expectation transforms product inside logarithm 
equality second form bayes rule 
universality ln lnw yields nal inequality 
convergence theorem convergence sequences nite alphabet drawn probability rst symbols 
universal conditional probability jx symbol related true conditional universal bayesian sequence prediction probability jx way jx jx ln ii jx jx iii jx jx iv jx jx ln jx jx jx jx vi nd relative entropies weight 
proof inequality ii follows de nitions entropy inequality 
de nition niteness sees 
inequality follows ii expectation sum 
iii direct consequence ii 
reason astonishing property single universal function converge lies fact sets random sequences di er di erent 
iv related incomparable convergence results iii 
prove iv abbreviations jx jx 
jx rst equality holds inequalities follow 
iv follows expectation sum 
follows iv de nition convergence implies convergence 
vi immediately follows inequality de nitions 
nd follows jensen inequality exchanging averages convex functions 
ut conditional probabilities basis prediction algorithms considered expect prediction performance guess 
performance measures de ned sections 
marcus hutter technical report elementary proof rely semi martingale convergence theorem pp 
proof lv 
furthermore iv gives speed convergence 
note subtle di erence iii 
random sequence possibly constant necessarily random sequence jx jx converges zero statement possible jx jx lim inf jx zero 
hand stay random sequence shows jx jx inf jx tends zero matter 
easy give example jx jx diverges 
choose jx jx contribution causes fall slower causing quotient diverge random sequence diverges interesting convergence results 
case discuss cases parts apply 
theorems remain valid nite linear combination dominance 
ensured min min generally nite linear combination dominance ensured dominate sense 
possibly interesting situation true generating distribution nearby distribution weight measure distance kullback leibler divergence jj ln assume bounded constant ln ln ln ln lnw remains valid de ne 
universal bayesian sequence prediction probability classes describe known known probability classes relates setting works area embeds historical context illustrates type classes mind discusses computational issues 
get wide class include computable probability distributions case assumption weak assumes strings drawn computable distribution valid physical theories environments computable probabilistic sense 
see favorable assign high weights 
simplicity favored complexity occam razor 
context means high weight assigned simple 
pre kolmogorov complexity universal complexity measure kol zl lv 
de ned length shortest self delimiting program universal turing machine computing de ne distributions calculated short programs high weights 
relative entropy bounded kolmogorov complexity case 
ln 
solomono universal semi measure obtained take multi set enumerated turing machine enumerates enumerable sol sol lv 
case called number wisdom interesting properties cal cha cha 
enlarged include cumulatively enumerable semi measures sch sch 
enumerable cumulatively enumerable cases nitely computable approximated arbitrary pre speci able precision 
consider approximable asymptotically computable distributions universal distribution de ned approximable sch 
interesting quickly approximable distribution speed prior de ned sch 
related levin complexity levin search lev lev unclear distributions dominated considers nite state automata general turing machines related quickly computable universal nite state prediction scheme feder fmg related famous lempel ziv data compression algorithm 
extra knowledge source generating sequence reduce increase detailed analysis speci classes 
note enumerable cumulatively enumerable case computable approximable nite state case 
called universal element lv 
need property may nite countable set distributions 
consider generic continuous classes considered section 
normalization treated di erently case marcus hutter technical report error bounds deterministic predictors start simple measure making wrong prediction counts error making correct prediction counts error 
hut error bounds proven binary alphabet 
generalization arbitrary alphabet involves minor additional complications serves complicated model arbitrary loss function 
optimal prediction scheme strings drawn probability distribution probability jx known 
predicts de nition observing prediction erroneous true th symbol probability event jx 
minimized maximizes jx 
generally prediction scheme predicting argmax jx distribution 
deterministic predictor interpreted maximizing distribution 
total expected numbers errors probability making wrong prediction th symbol total expected number errors rst predictions predictor jx known obviously best prediction scheme sense making number expected errors jx min jx jx 
special interest universal predictor converges prediction converge prediction optimal may errors predictor note discontinuous function proved 
problem occurs related prediction schemes predictor regularized continuous fmg 
fortunately necessary 
prove error bound 
theorem error bound sequences nite alphabet drawn probability rst symbols 
system predicts de nition maximizes jx 
universal prediction universal bayesian sequence prediction scheme universal prior 
optimal informed prediction scheme 
total expected number prediction errors de ned bounded way lnw squared distance relative entropy weight 
rst bound contains particularly useful major bound prove follow easily 
furthermore somewhat nicer structure second bound 
section show second bound optimal 
bound discuss asymptotics second bound 
observe number errors universal predictor nite number errors informed predictor nite 
especially case deterministic case nite number errors deterministic environments 
proven elementary means 
assume sequence generated wrong prediction jx jx implies jx ln jx ln ln 
correct prediction ln obvious 
proves ln log combinatoric argument section shows log jmj 
shows upper bound log jmj uniform sharp 
theorem get slightly weaker bound lnw complicated probabilistic environments ideal informed system nite number errors theorem ensures error regret order regret quanti ed terms information content relative weight 
ensures error densities systems converge 
theorem ensures quotient converges gives speed convergence 
increasing rst occurrence theorem second get bound shows causal predictor whatsoever signi cantly errors section show second bound theorem general improved predictor especially exist upper bound achieved 
see hut discussion bounds binary alphabet 
proof theorem rst inequality theorem proven 
second inequality start modestly try nd constants satisfy linear remember named probability distribution deterministic exactly sequence 
marcus hutter technical report inequality bs show bs follow immediately summation de nition abbreviations abbreviations various error functions expressed inserting get de nition prove sequence inequalities show positive suitable proves 
obviously positive 
assume 
square keep contributions de nition constraints zm 
easy see square terms function minimized 
furthermore de ne ym increase 
bx quadratic minimized inserting gives ab inequality holds provided insert minimize leading upper bound rst bound theorem 
second bound prove universal bayesian sequence prediction square sides expressions simplify just get 
implies 
inequality theorem simple triangle inequality 
completes proof theorem ut note third bound implies second bounds equal 
loss bounds unit loss function prediction basis decision 
decision results action leads reward loss 
action uence environment enter domain acting agents analyzed context universal probability hut 
stay framework passive prediction assume action uence environment 
ir received loss action th symbol sequence 
demand normalized 
instance sequence weather forecasts base decision take umbrella wear action umbrella wearing uence weather ignoring butter ect 
losses loss sunny rainy umbrella note small loss assignment making right decision take umbrella rains sun preferable rain 
cases prediction identi ed action forecast sunny identi ed action wear rainy take umbrella 
cases 
error assignment previous subsection falls class special loss function 
assigns unit loss erroneous prediction loss correct prediction 
convenience name action prediction true probability symbol jx 
expected loss marcus hutter technical report predicting goal minimize expected loss 
generally de ne prediction scheme arg min jx minimizes expected loss 
true distribution actual expected loss predicts th symbol total expected loss rst predictions causal prediction scheme deterministic probabilistic constraint predicting losses similarly de ned 
known obviously best prediction scheme sense achieving minimal expected loss min 
predictor universal distribution special interest 
theorem generalizes arbitrary loss functions 
theorem unit loss bound sequences nite alphabet drawn probability rst symbols 
system action predicting receives loss true th symbol sequence 
system acts predicts minimize expected loss 
universal prediction scheme universal prior 
optimal informed prediction scheme 
total expected losses de ned bounded way lnw relative entropy weight 
loss bounds form error bounds substituting theorem discussion theorem applies 
able derive loss bounds terms error case show substituting theorem gives invalid bound 
convenience collect important consequences theorem corollary 
argmin 
de ned minimizes argument 
tie broken arbitrarily 
nite exists 
nite action space assume minimizing exists 
instance case compact xy continuous lim xy exists larger equal xy universal bayesian sequence prediction corollary unit loss bound conditions theorem relations hold 
nite nite ii ln deterministic xy iii iv prediction scheme 
vi vii loss bound merhav feder rst general loss bound structural assumptions boundedness derived survey merhav feder mf sec 
showed regret bounded max nd max 
assuming max general max recovered scaling bound reads notation nd subsection prove expectation average jensen inequality concave square root similarly directly theorem vi shows 
bound bound theorem general incomparable 
nite bound best factor additive constant better bound 
hand large bound tighter 
condition satis ed best predictor su ers small instantaneous loss average 
signi cant improvement occurs grow linearly instance nite see corollary especially ii 
example loss functions case unit error assignment xy xy xy xy discussed proven section 
arg min jx arg max jx marcus hutter technical report case total expected number prediction errors 
weather example threshold strategy argmin ijx 
special error case xy xy bit highest probability predicted 
consider standard loss functions binary outcome continuous action unit interval 
absolute loss de ned xy jx yj 
scheme predicts argmin yg predictions lie subset jx yj xy case coincides binary error case 
holds loss jx yj expected loss ijx quadratic loss xy action prediction argmin proportional probability jx loss jx yj get arbitrary nite alphabet predictions quadratic loss may generalized xy hellinger loss written binary outcome form xy yj logarithmic loss xy yj unbounded 
corresponding action expected loss ln jx 
total loss regret lnw nitely bounded anyway theorem needed 
continuous outcome spaces brie discussed section 
proof theorem rst inequality theorem proven 
second inequality start theorem looking constants satisfy linear inequality show follow immediately summation de nition abbreviations abbreviations loss entropy expressed im ln inserting get im ln de nition im ij ij universal bayesian sequence prediction need rst constraint second appendix reduce problem binary case consider 
take convenience 
ln im cases im im contradict rst second inequality 
assume symmetric case proven analogously reduced rst case renumbering indices 
abbreviations write ln ln yb ac zb 
constraint dropped turn true furthermore assume yb trivially positive 
multiplying constant decrease rst consider case multiply term replace constraint known decrease replacing dropping ac 
proven prove ln ln appendix prove holds 
case treated similarly 
scale replace constraint know decrease replacing dropping ac 
proven prove ln ln appendix prove holds 
summary proved holds 
inserting minimizing leads bound theorem inequalities hold minimization argument proves slightly tighter second bound theorem 
unfortunately current proof long complex involves numerical graphical analysis determining intersection properties higher order polynomials 
hopefully simpli ed proof postponed 
cautious reader may check inequalities numerically ut convergence instantaneous losses nitely bounded theorem directly conclude 
follow continuity continuous marcus hutter technical report functions 
continuous piecewise linear concave function general discontinuous function 
fortunately continuous necessary point 
allows bound terms jx jx 
theorem instantaneous loss bound conditions theorem relations hold instantaneous losses time informed universal prediction schemes ln ii jx jx iii proof ii follows im im jy 
im jy ln arrive rst inequality added im positive due 
im 
inequality follows lemma 
proven theorem ii 
follows inserting ii 
iii follows proof theorem inserting 
convergence zero holds random sequences probability bounded 
losses need converge 
ut note inequalities ii iii hold individual sequences 
sum average taken current outcome history xed 
bound ii iii general incomparable large especially bound iii tighter bound ii 
general loss restrictions imposed loss theorem static unit interval 
look proof theorem see time independence 
proof valid individual loss function step loss depend actual history case loss bounded general interval min max reduced unit interval case rescaling 
introduce scaled loss min 
max min universal bayesian sequence prediction prediction scheme identical original prediction scheme ected linear transformation argument 
follows min 
min 
involved 
theorem valid primed quantities 
inserting rearranging terms get theorem general loss bound sequences nite alphabet drawn probability rst symbols 
system action predicting receives loss min min 
true th symbol sequence 
system acts predicts minimize expected loss 
universal prediction scheme universal prior 
optimal informed prediction scheme 
total expected losses de ned bounded way 
min 

lnw relative entropy weight 
application games chance consider investing stock market 
time amount money invested portfolio access past knowledge charts 
choice investment receive new information new portfolio value best expect probabilistic model behaviour stock market 
goal maximize net expected pro knows assumption traders computable pro table try nd approximate 
theorem know solomono universal prior jx converges computable jx probability 
computable asymptotically pro table trading scheme scheme pro table long run 
get practically useful computable scheme restrict nite set computable distributions bounded levin complexity kt lv 
convergence pleasing really interested asymptotically pro table long takes pro table 
explored 
games chance theorem estimate time needed reach winning threshold game chance 
assume game sequence possibly correlated games allows sequence bets observations 
step bet depending marcus hutter technical report history certain amount money take action observe outcome receive reward pro want maximize loss want minimize de ned negative pro probability outcome possibly depending history jx 
total expected pro scheme knew optimal strategy maximize expected pro just assume winning strategy 
favorable position knowing know assume instance computable probability distribution 
theorem see average pro round universal scheme converges average pro round optimal informed scheme asymptotically money knowing just universal scheme 
theorem allows lower bound universal pro 
np max 

max maximal pro round 
pro range 
time needed perform estimated 
interesting quantity expected number rounds needed reach winning zone 
show positive 
max 
theorem time win sequences nite alphabet drawn probability rst symbols 
step bet depending history take action observe outcome net pro max 
max 
system acts maximize expected pro total average expected pro rst rounds 
universal optimal informed prediction scheme holds ii 

weight 
dividing see leading order bounded 
max proves 
condition ii weakening 
trivially positive min wonderful case pro ts positive 
negative min condition ii implies 
max implies positive proves ii 
winning strategy exists asymptotically winning strategy average pro universal bayesian sequence prediction example consider game dice black white faces black white faces 
dealer repeatedly throws dice uses die deterministic rule correlates throws rst die round th digit 
bet black white stake round return correct prediction 
pro coloring dice selection strategy dealer unambiguously determine 
jx depending die chosen 
bet probable outcome 
knew expected pro round 
don know solomono universal prior 
ln length shortest program coding see subsection 
know betting outcome higher probability leads asymptotically pro theorem reaches winning threshold thresh ln 
theorem ii sharper thresh ln 
max 

die selection strategy re ected complicated prediction system reaches winning zone rounds 
number rounds really small expected pro round order magnitude smaller return 
leads constant orders magnitude size front 
stated due large stochastic noise dicult extract signal structure rule see subsection 
furthermore bound turnaround value thresh true expected turnaround smaller 
game exists computable winning strategy guaranteed get winning zone 
information theoretic interpretation try give intuitive explanation theorem ii 
know jx converges jx 
sense learns past data information content relative ln 

ln 
think pre code length 
ln exists kraft inequality 
ln satis ed 

ln bits learned worst case information conveyed form received pro remember know pro cycle starts 
assume distribution pro ts interval min max mainly due noise small informative signal amplitude reliably determine sign signal amplitude disturbed noise amplitude 
resubmit bit 
times reduces standard deviation signal amplitude 
learn ln bits transmitted requires 
ln cycles 
expression coincides condition ii 
identifying signal marcus hutter technical report amplitude weakest part consideration argument true 
may interesting analogy rigorous may lead simpler proof ii theorems complex proofs 
optimality properties lower error bound want show exists class distributions predictor ignorant distribution observed sequence sampled minimal additional number errors compared best informed predictor deterministic environments lower bound easily obtained combinatoric argument 
consider class containing binary sequences pre length occurs exactly 
assume deterministic predictor knowing sequence advance prediction times exists sequence opposite symbol log jmj lower worst case bound predictor includes course 
shows upper bound log jmj uniform obtained discussion theorem sharp 
general probabilistic case show similar argument upper bound theorem sharp 
theorem lower error bound deterministic predictor knowing distribution observed sequence sampled 
knows depends time access previous outcomes weights total expected number errors de ned 
equalities especially hold universal predictor proof proof parallels generalizes deterministic case 
consider class distributions binary alphabet indexed want distribution posterior probability posterior probability independent past 
universal bayesian sequence prediction interested predictions time completeness may de ne assign probability informed scheme predicts bit highest probability seek maximize predictor 
assume predicts possibly depending history 
want lower bounds seek worst case 
success lowest possible probability 
need eliminate favor assume uniform weights get unbiased bernoulli sequence jx 
proves instantaneous regret formula inserting solving get nally get proves total regret formula theorem 
ut 

error bound theorem replaced asymptotically tight implies 
shows restrictions loss function exclude error loss loss bound theorem improved 
furthermore sn nd shows bound merhav feder tight 
independent set leading tight lower bound jx minf lnw 
show lnn lnw pareto optimality subsection want establish di erent kind optimality property 
performance measures relative considered previous marcus hutter technical report sections 
easy nd tailored probably expense increasing 
know advance may ask exists better equal performance strictly better performance 
clearly render suboptimal show performance measures studied 
de nition pareto optimality performance measure relative 
universal prior called pareto optimal strict inequality 
theorem pareto optimality universal prior pareto optimal instantaneous total squared distances entropy distances errors losses 
proof rst proof theorem instantaneous expected loss need general expected instantaneous losses jx predictor 
want arrive contradiction assuming assuming existence predictor strict inequality 
implicit assumption assumption exist 
exists jx exists 
equalities follow inserting 
strict inequality follows assumption 
inequality follows fact minimizes de nition expected loss similarly 
contradiction proves pareto optimality way prove pareto optimality total loss de ning expected total losses predictor assuming strict inequality get contradiction help de nition look deterministic predictor exists universal bayesian sequence prediction 
instantaneous total expected errors considered special loss functions 
pareto optimality understood geometrical insight 
formal proof goes follows abbreviations jx jx jx ask vector 
implies 
implies proving unique pareto optimality similarly assumption ln ln implies ln ln ln ln implies proving unique pareto optimality proofs similar 
ut proven uniquely pareto optimal case actions predictions invoke unique ties argmax broken consistent way counts 
measures relevant decision theoretic point view loss functions welcomed property pareto optimal pareto optimal performance measures 
theorem non pareto optimality pareto optimal norm jj 
jj positive linear combinations norms power pareto optimal esp jj 
jj general pareto optimal norm jj 
jj positive linear combinations jj 
jj 
positive linear combinations pareto optimal intuition problem gained considering probability vectors 
ir 
probability triangle wx mixture consider sets fr analogously empty contains interior pareto optimal 
visualize boundaries areas qualitatively various performance measures gives intuition prove pareto optimality construct counter examples 
proof theorem 
marcus hutter technical report balanced pareto optimality pareto optimality regarded necessary condition prediction scheme aiming optimal 
practical point view signi cant decrease may desirable causes small increase 
impossibility balanced improvement demanding condition pure pareto optimality 
theorem shows balanced pareto optimal 
consider performance measure suppress index convenience 
theorem balanced pareto optimality 

implies assume larger loss environments total weighted amount 

smaller loss mnl improvement bounded 


especially 
max 
means weighted loss decrease 
compensated large weighted increase 
environments 
increase small decrease small 
special case single environment increased loss 
decrease bound 

increase amount 
cause decrease amount times factor increase cause smaller decrease simpler environments scaled decrease complex environments 
note pure pareto optimality follows balanced pareto optimality special case increase 

proof 
follows 
linearity remainder theorem obvious 

bounding weighted average 
maximum ut optimal choice weights indicate dependency explicitly writing shown prediction schemes balanced pareto optimal prediction scheme bayes mix uniformly better 
assumptions environment large possible 
subsection discussed set enumerable regarded suciently large computational point view see sch larger sets computational realm 
agreeing leaves open question choose weights prior believes pareto optimal leads asymptotically optimal predictions 
universal bayesian sequence prediction derived bounds mean squared sum lnw loss regret lnw lnw bounds monotonically decrease increasing desirable assign high weights 
due semi probability constraint nd compromise 
argue class enumerable weight functions short program optimal compromise gives solomono prior 
consider class enumerable weight function short program fv ir 

corollary lv says log enumerable discrete 
identifying program index describing get ln ln means bounds depending lnw larger bounds depending lose additive constant order bounds solomono prior safe side getting best bounds environments 
theorem optimality universal weights set enumerable weight functions short program universal weights lead smallest performance bounds additive lnw constant enumerable environments 
justi es solomono prior solomono prior assigns high probability environment low kolmogorov complexity may interpret result justi cation occam razor note bootstrap argument implicitly occam razor justify restriction enumerable 
considered weight functions low complexity 
enter assumption came result speci universal weights optimal 
occam razor versus free regard theorem free lunch nfl theorem wm :10.1.1.39.6926:10.1.1.39.6926
environments completely random small concession loss completely uninteresting environments provides margin 
yield distinguished performance non random interesting environments 
interpret results stated proven probability measures 
hand class considered class enumerable semi elements semi 
combinations sense 
convergence error bound theorem loss bound statements hold holds maximal semi probability fails semi probability 
direction shown easy direct argument sch 
marcus hutter technical report nfl theorems optimization search wm balanced pareto optimality results :10.1.1.39.6926
interestingly prediction bayes mixes pareto optimal search optimization algorithm pareto optimal 
ongoing battle believers occam razor believers free dealt sto 
continuous probability classes considered far countable probability classes sense computational point view emphasized subsection 
hand statistical parameter estimation continuous hypothesis class bernoulli process unknown 
ir family probability distributions parameterized dimensional continuous parameter 
true generating distribution interior compact set 
may restrict countable dense subset computable rational 
computable real rational theorem applies 
practical point view assumption computable serious 
traditional analysis point view quantities results depending smoothly weird fashion depending computational complexity 
instance weight continuous probability density 
important property 
obtained dropping sum 
analogous construction restrict integral small vicinity 
suciently smooth expect jn 

jn volume turn leads lnw 
largest possible region ln approximately average 
averaged instantaneous mean total curvature matrices ln ln jx ln jx ln ln fisher information may viewed measures parametric complexity equality shown fact expected value 
ln coincides rr ln nite similar line reasoning universal bayesian sequence prediction theorem continuous entropy bound twice continuously di erentiable ir continuous positive furthermore assume inverse mean fisher information matrix exists bounded uniformly continuous relative entropy de ned bounded ln ln ln ln det weight density tends zero 
independent identically distributed distributions 
bound proven cb theorem 
case cb independent stationary th order markov processes constant 
proof generalizes arbitrary replacing cb proof 
proof go vicinity jj jj contract point set 
positive semi de nite seen de nition 
boundedness condition implies strictly positive lower bound independent eigenvalues suciently large ensures uniform continuity ensures remainder taylor expansion independent note twice continuous di erentiability cb condition follows nite twice continuous di erentiability additional technical conditions prove equality lnw ln case cb probably valid general 
lnw part bound countable ln understood follows consider restrict continuous nite binary fractions 
assign weight binary representation length 
ln case 
nite binary fraction 
continuous parameter typically estimated accuracy observations 
data allow distinguish true 
binary representation length log 
expect lnn lnn dimensional parameter space 
general term depends parametric complexity explicated third term theorem 
see cb alternative explanation 
note uniform weight lead uniform bound discrete case 
uniform bound obtained scalar case je reys prior det exists ris 
theorems applicable case continuously parameterized probability classes 
theorem valid mixture discrete continuous case 
marcus hutter technical report applications partial sequence prediction ways treat partial sequence prediction 
mean symbol sequence need predicted say sequences form want predict 
rst way keep prediction schemes sections mainly time dependent loss function assigns zero loss zy positions 
dummy prediction consistent 
losses predicting generally non zero 
solution satisfactory long drawn probability distribution 
second preferable way rely probability distribution replace distributions distributions jz conditioned conditions cause problems essentially thought xed oracles 
bounds theorems hold case individual 
independent experiments classi cation typical experimental situation sequence independent experiments predictions observations 
time arranges experiment observes data tries prediction nally observes true outcome parameterized class models hypothesis space jz wants infer true order improved predictions 
special case partial sequence prediction hypothesis space jz jz 
jz consists distributions note setting line learning classi cation tasks classi ed comparison weighted majority schools universal sequence prediction considered expected performance bounds bayesian prediction mixtures 
approach weighted majority wm algorithms worst case loss bounds spirit littlestone warmuth vovk 
schools usually refer 
brie describe wm compare approaches 
comprehensive comparison see mf :10.1.1.8.1845
focus topics covered mf :10.1.1.8.1845
wm invented lw lw developed ces kw 
variations known names weighted average aggregating strategy learning expert advice boosting hedge algorithm 
invented 
early works direction daw ris 
see review 
describe setting basic idea universal bayesian sequence prediction wm binary alphabet 
consider nite binary sequence nite set experts making predictions unit interval past observations loss expert step de ned jx case binary predictions jx coincides error measure 
wm algorithm combines predictions experts 
forms prediction weighted average expert predictions certain update rules weights depending parameter various bounds total loss jx wm terms total loss jx best expert proven 
possible ne tune eliminate necessity knowing advance 
rst bound kind obtained ces ln je ln je constants improved ag ye 
bound theorem uniform weights increased reads ln jmj ln jmj quite similar structure algorithms settings proofs interpretation quite di erent 
wm performs environment relative set experts predictor competes best possible predictor predictor expectation set environments wm depends set expert depends set environments basic algorithm extended di erent directions incorporation di erent initial weights je 
ln lw general loss functions continuous valued outcomes multi dimensional predictions kw absolute loss :10.1.1.52.856
yam lies somewhat wm wm techniques prove expected loss bounds sequences independent symbols experiments limited classes loss functions 
note predictions wm continuous 
appropriate weather forecasters announce probability rain decision wear take umbrella binary su ered loss depends binary decision probability estimate 
possible convert continuous prediction wm probabilistic binary prediction predicting probability 
jx probability making error 
note expectation taken probabilistic prediction deterministic algorithm expectation taken environmental distribution 
multi dimensional case kw interpreted probabilistic prediction symbols alphabet error bounds absolute loss proven 
fs regret bounded arbitrary unit loss function alphabet upper bound known advance 
interesting generalize wm bound arbitrary alphabet general loss functions probabilistic interpretation 
original wm version lw discrete prediction necessarily double errors best expert historical interest :10.1.1.37.1595
marcus hutter technical report outlook discuss directions ndings may extended 
nite alphabet cases basic prediction unit letter number inducing number sequences word completing sentences real number vector physical measurements 
prediction may generalized block block prediction symbols suitably nite alphabet generalized countable numbers words continuous real vector alphabet 
theorems independent size generalize countably nite alphabets appropriately limit jx continuous alphabets separability argument 
proofs independent size may directly replace nite sums nite sums integrals carefully check validity operation 
expect theorems remain valid full generality minor technical existence convergence constraints 
nite prediction space problem long assumed existence 
case exist may de ne way achieve loss larger mum loss 
expect small nite correction order loss bounds 
delayed probabilistic prediction schemes theorems may generalized delayed sequence prediction true symbol cycle delayed feedback common practical problems 
expect bounds replaced 
error bounds probabilistic suboptimal scheme de ned analyzed hut generalized arbitrary alphabet 
active systems prediction means guessing uencing 
small step direction active systems allow system act receive loss depending action outcome probability independent action loss function known advance 
ensures greedy strategy optimal 
loss function may generalized depend history historic actions independent action 
interesting know scheme loss bounds generalize case 
full model acting agent uencing environment developed hut loss bounds proven 
universal bayesian sequence prediction miscellaneous direction investigate learning aspect universal prediction 
prediction schemes explicitly learn exploit model environment 
learning exploitation framework universal bayesian prediction 
separation aspects spirit hypothesis learning mdl vl lead new insights 
separation noise useful data usually important issue play role 
attempt information theoretic interpretation theorem may rigorous way 
may lead simpler proof theorem loss bounds 
uni ed picture loss bounds obtained loss bounds weighted majority wm algorithm fruitful 
yamanishi yam wm methods prove expected loss bounds bayesian prediction proof technique vice versa prove general loss bounds wm 
maximum likelihood predictors may studied 
system applied speci induction problems speci computable 
summary compared universal predictions bayes mixtures infeasible informed predictor unknown true generating distribution 
shown universal posterior converges 
main focus decisiontheoretic setting prediction generally action results loss true symbol sequence 
shown predictor su ers slightly loss predictor 
shown derived error loss bounds improved general making extra assumptions true independent predictor 
shown pareto optimality sense predictor performs better equal environments strictly better 
optimal predictors cases mixture distributions 
gave occam razor argument solomono prior weights optimal kolmogorov complexity 
course optimality depends setup assumptions chosen criteria 
instance universal predictor pareto optimal popular decision theoretic performance measures 
bayes predictors necessarily optimal worst case criteria cbl 
derived bound relative entropy case continuously parameterized family environments allowed generalize loss bounds continuous furthermore discussed duality bayes worst case wm approaches results classi cation tasks games chances nite alphabet active systems uencing environment 
want ray solomono urgen schmidhuber valuable discussions encouraging generalize error bounds obtained marcus hutter technical report hut various ways 
furthermore want anonymous referees preliminary versions pointing relevant literature valuable comments helped improving 
supported snf urgen schmidhuber 
entropy inequalities lemma show ln convex function 
get inequality lemma jxj get inequality 
prove partition ng fg de ne known relative entropy positive ln note probability distributions 
satisfy conditions inserting rearranging terms get ln ln sum de ne get ln ln ln ln inequality elementary known 
special choice fi upper bound follows jy jy jy jy zj ln explicate subtlety sketch proofs 
subtleties regarding checked passed 
ln 
positive means 
probability constraints apply appendices 

universal bayesian sequence prediction follows symmetry 
follows convexity 
true positive negative due special choice 
follow de nition obvious 
follows monotonicity positive arguments 
inequality follows summation noting independent 
proves lemma 
inserting yields lemma inserting jxj yields lemma 
lemma proven di erently 
arbitrary de ne ln yg ln shows implies ln proves lemma 
ut binary loss inequality de nition ln ln 
show suitable 
showing extremal values boundaries 
choose 
boundary lower bound relative entropy sum squares lemma quadratic minimum implies ab furthermore 
implies implies extremal condition keeping xed leads 
inserting convexity de nition leads 
inserting adding inequalities gives 
get induction 
inserting convexity de nition symmetry get 
inserting get proves monotonically increasing positive arguments 
marcus hutter technical report inserting de nition replacing relative entropy sum squares lemma get 
reduced problem showing 
bracket positive positive 
bracket negative decrease increasing 
resulting expression quadratic minima boundary values sucient check ab ab true summary proved ut binary loss inequality de nition ln ln 
show suitable similarly appendix proving extremal values boundaries 

boundary checked appendix extremal condition keeping xed leads 
inserting de nition replacing relative entropy sum squares lemma get 
reduced problem showing 
sucient show bracket positive 
solve get satis ed 
summary proved ut universal bayesian sequence prediction general loss inequality reduce ln im im binary case 
keeping xed showing function positive extrema interior simplex 
fz domain boundaries 
boundaries safe 
variation leads minimum im im rst inequality 
outside valid domain due constraint valid minima attained boundary 
fz 
implement constraints help lagrange multipliers leads 
summing equation obtain 
function formal expression 
eliminate favor get ln im principle function treat directly independent variable eliminated 
step determine extrema function 
clearness state line reasoning 
case 
triangle 
linear assumes extrema vertices triangle 
take account constraint plane intersects triangle 
nite line 
fg boundaries treated 
linear assumes extrema ends line edges triangle 
zero 
similar line arguments conclude necessary condition minimum boundary non zero 
implies zero 
eliminated favor analogous necessarily imply 
ectively reduced problem showing case 
go back step prove implies 
proof implies arguments holds set show ut case proven main text appendices marcus hutter technical report ag auer gentile 
adaptive self con dent line learning algorithms 
proceedings th conference computational learning theory pages 
morgan kaufmann san francisco 
angluin smith 
inductive inference theory methods 
acm computing surveys 
cal calude recursively enumerable reals chaitin numbers 
th annual symposium theoretical aspects computer science volume lncs pages paris france 
springer 
cb clarke barron 
information theoretic asymptotics bayes methods 
ieee transactions information theory 
cbl cesa bianchi lugosi 
worst case bounds logarithmic loss predictors 
machine learning 
ces cesa bianchi expert advice 
journal acm 
cha chaitin 
theory program size formally identical information theory 
journal acm 
cha chaitin 
algorithmic information evolution 
nicolis perspectives biological complexity press pages 
daw dawid 
statistical theory 
prequential approach 
statist 
soc 


stochastic processes 
john wiley sons new york 
fmg feder merhav gutman 
universal prediction individual sequences 
ieee transactions information theory 
fs freund schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences 
gr gr 
minimum length principle reasoning uncertainty 
phd thesis universiteit van amsterdam 
acs tromp vit anyi 
algorithmic statistics 
ieee transactions information theory 
haussler kivinen warmuth 
sequential prediction individual sequences general loss functions 
ieee transactions information theory 
hut hutter 
new error bounds solomono prediction 
journal computer system sciences 
universal bayesian sequence prediction hut hutter 
universal theory arti cial intelligence algorithmic probability sequential decisions 
proceedings th conference machine learning ecml pages 
kol kolmogorov 
approaches quantitative de nition information 
problems information transmission 
kw kivinen warmuth 
averaging expert predictions 
fischer simon editors proceedings th european conference computational learning theory eurocolt volume lnai pages berlin 
springer 
lev levin 
universal sequential search problems 
problems information transmission 
lev levin 
randomness conservation inequalities information independence mathematical theories 
information control 
lv li vit anyi 
inductive reasoning kolmogorov complexity 
journal computer system sciences 
lv li vit anyi 
kolmogorov complexity applications 
springer nd edition 
lw littlestone warmuth :10.1.1.37.1595
weighted majority algorithm 
th annual symposium foundations computer science pages research triangle park north carolina 
ieee 
lw littlestone warmuth 
weighted majority algorithm 
information computation 
mf merhav feder :10.1.1.8.1845
universal prediction 
ieee transactions information theory 
ris rissanen 
stochastic complexity statistical inquiry 
world scienti publ 

ris rissanen 
fisher information stochastic complexity 
ieee trans information theory january 
sch schmidhuber 
algorithmic theories 
report idsia idsia lugano switzerland 
sch schmidhuber 
hierarchies generalized kolmogorov complexities universal measures computable limit 
international journal foundations computer science 
press 
sol solomono formal theory inductive inference part 
inform 
control 
sol solomono complexity induction systems comparisons convergence theorems 
ieee trans 
inform 
theory 
sol solomono discovery algorithmic probability 
journal computer system sciences 
marcus hutter technical report sto stork 
foundations occam razor parsimony learning 
nips workshop 
www rii com stork html 
vl vit anyi li 
minimum description length induction bayesianism kolmogorov complexity 
ieee transactions information theory 
vovk 
randomness criterion 
doklady russian academy sciences doklady 
mathematics soviet mathematics doklady 
vovk 
universal forecasting algorithms 
information computation 
vovk 
competitive line statistics 
technical report docs university london 
wm david wolpert william macready :10.1.1.39.6926
free lunch theorems optimization 
ieee transactions evolutionary computation 
yam yamanishi 
decision theoretic extension stochastic complexity applications learning 
ieee transactions information theory 
ye el yaniv 
smooth online learning expert advice 
submitted publication 
zl levin 
complexity nite objects development concepts information randomness means theory algorithms 
rms russian mathematical surveys 
