active sampling class probability estimation ranking bus utexas edu department management science information systems red school business university texas austin austin texas usa foster provost stern nyu edu department information systems leonard stern school business new york university west fourth street new york ny usa cost sensitive environments class probability estimates decision makers evaluate expected utility set alternatives 
supervised learning build class probability estimates costly obtain training data class labels 
active learning acquires data incrementally phase identifying especially useful additional data labeling examples needed learning 
outline critical features active learner sampling active learning method estimating class probabilities class rankings 
boot lv identifies particularly informative new data learning variance probability estimates uses weighted sampling account potential example informative value rest input space 
show empirically method reduces number data items obtained labeled wide variety domains 
investigate contribution components algorithm show provides valuable information help identify informative examples 
compare bootstrap lv uncertainty sampling existing active learning method designed maximize classification accuracy 
results show bootstrap lv uses fewer examples exhibit certain estimation accuracy provide insights behavior algorithms 
experiment new active sampling algorithm drawing uncertainty sampling bootstrap lv show significantly competitive bootstrap lv compared uncertainty sampling 
analysis suggests general implications improving existing active sampling algorithms classification 
keywords active learning cost sensitive learning class probability estimation ranking supervised learning decision trees uncertainty sampling 
appear machine learning 
supervised classifier learning requires data class labels 
applications class labels costly 
example train diagnostic models experts may need read historical cases 
train document classifiers experts may provost need read documents assign labels 
train customer response models consumers may costly incentives reveal preferences 
active learning acquires labeled data incrementally model learned far select particularly helpful additional training examples labeling 
successful active learning methods reduce number instances labeled achieve particular level accuracy 
existing methods particularly empirical approaches active learning address classification problems assume task assign cases class fixed set classes 
applications require simple classification 
particular probability estimates central decision theory allowing decision maker incorporate costs benefits evaluating alternatives 
example targeted marketing estimated probability customer respond offer combined estimated profit zadrozny elkan evaluate various offer propositions 
applications require ranking cases likelihood class membership improve response rate offer propositions add flexibility user processing 
example documents ranked probability interest user offers consumers may proposed order probability purchase expected benefit seller 
reasons focus learning class probability estimation cpe models 
cpe error learning curves active sampling vs random sampling consider active learning produce accurate cpes class rankings fewer labeled training examples 
assume unspecified cost associated acquiring labels specifically generation obtaining training examples 
shows desired behavior active classification accuracy criticized previously metric machine learning research pro 
training set size random sampling active sampling active sampling class probability estimation ranking learner 
horizontal axis represents information needed learning number labeled training examples vertical axis represents error rate probabilities produced learned model 
learning curve shows error rate decreases training data 
upper curve represents decrease error sampling examples randomly labeling training lower curve represents sampling actively 
curves form banana shape early curves comparable model available guide active sampling 
active sampling curve soon accelerates careful choice training examples 
data random sampling eventually catches 
introduce new sampling active learning technique bootstrap lv learning cpes 
bootstrap lv uses bootstrap samples efron tibshirani available labeled data examine variance probability estimates data employs weight sampling procedure select particularly informative examples labeling learning 
show empirically range data sets bootstrap lv decreases number labeled instances needed achieve accurate probability estimates alternatively increases accuracy probability estimates fixed number training data 
analysis algorithm characteristics performance reveals contributions components 
results analysis lead design new algorithm active sampling competitive bootstrap lv popular existing method computational advantages bootstrap lv 
final result demonstrates components bootstrap lv algorithm contribute efficacy highlights existing algorithms perform cpe 

active learning bootstrap lv algorithm fundamental notion active sampling long history machine learning 
knowledge discuss explicitly simon lea winston 
simon lea describe machine learning different types problem solving learning involves simultaneous search spaces hypothesis space instance space 
results searching hypothesis space affect instance space sampled 
porter kibler porter kibler address symbiosis learning problem solving propose learning apprentice system learns problem solving rules 
method reduces reliance teacher provide examples acting system unable determine 
winston winston discusses best examples select learning near misses instances class members reasons 
subsequently theoretical results showed number training data reduced substantially selected carefully angluin 
term active learning coined describe induction algorithm controls selection potential unlabeled training examples cohn :10.1.1.119.2797
generic algorithm active learning shown 
learner applied initial set labeled examples usually selected random provided expert 
subsequently sets examples selected phases set provost unlabeled examples ul predefined condition met labeling budget exhausted 
ul large subset randomly sampled examples ul may substitute complete set roy mccallum :10.1.1.28.9963
phase candidate example xi ul assigned effectiveness score es objective function reflecting contribution subsequent learning 
examples selected labeling effectiveness scores 
multiple examples single example selected phase due computational constraints 
examples selected labels obtained querying expert added learner applied 
input initial labeled set unlabeled set ul inducer stopping criterion integer specifying number actively selected examples phase 
stopping criterion met perform phase apply inducer example ul compute es effectiveness score xi select subset size ul es remove ul label examples add output estimator induced final labeled set generic active learning algorithm objective active learning select examples reduce generalization error model 
generalization error expected error entire example space 
evaluating training example optimal active learning approach evaluate expected reduction generalization error example added training set model induced roy mccallum :10.1.1.28.9963
example expected reduce generalization error added training set 
unfortunately discuss assessing expected reduction cpe generalization error straightforward 
interested active learning scheme apply arbitrary learners computational considerations may prohibit examining models resulting adding potential unlabeled example training set prescribed roy mccallum roy mccallum :10.1.1.28.9963
resort indirect estimation potential training examples informative value 
consider potential training example help improve estimation examples space describe detail 
generic framework bootstrap lv embodies particular instantiation steps 
description provide pertains binary classification problems 
active sampling class probability estimation ranking goal reduce class probability estimation cpe error useful understand error sources 
model estimation particular input depends sample model induced treated random variable 
underlying function describing probability class membership case described input indication quality current class probability estimate example training set expected estimation absolute error reflecting discrepancy estimated probability true probability may infer discrepancy additional information needed improve model estimation 
note unfortunately inductive learning setting typically know class probability input know true class particular instance described common formulation friedman estimation error decomposes expected squared estimation error sum terms represents expectation training sets term sum referred variance estimation reflects sensitivity estimation training sample 
second term referred squared bias reflecting extent induced model approximate target function friedman 
calculating estimation error estimation bias requires knowing actual probability function mentioned available inductive learning algorithm consider 
impossible compute directly 
estimation variance reflects behavior estimation procedure underlying probability function 
order reduce estimation error bootstrap lv algorithm estimates tries reduce estimation variance 
estimation variance certain input referred local variance lv differentiate model expected variance entire input space 
ignore bias alternatively assume bias zero 
bootstrap lv algorithm shown estimates local variance potential training example 
lv high algorithm infers input captured model available data 
local variance reflects potential error reduction variance reduced examples available learner 
bootstrap lv employs lv estimations specialized sampling procedure identify examples particularly reduce average estimation error entire example space generalization error 
describe estimation local variance 
discuss sampling procedure 
efficient closed form estimation local variance may obtained arbitrary learners estimate empirically 
variance stems estimation induced random sample 
emulate series samples generating set bootstrap subsamples efron tibshirani generate set models applying inducer bootstrap sample resulting estimators calculate estimated variance example xi ul estimate variance cpes provost predicted estimators 
example ul assigned effectiveness score proportional local variance 
local variance provides indication potential error reduction individual training example 
necessarily provide indication learned examples space 
recall objective reduce generalization error training model fewer particularly informative examples training example affect estimation error examples example space 
may example high variance captured model outlier similar examples space 
algorithm bootstrap lv input initial labeled set sampled random unlabeled set ul inducer stopping criterion sample size stopping criterion met generate bootstrap subsamples apply inducer subsample induce estimator compute examples xi xi ul ds xi pi min sample probability distribution subset examples ul replacement remove ul label examples add output estimator induced bootstrap lv algorithm existing active learning algorithms select examples order effectiveness score examples highest scores selected labeling 
refer approach direct selection 
direct selection ignores information class probability estimation error examples space may affected adding example training set 
information essential evaluate expected effect example may generalization error 
random sampling referred active learning literature learning cohn lewis gale :10.1.1.119.2797
random sampling powerful allows incorporation information distribution examples information known explicitly 
example consider case examples labeling sampled random 
example may inform learning examples space similar examples 
consider set similar examples 
random sampling larger set example set sampled providing infor active sampling class probability estimation ranking mation larger number examples 
note property obtained having capture explicitly examples similar 
order reduce error example space bootstrap lv incorporates sampling selection training examples weight sampling examples labeling 
particular probability example sampled proportional effectiveness score local variance 
specifically distribution examples sampled pi min ds xi xi denotes estimated probability estimator assigns event example belongs classes choice performing calculation class arbitrary variance classes equal xi min average probability estimation assigned minority class estimators normalizing factor size ul distribution 
additional technical point note 
consider case classes represented equally training data 
high variance exists regions domain minority class assigned high probability region relatively better understood regions variance majority class assigned high probability 
case class probability estimation may exhibiting high variance due simply lack representation minority class training data benefit sampling subset examples 
estimated variance divided average value minority class probability estimates minority class deter min mined initial random sample 

related cohn cohn propose active learning approach statistical learning models generating queries training examples input space inputs learning algorithm 
approach directly evaluates effectiveness score informative contribution example learning task 
phase expectation variance model example space generate example minimizes variance 
requires computation closed form learner variance approach impracticable arbitrary models 
addition queries generated interested identifying informative examples existing set available unlabeled examples subset set possible queries 
efficient closed form estimation expected generalization error available models result adding potential training example training set induced order estimate expected changes generalization error 
roy mccallum roy mccallum propose approach building classifiers 
phase update current model additional train min provost ing example possible label calculate effectiveness score measured class entropy estimate improvement classification error 
select example bringing greatest expected reduction entropy 
algorithm shown effective reducing number examples needed obtain certain level accuracy 
learning algorithms induction new model possible training example may prohibitively expensive 
critical requirement approach allowing computationally tractable learning algorithm allow efficient incremental updates model na bayes algorithm classify text documents experiments roy mccallum :10.1.1.28.9963
efficient closed form computation error incremental model updating possible various active learning approaches compute alternative effectiveness scores 
example query committee qbc algorithm seung proposed select training examples actively training binary classifier 
examples sampled random generating stream potential training examples example considered informative labeled classifiers sampled current version space disagree regarding class prediction 
qbc algorithm employs disagreement binary effectiveness score designed capture uncertainty exists regarding class prediction current labeled examples 
mccallum nigam mccallum nigam note disadvantage stream qbc approach lies decision label example document example individually irrespective alternatives attractive method compare estimation uncertainty unlabeled training examples allowing select phase example largest classification uncertainty 
various approaches developed query committee framework identify informative examples constructing classifiers variety measures quantify level uncertainty likelihood classification error current labeled data 
particular effectiveness scores quantify estimated informative value example obtain ranking examples informative values 
subsequently example highest effectiveness score selected 
instance abe abe bagging boosting generate committee classifiers quantify disagreement margin difference weight assigned class 
examples minimum margin selected labeling 
final classifier composed ensemble classifiers votes class prediction 
uncertainty sampling lewis gale designed select informative examples construct binary classifiers adopting uncertainty notion underlying qbc approach generating committee hypotheses estimate uncertainty algorithm employs single probabilistic classifier 
examples probabilities class membership closest selected labeling 
uncertainty sampling attractive properties return 
active sampling class probability estimation ranking methods designed improve cpes rankings concern indicated effectiveness scores designed improve classification 
addition opposed approach propose methods incorporate effect potential additional training example examples example space 
particularly disregard potential training example reduce error estimation examples 
examples current estimation uncertain may significant contribution reducing estimation error examples instance space 
failure account effect noted argamon engelson dagan argamon engelson dagan mccallum nigam mccallum nigam proposed incorporate instance density measure explicitly effectiveness score density measure reflects similar examples space examined 
underlying assumption proposed similarity measure captures relative effect example reducing classification error examples space 
approach shown effective selecting informative examples document classification 
proposed density measure specific document items similarity measures available tf idf 
clear appropriate density measure arbitrary domain 
approach uses weight sampling argue implicitly incorporates properties domain support selection examples informative regarding examples space 
note weight sampling employed adaboost algorithm freund shapire iyengar iyengar base active learning approach 
algorithm results ensemble classifiers weight sampling select examples successive classifiers ensemble generated select examples labeling 
iyengar note better results obtained examples sampled compared examples selected order error measure 
propose study phenomenon hypothesize sampling allows approach avoid selecting examples repeatedly 
argue addition weight sampling acts increase likelihood selecting examples particularly informative reducing generalization error 
discuss previous paragraph selecting examples address relevance training example examples order identify examples better decrease average estimation error generalization error 
domain specific approach mccallum nigam modeled example space explicitly incorporated measure space density effectiveness score mccallum nigam weight sampling mechanism applied seamlessly arbitrary domains 
sum bootstrap lv employs effectiveness score identifies examples cpe large variance respect training data 
uses measure roy mccallum note domain specific limitation approach roy mc 
provost indicate potential improvement class probability estimation error classification accuracy 
bootstrap lv estimates local variance empirically enabling computation arbitrary modeling scheme 
lastly sampling mechanism complement selection examples learning 
argue weight sampling help account informative value example confers examples space 

experimental evaluation experiments describe examine bootstrap lv performance range domains order assess general ability identify particularly useful examples learning 
section experimental setting 
sections discuss results learning simple probability estimation trees learning bagged probability estimation trees respectively 
discuss additional evaluation measures section 
section compare bootstrap lv uncertainty sampling active learning approach designed improve classification accuracy order provide insight operation algorithm advantage compared existing approaches 
experiments new active learning algorithm inspired empirical investigation provide insight elements bootstrap lv algorithm 
experimental setting applied bootstrap lv data sets uci machine learning repository blake previously evaluate rule learning algorithms cohen singer 
data sets classes mapped class problems 
data sets minority class associated class remaining classes mapped second class 
experiments tree induction produce class probability estimates 
particular experiments underlying probability estimator probability estimation tree pet unpruned decision tree quinlan laplace correction cestnik applied leaves 
pruning laplace correction shown improve cpes produced pets bauer kohavi provost provost domingos :10.1.1.33.353:10.1.1.33.353:10.1.1.33.353:10.1.1.21.805
models learned data performance improves typically learning curve bootstrap lv aims obtain comparable performance fewer labeled data recall 
evaluate predictive quality cpe models induced bootstrap lv desirable compare true class probability values example computing mean absolute error respect actual probabilities 
data sets contain class membership information true class probabilities unknown 
compare probabilities assigned probability estimation trees easy build fast computationally robust data sets comprehensible human experts produce surprisingly probability rankings provost domingos 
active sampling class probability estimation ranking model induced bootstrap lv phase assigned best estimator surrogates true probabilities induced entire set available training examples ul labels examples known 
particular induce bagged pets shown produce superior probability estimates compared individual pets bauer kohavi provost provost domingos :10.1.1.33.353:10.1.1.33.353:10.1.1.33.353:10.1.1.21.805
calculate mean absolute error denoted best estimate mean absolute error estimator respect estimation 
pe xi estimated probability pe xi probability estimated number test examples examined 
compare performance bootstrap lv method denoted random estimators induced inducer training set size examples sampled random 
compare different sizes labeled set order large sample sizes large data sets small ones small data sets applied different numbers sampling phases different data sets varying data set phase number examples added results averaged random way partitions data set initial labeled set unlabeled set test set estimators evaluated 
control partitions random bootstrap lv 
banana curve shows relative performance car data set 
shown error estimator induced bootstrap lv decreases faster initially exhibiting lower error fewer examples 
demonstrates examples actively added labeled set informative average allowing inducer construct better estimator certain number training examples 
note visibility algorithms performances initial labeled set algorithms perform identically shown 
evaluations active learning algorithms initial part learning curve demonstrate efficacy algorithm 
summarize comparative performance competing algorithms entire leaning curve 
particular objective bootstrap lv enable learning fewer examples order obtain certain level cpe accuracy 
data set calculate set measures pertaining saving obtained bootstrap lv terms number examples need labeled boot lv random 
number examples saved bootstrap lv certain performance level demonstrated 
sampling phase algorithm calculate difference number examples needed boot lv obtain exhibited error level number needed random obtain error level 
calculate average saving sampling phases referred average saving saving percentage number examples needed random percentage examples saved bootstrap lv random referred average relative saving instance provost car domain average saving examples average relative saving examples needed random natural banana shape learning curves ideal case performance estimators induced samples considerably different final sampling phases available examples sampling methods samples obtained methods increasingly similar 
average phases provides indication bootstrap lv produces superior estimations 
telling examine improvement fat part banana benefit active learning concentrated 
allow stable assessment presenting saving exhibited bootstrap lv single best sampling phase average saving top sampling phases 
call top saving top saving percentage examples needed random referred top relative saving instance car domain top saving examples examples needed random 
percentage sampling phases saving obtained random needed examples obtain error level exhibited bootstrap lv 
refer percentage phases savings 
error reduction car examples saved training set size learning behavior bootstrap lv random car data set data set error reduction achieved boot lv respect random number training examples 
demonstrated 
calculate average error reduction phases largest error reduction observed refer random sampling active sampling active sampling class probability estimation ranking top error reduction 
car domain top error reduction 
results bootstrap lv versus random sampling data sets bootstrap lv exhibits dramatic results car data set shows results pendigits data set impressive win 
bootstrap lv achieves minimal level error examples 
random requires examples obtain error level 
important note active learning algorithm performance particularly interesting initial sampling phases demonstrating performance obtained relatively small portion data small labeling cost 
similarly results initial phases error exhibited model induced bootstrap lv selection training examples reduced substantially faster examples sampled randomly 
training set size cpe learning curves pendigits data set 
bootstrap lv accelerates error reduction considerably initial sampling phases 
data sets bootstrap lv succeed accelerating learning shown weather data set 
note accuracy comparable obtained random sampling curve consistently resides 
discussed 
random bootstrap lv provost table presents summary results data sets 
second column shows percentage phases savings 
third fourth columns show top relative saving top saving respectively 
fifth sixth columns table show average relative saving average saving sampling phases applying bootstrap lv 
seventh column presents top error reduction 
training set size cpe learning curves weather data set bootstrap lv provide improvement cpe summarizing results conservative regard methods comparable percent phases saving 
condition bootstrap lv deemed superior exhibits superior performance phases examined 
addition order bootstrap lv superior require average relative saving higher symmetrically random superior average percentage gain lower 
seen table bold data sets bootstrap lv exhibited superior performance 
particularly data sets percentage phases savings 
top relative saving data sets bootstrap lv number examples needed random achieve accuracy level 
data set example bootstrap lv gradually improves saving examples needing fewer examples required random obtain level accuracy 
results pertain average improvement obtained top phases maximal savings greater 
random bootstrap lv active sampling class probability estimation ranking table improvement examples needed improvement error bootstrap lv versus random data set phases savings top relative saving top saving avg 
relative saving avg 
saving top relative error reduction abalone adult breast cancer car coding connect german hypothyroid kr vs kp letter letter vowel move ocr optdigits pendigits sick euthyroid solar flare weather yeast german credit database letter recognition letter letter recognition vowels measures pertaining number examples saved error reduction complement provide interesting insight 
instance number examples saved help evaluate difficulty error reduction reflected number examples required random obtain reduction 
example top relative error reduction connect table shows random needs additional examples average obtain improvement 
single data set weather bootstrap lv exhibited negative average saving 
percentage phases savings showing bootstrap lv uses fewer examples phases examined indicate methods exhibit comparable learning curves data set 
examination learning curves data sets bootstrap lv exhibits insignificant improvement reveals training examples chosen random contribute error reduction constant rate 
shown weather data set data sets learning curves provost weather data set data sets learning curves data sets letter vowel atypical shape additional examples bring constant reduction error expected decreasing marginal error reduction 
may indicate training examples equally informative regardless examples training 
intelligent selection training examples improve learning produce results comparable obtained random selection 
german random bootstrap lv training set size solar flare random bootstrap lv training set size learning curves data sets bootstrap lv random show comparable performance experiments bagged pets order verify bootstrap lv effective solely pets experimented different cpe learner 
bagged pets creates ensemble bagged trees tree induced different bootstrap efron move training set size letter vowel random bootstrap lv random bootstrap lv training set size active sampling class probability estimation ranking tibshirani sample 
trees estimate class probability instance averaging cpes individual pets ensemble 
bagged pets substantially complex simple pets shown generally produce superior cpes compared simple pets bauer kohavi provost provost domingos :10.1.1.33.353:10.1.1.33.353:10.1.1.33.353:10.1.1.21.805
bootstrap lv performance bagged pets results obtained individual pets 
particularly data sets bootstrap lv exhibited percentage phases savings percentage phases savings 
top relative saving greater data sets 
data sets percentage phases savings 
shows comparison bootstrap lv random simple pets bagged pets 
error exhibited bagged pets lower simple pets models bootstrap lv achieves lowest error considerably fewer examples required random 
training set size cpe learning curves hypothyroid data set showing performance bootstrap lv random bagged pets simple pets 
evaluation criteria evaluated bootstrap lv alternative performance measures mean squared error measure bauer kohavi area roc curve denoted auc bradley specifically evaluates ranking accuracy 
results measures agree obtained 
example bootstrap lv generally leads roc curves fewer examples 
presents learning curves measures car pendigits hypothyroid data sets learning curves earlier 
random bootstrap lv pet random pet bootstrap lv auc auc auc car training set size pendigits training set size hypothyroid provost random lv learning curves area roc curve mse comparing boot lv random random bootstrap lv random bootstrap lv training set size mse mse mse training set size car pendigits random bootstrap lv random lv training set size hypothyroid random bootstrap lv training set size active sampling class probability estimation ranking comparisons uncertainty sampling compare bootstrap lv active learning algorithm previously shown improve classification accuracy improved classification accuracy may result improved class probability estimation error 
comparison shows focusing improving cpes adds value provides interesting insight properties algorithms 
comparison selected known uncertainty sampling algorithm lewis gale proposed active learning binary classifiers 
choice generality algorithm allowing applied arbitrary modeling scheme produces cpes arbitrary data set 
addition uncertainty sampling focuses identifying training examples change classifier architecture 
contrast active learning algorithms result ensemble classifiers abe 
comparing active learning single classifiers active random sampling confounds effects active learning producing ensembles 
uncertainty sampling allows compare selection mechanism algorithms wide range domains 
summary comparison results table measures table baseline comparison uncer tainty sampling random 
bootstrap lv exhibits markedly superior performance compared uncertainty sam pling 
particularly bootstrap lv superior data sets bold data sets methods exhibit comparable performance savings exhibited phases 
uncertainty sampling exhibits superior performance data set solar flare produces better probability estimations prior comparison data set bootstrap lv considerably better ran dom 
factors contribute weak performance uncertainty sampling cpe compared bootstrap lv 
understand recall differences uncertainty sampling bootstrap lv effectiveness score algorithm assigns potential training examples mechanisms employ sample select examples labeling 
consider 
uses direct selection uncer tainty sampling account potential relevance training example improving estimation examples space 
susceptible selecting examples little contribution average error example space 
may degrade performance particularly compared random sampling 
second effectiveness score causes uncertainty sampling prefer examples cpe close 
examples true class probability close captured correctly model cpe close ensembles usually improve learning curves random selection 
provost selected provide little new information learning 
similarly uncertainty sampling select examples cpes close estimations erroneous 
note uncer tainty sampling designed classification reasonable 
cpe correct side decision boundary sufficient correct classification may exhibit large estimation error 
policy productive selecting examples improve classification accuracy deny information important learner improve model cpes 
table improvement number training examples required achieve certain accuracy level improvement error number training examples data set bootstrap lv versus uncertainty sampling phases savings top relative saving top saving avg 
relative saving avg 
saving top relative error reduction abalone adult breast cancer car coding connect german hypothyroid kr vs kp letter letter vowel move ocr optdigits pendigits sick euthyroid solar flare weather yeast active sampling class probability estimation ranking note cpes extreme correct side decision bound ary effort select examples improve cpe may undermine improvement classification accuracy 
may inferred friedman analysis classification error 
particular binary classification error minimized class occur predicted 
probability due erroneous cpe predicted class class denoted df df true 
assuming approximated standard normal distribution 
probability ef sign var upper tail area standard normal distribution denotes statistical expectation 
certain estimation variance true class probability expected probability estimation ef side decision boundary farther ef probability classification error reduced probable estimated class probability wrong side decision boundary 
active learning algorithm aiming improve classification accuracy may beneficial improve cpes 
instance consider true class probability mean estimation 
attempt alter procedure reduce mean estimation increases likelihood estimation particularly estimation variance large increasing likelihood classification error 
effect weight sampling argued earlier weight sampling reduce generalization error 
particularly argued ability account example potential reducing error examples example space 
shows pendigits data set error obtained weight sampling viz bootstrap lv bootstrap lv direct selection effectiveness score random sampling 
readability samples 
seen initial critical sampling phases active learning weight sampling results lower error compared direct selection random sampling 
phenomenon seen data sets 
superiority bootstrap lv random sampling demonstrates weights assigned examples bootstrap lv underlie sampling process provide useful information selecting informative training examples 
note respect active leaning algorithm estimation procedure variance expectation appear formulation incorporates choice training examples 
provost models induced weights ignored examples sampled random weights equal inferior induced assigned weights incorporated direct sampling process 
training set size learning curves weight sampling direct selection bootstrap lv effectiveness score random additionally weight sampling important direct selection provides results inferior bootstrap lv 
discussed section argue weight sampling important order select examples affect examples space avoid selecting training examples captured model estimation may improved provide information examples space reduce generalization error significantly 
considerations may result worse error reduction direct selection observed comparison bootstrap lv 
apparently choosing examples potential reducing error single example methods sufficient 
important consider effect training example general population examples space 
improving uncertainty sampling cpe demonstrate effect weight sampling identifying informative examples propose improvement uncertainty sampling incorporating weights reflect uncertainty sampling rationale effectiveness score direct selection random bootstrap lv active sampling class probability estimation ranking weight sample examples weights 
show performance algorithm improves cpe compare bootstrap lv 
uncertainty sampling selects examples cpe close assign example weight reflects distance 
particular sampling phase weight assigned example 

pi normalization factor distribution 
probability example sampled increases closer cpe 
algorithm denoted weighted uncertainty sampling wus described 
input initial labeled set unlabeled set ul inducer stopping criterion integer specifying number actively selected examples phase 
stopping criterion met perform phase apply inducer 

pi example xi xi ul assign weight ws xi sample probability distribution subset examples ul replacement remove ul label examples add output estimator induced weighted uncertainty sampling algorithm comparing new wus algorithm bootstrap lv cpe see wus competitive bootstrap lv uncertainty sampling summary results table 
bootstrap lv outperforms wus data sets bold bootstrap lv wus comparable data sets wus superior italicized 
comparison bootstrap lv provides superior cpes compared uncertainty sampling data sets 
data sets uncertainty sampling inferior bootstrap lv wus exhibits comparable performance bootstrap lv 
bootstrap lv remains superior new weighted uncer tainty sampling algorithm exhibits improved performance compared uncertainty sampling 
shows cpe learning curves bootstrap lv uncertainty sampling wus connect data set 
uncertainty sampling inferior boot lv connect data set wus performance comparable boot lv 
assert attributed primarily wus accounting broader set considerations selecting examples particularly wus consideration potential error reduction effect example may examples space 
provost shows learning curves algorithms sick euthyroid data set similarly wus performance considerably better uncer tainty sampling cpe generalization error bootstrap lv better obtained wus 
improved performance bootstrap lv data sets demonstrates weights assigned bootstrap lv better support sampling mechanism identifying informative examples improve cpe 
discussed section weights assigned examples wus may adequate improve cpes 
particularly focus selecting examples cpe closer avoiding examples cpe closer hinders reduction cpe generalization error 
table improvement examples needed improvement error data set bootstrap lv versus weighted uncertainty sampling phases savings top relative saving top saving avg relative saving avg saving top relative error reduction abalone adult breast cancer car coding connect german hypothyroid kr vs kp letter letter vowel move ocr optdigits pendigits sick euthyroid solar flare weather yeast active sampling class probability estimation ranking example wus superior uncertainty sampling achieves performance comparable bootstrap lv connect training set size sick euthyroid bootstrap lv remains superior wus shows significant improvements compared uncertainty sampling bootstrap lv uncertainty sampling wus bootstrap lv uncertainty wus training set size provost results suggest informative effectiveness score weight sampling provides important additional information improving selection informative training examples 
performance algorithms effectiveness score computed bootstrap lv superior score assigned examples wus effectiveness score wus informative 
discussed earlier preferring examples cpe close wus identifies examples class uncertain uncertainty apparently implies uncertainty regarding cpe benefit gaining relevant evidence 
bootstrap lv produces better results wus may fail identify cpe uncertainties particularly uncertainties imply class uncertainty 
addition mentioned cpe close necessarily imply cpe uncertainty true cpe close correctly estimated model 
results wus suggest algorithms improving classification accuracy capitalize weight sampling 
example wus may exhibit improved performance compared uncertainty sampling classification accuracy 
similarly effectiveness scores proposed identify examples increase classification accuracy entropy incorporate additional measures capture effect training example examples space benefit weight sampling 
limitations advantages gained bootstrap lv come computational cost 
phase models induced bootstrap samples 
number training examples cost generating bootstrap sample ko arbitrary modeling scheme computational complexity inducing model examples added complexity inducing models kc order compute weights weight sampling model applied estimate class probability examples ul average complexity applying model particular input example depends type model 
phase bootstrap lv subsequently samples examples log ul constitutes cost procedure generic complexity sorting list selected random numbers scanning ul corresponding examples 
computation cost phase ul log dependent modeling scheme 
number examples sampled phase relatively small dominant computational components cost generating model done times cost applying model done ul mentioned earlier large unlabeled set sample ul 
addition typical shape learning curve certain training set size marginal error reduction insignificant active learning random sampling employed 
intelligent active sampling class probability estimation ranking selection examples learning critical early part curve small 
remains relatively small multiple model induction bootstrap samples constitute considerable computational toll 
bootstrap lv provides appropriate solution labeling costs important computational costs primary concern obtain accurate cpe ranking minimal costly labeling 
bootstrap lv address computational concerns explicitly lewis catlett lewis catlett 
uncertainty sampling simpler computationally performance significantly inferior bootstrap lv initial sampling phases inferior random sampling 
bootstrap lv performance surpasses performance weighted uncertainty sampling 
weighted uncertainty sampling incorporates cpe uncertainty measure computationally simpler may considered active learning cpes computational concerns particularly critical 
lastly bootstrap lv relies detecting variance cpe infer examples useful obtaining accurate estimation 
performance may hampered low variance model logistic regression learning 
bootstrap lv designed fewer labeled training data produce accurate class probability estimates 
algorithm addresses key components active learning effectiveness score selection procedure complement identify particularly informative examples learning class probability estimates 
bootstrap lv domain independent restricted particular learning algorithm 
empirical evaluation approach shows performs fewer training data 
evaluation encompasses wide range benchmark domains providing evidence general efficacy algorithm 
particular results show information provided effectiveness scores improves random sampling weights equal 
show bootstrap lv outperforms existing active learning method uncertainty sampling 
investigate properties algorithms explain results 
example demonstrate weights assigned potential training examples weight sampling procedure combine produce superior cpes 
lastly results investigation propose active learning algorithm weighted uncertainty sampling assigns effectiveness scores reflecting rationale uncertainty sampling effectiveness score addition employs scores weight sample examples training bootstrap lv 
comparison bootstrap lv reveals bootstrap lv superior improving cpes demonstrating value bootstrap lv effectiveness score demonstrates advantages conferred weight sampling 
improvement direct selection suggests application weight sampling effectiveness scores proposed literature active learning classifiers 
provost making decisions cost sensitive environments takes decision theoretic approach evaluating alternatives requiring estimation probabilities events classes order assess alternative decisions 
environments labeling costs taken account 
shown active sampling effective reducing cost labeling necessary build accurate models class probability estimation ranking 
acknowledgments vijay iyengar helpful comments 
ibm faculty partnership award penn state ebusiness research center support 
sponsored part defense advanced research projects agency darpa air force research laboratory air force materiel command usaf agreement number 
government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon 
views contained authors interpreted necessarily representing official policies endorsements expressed implied defense advanced research projects agency darpa air force research laboratory government 
abe 

query learning strategies boosting bag ging 
proceedings fifteenth international conference machine learning pp 

angluin 

queries concept learning 
machine learning 
argamon engelson dagan argamon engelson dagan commit tee sample selection probabilistic classifiers journal artificial intelligence research volume pages 
bauer kohavi 

empirical comparison voting classification algo rithms bagging boosting variants 
machine learning 
blake merz 

uci repository machine learning databases 
irvine ca university california department information computer science www ics uci edu mlearn mlrepository html 
bradley 

area roc curve evaluation machine learning algorithms 
pattern recognition 
breiman 

predictors 
machine learning 
active sampling class probability estimation ranking cestnik 

estimating probabilities crucial task machine learning 
pro ceedings ninth european conference artificial intelligence 
cohn atlas ladner 

improved generalization active learn ing 
machine learning 
cohn ghahramani jordan 

active learning statistical mod els 
journal artificial intelligence research 
cohen singer 
simple fast effective rule learner 
proceed ings sixteenth national conference american association artificial intelligence 
efron tibshirani 

bootstrap chapman hall 
freund schapire 

experiments new boosting algorithm 
proceedings international conference machine learning 
friedman 


bias variance loss curse dimensionality 
journal knowledge discovery data mining 
iyengar apte zhang 

active learning adaptive pling 
sixth acm sigkdd international conference knowledge discovery data mining 

lewis gale 

sequential algorithm training text classifiers 
proceedings seventeenth annual international acm sigir conference research development information retrieval 
lewis catlett 

heterogeneous uncertainty sampling 
proceedings eleventh international conference machine learning 
nigam 

employing em pool active learning text classification 
proceedings fifteenth international conference machine learn ing pp 

provost 

tree induction vs logistic sion learning curve analysis 
working stern school business new york university ny ny 
appear journal machine learning research 
porter kibler 

experimental goal regression method learning problem solving heuristics 
machine learning 
provost provost fawcett kohavi 

case accuracy estimation comparing classifiers 
proceedings fifteenth international conference machine learning 
provost domingos 

trained pets improving probability esti mation trees 
working stern school business nyu 
quinlan 

programs machine learning 
morgan kaufman san mateo california 
roy mccallum 

optimal active learning sampling estimation error reduction 
proceedings eighteenth international conference machine learning 
seung opper 

query committee 
proceed ings fifth annual acm workshop computational learning theory 
simon 
lea 

problem solving rule induction unified view 
gregg ed knowledge cognition 
chap 

md erlbaum 
turney 
types cost inductive concept learning workshop cost sensitive learning icml stanford university california 
winston 

learning structural descriptions examples 
psychol ogy computer vision winston ed mcgraw hill new york 
zadrozny elkan 

learning making decisions costs prob abilities unknown 
proceedings seventh acm sigkdd international conference knowledge discovery data mining 


