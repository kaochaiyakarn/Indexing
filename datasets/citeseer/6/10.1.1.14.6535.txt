context sensitive learning methods text categorization william cohen yoram singer labs implemented machine learning algorithms ripper sleeping experts phrases evaluated number large text categorization problems 
algorithms construct classifiers allow context word affect presence absence contribute classification 
ripper sleeping experts differ radically respects differences include different notions constitutes context different ways combining contexts construct classifier different methods search combination contexts different criteria contexts included combination 
spite differences ripper sleeping experts perform extremely wide variety categorization problems generally outperforming previously applied learning methods 
view result confirmation usefulness classifiers represent contextual information 
categories subject descriptors information storage retrieval information search retrieval artificial intelligence learning concept learning parameter learning pattern recognition applications text processing general terms algorithms experimentation additional key words phrases context sensitive models mistake driven algorithms line learning rule learning text categorization 
learning methods frequently automatically construct classifiers labeled documents lewis lewis ringuette lewis gale apt yang chute hull wiener cohen 
article investigate performance implemented machine learning algorithms number large text categorization problems 
algorithms considered set valued ripper rule learning algorithm cohen earlier version article appeared proceedings th annual international acm conference research development information retrieval sigir pp 

authors address labs park avenue florham park nj email research att com singer research att com 
permission digital hard copy part personal classroom granted fee provided copies distributed profit commercial advantage copyright notice title publication date appear notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee 
acm acm transactions information systems vol 
april pages 
cohen singer sleeping experts new line learning method freund 
algorithms share features attractive large text categorization problems 
algorithms efficient large noisy corpora running linear nearly linear time 
second algorithms called direct representation text document represented ordered list tokens particular necessary extract corpus small set informative features 
third algorithms allow context word influence presence absence contribute classification 
common text categorization schemes naive bayes rocchio algorithm produce linear classifiers features correspond individual terms typically words word stems 
linear classifier presence absence word document influence predicted class regardless words document linear classifier effectively assumes context word appears encoded words document effect meaning assumption obviously unrealistic hope performance improved relaxing 
way relax assumption build linear classifier uses complex features example features test occurrence word enclosing context 
principle technical challenge finding useful complex features potentially enormous space potential features 
algorithms investigate sleeping experts algorithm type 
second way introducing context learn nonlinear classifier 
second algorithm investigate ripper learns nonlinear classifier form boolean combination simple terms alternatively ripper thought learning disjunction contexts context defined conjunction simple terms 
principle technical challenge approach followed learn nonlinear classifiers efficiently 
final commonality sleeping experts ripper algorithms aesthetic advantage contexts determined learning algorithm 
external process selects combinations words contexts integrates predictions classifiers learned different externally imposed notions context contexts created naturally product process finding predictive classifier 
sharing important properties ripper sleeping experts differ radically respects 
ripper context word conjunction form document document document acm transactions information systems vol 
april 
context sensitive learning methods text categorization words context word ripper consists usually small number words cooccur may occur order location document 
contrast sleeping experts uses sparse phrases represent context 
sparse phrase sequence nearby necessarily consecutive words context word sleeping experts consists nearby words fixed order 
ripper attempts find simple hypothesis form small disjunction conjunctions form shown accurately classifies training data 
solve difficult optimization problem number heuristic methods 
contrast sleeping experts hypothesis linear combination sparse phrases occur training corpus typically enormous set 
construct large linear combination sleeping experts uses multiplicative update algorithm strong theoretical justifications competitive analysis learning algorithms cesa bianchi freund weak hypothesis boosting freund schapire 
conjunctions included ripper hypotheses represent contexts positively correlated class learned 
sleeping experts phrases positively negatively correlated class interest negatively correlated phrases taken evidence membership category 
interesting observations article ripper sleeping experts phrases perform extremely wide variety categorization problems generally outperforming previously applied learning methods 
fact holds spite fundamental differences listed constitutes context combine contexts search combination contexts contexts included combination 
view result confirmation usefulness learning classifiers represent contextual information 
remainder article learning algorithms describe benchmark problems experimental results obtained 

learning algorithms ripper ripper learns rule sets 
describe algorithm ripper 
classifier constructed ripper set rules illustrated learned ireland category ap titles corpus see section 
set course types context learning algorithm notions context adopted ripper sleeping experts just alternatives 
acm transactions information systems vol 
april 
cohen singer fig 

learned ruleset category ireland 
rules interpreted disjunction conjunctions instance document considered category ireland word ireland appears word ira appears word killed appears word ira appears word shot appears word ira appears word appears rule sets enjoy properties useful certain situations 
rule set compact relatively easy people understand may easier users accept learned classifier reasonable 
rule sets easily converted queries boolean search engine cohen singer 
number subtleties involved learning rule sets 
particular relatively straightforward greedy algorithms give rule sets unnecessarily high error rates furthermore algorithms find accurate rule sets tend relatively inefficient large noisy data sets 
problems algorithm ripper relatively complex 
detailed description ripper motivation details algorithm cohen simply summarize algorithm 
restrict problems classes 
case rules learned ripper identical consequent name positive class 
algorithm ripper summarized consists main stages 
stage greedy process constructs initial rule set 
stage earlier rule learning algorithm called incremental reduced error pruning irep rnkranz widmer turn earlier due quinlan cohen brunk pazzani pagallo haussler 
second stage optimization phase attempts improve compactness accuracy rule set 
stage building initial rule set 
stage ripper variant irep call irep 
irep set covering algorithm constructs rule time removes examples covered new acm transactions information systems vol 
april 
context sensitive learning methods text categorization fig 

ripper algorithm 
rule soon rule constructed 
heuristics constructing rule intended ensure rule covers positive examples negative examples 
construct rule uncovered examples randomly partitioned subsets growing set containing thirds examples pruning set containing remaining third 
irep grow rule simplify prune rule 
rule grown repeatedly adding conditions rule empty antecedent 
done greedy fashion stage single condition added rule producing longer specialized rule 
condition added yields largest information gain relative quinlan 
information gain defined gain ri ri ti log ti ti ti log ti ti ti tj respectively tj number positive negative examples growing set covered rule rj 
information gain rewards rules ri increase density positive examples covered rule greatly reducing total number covered positive examples 
greedy addition new literals continues clause covers negative example covered rule satisfies rule antecedent 
acm transactions information systems vol 
april 
cohen singer examples growing set condition positive information gain 
growing rule rule pruned simplified 
greedy process 
stage irep considers deleting final sequence conditions rule chooses deletion maximizes function ui ri ui ui ui respectively ui number positive negative exam ples pruning set covered new rule 
pruning pruned clause added rule set examples covered removed 
adding rules 
requirement information gain nonzero means rule cover positive example guaranteeing irep eventually terminate 
noisy data possible rules constructed covers examples computationally expensive large data sets 
irep includes additional heuristic attempts determine current rule set large training data 
stopping criterion minimum description length mdl heuristic 
mdl heuristics assumption best model set data model allows succinctly encode data 
generally encoding data done parts model encoded errors model data encoded 
preferred model smallest description length number bits required part encoding 
encode errors model ripper uses encoding scheme proposed quinlan 
variant scheme encode rules 
irep rule added rule set total description length current rule set examples computed 
irep stops adding rules description length bits larger smallest description length obtained far positive examples 
rule set compressed examining rule turn starting rule added deleting rules increase description length 
quinlan scheme log log bits encode subset elements set size represents expected value fraction encode rule encoding length rule antecedent encoding antecedent element subset set possible conditions default 
acm transactions information systems vol 
april 
context sensitive learning methods text categorization stage optimizing rule set 
ripper stops adding rules rule set optimized reduce size improve accuracy 
rules considered turn order added 
rule alternative rules constructed 
replacement formed growing pruning rule pruning guided minimize error entire rule set replacing pruning data 
revision formed analogously grown greedily adding literals empty rule 
decision final theory include revised rule replacement rule original rule 
decision description length heuristic definition smallest description length compression preferred 
optimization definition may cover fewer positive examples irep called uncovered positive examples additional rules generates added 
optimization step repeated occasionally resulting improvements rule set 
experiments large collection propositional learning benchmark problems indicate rounds optimization usually sufficient ripper optimization step default repeated twice 
emphasized ripper fairly complex algorithm algorithm parameters fixed experiments conducted 
primary set benchmarks developing algorithm setting parameters set classification problems taken uc irvine repository cohen 
extensions ripper motivated text 
running experiments ripper modified appropriate text categorization problems 
extension allows user specify loss ratio lewis catlett 
loss ratio indicates ratio cost false negative cost false positive goal learning minimize misclassification cost unseen data 
appropriate loss ratio ripper trade recall category precision 
loss ratios ripper implemented appropriately changing weights false positive errors false negative errors pruning optimization stages learning algorithm 
second extension ripper motivated large number features typically available text categorization problems 
initial implementation ripper examples represented feature vectors 
implementation learn rule sets shown constructing appropriate set boolean features example construct boolean feature word appearing corpus letting true example appears corresponding document 
corpus documents words represented matrix 
acm transactions information systems vol 
april 
cohen singer text classification representation inefficient usage space moderately large corpus usually contain different words appear particular document 
avoid problem ripper extended allow value attribute set symbols instance document represented single attribute having value set words appear document 
primitive tests set valued attribute tests allowed rules form wi implementation extension explained briefly detail cohen 
constructing rule ripper find single test maximizes information gain ripper run time spent operation 
set valued attributes done steps 
ripper iterates set examples covered current rule recording set words ws appear elements attribute example recording word wi ws statistics pi number times wi appears positive example ni number times wi appears negative example second ripper iterates words ws statistics compute gain possible test form wi entire process requires time linear size notice symbols wi appear elements attribute training example considered ripper 
optionally ripper include tests form wi feature rules 
extending ripper find tests simple matter formal reasons wary cohen 
article typically results ripper negative word tests forbidden 
experiments document generally represented single set valued feature value set words appearing document 
implementation set include multiple occurrences element extra occurrences simply ignored represent document list words appear 
alternatively set valued features encode sophisticated representations document set word stems set words set phrases 
article elected sets unstemmed words word sequence alphanumeric characters ignoring case 
choice largely sake simplicity consistent emphasis direct representations text 
results improved somewhat word stems words experimental results retrieval problems show stemming lead slight gain average performance hull grefenstette plausible believe gain achieved classification 
similar reasons adopted unusual approach handling large number features available text categorization problems acm transactions information systems vol 
april 
context sensitive learning methods text categorization previous researchers generally relied preprocessing reduce number possible features particularly applying symbolic learning systems text categorization elected extend algorithms directly handle large sparse feature sets directly little feature selection 
reasons choice 
reason evidence effectiveness feature selection mixed contexts appropriate feature selection methods improved generalization performance almuallim dietterich john cases performance degraded feature selection lewis ringuette cohen 
important able efficiently large feature sets explore wide range levels feature selection 
second reason learning system accurate generalization performance easy 
feature selection improve generalization performance application learning system somewhat complex introduces set parameters set 
sleeping experts phrases background 
sleeping experts new framework combining advice different experts words predictions classifiers developed computational learning community see instance littlestone littlestone warmuth kivinen warmuth freund 
prediction algorithms framework pool fixed possibly infinite experts usually simple fixed classifier build master algorithm combines classifications experts manner 
typically master algorithm classifies example weighted combination predictions experts 
building master algorithm matter finding appropriate weight experts 
vast majority weight allocation algorithms line algorithms examples fed master algorithm updates weight different experts performance prediction example 
weight updating methods examined analyzed 
article multiplicative update method weights individual experts modified multiplying constant 
algorithm includes certain normalization steps describe shortly 
empirical evidence indicates multiplicative update algorithms outperform traditional learning techniques linear classifiers blum lewis 
formal results show circumstances high dimensional weight allocation problems handled moderate amounts training data appropriate multiplicative weight update algorithm littlestone warmuth acm transactions information systems vol 
april 
cohen singer table experts large weights category ireland log weight number occurrences phrase ireland ireland ireland ireland belfast ira says northern ireland catholic man killed ira claim moderate catholic ira supporters west belfast kivinen warmuth freund 
particular certain multiplicative update procedures learn quickly set experts huge long experts accurate contrast formal bounds performance traditional additive update rules suggest number experts small order guarantee generalization performance 
results suggest multiplicative update procedures perform text categorization problems provided appropriate large set experts 
describe application multiplicative update algorithms experts correspond roughly length phrases occur corpus 
sleeping experts algorithm 
sleeping experts algorithm update algorithm formal advances multiplicative update algorithms 
weight allocation algorithm called hedge freund schapire applicable broad class learning problems loss functions 
second model blum 
setting may number experts post predictions example remainder said sleeping example 
context document classification expert lexical unit 
expert awake predicts unit appears document 
order test compare simple context sensitive models chose pool possible experts set sparse phrases appear document 
ith word appearing document 
expert ordered list words form 
ij put way expert corresponds phrase may appear position text bound particular positions 
allow holes phrase term sparse phrases generalization word gram model 
note set acm transactions information systems vol 
april 
context sensitive learning methods text categorization fig 

sleeping experts phrases algorithm 
sparse phrases exactly set words appearing corpus 
table gives examples sparse phrases useful ireland category 
sleeping experts framework weight associated phrase learned line manner minimize cost function case classification error 
order fair cost functions associate different utility value possible pair correct outcome outcome predicted algorithm implemented sleeping experts framework 
cost functions result nonsymmetric weight update scope article 
acm transactions information systems vol 
april 
cohen singer comparison systems update weights different phrases training phase keep fixed test phase 
give description sleeping experts procedure phrases 
pseudocode algorithm shown 
master algorithm maintains pool set recording sparse phrases appeared previous documents set containing weight sparse phrase pool 
times weights nonnegative weights need sum 
time step new document 
content tth document denoted length number words document 
set active phrases denoted wt ij phrase sequence words possibly containing gaps appearing fixed order position document 
set task master algorithm prediction prediction active miniexperts 
implementation associate phrase miniexperts denoted 
consistently predicts document belongs class sparse phrase appears document second consistently predicts document belong class 
clearly right correct classification document 
prediction master algorithm decides distribution active miniexperts determined restricting set weights set active miniexperts normalizing weights 
set active miniexperts denoted 
denote vector normalized weights pw pw prediction master algorithm denoted depends weighted sum miniexperts 
half miniexperts acm transactions information systems vol 
april 
context sensitive learning methods text categorization predict active sum efficiently calculated summing weights miniexperts predict yt tp master algorithm combines predictions experts single prediction number zero 
order classify document set threshold classify document positive class minimizing error 
achieve desired precision recall threshold adjusted low value usually results higher recall high value usually results higher precision 
receiving predictions miniexperts master algorithm receives true classification document updates weights miniexperts appropriately 
goal update weights active miniexperts reflect correlation correct classification document 
update weights weight expert multiplied factor loss parameter called learning rate controls quickly weights updated 
values range 
implementation absolute error loss function predicted incorrectly predicted correctly 
rule summarized follows miniexperts phrase multiply weight incorrect keep weight correct unchanged 
updating weights miniexperts weights active miniexperts normalized total weight active miniexperts change effect renormalization increase weights miniexperts correct 
illustration effect weight update procedure table lists experts weight miniexperts predicting document belongs category ireland ap titles corpus large 
symbol place holder stands word sparse gram 
expert give logarithm weight miniexperts number positive respectively negative examples phrase appears 
category set sparse goal minimize absolute loss master predictions nonlinear function denoted applied weighted sum experts predictions 
monotonically increasing see vovk cesa bianchi classification purposes simply weighted sum predictions 
acm transactions information systems vol 
april 
cohen singer grams construct set miniexperts 
total sparse phrases miniexperts 
ripper sleeping experts applied separately category built separate pool experts different classification problem 
classify new document pool finds sparse grams appearing document computes weights corresponding miniexperts 
instance classifying documents said soldiers killed ira bombing taxi driver killed ira relevant set phrases include killed ira bombing 
documents classified correctly miniexperts associated phrases total weight miniexperts predicting ireland larger total weight miniexperts predicting ireland 
discussion 
result sleeping experts algorithm large pool sparse phrases associated weight 
testing performance sleeping experts algorithm new unclassified document phrases appear document extracted pool weights normalized sum 
steps training phase updating weights master prediction computed weighted sum active miniexperts 
actual classification done comparing master prediction threshold emphasized update prediction steps depend active experts phrases appearing document 
means run time steps depends size document processed total number experts 
properties algorithm require explanation 
phrase appears documents relevant class considered number times phrase correct approximately equal number times incorrect 
means time weight miniexperts corresponding phrase demoted 
second relevant phrases appear document total weight active miniexperts large promotion correct miniexperts due renormalization small relevant phrases total weight active miniexperts small renormalization results higher promotion correct miniexperts 
clearly desirable property text categorization allows rare highly correlated phrase large influence classification 
analysis general sleeping experts algorithm various settings provided freund 
analysis technique directly assume pool implemented hash table weights updated constant time 
acm transactions information systems vol 
april 
context sensitive learning methods text categorization applied setting 
briefly defines loss master algorithm average loss respect instantaneous distribution defined set active miniexperts example cumulative loss master algorithm bounded relative loss suffered best possible fixed weight vector 
results hold pure line model weights updated round weight update turned assumptions deriving cumulative loss bounds longer hold 
experiments described indicate algorithm performs batch setting weights updated training phase held fixed set test cases classified 
note types experts constructed 
instance expert may record number appearances phrase current document previous documents inverse document frequency mixture poisson models church gale predict classification 
focus article methods exploiting contextual information frequency statistics restrict experts presence absence phrases sparse gram 
enables fairer comparison ripper rules presence absence words documents 
rocchio implemented version rocchio algorithm rocchio adapted text categorization ittner 
represent data training test documents vectors numeric weights 
weight vector mth document vm vl number indexing terms 
single words terms 
follow tf idf weighting salton define weight vk log nd nk vk fj log nd nj nd number documents nk number documents indexing term appears fk fk log number occurrences indexing term document rocchio algorithm prototype produced class prototype represented single vector dimension original weight vectors nd class kth term prototype defined acm transactions information systems vol 
april 
cohen singer def max rc vk rc vk set documents class set documents parameters control relative contribution positive negative examples prototypes vector ittner buckley values 
documents classified converted weight vectors compared prototype computing dot product 
novel document classified member class distance dot product prototype threshold chosen balance recall precision set manner 
experiments chosen minimize error training set minimize total loss training set specific loss ratio 
experiments described compare results previously published results possible 
rocchio additional point supplementing previous results allows compare performance ripper sleep understood existing algorithm precisely experimental conditions 

experimental results ap titles corpus benchmark corpus ap newswire headlines tagged relevant irrelevant topics federal budget ratings data set described detail lewis catlett lewis gale 
corpus contains documents training set documents test set 
headlines average words long total vocabulary words 
preprocessing text done convert words lower case remove punctuation marks 
representative categories 
examples titles corpus table ii 
previous lewis gale lewis catlett domain evaluate performance new sampling method called uncertainty sampling 
lewis catlett error rate estimated number errors test set principle evaluation metric 
subsamples train learning systems entire training set 
discarded categories 
category hard learn learning algorithms generate classifiers extreme recall precision values 
tends distort averages categories 
acm transactions information systems vol 
april 
context sensitive learning methods text categorization table ii 
titles trec ap corpus title class george burns back best seller list bond prices rise bonds war roses wins box office battle place deutsche bank controls percent morgan cheney gets spend budget troops seize major rebel base thai frontier park returns playground status bush leaves anti drug rally bush dollar little changed gold higher adopts bush strategy executive experience counts west german mobile phone license awarded international consortium german paulson says bid ready soon gulf iran said advising friends lebanon release hostages hostages belfast police find arrest ireland mark second anniversary israel won rise japan defense japan bill leads nbc ratings victory tells may star wars stock analysts see consumed crisis yugoslavia snow hits plains weather table iii 
ripper sleeping experts rocchio algorithm ap titles full sample number errors experts domain rocchio ripper words word bonds budget hostages ireland average table iii summarizes performance learning systems task 
performance sleep word phrases phrases provide numbers single word phrases second example linear context insensitive classifier 
average ripper sleeping experts word phrases achieve lower error rates linear classifiers 
ripper achieves lower error rate rocchio better linear classifiers categories higher error rate twice 
word phrases sleeping experts achieves lower error rate roc acm transactions information systems vol 
april 
cohen singer table iv 
additional comparisons full ap titles benchmark measurements averages categories learner number errors recall precision rocchio probabilistic classifier subsample ripper negative tests ripper experts words experts word categories achieves identical error rate 
cases improvement error rate statistically significant tailed paired test 
table iv gives additional points benchmark 
lewis gale measure principle evaluation metric compare results record average measure table widely measurements precision recall 
rows table iv show average performance rocchio algorithm probabilistic classifier lewis gale decision tree learner lewis catlett ripper negative word tests allowed ripper allowing tests form 
sleeping experts single word phrases included additional example linear classifier 
contains special mechanism handle large feature sets encountered ir problems run relatively small subsample examples probably accounts relatively poor performance 
results broader comparison qualitatively 
notice algorithms considered explicitly attempt maximize measure attempt minimize error rate 
ripper sleeping experts phrases outperform linear classifiers respect measure 
fact sleeping experts word phrases best new learning methods respect error rates strictly dominates rocchio performance metrics 
trec ap corpus lewis describe experiments slightly different version ap titles corpus 
data set somewhat smaller categories readily available researchers ripper 
sleeping experts 
measure defined van rijsbergen pp 
precision recall precision recall parameter controls importance precision relative recall value corresponds equal weighting precision recall higher scores indicating better performance 
acm transactions information systems vol 
april 
table ripper sleeping experts rocchio algorithm ap titles trec ap sample number errors experts domain frequency rocchio ripper words word bonds budget bush german gulf hostages ireland israel japan weather yugoslavia average table vi 
additional comparisons trec ap titles benchmark measurements averages categories learner context sensitive learning methods text categorization number errors recall precision rocchio error rocchio widrow hoff ripper experts words experts word subset ap stories trec conferences instance see lewis catlett lewis gale 
final difference split training testing sets done chronologically randomly 
test set drawn somewhat different distribution training set 
ran ripper rocchio algorithm sleeping experts problems 
results shown table 
column labeled frequency discussed section 
results broadly similar performance sleeping experts stronger relative algorithms 
sleeping experts statistically significantly acm transactions information systems vol 
april 
cohen singer better rocchio tailed test ripper outperforms rocchio confidence paired test 
comparison results lewis table vi indicates average performance ripper sleeping experts implementation rocchio algorithm additional linear classifiers tested lewis widrow hoff exponentiated gradient line scheme uses multiplicative updates version rocchio uses threshold chosen optimize measure 
recall implementation rocchio chooses threshold minimize error rate 
reuters corpus experiments 
set experiments conducted reuters data set lewis corpus news stories tagged different topics 
largely followed methodology lewis ringuette 
documents split training test sets described lewis discarding stories test data earlier experiment 
resulted corpus training cases test cases 
words converted lower case punctuation marks removed function words standard list removed 
example article reuters corpus preprocessing 
evaluate performance precision recall 
measurements microaveraged microaveraging total number false positive false negative true positive true negative predictions categories computed totals compute recall precision 
performance summarized break point hypothetical point obtained interpolation precision equals recall 
table vii summarizes microaveraged break points sleeping experts word phrases word phrases single word phrases ripper negative tests rocchio simple decision tree learning system bayesian classifier 
word phrases sleeping experts provided additional improvement word phrases problems 
figures lewis ringuette 
additional point comparison show results duplicating experiments conducted apt corpus 
main difference apt discarded stories tagged categories resulting smaller corpus training sleeping experts ripper 
function words include high frequency words list lewis 
acm transactions information systems vol 
april 
context sensitive learning methods text categorization fig 

example article reuters preprocessing 
cases test cases 
additionally classes considered appearing document reduced corpus 
data set show results ripper swap learning system precise discarded data set documents tagged topics legal 
particular documents tagged topic word bypass included 
note lewis experiments documents treated negative examples categories 
acm transactions information systems vol 
april 
cohen singer apt sleeping experts rocchio 
different numbers swap reflect different methods representing text 
methods document represented relatively small number features depending category corresponds specific word 
generally features corresponded words high mutual information category features numbers indicating frequency word document boolean variables indicating presence word 
discussion 
table vii 
break summaries reuters data set learner options break lewis experts words experts words ripper ripper negative tests decision tree boolean feat 
rocchio experts word prop 
bayes boolean feat 
apt ripper negative tests headlines swap freq 
feat 
headlines ripper negative tests ripper swap freq 
feat 
swap boolean feat 
experts words experts words rocchio experts word previous 
corpus number learning algorithms table vii context 
hypotheses swap instance extremely similar ripper decision trees notion context ripper 
fact decision trees lewis ringuette converted rule set negative tests 
shown table vii learning algorithms context uniformly better 
long history applying learning algorithms context data set contribution experiments article ripper sleeping experts earlier systems direct representation text representing documents internally list tokens algorithms nontrivial process convert original documents feature vectors 
cases process quite expensive terms storage example wiener study neural networks acm transactions information systems vol 
april 
context sensitive learning methods text categorization reuters corpus learn nonlinear classifiers variety representations noted representation schemes required different dimensional vector representation document topic 
clearly direct representation efficient corpora categories 
evidence reducing dimensionality examples may degrade generalization performance 
instance lewis ringuette report reuters problem performance improved monotonically features added decision tree learner lewis ringuette decision tree learner swap limited number features presumably efficiency reasons 
conjecture ability ripper sleeping experts larger vocabulary contributes improved performance 
note ripper sleeping experts reasonably efficient large vocabularies learning time ripper sleeping experts word phrases averages little seconds category larger lewis data set 
special notions context 
swap performance slightly improved special representation frequency associated word adjusted word appeared document specifically frequency counts words appearing title increased double counting occurrence title 
representation microaveraged break point swap improved 
representational trick viewed making notion context context represented headline document special importance 
repeated experiments ripper negative tests apt sample representation document represented set valued features set words appearing story set words appearing headline 
representation allows ripper construct rule sets explicitly require word appear headline context 
surprisingly light previous results representational trick improves performance ripper microaveraged break improves 
times mips irix mhz processors 
different experts representing single word phrases half word phrases word phrases 
word phrases occur corpus contribution classification 
classification keep information experts corresponds phrases appear twice corpus 
hash tables access expert constant time phrase represents key 
scheme amount memory maintaining pool grows linearly total size text fixed gram size 
acm transactions information systems vol 
april 
cohen singer table viii 
break summaries reuters data set learner options break experts words experts words ripper negative tests ripper experts word rocchio modapte experts words experts words ripper negative tests ripper experts word rocchio reuters collection note table vii indicates substantial difference difficulty categories data sets apt lewis ringuette instance microaveraged break point rocchio algorithm apt data sets lewis ringuette data sets 
differences unsurprising light differences number examples number categories particularly categories examples restricted systematically random sampling 
wiener describe series experiments corpus apparently third split training testing data resulting corpus training examples test examples 
means clear data sets contains representative learning problems certain performance comparisons different data sets best approximate 
address problem second version reuters corpus reuters corpus available 
main differences small number duplicate documents removed corpus formatted documented facilitate systematic reproducible experimentation 
different training testing splits corpus proposed modapte split contains training examples test examples split contains training examples test cases 
modapte split appears similar partition wiener 
table viii presents results variant corpora confirm hypothesis context important 
www research att com lewis reuters html 
acm transactions information systems vol 
april 
context sensitive learning methods text categorization fig 

cumulative difference errors trec ap 
analysis experimental results effect class frequency performance 
microaveraged break widely performance test data set known drawbacks metric particular microaveraged measurements tend dominated frequent categories 
macroaveraged measurements complementary disadvantage data sets reuters data sets contain rare categories tend dominated performance rare categories performance hardest estimate test data 
section analyze data somewhat detail order determine circumstances favorable learning algorithms 
reviewing experimental results property appears greatly affect relative performance learning algorithms considered article relative frequency class learned 
relative frequency mean simply fraction times class appears training data 
trec ap data set reuters data sets broad range class frequencies 
general rocchio perform better relative learning algorithms low frequency classes classes fewest positive examples data 
instance trec ap data set categories frequent occurs times entire corpus frequent occurs times 
see table 
ripper sleeping experts word phrases slightly higher acm transactions information systems vol 
april 
cohen singer fig 

cumulative difference errors modapte 
error rate rocchio average frequent classes 
algorithms dominate rocchio frequent classes sleeping experts achieves lower error frequent classes ripper outperforms rocchio frequent classes obtains essentially equivalent performance additional error remaining class 
analyzed effect frequency classes reuters modapte data set observed similar effect 
apparent systematic difference ripper rocchio categories positive examples training set 
frequent classes ripper tends outperform rocchio 
similar behavior observed sleeping experts algorithm rocchio sleeping experts achieved similar error rates positive examples training set 
effect class frequency learners easily seen graphs figures 
category associated training set test set define edge learner relative second learner category number errors test set minus number errors test set words edge relative simply number errors avoided 
graphs figures show frequency cumulative edge ripper sleeping experts relative rocchio analysis measured frequency classes training data 
acm transactions information systems vol 
april 
context sensitive learning methods text categorization problems frequency equal note cumulative edge close zero performance comparable rocchio problems frequency cumulative edge curve tends upward respectively downward indicates learning algorithm generally outperforming rocchio respectively outperformed rocchio respect error rate 
graphs illustrate trends discussed 
cases cumulative edge small relatively low frequency classes begins drift upward 
points upward drift begins different domains clearly time relative frequency reaches range data set examples reuters examples trec ap 
graphs clearly indicate performance competitive frequency range cumulative edge novel algorithms due relatively small number relatively frequent classes 
retrospect competitive performance rocchio rare classes surprising 
rocchio uses inverse document frequency idf weighting scheme classifier weight rare words heavily bias clearly appropriate learning rare class 
historically rocchio successfully improving performance ad hoc queries large database typically small fraction documents database relevant 
course text categorization domains accurate classification frequent classes important example domain categorization user incoming email cohen 
practical settings may priori reasons prefer learning algorithm 
instance rocchio advantage hypotheses easily adapted retrieval statistical ir systems ripper advantage hypotheses easily incorporated boolean retrieval system sleeping experts advantage line algorithm strong performance guarantees 
experiments section suggest additional reason prefer rocchio expectation classes rare conversely prefer ripper sleeping experts classes frequent 
experiments section suggest average case performance ripper sleeping experts improved focusing performance rare classes 
natural extension algorithms described article improve performance rare classes augment features indicating presence absence words additional information inverse document frequency 
instance sleeping experts framework modify experts predictions weighted graph reuters data set plotted log frequency wide range frequencies data 
acm transactions information systems vol 
april 
cohen singer idf weight corresponding phase 
similarly ripper extended prefer tests words high idf weights 
plan explore extensions research 
sensitivity rocchio parameter settings 
experiments reported parameters rocchio chosen experiments performed different researchers different classification task ittner 
performed smaller scale experiments explore sensitivity rocchio parameters degree performance improved parameter tuning 
choose different categories modapte split reuters data set ran rocchio different combinations 
results indicate rocchio sensitive 
chooses category optimal parameter values category parameter settings best test set average error rate reduced factor relative default settings 
suggests performance rocchio improved substantially appropriate parameter tuning 
experiments show finding optimal parameter settings trivial 
instance adopts simple procedure selecting category parameter settings give lowest training set error improvement error rate quite small furthermore error rate higher frequent category weighted heavily microaveraged crossover measure 
error rate category acq raised 
clear different categories behave quite differently different variations rocchio 
implemented variation rocchio prototype vectors built positive examples negative examples new instances assigned category depending distance closest prototypes armstrong pazzani 
method generally competitive traditional version rocchio obtaining average error rate obtained lower error rate frequent category acq versus default version rocchio 
summary suspect automatic tuning methods improve rocchio performance done automatic means category category basis cross validation method estimate test set error different parameter settings categories acq corn grain fx trade ship gnp pet chem selected set discarded certain combinations unreasonable redundant combinations combinations 
case choosing parameter setting lowest training set error simple plausible choice linear models produced rocchio training set error closely correlated test set error 
acm transactions information systems vol 
april 
context sensitive learning methods text categorization incorporating extensions rocchio algorithm dynamic feedback optimization buckley salton 
development automatic methods intriguing direction research plan pursue schapire 
conclude section methodological point 
sensitivity rocchio parameter settings represents opportunity danger 
absence clearly defined preferably automatic parameter tuning mechanism quite possible experimenter unwittingly overfit benchmark problem finding parameter settings perform test set problem perform general discovered test set 
phenomenon possible small set benchmark problems associated ap headline corpus reuters corpora 
reuters categories performance dominated frequent ones 
evaluating learning system methodologically safer parameter settings tuning mechanisms developed problems different ones benchmarks 
article followed procedure possible parameters rocchio set previous text categorization retrieval buckley ittner ripper algorithm developed tuned set classification problems taken uc irvine repository cohen design driven entirely theoretical results 

concluding remarks summarize evaluated new text categorization algorithms 
algorithms allow context word influence presence absence contribute classification 
algorithms different representations classifier different search methods find classifier different notions context 
ripper predictions rules test simultaneous presence absence words sleeping experts predictions sparse phrases 
learners word effect prediction context sensitive sense effect depends remainder document containing word 
ripper instance word appears conjunction 
wk effect predicting class document words appear note wk may occur relative order 
effect word predicting class document depends sparse phrases appears words context depends words appear close relative order nearby words 
acm transactions information systems vol 
april 
cohen singer performed experiments large text categorization benchmarks corpus ap titles corpus news stories 
context sensitive learning algorithms generally performed better algorithms learn context insensitive classifiers 
rule learning results improve previous methods output boolean text classifiers quality results reuters corpus scale problems demonstrated practically solvable ap titles corpus 
ap titles algorithms achieved lower error rates rocchio algorithm case error rates uniformly lower 
complete version reuters corpus algorithms performed better comparable algorithms previously applied corpus 
restricted version corpus algorithms performed better previously applied linear classifier 
view results confirmation usefulness practicality learning classifiers represent contextual information 
possible compared results previous results tasks 
large number relevant studies see instance lewis yang yang chute apt hull wiener cohen schutze ng direct comparison impossible due diversity different data sets experiments different methods preprocess partition data different measures evaluate performance 
difficult predict ripper sleeping experts perform relative previously published algorithms 
note ripper sleeping experts certain properties previously published text categorization algorithms ripper constructs boolean classifier direct representation training corpus sleeping experts strong formal guarantees performance freund strict line model learning 
furthermore ripper sleeping experts statistical assumptions data contrast approaches schutze ripper sleeping experts attempt minimize error directly opposed related measure squared loss yang chute lewis 
article compared evaluated context sensitive learning methods note closing potentially valuable research topic combine different learning methods exploit stronger features 
instance output ripper derive additional set experts corresponding rules ripper builds 
set added existing set sparse phrases sleeping experts allowing learning system model short range contextual information sparse grams long range correlations conjunctions rule set built ripper 
acm transactions information systems vol 
april 
acknowledgments authors david lewis comments draft article help preparing data experiments sigir reviewers number useful suggestions 
marti hearst mehran sahami anonymous reviewers tois constructive comments suggestions 
context sensitive learning methods text categorization almuallim dietterich 
learning irrelevant features 
proceedings th national conference artificial intelligence aaai july dean mckeown eds 
mit press cambridge ma 
apt damerau weiss 
language independent automated learning text categorization models 
proceedings th annual international acm conference research development information retrieval sigir dublin ireland july croft van rijsbergen eds 
springer verlag new york ny 
apt damerau weiss 
automated learning decision rules text categorization 
acm trans 
inf 
syst 
july 
armstrong joachims mitchell 
webwatcher learning apprentice world wide web 
proceedings aaai spring symposium information gathering heterogenous distributed environments stanford ca mar 
aaai press menlo park ca 
blum 
learning boolean functions infinite attribute space 
proceedings nd annual acm symposium theory computing stoc seattle wa may ortiz ed 
acm press new york ny 
blum 
empirical support winnow weighted majority algorithms results calendar scheduling domain 
proceedings th international conference machine learning lake tahoe ca 
brunk pazzani 
noise tolerant relational concept learning algorithms 
proceedings th international workshop machine learning ithaca ny 
morgan kaufmann san mateo california 
buckley salton 
optimization relevance feedback weights 
proceedings th annual international acm sigir conference research development information retrieval sigir seattle wa july fox ingwersen fidel eds 
acm press new york ny 
buckley salton allan 
effect adding relevance information relevance feedback environment 
proceedings th annual international acm conference research development information retrieval sigir dublin ireland july croft van rijsbergen eds 
springer verlag new york ny 
cesa bianchi freund helmbold haussler schapire warmuth 
expert advice 
proceedings th annual acm symposium theory computing stoc san diego ca may kosaraju johnson aggarwal eds 
acm press new york ny 
church gale 
poisson mixtures 
nat 
lang 
eng 

cohen 
efficient pruning methods separate conquer rule learning systems 
proceedings th international joint conference artificial intelligence chambery france 
cohen 
fast effective rule induction 
proceedings th international conference machine learning lake tahoe ca 
cohen 
text categorization relational learning 
proceedings th international conference machine learning lake tahoe ca 
acm transactions information systems vol 
april 
cohen singer cohen 
learning rules classify mail 
proceedings aaai spring symposium machine learning information access palo alto ca 
aaai press menlo park ca 
cohen 
learning set valued features 
proceedings th national conference artificial intelligence portland 
cohen singer 
learning query web 
proceedings aaai workshop internet information systems 
aaai press menlo park ca 
freund schapire 
decision theoretic generalization line learning application boosting 
proceedings nd european conference computational learning theory 
springer verlag berlin germany 
freund schapire singer warmuth 
combining predictors specialize 
proceedings th annual acm symposium theory computing 
acm press new york ny 
rnkranz widmer 
incremental reduced error pruning 
proceedings th annual conference machine learning new brunswick nj 
morgan kaufmann publishers san francisco ca 
hull grefenstette 
stemming algorithms case study detailed evaluation 
am 
soc 
inf 
sci 

hull pedersen schutze 
method combination document filtering 
proceedings th annual international acm sigir conference research development information retrieval sigir seattle wa july fox ingwersen fidel eds 
acm press new york ny 
ittner lewis ahn 
text categorization low quality images 
symposium document analysis information retrieval las vegas nv 

john kohavi 
irrelevant features subset selection problem 
proceedings th annual conference machine learning new brunswick nj 
morgan kaufmann publishers san francisco ca 
kivinen warmuth 
exponentiated gradient versus gradient descent linear predictors 
tech 
rep ucsc crl 
computer research laboratory university california santa cruz ca 
lewis 
evaluation phrasal clustered representations text categorization task 
proceedings th annual international acm conference research development information retrieval sigir copenhagen denmark june belkin ingwersen pejtersen fox eds 
acm press new york ny 
lewis 
representation learning information retrieval 
ph dissertation 
department computer science university massachusetts amherst ma 
lewis catlett 
heterogeneous uncertainty sampling supervised learning 
proceedings th annual conference machine learning new brunswick nj 
morgan kaufmann publishers san francisco ca 
lewis gale 
training text classifiers uncertainty sampling 
proceedings th annual international acm conference research development information retrieval sigir dublin ireland july croft van rijsbergen eds 
springer verlag new york ny 
lewis ringuette 
comparison learning algorithms text categorization 
symposium document analysis information retrieval las vegas nv 
lewis schapire callan papka 
training algorithms linear classifiers 
proceedings th annual acm international sigir conference research development information retrieval zurich switzerland 
acm press new york ny 
littlestone 
learning quickly irrelevant attributes abound new algorithm 
mach 
learn 

littlestone warmuth 
weighted majority algorithm 
inf 
comput 
feb 
acm transactions information systems vol 
april 
context sensitive learning methods text categorization ng gog low 
feature selection perceptron learning usability case study text categorization 
proceedings th annual international acm sigir conference research development information retrieval 
acm press new york ny 
pagallo haussler 
boolean feature discovery empirical learning 
mach 
learn 
mar 
pazzani nguyen 
learning www information filtering seeking agent 
proceedings ai tools conference washington dc 
langley 
learning logical definitions relations 
mach 
learn 
aug 
quinlan 
mdl categorical theories continued 
proceedings th international conference machine learning lake tahoe ca 
rocchio 
relevance feedback information retrieval 
smart retrieval system experiments automatic document processing salton ed 
prentice hall englewood cliffs nj 
salton 
developments automatic text retrieval 
science 
schapire singer singhal 
boosting rocchio applied text filtering 
proceedings st annual acm international conference research development information retrieval 
acm press new york ny 
schutze hull pedersen 
comparison classifiers document representations routing problem 
proceedings th annual acm international sigir conference research development information retrieval zurich switzerland 
acm press new york ny 
van rijsbergen 
information retrieval 
nd ed 
butterworths london uk 
vovk 
aggregating strategies 
proceedings rd annual workshop computational learning theory colt rochester ny aug case eds 
morgan kaufmann publishers san francisco ca 
wiener pederson 
neural network approach topic spotting 
symposium document analysis information retrieval las vegas nv 

yang 
expert network effective efficient learning human decisions text categorization retrieval 
proceedings th annual international acm conference research development information retrieval sigir dublin ireland july croft van rijsbergen eds 
springer verlag new york ny 
yang chute 
example mapping method text categorization retrieval 
acm trans 
inf 
syst 
july 
received june revised october april accepted april acm transactions information systems vol 
april 
