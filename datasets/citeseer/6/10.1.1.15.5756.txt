estimating mixtures regressions department mathematical sciences university bath uk ana department statistics universidad aut de madrid spain christian universit paris dauphine crest paris summary 
show bayesian inference switching regression models generalisations achieved specification loss functions overcome label switching problem common mixture models 
derive extension models number components mixture unknown death technique developed stephens 
methods illustrated various real datasets 
keywords bayesian inference birth death process label switching logistic regression loss functions mcmc algorithms poisson regression switching regression 
switching regression model known econometrics literature arises observed quantity depends vector covariates linear way 
vary set possible values probabilities 
words assuming normal distribution perturbation conditional distribution mixture normal distributions yjx 
illustrated simple regression case 
model introduced mainly studied likelihood point view 
appeal clear considering datasets left describes equivalence ratio richness air ethanol mix engine concentration oxide exhaust 
right plots pairs gnp various countries gnp denotes gross national product capita estimated emission capita year 
left graph clearly indicates eu tmr network erb ct computational statistical methods analysis spatial data 
supported crest visit may june pb 
fig 

conditional distributions values regression lines 
oxide 
gnp mex usa jap kor ost nz aus bel cz fin fra deu hun itl hol pol por esp sw ch tur uk rus fig 

left equivalence ratio exhaust oxide concentration source right representation gnp emission levels various countries source oecd 
mixtures regressions di erent oxide concentration dependencies 
data right graph quite spread clear linear trend groups linear model reasonable approximation 
identify groups corresponding linear models interest low gnp countries may help clarify development path embarking 
econometrics chemometrics exist areas heterogeneous covariate dependent populations studied classi cation level identify homogeneous subpopulations inference level estimate corresponding models 
see instance bar shalom ramsey kiefer 
extensions cover time dependent models hidden markov models nonlinear switching regressions example hamilton sto er 

framework wider just simple linear regressions 
instance consider mixtures logistic regressions jx exp exp seen example describing investment choices socio economic covariates mixtures poisson regressions yjx exp illustrated datasets relating numbers accidents covariates 
organised follows section describe bayesian approach models uncover diculties arise processing including speci cities mcmc algorithms settings discussing limitations 
section discuss di erent choices loss functions depending di erent inferential imperatives designed overcome diculties 
section natural extension unknown numbers components birth death process technology normal regression regression models 
illustrate methods simulated real datasets 

bayesian approach section brie review aspects bayesian processing models describe inferential diculties arise result lack identi ability 

prior specification inferential diculties describe section particular speci prior modelling appear generic issue 
choose simulations applications standard conjugate priors independent normal priors regression coecients inverse gamma priors exp distribution dealing normalised dataset dirichlet prior weight vector 
particular choice allows gibbs sampler choices equally acceptable implemented metropolis hastings algorithm described paragraph 
notice rst improper priors standard priors acceptable setup lead unde ned posterior distributions sample size see robert secondly exchangeable priors components priors invariant permutations component labels produce posterior distributions identical marginal distributions components 
feature root inferential diculties discussed section 

mcmc algorithms switching regressions 
gibbs sampling gibbs sampler commonly approach mixture estimation diebolt robert consists augmenting sample 
yn arti cial allocation variables order de mix observed sample 
allows simulation parameters component conditionally allocations account observations allocated component 
instance mixture simple regressions conditional full distributions involved case simply normal gamma logistic regressions second level completion helpful simulate full conditionals observations grouped allocations densities form exp exp directly tractable simulation distribution done slice sampler see damien robert casella leading truncated normal proposals 
case poisson regressions posterior distribution directly exp 
similar slice sampling solution applies 

random walk metropolis hastings algorithms gibbs sampler plus possibly slice sampler natural necessary 
metropolis hastings algorithm properly calibrated cauchy perturbation may types regression models considered see robert casella details 
unconstrained parameterisation log transform variances scale parameters logit transform weights proposal multivariate cauchy calibrated mixtures regressions burn step lead acceptance rate 
logistic models standard random walk proposal mix faster slice sampler see examples observed phenomenon poisson regression 
addition programming ort associated metropolis hastings solution lower gibbs sampler programme applies types regression likelihood prior properly modi ed robert 

inferential difficulties left presents metropolis hastings output projection mcmc sample plane regression coecients simple linear regression ethanol dataset left 
clusters corresponding components clearly identi ed lead regression lines model clear separation components data 
unfortunately ideal situation rarely encountered general scenario closer picture right regression coecients components associated dataset take values area space 
clearly components sequence iterations separated iteration 
phenomenon called label switching known follows lack identi ability mixture models see celeux 
fact mentioned section posterior distribution invariant permutation labels mixture model 
means component mixture individually extracted likelihood component labels distinguished identi ability constraint imposed priori see discussion richardson green di erent perspectives identifying constraints 
prior implies exchangeability parameters symmetry posterior distribution terms labelling parameters number regression lines implies corresponding marginal distributions equal 
consequently posterior marginal means jx equal provide sensible estimators regression lines 
obviously settings di erent modes posterior distribution separated mcmc sampler visits modes label switching occur shown left 
cases sensus mcmc sampler function correctly ergodic averages provide acceptable answers shown left dataset component mixture modelling 
celeux 
considered problem cases normal exponential mixture distributions 
showed solution identi ability constraints plug estimates mixture parameters ordered mcmc samples richardson green provide sensible solutions 
true case regression mcmc samples separated components 
left gives answers unordered ordered mcmc samplers right corresponds simulated dataset observations component mixture upper left hand corner graph shows graphs corresponding orderings di er standard averages happen closer true regression lines 
see beta beta theta gibbs sampler observations fig 

left output random walk metropolis hastings sampler projected space ethanol dataset right output gibbs sampler projected space dataset component mixture model 
graphs mcmc output component mixture represented di erent symbol colour 
average gnp beta order gnp beta order gnp 
true lines 
averaged lines 
theta order lines 
theta order lines random walk observations fig 

left estimated regressions lines dataset component mixture model random walk metropolis hastings algorithm leftmost graph averages component component central graph averages reordering rightmost graph averages reordering right graph simulated dataset observations component mixture upper left corner random walk metropolis hastings algorithm 
mixtures regressions 


fig 

left sampling representation posterior distribution regression lines ethanol dataset component mixture model obtained selecting points random lines iterations right graph dataset component mixture model 
left plot corresponding mcmc samples 

information content mcmc output statement problem imply inference case mcmc output contains sucient information regression lines parameters model 
illustrated gives sampling representation posterior distribution regression lines datasets section 
approximation obtained way iteration mcmc sampler selected random range min xmax points represented graph component chosen probability different colour associated component right shows strong mixing colours means mcmc approximations posterior expectations jx perform poorly 
exhibits equally strong stability location points means main regression lines located sight inference meaningful setting 
regard little di erence left right graphs mcmc parameter samples dissimilar possible extremely mixed mixing labels 
notice natural byproduct sampling graphical device provide evaluation variation regression lines monte carlo con dence band selecting points closest lines 
inferential diculty lies formalising derivation representative regressions lines gures right 
main goal solutions problem appropriate loss functions 

loss inference unreliability ad hoc average ordered estimates concentrate speci cation loss functions various inferential questions arise setting 
celeux 
loss functions rely labeling components ected lack identi ability 
corresponding bayes estimator arg min jx equally una ected label indexes 
see stephens cluster approach mixture setting 
choices possible nd explicitely 
large class losses computationally feasible step approach rue rst step mcmc ergodic averages approximate jx 
second step perform optimisation estimated expected losses 
order approach computationally viable practical restriction loss function possible separate terms sense evaluations estimated jx respect di erent values performed single common mcmc sample posterior jx 
restrict loss functions requirement holds explain estimate obtained estimation problems consider 

estimating mixture regression lines natural choice loss function considers di erence density functions parameters parameters 
set regression lines de nes normal mixture density particular value illustrated 
assumptions distribution regressors natural solution advocated robert sum distances conditional distributions 
jx 
jx observed values 
density di erence kullback leibler distance log log dy denotes conditional density component regression mixture regressor variables omitted ease notation 
notice loss function involve allocation variables appear completed density gibbs sampler 
case qualitative regression models di erence kullback leibler distance modi ed yjx log yjx yjx yjx log yjx account discrete nature observation mixtures regressions theta theta random walk observations 
true lines 
loss estimated lines gibbs sampler observations fig 

left random walk mcmc sample associated simulated dataset observations space component mixture model right corresponding estimates regression lines obtained loss function right true lines left dataset represented plot 
notice choice kullback leibler distance defended reasons bernardo smith reasons ranging information theory asymptotics including fact loss function invariant distance instance 
additional incentive chosing distance invariant relabelling 
lends step estimation maximisation approach decompose posterior expected loss follows jx log log dy jx log log jx log jx log dy assuming order integration may interchanged 
considering terms right hand side equation individually rst seen irrelevant purposes depend 
second fourth terms involve posterior expectation propose markov chain monte carlo step estimate values series grids summation 
numerical integration second third fourth terms grids evaluate estimated expected posterior loss constant independent value evaluation requires numerical integrations 
optimisation step performed explicitely resort simulated annealing celeux 

computational burden grows number distinct observed values feasible examples 
rst illustration depicts mcmc sample corresponding simulated dataset right plus comparison estimated true regression lines 
seen right particularly left shows mcmc clusters hardly overlap 
gnp 
random walk observations 
fig 

left comparisons loss estimates component mixtures dataset random walk mcmc sample right repartition derived sample regression lines component mixture model 
moving dataset introduced left compares estimators obtained tting component component model data 
component model satisfactory sense corresponds posterior spread regression lines right obtained sampling device explained regression lines proposed component case identical 
estimated weight third component equal 

estimating clusters applications primary information interest regression lines clusters groupings allocations observations 
need loss function arbitrariness component labels immaterial 
recalling notation denotes observation allocated line suggest function compares con gurations noting pairs observations allocated component set allocations di erent components set ii ii ii ii posterior expectation ii posterior probability event zjx zjx ii ii zjx automatically generated part gibbs sampler steps straightforward add extra data augmentation step samplers stability properties mcmc chains 
notice data augmentation step fairly generic costless opposed gibbs parameter step involves speci programming 
programs available request authors 
mixtures regressions 


fig 

left estimated regression lines dataset fan chen component model loss function right colour representation allocations observations components blue green loss function 
consider illustration loss functions far dataset fan chen relates daily measurements nitrogen hong kong number daily hospital admissions circulation respiration problems 
components mcmc clusters clearly separated posterior distribution regression lines gives distinct bands 
loss analysis mcmc sample agreement empirical analysis estimated lines right correspond bands dataset neatly separated groups shown left division close average regression lines 
notice estimated weights components respectively 
fact regression line quite interesting suggests days level directly ect hospital admissions 

estimating lines plus allocated observations scenario question interest identify lines groups observations belong 
mean wish state subsets observations associated regression line parameters notice scenario redundancy estimating allocations observation weights associated line 
consider allocations parameters di erent regressions 
parameter interest de ned set 
propose form kullback leibler distance loss function considering allocation observation say density question regression parameters mixture regression 
di erence densities illustrated comparing figures showing observation fig 

component labelled regression mixture conditional distributions yjx full regression mixture normal densities corresponding th regression labelled loss function log log dy denotes density regression line corresponding allocation corresponding vocabulary em algorithm completed likelihood observed likelihood 
employ markov chain monte carlo ideas section estimate expected posterior values grid numerical integration simulation optimisation step 

unknown numbers components degree heterogeneity data unknown case dataset section unreasonable postulate xed number components mixtures estimated 
bayesian point view implies prior distribution illustration purposes simplicity sake choose poisson distribution prior examples 
complex estimation problem addressed standard mixtures reversible jump techniques richardson green opt alternative proposed stephens birth death processes 
approaches equally valid theoretical grounds satisfy required detailed balance conditions choice solely practical considerations birth death process avoids speci cation calibrated moves splits merges richardson green corresponding computation jacobians transforms plus allows easier changes prior distributions parameterisation 
relevant point implementation birth procedure completely free mcmc algorithm chosen xed steps 
fact works supplementary mcmc algorithm 
property allows modular programming implementing method improves portability 
notice reversible jump algorithm restricted birth death moves birth proposals prior distribution enjoy similar properties 
see connections approaches 
mixtures regressions 
run till 
compute yj 

log 
probability remove component probability add component prior 
run mcmc fig 

algorithmic representation birth death process 
parameter mean poisson prior parameter dirichlet prior mcmc function generic mcmc algorithm fixed 
birth death processes go details birth death procedure straightforward adaptation stephens algorithm 
gives description method program manner 
method depends single parameter birth rate point process 
practice better mixing associated higher values expected expense running simulations monitoring integer instants 
uence number iterations mcmc sampler run current value step 
small chose experiments 
algorithms perform quite satisfactorily case produces output label switching naturally high meaning component say vacuous number components varies iteration 
inferential issues addressed section reproduced current setup study implementation loss functions case varying notice rst inferential point view little di erence xed setting 
inference conditional argued robert general principle bayesian model choice parameters appearing di erent 


components frequency 

components frequency 

components frequency 

component frequency fig 

left successive values consecutive iterations death algorithm corresponding likelihoods dataset right representation posterior distributions regression lines values models considered separate entities 
analysis value xed case 
obviously running analysis varying leads inference value supported data posterior probabilities predictive plots regression lines approximated xed case 
instance probable value simulated dataset observations 

normal regression dataset shown solution favoured data poisson prior mean 
interesting see birth death process creates mixing ensure moves rate 
see likelihood green closely follows graph notice case stable third line appearing coherent ndings xed 
weight associated third regression line estimated loss section negligible 

logistic regression mixture logistic regression models appropriate analysis dataset institut national de la statistique des etudes survey nancial assets 
dataset individuals relates possession certain nancial assets mixed portfolios actions obligations 
proposed banks socioeconomic covariates selected inactivity dummy variable inactive active age group yearly income fortune variables standardized 
prior ij normal distribution 
right shows histograms trace output gibbs sampler xed number components equal 
demonstrates need loss function processing label switching phenomenon occurs quite intensely demonstrated particular identical histograms 
varies mixing induced birth death algorithm quite high shown left contains sequence mixtures regressions table 
loss estimates logistic coefficients dataset produced procedure probable values numbers occurences corresponding indicated parentheses total number simulations 
successive values parameters 
shapes posterior distributions ij averaging similar obtained right 
notice loglikelihood follows accurately shape curve part iterations may indicate better values due process 
table provides estimates logistic coecient obtained loss function 
shows persistence rst component values negative ect yearly income explained tendency people higher incomes choose risky pro table nancial assets 
similar analysis second component third group younger active people higher incomes assets 
notice component fairly low probability 

poisson regression considering poisson regression mixture setting similar previous paragraph 
rst check performances birth death algorithm dataset analysed relates monthly unemployment rate monthly number accidents thousands michigan 
model expf log denotes number accidents corresponding unemployment rate 
noted possible seasonal ect dependence postulated weights weights beta beta beta beta beta beta beta beta fig 

left plot successive values consecutive iterations death algorithm dataset superposition corresponding value log likelihood right histograms series sample 
prior number components 
result study series produced birth death algorithm composed frequency approximately frequency approximately 
examination log likelihood shows markov chain remain neighbourhood local maximum explores certain extent posterior surface 
standard posterior mean acceptable estimate plug estimate regression function provides reasonable dataset distinguished loss estimate derived 
loss estimate produces second component small weight ignored 
example compared performances versions birth algorithm steps gibbs sampler steps cauchy random walk sampler integer times 
observed sampler higher mixing rate terms exploring parameter space 
true dataset 
turn second accident dataset provided institut national de recherche sur les transports leur relates number accidents selected crossroads average trac primary secondary roads connecting crossroads 
expectation poisson distribution writes expf log log 
analysis uses reversible jump techniques probable value comes close second 
mcmc sample conditional accumulates values near case lead higher values likelihood 
component case third component corresponds high risk crossroads increasing risk factor elasticity due secondary road decreasing elasticity primary road component case high realistic trac safety point view 
mixtures regressions fig 

regression curve plug loss estimate poisson modelling michigan accident dataset 
table 
estimated parameters model loss function probable values accident dataset numbers occurences corresponding indicated parentheses total number simulations 
number components left gives contour plot posterior expectations mixtures averages depend labelling obtained averaging mcmc iterations producing expected increase number accidents right inserts average predictive distribution number accidents observed couple wider spread larger values gure convincingly illustrates potential mcmc sample providing various interpretations estimated model 
represents loss estimates table contour lines derived plug expectations colour allocations observations 
notice di erent contour systems relate average sharp decrease components 

decision theory criticized practical basis ground actual decision makers build realistic loss functions demonstrated ts 
tp fig 

left contour plot posterior expectations exp log ts log tp accident dataset right corresponding average predictive distribution observed ts tp 
ts tp ts tp ts tp ts ts ts ts fig 

left contour representation estimated model conditional loss function 
numbers accidents represented colour contours component allocated right representation 
mixtures regressions formal loss functions kullback leibler functional distance produce valuable answers non identi able setting mixtures regressions standard empirical measures posterior mean 
simulation steps utilised produce sample posterior distribution standard aware previous uses metropolis hastings procedures settings computational part invoked minimisation posterior loss intense feasible 
shown inference drawn number components mixture coupling simple procedure stephens mcmc algorithms developped loss derivation estimates applies variable dimension setting conditioning examples demonstrated mixing behaviour samplers satisfactory regressors 
various tools graphical representations introduced wide variety regression problems prior distributions minimal programming cost 
study indicated addition prior distribution ect resulting inference poisson prior provide posterior distribution di ers widely posterior associated poisson prior 
feature expected ill posed nature mixture models 
limited uence predictive distribution sense curves vary priors 
authors thankful participants second tmr workshop crete may comments particular oslo support 
dataset kindly provided denis eres crest dataset val erie crest extremely grateful gave permission second dataset commented previous version 
discussions wells cornell suggestions associate editor referees equally helpful 

contributions la th eorie des algorithmes mcmc 
ph thesis universit de 
bernardo smith 
bayesian theory 
wiley new york 
robert 
bayesian estimation switching arma models 
econometrics 
bar shalom 
tracking methods multi target environment 
ieee trans 
automatic control 

accidents en sur routes mod du nombre accidents pr sur un applications 
rapport 
robert 
years running 
amer 
statist 
assoc 

robert en 
discrete time continuous time jump processes applications hidden markov models 
tech 
report crest paris 
celeux robert 
computational inferential diculties mixture posterior distributions 
amer 
statist 
assoc 

damien wake eld walker 
gibbs sampling bayesian non conjugate hierarchical models auxiliary variables 
royal statist 
soc 
ser 

diebolt robert 
estimation nite mixture distributions bayesian sampling 
royal statist 
soc 
ser 

robert 
bayesian variable selection qualitative models projections 
proc 
workshop model selection ed discussion 
cnr di bologna 
fan chen 
step local quasi likelihood estimation 
royal statist 
soc 
ser 


markov model switching regression 
econometrics 
hamilton 
new approach economic analysis nonstationary time series business cycle 
econometrica 
tsai 
smoothing parameter selection nonparametric regression improved akaike information criterion 
royal statist 
soc 
ser 


bayesian inference semiparametric regression fourier representation 
royal statist 
soc 
ser 

kiefer 
note switching regression logistic discrimination 
econometrica 

ramsey 
estimating mixtures normal distributions switching regressions discussion 
amer 
statist 
assoc 

richardson green 
bayesian analysis mixtures unknown number components discussion 
royal statist 
soc 
ser 

robert 
inference mixture models 
markov chain monte carlo practice gilks richardson spiegelhalter eds 
chapman hall london 
robert 
discussion bayesian analysis mixtures unknown number components richardson green 
royal statist 
soc 
ser 

robert casella 
monte carlo statistical methods 
springer verlag new york 
rue 
new loss functions bayesian imaging 
amer 
statist 
assoc 

sto er 
dynamic linear models switching 
amer 
statist 
assoc 

stephens 
bayesian methods mixtures normal distributions alternative reversible jump methods 
ann 
statist 

stephens 
dealing label switching mixture models 
royal statist 
soc 
ser 

richardson green 
bayesian estimation poisson mixtures 
nonparametric statist 
appear 
