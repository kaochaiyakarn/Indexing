relevance models topic detection tracking victor lavrenko james allan edward daniel pollard stephen thomas center intelligent information retrieval department science university massachusetts amherst ma extend relevance modeling link detection task topic detection tracking tdt show substantially improves performance 
relevance modeling statistical language modeling technique related query expansion enhance topic model estimate associated news story boosting probability words associated story appear story 
apply relevance modeling tdt extended stories short queries similarity comparison changed modified form kullback leibler 
demonstrate relevance models result substantial improvements language modeling baseline 
show relevance modeling possible choose single parameter cross mode comparisons stories 

topic detection tracking tdt research program investigating methods automatically organizing news stories events discuss 
tdt includes evaluation tasks explores aspect organization splitting continuous stream news stories single topic segmentation gathering stories groups discuss single topic detection identifying onset new topic news story detection exploiting user feedback monitor stream news additional stories specified topic tracking 
tdt evaluation task link detection requires determining randomly selected stories discuss topic 
tasks value link detection component technology address tasks 
example order recognize start fact relationships exist tasks 
tracking task basis tasks new topic candidate story compared prior stories see topic appeared earlier 
similarly tracking stories specified topic done comparing arriving story user supplied list topic stories 
core approaches link detection tdt tasks comparing word overlap stories words common stories topic 
method basis vector space approaches statistical language model techniques 
field information retrieval research focuses techniques selecting words compare weighted best compare sets weighted words 
research idea common tdt expanding words story include strongly related words 
idea increase likelihood stories topic important overlapping words 
vector space models query expansion techniques local context analysis expand list words 
theoretically grounded approaches statistical language modeling implicitly include related words background model part smoothing process 
explore technique expanding set words associated story relevance models theoretical extension statistical language modeling developed task document retrieval 
section outline basic ideas relevance models 
section relevance models adapted tdt tasks stories compared query document comparison method different 
section describes experiments done shows relevance models improve effectiveness link detection task 
section briefly explore expansion process quite different depending stories come single source modality 
section draws speculates area 

relevance models lavrenko croft define relevance model mechanism determines probability observing word document relevant query link detection clear value 
represents class documents relevant query 
difficult aspect relevance models estimating model absence significant amounts data typically system available query set documents relevance judgments 
way address problem assumption absence training data query 
qk approximate 
qk 
probability occurrence query word 
implement ideas relevance models system follows steps query retrieve set highly ranked documents rq 
step yields set documents contain query words 
documents fact weighted probability relevant query rq 
word documents calculate probability word occurs set 
calculate rq value approximate rq language modeling approaches calculated maximum likelihood estimate smoothed background model pml cfw coll size number times word occurs document cfw total number times occurs large background collection coll size total number words background collection 
value number documents include rq determined empirically training data 
experiments reported rq 
values reported 

relevance models tdt relevance model described built small query estimate probability documents relevant query 
tdt link detection task stories compared decide topic 
building relevance model story model intended capture topic stories 
starting entire story short query estimation done slightly differently 
models created decide discuss topic comparing models directly 
helpful modified form kullback divergence needed explore impact different modalities comparisons 
retrieval tasks collection searched contains homogeneous set documents 
tdt collections hand come newswire text speech recognition output closed captioning machine translation combinations 
comparisons modalities significant impact results 
discuss addressed problem 
building topic models order build topic relevance model story start process described 
story 
qk query training stories news 
training story ranked 
qk 
unfortunately probabilities directly generally forced zero terms story large 
see case consider calculate probability 
qk 
qk 
qk denominator 
qk constant different documents picked uniform prior 
probability query document 
qk impact value posterior 
usually assume independence query words calculate 
qk 
qk qi larger terms resulting product small recall qi 
result get floating point underflow importantly probability reasonable value story 

qk driven zero highest ranked story original document 
qk 
happens value relevance modeling disappears stories mixed model 
address problem flatten probability 
qk th root avoids problems listed 
heuristic adjustment hope circumvent relevance models 
measuring topic similarity built models topic need compare models determine chance represent topic 
stories assume relevance models respectively 
parallel information retrieval relevance models calculate possibly average 
models estimated similar amounts data compare models directly 
kullback leibler divergence standard way compare probability distributions defined log kl divergence asymmetric unacceptable link detection metric 
compute symmetric version summing divergence directions 
kullback divergence measure dissimilarity distributions negation quantity measure similarity 
yields reasonable approach problem models ambiguous relevance modeling failed created model looks general english matching little significance 
models general english valuable identical describe topic 
address problem leverage notion query clarity kl divergence distribution general english 
distribution clear focused general english unclear identical general english 
non symmetric version topic similarity measure clarity degree similar increased extent clear model differs general english 
simple algebraic manipulation similarity measure written log ge form bears strong resemblance log likelihood ratio number tdt participants 
note adding clarity resulted denominator plays role similar role idf document retrieval 
get final similarity measure calculate quantity way swapping add 

applying relevance models section evaluate performance relevance models described link detection task tdt 
describe experimental setup evaluation methodology 
provide empirical support choice parameters system 
show relevance models significantly outperform simple language models heuristic techniques 
experimental setup dataset experiments performed month subset tdt dataset 
corpus contains news stories totaling words 
news stories collected different sources newswire sources associated press new york times radio sources voice america public radio international television sources cnn abc 
stories cover january april 
radio television sources manually transcribed quality 
pre processing stage stories stemmed words inquery stoplist removed 
topics tdt concerned detecting organizing topics news stories 
human annotators identified total topics tdt dataset ranging asian financial crisis monica scandal execution tucker 
topic centered specific event occurs specific place time specific people involved 
topics sufficiently represented months form basis evaluation 
evaluation paradigm system evaluated terms ability detect pairs stories discuss topic 
total story pairs drawn dataset official tdt sampling 
manually judged target discussing topic judged target discussing different topics 
evaluation link detection system emits decision story pair 
system emits target pair get false alarm error system emits target pair get error 
system correct 
link detection evaluated terms decision cost weighted sum probabilities getting false alarm cost cf current evaluations link detection typically set cf 
note answering system misses cost similarly answering guarantees cost 
penalize systems doing better simple strategy cost normalized dividing minimum values 
normalized cost value near reflects system marginal value 
operational link detection system requires threshold selection strategy making decisions 
research setting common practice ignore line threshold selection perform evaluations threshold gives best possible cost 
experiments report minimum normalized detection cost 
value clarity adjusted kl section describe clarity adjusted kl divergence provide argument believe measure may perform better straight kl divergence 
evaluate value clarity adjustment perform simple experiment constructing relevance models 
pair stories construct maximum likelihood language models story smooth background model described equation measure divergence 
consider different divergence measures 
simple kl divergence kl 
symmetric version kl 
clarity adjusted clarity 
symmetric shows minimum detection cost measures function smoothing parameter equation 
observe clarity adjusted kl leads significantly lower errors values 
clarity adjustment leads smaller dependency tuning easier 
note simple adjusted kl get significantly better performance symmetric divergence 
best performance achieved symmetric version clarity adjusted kl 
min 
detection cost effect clarity adjustment kl divergence kl symmetric kl kl clarity sym 
kl clarity smoothing parameter clarity adjustment leads significantly lower error rates 
symmetric versions kl perform better asymmetric versions 
symmetric kl clarity best stable respect smoothing parameter 
performance baseline comparisons relevance models 
baseline competitive state art results reported official tdt evaluations 
relevance model performance compare baseline performance achieve relevance models 
pair stories construct relevance model story described sections 
efficiency purposes story reduced words highest frequencies words query retrieve set related stories rq 
set rq limited contain documents 
relevance model constructed set documents equation 
symmetric clarity adjusted kl measure divergence resulting relevance models 
smoothing parameter set 
empirical justifications parameter settings described section 
shows detection error tradeoff det curve performance relevance models compared best language modeling baseline 
det curve plot false alarm probabilities function sliding threshold 
point curve marks optimal threshold corresponding minimum cost 
note values plotted gaussian scale axes go false alarm full range det curves 
observe relevance modeling system noticeably outperforms baseline threshold settings 
improvements particularly dramatic optimal threshold 
minimum cost reduced 
outside displayed region high precision relevance modeling system noticeably outperforms baseline 
high recall baseline performs somewhat better 
results achieved careful tuning parameters month subset tdt corpus represent blind evaluation 
parameters official tdt blind eval probability detection error tradeoff best language model min cost best relevance model min cost false alarms probability relevance models noticeably outperform baseline threshold settings region interest 
minimum detection cost reduced uation month tdt corpus 
results case comparable described see details 
system relevance models significantly outperformed state art vector space system cosine okapi tf idf weighting 
normalized minimum cost reduced 
suggests parameter settings generalized reasonably new dataset new set topics 
parameter selection section provide empirical justification values parameters estimation relevance models 
explore parameters number words represent document size retrieved set rq smoothing parameter 
query size efficiency issues prompted represent document words highest frequencies document 
ideally able words document running document query computationally expensive running word query quite feasible 
show words reasonable value look resulting precision set rq different sizes rq 
shows precision function number words query 
precision proportion documents rq discuss topic query document 
show precision sets rq size 
observe cases precision improve significantly words 
fewer words precision degrades noticeably 
remaining experiments word queries 
note stopwords removed documents prior selecting high frequency words 
size retrieved set theory estimating relevance model involves computing equation documents dataset zero due smoothing 
practice expensive unnecessary 
vast majority words represent document lead improved precision set rq 
result consistent various sizes rq 
documents close zero including summation little effect 
reason consider limiting rq documents highest 
shows relevance models performance function smoothing parameter fixed 
observe documents highest results performance 
fewer documents adverse effects 
settled top ranked documents 
smoothing smoothing critical component system statistical language modeling 
numerous studies shown smoothing strong impact performance information retrieval systems :10.1.1.136.8113
section observed smoothing equation strong effect performance maximum likelihood models 
interestingly smoothing unusual effect relevance models 
obtained best performance close smoothing document models 
result somewhat unexpected deserves brief explanation 
zhai lafferty smoothing plays dual role applications statistical language models information retrieval 
smoothing ensures non zero probabilities word document model acts variance reduction technique 
second smoothing idf effect document scoring 
roles captured different mechanisms model 
estimate relevance model story mix neighboring document models equation 
results non zero probabilities words occur original story value smoothing avoid zeros 
mentioned section clarity adjustment kl divergence effect similar idf 
cross modal evaluation essential part tdt able deal mul documents rq results consistently performance 
tiple sources news 
tdt corpus experiments includes news different sources 
associated press new york times printed sources represent broadcast news transcribed audio signal 
spoken text different properties compared written sources part tdt challenge development algorithms cope source related differences reporting 
determine algorithms perform different source conditions partitioned set pairs subsets 
pairs stories come broadcast source set labeled bn broadcast news 
pairs stories come printed source set labeled nw newswire 
pairs story broadcast source printed source set labeled shows performance baseline relevance modeling systems subsets described 
performance shown function smoothing parameter 
observe performance varies significantly subset 
interestingly systems perform best condition intuitively appears challenging dealing different language styles 
interesting issue value gives best performance 
note baseline system optimal value different condition bn optimized near nw near optimal near 
means baseline system select single value sources 
contrast relevance modeling system conditions optimized set value close 
encouraging result shows relevance models sensitive source conditions 
min 
detection cost performance maximum likelihood models bn broadcast news nw newswire smoothing parameter min 
detection cost performance relevance models smoothing parameter performance different source conditions 
left baseline optimal smoothing value different condition 
right relevance models conditions optimized approaches 
probability detection error tradeoff best language model best relevance model random performance false alarms probability probability detection error tradeoff best tf idf cosine best relevance model random performance bn nw false alarms probability performance relevance models training data 
relevance models optimal parameters outperform optimal language modeling system left optimal vector space system cosine okapi term weighting right 
minimum detection cost reduced respectively shown 
performance relevance models official tdt evaluation 
parameters tuned training dataset part evaluation dataset prior evaluation 
relevance models right consistently outperform vector space model left 
minimum detection cost reduced 

shown relevance model technique extended tdt link detection task 
models calculated differently avoid problem small probabilities due large queries 
preferable compare models directly version kl divergence incorporates model clarity close model general english 
demonstrated substantial performance improvement relevance models 
parameter selection problem shown somewhat simpler cost values sensitive parameter changes relevance models 
effect easier parameter selection carried problem cross mode comparisons stories 
relevance models different choices similar cost values improving error tradeoffs 

course encountered number interesting questions hope answer research 
surprised gain achieved clarity adjustment straight kl divergence investigate theoretical implications remarkably performance 
second satisfied heuristic nature flattening posterior probabilities section investigating formal approaches 
extended number important directions 
dictated multi lingual nature tdt link detection system capable dealing stories multiple languages 
actively investigating techniques estimating multi lingual relevance models language models contain mixture english non english words 
interested extending framework relevance models case stories discuss multiple topics 
case multiple relevance models formed story question 
acknowledgments supported part center intelligent information retrieval part national science foundation number eia part sd number 
opinions findings recommendations expressed material authors necessarily reflect sponsors 

allan editor 
topic detection tracking event information organization 
kluwer academic publishers boston 
allan connell croft feng fisher li 
inquery trec 
proceedings text retrieval conference trec pages 
nist 
allan lavrenko jin 
story detection tdt hard 
proceedings conference information knowledge management cikm pages 
allan lavrenko swan 
explorations topic tracking detection 
allan pages 
broglio callan croft 
inquery system overview 
proceedings tipster text program phase pages san francisco 
morgan kaufmann 
chen goodman 
empirical study smoothing techniques language modeling 
proceedings th annual meeting acl 
graff liberman 
corpora topic detection tracking 
allan pages 
croft townsend lavrenko 
relevance feedback personalization language modeling perspective 
proceedings delos nsf workshop personalization recommender systems digital libraries pages 
fiscus doddington 
topic detection tracking evaluation overview 
allan pages 
lafferty zhai 
study smoothing methods language models applied ad hoc information retrieval 
proceedings th annual international acm sigir conference pages 
lavrenko croft 
relevance language models 
proceedings acm sigir conference research information retrieval pages 
leek schwartz 
probabilistic approaches topic detection tracking 
allan pages 
martin doddington kamm 
det curve assessment detection task performance 
eurospeech pages 
nist 
proceedings tdt workshop 
notebook publication participants nov 
ponte 
information retrieval smoothing 
proceedings workshop language modeling information retrieval pages 
ponte croft 
text segmentation topic 
proceedings european conference research advanced technology digital libraries ecdl pages 
schultz liberman 
universal dictionary multi language ir applications 
allan pages 
xu croft 
improving effectiveness retrieval local context analysis 
acm transactions information systems tois 
yamron gillick van 
statistical models topical content 
allan pages 
