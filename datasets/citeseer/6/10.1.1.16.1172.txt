adaptive text mining inferring structure sequences ian witten department computer science university waikato hamilton new zealand 
mail cs waikato ac nz text mining inferring structure sequences representing natural language text may defined process analyzing text extract information useful particular purposes 
hand crafted heuristics common practical approach extracting information text general generalizable approach requires adaptive techniques 
studies way adaptive techniques text compression applied text mining 
develops examples extraction hierarchical phrase structures text identification keyphrases documents locating proper names quantities interest piece text text categorization word segmentation acronym extraction structure recognition 
conclude compression forms sound unifying principle allows text mining problems adaptively 
keywords text mining phrase hierarchies keyphrase extraction generic entity extraction text categorization word segmentation acronym extraction compression algorithms adaptive techniques 
text mining inferring structure sequences representing natural language text may defined process analyzing text extract information useful particular purposes called metadata 
compared kind data stored databases text unstructured amorphous contains information different levels 
motivation trying extract information compelling success partial 
despite fact problems difficult define clearly interest text mining burgeoning perceived enormous potential practical utility 
text compression identifying patterns exploited provide compact representation text 
relatively mature technology offers key insights text mining 
research compression taken pragmatic view files need processed may contain normative approach classical language analysis generally assumes idealized input 
modern compression methods avoid making prior assumptions input adaptive techniques 
practice text particularly text gathered web principal source material today messy useful clues come 
adaptation exactly required deal vagaries text universally encountered real world 
studies way adaptive techniques text discrete algorithms vol 
pp 
hermes science publications discrete algorithms vol 
sion applied text mining 
useful kind pattern concerns repetition words phrases 
called dictionary methods compression capitalize repetitions represent structure terms set substrings text achieve compression replacing fragments text index dictionary 
innovation hierarchical dictionary methods extend dictionary non trivial hierarchical structure inferred input sequence 
fulfilling original purpose forming excellent basis compression hierarchies expose interesting structure text useful supporting information browsing interfaces example 
section describes schemes generating phrase hierarchies operate time linear size input practical large volumes text 
keyphrases important kind metadata documents 
topic search summarize cluster documents 
highly desirable automate keyphrase extraction process small minority documents author assigned keyphrases manual assignment keyphrases existing documents laborious 
appropriate keyphrases selected set repeated phrases mentioned 
order temporarily depart theme text compression section look simple machine learning selection criteria success keyphrase assignment 
returning applications text compression character compression methods offer alternative dictionary compression open door new adaptive techniques text mining 
character language models provide promising way recognize lexical tokens 
business professional documents packed loosely structured information phone fax numbers street addresses email addresses signatures tables contents lists tables figures captions meeting announcements urls 
addition countless domain specific structures isbn numbers stock symbols chemical structures mathematical equations name 
tokens compressed models derived different training data classified model supports economical representation 
look application section 
areas compression text mining text categorization segmentation tokens acronym extraction 
review section concluding speculative material structure recognition 
generating phrase hierarchies dictionary compression methods capitalize repetitions 
simplest form replace subsequent occurrences substring instance 
standard compression methods non hierarchical hierarchical dictionary schemes emerged form grammar text replacing repeated string production rule 
schemes usually operate online making replacement soon repetition detected 
online algorithms process input stream single pass emit compressed output long seen input 
historically virtually adaptive text mining inferring structure sequences compression algorithms online main memory principal limiting factor large scale application string processing algorithms compression 
offline operation permits greater freedom choosing order replacement 
offline algorithms examine input considered fashion raises question seek frequent repetitions long repetitions combination frequency length 
section describes algorithms inferring hierarchies repetitions sequences developed text compression 
surprisingly implemented way operate time linear length input sequence 
severe restriction apart standard compression algorithms produce non hierarchical structure tail recursive hierarchical structure linear time algorithms detecting hierarchical repetition sequences known 
sequitur line technique online operation severely restricts opportunities detecting repetitions alternative proceeding greedy left right manner 
may possible postpone decision making retaining buffer history improve quality rules generated point input processed greedily commitment particular decomposition inherent nature single pass online processing 
sequitur algorithm creates hierarchical dictionary string greedy left right fashion 
builds hierarchy phrases forming new rule existing pairs symbols including non terminal symbols 
rules non productive yield net space saving deleted head replaced symbols comprise right hand side deleted rules 
allows rules concatenate symbols formed 
example string gives rise grammar aa bc surprisingly sequitur operates time linear size input 
proof sketched contains explanation algorithm works 
sequitur operates reading new symbol processing appending top level string examining symbols string 
zero transformations described applied applies grammar 
cycle repeated reading new symbol 
point time algorithm reached particular point input string generated certain set rules 
number rules sum number symbols right hand side rules 
recall top level string represents input read far forms rules grammar begins null right hand side 
initially zero 
discrete algorithms vol 
transformations 
occur new symbol processed third fire applied cycle 

digram comprising symbols matches existing rule grammar 
substitute head rule digram 
decreases remains 

digram comprising symbols occurs right hand side rule 
create new rule substitute head occurrences 
increases remains increases account new rule decreases account substitutions 

rule exists head occurs right hand sides rules 
eliminate rule substituting body head 
decreases decreases single occurrence rule head disappears 
show algorithm operates linear time demonstrate total number rules applied exceed number input symbols 
consider quantity initially negative 
increases input symbol processed easy see decrease rule applied 
number rules applied twice number input symbols 
frequent sequitur processes symbols order appear 
occurring repetition replaced rule second occurring repetition 
online operation required policy relaxed 
raises question exist heuristics selecting substrings replacement yield better compression performance 
obvious possibilities replacing frequent digram replacing longest repetition 
idea forming rule frequently occurring digram substituting head rule digram input string continuing terminating condition met proposed quarter century ago wolff reinvented times 
common repeated digram replaced process continues digram appears 
algorithm operates offline scan entire string making replacement 
wolff algorithm inefficient takes time multiple passes string recalculating digram frequencies scratch time new rule created 
larsson moffat devised clever algorithm dubbed re pair time linear length input string creates just structure rules hierarchy generated giving preference basis frequency 
reduce execution time linear incrementally updating digram counts substitutions priority queue keep track common 
adaptive text mining inferring structure sequences example frequent heuristic operation consider string 
frequent digram ba occurs times 
creating new rule yields grammar ba 
replacing aa gives ba aa grammar eleven symbols including rule symbols 
happens length original string terminator 
longest second heuristic choosing order replacements process longest repetition 
string longest repetition appears twice 
creating new rule gives 
replacing ba yields baba aba ba resulting grammar total twelve symbols 
bentley mcilroy explored longest heuristic long repetitions removed lz pointer style approach invoking gzip compress shorter repetitions 
linear time solution 
suffix trees provide efficient mechanism identifying longest repetitions 
suffix tree longest repetition corresponds deepest internal node measured symbols root 
deepest non terminal traversing tree takes time linear length input correspondence leaf nodes symbols string 
left problems find longest repetitions update tree creating rule 
farach colton nevill manning private communication shown possible build tree update replacement time linear 
tree updated linear amortized time making preliminary pass sorting depths internal nodes 
sorting done linear time radix sort repetition longer symbols 
algorithm relies fact deepest node modified point 
discrete algorithms vol 
discussion interesting compare performance algorithms described sequitur frequent longest 
hard devise short strings outperforms 
practice longest significantly inferior techniques simple artificial sequences number rules produces grows linearly sequence length number rules produced frequent grows logarithmically 
experiments natural language text indicate terms total number symbols resulting grammar crude measure compression frequent outperforms sequitur longest lagging 
applications hierarchical structure inference techniques domains related closely text mining compression 
example hierarchical phrase structures suggest new way approaching problem oneself contents large collection electronic text 
nevill manning hierarchical structure inferred sequitur interactively user 
users select word lexicon collection see phrases appears select see larger phrases appears 
larus gives application program optimization step identify frequently executed sequences instructions paths yield greatest improvement optimized 
martin techniques segment input speech synthesis phonemes attached rules appropriate levels 
extracting keyphrases automatic keyphrase extraction promising area text mining keyphrases important means document summarization clustering topic search 
minority documents author assigned keyphrases manually assigning keyphrases existing documents laborious 
highly desirable automate keyphrase extraction process 
phrase extraction techniques described provide excellent basis selecting candidate keyphrases 
order go decide phrases keyphrases need step outside area compression techniques machine learning 
combined phrase extraction simple procedure naive bayes learning scheme shown perform comparably state art keyphrase extraction 
performance boosted automatically tailoring extraction process particular document collection hand experiments large collection technical reports computer science shown quality extracted keyphrases improves significantly domain specific information exploited 
background adaptive text mining inferring structure sequences solutions proposed generating extracting summary information texts 
specific domain keyphrases fundamentally different approaches keyphrase assignment keyphrase extraction 
machine learning methods require training purposes set documents keyphrases identified 
keyphrase assignment predefined set keyphrases chosen controlled vocabulary 
training data provides keyphrase set documents associated 
keyphrase classifier created training documents ones associated positive examples remainder negative examples 
new document processed classifier assigned keyphrases associated classify positively 
keyphrases assigned ones controlled vocabulary 
contrast keyphrase extraction forms basis method described employs linguistic information retrieval techniques extract phrases new document characterize 
training set tune parameters extraction algorithm phrase new document potential keyphrase 
turney describes system keyphrase extraction genex set parametrized heuristic rules fine tuned genetic algorithm 
genetic algorithm optimizes number correctly identified keyphrases training documents adjusting rules parameters 
turney compares genex straightforward application standard machine learning technique bagged decision trees concludes genex performs better 
shows generalizes collections trained collection journal articles successfully extracts keyphrases collection web pages different topic 
important feature training genex new collection computationally expensive 
keyphrase extraction keyphrase extraction classification task 
phrase document keyphrase problem correctly classify phrases categories 
machine learning provides shelf tools problem 
terminology machine learning phrases document examples learning problem find mapping examples classes keyphrase keyphrase 
learning techniques automatically generate mapping provided set training examples examples class labels assigned 
context keyphrase extraction simply set phrases identified keyphrases 
learning scheme generated mapping training data applied unlabeled data extracting keyphrases new documents 
phrases document equally keyphrases priori 
order facilitate learning process phrases eliminated examples learning scheme 
experimented ways discrete algorithms vol 
doing involving hierarchical phrase extraction algorithms described 
process words case folded stemmed iterated lovins method 
involves classic lovins stemmer discard suffix repeating process stem remains iterating change 
final step preparing phrases learning scheme remove stemmed phrases occur document 
candidate phrases generated text necessary derive selected properties 
machine learning properties called attributes example 
potential attributes immediately spring mind number words phrase number characters position phrase document experiments attributes turned useful discriminating keyphrases non keyphrases 
distance document phrase appearance 
second influential term frequency times inverse document frequency tf idf score phrase 
standard measure information retrieval favors terms occur frequently document term frequency ones occur different documents inverse document frequency grounds common terms poor discriminators 
attributes real numbers 
naive bayes learning method simple quick effective conditions class probabilities attribute assumes attributes statistically independent 
order possible compute conditional probabilities discretize attributes prior applying learning scheme quantizing numeric attributes ranges value resulting new attribute represents range values original numeric attribute 
fayyad irani discretization scheme minimum description length principle suitable purpose 
naive bayes learning scheme simple application bayes formula 
assumes attributes case tf idf distance independent class 
making assumption probability phrase keyphrase discretized tf idf value discretized distance easily computed probability keyphrase tf idf score probability keyphrase distance priori probability phrase keyphrase suitable normalization factor 
probabilities estimated counting number times corresponding event occurs training data 
procedure generate bayes model set training documents keyphrases known example author provided 
resulting model applied straightforward way new document keyphrases extracted 
tf idf scores distance values calculated phrases new document procedure described discretization obtained training documents 
attributes tf idf distance computed knowing phrase keyphrase 
naive bayes model applied phrase giving estimated probability phrase keyphrase 
result list phrases ranked associated probabilities 
highest ranked phrases output user determined parameter 
adaptive text mining inferring structure sequences experimental results evaluated keyphrase extraction method different document collections author assigned keyphrases 
criterion success extent algorithm produces stemmed phrases authors 
method evaluation turney comparing results genex conclude methods perform level 
interesting question keyphrase extraction performance scales amount training data available 
ways quantity available documents influence performance fresh data 
training documents computing discretization attributes tf idf distance corresponding parameters naive bayes model 
essential documents keyphrases assigned learning method needs labeled examples 
second training documents calculate document frequency phrase turn derive tf idf score 
case unlabeled documents appropriate phrase labels 
investigate effects performed experiments large collection computer science technical reports cstr new zealand digital library 
results show keyphrase extraction performance close optimum training documents generating classifier computing global frequencies 
words labeled documents sufficient push performance limit 
see subsection domain specific information exploited learning extracting process larger volumes labeled training documents prove beneficial 
exploiting domain specific information simple modification procedure enables exploit collection specific knowledge likelihood particular phrase keyphrase 
just keep track number times candidate phrase occurs keyphrase training documents information form additional third attribute learning extraction process 
new attribute sense documents keyphrases extracted domain training documents 
biasing extraction algorithm phrases occurred author assigned keyphrases training possibly beneficial effect 
order information provided new attribute necessary re train extraction algorithm keyphrases extracted documents different topic 
training time critical factor 
empirically verified exploiting domain specific information increases number correctly extracted keyphrases performing experiments cstr collection mentioned 
order isolate effect changing discrete algorithms vol 
number documents computing keyphrase separate set documents keyphrase frequency corpus counting number times phrase occurs keyphrase 
attribute improved keyphrase extraction markedly size keyphrase frequency corpus increased zero keyphrase frequency attribute improved markedly increased 
actual set training documents held constant set test documents experiment 
discussion conclude simple algorithm keyphrase extraction filters phrases extracted hierarchical decomposition scheme described section naive bayes machine learning method performs comparably state art 
furthermore performance boosted exploiting domainspecific information likelihood keyphrases 
new algorithm particularly suited making information trained quickly new domain 
experiments large collection computer science technical reports confirm modification significantly improves quality keyphrases extracted 
generic entity extraction return main theme adaptive techniques developed text compression purposes text mining 
section review applications character compression methods 
known ppm text compression scheme order mentioned escape method 
methods results particularly sensitive compression scheme prediction assumed 
named entities defined proper names quantities interest piece text including personal organization location names dates times percentages monetary amounts 
standard approach extracting text manual grammars hand crafted particular data extracted 
commercial text mining software includes ibm intelligent miner text uses specific recognition modules carefully programmed different data types apple data detectors uses language grammars text tokenization tool 
alternative approach generic entity extraction compression training explicit programming detect instances sublanguages running text 
example adaptive text mining inferring structure sequences order assess power language models discriminate tokens experiments conducted information items extracted manually issues page word weekly electronic newsletter 
items kind readers wish take action classified generic types people names dates time periods locations sources journals book series organizations urls email addresses phone numbers fax numbers sums money 
types subjective dates time periods lumped purposes distinguished personal organizational names separated purposes amalgamated 
methodology describe accommodates options particular ontology 
discriminating isolated tokens experiment involved ability discriminate different token types tokens taken isolation 
lists names dates locations issues newsletter input ppm compression scheme separately form compression models 
issue contained tokens unevenly distributed token types 
addition plain text model formed full text issues 
models identify tokens newsletter form part training data basis model compresses 
plain text model principle assigned token compresses better specialized models fact occurred 
tokens test data appeared training data label remainder new 
total identified correctly remaining incorrectly errors new symbols 
old symbols contain line breaks appear training data example test data parallel computing split lines indicated 
items identified correctly 
individual errors easily explained errors 
example place mis identified people names time periods spring mis identified sources confusion newsgroups comp software year people names confused organizational names 
distinguishing tokens context tokens appear text contextual information provides additional cues disambiguating 
identification done conservatively strings plain text misinterpreted tokens strings plain text countless opportunities error 
context helps recognition email addresses particular newsletter discrete algorithms vol 
angle brackets 
conversely identification may misleading context names preceded reduces weight capitalization evidence word capitalization routinely follows period 
second experiment evaluated effect context assuming tokens located test issue task identify types situ 
ifa stretch text identified token appropriate type compress better specialized model token markers coded indicate fact 
investigate tokens data replaced surrogate symbol treated ppm single character different ascii characters 
different surrogate token type 
new model generated modified training data test article compressed model give baseline entropy bits 
token turn taken individually restored test article plain text result give entropy bits 
greater information required represent token certainly exceeds required represent type 
suppose token entropy respect model 
net space saved recognizing token belonging model quantity evaluated model determine classified token best best left plain text 
procedure repeated token 
context taken account error rate token increases 
errors caused failure recognize token different plain text rate actual mis recognitions just mis recognitions identified name 
mark string token requires insertion extra symbols token additional overhead causes noted failures recognize tokens 
tradeoff actual errors failures identify adjusted non zero threshold comparing compression particular token compression characters interpreted plain text 
allows small increase number errors sacrificed larger decrease identification failures 
locating tokens context tokens located considering input interleaved sequence information different sources 
token bracketed token token markers problem correct text inserting markers appropriately 
markers identify type token question token name token written 
token encountered encoder switches compression model appropriate token type initialized null prior context 
token encountered en adaptive text mining inferring structure sequences coder reverts plain text model effect replacing token single symbol representing token type 
algorithm takes string text works optimal sequence models produce placement 
works viterbi style processing input characters build tree path root leaf represents string characters possible interpretation input 
paths alternative output strings token token symbols appear 
entropy path calculated starting root coding symbol path model force symbol reached 
context re initialized unique starting token token encountered appropriate model entered 
encountering token itis encoded context reverts 
causes tree branch insertion token symbols possible token type token symbol currently active token type nesting properly respected 
expand tree list open leaves maintained recording point input string reached entropy value point 
lowest entropy leaf chosen expansion stage 
tree list open leaves pruned grow large quickly 
beam search pruning operations applied remove leaves list prevent corresponding paths growing 
evaluate procedure locating tokens context training data issues newsletter single issue testing 
errors mis recognitions noted identifying tokens context rates respectively occur locating tokens 
inevitably incorrect positive identifications number tokens segment plain text erroneously declared token 
addition tokens suffered incorrect boundary placement algorithm reported token approximately place original boundaries slightly perturbed 
tokens suffered discrepancies errors inadvertently person marked test data 
discussion find initial results encouraging 
ways improved 
amount training data tokens distributed token types small 
data certainly contains markup errors probably rate tokens test file 
mistakes similar categories example fax numbers contained embedded phone numbers distinguished occurrence word fax times confused phone numbers counted error 
mistakes perfectly natural norman name place example 
addition improvements pruning algorithm 
discrete algorithms vol 
text mining tasks character compression applied ways text mining tasks 
examples 
text categorization central feature approach generic entity extraction described previous section basic assumption token identified compressing different models seeing produces fewest bits output 
examine extends text categorization assignment natural language texts predefined categories content 
classified documents define categories build model classify new articles 
text categorization hot topic machine learning 
typical approaches extract features generally words text feature vectors input machine learning scheme learns classify documents 
bag words model neglects word order contextual effects 
raises problems define word numbers non alphabetic strings apply stemming 
different features selection process applied determine important words remainder discarded 
compression offer promising alternative approach categorization potential advantages yields judgement document discard information pre selecting features avoids messy problem defining word boundaries deals uniformly morphological variants words depending model order take account phrasal effects span word boundaries offers uniform way dealing different types documents example files computer system minimizes arbitrary decisions inevitably need taken render learning scheme practical 
performed extensive experiments ppm categorization standard dataset 
best results obtained order values degraded performance cases presumably amount training data available insufficient justify complex models 
benchmark data results reuters collection newswire stories standard testbed evaluation text categorization schemes 
total stories averaging words classified categories 
stories adaptive text mining inferring structure sequences signed multiple categories assigned category 
distribution categories highly skewed largest earnings corporate acquisitions money market grain crude oil trade issues interest shipping wheat corn contain stories average stories 
pairwise discrimination applying straightforward compression methodology problem text categorization quickly yields encouraging results 
class case distinguish documents class documents class form separate models training documents class 
test document different training documents compress model calculate gain symbol compression obtained 
assign document class depending difference positive negative principle compress documents class better similarly 
encouraging results obtained 
building positive negative models extend multiply classified articles decide model belongs particular category independently belongs category 
build positive negative models category articles belong category second 
setting threshold deciding new article fact assigned category presents tradeoff making decision liberally increasing chance article correctly identified increasing number false positives conservatively reducing number false positives expense increased false negatives tradeoff captured standard information retrieval notions precision number articles algorithm correctly assigns category expressed proportion documents assigns category recall number articles algorithm correctly assigns category expressed proportion articles category 
allow comparison results maximize average recall precision called breakeven point basic strategy calculate predicted probability article having classification compare predetermined threshold declare article classification exceeds threshold 
choose threshold individually class maximize average recall precision class 
training data divided new training set validation set ratio 
threshold chosen maximize average recall precision category breakeven point validation set 
maximum utility information available rebuilding positive negative models discrete algorithms vol 
full training data 
additional benefit threshold selection automatically compensates fact positive negative models different amounts training data 
general expects achieve better compression data 
results compared method results reported naive bayes linear support vector machine methods 
compression method outperforms naive bayes largest categories grain exception worse smallest ones 
uniformly inferior support vector method money market exception 
compared compression categorization produces particularly bad results categories wheat corn proper subsets category grain 
articles grain summarize result harvesting grain products example listing obtained crop similar terminology 
consequently model wheat assign high score article grain 
occurrence word wheat notable difference article grain belongs wheat 
presence single word significant effect compression new method performs poorly categories 
support vector machines perform internal feature selection focus single word discriminating feature category 
comparison naive bayes performs badly categories new method mechanism internal feature selection 
modifications initial results obtained quickly encouraging 
attempts improve met failure 
force compression models discriminate successfully similar categories experimented costly approach 
building positive negative model built positive negative models categories 
negative model articles belonging corresponding category occur set positive articles 
classification article assigned category positive model compressed negative models 
results improved slightly categories wheat corn 
support vector method performed far better 
compared standard compression method performance deteriorated categories 
experimented modifications standard procedure produced significant improvement results reported rebuilding models full training data number stories building positive negative models adaptive text mining inferring structure sequences usually far stories available negative priming models fresh reuters data outside training test sets priming models full training data positive negative articles artificially increasing counts priming data training data vice versa quarter original training data validation word model order escaping character model order unseen words 
discussion compared state art machine learning techniques categorizing english text compression method produces inferior results insensitive subtle differences articles belong category 
believe results specific ppm compression scheme 
occurrence single word counts compression scheme fail classify article correctly 
machine learning schemes fare better automatically eliminate irrelevant features 
compared word approaches compression methods avoid ad hoc decisions preparing input text actual learning task 
methods transcend restriction alphabetic text apply arbitrary files 
feature selection essential text categorization tasks incorporated compression methods 
segmentation tokens conventional text categorization just example text mining methods presuppose input divided lexical tokens 
words delimited non alphanumeric characters provide natural tokenization items ordinary text assumption fails particular cases 
example generic tokenization allow date structures jul newsletters section parsed 
general prior segmentation tokens runs risk obscuring information 
simple special case scheme compression entity extraction divide text words training data segmented hand 
excellent testbed research problem segmenting chinese text written spaces word delimiters 
chinese readers accustomed inferring corresponding sequence words read considerable ambiguity placement boundaries resolved process 
interpreting text sequence words necessary information retrieval storage tasks example full text search word compression 
inserting spaces text viewed hidden markov modeling problem 
pair characters lies potential space 
segmentation achieved training character compression model pre segmented text viterbi style algorithm interpolate spaces way maximizes discrete algorithms vol 
text unit new york kent cigarettes stopped cigarette filters 
input 
output unit new york kent cigarettes stopped croc cigarette filters 
table 
segmenting words english text probability text 
non chinese readers illustrate success space insertion method showing application english text table due teahan 
top original text including spaces proper places input segmentation procedure output ppm segmentation method 
experiment ppm trained sample english recall precision space insertion 
corresponding figures method compression techniques respectively result particularly striking ppm trained small fraction amount text scheme 
ppm performs unknown words occur brown corpus correctly segmented table 
errors 
space inserted single word requires bits encode requires bits 
second extra space added reduced number bits required 
existing techniques chinese text segmentation word rely hand crafted segmentation rules 
contrast compression methodology character level models formed adaptively training text 
models rely dictionary fall back general properties language statistics process novel words 
excellent results obtained new scheme 
acronym extraction identifying acronyms documents certainly looking patterns text presents different kind problem 
webster defines acronym word formed letters series words radar radio detecting ranging 
acronyms defined preceding textual explanation webster example 
finding acronyms definitions particular technical document problem previously tackled ad hoc heuristics 
information desired acronyms definitions relational distinguishes text mining problems discussed 
adaptive text mining inferring structure sequences immediately obvious compression assist locating relational information 
language statistics certainly differ acronyms running text higher density capital letters far higher density non initial capital letters 
acronym definitions recognized reliably basis readily distinguished ordinary language letter statistics 
experimented coding potential acronyms respect initial letters neighboring words compression achieved signal occurrence acronym definition 
criterion candidate acronym coded efficiently special model regular text compression scheme 
phrase declared acronym definition discrepancy number bits required code general purpose compressor acronym model exceeds certain threshold 
pre filter data identifying acronym candidates initial decided consider words upper case 
determined windows candidate containing preceding words words 
range covered acronym definitions test data 
compressing acronyms candidate coded group models express acronym terms leading letters words side 
group comprises separate models 
tells acronym precedes follows definition 
second gives distance acronym word definition 
third identifies sequence words text set offsets previous word 
fourth gives number letters taken word 
models order ppm model standard escape mechanism 
compressing acronym candidates respect context legal encodings acronym compared compresses best selected 
comparison compress acronym text model preceding context account 
candidate declared acronym predetermined threshold 
subtracting number bits easily justified ratio fact far better results obtained ratio method 
believe reason linked curious fact standard text model longer acronyms tend compress fewer bits shorter ones 
short acronyms long ones tend pronounced words 
affects choice letters longer acronyms closely resemble natural words 
discrete algorithms vol 
experimental results test ideas conducted experiment sizable sample technical reports calculated recall precision acronym identification 
operating point recall precision curve adjusted varying 
direct comparison acronym extraction methods possible different text corpora scheme performs provides viable basis extracting acronyms definitions plain text 
compared methods reduces need come heuristics deciding accept candidate acronym prior choices deciding code acronyms respect definitions 
structure recognition shown compression useful tool token classification tasks impressive document categorization 
discriminant compression tends weaken size individual items grows single holistic measure may appropriate 
decisions depend occurrence non occurrence special words feature selection essential 
token discrimination different kinds token may distinguishable context occur example author names editor names doubt enjoy identical statistical properties distinguished bibliographic local context 
size individual tokens reduced extending techniques described hierarchically 
allows subtle interactions captured 
names decomposable initial surname email addresses username domain top level domain fax numbers contain embedded phone numbers 
analyzing errors generic entity extraction experiments section refined markup training documents decompositions 
instance name ian 
witten email cs waikato ac nz fax fax term soft parsing denote inference effectively grammar example strings exactly compression methodology 
training models built component structured item item 
example model trained appear training data name model trained patterns followed space followed middle initial followed period space followed surname lower level items middle initial surname treated ppm single character identifies kind token occurs 
test file processed locate tokens context new tags inserted 
algorithm described section accommodates nested tokens modification 
initial results mixed 
errors corrected names adaptive text mining inferring structure sequences confused token types correctly marked problems remain fax phone number mix new ones emerge 
caused pruning strategies due insufficient training data 
despite inconclusive initial results believe soft parsing prove invaluable situations strong hierarchical context tables 
possible technique extended kinds tasks considered 
example mark acronym definition 
webster radar example look acronym series words radar radio detecting ranging 
capture essential feature acronyms word defined built characters definition search algorithm needs extended consider possibility 
text categorization important features highlighted 
word wheat distinguishes articles wheat articles grain category marked training data automatic feature selection process markup inferred test data 
techniques may allow compression generalization tackle problems require feature selection 
text mining burgeoning new area increasingly important 
argued examples compression forms sound unifying principle allows text mining problems adaptively 
word character compression methods applied different kinds text mining tasks 
phrase hierarchies extracted documents developed algorithms inferring hierarchies repetitions sequences proposed text compression 
focused applications phrase hierarchies applied diverse range areas including browsing digital libraries 
extraction different kinds entities text commonly approached hand tailored heuristics 
adaptive methods offer significant advantages construction debugging maintenance 
suffer necessity mark large numbers training documents alleviated priming compression models appropriate data lists names addresses gathered external sources 
kinds text mining problems compression techniques examples word segmentation acronym extraction 
notably text categorization suited holistic approach compression offers 
hierarchical decomposition strengthen context incorporate results automatic feature selection 
adaptive text mining compression techniques infancy 
watch grow 
discrete algorithms vol 
acknowledgments research reported undertaken conjunction 
particular bray john cleary eibe frank stuart gordon paynter craig nevill manning bill teahan wen stuart yeates contributed greatly described 
bell cleary witten 
text compression 
prentice hall englewood cliffs new jersey 
bentley mcilroy 
data compression long common strings proc data compression conference pp 

ieee press los alamitos ca 
rau 
automatic condensation electronic publications sentence selection information processing management vol 
pp 

breiman 
bagging predictors machine learning vol 
pp 

chinchor 
overview muc met proc message understanding conference muc 
cleary witten 
data compression adaptive coding partial string matching ieee trans communications vol 
pp 

dumais platt heckerman sahami 
inductive learning algorithms representations text categorization proceedings th international conference information knowledge management 
fayyad irani 
multi interval discretization attributes classification learning proc international joint conference artifical intelligence pp 

frank paynter witten gutwin nevill manning 
domain specific keyphrase extraction int joint conference artificial intelligence stockholm sweden pp 

frank chiu witten 
text categorization compression models proc data compression conference poster 
ieee press los alamitos ca 
full version available working department computer science university waikato 
grover matheson mikheev 
ttt text tokenization tool www ltg ed ac uk gusfield 
algorithms strings trees sequences 
cambridge university press cambridge uk 
howard 
design analysis efficient lossless data compression systems 
phd thesis brown university 
johnson paice black neal 
application linguistic processing automatic generation document text management vol 
pp 

kupiec pedersen chen 
trainable document summarizer proc acm sigir conference research development information retrieval pp 

acm press 
larsson moffat 
offline dictionary compression proc data compression conference pp 

ieee press los alamitos ca 
larus 
program paths proc 
sigplan conf 
programming languages design implementation may lovins 
development stemming algorithm mechanical translation computational linguistics vol 
pp 

martin 
intelligent speech synthesis sequitur algorithm graphical training server software thesis engineering science university toronto 
nardi miller wright 
collaborative programmable intelligent agents comm acm vol 
pp 

adaptive text mining inferring structure sequences nevill manning witten 
identifying hierarchical structure sequences linear time algorithm artificial intelligence research vol 
pp 

nevill manning witten 
phrase hierarchy inference compression bounded space proc 
data compression conference storer cohn eds los alamitos ca ieee press 

nevill manning witten paynter 
lexically generated subject hierarchies browsing large collections international journal digital libraries vol 
pp 

nevill manning witten 
press online offline heuristics inferring hierarchies repetitions sequences proc 
ieee 
ponte croft 
retargetable word segmentation procedure information retrieval proc document analysis information retrieval las vegas nevada 
teahan 
modelling english text 
phd thesis university waikato nz 
teahan yen mcnab witten 
press compression algorithm chinese word segmentation computational linguistics 

text mining technology turning information knowledge 
ibm white 
turney 
press learning extract keyphrases text information retrieval 
viterbi 
error bounds convolutional codes asymptotically optimum decoding algorithm ieee trans 
information theory pp 
april 
witten bray teahan 
text mining new frontier lossless compression proc data compression conference pp 

ieee press los alamitos ca 
witten moffat bell 
managing gigabytes compressing indexing documents images 
second edition morgan kaufmann san francisco california 
wolff 
algorithm segmentation artificial language analogue british psychology vol 
pp 

yeates bainbridge witten 
compression identify acronyms text proc data compression conference poster 
ieee press los alamitos ca 
full version available working department computer science university waikato 
ziv lempel 
universal algorithm sequential data compression ieee trans information theory vol 
pp 
may ziv lempel 
compression individual sequences variable rate coding ieee trans information theory vol 
pp 
september 
received february 
