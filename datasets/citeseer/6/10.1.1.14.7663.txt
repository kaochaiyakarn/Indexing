similarity methods word sense disambiguation ido dagan dept mathematics computer science bar ilan university ramat gan israel dagan macs 
ac 
il compare similarity esti mation methods back maximum likelihood estimation meth ods pseudo word sense disam task controlled unigram bigram fre quency 
similarity meth ods perform better particular task 
conclude events occur training set major impact similarity estimates 
problem data sparseness affects sta tistical methods natural language process ing 
large training sets tend resent low probability events rare events may appear training corpus 
concentrate problem es probability unseen word pairs pairs occur train ing set 
katz back scheme katz widely bigram language modeling esti mates probability unseen bigram utilizing unigram estimates 
un desirable result assigning unseen bigrams probability uni grams frequency 
class methods brown pereira tishby lee resnik cluster words classes similar words base estimate word pair lillian lee fernando pereira div 
engineering labs research applied sciences mountain ave harvard university murray hill nj usa cambridge ma usa pereira research att 
corn eecs harvard edu probability averaged cooccurrence prob ability classes words long 
word modeled average behavior words may cause word idiosyncrasies ig 
instance word red act generic color word cases distinctive cooccurrence patterns re spect words apple banana 
consider similarity esti mation schemes require building general word classes 
estimates similar words word com evidence provided word weighted function similarity dagan markus markovitch pro pose scheme predicting un seen cooccurrences 
scheme assign probabil ities 
follows focus probabilistic similarity estimation methods 
compared methods cluding dagan pereira lee cooccurrence smoothing method essen steinbiss classical es methods including katz decision task involving unseen pairs direct ob jects verbs unigram frequency eliminated factor 
similarity schemes performed better back ex pected yield accuracy experimental setting 
furthermore scheme total divergence empirical dis tributions average yielded statistically significant improvement error rate smoothing 
investigated effect removing extremely low frequency events train ing set 
contrast back smoothing events dis training little discernible ef fect similarity smoothing methods suf fer noticeable performance degradation singletons events occur exactly omitted 
distributional similarity models wish model conditional probability distri butions arising linguis tic objects typically words certain configura tions 
consider pairs wl vi appropriate sets ily disjoint 
follows subscript th element pair wi conditional probability empirical estimate true probability unknown pair second element element wl denotes probability estimate base language model wl word pair second word 
denotes base estimate unigram probability word similarity language model consists parts scheme deciding word pairs require similarity estimate method combining information simi lar words course function measuring similarity words 
give de tails parts sections 
concerned similarity words 
best knowledge particular distribution dissimilarity function statistical language processing 
function im plicit earlier distributional clustering pereira tishby lee tishby distributional similarity sug yoav freund related results hoeffding probability sample drawn joint distribution 
discounting redistribution data sparseness maximum likelihood estimate mle word pair probabilities un reliable 
mle probability word pair wl conditional appearance word wl simply pml wl wl wl frequency wl training corpus wl frequency wt 
pml zero unseen word pair leads extremely inaccurate estimates word pair probabilities 
previous proposals problem jelinek mercer roukos katz church gale adjust mle total prob ability seen word pairs ing probability mass redistributed unseen pairs 
general ad involves interpolation mle linear combination estimator guaranteed nonzero unseen word pairs discounting reduced mle seen word pairs prob ability mass left reduction model unseen pairs 
discounting approach adopted katz pd wx wl lwl wl pr wl 
pd represents turing dis counted estimate katz seen word pairs pr denotes model probabil ity redistribution unseen word pairs 
wl normalization factor 
dagan pereira lee modify katz formulation writing pr wl enabling similarity estimates unseen word pairs basing estimate pair un frequency 
observe similarity estimates unseen word pairs 
investigate estimates pr wl derived averaging information words distributionally similar wl 
combining evidence similarity models assume word similar word wl yield formation probability unseen word pairs involving wl 
weighted aver age evidence provided similar words weight particular word depends similarity wl 
precisely wl denote function similarity wl wl denote set words similar wl 
general form similarity model consider weighted linear combination predictions similar words wl psim normalization factor 
formula occur wl tends occur words similar wi 
considerable latitude allowed defining set wx evidenced previous put form 
essen steinbiss karov edelman implicitly set wl 
may desirable restrict wl fashion especially large 
instance dagan 
pereira lee closest fewer words dissimilarity wl threshold value tuned experimentally 
directly replace wl back equation psim wl 
variations possible interpolating unigram probability lwl psim lwl determined experimentally dagan pereira lee 
represents ef fect linear combination similarity es back estimate exactly katz back scheme 
focus alternatives consider approach rest pr wl wl 
measures similarity consider word similarity func tions derived automatically statistics training corpus opposed functions derived manually constructed word classes resnik 
similarity functions describe depend just base language model discounted model 
section 
kl divergence kullback leibler kl divergence stan dard information theoretic measure dis similarity probability mass func tions cover thomas 
ap ply conditional distribution wl duced wl words wx lw lwl log wu wx wl defined case wl 
unfortunately general case samples need smoothed estimates redistribute probability mass zero frequency events 
smoothed es wl requires sum expensive large vocabularies consideration 
smoothed denominator distribution set wl lo free parameter 
total divergence average related measure total kl divergence average distribu tions wl wx wl wl shorthand distribution iw ii wl furthermore letting wj lw straightforward show grouping terms ap wi wb log 
wl bounded ranging log smoothed estimates required probability ratios involved 
addi tion calculation wl requires sum ming non zero sparse data computation quite fast 
kl divergence case set wl wl 
li norm norm defined wi wl ip jl 
grouping terms express wi form depending common wl ec ip ec form clear wl equality words lw strictly positive 
require weighting scheme decreasing set wl wl fl fl free 
confusion probability essen steinbiss introduced sion probability estimates probabil ity word substituted word wl pc lwl wl wl measures described wl may necessarily closest word may exist word pc wl pc wl confusion probability computed empirical estimates provided unigram estimates nonzero assume 
fact smoothed estimates katz back scheme problem estimates typically preserve consistency respect marginal estimates bayes rule 
con sistent estimates mle rewrite pc follows lwl jp 
pc form reveals important difference confusion probability func tions described previous sec tions 
functions rate similar wl roughly high wj pc wl greater wj large wj ratio wl large may think exceptional infrequent expect wj large 
summary features measures similarity listed summarized table 
base lm constraints conditions satisfied probability estimates base alternative definitions 
model yielded best experimental results 
language model 
column indicates weight wl associated similarity function depends parameter needs tuned experimentally 
experimental results evaluated similarity measures listed word sense disambiguation task method noun verbs decides verb noun direct object 
measure absolute quality assign ment probabilities case perplexity evaluation relative quality 
able ignore constant factors normalize similar ity measures calculate denominator equation 
task pseudo word sense disambiguation usual word sense disambiguation prob lem method tested ambiguous word context asked identify correct sense word context 
example test instance sentence fragment bank disambiguation method decide bank refers river bank savings bank alternative 
sense disambiguation clearly im portant task presents numerous tal difficulties 
notion sense clearly defined instance dictionaries may provide sense distinctions fine coarse data hand 
needs training data cor rect senses assigned re quire considerable human effort 
circumvent difficulties set pseudo word disambiguation ex periment gale church yarowsky general format follows 
construct list pseudo words combination different words 
word con tributes exactly pseudo word 
replace test set cor responding pseudo word 
example choose create pseudo word words take change test data plans take plans take action take action method tested choose words pseudo word 
data statistical part speech tagger church pattern matching con tools due david yarowsky identify transitive main verbs head nouns corresponding direct objects mil lion words associated press newswire 
selected noun verb pairs frequent nouns corpus 
pairs undoubtedly somewhat noisy er inherent part speech tagging pattern matching 
pairs de rived building base bigram language mod els reserving testing purposes 
similarity measures re quire smoothed language models calculated katz back language model equation pr wl maximum likelihood model pml fur thermore wished investigate katz claim delete singletons word pairs occur training set affecting model performance katz training set contained singletons 
built base language models sum table 
mle katz singletons singletons pairs pairs mle mle ol bo bo ol table base language models wished test effectiveness ing similarity unseen word cooccurrences removed test set verb object pairs name pc range log maxw base lm constraints wx bayes consistency table summary similarity function properties occurred training set resulted unseen pairs occurred multiple times 
unseen pairs divided equal sized parts formed basis cross validation runs ti performance test set sets com set tuning parameters necessary simple grid search 
test pseudo words created pairs verbs similar frequencies control word frequency decision task 
error rate performance metric defined incorrect choices ties size test corpus 
tie occurs words making pseudo word deemed equally 
baseline experiments performances base language models shown table 
mle mle ol error rates exactly cause test sets consist unseen bigrams assigned probability maximum likelihood estimates ties method 
back models bo bo ol perform similarly 
mle mle ol bo bo ol ir table base language model error rates back models consistently formed worse mle models chose tune 
mle models subse quent experiments 
ran com measures unsmoothed data lt norm wx total divergence aver age wx confusion probability pc 
full give de tailed examples showing different neighbor induced different measures omit reasons space 
performance similarity methods shows results test sets mle base language model 
parameter set optimal value corresponding training set 
rand shown comparison purposes sim ply chooses weights wl randomly 
wl set equal vt cases 
similarity methods consistently outperform mle method recall ways error rate katz back method error rate huge margin con clude information word pairs useful unseen pairs unigram fre quency informative 
similarity methods better rand indicates simply combine information words ily quite important take word similarity account 
cases edged methods 
average improvement pc difference signifi cant level paired test 
noted bo data kl divergence performed slightly better norm 
err rates sets language 
ii error rates test set base language model mle 
methods going left right rand pc performances shown settings optimal corresponding training set 
ranged results mle ol case depicted 
see similarity methods achieving far lower error rates mle back rand methods performed best 
singletons omitted difference pc greater average difference significant level paired test 
important observation meth ods including rand effective singletons included base language model case unseen word pairs katz claim singletons safely ig back model hold similarity models 
similarity language models provide appealing approach dealing data sparseness 
described compared performance models classical estimation methods mle method katz back scheme pseudo word disambiguation task 
observed similarity methods perform better unseen word pairs measure 
tt tes test 
model mle ot 


ii 
error rates test set base language model mle ol 
ranged kl divergence average best 
investigated katz claim discard singletons training data re compact language model significant loss performance 
re sults indicate similarity language modeling singletons quite important omission leads significant degradation formance 
acknowledgments alshawi joshua goodman rebecca hwa stuart shieber yoram singer helpful comments discus sions 
part done second authors visiting labs 
material supported part national science foundation 
iri 
second author gratefully acknowledges support na tional science foundation graduate fellowship 
brown peter vincent dellapietra peter desouza jennifer lai robert mercer 

class gram models natural lan guage 
computational linguistics december 
church kenneth 

stochastic parts program noun phrase parser unrestricted text 
proceedings second conference applied natural language processing pages 
church kenneth william gale 

comparison enhanced turing deleted estimation methods estimating proba english bigrams 
computer speech language 
cover thomas joy thomas 

ele ments information theory 
john wiley 
dagan ido fernando pereira lillian lee 

similarity estimation word cooccurrence probabilities 
proceedings nd annual meeting acl pages las cruces nm 
essen ute volker steinbiss 

occurrence smoothing stochastic language modeling 
proceedings icassp volume pages 
gale william kenneth church david yarowsky 

methods word sense disambiguation 
working notes aaai fall symposium series probabilistic ap proaches natural language pages 

population frequencies species estimation population parame ters 
biometrika 
hoeffding 

asymptotically optimal tests distributions 
annals mathematical statistics pages 
jelinek frederick robert mercer salim roukos 

principles lexical language modeling speech recognition 
furui mohan sondhi editors advances speech signal processing 
mercer dekker pages 
karov yael shimon edelman 

learning similarity word sense disambiguation sparse data 
rth workshop large corpora 
katz slava 
estimation probabilities sparse data language model com ponent speech recognizer 
ieee transac tions acoustics speech signal processing assp march 
pereira fernando naftali tishby lillian lee 

distributional clustering english words 
proceedings st annual meeting acl pages columbus oh 
resnik philip 

wordnet distributional analysis class approach lexical discov ery 
aaai workshop statistically natu ral language processing techniques pages july 
hinrich 

context space 
ing notes aaai fall symposium probabilistic approaches natural language 
