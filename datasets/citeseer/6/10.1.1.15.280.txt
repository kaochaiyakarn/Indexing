evolutionary artificial neural networks approach breast cancer diagnosis hussein school computer science university new south wales australian defence force academy campus drive canberra act australia edu au presents evolutionary artificial neural network approach pareto differential evolution algorithm augmented local search prediction breast cancer 
approach named memetic pareto artificial neural network 
artificial neural networks improve medical practitioners diagnosis breast cancer 
abilities approximate nonlinear functions capture complex relationships data instrumental abilities support medical domain 
compare results evolutionary programming approach standard backpropagation show experimentally better generalization lower computational cost 
keywords pareto optimization differential evolution artificial neural networks breast cancer 
economic social values breast cancer diagnosis bcd high 
re sult problem attracted researchers area computational intelligence 
importance achieving highly accurate classification artificial neural networks anns common methods bcd 
research area anns medical purposes specifically bcd center attention years 
unfortunately knowledge type research able enter clinic terms routine replace radiologist 
ascribed number factors 
problem misconception machines replace job radiologist 
imperative clarify ann aim replace radiologist support 
second reason human factor people doubt abilities machine especially comes medical diagnosis 
needs notice rarely case patient asks doctor think cancer 
inclined ask disease asking 
framework ann research aims provide filter distinguishes cases cancer reducing cost medication helping doctors focus real patients 
objective anns support doctors replace 
addition anns provide valuable tool minimize disagreement inconsistencies mammographic interpretation 
know excellent doctor accurate decisions build concept trust rarely need justification doctor decisions 
line thinking anns clinic needed anns ensure accuracy reliability tools bcd 
need form model trust human machines accurate performance machine 
patient diagnosed ann cancer number times decisions confirmed human doctor model trust emerge patient machine 
result anns application medical domain general extensive 
previous research arena undertaken various researchers 
wu ann learn instances containing mammographic features rated 
ann trained backpropagation algorithm bp hidden nodes single output node trained produce benign 
performance anns competitive domain expert considerable amount feature selection performance anns improved significantly outperformed domain expert 
bp undertaken floyd input parameters mass size margin asymmetric density architectural distortion number mor density distribution 
extensive experiments bp limited dataset cases achieved classification accuracy 
suggested fogel results questionable impose significant bias data 
employ test set model may fitted data 
suggested usage bp followed similar bp approach previous different input sets derived group blood tests 
instances inputs ann failed perform 
bp suffers disadvantage easily trapped local minimum 
fogel evolutionary programming approach train ann overcome disadvantage bp 
population networks evolved population generations fore generate potential networks 
approach tested wisconsin dataset obtainable anonymous ftp ice uci edu 
managed achieve significant result test cases correctly classified 
apart trials repeats experiment dependence approach predefined network architecture approach performed considerably compared previous studies 
study setiono rule extraction anns algorithm extract useful rules predict breast cancer wisconsin dataset 
needed train ann bp achieved accuracy level test data approximately 
applying rule extraction technique accuracy extracted rule set change 
setiono feature selection training ann 
new rule sets average accuracy 
improvement compared initial results 
comparable results fogel 
bp ann attempt input features heuristics determine number hidden units hidden units 
instances significant amount feature selection reduced number input features maintaining classification accuracy 
comparison data analysis ann 
ann approach significantly better data en velopment analysis approach improvement classification accuracy 
sum previous methods fogel depended conventional bp algorithm easily trapped local minimum requires extensive computational time find best number hidden units 
approach fogel suc attempt evolutionary computations solving problem 
approach achieved better predictive accuracy suffered high computational cost knowing right number hidden units advance experimenting number different networks find best number hidden units 
results previous studies indicated need approach expensive find right number hidden units interference user accurate fogel 
light objective memetic 
evolutionary algorithms eas augmented local search pareto artificial neural networks algorithm bcd 
approach accurate method fogel lower computational cost 
rest organized follows section background mate covered followed explanation method section 
results discussed section drawn section 
background materials evolutionary artificial neural networks eanns key research area decade 
hand methods techniques developed find better approaches ing artificial neural networks precisely sake multi layer feed forward anns 
hand finding anns architecture debatable issue field anns 
methods network growing cascade correlation net pruning optimal brain damage overcome long process determining network architecture 
methods suffer slow convergence long training time 
addition gradient techniques easily stuck local minimum 
eanns provide successful platform optimizing network performance architecture simultaneously 
unfortunately research undertaken eann literature emphasize trade architecture generalization ability network 
network hidden units may perform better training set may generalize test set 
trade known problem optimization known multi objective optimization problem mop 
mop active area research field operations research decades weighted sum method kuhn tucker 
research mop flour decade massive evolutionary approaches single objective optimization 
applying ea mops provided natural ingredients solving complex mops 
approaches briefly outlined section 
trade network architecture taken number hidden units generalization error eann problem effect mop 
natural raise question applying multi objective approach eann 
section introduce necessary background materials multi objective opti mization anns eanns pareto differential evolution pde approach 
multi objective optimization consider mop model optimize subject vector decision variables 
xn vector objective functions 
fk 

fk functions nonempty set vector represents set constraints 
mops aim find optimal solution optimize 
objective function fi maximization minimization 
loss generality assume objectives minimized clarity purposes 
may note maximization objective transformed minimization multiplying 
define concept non dominated solutions mops need define operators assume vectors define operator iff xi yi xi yi 
iff xi yi xi yi operators seen equal equal operators respec tively vectors 
define concepts local global optimality mops 
definition neighborhood open ball open ball 
neighborhood centered defined euclidean distance 
definition local efficient non inferior pareto optimal solution vector said local efficient solution mop iff positive 
definition global efficient non inferior pareto optimal solution vector said global efficient solution mop iff 
definition local non dominated solution vector said local non dominated solution mop iff projection decision space local efficient solution mop 
definition global non dominated solution vector said global non dominated solution mop iff projection decision space global efficient solution mop 
term non dominated solution shortcut term local non dominated solution 
artificial neural networks may define ann graph set neurons called nodes denotes connections called arcs synapses neurons represents learning rule neurons able adjust strengths interconnections 
neuron receives inputs called activation external source neurons network 
undertakes processing input sends result output 
underlying function neuron called activation function 
activation calculated weighted sum inputs node addition constant value called bias 
bias easily augmented input set considered input 
notations single hidden layer mlp number input hidden units respectively 


pth pattern input feature space dimension total number patterns 
loss generality yo corresponding scalar pattern hypothesis space yo 
wih weights connecting input unit 
hidden unit 
hidden unit output unit assumed respectively 
xp ah ah 
hth hidden unit output correspond ing input pattern ah activation hidden unit 
activation function taken logistic function dz function sharpness steepness taken mentioned 
ao ao network output ao activation output unit corresponding input pattern mlps essence non parametric regression methods approximate underlying func tionality data minimising risk function 
data network risk function approximated empirically remp summing data instances follows remp common loss functions training ann quadratic error function entropy 
loss function quadratic task minimize mean expected square error mse actual observed target values corresponding values predicted network equation 
backpropagation algorithm developed initially werbos independently rumelhart group commonly training network 
bp deploys gradient empirical risk function alter parameter set risk minimum 
bp simple form uses single parameter representing learning rate 
complete description derivations algorithm see example 
algorithm described steps 
termination conditions satisfied input output pairs training set apply steps inject input pattern network ii 
calculate output hidden unit iii 
calculate output yo output unit iv 
output unit calculate ro ro rate change error output unit hidden unit rh rh rate change error hidden unit vi 
update weight network learning rate follows evolutionary artificial neural networks wih wih wih wih decades research eann witnessed period 
yao presents thorough review field just area eann 
may indicate extensive need finding better ways evolve ann 
major advantage evolutionary approach traditional learning algorithms backpropagation bp ability escape local optimum 
advantages include robustness ability adopt changing environment 
literature research eann approaches evolving weights network evolving architecture evolving simultaneously 
eann approach uses binary representation evolve weight matrix real 
obvious advantage binary encoding eann real 
real encoding advantages including compact natural representation 
key problem trapped local minimum bp traditional training algorithms choice correct architecture number hidden nodes connections 
problem tackled evolutionary approach studies 
studies weights architectures evolved simultaneously 
major disadvantage eann approach computationally expensive evolu tionary approach normally slow 
overcome slow convergence evolutionary approach ann hybrid techniques speed convergence augmenting evolutionary algorithms local search technique 
memetic approach bp 
general previous approaches evolve weights networks architec ture 
evolve simultaneously stages 
approaches considers dependency network architecture weights 
objective proposed method train determine number hidden units network simultaneously 
section proposed method introduced 
pareto differential evolution described pareto frontier differential evolution pde algorithm vector optimization problems 
algorithm adaptation original differential evolution de introduced storn price optimization problems continuous domains 
pde method outperformed previous methods benchmark problems 
pde works follows 
create initial population potential solutions random gaussian distribution 

repeat delete dominated solutions repeat select random individual main parent individuals supporting parents 
ii 
probability called crossover probability variable main parent perturbed adding ratio difference values variable supporting parents 
step length parameter generated gaussian distribution 
variable changed 
process represents crossover operator de 
iii 
resultant vector dominates main parent resultant vector placed population omitted 
population size maximum 
termination conditions satisfied algorithm works follows 
assuming variables bounded initial population generated random gaussian distribution mean stan dard deviation 
dominated solutions removed population 
remaining non dominated solutions retained reproduction 
parents selected random main trial solution supporting parents 
child generated parents placed population dominates main parent new selection process takes place 
process continues population completed 
algorithm representation deciding appropriate representation tried choose representation architectures modifications 
chromosome class contains matrix vector 
matrix dimension 
element ij weight connecting unit unit 
input unit 
output unit 
hidden unit 
output unit 
representation characteristics current version easily incorporated algorithm 
allows direct connection input output units allow single output unit representation 

allows recurrent connections output units 
vector dimension binary value indicate hidden unit exists network works switch turn hidden unit 
sum represents actual number hidden units network maximum number hidden units 
representation allows simultaneous training weights network selecting subset hidden units 
methods name indicates proposed method multi objective problem ob minimize error minimize number hidden units 
pareto frontier tradeoff objectives set networks different number hidden units note definition pareto optimal solutions 
algorithm return pareto networks number hidden units 
take place actual number pareto optimal solutions population 
condition de having parents generation parents pareto optimal solutions removed population population re evaluated 
exemplify assume single pareto optimal solution population need 
process simply starts removing pareto optimal solution population finding pareto optimal solutions remainder pop ulation 
solutions dominating rest population added pareto set number pareto solutions set 
proposed method original pde algorithm local search 
bp form memetic approach 
initial investigations algorithm quite slow local search improved performance 
consists steps 
create random initial population potential solutions 
elements weight matrix assigned random values gaussian distribution 
elements binary vector assigned value probability randomly generated number uniform distribution 
repeat evaluate individuals population label non dominated 
number non dominated individuals repeat number non dominated individuals greater equal find non dominated solution labelled 
ii 
label solution non dominated 
delete dominated solutions population 
mark training set validation set bp 
repeat select random individual main parent individuals supporting parents 
ii 
crossover probability uniform child child ih ih ih child ih child ih probability uniform child ho ho ho ih ho child ho ho weight main parent perturbed adding ratio difference values variable supporting parents 
variable changed 
iii 
mutation probability uniform iv 
apply bp child 
child ih child ho child ih mutation rate child ho mutation rate child child child dominates main parent place population 
population size 
termination conditions satisfied go 
may note generation starts instances training set marked validation set bp bp original training set training validation 
termination condition experiments occurs maximum number epochs reached epoch equivalent pass training set 
iteration bp equivalent epoch training set training validation complete pass original training set 
cross validation set select best generalized network bp training 
hidden units inactive hidden units disregarded application bp weights associated connections change 
connections may stages need alter way bp 
case error output level calculated active hidden units propagated back network distributed hidden units 
pareto approach helps determine number hidden units interference user 
alternative method formulating problem single objective weighted sum objectives 
efficient way solve problem 
weighted sum method generate pareto solution time 
second assumes convexity pareto frontier 
third add error number hidden units different dimensionality objectives 
unify dimensionality question determining appropriate values weights remains need run algorithm number times different weights effect equivalent running conventional evolutionary approach varying number hidden units 
sum multiobjective approach provides flexibility reduces user load 
section outline performance breast cancer dataset 
diagnosis breast cancer breast cancer dataset section diagnose cancer breast cancer wisconsin dataset available anonymous ftp ice uci edu 
brief description dataset 
dataset contains patterns attributes viz clump thickness uniformity cell size uniformity cell shape marginal adhesion single cell size bare nuclei chromatin normal 
class output includes classes benign malignant 
original dataset obtained wolberg mangasarian 
experimental setup consistent literature removed sixteen instances missing values dataset construct new dataset instances 
instances new dataset chosen training set remaining test set 
undertakes repeats decided repeats get accurate figures regarding performance 
varied crossover mutation probabilities increment different experiments experiment ran times 
maximum number epochs set maximum number hidden units set population size learning rate bp number epochs bp set 
noticeable point population size needs larger maximum number hidden units maximum number pareto solutions case maximum number hidden units definition pareto optimality 
results average test error selected pareto network smallest training error corresponding number hidden units calculated standard ations shown table 
interesting see small standard deviations test error dataset indicate consistency stability accuracy method 
table average test accuracy standard deviations selected pareto network smallest training error crossover mutation 
average test error breast cancer dataset obtained crossover prob ability mutation probability 
average number hidden units breast cancer dataset obtained crossover probability mutation probability 
average number objective evaluations breast cancer dataset obtained crossover probability mutation probability 
figures average test errors number hidden units number objective evaluations corresponding selected network plotted eleventh mutation ties eleventh crossover probabilities respectively 
best performance occurs small crossover mutation probabilities 
quite important entails building blocks effective better performance occurred maximum crossover probability 
may note crossover de somewhat guided mutation operator 
best result crossover rate achieved mutation rate ranging crossover rate mutation rate achieved slightly better results corresponding mutation rate 
general conclude crossover de sort guided mutation mutation important problem 
summary best performances breast cancer dataset achieved average accuracy average number hidden units 
comparisons discussions summary results table 
comparing fogel achieves slightly better performance test set opposed fogel lower standard deviation compared 
terms computational time achieved significant reduction epochs opposed fogel computational cost needed fogel 
terms resources population smaller fogel 
opposed may indicate capable maintaining diversity mop approach better evolutionary programming fogel approach 
table comparison algorithms 
average test accuracy standard deviations best performance achieved method 
study part analysis results traditional bp algorithm training artificial neural networks 
bp achieved accuracy computational cost epochs 
observe lower computational cost bp accomplished better generalization 
notice optimized architecture improving generalization ability 
terms amount computations far faster bp fogel methods run fixed architecture 
addition total number epochs required small compared corresponding number epochs needed bp 
introduced evolutionary multi objective approach artificial neural net works breast cancer diagnosis 
showed empirically proposed approach better generalization previous approaches lower computational cost 
needed evaluating performance proposed method medical datasets 
author xin yao bob mckay insightful comments discussing initial idea 
show appreciation anonymous reviewers invaluable comments suggestions 
newton 
pareto differential evolution approach vector optimisation problems 
ieee congress evolutionary computation ieee publishing seoul korea 
finn 
net method generating non deterministic dynamic multivariate decision trees 
knowledge information systems 
blake merz 
uci repository machine learning databases www ics uci edu mlearn mlrepository html 
university california irvine dept information computer sciences 
bornholdt 
general asymmetric neural networks structure design genetic algorithms 
neural networks 
fahlman lebiere 
cascade correlation learning architecture 
technical report cmu cw mellon university pittsburgh pa 
floyd lo yun sullivan 
prediction breast cancer artificial neural network 
cancer 
fogel fogel porto 
evolving neural networks 
biological cybernetics 
fogel wasson 
evolving neural networks detecting breast cancer 
cancer letters 
fogel wasson porto 
step computer assisted mammography evolutionary programming neural networks 
cancer letters 
bekic 
neural networks approach early breast cancer detection 
systems architecture 
haykin 
neural networks comprehensive foundation 
hall new jersey usa edition 
janson 
application genetic algorithms training higher order neural networks 
systems engineering 
janson 
training product unit neural networks genetic algorithms 
ieee expert 
kitano 
designing neural networks genetic algorithms graph generation system 
complex systems 
kuhn tucker 
nonlinear programming 
proceedings second berkeley symposium mathematical statistics probability pages 
lecun denker solla howard jackel 
optimal brain damage 
touretzky editor advances neural information processing systems ii pages 
morgan kauffman san mateo ca 
maniezzo 
genetic evolution topology weight distribution neural networks 
ieee transactions neural networks 
menczer parisi 
evidence hyperplanes genetic learning neural networks 
biological cybernetics 
montana davis 
training feedforward artificial neural networks genetic algo rithms 
pages morgan kauffman san mateo ca 
moscato 
memetic algorithms short 
corne dorigo glover editors new ideas optimization pages 
mcgraw hill london 
furst 
distributed genetic algorithm neural network design training 
complex systems 
herman benner 
association statistical mathematical neural approaches mining breast cancer patterns 
expert systems applications 
porto fogel fogel 
alternative neural network training methods 
ieee expert 
poli 
evolving topology weights neural networks dual representation 
applied intelligence 
rumelhart hinton williams 
learning internal representations error propagation 
mcclelland rumelhart pdp research group eds editors parallel distributed processing explorations microstructure cognition foundations mit press cambridge pages 
setiono 
extracting rules pruned neural networks breast cancer diagnosis 
artificial intelligence medicine 
setiono 
generating concise accurate classification rules breast cancer diagnosis 
artificial intelligence medicine 
setiono huan 
understanding neural networks rule extraction 
proceedings international joint conference artificial intelligence pages 
morgan kaufmann 
setiono huan 
neural networks oblique decision rules 
neuro computing 
storn price 
differential evolution simple efficient adaptive scheme global optimization continuous spaces 
technical report tr international computer science institute berkeley 
werbos 
regression new tools prediction analysis behavioral sciences 
phd thesis harvard university 
morgan 
application backpropagation neural networks diagnosis breast ovarian cancer 
cancer letter 
wolberg mangasarian 
method pattern separation medical diagnosis applied breast 
proceedings national academy sciences national academy sciences washington dc 
wu doi schmidt metz 
artificial neural networks mammography application decision making diagnosis breast cancer 
radiology 
yan zhu hu 
hybrid genetic bp algorithm application radar target classification 
proceedings ieee national aerospace electronics conference ieee press usa pages 
yao 
evolutionary artificial neural networks 
international journal neural systems 
yao 
review evolutionary artificial neural networks 
international journal intelligent systems 
yao 
evolving artificial neural networks 
proceedings ieee ieee press usa 
yao liu 
new evolutionary system evolving artificial neural networks 
ieee trans 
neural networks 
yao liu 
making population information evolutionary artificial neural networks 
ieee trans 
systems man cybernetics part cybernetics 
yao liu 
designing artificial neural networks evolution 
applied mathe computation 
table average test accuracy standard deviations selected pareto network smallest training error crossover mutation 
error number hidden units table comparison algorithms 
average test accuracy standard deviations best performance achieved method 
method error number epochs fogel back prop bit test error crossover rate mutation rate bit test error crossover rate mutation rate bit test error bit test error crossover rate mutation rate bit test error mutation rate crossover rate mutation rate crossover rate bit test error bit test error crossover rate mutation rate bit test error crossover rate mutation rate crossover rate mutation rate bit test error mutation rate bit test error crossover rate mutation rate bit test error crossover rate mutation rate average test error breast cancer dataset obtained crossover probability mutation probability 
crossover rate number hidden units crossover rate mutation rate number hidden units crossover rate mutation rate number hidden units number hidden units crossover rate mutation rate number hidden units crossover rate mutation rate crossover rate mutation rate number hidden units number hidden units crossover rate mutation rate number hidden units crossover rate mutation rate crossover rate mutation rate number hidden units mutation rate number hidden units crossover rate mutation rate number hidden units crossover rate crossover rate mutation rate average number hidden units breast cancer dataset obtained crossover probability mutation probability 
number objective evaluations crossover rate mutation rate number objective evaluations crossover rate mutation rate number objective evaluations number objective evaluations number objective evaluations crossover rate mutation rate mutation rate crossover rate mutation rate crossover rate number objective evaluations number objective evaluations crossover rate mutation rate number objective evaluations crossover rate mutation rate crossover rate mutation rate number objective evaluations mutation rate number objective evaluations crossover rate mutation rate number objective evaluations crossover rate mutation rate average number objective evaluations breast cancer dataset obtained crossover probability mutation probability 
crossover rate table average test accuracy standard deviations selected pareto network smallest training error crossover mutation 
table comparison algorithms 
average test accuracy standard deviations best performance achieved method 
average test error breast cancer dataset obtained crossover prob ability mutation probability 
average number hidden units breast cancer dataset obtained crossover probability mutation probability 
average number objective evaluations breast cancer dataset obtained crossover probability mutation probability 

