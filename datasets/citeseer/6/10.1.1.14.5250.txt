unifying theorem spectral embedding clustering brand kun huang mitsubishi electric research labs cambridge massachusetts usa electrical computer engineering university illinois urbana usa spectral methods selected eigenvectors data affinity matrix obtain data representation trivially clustered embedded low dimensional space 
theorem explains broad classes affinity matrices works successively smaller fewer fewer affinity matrix dominant eigenvalues eigenvectors angles similar vectors new representation shrink angles dissimilar vectors grow 
specifically sum squared cosines angles strictly increasing dimensionality representation decreases 
spectral methods truncated amplifies structure data heuristic post processing succeed 
result construct nonlinear dimensionality reduction nldr algorithm data sampled manifolds intrinsic coordinate system linear cyclic axes novel clustering projections algorithm requires post processing gives superior performance challenge problems literature 
spectral methods multivariate data analysis notable practical successes rapidly developing theoretical underpinnings 
spectral algorithm typically begins affinity matrix pairwise relationships samples variates derives useful representation data eigenvalue decomposition evd just eigenvectors truncated 
classic dimensionality reduction nonlinear embedding algorithms character principal components analysis pca uses variates covariance matrix multidimensional scaling mds uses samples pairwise distance ma trix kernel pc uses kernel matrix ker nel function xi represents dot product samples unknown feature space locally linear embed ding lee uses matrix containing correlations samples barycentric coordinates 
spectral methods successful data clusterings graph partitionings spectral bipartitioning cuts graph thresholding second eigenvector graph normalized laplacian matrix numerous clustering algorithms selected eigenvectors dot product kernel matrices re represent data clustering simpler heuristics thresholding kmeans :10.1.1.43.7945:10.1.1.19.8100:10.1.1.143.153:10.1.1.33.1501
statistical basis optimality pca understood virtually spectral motivated imperfect analogies data derived graphs physical problems harmonic analysis random walks approximations problems vector quantization min cut max flow 
underlying notion truncated eigenvector basis problem simpler subsequent analysis 
theoretical goal explain works 
embeddings clusterings imply loss information little effort bound quantify lost characterize conserved 
acutely true vast majority algorithms spectral analysis just prelude data analysis 
promising steps right direction include alpert yao equates spectral partitioning vector quantization implying objective function analyses fiedler ona freeman shi malik meila shi weiss justify eigenvectors cluster indicator data clustered nearly :10.1.1.43.7945:10.1.1.19.8100:10.1.1.33.1501
particular affinity matrix block structure eigenvectors piece wise constant items share cluster membership elements eigenvectors feature space linearization space aizerman euclidean relationships points consistent kernel similarity measure 
eigenvectors graph normalized laplacian matrix analogous modes vibration graph exhibit 
eigenvectors stochastic matrix describe steady state properties infinite random walk graph :10.1.1.33.1501
value :10.1.1.43.7945:10.1.1.19.8100
nearly clustered data eigenvectors approximately piecewise 
leaves open questions particularly data nearly clustered special properties affinity matrix 
stochasticity 
unit diagonal positive definiteness 
unit spectral radius 
eigenvectors 
information conserved truncated 
obviously answers questions inform post processing eigenvectors 
develop unifying view spectral methods answers questions gives guidance construction clustering nonlinear dimensionality reduction algorithms 
discussion useful think affinity matrix terms possibly unknown kernel affinity value aij xi xj xi xj dot product vectors representing usually unknown locations points high dimensional feature space associated kernel 
spectral analysis gives new data representation derived eigenvalues eigenvectors symmetric affinity matrix summarize main theoretical result eigenvalue scaled eigenvector representation data encodes angles equivalently correlations points embedded surface hypersphere 
representation truncated suppressing smallest magnitude eigenvalues angles equiv correlations high affinity points distorted highlighting manifold structure data 
representation truncated angles equiv correlations decrease points having high affinity increase points having low affinity highlighting cluster structure data 
short nonlinear dimensionality reduction clustering obtained process 
theorem limited symmetric non negative definite affinity matrices corollary establishes relevance non positive matrices asymmetric matrices grams 
remainder leverage theorem novel methods nonlinear dimensionality reduction nldr clustering 
nldr algorithm maps data mixed vector space linear cyclic nature axis determined statistical tests 
clustering algorithm works entirely projections information loss easily characterized minimized bounded step 
experiments show produces high quality clusterings wide variety challenge problems exhibited literature 
solve unusually difficult visual segmentation prob lem 
polarization theorem non negative definite symmetric matrix having eigenvalue decomposition evd vav eigenvalues sorted descending order diagonal define representation truncated representation top rows princi pal eigenvectors scaled square roots associated eigenvalues 
known property truncated best rank approxima tion respect frobenius norm equivalently energy preserving projection rank angle preserving projection column vectors surface dimensional hypersphere obtained scaling column unit norm 
angle column vectors xi xj equivalently yi yj xi arccos correlation equivalently yi yj corr xi xj ty cos 
vectors xi may state main result theorem polarization positive resp nonnegative projected successively lower ranks sum squared angle cosines ij equivalently squared cor relations ilv ll strictly increasing resp non decreasing 
short dimensionality representation reduced distribution cosines migrates away poles angles migrate oij oij 
full proof requires large number lemmas runs pages page limits published separately 
proof sketch gives flavor argument identity diag diag allows derive distribution nonzero eigenvalues cosine matrix yd diag diag diag diag 
show variance eigenvalues grows 
projection hypersphere keeps mean root eigenvalue constant fore sum ti trace ily cos oil grows monotonically 
corollaries developed algorithms remainder corollary embedding suppressing eigenvalues gives small angles distorted 
unsurprising corollary quite similar motivation pca 
case mass preserving embedding spreads data hypersphere surface preserving small angles accurately cosines comprise energy affinity matrix 
means local relations nearby points pre served 
section show allows construct relatively low dimensional embedding affinity data unusual feature embedding space may linear cyclic degrees freedom 
corollary clustering truncation 
fies distribution points dimensional hypersphere causing points high move move apart 
short distribution approaches clustering small methods subset eigenvectors emphasizes data cluster structure improving output heuristic clustering procedure 
mean lowest dimensional embedding best clustering tradeoff amplifying cluster structure losing information 
section show large subset eigenvectors depend entirely projections clustering removing need heuristic post processing 
theorem limited non negative symmetric affinity matrices explanatory value spectral methods employ selected eigenvectors nonpositive matrixes real symmetric matrix written positive semi definite matrices satisfy rank rank rank constructed positive part spectrum best leastsquares gram approximation offering real valued decomposition 
theorem applies example weiss showed clustering methods related rain cut problem nature posed generalized eigenvalue problems non positive laplacian matrices algorithms ultimately consult single eigenvector positive part spectrum normalized laplacian matrix :10.1.1.143.153
worth noting follows basic properties normalized laplacian eigenvector approximately piecewise constant presenting uncertainty final clustering eigenvalue multiplicity choice eigenvector ambiguous 
polarization theorem suggests additional relevant information lies remaining eigenvectors construct algorithm exploits information eliminates abovementioned ambiguities 
dimensionality reduction motivated corollary observe low dimensional nonlinear embedding data surface dimensional hypersphere arclength points inversely related affinity score 
choose truncates lesser eigenvalues affinity matrix pca choice usually matter eigenvalue spectrum prior knowledge true noise levels data affect kernel 
difficult spherical embeddings goal re embed data vector space possible 
point hypersphere surface having smallest arc length points jul orthogonal basis hyperplane tangent sur face 
note 
ud satisfies 
ui specifies direction hypersphere visualized great circle parallel ui passing ui interpreted axes projection cartesian product dimensional vector space dimensional space 
data wraps fully hypersphere direction ui axis cyclic data wraps hypersphere direction uj axis linear 
pca axes statistically motivated multivariate gaussian distribution tangent point axes estimated fitting gaussian distribution surface hypersphere 
complex bingham distribu tion multivariate gaussian density qd conditioned fact vectors unit length denotes complex conjugate transpose exp zy 
complex bingham parameterized hermitian ma trix cal eigenvalues concentration parameters density 
strongly negative ci density extent direction ui 
eigenvector points mode distribution ua 
normalizing constant calculated matrix confluent hypergeometric function case compact form discovered kent fl diag ll mardia showed direction vec tors concentration parameters rd related scatter evd 
xi yy space cyclic axes cal space point unique set ordinates modulo poles presenting singularities 
walking cyclic variates leg arm phase tion featureless cone space described variates euler angles 
satisfying large sample sizes concentrated density cj xj constant numerical solution concentration parameters higher dimensions feasible nontrivial 
fortunately dimensionality reduction knowing evd scatter suffices eigenvectors 
ud exactly maximum likelihood estimate point tangency modal direction axes tangent space ues xd give rough indication axes cyclic small eigenvalue xi indicates data approximately linear direction ui large eigenvalue xi indicates axis cyclic 
random points cylinder side view 

spherical embedding affinity matrix side view bingham axes dimensionality reduction points distributed cylinder space space tl linear axis cyclic axis 
points randomly generated surface cylinder embedded space contaminated isotropic gaussian noise 
top images show projection points 
bottom images show embedding affinity matrix surface sphere 
embedding forms wide belt equator 
arrows show nodal direction degrees freedom bingham distribution fitted embedding 
statistical tests uniformity indicate data cyclic equator linear direction 
test precisely data cyclic direction ui surface hypersphere consider projection data great circle parallel ui passing mode ua 
uniform distribution circle ql implies data cyclic direction ui 
projection zl zv zj ud projection apply result mardia gives statistical test points swiss roll noise side view 

spherical embedding affinities top view map vector space dimensionality reduction points distributed plane space vector space 
data analyzed spherical embedding wrap way equator 
statistical test indicates data bingham distributed directions sphere surface mapped vector space recovering original planar coordinate system 
original data manifold passes close points particularly ends affinity nonlocal neighbors resulting distortion recovered coordinates 
assess hypothesis distribution perimeter unit circle uniform complex bingham eigenvalues normalized scatter zz large data sets ti chi squared distribution degrees freedom 
may reject hypothesis axis ui uniform cyclic confidence pr tl 
identified non cyclic axes isolated projecting union non cyclic axes rescaling resulting vectors unit norm 
projection reduced hypersphere modal point bingham axes similarly axis aligned 
azimuthal equidistant mapping modal point takes points vector space 
process illustrated points sam manifold having genus cylinder points sampled manifold having genus rectangular plane swiss roll 
affinity matrix data sets aij xi oc dimensional manifold embedded pn mapped pd internal distortions surface unrolled flattened stretching 
exp taken average distances point closest neighbor denote os 
simplicity analysis normalize affinity matrix projecting nearest doubly stochastic matrix diag fast modification sinkhorn procedure solve satisfying pi pt pt 
crucial doubly stochastic matrix properties appealing model data interpreted transition probabilities random walk data largest eigenvalue xmax corresponding eigenvector ul implies stationary distribution random walk uniform point equally probable 
obtain embeddings figures discarded totally uninformative constructed section eigenvectors fitted bingham densities results de termine appropriate embedding embeddings ra testing uniformity hypotheses requires explicit calculation bingham concentration parameters 
practice find ml modal estimator complex bingham distribution sensitive noise samples may necessary get estimate 
fisher watson distributions may better behaved currently tractable analytically computationally 
turn problem clustering obtain easily analyzed highly competitive algorithm 
clustering spectral methods extensively studied graph partitioning clustering problems 
showed eigenvector laplacian matrix corresponding second eigenvalue gives embedding graph real line cutting embedding origin gives bipartitioning graph 
extended way partitioning feature points mapped dimensional space new coordinates normalized row vector matrix formed eigenvectors affinity matrix 
similarly ng normalized row vectors matrix formed weighted eigenvectors input means clusterer analysis show results stable data nearly clustered :10.1.1.19.8100
chan directional angle row vectors eigenvectors laplacian matrix new distance measure partitioning 
yao equated partitioning problem clustering row vectors eigenvectors better 
spectral bipartitioning methods adapted visual clustering problems perona freeman shi malik 
analyses weiss meila shi showed normalizing nearly block structured affinity matrix eigenvectors approximately piece spectral clustering data distributed rings :10.1.1.143.153
cluster assignments indicated different markers 
initial log affinity matrix sorted true clusters algorithm blind orderings 
matrix convergence iterations 
converged representation points belonging cluster located corner sphere 
wise constant easy interpret cluster assignments 
structure guaranteed real problems post processing necessary 
goal visiting crowded field eliminate heuristic post processing steps 
theoretical result constructed possible spectral clustering algorithms post evd clustering thresholding stochastic eigenvectors form discrete indicator matrix showing membership point 
basic strategy cast clustering alternating projections projection low rank projection set zero diagonal doubly stochastic 
cases easy characterize conserved lost 
projection lower rank exactly process character ized polarization theorem distribu tion angles minimal loss energy iia ii 
projection zero diagonal doubly stochastic ma trix diag diag diag diag suppresses differences stationary probability points induced projection low rank 
disregarding suppressed diagonal projection simply angle preserving rescaling embedding vectors diag diag diag diag diag diag 
suppressing diagonal induces negative eigenvalues spectrum associated removing energy placed diagonal positive eigenvalues eigenvalues account gallery challenge problems adapted successfully clustered method :10.1.1.19.8100
cluster membership indicated marker symbol 
radial kernel methods clustering reflects connectivity scale similarity size kernel effect results data set differently clustered os setting os breaks sides corners 
similar data set search :10.1.1.19.8100
similar results observed os os 
half energy project lower rank suppressing negative eigenvalues uninformative unit eigenvalue 
gives automatic determination bound loss variance 
alternating projections terminate resulting matrix stochastic unit eigenvalues implying reducibility 
reducible matrix row column permuted block diagonal form 
analysis shown unique positive eigenvalues alternating projections drive leading ues positive bound eigenvalues negative bound 
formally vector norm eigenvalues increases sum remains constant 
reducible multiple stochastic eigenvalues matrix columns stochastic eigenvectors exactly unique rows 
matrix formed rows 
product diagonal prod cluster indicator matrix maps points single cluster unique positive axis yk 
remains shown conditions sufficient guarantee absolute convergence 
experiments including figures converged quite quickly 
affinity matrix produced gaussian kernel procedure groups points similar affinity values essentially creating clusters inter point distances scale 
clusters may nonconvex wrap bef ab results generally agreement human judgment authors di connections gaussian kernel clustering human perceptual gestalts :10.1.1.17.8935
procedure automatically produces multi way partitions prior knowledge number clusters 
data contains clusters different scales matrix may reducible clusters revealed giving partial clustering 
find remaining non stochastic eigenvalues close indicating clusters 
partition matrix stochastic eigenvectors continue alternating projections submatrices obtaining hierarchical clustering 
treats known problem way 
application motion segmentation spectral clustering preferred method segmentation problems computer vision :10.1.1.143.153:10.1.1.17.8935
motion segmentation takes points frame segmentation individual clusters separated show correctness segmentation nonrigid motion segmentation spectral clustering 
frames tracking data people scaled centered superimposed remove spatial cues aid grouping 
tracking matrix factored ms svd spectral clustering columns correctly groups points basis correlated motion time 
grayscale images depict top eigenvectors affinity matrix initialization far left matrix convergence far right iterations 
initialization noisy hint clustering eigenvectors convergence clustering clear piece wise constant eigenvectors associated unit eigenvalues 
eigenvalues near indicating faces results segmentation jaws 




ooo ooo hierarchical clustering problem treated :10.1.1.19.8100
algorithm gives bipartitioning recursively analyzes subsets submatrices partitioning 
second round subset immediately give 
point appears mis classified assignment may consistent kernel distances points consistent scale inter point distances line tight cluster 
tracking data number points scene seeks group points independently moving objects basis correlated motion 
projection points rigid surface object image pfj xn motion matrix encodes position object relative cam era shape matrix gives location points object centered homogeneous coordinates multiple frames multiple objects ms mtt mfi st 
column describes point columns representing points different objects orthogonal columns object may orthogonal column orthogonal columns object 
unfortunately recovered directly thin svd factor ig mg unknown matrix may ar re order mix columns destroying orthogonal structure 
motions objects approximately independent columns belonging ob ject similar columns belonging different objects sense inner product positive motions corresponding points highly correlated 
spectrally clustering columns highly successful segmenting set points objects 
objections leveled approach applied sequences points segmented easily simpler criteria spatial grouping 
understandable tracking data overlapping objects 
clear method extend nonrigid objects motions points object weakly correlated 
satisfy objections constructed unrealistically hard segmentation problem superimposing dense face tracking data talking heads videos 
motion highly nonrigid 
remove spatial separation cues data head centered origin frame data combined faces overlap spatial translational cues segmentation 
centering motion points face anti correlated points face lower lip versus points 
accommodate nonrigid motion rank svd factor xv allows modes deformation object yields harder higher dimensional clustering problem 
shows perfectly clustered yielding correct motion segmentation 
tracking data evolution dominant eigenvectors extracted clusters shown accompanying video 
summary spectral methods practitioners long understood representation derived selected eigenvectors affinity matrix embedding problems easier algorithms 
date formal analyses approach problems obvious cluster structure certain kinds affinity matrix 
polarization theorem section provides unified explanation virtually algorithms affinity matrices cited exists eigenvector representation matches angles data points space representation reduced angles similar points shrink angles dissimilar points grow 
highlights cluster data segmentation heuristic methods significantly 
theorem invites look representation embedding data surface hypersphere inner vectors gives cosine angle 
insight led algorithms finds nonlinear low dimensional data spaces having mixture linear cyclic axes performs repeated projections data eliminating heuristic post clustering 
clustering algorithm appeal steps characterized terms information preserved lost information loss bounded minimized 
performs practice synthetic challenge problems literature real world motion segmentation problem considerably harder contemplated vision 
exploring better spherical data bounds convergence rates rate angles change dimensionality reduced 
benefitted conversations sue yoav freund josh paul viola extend 
aizerman braverman 
theoretical foundations potential function method pattern recognition learning 
automation remote control 
alpert yao 
spectral partitioning eigenvectors better 
proc 
acm ieee design automation 
azar fiat karlin mcsherry 
spectral analysis data 
proceedings rd theory computing pp 
hancock 
hierarchical framework spectral con 
proc 
euro 
conf computer vision pp 

chan zien 
spectral way ratio cut partitioning clustering 
ieee trans 
computer aided design integrated circuits systems 
chung 
spectral graph theory volume cbms regional conf erence series mathematics 
american mathematical society 
costeira kanade 
multi body method motion analysis 
proc 
lnt 
conf 
computer vision pp 


lecture notes graph partitioning part april 
notes uc berkeley cs 
fiedler 
algebraic connectivity graphs 
mathematics journal 
gdalyahu weinshall werman 
self organization vision stochastic clustering image segmentation perceptual grouping image database organization 
ieee trans 
pattern analysis machine intelligence 
hotelling 
analysis complex statistical variables principal components 
educational psychology 
mardia 
maximum likelihood estimators matrix von mises fisher bingham distributions 
annals statistics 
kannan vempala 
clusterings bad spectral 
st foundations computer science 
kent 
fisher bingham distribution sphere 
royal statistical 
kent 
complex bingham distribution shape analysis 
royal statistical society 
ig wish 
multidimensional scaling 
sage publications beverly hills 
mardia 
directional statistics shape analysis 
technical report leeds department statistics 
meila shi 
learning segmentation random walks 
proc 
adv 
neural systems pp 

meila shi 
random walks view spectral segmentation 
ai statistics 
ng jordan weiss :10.1.1.19.8100
spectral clustering analysis algorithm 
proc 
adv 
neural info 
processing systems 
perona freeman 
factorization approach grouping 
proc 
euro 
conf 
computer vision pp 

kelly hancock 
pairwise clustering matrix factorisation em algorithm 
proc 
euro 
conf 
computer vision pp 

roweis saul 
nonlinear dimensionality reduction locally embedding 
science december 
sch kopf smola 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
scott longuet higgins 
feature grouping eigenvectors proximity matrix 
proc 
british machine vision conference pp 

scott longuet higgins 
algorithm associating features patterns 
proc 
royal london 
shi malik 
motion segmentation tracking normalized cuts 
proc 
lnt 
computer vision pp 

sinkhorn 
relationship arbitrary positive matrices doubly stochastic matrices 
annals ical statistics 
weiss 
segmentation eigenvectors unifying view 
proc 
int 
conf computer vision pp 

