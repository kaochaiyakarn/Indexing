generalized linear models geo rey gordon cs cmu edu introduce generalized linear model statistical estimator combines features nonlinear regression factor analysis 
gl approximately decomposes rectangular matrix simpler representation 
low rank matrices link functions 
gl ms include useful models special cases including principal components analysis exponential family pca infomax formulation independent components analysis linear regression generalized linear models 
include new interesting special cases describe 
iterative procedure optimizes parameters gl procedure reduces known algorithms special cases listed special cases new 
matrix contain independent sample unknown distribution 
column represents training example row represents measured feature examples 
reasonable assume features redundant exists reduced set features contains information reduced features linear functions original features distributions elements gaussian redundancy means write product smaller matrices small sum squared errors 
factorization essentially singular value decomposition span rst dimensions left principal subspace span rst dimensions right principal subspace 
requirements uniquely determine svd traditionally imposes additional restrictions ignore 
svd long list successes machine learning including information retrieval applications latent semantic analysis link analysis pattern recognition applications eigenfaces structure motion algorithms data compression tools 
unfortunately svd assumptions limit accuracy learning tool 
rst assumption sum squared errors uv loss function 
squared error loss means predicting answer bad saying answer 
second assumption reduced features linear functions original features 
nonlinear function uv nonlinear functions matrices address shortcomings propose model expected value propose allowing non quadratic loss functions error parameter matrices xed functions 


called link functions 
analogy generalized linear models call equation generalized linear model generalized uses link functions parameters prediction linear svd bilinear 
long choose link loss functions match see de nition matching link loss exist ecient algorithms nding generalization svd gl ms drop replacements svds applications mentioned better reconstruction performance svd error model inaccurate 
addition open new applications see section svd unable provide suciently accurate reconstruction 
matching link loss functions try optimize predictions nonlinear model need worry getting stuck local minima 
example problem try single sigmoid unit parameters training inputs target outputs squared error loss logit 
logit small training sets number local minima grow exponentially dimension 
hand optimize predictions logarithmic loss function log log squared error optimization problem convex 
logistic link works log loss produce convex optimization problem say match 
matching link loss pairs important minimizing convex loss function usually far easier minimizing nonconvex 
convex function generate matching pair link loss functions 
loss function corresponds df de ned min df 
convex dual df generalized bregman divergence 
expression nonnegative globally convex linear function 
write gradient derivative respect zero words loss implies best prediction matching link function 
need facts convex duals 
rst convex second gradient equal 
convex duality de ned aren di erentiable 
replace derivatives subgradients 
loss functions gl ms gl ms matching loss functions particularly important need deal separate nonlinear link functions 
usually able avoid local minima entirely loss function convex groups parameters hold remaining parameters xed 
specify gl picking link functions matching loss functions 
combine individual loss functions loss function described section tting gl reduce minimizing loss function respect parameters 
choice links results di erent gl potentially di erent decomposition choice link functions inject domain knowledge sort noise parameter matrices priori 
useful link functions include corresponding squared error gaussian noise log unnormalized kl divergence poisson noise log loss bernoulli noise 
loss functions necessary analysis algorithms need link functions cases derivatives 
pick loss functions di erentiate get matching link functions pick link functions directly worry corresponding loss functions 
order analysis apply link functions derivatives possibly unknown convex functions 
loss functions df dg dh 


convex functions 
abuse notation call loss functions prediction loss derivative prediction link provides model noise parameter losses derivatives parameter links tell values priori 
convention takes matrix argument say input output matrices similarly 
model xed point equations de ne gl specifying loss function relates parameter matrices data matrix write gl loss function uv uv matrix dot product written tr 
expression sum bregman divergences ignoring terms don depend df uv dg dh 
divergence tends pull uv divergences favor small justify examine happens compute derivatives respect set 
result set xed point equations optimal parameter settings satisfy uv uv understand equations consider special cases 
go zero pressure keep small uv tells column error matrix orthogonal column second set prediction link uv uv uv tells choose uv reconstructs smallest possible sum squared errors 
algorithms tting gl ms solve equations di erent algorithms 
example gradient descent generalized gradient descent update rule learning rate uv uv advantage algorithms simple implement don require additional assumptions erentiable subgradients 
focus di erent algorithm 
algorithm newton method reduces known algorithms special cases gl ms course goal solving algorithm method choice implementation gl simplest algorithm works 
newton algorithm need place restrictions prediction parameter loss functions 
restrictions necessary newton algorithm general loss functions give valid gl ms require di erent algorithms 
require di erentiable 
second restrict ij ij ij 

de nitions second derivatives zero simplifying speeding computation 
write ij respective derivatives 
restrictions linearize current guess parameters solve resulting equations nd updated parameters 
simplify notation think separate equations column linearizing respect 
gives new 



uv 

matrix hessian 
equivalently inverse hessian 
diagonal matrix contains second derivatives respect jth column argument 
dv 


diag 
uv 
collecting terms involving new 
yields new 
uv 


uv 


recognize weighted squares problem weights prior precision prior mean 

target outputs uv 

uv 
similarly linearize respect rows nd equation new 






hessian contains second derivatives respect ith row argument 
solve copy simultaneously column replace new solve copy simultaneously row replace new alternating updates tend reduce 
related models important special cases gl ms derive section include principal components analysis sensible pca linear regression generalized linear models weighted majority algorithm 
newton algorithm turns power iteration pca iteratively reweighted squares 
gl ms related generalized bilinear models include special cases ica weighted majority example section 
natural generalizations gl ms multilinear interactions 
models non negative matrix factorization generalized low rank approximation cousins gl ms loss function convex factor xed bregman divergence 
independent components analysis ica assume hidden matrix size independent random variables generated applying square matrix seek recover mixing matrix sources words want decompose uv elements nearly independent possible 
infomax algorithm ica assumes elements follow distributions heavy tails high kurtosis 
assumption helps nd independent components sum heavy tailed random variables tends lighter tails search trying elements follow heavy tailed distribution 
notation xed point infomax algorithm ica tanh see equation 
reproduce prediction link identity duals parameter loss functions log det ij log cosh ij guarantee convergence check decreases reduce step size encounter problems 
positive de nite newton update directions descent directions exists small step size 
check necessary practice 
small positive real number 
equations uv tanh uv derivative log cosh tanh derivative log det right multiplying uv substituting yields tanh uv uv equivalent limit vanishing 
exponential family pca duplicate exponential family pca set prediction link arbitrarily parameter links large multiples identity 
newton algorithm applicable assumptions uv new 
uv 


uv 
equation corresponding modi cation provide faster algorithm proposed updates part time keeps updating part convergence moving 
example robot belief states shows map corridor cmu cs building 
robot navigating corridor sense side walls compute accurate estimate lateral position 
near observable feature lab door near middle corridor accurately resolve position corridor tell pointing left right 
order plan achieve goal environment robot maintain belief state probability distribution representing best information unobserved state variables 
map shows robot starting belief state corridor facing doesn know 
collected training set belief states driving robot corridor feeding sensor readings belief tracker 
simulate larger environment greater uncertainty arti cially reduced sensor range increased error 
shows collected beliefs 
planning dicult belief states high dimensional simple world states positions cm intervals corridor orientations belief vector fortunately robot encounters belief states 
regularity planning tractable identify features extract important information belief states plan low dimensional feature space high dimensional belief space 
factored matrix belief states feature space ranks 
prediction link exp componentwise link ensures predicted beliefs positive treats errors small probabilities proportionally important errors large ones 
matching loss poisson log likelihood unnormalized kl divergence 
parameter link corresponding kv weak bias small 
belief states 
left panel overhead map corridor initial belief belief state just robot nds direction pointing belief just nding 
right panel reconstruction features 
set kuk 

rst column contains 
loss function xes rst column representing knowledge feature normalizing constant belief sums 
subgradient equation exp uv matrix arbitrary rst column elements matrix degrees freedom compensate constraints newton algorithm handles modi ed xed point equation diculty 
gl principled ecient way decompose matrix probability distributions 
far know model algorithm described literature 
shows reconstructions representative belief state features normalizing constant discarded planning 
reconstruction consistently beliefs reconstruction minor artifacts beliefs 
small number restarts required achieve decompositions optimization problem constrained 
comparison traditional svd requires matrix rank achieve mean squared reconstruction error rank decomposition 
requires rank match rank decomposition 
examination learned matrix shown reveals corridor mapped smooth curves feature space orientation 
corresponding states opposite orientations mapped similar feature vectors half corridor training beliefs confused orientation training beliefs indicated connection orientations 
reconstruction artifacts occur curve nearly self intersects causes confusion states 
happens local minima loss function exibility optimizer able curves avoid self intersection 
success compressing belief state translates directly success planning see details 
comparison traditional svd beliefs log beliefs produces feature sets unusable planning achieve suciently reconstruction features 
discussion introduced new general class nonlinear regression factor analysis model presenting derivation algorithm reductions previously known special cases practical example 
model drop replacement pca provide better reconstruction performance cases pca error model inaccurate 
research includes online algorithms parameter adjustment extensions missing data exploration new link functions 
acknowledgments nick roy helpful comments providing data analyzed section 
supported afrl contract darpa mica program afrl contract darpa program 
opinions author re ect government agencies 
landauer foltz laham 
latent semantic analysis 
discourse processes 
jon kleinberg 
authoritative sources hyperlinked environment 
journal acm 
turk pentland 
eigenfaces recognition 
journal cognitive neuroscience 
carlo tomasi takeo kanade 
shape motion image streams orthography factorization method 
int 
computer vision 
leary peleg 
digital image compression outer product expansion 
ieee trans 
communications 
mccullagh nelder 
generalized linear models 
chapman hall london nd edition 
peter auer mark manfred warmuth 
exponentially local minima single neurons 
nips vol 

mit press 
rockafellar 
convex analysis 
princeton university press new jersey 
geo rey gordon 
approximate solutions markov decision processes 
phd thesis carnegie mellon university 
daniel lee sebastian seung 
algorithms nonnegative matrix factorization 
nips vol 

mit press 
nathan 
personal communication 
anthony bell terrence sejnowski 
independent components natural scenes edge lters 
vision research 
michael collins sanjoy dasgupta robert schapire 
generalization principal component analysis exponential family 
nips vol 

mit press 
fox burgard dellaert thrun 
monte carlo localization ecient position estimation mobile robots 
aaai 
nicholas roy geo rey gordon 
exponential family pca belief compression pomdps 
nips vol 

mit press 
sam roweis 
em algorithms pca spca 
nips vol 

mit press 
