norm support vector machines ji zhu rosset trevor hastie rob tibshirani department statistics stanford university stanford ca hastie stat stanford edu standard norm svm known performance classification 
consider norm svm 
argue norm svm may advantage standard norm svm especially redundant noise features 
propose efficient algorithm computes solution path norm svm facilitates adaptive selection tuning parameter norm svm 
standard class classification problems set training data xl 
xn yn input xi output yi bin 
wish find role training dam new input assign class 
handle problem consider norm support vector machine svm min yi jhj ill hi hq basis functions tuning ter 
solution denoted fitted model classification role signif 
norm svm successfully 
gue norm svm may advantage norm svm especially redundant noise features 
get fitted model thin performs dam need select appropriate tuning practice people usually pre specify finite set values covers wide range validation data set cross validation select value gives best performance set 
illustrate solution path piece wise linear function space propose efficient algorithm compute exact solution path help understand solution changes facilitate adaptive selection tuning parameter mild assumptions show computational cost compute solution path nq min worst case nq best case 
delving technical details illustrate concept piece wise linearity solution path simple example 
generate training data classes 
class standard normal independent inputs xl 
second class standard normal independent inputs conditioned 
dictionary basis functions xl xe xl 
solution path function shown 
segment adjacent vertical lines linear 
right derivative respect piece wise constant 
solid paths xl relevant features 
solution path function section motivate interested norm svm 
section describe algorithm computes solution path 
section show numerical results simulation data real world data 
regularized support vector machines standard norm svm equivalent fit model min yi jhj xi tuning parameter 
practice people usually choose hj basis functions reproducing kernel hilbert space 
kernel trick allows dimension transformed feature space large infinite cases causing extra computational burden 
concentrate basis representation kernel representation 
notice form loss penalty tuning parameter controls tradeoff loss penalty 
loss called hinge loss penalty called ridge penalty 
idea penalizing sum squares parameters neural networks known weight decay 
ridge penalty shrinks fitted coefficients zero 
known shrinkage effect controlling variances possibly improves fitted model prediction accuracy especially highly correlated features 
statistical function estimation point view ridge penalty possibly explain success svm 
hand computational learning theory associated performance svm margin maximizing property property hinge loss 
effort build connection different views 
replace ridge penalty ll norm lasso penalty consider norm svm problem yi oq jhj xi xll equivalent lagrange version optimization problem 
lasso penalty proposed regression problems response continuous categorical 
ill classification problems framework svms 
similar ridge penalty lasso penalty shrinks fitted coefficients zero benefits reduction fitted coefficients variances 
property lasso penalty nature penalty making large small cause coefficients exactly zero 
example fitted coefficients non zero 
lasso penalty kind continuous feature selection case ridge penalty 
equal zero 
interesting note ridge penalty corresponds gaussian prior lasso penalty corresponds double exponential prior 
double exponential density heavier tails gaussian density 
reflects greater tendency lasso produce large fitted coefficients leave especially high dimensional problems 
consider situation small number training data large number basis functions 
argue sparse scenario small number true coefficients non zero lasso penalty works better ridge penalty non sparse scenario true coefficients gaussian distribution lasso penalty ridge penalty fit coefficients data estimate non zero coefficients 
curse dimensionality toll 
observations propose bet sparsity principle highdimensional problems encourages lasso penalty 
algorithm section gives motivation interested norm svm 
solve norm svm value transform linear programming problem standard software packages get fitted model performs data need select appropriate value tuning section propose efficient algorithm computes solution path facilitates adaptive selection piece wise linearity follow solution path increases notice yi lll piece wise linear karush kuhn tucker conditions change increases residual yi changes non zero zero fitted coefficient changes non zero zero correspond nonsmooth points yi lll 
implies derivative respect piece wise constant karush kuhn tucker conditions change derivative change 
indicates solution path piece wise linear 
see details 
compute solution path need find joints asterisk points piece wise linear path straight lines interpolate equivalently start find right derivative increase change derivative gets joint 
initial solution notation 
yi yi right derivative denotes components indices loss generality assume yi yi 
compute path follows need compute derivative 
consider modified problem min yi yi xi 
notice yi loss yi loss 
setup derivative respect matter value show coincides right derivative sufficiently small 
setup helps find initial derivative 
solving transformed simple linear programming problem get initial equal 
uo starts increases 
main algorithm main algorithm computes solution path proceeds 
increase events happens training point hits basis function leaves current denoted la ola la 
solve xi uj hi xi sign uj uo uj uj 
unknowns 
compute 
yi uo xi uj hj xi 

solve uo xi es ev sign uj uo uj unknowns 
compute yi uo xi 

computed values step step 
ifi isi values 
choose smallest negative 
smallest non negative algorithm terminates smallest negative corresponds step update fu uj 
smallest negative corresponds step update 
cases changes ld ld uo go back step 
get path wise line 
remarks due page limit omit proof algorithm give exact solution path see detailed proof 
explain little step algorithm tries 
step algorithm indicates gets joint solution path right derivative needs changed residual changes non zero zero coefficient basis function changes non zero zero increases 
possible types actions algorithm take add basis function remove point step computes possible right derivative adding basis function hj 

step computes possible right derivative removing point possible right derivative determined training points kept increases joint step occurs 
indicates fast loss decrease changes step takes action corresponding smallest negative 
loss decreased algorithm terminates 
table simulation results norm norm svm simulation noise input noise inputs noise inputs noise inputs noise inputs test error se norm norm penalty joints computational cost proposed algorithm computes solution path 
natural question computational cost algorithm 
suppose isi joint piece wise linear solution path takes qm compute step step algorithm sherman morrison updating formula 
assume training data separable dictionary training data eventually going loss equal zero 
reasonable assume number joints piece wise linear solution path 
maximum value min minimum value get worst computational cost nq min best computational cost nq 
notice rough calculation computational cost mild assumptions 
simulation results section indicate number joints tends min 
numerical results section simulation real data results illustrate norm svm 
simulation results data generation mechanism described section generate training data classes harder problems sequentially augment inputs additional standard normal noise inputs 
second class completely surrounds skin surrounding dimensional subspace 
bayes error rate problem irrespective dimension 
original input space hyperplane separate classes enlarged feature space corresponding nd degree poly nomial kernel dictionary basis functions xj 
generate test data compare norm svm standard norm svm 
average test errors simulations different numbers noise inputs shown table 
norm svm norm svm choose tuning parameters minimize test error fair possible method 
comparison include results non penalized svm 
table see non penalized svm performs significantly worse penalized ones norm svm norm svm perform similarly noise input line norm svm adversely affected noise inputs line line 
norm svm ability select relevant features ignore redundant features suffer noise inputs norm svm 
table shows number basis functions number joints piece wise linear solution path 
notice di joints 
shows norm svm result simulation 
left middle panels norm svm noise inputs 
left panel piece wise linear solution path 
upper paths correspond sh relevant features 
middle panel test error solution path 
dash lines correspond minimum test error 
fight panel illustrates linear relationship number basis functions number joints solution path real data results section apply norm svm classification gene microarrays 
classification patient samples important aspect cancer diagnosis treatment 
norm svm successfully applied microarray cancer diagnosis problems 
weakness norm svm predicts cancer class label automatically select relevant genes classification 
primary goal microarray cancer diagnosis identify genes responsible classification class prediction 
proposed gene selection methods call univariate ranking ur recursive feature elimination rfe see combined norm svm 
procedures step procedures depend external gene selection methods 
hand norm svm inherent gene feature selection property due lasso penalty 
norm svm achieves goals classification patients selection genes simultaneously 
apply norm svm leukemia data 
data set consists training data test data types acute leukemia acute leukemia aml acute leukemia 
datum vector genes 
original input acj jth gene expression level basis function tuning parameter chosen fold cross validation final model fitted training data evaluated test data 
number joints solution path appears table 
see norm svm performs similarly methods classification advantage automatically selecting relevant genes 
notice maximum number genes norm svm select upper bounded usually microarray problems 
considered norm svm 
illustrate norm svm may advantage norm svm especially redundant features 
solution path norm svm piece wise linear function tuning table results microarray classification method cv error test error genes norm svm ur norm svm rfe norm svm parameter 
proposed efficient algorithm compute solution path norm svm facilitate adaptive selection tuning parameter 
acknowledgments hastie partially supported nsf dms nih roi ca 
tibshirani partially supported nsf dms nih roi ca 
bradley mangasarian 
feature selection concave minimization support vector machines 
shavlik eds icml 
morgan kaufmann 
pontil poggio 
regularization networks support vector machines 
advances large margin classifiers 
mit press 
friedman hastie rosset tibshirani zhu 
discussion consistency boosting jiang lugosi zhang 
annals statistics 
appear 
golub slonim tamayo mesirov loh downing 
molecular classification cancer class discovery class prediction gene expression monitoring 
science 
guyon weston vapnik 
gene selection cancer classification support vector machines 
machine learning 
hastie tibshirani friedman 
elements statistical learning 
springerverlag new york 
mukherjee tamayo slonim verri golub mesirov poggio 
support vector machine classification microarray data 
technical report ai memo mit 
rosset zhu hastie 
boosting regularized path maximum margin classifier 
technical report department statistics stanford university ca 
song bi bennett cramer 
prediction protein retention times exchange chromatography systems support vector regression 
journal chemical information computer sciences september 
tibshirani 
regression shrinkage selection lasso 

vapnik 
tha nature statistical learning theory 
springer verlag new york 
wahba 
support vector machine reproducing kernel hilbert spaces randomized 
advances kernel methods support vector learning mit press 
zhu 
flexible statistical modeling 
ph thesis 
stanford university 
zhu hastie 
classification gene microarrays penalized logistic regression 
biostatistics 
accepted 
