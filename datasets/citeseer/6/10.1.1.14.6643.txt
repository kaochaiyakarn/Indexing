independent relevance methods evaluation metrics subtopic retrieval cheng zhai computer science department university illinois urbana champaign urbana il cs uiuc edu non traditional retrieval problem call subtopic retrieval 
subtopic retrieval problem concerned finding documents cover different subtopics query topic 
means utility document ranking dependent documents ranking violating assumption independent relevance assumed traditional retrieval methods 
subtopic retrieval poses challenges evaluating performance developing effective algorithms 
propose framework evaluating subtopic retrieval generalizes traditional precision recall metrics accounting intrinsic topic difficulty redundancy documents 
propose systematically evaluate methods performing subtopic retrieval statistical language models maximal marginal relevance mmr ranking strategy 
mixture model combined query likelihood relevance ranking shown modestly outperform baseline relevance ranking data set trec interactive track 
keywords subtopic retrieval maximal marginal relevance language models 
notion relevance central theoretical practical information retrieval models 
traditional retrieval models assume relevance document independent relevance documents 
possible formulate retrieval problem computing relevance value document separately ranking documents relevance 
reality independent relevance assumption rarely holds utility retrieving document general may depend documents user seen 
extreme example relevant document may useless user user seen document content 
permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigir toronto canada copyright acm xxxxx xx xx xx 
william cohen center automated learning discovery carnegie mellon university pittsburgh pa william com john lafferty school computer science carnegie mellon university pittsburgh pa lafferty cs cmu edu interesting discussion need non traditional ranking perspective optimal search behavior 
study subtopic retrieval problem requires modeling dependent relevance 
subtopic retrieval problem finding documents cover different subtopics general topic possible 
example student doing literature survey machine learning may interested finding documents cover representative approaches machine learning relations approaches 
general topic unique structure involves different subtopics 
user high recall retrieval preference presumably cover subtopics prefer ranking documents top documents cover different subtopics 
closely related problem called aspect retrieval investigated interactive track trec purpose study interactive retrieval system best support user gather information different aspects topic 
study particular class automatic methods problem methods producing ranked list documents ordered give subtopic retrieval 
words retain basic query ranked list model traditional retrieval seek modify ranking include documents relevant subtopics 
clearly methods traditional relevance ranking optimal problem 
traditional evaluation metrics inappropriate new retrieval task 
initial study new problem describing evaluation metrics possible methods experimental results methods 

data set order measure ranking covers different subtopics high level topic judgments tell documents cover subtopics 
fortunately trec interactive track accumulated judgments 
years trec trec trec interactive track document collection financial times london collection part trec ad hoc collection 
collection mb size contains documents average document length roughly words 
year interactive track task introduces new topics 
reported collect topics years form new topics slightly modifying original ad hoc trec topics typically removing narrative section adding instance section explain subtopic means topic 
example query trec number 
number title robotics description applications robotics world today 
instances time please find different applications sort described 
please save document different application 
document discusses applications need save documents repeat goal identify different applications sort described possible 
topic trec nist assessors read pool documents submitted trec participants identify list instances subtopics determine documents contain cover instances 
example sample topic shown identified different subtopics shown clean room applications healthcare precision engineering spot welding robotics controlling inventory storage devices pipe laying robots talking robot robots loading unloading memory tapes 
topic judgment document represented bit vector bits indicating document covers corresponding subtopic 
data set number subtopics range vector length ranges average 
number judged relevant documents available differs different topics range average documents topic 
judgments non relevant documents judgments assume document non relevant covers relevant subtopic 
strong assumption hope biased evaluation useful comparing different rankings 

evaluation metrics subtopic retrieval difficulty varies recall wish explore methods producing ranked list performs subtopic retrieval task 
immediately obvious evaluate ranking 
intuitively desirable include documents different subtopics early ranking undesirable include documents redundantly cover subtopics 
natural way quantify success goal covering different subtopics quickly measure number different subtopics covered function rank 
precisely consider topic na subtopics 
ranking 
dm documents 
subtopics di set subtopics di relevant 
define subtopic recall total number judgments including relevant nonrelevant documents range documents average documents 
rank optimal system actual system recall typical curves functions minrank minrank sopt defined minimal rank subtopic recall reached system optimal system sopt 
subtopic precision defined ratio minrank sopt minrank 
recall rank percentage subtopics covered documents recall subtopics di clearly desirable subtopic recall grow quickly increases 
clear constitutes level recall particular topic example consider topics subtopics 
topic relevant documents subtopics document di covers exactly distinct subtopic ai 
topic relevant documents subtopics document di covers subtopics ai am 
am ranking 
dm clearly best possible subtopic recall small rankings better 
similarly natural measure redundancy degree documents ranking repeat subtopics ranking appear worse ranking 
accounting intrinsic difficulty example suggests measure meaningful different topics account intrinsic difficulty ranking documents topic 
propose evaluation measure 
ir system produces rankings recall level define minrank minimal rank ranking produced recall define subtopic precision precision recall na precision minrank sopt minrank sopt system produces optimal ranking obtains recall minrank sopt smallest ranking size subtopic recall claim subtopic recall precision defined natural generalizations ordinary recall precision 
precisely claim minrank defined terms ordinary recall subtopic recall ordinary precision defined ratio minrank sopt minrank 
see consider hypothetical curves minrank minrank sopt shown 
suppose sopt ordinary retrieval systems minrank defined terms ordinary recall 
sopt orders relevant documents minrank sopt nr nr number relevant documents topic 
consider non optimal system precision recall kr documents 
recall retrieves rnr relevant documents kr precision rnr kr minrank sopt minrank 
hypothetical curves consistent performance ordinary ranked retrieval systems minrank sopt grows linearly minrank gradually distant line optimal system reflecting fact precision decreases recall increases 
shape minrank sopt predictable ordinary retrieval necessary explicitly account measuring performance 
subtopic retrieval minrank sopt may complex shape 
concrete examples left hand graphs figures show subtopic recall subtopic precision various ranking schemes interpolated points usual way averaged topics test suite 
penalizing redundancy intuitively undesirable include documents redundantly cover subtopics intuition accounted measures subtopic recall precision 
way penalize redundancy include explicit measure cost ranking 
cost ranking defined cost 
dk subtopics di subtopics di kb cost presenting document di user incremental cost user processing single subtopic di 
proceeding analogy measure introduced define mincost minimal cost ranking produced recall define weighted subtopic precision ws precision recall level ws precision mincost sopt mincost sopt produces optimal lowest cost ranking obtains recall note precision special case 
costs ws precision 
concrete examples right hand graphs figures show subtopic recall weighted subtopic precision various ranking schemes 
computing metrics computing precision ws precision require computing optimal values minrank sopt mincost sopt 
unfortunately non trivial relevance judgements 
reduced minimum set covering problem np hard 
fortunately benchmark problems moderate size complexity minimum set cover computed quite quickly simple pruning heuristics 
furthermore simple greedy approximation obtain results nearly indistinguishable exact optimization highest recall greedy ranking algorithm inputs set unranked documents ranking size 
di arg max value 
di di endfor return ranking 
dk generic greedy ranking algorithm values mincost 
see appendix 
evaluations exact values minrank queries 
exact values mincost queries query greedy approximation mincost query 

subtopic retrieval methods computationally complex find optimal ranking subtopic retrieval problem subtopics known kind approximation necessary practice 
natural approximation greedy algorithm ranks documents placing rank document di best rank relative documents ranking 
generic version greedy algorithm shown 
key appropriately define value function quantify notion best document di rank intuitively di cover subtopics covered previous documents 
di subtopics covered previous documents 
course compute metric explicitly value function subtopics known retrieval system initial query topic 
evaluation metric subtopic model 
alternative explicitly modeling subtopics similarity function implicitly accounts subtopic redundancy 
similarity approach maximal marginal relevance mmr ranking strategy 
mmr instantiates greedy algorithm value function 
di sim max sim dj original query parameter controlling relative importance relevance novelty sim typical retrieval similarity function sim document similarity function intended capture redundancy equivalently novelty 
study novelty relevancy language modeling framework 
ways measure novelty document kl divergence measure simple mixture model 
discuss combine novelty relevance cost function 
novelty redundancy measures unigram language models previously selected documents refer language models 
consider candidate document di corresponding language model goal define novelty score indicate novel information document di contains 
fact set cover hard approximate logarithmic factor level approximation achieved greedy algorithm 
precision recall baseline ws precision recall baseline comparison curves precision left ws precision right versus recall novelty measures baseline relevance ranking 
single topic model consider simplest case single model subscript indicates old 
suppose new document model 
define 
notice novelty asymmetric measure interested measuring information new relative way 
unigram language models natural asymmetric distance measure kl divergence interpreted inefficiency compression due approximating true distribution leads value function 
plausible novelty measure simple mixture model 
assume component generative mixture model new document component old topic model background language model general english model 
observed new document estimate mixing weight background model topic model serve measure novelty redundancy 
estimated weight interpreted extent new document explained background model opposed topic model 
similar idea component mixture models explored measure redundancy information filtering 
formally background language model mixing weight 
log likelihood new document wn log wi wi estimated novelty score arg max em algorithm find unique maximizes score 
multiple topic models topic model appropriate account previous models compute summarized novelty value document 
possibility compute mixture average topic models problem reduced single model case 
possibility compute novelty score di previous dj topic model combine scores 
method straightforward 
second obvious possibilities combining individual novelty scores minimum maximum average 
maximum distance unreasonable document judged novel different single old document dj case identical dj 
novelty measures single model reasonable ways computing combined novelty score multiple models different novelty measures shown table 
aggregation basic 
di di vs dj scores combined measure averaged min average kl mixture table novelty measures language models 
comparison novelty measures compared novelty measures subtopic retrieval task 
order focus effectiveness novelty detection considered special task re ranking relevant documents greedy algorithm value functions appropriate aggregations functions 
novelty measures select document query likelihood relevance value function essentially different rankings start presumably relevant document 
evaluated ranking precision measures 
results shown 
observations 
best performing novelty ranking followed 
particularly high recall levels noticeably better measures 
measures relevance ranking relatively low levels subtopic recall relatively poor higher levels subtopic recall 
novelty ranking schemes outperform relevance measure consistently ws precision measure 
expected ws measure heavily penalizes redundancy 
kl ranking schemes generally inferior mixture ranking schemes measures 
surprisingly generally inferior baseline relevance ranking especially high subtopic recall levels 
measure performs slightly better measure similarly measure performs slightly better measure 
note similar original mmr measure 
combining relevance novelty consider combine novelty relevance retrieval model 
relevance retrieval experiments kl divergence measure relevance measure novelty :10.1.1.136.8113
unfortunately direct interpolation measures sense scale 
note estimate loosely interpreted expected percentage novel information document probability randomly chosen word document represents new information 
may consider probabilities associated document probability relevance rel probability word document carries new information new 
leads general form scoring function di di rel di new di rel di new di rel di new di rel di new di cost constants 
non relevant document carries new information interesting user assume 
furthermore assume cost document relevant new 
intuitively cost user seeing relevant redundant document cost seeing non relevant document 
assume user cares redundancy allows re write scoring function equivalent form di di rel new rank rel new rank indicates scores differ constant give identical rankings 
note higher new cost score better lower 
higher rel score lower amount reduction affected cost ratio ratio indicates relative cost seeing non relevant document compared seeing relevant redundant document 
ratio large influence new negligible 
means user low tolerance non relevant document optimal ranking essentially relevance affected novelty documents 
general tradeoff retrieving documents new content avoiding retrieval non relevant documents 
technical problem remains usually rel available score documents function 
possible solution consider ranking documents query likelihood equivalent ranking kl divergence 
may assume rel proportional 
assumption scoring function rewritten di di rank di query estimated novelty coefficient mixture model method 
refer scoring function cost combination relevance novelty 

experiments order evaluate effectiveness proposed method combining novelty relevance compared relevance ranking baseline 
baseline best relevance ranking terms subtopic coverage measure original short queries 
baseline ranking achieved dirichlet prior ranking method smoothing parameter set :10.1.1.136.8113
explored tasks re ranking relevant documents task evaluate novelty methods ranking mixture relevant non relevant documents 
task real problem subtopic retrieval 
sake efficiency results reranking mixture relevant non relevant documents cost ranking scheme re rank top ranked documents returned baseline ranking 
comparison point tried top simple baseline 
intuitively adds new terms query expected increase diversity decrease redundancy documents returned relevant 
feedback approach constructs expanded query model interpolation original maximum likelihood query model pseudo feedback model weight 
feedback model estimated top documents simple baseline results mixture model approach feedback background noise parameter set dirichlet prior smoothing parameter set approximately optimal scoring expanded query 
varied cost parameter 
note unreasonable set value mean larger relevance value corresponds greater cost 
large combination relies relevance formula completely dominated relevance 
expect subtopic performance achieved improving relevance ranking removing redundancy 
re ranking relevant documents presents results simpler task re ranking relevant documents 
show results cost method 
combining relevance novelty weighting scheme gives consistent improvement baselines lowest recall levels measures 
contrast novelty scores improved baseline higher subtopic recall levels 
de precision baseline baseline fb cost rho cost rho recall ws precision baseline baseline fb cost rho cost rho recall comparison curves precision left ws precision right versus recall task re ranking relevant documents cost combination novelty kl divergence measure relevance 
ws precision ranking system target baseline cost rho recall ws precision system optimal target baseline cost rho recall comparison relative ws precision versus recall task re ranking mixed pool documents 
left ws precision real ranking system relative hypothetical ranker rejects non relevant documents re order documents 
right ws precision hypothetical relevant document relative optimal ranking 
behavior method combines relevance low subtopic recall levels novelty high recall levels 
feedback barely improves baseline retrieval method 
ranking mixed documents results table difficult task ranking mixed pool documents 
see cost combination method improves baseline measures slightly larger values 
interestingly pseudo feedback approach improves slightly baseline method precision ws precision 
fact precision improvement obtained feedback method somewhat larger improvement obtained cost combination novelty relevance 
analysis discussion graphs shown results curves methods track quite closely 
ranking method avg precision avg ws precision baseline cost cost baseline fb upper bound table comparison precision ws precision averaged recall levels task re ranking mixture relevant non relevant documents combination novelty kl divergence relevance ranking 
addition non relevant documents performance gains due improving novelty documents ranking largely offset corresponding performance losses due imperfect relevance ranking 
relevant document overlap relevant document non relevant document emphasizing novelty may tend move non relevant documents ranking 
possible gains obtained increasing rank novel relevant documents largely offset cost pulling non relevant documents ranking 
hypothesis supported performance method task re ranking relevant documents 
test possibility conducted test 
recall definitions weighted precision recall comparing ranking system optimal system sopt 
methodology compare ranking systems 
simplify discussion call system playing part test sort benchmarked system system playing part sopt target system 
define ws precision benchmarked system relative target system ws precision mincost mincost relative ws precision measure difference performance lower ws precision larger performance difference 
took rankings produced baseline retrieval sys tem henceforth base removed non relevant documents produce rankings hypothetical system base 
performed transformation cost ranking henceforth cost produce rankings hypothetical system cost 
conjecture cost method ranks relevant documents better baseline system ranks non relevant documents higher 
stated terms hypothetical ranking systems conjecture ws precision base relative base higher indicate smaller difference formance ws precision cost relative cost ws precision base relative sopt lower larger performance difference ws precision cost relative sopt 
conjecture confirmed experiments results shown 
clarity show ws precision intermediate levels recall differences systems greatest 
final set experiments ranking mixed pool documents observation methods considered modestly improves performance original relevance baseline 
query created subtopic query subquery subtopic concatenating original query description subtopic 
instance sample query created subqueries applications robotics world today 
clean room applications healthcare precision engineering retrieved top documents subquery baseline method pseudo feedback placed documents returned subquery single pool ran noise tolerant version greedy set covering algorithm described appendix 
algorithm uses value function expected number new subtopics covered document subquery relevance scores estimate relevance document subtopic 
mmr style algorithms considered algorithm uses explicit model subtopics acquired subtopic descriptions pseudo feedback 
quite unreasonable assume information available practice recall trec interactive track finding descrip tions topics goal user 
may useful consider performance system informal upper bound performance retrieval systems operate explicit model subtopics 
performance method shown table title upper bound average precision averaged ws precision improved surprisingly little precision improved best realistic method baseline feedback ws precision improved best realistic method cost retrieval 

concluding remarks studied non traditional subtopic retrieval problem document ranking dependent relevance independent relevance assumed traditional retrieval methods 
subtopic retrieval problem finding documents cover different subtopics possible 
accurate formulation retrieval problem especially user prefers high recall reflects user preference removing redundancy addition preference relevant documents 
traditional retrieval methods evaluation metrics insufficient subtopic retrieval task requires modeling dependent relevance 
proposed new evaluation framework subtopic retrieval metrics recall subtopic recall precision subtopic precision 
measures generalize traditional relevance recall precision metrics account intrinsic difficulty individual topics feature necessary subtopic retrieval evaluation 
introduced ws precision weighted subtopic precision generation precision incorporates cost redundancy 
proposed methods performing subtopic retrieval statistical language models motivation maximal marginal relevance technique 
evaluated novelty measures simple mixture model effective 
proposed cost combination mixture model novelty measure query likelihood relevance ranking 
method shown slightly outperform tuned relevance ranking baseline 
improvement clearly seen ranking relevant documents working mixed set relevant non relevant documents improvement quite small slightly worse tuned pseudo feedback relevance ranking documents 
indicates relevance novelty redundancy play role subtopic retrieval relevance dominating factor data set 
need study interaction relevance redundancy synthetic data control factors level redundancy number subtopics 
major deficiency mmr style approaches considered independent treatment relevance novelty 
result direct measure relevance new information contained new document 
document formed concatenating seen redundant relevant document lot new non relevant information may ranked high useless user 
direction explore model subtopics directly models latent dirichlet allocation 
appendix computing minrank mincost definition minrank sopt smallest set documents 
dk subtopics di rna na number subtopics 
equivalent minimum set cover problem np hard 
compute minrank iterative deepening algorithm 
document di set ai subtopics di created 



find size subset covers largest number subtopics 
iteration halted large cover subtopics 
efficiently tricks 
set ai strictly dominated aj ai aj removed ai aj set largest index removed 
second main diagonal set set ai containing exactly subtopic ai ai contained ai 
main diagonal sets removed added cover arbitrary order subtopics covered 
third upper bound number subtopics covered set size computed 
upper bound smaller number subtopics na sum ai 
ai id indices largest sets 
searching optimal size set search halted set size 
compute mincost similar scheme iterative deepening performed possible cost levels including cost obtained greedy set covering scheme described set covered subtopics 
heuristics mincost similar minrank possible eliminate dominated documents remove duplicate documents cover exactly set subtopics document 
implemented greedy set covering schemes minrank mincost 
minrank value di 
di defined anew di anew di number subtopics previous document 
di relevant mincost value function ratio anew di subtopics di 
known algorithm achieves factor log optimal solution worse case 
problems perform extremely 
compared exact method described greedy algorithm minrank obtains optimal result 
queries mincost computed exactly ws precision greedy approximation recall values recall ws precision greedy approximation 
section version greedy algorithm uses imperfect corrupted relevance judgements 
true relevance judgements viewed matrix ri subtopic ai relevant document dj ri 
noisy version greedy algorithm assumes estimate ri numbers ri uses value function value di dj 
dj ri ri test method began true relevance judgements ri replaced real numbers ri drawn binomial parameters ri say picked probability ri ri flipped coins bias saved fraction heads new value ri experiments corrupted data confirm noisy greedy set covering algorithm works relatively weak signal estimates ri algorithm obtains average precision precision precision 
notice system optimizes precision 
blei ng jordan 
latent dirichlet allocation 
journal machine learning research january 
carbonell goldstein 
mmr reranking reordering documents producing summaries 
proceedings sigir 
feige 
threshold ln approximating set cover 
journal acm july 
lafferty zhai 
document language models query models risk minimization information retrieval 
proceedings sigir pages sept 
callan 
experiments lemur toolkit 
proceedings text retrieval conference 
robertson 
probability ranking principle ir 
journal documentation dec 
varian 
economics search invited talk sigir 
sigir forum 
voorhees harman editors 
proceedings text retrieval conference trec 
nist special publications 
trec nist gov pubs html 
zhai lafferty 
model feedback retrieval model 
tenth international conference information knowledge management cikm pages 
zhai lafferty :10.1.1.136.8113
study smoothing methods language models applied ad hoc information retrieval 
proceedings sigir pages sept 
zhang callan minka 
redundancy detection adaptive filtering 
proceedings sigir aug 
