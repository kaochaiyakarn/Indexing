language independent named entity recognition combining morphological contextual evidence david yarowsky department computer science center language speech processing johns hopkins university baltimore maryland yarowsky cs jhu edu identifying classifying personal geographic institutional names text important task numerous applications 
describes evaluates language independent bootstrapping algorithm iterative learning re estimation contextual morphological patterns captured smoothed trie models 
algorithm learns unannotated text achieves competitive performance trained short labelled name list required language specific information tools 
ability determine named entities text established important task natural language processing areas including information retrieval machine translation informa tion extraction language understanding 
message understanding conference muc separate named entity recognition task developed best systems achieved impressive accuracy measure approaching 
underlined systems trained specific domain particular lan guage english typically making hand coded rules taggers parsers semantic lexicons 
named entity recognizers published tagged text perform morphological analysis semantic information contextual clues 
systems extensive knowledge particular language nominator choi typically large data files containing lists names exceptions personal organizational iden 
aim build maximally languageindependent system named entity identification classification minimal information source language 
applicability ai style algorithms supervised methods limited multilingual case cost knowledge databases manually annotated corpora 
suitable approach consider em style bootstrapping algorithm 
terms world knowledge simplest rel resource task database known names 
entity class recognized tagged assumed user provide short list order unambiguous examples seeds 
course examples pro vided better results try prove minimal knowledge results achieved 
additionally basic particularities language known capitalization exists relevant languages capitalization german capitalization great help allowable word separators exist frequent exceptions pronoun english 
information utilised required assumptions general model 
word internal contextual information algorithm relies word internal contextual clues relatively independent evidence sources drive bootstrapping algorithm 
category refers morphological structure word paradigm certain classes 
entities prefixes suffixes indicators 
example knowing maria feminine names romanian classification may 
guess common prefix 
suffixes typically informative example perfect indicator name romanian applies polish son english morphological information automatically learned bootstrapping 
contextual patterns mayor left context clearly crucial named entity identification classification especially names follow typical morphological pattern word class foreign origin polysemous example places institutions named persons washington madison cases vice versa ion popescu name romanian writer added name name river 
clearly 
cases context occurrence new word morphological information decision 
noted katz newly introduced entity repeated breaking monotonous ef fect pronoun 
emphasis clarity 
claims number instances new entity associated document length importance entity regard subject discourse 
property conjunction sense discourse tendency noted gale church yarowsky words strongly tend exhibit sese document discourse 
gathering information entity occurrences text morphological clues expect classify entities effectively considered isolation especially important regard subject 
analyzing large texts segmentation phase considered instances name segment high probability belonging class contextual information instances segment jointly making decision 
precision segmentation critical language independent segmentation system richmond smith adequately reliable task 
tokenized text vs plain text basic alternatives handling text 
tokenize classify individual tokens group tokens 
alternative works languages word separators spaces punctuation relatively simple set separator patterns adequately tokenize text 
second alternative classify entities simply respect starting character position knowing word boundaries just probability learned automatically boundary neighboring contexts 
second alternative works languages chinese separators words typically 
class languages define priori prob abilities boundaries match actual separators second approach represents generalization tokenized text 
method text tokenized presents 
advantage statistics tokens types kept results show statistics types reliable tokens 
second method single definition type multiple possible boundaries token instance ways gather statistics considering may call probable types boundary probabilities keeping statistics sistrings semi infinite strings 
advantages disadvantages methods discussed 
basic model describing algorithm brief overview goals language independent core model ability exploit basic language specific fea tures ability learn small named entity lists order total training names capability handle large small texts class scalability properties possibility defining named entity types desired different languages different purposes user choose different classes words recognized capability store information learned instance important concepts model trie structures morphological contextual information tries provide effective efficient flexible data structure storing contextual morphological patterns statistics 
compact representations 
second support natural hierarchical smoothing procedure dis class statistics 
consider character tries node contains probability distribution working tokenized text distributions considered node tokens types 
distribution stored node contain probability name class history node 
distribu tion standard classes named question able unassigned probability mass terms entity classes motivated non entity 
simplify notations refer start point bounded portion text analyzed order determine represents named entity token 
tries context left right internal morphological patterns tokens 
shows example morphological prefix trie stores characters tokens root information stored node character raw distribution quest non entity person place inst list right context links nice morphological prefix trie alex anda nice couple left right starting points optional word boundaries indicated 
suffix tries typically informative equivalent structure reversed direction 
left right context tries structure list links refers tokens particular context represented path root current node 
right context letters introduced trie normal order left context considered reversed order example anda left context dna 
similarly nodes con text tries contain links tokens occurred particular contexts defined paths 
bipartite graph structures created way links 
reasons explained raw counts kept distributions 
probability token context indicating class computed path root terminal node ken context 
way effective smoothing re rare tokens contexts 
considering token context formed characters ln path trie root 
general smoothing model ip ii li eai reasonable expect smaller lambdas correspond smaller indices 


order keep number parameters low model tn lll li having small value symbol raw distributions frequencies normalization step needed compute final probability dis tribution 
simpler model just parameter setting limited flexibility optimizing hierarchical inheritance probability class letter informative languages english romanian contrast may extremely important japanese 
em style bootstrapping basic concept bootstrapping procedure iteratively leverage relatively independent sources information 
seed names class algorithm learns contextual patterns indicative classes itera tively learns new class members word internal morphological clues 
cycle probability distributions class token prefix suffix context incrementally refined 
details describing stage algorithm 
probability mass opposed classical maximum entropy principle faced highly skewed observed class dis tribution little confidence due small sample size typical response uncertainty statistical machine learning systems backoff smooth general class distribution typically uniform 
unfortunately representation difficult distinguish conditional distribution large sample estimated confidence just happens similar fairly uniform true distribution 
representation obscure distinction represents uncertainty distribution separately 
resolve retaining sin gle probability distribution classes adding separate unassigned cell reflects uncertainty distribution 
probability mass continues distributed remaining class cells proportional observed distribution data total sum reflects confidence distribution equal questionable 
approach advantage explicitly representing class distribu tion facilitating tle development system retaining single probability distribution simplifies trie architecture model 
incremental learning essentially process gradually shifting probability mass questionable uncertain primary categories 
algorithm 
divided stages summarized blow 
stage build initial training list class representatives stage read text build left right morphological context tries stage introduce training information tries re estimate distributions boot strapping stage identify classify named entities text competing classifiers stage update entity context training space new extracted information stage stage performed language task consists defining classes filling initial class seed data examples provided user 
list class training names unambiguous possible ideally relatively common 
necessary relatively large unannotated text bootstrapping contextual models classifying new named en 
examples training seeds text romanian language tables primary experiments reported studied relatively difficult way named entity partition names family names place names 
tend relatively hard distinguish languages 
text refers mayor small town alba county drunk wedding hand kissed groom 
simpler person place distinction compa muc task evaluated table 
training data seed name name place andrei adam popescu alba alexandru bogdan table sample training romanian target evaluation text labels training alba david legend de 
mai din este mai tot beat crit care la mina mina si de de 
andrei care se stabilise de la ins pe lui adrian de se 
table sample test data romanian stage ways start stage tokenizing text considering raw form 
tokenization token inserted morphological tries keeps letters tokens normal prefix order keeps letter reverse suffix order 
letter path raw distributions changed adding priori probability probability distribution order node ii non suffix 
sterian subtree lo subtree subtree example normalized unsmoothed distributions suffix morphological trie romanian 
paths shown name entity contained training word list ster name training data partial path tokens 
token belonging class language dependent information may 
example case indo european languages token starts upper case letter add full count probability mass questionable sum entity initially fully ambiguous 
token starts lower case name case add bulk probability mass non entity remainder questionable unassigned language specific orthographic clues potentially affect initial probability mass assignment 
tokenization applied consider possible starting points 
strings simplicity refer tokens introduced prefix ical trie ones introduced suffix trie may differ 
left context token introduced letters reverse order left context trie pointers token prefix trie right context token introduced normal order right context trie keeping pointers token suffix trie 
distributions paths modified pt distribution targeted token 
stage stage core bootstrapping phase algorithm 
essence contextual models better estimated identify additional named entities increasing confidence allowing reestimation improvement internal morphological models 
additional training data yields allows contextual models augmented reestimated cycle continues convergence 
approach bootstrapping process standard continuous em expectationmaximization family algorithms baum dempster 
proposed approach lined discrete variant computationally intensive advantage distinguishing unknown probability distributions simply evenly distributed 
approach conservative utilizes class estimations newly classified data retraining process class probability passes confidence threshold defined 
concept confidence threshold captured definitions dominant semi dominant 
consider discrete finite probability distri bution pn 
say dominant pi words pi say semi dominant respect event dominant exist pj comments definitions necessary easily observed distribution dominant maximum value 
second definition sense consider particular event relevant result measured 
event normalizing rest values obtain new distribution size having dominant 
core stage bootstrapping procedure 
known names original training list learned data inserted sequentially morphological tries modifying probability distributions nodes paths accordingly data structure illustrated figures new distribution nodes path oa known token gains dominant example place effect change propagated node distributions change 
distribution context paths token occurred text modified subtracting questionable mass quantity proportional number times respective token context adding dominant position place mass newly obtained distributions gained dominant example place context trie bootstrapping procedure called tokens occurred context recursively 
important consider raw distributions normalize 
example word occurs times right context merge meaning goes distribution identi fied dominant name units questionable mass moved name mass path right context trie 
semi account fact semi 
may change time probability mass moved questionable position previous semi dominant position semi dominant state reached fore 
may easily observed stage sequential characteristic updating done reading name incrementally 
ing order affect process fact dominant state reached change dominant state probability mass moved questionable 
case semi data ordering training file influence learning procedure 
conservative strategy domi semi hand disadvantage cancelling postponing utilisation words 
example questionable name mass subsequent reestimation iterations initiated data alternative name classes 
considering advantages disadvantages conservative semi dominant approach default model 
stage stage text re analysed sequentially token start point pair decision 
bipartite structure pairs tries central role stage left context prefix tries interact right context suffix tries interference pairs bootstrapping stage 
instance token text classifiers available different trie 
decision regard presence entity classification combining 
comparative trials indicate higher performance achieved initially having clas vote 
results indicate accurate classifications obtained independently bootstrapped morphological tries incor morphological information token classified bootstrapping incorporate information con texts token occurred 
agree semi corresponding class returned 
agreement tested paired independent classifiers order empirically measured reliability agreement simple linear combination considered decision 
approach yields higher measure simple interpolation classifiers default parameters 
stage newly classified tokens contexts saved potential seed data subsequent named entity classification new texts 
results basic measures evaluation precision recall 
precision represents percentage entities system recognized correct 
recall represents percentage correct named entities text system identified 
measures incorporated measure pr 
inappropriate compare results language independent system ones designed language 
day palmer observed fact existing systems perform extremely mixed case english newswire corpora certainly related years research organized evaluations specific task language 
clear resources required adapt systems new languages 
important mention measure human performance task sundheim 
experiments romanian text consistent 
baseline measures order obtain baseline performance method considered performance system tags examples original training 
consider plausible lower bound measure training words selected test text 
day palmer showed baseline measure score enamex task varies english chinese 
important mention computed figures trained language independent sys tem large annotated corpora wall street journal english 
fact precision obtained baseline approach indicates seed training names class completely unambiguous certain degree ambiguity generally unavoidable case mainly cause interference names names 
significant performance measure forced classification accuracy entities previously identified text task selecting name class 
obtain baseline performance measure considered system uses original training word labels exact match entities labeled default name tag common class languages studied 
baseline accuracy measured romanian 
system cies range data 
evaluation basic estimation methods results shown table obtained romanian text having words entities training seed set names names names city country names 
baseline measures default system described 
configuration parameters system optimized romanian greedy search independent development test set yielding slight increase measure 
configuration default parameters conservative dominant criterion clearly favoring precision expense re call 
configuration relevant enamex task represents performance system classes name name combined person entities adjacent consider group person entity 
configuration shows performance standard continuous em smoothing data data structures 
evaluation language knowledge source table shows system performance fairly diverse languages romanian english greek turkish hindi 
initial rows provide basic details training data available language 
note annotators generating lists seed words access development test extract samples constrained text add additional ones memory 
furthermore quite unpredictable contexts word development texts appeared times appear 
total number tual matches seed words quite variable difficult control 
case additional contexts bring comparable new benefit secondary instances word related document collection tend similar identical surrounding contexts instance mayor xxx xxx sai general quite difficult control actual training information content just number raw seed word types notated 
languages levels information sources evaluated 
baseline case previ ously described table 
context case restricts system training left right contextual tries ignoring prefix suffix morphological information 
morphology case contrast restricts system prefix suffix morphological models 
estimated training words total independent source information context bootstrapping iterate available path romanian precision recall measure accuracy baseline system performance default settings semi re estimated smoothing parameters learning wi th person place classes continuous em approach table comparison performance basic estimation methods language romanian english greek turkish hindi training text siz total training seed words contextual matches seeds labeled entities testing baseline precision measure context precision measure morphology precision recall measure context morphology precision measure full bootstrapping precision measure baseline classification accuracy system accuracy table comparison performance models learn behaviour previously unseen affixes conquer new territory 
model entirely static just initial training data 
reasons context model static 
case possible bootstrapping path alternating left right context exo pand coverage new contexts tends robust pursued 
interestingly recall morphology typically higher context case 
reason morphology models full hierarchically smoothed character tries ther word token tries denser initial statistics small training data sets proving greater partial matching potential previously unseen words 
effort test contribution full iterative context morphology results combination tries bootstrapping 
trained training examples 
performance combined sources language knowledge source cases greater morphology context source 
furthermore full iterative bootstrapping clearly yields substantial improvement static models exclusively form increased recall corresponding boost measure 
cross language analysis yields insight 
recall higher languages case explicitly marked clue named entity identification romanian english greek turkish language hindi case distinctions word potentially named entity 
language german roughly middle lower case words low probability named entities capitalized words highly ambiguous common proper nouns 
approximately words hindi text named entities additional orthographic clues prior probability non entity strong morphological contextual evi lk size raw text bootstrapping words precision recall measure total number seed names learning curves romanian dence favor named entity classes compelling overcome bias 
training words context difficult face strong odds named entity classes conservative nature learning algorithm entity label correctly words baseline model 
con trast performance entity classification identification measured forced choice accuracy labelling entities comparable languages accuracy relative baseline 
evaluation different training set sizes demonstrates performance algorithm highly sensitive size training data 
romanian graph shows size raw text bootstrapping increases measure performance increases roughly due exclusively increases precision 
approximately number unique entities identified due increased number examples classification accurate 
aging trend web online sources provides virtually unlimited raw text major languages substantial line text virtually languages 
extrapolating far word level relatively low cost feasible 
second graph shows measure performance increases roughly total length seed range 
increase due entirely improved recall doubles small range 
trend sug note baseline competitive typical assigns majority tag name exact match training wordlist deepak common occurrence repeated high frequency names hindi data training classification baseline answer considerable benefit gained additional human annotation seed wordlist acquisition existing online lexicons 
relative case raw text acquisition additional annotations tend clear cost benefit tradeoff investment annotation 
summary evaluation results satisfying show clear consistent trends diverse languages show clear trends improvement training resources grow show comparable robust classification results achieved diver sity languages 
natural steps include incorporating language independent word segmentation phase proposed amitay richmond smith improve performance large texts 
different statistics pre computed different languages language families stored external files 
example priori probability named entity set characteristics representation text position capitalization relative position entities name followed name 
step implementation supervised active learning system algo rithm relevant words disambiguation user classi fied feedback bootstrapping 
selection candidate examples tagging unassigned probability mass frequency occurrence 
active learning strategies lewis gale natural path efficiently selecting contexts human annotation 
algorithm mini mally supervised named entity ers short name lists seed data typically example words entity class 
algorithm uses hierarchically smoothed trie structures mod eling morphological contextual probabilities ef age independent framework coming need fixed token boundaries tory lengths 
combination relatively indepen dent morphological contextual evidence sources iterative bootstrapping framework converges successful named entity recognizer achieving competitive measure measuring named identification classification applied romanian text 
fixed way classification accuracy entities ranges diverse languages difficult firstname lastname place partition approaches accuracy simpler person place discrimination 
results achieved unannotated training texts absolutely required language specific information tools requiring minutes total human effort training short wordlist creation observed robust consistent performance rapid low cost quite different languages shows potential fur ther successful diverse applications new languages domains 
authors eric brill radu florian shankar kumar jun wu feedback help annotating named entity data languages studied 
smith 

de subject boundaries text language independent statistical proceedings second conference empirical methods natural language processing pp 

aone lovell 
learning tag multilingual texts observation 
proceedings second conference empirical methods natural language processing pp 

baum 
inequality associated max technique statistical estimation probabilistic functions markov process 
equalities 
choi wacholder 

disam proper names text 
proceeding fifth conference applied natural language processing pp 

day palmer 

statistical pro file named entity task 
proceedings fifth conference applied natural language processing pp 

laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
gale church yarowsky 

method disambiguating word senses large corpus 
computers humanities 
gale church yarowsky 

sense discourse 
proceedings jth darpa speech natural language workshop pp 

gale church yarowsky 

discrimination decisions dimensional spaces 
annals operation research 
katz 
distribution context words phrases text language modeling 
natural language engineering 
lewis gale 

sequential algorithm training text classifiers 
proceedings sigir pp 
dublin 
sundheim 
overview results muc evaluation 
proceedings sixth message understanding conference pp 

yarowsky 
unsupervised word sense disambiguation rivaling supervised methods 
proceedings rd annual meeting association computational linguistics pp 

