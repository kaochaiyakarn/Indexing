conference submission 
incorporating prior knowledge boosting robert schapire schapire research att com marie marie eurecom fr research att com narendra gupta research att com labs gamma research shannon laboratory park avenue florham park nj describe modification adaboost algorithm permits incorporation prior human knowledge means compensating shortage training data 
give convergence result algorithm 
describe experiments datasets showing prior knowledge substantially improve performance 

machine learning methods freund schapire adaboost algorithm entirely datadriven sense classifier generates derived exclusively evidence training data :10.1.1.32.8918
data abundant approach sense 
applications data may severely limited may human knowledge principle compensate lack data 
standard form boosting allow direct incorporation prior knowledge 
describe new modification boosting combines balances human expertise available training data 
aim approach allows human rough judgments refined reinforced adjusted statistics training data manner permit data entirely overwhelm human judgments 
basic idea approach modify loss function boosting algorithm balances terms measuring fit training data measuring fit human built model 
actual algorithmic modification entails turns simple requiring addition weighted pseudo examples training set 
allow prior knowledge may form provides guesses rough conditional probability class labels training example 
include example model easily built text categorization tasks human chosen keywords 
approach boosting style algorithm logistic regression described collins schapire singer results prove simple convergence theorem algorithm 
arose development spoken dialogue systems systems computer formulate appropriate response utterances telephone caller 
key task extraction meaning caller said extent utterance classified fixed set categories 
construction classifier done machine learning 
cases system deployed data collected actual data easily collected system deployed 
permitted human crafted knowledge compensate initial dearth data collected deployment 
describe experiments datasets derived spoken dialogue applications 
proprietary datasets conducted experiments benchmark datasets 
case compared boosting prior knowledge 
results show prior knowledge substantially improve performance particularly data substantially limited 

boosting logistic regression review logistic regression boosting style algorithm described collins schapire singer 
spaces instances labels respectively 
assume labels gamma 
input ym gamma ffl exp gamma ffl obtain base function base learner base learner minimize objective function gammay output final classifier 
binary boosting algorithm 
ym sequence training examples theta discussing probabilities assume training test examples selected independently distribution theta eventual goal classification focus estimating probabilities converted classifications obvious way thresholding 
specifically training data wish build rule estimates conditional probability test example chosen logistic regression building real valued function estimating probability oe oe gammaz particular form linear combination base functions 
model postulated attempt find maximizing conditional likelihood data equivalently minimizing negative log conditional likelihood works ln exp gammay collins schapire singer describe variant freund schapire adaboost algorithm minimizing eq :10.1.1.32.8918
functions linear combinations base functions 
pseudo code algorithm call adaboost shown 
adaboost adaboost works rounds 
round set weights training set computed eq 
find base function base function minimize eq 
space base functions schapire singer confidence rated variant adaboost 
rounds sum output final function procedure fact identical confidence rated adaboost compute rule exp gammay gamma convergence results techniques collins schapire singer prove convergence algorithm minimum eq 
provided base functions particular form space base functions semi finite meaning contains finite set functions 
function written linear combination functions 
ffg ff theorem assume base functions fig 
minimize eq 
semi finite space loss eq 
final function converges loss linear combinations functions proof sketch prove result need show round adaboost progress collins schapire singer sequential update algorithm applied finite set particular note gammay min gammay min ff gammay ffg gammay ff ff choices algorithm 
additional steps proof convergence easily modified 
base learning algorithm experiments finding base functions schapire singer boostexter system :10.1.1.134.3024
experiments deal text base function tests presence absence particular word short phrase simple pattern henceforth referred simply term 
term value output value output 
instance base function word occurs text output output gamma 
schapire singer describe base learning algorithm efficiently finds best base function form minimizing eq :10.1.1.134.3024:10.1.1.134.3024

seen space base functions semi finite finitely terms rule form decomposed respectively outputs term respectively absent 

incorporating prior knowledge describe modification boosting incorporate prior knowledge 
approach human expert constructing rule mapping instance estimated conditional probability distribution yjx possible label values gamma 
discuss methods constructing rule 
background prior model training data possibly conflicting goals constructing predictor fit data fit prior model 
measure fit data log conditional likelihood eq 

measure fit prior model example relative entropy called kullback leibler divergence prior model distribution distribution labels associated constructed logistic model oe 
precisely letting jx measure fit prior model re oe re ln gamma ln gamma gamma binary relative entropy 
relative importance terms controlled parameter putting get objective function ln exp gammay oe rewritten ln gammay ln gammaf gamma ln term independent disregarded 
note objective function form eq 
larger set addition nonnegative weights term 
minimize eq 
apply adaboost procedure described section larger weighted training set 
new set includes original training examples unit weight 
addition training example create new training examples gamma weights gamma respectively 
triple number examples noticing occurs twice get away doubling training set 
training weights computing exp gamma ranges examples new training set 
modification theorem weighted training sets straightforward 
final modification add th base function incorporate right start 
particular take oe gamma ln gamma include computing final classifier multiclass problems assumed binary prediction problem gamma 
generally follow schapire singer approach multiclass problems classes allowed furthermore example may belong multiple classes 
intuitive idea reduce binary questions ask example classes 
particular suppose classes kg 
label vector gamma th component indicates example class 
purpose find function theta oe estimated probability example belongs class 
treating class separately objective function eq 
ln gammay boosting algorithm adaboost modified straightforwardly maintaining weights pairs eq 
exp gamma eq 
gammay done schapire singer base learner finds rules test presence absence term outputs vector numbers class depending result test :10.1.1.134.3024:10.1.1.134.3024
prior knowledge gives guessed estimates jx conditional probability example belongs class 
require probability distribution 
objective function eqs 
ln gammay jx oe ln gammay jx ln gammaf gamma jx ln handle objective function similar binary case create new training set weights example label pairs original examples occur unit weight 
example replicated twice gamma ones vector 
letting indices new replicated examples weights respectively jx gamma jx 
experiments section describe experiments comparing boosting prior knowledge boosting knowledge particularly data substantially limited 
compare text categorization methods purpose study schapire singer carried extensive experiments comparing boosting methods text categorization problems :10.1.1.134.3024:10.1.1.134.3024
publicly available text categorization datasets proprietary speech categorization datasets 
datasets come application original motivation 
chose datasets large naturally lent easy construction human crafted model 
substantially larger number datasets inherently intensive subjective nature building models 
benchmark datasets set experiments benchmark text categorization datasets ffl ap titles corpus associated press newswire headlines 
object classify headlines topic 
preparation dataset described schapire singer consisting examples classes :10.1.1.134.3024:10.1.1.134.3024
ffl newsgroups dataset consists usenet articles collected lang different newsgroups 
object predict newsgroup particular article posted 
articles collected newsgroup 
removing duplicates total number articles dropped 
prior model 
framework permits prior knowledge kind long provides estimates rough probability example belonging class 
describe possible technique creating rough model 
dataset authors access list categories data thought handful keywords class 
lists keywords shown tables 
keywords produced entirely subjective process free association general knowledge categories time period data connected information access data 
step required direct human involvement rest process generating prior model fully automatic 
keywords build simple naive model 
purposely model far perfect see algorithm performs prior knowledge rough expected practice 
began defining conditional probability class presence absence keyword denoted jw jw 
class keywords japan japan tokyo yen bush bush george president election israel israel jerusalem peres sharon palestinian israeli britain british england english london thatcher gulf gulf iraq hussein kuwait german german germany bonn berlin mark weather weather rain snow cold ice sun sunny cloudy dollar gold price hostages hostages holding hostage budget budget deficit taxes arts art painting artist music entertainment museum theater boston taxes governor yugoslavia yugoslavia dan ireland ireland ira dublin bonds bond bonds yield interest rating tv box office movie stock bond bonds stocks price earnings table 
keywords class ap titles dataset 
jw ae keyword gamma nw nw number classes listing keyword 
words keyword listed single class seeing word gives probability correct class listed classes probability divided equally 
remaining probability divided equally classes listing keyword 
assign equal probability classes jw define prior distribution classes uniform rules naive assumption keywords conditionally independent class 
bayes rule compute probability class presence absence keywords 
estimate jx 
experimental set 
dataset run randomly permuted data 
trained boosting prior knowledge examples class keywords alt atheism god atheism christ jesus religion comp graphics graphics color computer computers plot screen comp os 
misc computer computers operating system microsoft windows ms dos comp sys ibm 
pc hardware computer computers ibm pc clone hardware cpu disk comp sys mac 
hardware computer computers mac macintosh hardware cpu disk comp windows 
computer computers windows unix misc sale asking selling price rec autos car drive fast jaguar toyota ford honda gm tire engine rec motorcycles motorcycle honda harley wheel engine throttle rec sport 
baseball baseball hit strike ball base bases runs outs rec sport 
hockey hockey stick puck goal check sci crypt cryptography encrypt cipher decrypt security secret key sci electronics electronics computer computers chip electric sci med medicine doctor science heal sick cancer sci space space astronaut nasa rocket space shuttle soc religion 
christian religion christian jesus christ god catholic talk politics 
guns guns gun nra brady kill shoot shot talk politics 
mideast mideast israel jordan palestinian lebanon iraq iran talk politics 
misc politics clinton president congress senate senator talk religion 
misc religion christian catholic god believe table 
keywords class newsgroups dataset 
newsgroups 
remaining examples starting example testing 
ran experiment times averaged results 
fixed number rounds boosting 
set parameter heuristic formula gamma chosen interpolate smoothly guesses appropriate values couple values 
experiments conducted determine better performance achieved choice results 
figs 
show results experiments 
figures show test error rate boosting prior knowledge measured error rate training examples data knowledge knowledge data 
comparison test error rate prior knowledge data separately ap titles dataset measured function number training examples 
function number training examples 
figures show error rate achieved prior model training examples 
error rate means fraction test examples top scoring label correct labels recall example may belong class 
results averaged runs 
fairly small datasets prior knowledge gives dramatic improvements straight boosting 
large training sets newsgroups dataset imperfect nature prior knowledge eventually hurts performance effect seen 
spoken dialogue datasets describe experiments datasets spoken dialogue applications 
applications goal extract meaning utterances spoken telephone callers 
utterances passed automatic speech recognizer 
goal train classifier categorize resulting noisy text 
classifier output passed dialogue manager carries dialogue formulated appropriate response caller utterance 
applications ffl may help 
goal identify particular call type collect call request billing information different classes 
experiments sentences training set error rate training examples data knowledge knowledge data 
comparison test error rate prior knowledge data separately newsgroups dataset measured function number training examples 
sentences test set 
information dataset provided gorin riccardi wright 
ffl application provides information natural voices text speech engine 
instance caller ask demo price information sales representative 
different classes 
trained models sentences tested sentences 
prior models built similar fashion preceding experiments allowed human freedom choosing probabilities rules 
see details 
performed similar experiments described section measuring classification accuracy function number examples training comparing models built training examples models built hand crafted rules prior knowledge training examples 
dataset trained models rounds number available training examples respectively 
parameter selected empirically number available training examples 
set number training examples equal greater 
dashed line shows classification accuracy models built hand crafted rules training examples solid lines show classification training sentences data knowledge knowledge data 
comparison performance data knowledge separately task 
accuracy models built training examples hand crafted rules 
improvement accuracy observed hand crafted rules training examples 
comes fact patterns hand crafted rules data sufficient number sentences statistical impact training data 
experiment fewer training examples available 
examples exploiting human expertise provided classification accuracy levels equivalent models trained times amount training data 
number training examples larger 
accuracy levels equivalent times amount training data 
larger sentences available models converge similar classification accuracy 
shows similar comparison task 
trained models number training examples respectively 
set number training examples equal 
shows improvement classification accuracy hand crafted rules 
improvement absolute training examples drops data available 
figures knowledge curves perfectly flat 
comes fact models knowledge take account empirical distribution classes available training examples uniform distribution done section 
training examples data knowledge data knowledge 
comparison performance data knowledge separately task 
experiment performed evaluate accuracy classifier new semantic classes added system training 
situation new functionalities needed system deployment data available 
fig 
shows classification accuracy additional semantic classes added model trained classes 
system performance drops general results demonstrate incorporating human judgment helps provide initial boost performance data 

variations extensions described extension particular boosting algorithm incorporate prior knowledge 
basic technique applied great variety boosting algorithms 
instance schapire singer boosting framework base functions map real numbers magnitude indicate level confidence 
choice orthogonal basic method incorporating prior knowledge 
approach substantially speed convergence weak base learner settings may wish standard base learner outputting hard predictions gamma goal simply weighted error minimization basic version adaboost 
chosen particular method extending binary adaboost multiclass case extension schapire singer call adaboost mh 
training examples data knowledge data 
adding new semantic classes model training 
multiclass extensions adaboost modified logistic regression collins schapire singer 
fact approach limited boosting algorithms 
basic idea modifying loss function logistic regression adding applied algorithm logistic regression 
note measure fit prior model eq 
independent actual training labels means need limit term labeled data case access abundant unlabeled data term 
idea research follow cotraining approach studied blum mitchell collins singer train models say force give similar predictions large set unlabeled data 
case term eq 
replaced re oe oe sum unlabeled dataset 
avrim blum tom mitchell 
combining labeled unlabeled data training 
proceedings eleventh annual conference computational learning theory pages 
michael collins robert schapire yoram singer 
logistic regression adaboost bregman distances 
machine learning 
michael collins yoram singer 
unsupervised models named entity classification 
empirical methods natural language processing large corpora 
yoav freund robert schapire :10.1.1.32.8918
decisiontheoretic generalization line learning application boosting 
journal computer system sciences august 
gorin riccardi wright 
may help 
speech communication october 
lang 
newsweeder learning filter netnews 
proceedings twelfth international conference machine learning pages 
david lewis jason catlett 
heterogeneous uncertainty sampling supervised learning 
machine learning proceedings eleventh international conference 
david lewis william gale 
training text classifiers uncertainty sampling 
seventeenth annual international acm sigir conference research development information retrieval 
schapire gupta riccardi bangalore alshawi douglas 
combining prior knowledge boosting call classification spoken language dialogue 
international conference speech signal processing 
robert schapire yoram singer 
improved boosting algorithms confidence rated predictions 
machine learning december 
robert schapire yoram singer :10.1.1.134.3024
boostexter boosting system text categorization 
machine learning may june 
