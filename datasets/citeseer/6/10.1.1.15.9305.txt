modeling adaptive autonomous agents pattie maes mit media laboratory ames street rm cambridge ma pattie media mit edu category researchers arti cial life concerned modeling building called adaptive autonomous agents 
autonomous agents systems inhabit dynamic unpredictable environment try satisfy set time dependent goals motivations 
agents said adaptive improve competence dealing goals experience 
autonomous agents constitute new approach study arti cial intelligence ai highly inspired biology particular ethology study animal behavior 
research autonomous agents brought new wave excitement eld ai 
re ects state art new approach 
attempts extract main ideas evaluates contributions far identi es current limitations open problems 
new wave emerged study arti cial intelligence ai 
time popular general belief ai failure insiders believe exciting happening new life brought eld 
new wave termed autonomous agent research behavior ai opposed main stream knowledgebased ai bottom ai versus top ai 
term animat approach shorthand arti cial animal coined wilson frequently 
people de nitions written overviews research autonomous agents brooks wilson meyer 
reasons giving try 
researchers skeptical approach 
claim isn di erent doing 
convinced approach founded scienti second reason account di erent papers listed 
brooks main originators new approach presents picture restricted robotic forms intelligence 
presents general perspective 
argues autonomous agent class problems require system autonomously ful ll goals dynamic unpredictable environment 
includes applications virtual actors interactive training entertainment systems interface agents process scheduling 
wilson account focuses scienti methodology research autonomous agents meyer aims give research performed far 
third reason approach number years time perform critical evaluation 
discusses basic problems research adaptive autonomous agents 
presents overview evaluation state art eld 
particular identi es general speci open problems remain solved 
overview papers necessarily biased 
biased research adaptive autonomous agents taken place ai laboratory media laboratory massachusetts institute technology 
structured follows 
section introduces concept adaptive autonomous agent de nes basic problems eld trying solve 
section discusses guiding principles research adaptive autonomous agents 
section identi es common characteristics solutions proposed 
section discusses example state art agents stemming di erent application domains mobile robotics interface agents scheduling systems 
section presents critical overview state art 
discusses main architectures proposed building agents 
particular addresses progress models action selection models learning experience 
section presents 
adaptive autonomous agent 
agent system tries ful ll set goals complex dynamic environment 
agent situated environment sense environment sensors act environment actuators 
agent goals take di erent forms goals particular states agent tries achieve selective reinforcement reward agent attempts maximize internal needs motivations agent keep certain viability zones 
agent called autonomous operates completely autonomously decides relate sensor data motor commands away goals attended successfully 
agent said adaptive able time agent better achieving goals experience 
notice continuum ways agent adaptive able adapt short term smaller changes environment dealing signi cant long term lasting changes environment able change improve behavior time 
depending type environment inhabits agent take di erent forms 
agents inhabiting physical world typically robots 
example agent autonomous vacuuming robot 
agents inhabiting cyberspace environment consisting computers networks called software agents interface agents 
example agent system navigates computer networks nd data particular nature 
agents inhabit simulated physical environments 
example agent synthetic actor computer animated world 
combinations types agents may exist 
example alive interactive environment animated virtual agents employ real sensors camera decide react person movements gestures 
arti cial intelligence aims study intelligence synthesizing arti cially intelligent systems mainstream ai far concentrated problems di erent modeling adaptive autonomous agents 
key points distinguish traditional ai study autonomous agents 
traditional ai focussed systems demonstrate isolated advanced competences medical diagnosis chess playing 
traditional ai systems provide depth width competence 
contrast autonomous agent multiple integrated competences 
typically competences lower level competences 
robot competences locomotion navigation keeping battery charged collecting objects systems simple competences reacting market system simple bidding buying behaviors executing simple software routine case interface agent 

traditional ai focussed closed systems direct interaction problem domain encode knowledge solve problems 
connection environment isvery controlled indirect human operator 
operator recognizes problem domain describes system symbolic language system understands 
system returns symbolic description answer solution implemented operator actual domain 
contrast autonomous agent open system 
agent situated environment 
directly connected problem domain sensors actuators 
ect change domain actuators 
problem domain typically dynamic means system limited amount time act unpredictable events happen 
typically incorporates acting agents human arti cial 

traditional ai systems deal problem time 
problem system solve system human operator 
system time constraints solving problem deal interrupts operator deal problems 
system point view problem domain change system computing 
contrast agent autonomous system completely self contained 
monitor environment gure problem goal addressed deal problems timely fashion 
typically agent deal con icting goals simultaneously 

traditional ai focuses question knowledge system 
ai systems declarative knowledge structures model aspects domain expertise 
internal structures apart interpreter static 
system active problem posed human operator case interpreter uses static knowledge structures determine solution problem 
contrast emphasis autonomous agent research behavior system demonstrates put environment 
internal structures agent dynamic behavior producing modules opposed static knowledge structures 
initiated goal formulated user 
important agent answer questions problem domain solves particular problem 
important user able inspect internal structures identify responsible particular aspects resulting behavior 
example acceptable goals plans emergent observable properties attributed particular internal structure result complex interaction set structures environment 

traditional ai usually concerned developmental aspect question knowledge structures got rst place change time 
tobe situations components breaking 
done traditional machine learning assumes lot background knowledge available 
background knowledge system knowledge reformulation knowledge compilation explanation learning concept learning 
contrast autonomous agent research strong emphasis adaptation developmental approach 
means system improves internal structures behavior time experience environment 
agent actively explores updates structures incremental inductive learning method 
cases means designer takes incremental approach building agent user gradually evolves sophisticated system adding structure existing working system 
main problem solved autonomous agent research come architecture autonomous agent result agent demonstrating adaptive robust ective behavior 
adaptive means agent improves goal achieving competence time 
robust means completely breaks demonstrates graceful degradation components agent fail unexpected situations happen 
ective means agent successful eventually achieving goals 
speci cally related subproblems solved 
problem action selection agent decide progress multiple time varying goals 
deal contingencies opportunities may arise 
arbitrate con icting goals 
deal noisy sensors actuators 
react timely fashion 

problem learning experience agent improve performance time experience 
decide exploit current best action versus exploring actions possibly discover better ways achieving goals 
incorporate feedback world internal behavior producing structures 
correct wrong ine ective behavior producing structures 
section discusses problems detail 
summary main goal research autonomous agents better understand principles organizations underlie adaptive robust ective behavior 
secondary goal develop tools techniques algorithms constructing autonomous agents embody principles organizations 
call totality set principles organization set tools algorithms techniques support architecture modeling autonomous agents 
things clear couple years exist architecture considered optimal respects better ones proposed 
goal research develop understanding architectures simple solution class agent problems 
speci cally problem class de ned terms particular characteristics agent resources memory sensors compute power particular characteristics tasks environment 
guiding principles study adaptive autonomous agents grounded important insights 
serve guiding principles research performed looking complete systems changes problems favorable way 
interaction dynamics lead emergent complexity 
rst realization viewing problem building intelligent system context things lot easier 
observation true levels 
intelligent functions modeled perception planning learning part complete intelligent system agent 
building systems integrated way developing modules implementing functions independently task lot easier 
example system learn rely planning cache computed plans reuse 
system sensors actuators perform tests environment need modeling environment inference reasoning 
system sensors easier job disambiguating natural language utterances related objects system currently perceives 

complete intelligent system part environment situated space 
implies need modeling world best model 
environment external memory example reminding system tasks performed ones perform 
environment usually particular characteristics exploited system 
example ces consist vertical walls horizontal doors typically particular size habitat constraints exploited system making task easier 

intelligent system situated space time 
implies system develop better task time particular task permit learning experience 
time allows construction iterative incremental solution problem example natural language system situated time need able disambiguate utterance 
engage discourse asking questions making particular remarks help gradually disambiguate speaker wants convey 

intelligent system typically part society 
agents environment dealing similar related problems 
need agent gure 
example mobile robot strategy closely person passing order achieve competence navigating ce environment bumping things 
maes kozierok report experiments agents learned perform certain tasks observing imitating users 
consequence ideas autonomous agent research concentrated modeling systems context 
expert systems research traditional ai concentrated hypothetical problems behavior ai agent research built real systems solve actual small problem concrete environment 
second major insight study autonomous agents founded interaction dynamics simple components lead emergent complexity see 
agent research founded belief shifting interaction domain opposed component domain easier solve problem building intelligent systems 
idea applies di erent levels 
interaction dynamics agent environment lead emergent structure emergent functionality 
idea inspired eld ethology 
stressed animal behavior understood sense context particular environment inhabits 
braitenberg convincingly illustrated similar idea book vehicles 
ai simon referred idea discussed example ant beach 
notes complexity ant behavior re ection complexity environment internal complexity 
think true human behavior 
years agre showed behavior complex goal directed action sequences modeled emergent property situatedness time cuts ways means agent react timely fashion able deal interrupts 
interaction dynamics complex environment re agent 
means internal structures controlling agent need complex produce complex resulting behavior 
su cient study particular properties environment nd interaction loop set feedback re ex mechanisms produce desired behavior 
consequence need better understanding particular characteristics environment 
want able understand prove aspects resulting performance autonomous agents model agent aswell environment 
consequence need better models interaction dynamics agent components agent environment 

simple interaction dynamics di erent components agent lead emergent structure emergent functionality 
example mataric wall robot single component expertise wall attributed 
module responsible steering robot wall distance wall threshold module responsible steering robot away wall distance threshold 
modules primarily responsible wall behavior 
interaction dynamics robot reliably 
maes networks component modules responsible action selection 
action selection behavior emergent property activation inhibition dynamics primitive components system 

interaction dynamics component agents social system lead emergent structure functionality 
deneubourg describes social insects simple local rules produce emergent complexity path food source food foraging trees malone collection autonomous bidding systems addresses complicated task process processor allocation 
studied di erent concepts complex methods solving problems gradually shaped social interaction di erent people 
important emergent complexity robust exible fault tolerant programmed top organized complexity 
case components really charge producing complexity 
components critical 
breaks system demonstrates graceful degradation performance 
components interact parallel system able adapt quickly environmental changes 
system explores multiple solutions parallel soon certain variables change system able switch alternative way doing things 
example maes system sequences actions evaluated parallel best determining behavior agent 
malone system mappings processes machines viewed explored parallel 
characteristics agent architectures architectures autonomous agents proposed characteristics common 
section lists shared characteristics contrasts characteristics traditional ai architectures 
di erences illustrated means concrete examples section 
task oriented modules traditional ai intelligent system typically decomposed functional modules perception execution natural language communication peripheral components learner planner inference engine central systems components 
modules typically developed independently 
rely central representation means interface 
central representation includes things beliefs updated perception component processed augmented inference engine natural language component desires goals intentions produced planner 
contrast agent viewed set competence modules called behaviors 
modules responsible particular small task oriented competence 
modules directly connected relevant sensors actuators 
modules interface extremely simple messages common representation beliefs 
communication modules broadcast nature happens basis 
typically messages consist activation energy simple suppression inhibition signals simple tokens restricted language 
addition communication simple messages modules communicate environment 
module may change aspect environment trigger module task speci solutions traditional ai di erent functional components system modeled general domain independent possible 
hope functional components di erent problem domains general domain independent planner learner 
component needs adapted central representation contains domain speci information model particular environment hand possibly heuristic knowledge 
contrast agent general task independent functional modules 
general perception module general planner competence modules responsible doing representation computation reasoning execution necessary particular competence responsible 
example obstacle avoidance module need bit information represent obstacle perceived critical range 
simple computation decide obstacle avoided 
competence modules self contained black boxes 
employ completely di erent techniques di erent hardware achieve competence 
part reason pragmatic approach pessimistic vision possible come general solution vision problem general solution planning problem view expressed minsky 
role representations de emphasized traditional ai key issue emphasized agent complete correct internal model accurate copy environment objects relationships inside system system rely predict problems solved 
contrast agent research little emphasis modeling environment 
central representation shared modules 
system attempt integrate information di erent sensors coherent objective interpretation current situation 
task oriented module locally represents needs represent toachieve competence 
localized representations different modules related inconsistent redundant 
competence module usage representations may minimized favor employing environment source information determiner action 
representations module propositional objective declarative nature employed traditional ai 
example index objects features properties signi cant task hand identities objects 
numeric procedural analog nature 
lot task speci problem solving performed perception part particular competence 
decentralized control structure traditional ai adopts sequential organization di erent modules system 
modules take turns active processing changing internal representations 
perception inference rst update internal model beliefs goals 
planning problem solving produce description solution problem plan answer question 
execution module human operator implements solution domain having knowledge understanding situation 
contrast agent architectures highly distributed decentralized 
competence modules agent operate parallel 
modules control modules 
simple arbitration method included order select fuse multiple con icting actuator commands commands di erent modules exclusive 
arbitration network winner take network hardcoded priority scheme 
distributed operation agent able react quickly changes environment goals system 
goal directed activity emergent property traditional ai models activity result deliberative thinking process 
central system evaluates current situation represented internal model uses search process systematically explore di erent ways situation changed achieve goal situation 
contrast agents activity modeled result deliberative process 
complex goal directed activity modeled emergent property interaction competence modules internally competence modules environment 
internal structure corresponding plan system 
agents explicit goals driven speci set xed goals 
architectures agent explicit representation possibly time varying goals modify priorities di erent modules time 
role learning development traditional ai learning typically consists compilation reformulation system knows 
example system cache plan reuse 
seldom system perform inductive learning new information corrective learning existing knowledge environmental experimentation feedback 
implies programmer completely responsible creating initial complete correct model system 
contrast learning development considered crucial aspects adaptive autonomous agent 
building adaptive system develop successful system achieves tasks considered better approach building successful system change environment task changes robot breaking legs 
systems evolution increasingly sophisticated adaptive behavior simulated programmer incrementally adding structure existing successful systems 
systems employ learning individual 
cases system concentrates learning new information behavior environment reformulating information 
learning algorithms implemented distributed way typically similar learning algorithm runs di erent competence modules 
related idea learning redundancy system multiple modules particular competence 
experience sorts modules implements competence reliable way preferred 
systems built principles task oriented modules solutions de emphasized representations decentralized control tend demonstrate adaptive robust behavior 
act react quickly fewer layers information processing distributed non synchronized require expensive computation prone problems combinatorial explosions rely search processes 
able adapt unforeseen situations opportunities contingencies rely environment source information determiner action possibly faulty outdated model 
robust modules critical attempt fully understand current situation time consuming problematic incorporate redundant methods adapt time 
example autonomous agents mobile robot consider mobile surveillance robot monitor ces 
task requires navigate room room 
traditional ai version robot similar way shakey 
perception module processes di erent sensor data integrates representation environment 
attempts update model possible 
model includes information location robot environment location type identity objects environment tables model planning module decide ful ll goal nding door current room avoiding obstacles 
planner goes systematic search produce list actions model ful ll goals 
execution module executes plan possibly checking certain points things going predicted 
control returned planner 
adaptive autonomous agent task constructed way inspired 
incremental way modules implemented corresponding di erent competences necessary task module recognizing going doors module wall module obstacle avoidance couple redundant ones di erent sensors critical competence 
modules operate parallel 
simple arbitration scheme example suppression inhibition wires modules su ces implement desired priority scheme obstacle avoidance modules priority going doors priority wall 
robot plan course action 
observer point view appear operate systematic rational way 
brooks argued convincingly writing demonstrations robots successful dealing task robust reliable way interface agent consider problem building intelligent autonomous system helps user certain computer tasks 
goal er assistance user automate actions user possible 
traditional ai approached problem way 
system elaborate amount problem domain knowledge engineer model user possibly user organization model tasks user engages including hierarchical speci cation subtasks knowledge vocabulary tasks 
runtime agent uses knowledge recognize intentions plans user 
example unix user enters command emacs tex system infers user planning produce written document 
plans course action goal assist user example consist action sequence text formatting command latex tex followed preview command dvi printing command lpr dvi 
problems approach exactly ones traditional ai robots hard provide complete consistent model model quickly outdated user ways performing tasks change 
computational complexity approach system react slowly 
sorts unpredicted ideally robot monitor results actions learn experience improve competence deal signi cant changes robot environment demonstrate robust ective autonomous behavior longer periods time 
events take place system deal example user change mind middle things perform tasks unorthodox non rational ways adaptive autonomous interface agent built follows 
competence modules constructed experts try experts small aspect task 
example module responsible invoking particular command lpr particular moment 
agent situated environment containing ideal source learning user behavior 
modules gathers information observing user keeping statistics particular aspect user behavior 
example mentioned module keep track situations user executed lpr command 
new situation comes similar memorized situations ers user execute lpr command 
experts di erent commands listed know active er assistance user 
observer point view system understands intentions user knows task producing document involves 
action sequences just emergent property distributed system 
system smoothly adapt changing habits user react fast way completely break 
system consider problem building scheduling system goal allocate processes processors real time 
domain dynamic new processing jobs formulated di erent machines time 
decision run processes locally di erent machine global goal minimize average amount time takes run process 
loads di erent available machines vary continuously 
certain machines suddenly unavailable scheduling processes requiring rescheduling jobs running machines time 
traditional ai system task contain lot knowledge scheduling particular con guration machines typical processing jobs hand 
system update representation current situation possible 
requires gathering data di erent machines network available workload processes running new processes formulated information gathered system perform systematic search possibly involving heuristics optimal allocation processes processors 
schedule produced processing jobs sent di erent machines assigned 
centralized way solving problem majority earlier area 
malone proposed di erent solution problem call agent 
enterprise system machines network autonomous charge load 
system metaphor market 
machine new processing task originates sends requests bids task done 
machines may respond bids giving estimated completion times re ect speed currently loaded les 
example task performed graphics rendering job machine software loaded better bid new job time space loading necessary software 
machine sent request bids collect bids receives small period time allocate job machine best bid remote local 
distributed scheduling method advantages 
system robust machines critical central scheduler 
user machine unavailable processing external jobs run time 
system adapt smoothly unexpected situation 
solution simple exible terms di erent factors take account 
overview state art section general overview agent approach building intelligent systems demonstrate adaptive robust behavior 
section provides detailed account speci architectures proposed 
addition lists limitations open problems particular architectures proposed 
section argued subproblems involved modeling adaptive autonomous agents problem action selection problem learning experience 
section structured subproblems 
architectures agents proposed far concentrated subproblem agent combines simplistic action selection sophisticated learning demonstrates sophisticated action selection doing learning 
proposals addressed problems architecture 
remainder section detailed description subproblems followed discussion progress discussion questions remain unresolved 
action selection problem problem action selection stated follows 
agent multiple time varying goals repertoire actions performed executable speci sensor data actions agent take optimize achievement goals notice consider learning experience problem slightly di erent goals agent learn better achieve goals 
theoretically possibly compute optimal action selection policy agent xed set goals lives deterministic probabilistic environment 
impossible real agents agent deal resource limitations time computation memory ii possibly incomplete incorrect information sensor data iii dynamic non deterministic non probabilistic environment iv time varying goals unknown possibly changing probability distributions 
goals agent tries satisfy take di erent forms goals called goals attainment states achieved negative goals states avoided needs drives desires tasks motivations constraints plan viability zones certain state variables agent typically multiple con icting goals 
complete system combination self preservation goals bump obstacles keep battery charged task oriented goals watch set ces 
goals agent implicit explicit 
case agent explicit internal representation goals trying achieve 
agent built away situated environment behavior tends achieve certain goals 
implicit goals necessarily xed 
changed agent reprogrammed 
complicated agents explicit goals vary time levels intensity opposed boolean nature 
example arti cial animal particular hunger level thirst level theoretically impossible prove optimal action selection policy agent eld evaluate particular proposed solution 
researchers adaptive autonomous agents interested provable optimality action selection agent takes optimal path goals action selection robust adaptive agent achieves goals requirements constraints imposed particular environment task hand 
issues means action selection mechanism favor actions contributing goals particular favor actions result progress goals able deal opportunities contingencies de nition substitute word competence module behavior action set competence modules try control actuators particular moment time ones priority outputs combined command actuators 
real time fast particular environment hand pace changes minimize unnecessary switching back forth actions contributing distinct goals improve basis experience section demonstrate graceful degradation components break unexpected changes happen get completely stuck loop deadlock situation agent pursue unachievable goal importantly environment task hand long agent manages achieve goals constraints time quality required problem situation solution considered acceptable 
example long robot manages nd recharging station battery dies su cient progress task speci goal surveying ces considered acceptable solution follow optimal paths 
brooks refers criterion adequacy 
mcfarland takes point 
argues ecological approach toevaluate agent behavior agent lls market niche agent considered successful 
mcfarland view agent behavior adaptive means optimize behavior respect selective pressures market place 
ultimately true particularly useful means comparing di erent proposals agent architectures 
tyrrell compares action selection proposals respect particular benchmark environment task 
maes wilson argued possible decide action selection model better mentions particular characteristics environment task agent 
example environment cost incorrect actions high agent anticipation cost incorrect actions matter agent performs incorrect actions environment lot things change quickly agent needs act quickly agent noisy sensors inertia action selection wrong sensor reading agent switch doing completely new di erent agent sensors rely environment guide selection actions agent fewer sensors need rely internal state memory decide 
todd wilson littman started build taxonomy environments taxonomy agents provide profound basis comparing di erent proposals 
progress di erent models action selection autonomous agent proposed di er way deal problems 
nature goals 

nature sensor data 

arbitration mechanism command fusion mechanism 
architectures proposed subdivided classes hand built flat networks 
anumber architectures proposed require designer agent solve action selection problem scratch agent built 
examples architectures subsumption architecture architectures reported minimalist version subsumption architecture 
architectures require designer agent carefully analyze environment task hand design set re ex modules way combining outputs modules means suppression inhibition wires simple arbitration circuitry 
class architectures deals problems way 
typically goals implicit exist may exist designer mind 
agent multiple goals di erent nature 
nature sensor data unlimited 
arbitration mechanism determining modules steer actuators implemented logical circuit set suppression inhibition wires 
circuit ensures module controls actuator times 
architectures support command fusion 
words action selection models possible modules simultaneously determine command sent actuators 
example possible average outputs modules 
disadvantage class solutions don er user guidance solve problem action selection new agent 
architecture provides philosophy set previous successful examples programming language building new agents 
disadvantage class solutions solution scale 
complex agents problem action selection arbitration modules hard solved hand 
arbitration network complicated spaghetti hard debug get right thing 
nal disadvantage architectures allow time varying goals typically goals explicitly represented agent 
compiled flat networks 
second class architectures attempts facilitate construction agents automating process designing arbitration circuitry competence modules 
examples architectures rex gaps system behavior networks reactive trees 
architectures require designer specify particular formalism goals agent goals reduced goals actions di erent modules actions conditions expected ects 
compiler analyzes speci cation generates circuit implement desired goal seeking behavior 
kaelbling rosenschein types goals sensors dealt restricted booleans 
hand restricts type agent practically built hand restrictions possible prove circuitry synthesized agent select right actions ful ll goals 
types architectures produce agents implicit xed time varying goals 
contrast previous class architectures goals explicit designer formal speci cation agent 
implies agent circuitry resynthesized agent ful ll di erent set goals 
maes proposes architecture explicit time varying goals 
arbitration network compiled explicit representation goals agent goals vary time hunger level arti cial animal motivation recharge battery robot 
particular system performs limited form arbitration prediction planning run time 
speci cally processes modeled terms time varying spreading activation process activation energy accumulate modules relevant particular goals intensities sensor data hand 
unfortunately system sensor data restricted booleans 
disadvantages class action selection architectures class agents built restricted 
case architectures er particular model action relevance previous category impose model 
second problem hard come declarative speci cation goals desired behavior agent 
agent action selection speci cation relies 
designer speci cation ects actions erroneous agent behavior desired 
complicated build certain kinds agents 
hand built hierarchical networks 
nal category action selection models proposes hierarchical organization di erent actions competence modules 
examples architectures rosenblatt payton sophisticated version subsumption architecture tyrrell :10.1.1.17.8228
architectures closely inspired models animal behavior stemming lorenz tinbergen 
typically architectures organize actions hierarchy ranges high level modes activities mid level composite actions detailed primitive actions 
primitive actions executable 
tyrrell blumberg demonstrated scaling problem complex agents di erent goals actions desirable structure networks may help decide actions relevant :10.1.1.17.8228:10.1.1.17.8228:10.1.1.17.8228
typically systems sort action selection higher abstraction levels prime bias selection primitive actions 
category systems supports complex animal motivations goals 
model sensors ect action selection sophisticated 
example architectures possible stimulus sensor data certain quality intensity ect action selection just food food quality food stimulus perceived 
case rst class architectures discussed architectures require designer build arbitration network hand 
di cult tricky task particular ethology models tend lot parameters need tuned obtain desired behavior 
open problems lot progress study action selection models problems remain unresolved little research performed nature goals goal interactions 
need study kinds goals architectures need support goals come change time jensen nice overview di erent models motivations ethology psychology come 
architectures proposed scaling larger problems disaster 
especially case hand built networks support designer agent building complicated arbitration network govern behavior 
obvious solution investigated evolve learn adapt network experience 
experiments lines performed far 
related ort put making pieces agent networks reusable agents 
rst third category architectures reduce action selection problem nontrivial engineering problem useful partial solutions proven agent abstracted reused agent 
example modules producing wall behavior robot abstracted reused robot comparable sensors comparable environment 
noted authors dynamics interactions agent environment di erent modules agent understood 
kaelbling rosenschein er logical model beer kiss steels started approaching problem dynamical systems perspective 
eld far able prove general emergent behavior distributed network competence modules 
proposed architectures deal problem command fusion 
typically module time determines command sent actuator 
way outputs multiple modules combined 
proposals solutions problem :10.1.1.17.8228
architectures completely decentralized keep central state 
result may su er lack minsky call brain 
get stuck loops deadlock situations keep activating actions proven result change state 
architectures apart narrow minded view relationship perception action 
example architectures support active goal driven perception actions obtain di erent sensor data learning experience problem previous section discussed architectures adaptive autonomous agents focus problem action selection 
architectures neglect issue learning experience 
means agents built architectures adaptive restricted sense able deal unexpected situations opportunities contingencies 
agents learn environment feedback 
better achieving goals experience 
second category agent architectures proposed focussed behavior action selection agent improve time 
learning experience necessity agent demonstrate robust autonomous behavior long periods time 
case hard program agent 
practically proven impossible correctly complex agent come correct speci cation behavior environment 
second components agent may break environment permanent way may require run time reprogramming 
adaptive behavior viewed nal static point 
true adaptive behavior inherently dynamic continuous process 
spirit eld arti cial life view adaptive behavior emergent property long term interaction feedback process agent environment 
problem learning experience de ned follows 
agent set actions competence modules ii certain sensor data iii multiple time varying goals agent improve action selection behavior experience 
agent incorporate feedback receives action away action selection behavior improves 
improvement typically means agent successful ful lling goals needs 
depending nature agent goals may mean di erent things 
case attainment goal goal mean average time average number actions required measure cost achieve goal decreases time 
case reinforcement maximization type goal mean average positive reinforcement received xed length time interval increases experience 
matter type goals deal model learning autonomous agent ful ll desiderata learning incremental agent learn experience 
separate learning performance phase 
learning biased learning knowledge relevant goals 
complex realistic environments agent ord learn fact possibly learned 
learning model able cope noise probabilistic environments faulty sensors learning unsupervised 
agent learn autonomously 
preferably learning model possible give agent initial built knowledge learn scratch particular situations prior knowledge easily available 
subproblems dealt designing architecture learning agent 
action selection mechanism adopted 

system learn 
create hypotheses tested 
decide hypotheses worth keeping determine behavior agent 

agent decide exploit versus explore 
decide activate believes optimal action current situation versus try suboptimal action learn possible nd better way doing things 
experimentation strategy agent 
notice respect rst problems learning architectures proposed adopt naive limited view 
set goals dealt simple goals xed time 
detailed problems come solving problems 
example learning architecture deal problem credit assignment previously activated actions gets partial credit certain desirable undesirable result happening 
evaluate compare di erent proposals learning experience 
problem action selection comparing proposals hard general case 
problem learning experience ill de ned speci es particular characteristics environment agent task 
sense compare proposals respect particular class problems 
example agent lot memory better memory intensive learning method doing lot generalization come concise representation learned 
environments initial knowledge easily available means desirable agent partially programmable opposed learning scratch 
depending environment agent hand role learning may di erent 
environment predictable changes slower pace agent lifetime need learning agent lifetime 
sort evolution learning species level able deal long term adaptation required 
todd wilson rst steps agents may comparisons meaningful 
progress architectures proposed literature assume agent set primitive actions competence modules 
concentrate learning arbitration network di erent actions modules agent attempts learn certain action activated action get control actuators 
architectures proposed allow learning new composite actions composite competence modules 
allow agent independently learn composite modules arbitration network composite modules 
di erent architectures proposed grouped classes reinforcement learning systems classi er systems model learners 
second class architectures really special case rst 
lot research performed classi er systems research typically discussed reinforcement learning point view discuss classes separately 
classes de ne learning problem follows set actions reward signal learn mapping situations actions called action policy agent policy maximizes accumulated discounted reward receives time 
case model learning architecture agent learns model actions ect environment actions map situations situations 
independent agent learns infers importance value certain actions certain situations interesting combinations types architectures exist 
example systems combine learning action policy learning model 
case action selection models architectures proposed inspired theories animal learning 
contrast school animal behavior studies comparative psychology particular theories reinforcement learning operant conditioning inspiration computational models proposed 
reinforcement learning 
idea reinforcement learning 
agent set actions engage ii set situations nd iii scalar reward signal received agent learn action policy mapping situations actions agent follows action selection policy maximizes cumulative discounted reward receives time 
learning particularly popular reinforcement learning strategy 
learning agent tries learn situation action pair value action situation 
speci cally algorithm learns dimensional matrix stores value possible combination situation action 
initialization values set initial value 
goal system update values converge maximum cumulative discounted reward expected action situation means maximum cumulative reward agent expect receive takes particular action situation immediate reward receives plus reward receive best actions 
reward discounted respect rewards expected near count rewards expected road 
di erent subproblems listed dealt reinforcement learning systems way 
action selection mechanism 
moment agent nds particular situation 
situation chooses action maximum value maximum cumulative discounted reward 
learning method 
agent performs action may receive reward possibly zero 
updates value situation action pair just exploited 
particular increases decreases value pair better re ect actual reward received plus maximum reward expect new situation nds 

exploration strategy 
certain percentage situations agent choose action maximizes reward performs random action gather data evidence possibly interesting alternative paths 
attractive features reinforcement learning formal foundation 
proven certain conditions nite number trials markovian environment agent converge optimal action selection policy 
unfortunately conditions seldom attainable real complex situations 
disadvantages reinforcement learning algorithms deal time varying goals action policy learned xed set goals ii goals change scratch attempts overcome problem iii realistic applications size state space number pairs large learning takes time practical result researchers started developing algorithms generalize state space iv learning happens fringe state space reward received system start learning sequence actions leading reward result takes lot time learn long action sequences attempts deal problem model assumes agent knows times situation faulty sensors hidden states di cult addresses particular problem vi hard build initial knowledge type architecture nally vii model learn multiple actions taken parallel theory reinforcement learning deal parallel actions adopting row classi er systems 
second category architectures learning agents classi er systems 
particular wilson booker studied classi er systems build adaptive autonomous agents 
architectures viewed special case reinforcement learning systems 
agent attempts learn optimize reward receives certain actions certain situations 
idea agent set rules called classi ers data rule performance 
system keeps strength rule represents value rule 
subproblems learning architecture dealt way 
action selection mechanism 
certain situation includes external state sensor data may include internal state condition list match current situation 
matching classi ers agent picks classi ers proportional strength 
actions proposed classi ers executed internal state 

learning method 
classi ers executed give strength classi ers set stage classi ers just active previous timestep 
called bucket brigade algorithm designed deal problem credit assignment 
agent executes actions may receive reward 
case reward increase strength classi ers just activated activated suggested action 
scheme ensures classi ers contribute reward received time higher strengths don activated 

exploration strategy 
number classi ers xed 
agent removes classi ers low strengths replaces mutations recombinations crossovers successful ones 
way agent keeps exploring evaluating di erent ways doing things keeping solutions 
interesting aspects agents classi er systems sophisticated experimentation strategy experimentation strategy reinforcement learning systems consists picking random data point 
hypothesis underlying strategy matrix combination actions executed parallel 
practice blow state space case 
nd better solution problem ective behavior making small changes existing solution existing successful behavior recombining existing promising solutions 
advantage classi er systems reinforcement learning systems builtin generalization mechanism generalizing situations actions don care symbol 
possible classi er systems sample parts state space di erent levels abstraction nd representation classi er useful particular problem agent 
unfortunately classi er system agents share limitations reinforcement learning agents particular problem time varying goals problem learning fringe iv mentioned 
addition may su er problem keep track tried 
classi er system agent may re evaluate classi er 
may throw strength low immediately create keeps memory tried hand fact system forgets non promising classi ers cient action selection time 
model builders 
nal class agents learn experience learn causal model actions policy map 
drescher model inspired piaget theories development infants probably sophisticated example 
agent builds probabilistic model ects action certain situation 
causal model arbitration process decide action relevant certain situation certain set goals 
action selection learning decoupled fact learning component agents combined di erent action selection mechanism 
architectures agent learn complete mapping possible situation action pair new situation result action situation 
agent learns mappings partial situations partial situations 
maps aspects situation matter sensor data necessary su cient conditions combined action aspects new resulting situation matter particular sensor readings change action situations described conditions 
combination set conditions ii primitive composite action iii set expected results probabilities results called schema module behavior 
model learning architectures deal subproblems de ned way 
action selection mechanism 
agents built architectures deal time varying multiple explicit goals 
set goals intensities compute runtime modules schemas learned goal reliable 
separate value assignment process decoupled learning process 
value assignment process favors modules prove reliable 
may trade reliability sequence actions length sequence actions leading goals 
typically spreading activation process simple marker propagation process assign values goals sensor data 

learning method 
particular action taken agent monitors changes happen environment 
uses information learn correlations particular conditions action pairs certain results 
action taken result lists probabilities applicable modules schemas action taken matching condition list updated re ect new example 
occasionally agent needs spin new schemas existing ones able represent con icting unreliable results 
agent able detect conditions need taken account module certain results reliable force create versions module longer slightly di erent conditions 

exploration strategy 
exploration strategy architectures varies 
drescher system demonstrating sophisticated learning extremely simple exploration strategy random 
agent basically learn performing random experiments 
foner discusses drescher agent learn faster learn relevant knowledge adopting smarter experimentation strategy focus attention mechanism 
maes architecture exploration strategy goal oriented agent biases experimentation actions show promise contribute goals 
system amount exploration versus exploitation emergent property action selection system learned fewer experiments performed 
main advantages model learners transfer behavior learned context context goal 
system builds model action situation results situation model road map particular set goals 
better environments goals relative importances goals may change time 
implies just learn fringe state space connected goals 
learn action opposed learning actions proven directly related goals 
addition easier designer agent incorporate background knowledge domain form causal model ects actions 
agent able correct knowledge proves incorrect 
disadvantage type architecture may take time select action direct mapping situations optimal actions 
open problems case action selection models lot problems modeling learning experience remain unsolved 
problems apply mentioned learning approaches scaling larger realistic problems typically problem learning algorithms 
computational complexity learning systems discussed big practically useful build complex agents solve real problems 
reason case algorithms incorporated interesting attention mechanisms 
example foner demonstrates incorporating attention mechanisms spatial locality improve tractability learning experience signi cant way 
algorithms discussed temporal locality heuristic ects assumed perceived soon actions caused 
algorithms proposed bad generalizing sensor data 
sensor data represented level granularity opposed coarse ner levels 
second algorithms proposed exploit structure similarity sensor data exploit fact di erent cells retina adjacent di erent cells retina ected similar ways certain actions 
done domain exploration strategies 
existing algorithms employ simple strategy possible agent experiments certain percentage time matter urgent needs motivations may matter interesting opportunities agent picks experiment perform random way opposed certain heuristics trying actions tried ii trying actions shown promise lack models learning perception interact 
model perception architectures narrow minded 
set sensor data agent tries correlate actions taken 
system couple learning actions learning perception 
learn pay attention learn features paid attention 
ideally agent create new features categories perceive environment categories goals environment require grow vertical edges develop detectors horizontal edges 
lack sophisticated models action selection learning interact 
particular current algorithms assume set primitive actions agent learns 
case sensor data sense set primitive actions learned basis discretization subdivision continuous space possible actions appropriate environment goals hand 
need understand better role learning adaptive phenomena cultural learning adaptation evolution see 
need better understand building blocks evolution provide facilitate learning provide built bias learning built specialized structures 
approaches taken inspired comparative psychology ethology 
lot learned inspired approach learning 
example shown animals built sensitive periods learning particular competences 
periods tend coincide situations lives optimal picking competence learned reduce complexity learning task 
autonomous agent research represents exciting new approach study intelligence 
far new approach demonstrated proofs concept 
particular encouraging successes reported area mobile robots software agents 
prototypes built solved real task previously solvable solvable means costly ective solution 
approach de nitely impact course arti cial intelligence witnessed explosion publications research projects area 
problems apparent require novel ideas better solutions 
main problem identi ed scaling approach larger complicated systems 
tools techniques proposed provide su cient support design hand build complex agent di erent goals 
learning techniques proposed computational complexities automated development adaptive agent problem realistic time 
addition order approach founded fundamental research undertaken 
need understand classes problems agents deal possible critically compare particular architectures proposals 
example di erent models action selection proposed understand problem action selection better grounds compare di erent proposals 
aside better evaluation criteria need better understanding underlying principles 
particular important understand mechanisms limitations emergent behavior 
globally desired structure functionality designed basis interactions simple modules 
conditions limitations emergent structure stable 
rst steps theory emergent functionality proposed tools complex dynamics 
far proposed theories applicable simple toy examples 
tension inherent agent approach unresolved 
research autonomous agents adopted task driven pragmatic solutions 
result agents built approach looking bag hacks tricks embodiment set general laws principles 
mean eld evolve systems engineering discipline nd path scienti discipline 
bruce blumberg leonard foner chris langton lashkari maja mataric stewart wilson provided valuable comments earlier draft 
chris langton provided necessary encouragement demonstrated lot patience 
agre dynamic structure everyday life cambridge university press 
ballard frames animate vision proceedings ijcai conference detroit 
bates loyall reilly broad agents proceedings aaai spring symposium integrated intelligent architectures stanford ca 
available sigart bulletin vol 
pp 
beer sterling biological perspective autonomous agent design designing autonomous agents theory practice biology engineering back edited maes mit press bradford books 
beer dynamical systems perspective autonomous agents technical report ces department computer engineering science case western reserve university 
belew evolution learning culture computational metaphors adaptive algorithms ucsd computer science engineering department cse technical report cs 
blumberg action selection lessons ethology submitted third international conference simulation adaptive behavior brighton august :10.1.1.17.8228
booker classi er systems learn internal world models machine learning journal volume number 
braitenberg vehicles experiments synthetic psychology mit press bradford books 
brooks robust layered control system mobile robot ieee journal robotics automation ra april 
brooks intelligence reason computers thought lecture proceedings ijcai sidney australia 
brooks challenges complete creature architectures animals animats proceedings international conference simulation adaptive behavior edited meyer 
wilson mit press bradford books 
brooks arti cial life real robots practice autonomous systems proceedings european conference arti cial life edited varela bourgine mit press bradford books 
chapman vision instruction action mit press 
connell minimalist mobile robotics academic press 
deneubourg goss franks franks dynamics collective sorting robot ants ant robots animals animats proceedings international conference simulation adaptive behavior edited meyer 
wilson mit press bradford books 
deneubourg theraulaz beckers swarm architectures practice autonomous systems proceedings european conference arti cial life edited varela bourgine mit press bradford books 
drescher minds constructivist approach arti cial intelligence mit press 
foner paying attention important focus attention improve unsupervised learning 
submitted third international conference simulation adaptive behavior brighton august 
holland escaping brittleness possibilities general purpose learning algorithms applied parallel rule systems machine learning arti cial intelligence approach volume ii edited michalski carbonell mitchell morgan kaufmann 
holland optimal allocation trials chapter adaption natural arti cial systems mit press bradford books 
horswill characterizing adaptation constraint practice autonomous systems proceedings european conference arti cial life edited varela bourgine mit press bradford books 
horswill specialization perceptual processes phd thesis ai laboratory mit 
kaelbling rosenschein action planning embedded agents designing autonomous agents theory practice biology engineering back edited maes mit press bradford books 
kaelbling adaptable mobile robot practice autonomous systems proceedings european conference arti cial life edited varela bourgine mit press bradford books 
kaelbling learning embedded systems mit press 
kaelbling learning achieve goals proceedings ijcai thirteenth international joint conference arti cial intelligence morgan kaufman 
kiss autonomous agents ai chaos theory animals animats proceedings international conference simulation adaptive behavior edited meyer 
wilson mit press bradford books 
kleinrock nilsson optimal scheduling algorithms time shared systems journal acm july 
koza evolution evolution computer programs control independently acting agents animals animats proceedings international conference simulation adaptive behavior edited meyer 
wilson mit press bradford books 
lin reinforcement learning robots neural networks phd thesis carnegie mellon university school computer science 
littman optimization categorization reinforcement learning environments 
animals animats proceedings second international conference simulation adaptive behavior edited meyer roitblat wilson mit press bradford books 
maes situated agents goals designing autonomous agents theory practice biology engineering back edited maes mit press bradford books 
maes designing autonomous agents theory practice biology engineering back mit press bradford books 
maes brooks learning coordinate behaviors proceedings aaai boston 
maes adaptive action selection proceedings thirteenth annual conference cognitive science society lawrence erlbaum associates 
maes bottom mechanism behavior selection arti cial creature animals animats proceedings international conference simulation adaptive behavior edited meyer 
wilson mit press bradford books 
maes learning behavior networks experience practice autonomous systems proceedings european conference arti cial life edited varela bourgine mit press bradford books 
maes alive arti cial life interactive video environment visual proceedings siggraph conference acm press 
maes kozierok learning interface agents proceedings aaai eleventh national conference arti cial intelligence mit press 
mahadevan connell automatic programming behavior robots reinforcement learning proceedings ninth national conference arti cial intelligence mit press 
malone fikes howard enterprise market task scheduler distributed computing environments ecology computation edited huberman north holland 
mataric behavioral synergy explicit integration special issue cognitive architectures volume number 
mataric integration representation goal driven behavior robots ieee transactions robotics automation vol 
june 
mcfarland means robot behavior adaptive animals animats proceedings international conference simulation adaptive behavior edited meyer 
wilson mit press bradford books 
meyer 
simulation adaptive behavior animats review prospects animals animats proceedings international conference simulation adaptive behavior edited meyer 
wilson mit press bradford books 
minsky society mind simon schuster new york 
nilsson shakey robot sri center technical note 
nilsson agent programs circuit semantics department computer science report number stan cs stanford university january 
payton rosenblatt works robust approach tolerant autonomous control journal applied intelligence vol 

resnick centralized mindset explorations massively parallel phd thesis mit media laboratory epistemology learning group 
rosenblatt payton fine grained alternative subsumption architecture mobile robot control proceedings international joint conference neural networks ijcnn washington dc june 
sheth maes evolving agents personalized information filtering proceedings ieee conference arti cial intelligence applications ieee press 
active language collaborative development cooking skill proceedings cognitive science conference lawrence erlbaum 
simon sciences arti cial mit press 
steels cooperation distributed agents self organization decentralized ai edited demazeau 
muller elsevier north holland 
steels exploiting analogical representations designing autonomous agents theory practice biology engineering back edited maes mit press bradford books 
steels theory emergent functionality animals animats proceedings international conference simulation adaptive behavior edited meyer 
wilson mit press bradford books 
suchman plans situated actions problem human machine communication cambridge university press 
sullivan tyler 
editors intelligent user interfaces acm press 
sutton integrated architectures learning planning reacting approximating dynamic programming proceedings seventh international conference machine learning 
sutton reinforcement learning architectures animats animals animats proceedings international conference simulation adaptive behavior edited meyer 
wilson mit press bradford books 
jensen ethological psychological models motivation synthesis animals animats proceedings international conference simulation adaptive behavior edited meyer 
wilson mit press bradford books 
todd wilson environment structure adaptive behavior ground 
animals animats proceedings second international conference simulation adaptive behavior edited meyer roitblat wilson mit press bradford books 
travers animal construction kits arti cial life edited langton addison wesley 
tyrrell de ning action selection problem proceedings th conference cognitive science society 
tyrrell computational mechanisms action selection phd thesis centre cognitive science university edinburgh 
watkins learning delayed rewards phd thesis king college cambridge 
wilson knowledge growth arti cial animal proceedings international conference genetic algorithms applications edited gre lawrence erlbaum associates 
wilson animat path ai animals animats proceedings international conference simulation adaptive behavior edited meyer 
wilson mit press bradford books 
whitehead ballard active perception reinforcement learning proceedings seventh international conference machine learning austin texas 

