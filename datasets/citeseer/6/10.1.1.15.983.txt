choice basis laplace approximation david mackay cavendish laboratory cambridge cb united kingdom 

cam 
ac 
uk submitted machine learning october th accepted pending minor modifications february rd version completed may th published volume october maximum optimization parameters laplace approximation marginal likelihood basis dependent methods 
note compares choices basis models parameterized probabilities showing possible improve traditional choice probability simplex transforming softmax basis 
laplace method approximates integral function fitting gaussian maximum computing volume gaussian 
method widely probabilistic modelling approximate value marginal interest model comparison ripley lindley smith spiegelhalter mackay chickering heckerman 
consider case models parameters probabilities example hidden markov models mixture models belief networks certain language models 
examine neglected issue choice basis laplace approximation 
known location maximum density invariant respect non linear reparameterization parameters clearly laplace integral invariant 
choice basis important areas statistical modelling people rarely consider changing obvious basis 
intuitively going laplace approximation try model density near possible gaussian 
models discussed likelihood function simple belief network hidden nodes product factors unknown probability vector form fip iip probability vector components vector counts fi number times outcome occurred sampled distribution discuss simplest case network node take possible values likelihood factor 
example system mind bent sided die probability rolling pi 
unknown vector inferred outcome rolls face came fi times wish calculate marginal likelihood depends prior distribution popular prior probability vector dirichlet distribution hagan parameterized measure vector coefficients ui dirichlet plu 
plu pi ei pi function dirac delta function simply restricts distribution simplex normalized ipi distribution restricted non negative pis 
normalizing constant dirichlet distribution ii ui define ui 
similarly define fi 
hyperparameter vector controls compact prior distribution large distribution concentrated mean distribution ui components small extreme large small probabilities expected 
case bent die model assuming dirichlet prior posterior probability data flp plu fiu lu dirichlet pl 
predictive distribution probability outcome ilf dirichlet pl pi dp fi ui belief networks hidden variables mixture models likelihood function obtained summation hidden variables dirichlet priors widely models 
traditional map method models lee gauvain maximize posterior probability parameters traditional laplace method model maximizing basis gaussian approximation basis chickering heckerman 
obvious difficulty prior ui posterior density diverge edge parameter space parameter pi equal zero sum ui effective count fi outcome 
laplace approximation valid maximum smooth hump happens traditional method trouble 
traditional solution problem forbid dirichlet priors ui gelman may reasons expecting priors ui problems 
argue terms traditional posterior probability artefacts choice basis 
change basis suggest maximum posteriori parameter estimation laplace approximations better conducted softmax representation widely neural networks bridle parameters replaced parameters exp ai pi ei exp ai 
please confuse function defined equation probability density 
probability vector components degrees freedom sum pi 
similarly components redundant degree freedom addition arbitrary multiple 
leaves unchanged 
free constrain degree freedom wish 
softmax basis dirichlet prior may written alu zs pi lo arbitrary density constraining redundant degree freedom zs appropriate normalizing constant 
prior longer terms exponents transformation space space proportional see appendix 
straightforward evaluate derivatives curvatures respect respect consider log likelihood ip example 
defining ei exp ai logz flog fi 
derivative curvature ol fi fpi oai opi 
note probable basis likelihood dirichlet prior alu amp pi amp fi ui recognize equivalent predictive distribution 
softmax basis traditional basis map parameter vector conveniently equal mean 
density density traditional density density softmax approximation bl intuition defect traditional laplace approximation possible advan tage softmax basis 
shows case binary probability alternatively represented parameter 
density dirichlet pl traditional space cr 
density approx gaussian 
note poorly gaussian tails match true density 
bl density transformed space cr 
gaussian approximation space 
cases gaussian approximation matching derivatives ui er log maximum 
gaussians variances respectively ui 
vertical lines equally spaced vertical lines show corresponding values comparison gives intuitive picture expect softmax representation superior representation gaussian approximations 
case traditional gaussian approximation puts probability mass outside interval inside 
curvature true density diverges 
function contrast density singularities 
gaussian approximation softmax basis imperfect dirichlet distribution falls exponentially large softmax gaussian approximation light tailed 
quantitative comparison traditional laplace approximation softmax laplace approximation simple case exact marginal likelihood computed 
case discussed inferring probability vector bent sided die rolls die 
test approximations cases prior matched source data cases prior data variance 
exact answer assume prior observe samples obtaining counts 
fr posterior probability distribution equation obtain marginal likelihood fiu equation section define ui 
softmax laplace approximation iii ri ui hi ui assume gaussian prior unconstrained direction space distribution alu exp zs approximate zs laplace method 
know maximum corresponding ibi ui curvature matrix require equation mij logp alu ua 
determinant readily evaluated identities der gg der gta tg gta tg obtain zs hp det pi hi ui ev independent ready approximate marginal likelihood traditional laplace approximation need version laplace approximation density multiplied delta function xt 
det flu flu fdp flp plu iv det fi ui aij pi ri ui log pi nta det ii pi traditional laplace method gives iip fi ui results simple experiments performed cases 
experiment ui set probability vector drawn corresponding dirichlet distribution method described gelman 
vector set np range values effective number data points fake data set non integer counts 
methods evaluating flu compared function experiment tests ability methods evaluate marginal likelihood ui prior matched data source 
second experiment keep data source change model prior ui tests accuracy methods case assumed prior expects spiky data source 
third experiment reverses situation probability vector drawn dirichlet distribution ui find marginal likelihoods prior ui expects overly smooth fourth experiment looks case ui source ui model 
results shown 
cases assumed ui traditional method plotted fails utterly account components pi having negative exponents 
traditional method best ui large data come source matches prior amount data large 
softmax method superior parameter settings globally superior limit large amounts data bins methods perform traditional approximation little accurate 
discussion aim advocate laplace approximations case methods markov chain monte carlo see example heal 
deterministic bayesian approximations basis independent development mackay 
map methods offers way evaluating marginal likelihoods satisfies desiderata 
laplace approximation dirichlet priors amount data 
singularities encountered contrast traditional map method 

simple cases predictive distribution easy compute maximum posteriori parameters equal predictive distribution 
cases exact results available softmax laplace approximation accurate traditional approximation 
plausible advantages carry case models hidden variables 
softmax parameterization convenient property setting parameters valid constraints 
easy integrate models computational packages optimizers 
david heckerman radford heal virginia de sa graeme mitchison helpful discus sions 
dirichlet prior soft max basis sketch proof density equation alu zs pi mock data source ui mock data source ui model prior ui traditional exact softmax 
effective number data superposed model prior ui traditional method breaks 
exact effective number data exact softmax effective number data effective number data comparison laplace approximations correct answer experi mental situations data sources priors 
probability vectors produced sources components rank order ui ui 
transform dirichlet distribution start special case confined dimensional subspace satisfying delta function 
case represent dimensional vector ai bi 
hi similarly represent dimensional vector pi qi 
qi find density proportional required density density proportional density finding determinant opi pip 
pi ik pk 
jik ob ob defining dimensional vectors pk 
nk der xy easily proved changing basis aligned coordinates find pi der np pi ipi ep delta function equation neglecting constant factors incorporated normalizing constants find iz pi 
result true oz integrate normalized distribution probability 
bridle 

probabilistic interpretation feedforward classification network outputs relationships statistical pattern recognition 
soulie editors neuro computing algorithms architectures applications 
springer verlag 
chickering heckerman 

efficient approximations marginal likelihood bayesian networks hidden variables 
microsoft research technical report msr tr 
gelman 

bayesian model building pure thought principles examples 
statistica sinica 
gelman carlin stern rubin 

bayesian data analysis 
chapman hall london 
jeffreys 

theory probability 
oxford univ press 
rd edition reprinted paperback 
lee gauvain 

speaker adaptation map estimation hmm parameters 
ieee proceedings pages ii 
lindley 

approximate bayesian methods 
bernardo degroot lindley smith editors bayesian statistics pages 
valencia university press valencia 
mackay 

practical bayesian framework backpropagation networks 
neural computation 
mackay 

ensemble learning hidden markov models 
available wol 
ra 
phy 
cam 
ac 
uk 
mackay peto 

hierarchical dirichlet language model 
natural language engineering 


bayesian mixture modelling 
smith erickson editors maximum entropy bayesian methods seattle pages 

hagan 

bayesian inference volume kendall advanced theory statistics 
edward arnold 


pattern recognition neural networks 
cambridge 
smith spiegelhalter 

factors choice criteria linear models 
journal royal statistical society 
august version 
