solving pomdps levin search eira saitta ed machine proceedings th international conference pages morgan kaufmann publishers san francisco ca 
marco wiering idsia schmidhuber idsia partially observable markov decision prob lems pomdps received lot reinforcement learning community 
attention paid levin universal search program space ls theoretically optimal wide variety search problems including pomdps 
experiments show ls solve partially ob mazes involving states obstacles solved var ious previous authors ls easily outperform learning 
note ls necessarily optimal incremental learning problems experience previous problems may help reduce search costs 
rea son introduce adaptive extension ls als uses experience increase probabilities instructions occurring successful programs ls 
deal cases als lead long term performance improvement technique environment independent rein acceleration eira safety belt eira currently known method guarantees lifelong history reward accelerations 
experiments additional demonstrate als dramatically reduce search time con sumed successive calls ls 
addi tional significant speed ups obtained combining als eira 
levin search ls 
machine learning researchers exists search algorithm amazing theoretical properties broad class search problems ls levin levin optimal order computational complexity 
instance suppose algorithm solves certain type maze task steps positive integer representing problem size 
universal ls solve task steps 
see li overview 
see schmidhuber implementations applications :10.1.1.15.696
search program space relevant pomdps 
ls smart way performing exhaustive search optimally allocating time pro grams computing solution candidates details section 
programs written general language memory disambiguate environmental inputs ls potential interest solving partially ob markov decision problems pomdps received lot attention years jaakkola kaelbling ring mccallum 
incremental extensions ls 
ls non incremental experience previous tasks speed performance new tasks 
immediately typical incremental reinforcement learning scenarios case success system rein real number tries maximize sum reinforcements obtained remainder system life 
proposals adaptive variants ls modify ls underlying probability distribution pro gram space solomonoff schmidhuber 
guarantee lifelong history probability modifications correspond lifelong history reinforcement accelerations 
eira 
problem addressed re 
certain times system life called checkpoints novel technique called environment independent reinforcement acceleration eira validates certain modifications system policy policy arbitrary modifiable algorithm mapping environmental inputs internal states outputs new internal states currently valid modifications justified sense valid modification followed longterm performance speed 
measure speed checkpoint eira looks entire time interval went modification occurred 
efficiently eira performs backtracking time required backtracking taken account measuring performance speed ups 
eira general sense combined favorite learning search algorithm 
essentially eira works safety belt favorite learning algorithm fails improve things long term reinforce ment intake speeds see details section 
outline 
section describes ls details 
section presents heuristic adaptation method als simple adaptive incremental extension ls related linear reward inaction algorithm kaelbling 
section briefly reviews eira shows combine als 
sec tion presents results illustrative application involving maze states obstacles mazes solved previous authors working pomdps show ls solve partially observable maze tasks huge state spaces non trivial low complexity solutions learning fails solve tasks 
show als previous experience significantly reduce search time 
show als augmented eira clearly outperform als 
section presents 
levin search ls basic concepts 
ls requires set primitive instructions pr composed form arbitrary sequential programs 
essentially ls generates tests solution candidates pro gram outputs represented strings finite order levin complexities kt log stands program computes time steps pm probability guessing fixed solomonoff levin distribution li set possible programs section distribution variable 
optimality 
amazingly primitives representing universal programming language broad class problems including inversion problems time limited optimization problems ls shown optimal respect total expected search time leaving aside constant factor independent problem size levin levin li 
ls received attention purely theoretical studies see watanabe 
practical implementation 
practical ls ver sion upper bound program length due obvious storage limitations 
ai denotes address th instruction 
program generated incrementally select instruction pm matrix mij denotes probability selecting pj instruction address ai instructions selected 
probability program product probabilities constituents 
ls inputs representation problem denoted ls output program computes solution problem 
sec remain fixed 
ls implemented tion mij sequence longer longer phases levin search problem probability matrix set number current phase equal 
follows denote set executed programs satis lying pm repeat lution generate pro gram run ei ther halts pm steps 
computed solution return exit 
set solution tmax 
return tmax prespecified constants 
pro cedure essentially der complexity described paragraph section see solomonoff li 
adaptive ls als mentioned ls necessarily optimal incremental learning problems experience previous problems may help reduce search costs 
incremental search method non incremental ls introduce simple heuristic adaptive ls extension als uses ex previous problems adaptively modify ls underlying probability distribution 
als essentially works follows ls program computed solution current problem probabilities instructions 
ql increased qi pr denotes th instruction denotes length ls find solution empty program defined 
probability adjustment controlled learning rate linear reward inaction algorithm kaelbling main difference als uses ls search program space opposed single action space 
section probability distribution pm determined initially mid sequence problems nk mid may undergo changes caused als als problems nt nk variable matrix fori levin search ni adapt 
procedure adapt works follows adapt program variable matrix qi pj mij mij mij mij mij called environment independent reinforcement ac eira 
eira ensures system keep probability modifications representing lifelong history performance improvements 
eira als basic set 
time variable matrix represents system current policy 
call procedure adapt invoked als modi fies policy 
consider complete sequence calls spanning entire system life starts time ends point fu ture time flows direction resets 
definition th call occurs time denoted generates policy tion denoted 
calls certain amount time consumed levin search details time measured follow section experiments 
goal 
als finds solution system receives reward 
goal receive reward quickly possible generating policy changes minimize computation time required calls levin search 
denote sum reinforcements time time 
reinforcement time ratios 
right call adapt eira see details essentially inval policy modifications consis tent called reinforcement acceleration cri rac 
define rac introduce measure indicating useful adapt current time simply compute reinforce ment time ratio particular time rac satisfied adapt computed valid invalidated policy modification critique adaptive ls 
als reasonable step making ls adaptive leads nice experimental results see section theoretical proof gen erate probability modifications speed process finding solutions new tasks als may produce harmful beneficial results 
address issue section augment als backtracking technique vk valid 
obviously rac holds history valid policy modification represents history long term reinforcement accelerations valid cation followed average reinforcement time previous ones 
note success adapt call depends success adapt calls setting stage 
represents essential difference previous performance criteria 
eira uses stack store information policy modifications computed calls adapt 
right fore adapt executed eira restores necessary previous policies rac holds 
eira processes pushing 
time eira pushes information stack ti previous values columns representing probability distributions changed adapt information may needed restoring old policy generated 
popping 
right call adapt conditions holds eira pops probability vectors stack invalidates corresponding policy modifications restoring previous policies 
valid valid policy modification generated earlier 
valid policy 
stack empty 
theoretical soundness 
induction shown backtracking procedure ensures rac holds popping process schmidhuber 
time eira straight forward generalization assumption modifications survived popping process remain useful 
general environments assumed 
note time system life single training example evaluate current long term usefulness previous adapt call average reinforcement time occurred 
popping process eira reevaluate usefulness far valid modifications 
conclude eira implicitly evaluates valid policy modification followed long term performance improve ment modification set stage useful modifications 
evidence contrary eira invalidates policy modifications rac fulfilled 
eira stack backtracking efficient sense valid modifications considered time single popping process may invalidate modifications 
partially observable maze problems section describe experiments validating usefulness ls als eira 
illustrative application partially observable maze states obstacles various authors ml show ls solve pomdps huge state spaces low complexity solutions learning variants fail solve tasks 
ex periments task requires find stochastic policy finding multiple goals 
show als previous experience speed process finding solutions eira combined als short als eira outperform als 
experiment big partially observable maze pom task 
shows maze single start position single goal position 
maze fields obstacles mazes previous authors working pomdps instance mccallum maze free fields mccallum 
goal find program agent move instructions 
programs composed primitive instructions 
instructions represent initial bias provided programmer follows superscripts indicate instruction num bers 
instructions syn tax repeat step forward condition cond rotate direction dir 
instruction cond front blocked dir left 
instruction cond front blocked dir right 
instruction cond left field free dir left 
instruction cond left field free dir right 
instruction cond left field free dir 
instruction cond right field free dir left 
instruction cond right field free dir right 
instruction cond right field free dir 
instruction jump address nr times 
parameters nr times address top top highest address current program 
jump uses additional hidden variable nr times go initially set nr times 
semantics nr times go continue execution address address 
nr times go nr times go set nr times go nr times 
note nr times may cause infinite loop 
jump instruction essential exploiting possibility solutions may consist repeatable action sequences subprograms having low algorithmic complexity 
ls incrementally growing time limit automatically deals programs don halt preventing consuming time 
mentioned section probability program product probabilities constituents 
deal probabilities jump parameters introduce additional variable matrices 
program instructions specify conditional probability lij jump address aj instruction address ai jump normalize entries ai ai ail ensures relevant entries sum 
provided instruction address ai jump aij specifies probability nr parameter set initialized uniformly adapted als just 
restricted ls 
note instructions sufficient build universal programming language experiments confined restricted version ls 
instruc tions build programs solving maze necessary completely reverse direction movement rotation degrees corridor 
note mainly jump instruction allows composing low complexity solutions subprograms ls provides sound way dealing infinite loops 
rules 
ls generates runs tests new program agent reset start position 
collisions walls halt program 
path generated program agent hit goal called solution agent required goal explicit halt instructions 
pomdp 
instructions sufficient tell agent exactly time agent perceive highly ambiguous types input executing appropriate primitive front blocked left field free right field free compare list primitives 
sort mem ory required disambiguate apparently equal situations encountered way goal 
learning instance guaranteed solve pomdps watkins dayan 
agent memory implicit state execution current program disambiguate ambiguous situa tions 
measuring time 
computational cost single levin search call adapt calls essentially sum costs programs tests 
measure cost single program simply count total number forward steps rota tions program execution number order total computation time 
note instructions cost step 
detect infinite loops ls measures time consumed jump instructions time step executed jump 
realistic application time consumed robot move far exceed time consumed jump instruction omitted negligible cost experimental results 
comparison 
compared ls variants learning watkins dayan random search 
random search repeatedly randomly se executes instructions goal hit levin search agent reset start position hits wall 
random search ls time limit testing may jump prevent wandering infinite loops 
variant uses instructions ad vantage distinguish possible states possible inputs task easier pomdp 
variant just tested see difficult problem pomdp setting 
second variant observe surrounding fields blocked possible inputs third variant receives unique representation executed instruc tions input possible inputs requires gigantic table 
initial ex periments second variant noticed input preventing collisions agent walks rotates front wall instruction cause collision 
improve second variant performance appropriately altered instructions instruction consists types rotations followed types forward walks total number instructions reason random search jump instruction 
parameters learning variants coarsely optimized number smaller mazes able solve 
set means phase ls procedure program probability may execute steps stopped 
typical result 
easy totally observable case learning took average steps simulations conducted solve maze 
expected partially observable cases learning variants random search able solve maze steps simulations conducted 
contrast ls able solve pomdp ls required steps find program computing step shortest path goal 
ls low complexity solution involves nested loops repeat step forward left field free jump repeat step forward left field free rotate left jump pm 
similar results obtained mazes having non trivial solutions low algorithmic complexity 
experiments illustrate smart search program space beneficial cases task appears complex solutions 
ls principled way dealing non halting programs time limits genetic programming gp ls may interest researchers working gp re lated fields gp algorithms evolve assembler computer programs best knowledge dickmanns 
als single tasks versus multiple tasks 
adaptive ls extension als single task repeatedly applying ls problem changing underlying probability distribution successive calls section probability matrix rapidly con late ls calls find solution immediately 
interesting solution single problem additional problems point investing additional efforts probability updates 
als interesting cases multiple tasks solution task conveys information helpful solving addi tional tasks 
section 
experiment learning find multiple goals task 
second experiment shows als experience significantly reduce average search time consumed successive ls calls cases multiple tasks solve als improved combining eira 
able run sufficient number simulations ob tain statistically significant results replace big maze smaller maze indicates different goal positions 
time goal positions contains food 
agent know 
agent finds food takes home eat 
time food appears location 
deterministic program generate shortest path 
best agent learn stochastic policy minimizing expected search time 
experiment consists simulations 
simulation goal positions randomly generated 
simulation consists epochs epoch consists runs th run th goal position starting start state 
adjusted solution 
comparison 
compared random search als als eira combination eira restores old policies necessary right als matrices adapted 
ls calls triggered als runtime set 
als performed best learning rate 
als eira performed best learning rate 
results 
methods goal posi tions running time limit steps goal 
learning curves 
ls calls triggered als take long time epochs search cost im proves factor scaling reasons show initial search costs 
table shows average number steps required find goal positions th epoch simulations 
results show als finds goal positions average faster ran dom search 
table shows table number steps required als als eira random search rs find goal positions starting start position 
table shows average number steps thou sands consumed ta epoch 
sd standard deviation max min stands worst best performance simulations different goal positions see see als dramatically reduces search costs successive ls calls 
ii average sd max min als eira als rs eira significantly improves results additional speed factor exceeds 
safety belt effect 
plots number epochs average probability programs computing solutions 
shows als eira tends keep probabilities lower als high program probabilities beneficial 
effectively eira controlling prior search space average search time reduced 
total stack size number instruction prob ability vectors stack ta trial average 
total amount policy mod number goal positions number epochs average solution length eira kept modifications 
remaining deemed observed followed long term reinforcement speed ups 
clearly eira prevents als policy modifications safety belt effect 
major points levin search useful solving pomdps 
demonstrated non trivial partially observable maze containing significantly states obstacles demonstrate previous pomdp algorithms mc ring littman cliff ross instance mccallum cheese maze free fields ring largest maze maze 
illustrates search program space significant advantages methods searching simple action space provided algorithmic complexity solutions low 
straightforward incremental adaptive extension non incremental ls als introduced dramatically reduce time consumed successive calls ls cases multi ple tasks solve 
als significantly benefit environment independent reinforcement acceleration eira 
eira helps get rid als generated policy modifications evidence contribute long term performance improvement 
provides example eira improve heuristic learning methods lifelong learning situations 
due eira generality limited run conjunction als combined kinds policy modify ing learning algorithms results add making eira appear promising general paradigm 

als extended adapts probability distribution underlying ls initial time limit required ls phase current als version keeps constant represents potential loss efficiency 
eira combined als ex tension 
eira combined genetic learning algorithms especially situations applicability algorithm questionable environment satisfy preconditions sound 
eira guarantee policy modifications appear negative long term effects learning processes 
separate pomdp experiments able show eira improve standard learning performance recall pomdp applications learning theoretically sound authors apply variants pomdps 
interesting application area may field bucket brigade classifier systems cliff ross show systems tend unstable forget solutions 
eira unfold safety belt effect 
valuable discussions sepp hochreiter zhao supported snf incremental self improvement 
cliff ross 

adding temporary memory 
adaptive behavior 
dickmanns schmidhuber 

der eine im prolog 
institut informatik lehrstuhl prof technische universit 
jaakkola singh jordan 

reinforcement learning algorithm partially observable markov decision problems 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press 
kaelbling 

learning embedded systems 
mit press 
kaelbling littman cassandra 

planning acting partially observ able stochastic domains 
technical report brown university providence ri 
levin 

universal sequential search prob lems 
problems information transmission 
levin 

randomness conservation ities information independence mathematical theories 
information control 
machine learning proceedings twelfth international conference pages 
morgan kaufmann publishers san francisco ca 
ring 

continual learning reinforce ment environments 
phd thesis university texas austin austin texas 
schmidhuber 

discovering solutions low kolmogorov complexity high generalization capability 
russell editors machine learning proceedings twelfth international conference pages 
morgan kaufmann publishers san francisco ca 
schmidhuber 

environment independent reinforcement acceleration 
technical report note idsia idsia 
invited talk hongkong university science technology 
solomonoff 

application algorithmic probability problems artificial intelligence 
kanal lemmer editors uncertainty intelligence pages 
elsevier science publishers 
watanabe 

kolmogorov complexity computational complexity 
eatcs monographs theoretical computer science springer 
watkins dayan 

learning 
machine learning 
li 

kolmogorov complexity applications 
springer 
littman 

memoryless policies cal limitations practical results 
cliff husbands wilson editors proc 
international conference simulation adaptive behavior animals animats pages 
mit press bradford books 
mccallum 

overcoming incomplete ception utile distinction memory 
machine learning proceedings tenth international conference 
morgan kaufmann amherst ma 


instance utile dis reinforcement learning hidden state 
prieditis russell editors apparently complex partially observable random als als nr epochs average number steps required find goal positions time starting anew start position plotted number epochs 
average probability programs computing solutions 
eira average probability certain solution computing programs 
