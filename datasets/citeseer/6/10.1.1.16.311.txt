named entity recognition gazetteers gazetteers lists names persons organizations locations entities mentioned bottleneck named entity ne recognition ner system 
proposes modified hidden markov model hmm hmm chunk tagger ner system built recognize classify names times numerical quantities 
modified hmm system able apply integrate types internal external evidences simple deterministic internal feature words capitalization internal semantic feature important triggers external macro context feature 
way ner problem resolved effectively gazetteers 
evaluation system muc muc english ne tasks achieves measures respectively 
shows gazetteers need bottleneck ner problem performance system gazetteers consistently better reported machine learning system comparable reported best system handcrafted rules 
named entity ne recognition ner classify word document falling predefined categories falls domain information extraction extracts specific kinds information documents opposed general task document management seeks extract information document 
ner serves important preprocessing tool tasks information extraction information retrieval text processing applications 
ner relatively simple fairly easy build system reasonable performance large number ambiguous cases difficult attain human performance 
instances word washington name person name city state 
copyright american association artificial intelligence www aaai org 
rights reserved 
machine learning approach id location organization 
jones lost pounds lose pounds weight money 
exist general problems robustness portability system recognize nes appear headlines sentences capitalization information missing 
decade ner drawn attention ne tasks chinchor chinchor muc muc person names location names organization names dates times percentages money amounts delimited text sgml mark ups 
lot ner address ambiguity robustness portability issues 
previous approaches mainly rule 
typical systems univ sheffield lasie ii humphreys univ edinburgh ltg mikheev mikheev english ner system yu chinese ner 
rule approaches lack ability coping problems robustness portability 
new language new class information spot write new set rules cover 
means steep maintenance cost handcrafted rule system 
current trend ner machine learning approach attractive trainable adaptable maintenance machine learning system cheaper rule 
representative machine learning approaches ner hmm bbn identifinder miller bikel maximum entropy new york univ meme borthwick decision tree new york univ system sekine sra system bennett 
variant eric brill transformation rules brill applied problem aberdeen 
approaches hmm outperforms ability capturing locality phenomena indicates names text efficient viterbi algorithm viterbi decoding ne class state sequence 
performance machine learning system poorer rule chinchor chinchor 
may current machine learning approaches capture important evidence ner problem effectively human experts rules machinelearning approaches provide important statistical learned information available human experts 
state art ner systems gazetteers different sizes pros cons mikheev 
reported cucchiarelli bottlenecks designing ner system limited availability gazetteers different domains different applications particularly different languages building gazetteers expensive time consuming 
avoid bottleneck study ner gazetteers see ner system gazetteers achieve state art performance 
defined mcdonald kinds evidences ner solve ambiguity robustness portability problems described 
internal evidence word word string second external evidence gathered context 
order solve problem ner gazetteers proposes modified hmm effectively apply integrate internal external evidences 
approach ner system hmm chunk tagger text chunking zhou ne regarded chunk 
approach justification ner problem successful results approach muc muc ne tasks 
hmm chunk tagger hmm modeling token sequence gn goal ner find stochastic optimal tag sequence tn maximizes log log log second item mutual information order simplify computation item assume mutual information independence mi mi ti log log ti applying equation log log log log basic premise model assumes mutual information independence traditional hmm assumes conditional probability traditional hmm maximize log apply bayes rule independence 
assumption looser assumption assumption effect sum assumptions way model apply context information determine tag current token 
equation see item computed applying chain rules 
normally tag assumed probabilistically dependent previous tags 
simple back bigram model bigram model backs powerful descriptive unigram model bigram seen training data 
choose bigram language model semantically appealing bigram model works remarkably effectively efficiently viterbi algorithm viterbi 
second item summation log probabilities individual tags 
third item corresponds lexical component tagger 
focus third item log ti main difference tagger traditional hmm taggers bbn identifinder 
hmm chunk tagger ne chunk tagging token fi wi wn word sequence fn word feature sequence 
ne chunk tag structural consists parts boundary category bc 
means current word entity means current word middle entity name 
entity category ec 
denote class entity name 
word feature wf 
limited number boundary entity categories word feature added structural tag represent accurate models 
arg max log arg max log log assume conditional probability independence arg max log arg max log gi ti log obtain equation assuming log log obviously exist constraints boundary entity categories shown table valid invalid means tag sequence determining word feature ti ti valid invalid valid means ti ti valid additional condition ec eci table constraints column bc row bc valid valid invalid invalid invalid invalid valid valid invalid invalid valid valid valid valid invalid invalid stated token denoted ordered pairs word fi wi simple deterministic computation performed word word string appropriate consideration context looked added context 
model word feature consists subfeatures classified internal sub features external sub features 
internal sub features word word string capture internal evidence external sub features derived sub feature context capture external evidence 
table sub feature simple deterministic internal feature words example explanation intuition digital number digit year digit year year decade product code date fraction date date money money percentage number ibm organization person name initial st abbreviation abbreviation word sentence useful capitalization information microsoft capitalized word lowercase un capitalized word words table sub feature semantic classification important triggers ne type triggers sub feature example explanation intuition percent percentage suffix money money prefix dollars money suffix date day date suffix monday week date july month date summer season date month period date quarter quarter half year weekend date fiscal modifier date time 
time suffix morning time period person person title president person designation michael person name loc river location suffix org organization suffix cardinal ordinal sixth cardinal ordinal numbers table sub feature external macro context feature means local document ne type sub feature example person gates person bill gates recognized person name loc loc new jersey recognized location name org un org united nation recognized org name internal sub features model captures types internal sub features simple deterministic internal feature words capitalization internal semantic feature important triggers basic sub feature exploited model shown table descending order priority 
example case non disjoint feature classes take precedence 
eleven features arise need distinguish annotate monetary amounts percentages times dates 
rest features distinguish types capitalization words punctuation marks 
particular feature arises fact word capitalized word sentence information capitalized note computed take precedence 
sub feature language dependent 
fortunately feature computation extremely small part implementation 
kind internal sub feature widely machine learning systems bbn new york univ mene 
rationale sub feature clear roman languages capitalization gives evidence nes 
numeric symbols automatically grouped categories 
semantic classification important triggers seen table unique system 
intuitions important triggers useful ner classified semantics 
sub feature applies single word multiple words 
external sub features external evidence external macro context feature currently captured model 
sub feature encountered ne candidate occurred list nes recognized document shown table word number matched ne recognized ne list matched word number word string matched ne corresponding ne type 
sub feature unique system 
intuition phenomena name alias 
decoding nes recognized document stored list 
system encounters ne candidate name alias algorithm invoked dynamically determine relationship nes stored recognized list 
initially consider part speech pos 
experimental result disappointing incorporation pos decreases performance 
may capitalization information word submerged pos tags performance pos tagging satisfactory especially unknown capitalized words nes include unknown capitalized words 
pos discarded model 
back modeling model word feature described main problem compute ti ideally sufficient training data event conditional probability wish calculate 
unfortunately rarely training data compute accurate probabilities decoding new data especially considering complex word feature described 
order resolve sparseness problem levels back modeling applied approximate ti level back scheme different contexts word features words ti approximated descending order ti ti fi fi fi wi ti ti fi wi fi ti ti fi fi wi ti ti fi wi fi ti ti fi wi fi ti ti fi wi ti ti fi ti ti fi fi fi ti ti fi wi ti ti fi fi fi ti ti fi fi ti ti fi second level back scheme different combinations sub features described section approximated descending order experimental results section report experimental results system english ner data muc muc ne tasks shown table impact training data size performance muc training data 
experiment muc data held development data muc formal test data held test data 
table statistics data muc muc ne tasks statistics kb training dry run formal test muc muc precision measure recall table performance system muc muc ne tasks muc muc table impact different sub features composition muc system muc system muc systems muc comparison system muc muc ne tasks training data size kb impact various training data performance muc muc ne tasks table shows performance system muc evaluation gives comparisons system 
precision measures number correct nes answer file total number nes answer file recall measures number correct nes answer file total number nes key file measure weighted harmonic mean precision recall rp 
shows performance significantly better reported machine learning algorithm 
performance consistently better approaches handcrafted rules 
learning technique important question training data required achieve acceptable performance 
generally performance vary training data size changes 
result shown muc ne task 
shows kb training data measures muc reducing kb significant decrease performance 
shows system room improvement performance 
may complex word feature corresponding sparseness problem existing system 
important question effect different sub features 
table answers question muc ne task applying achieves measure 
useful ner increases measure impressive 
impressive measure improvement 
inspection errors confirms potential contribution gazetteers comes explicit indicator information ne nes macro context document 
nes potentially contributed gazetteers known ones microsoft ibm bach composer introduced texts helpful context 
proposes modified hmm new generative model mutual information independence assumption conditional probability independence assumption bayes rule applied 
shows hmm chunk tagger effectively apply integrate different kinds sub features ranging internal word information semantic information macro context document capture internal external evidences ner gazetteers 
shows gazetteers need bottleneck ner problem performance system gazetteers consistently better reported machinelearning system comparable reported best system handcrafted rules 
experimental results impressive done potentially improve performance 
near feature incorporate system effective name alias algorithm 
effective strategy back modeling smoothing 
aberdeen aberdeen day hirschman robinson vilain 
mitre description alembic system muc 
muc 
pages 
columbia maryland 

bennett bennett aone lovell 
learning tag multilingual texts observation 
emnlp 
pages 
providence rhode island 

bikel daniel bikel richard schwartz ralph weischedel 
algorithm learns name 
machine learning special issue nlp 

borthwick borthwick sterling agichtein grishman 
nyu description mene named entity system muc 
muc 
fairfax virginia 

borthwick andrew borthwick 
maximum entropy approach named entity recognition 
ph thesis 
new york university 
september 
brill eric brill 
transform error driven learning natural language processing case study part speech tagging 
computational linguistics 
pages 

chinchor nancy chinchor 
muc named entity task definition version 
muc 
columbia maryland 

chinchor nancy chinchor 
statistical significance muc results 
muc 
columbia maryland 

chinchor nancy chinchor 
muc named entity task definition version 
muc 
fairfax virginia 

chinchor nancy chinchor 
statistical significance muc results 
muc 
fairfax virginia 

cucchiarelli cucchiarelli paola velardi 
automatic semantic tagging unknown proper nouns 
coling acl 
pages 
montreal canada 
august 
humphreys humphreys gaizauskas mitchell cunningham wilks 
univ sheffield description lasie ii system muc 
muc 
fairfax virginia 

krupka krupka hausman 
description tm extractor system muc 
muc 
fairfax virginia 

mcdonald mcdonald 
internal external evidence identification semantic categorization proper names 
boguraev pustejovsky editors corpus processing lexical acquisition 
pages 
mit press 
cambridge ma 

miller miller crystal fox ramshaw schwartz stone weischedel annotation group 
bbn description sift system muc 
muc 
fairfax virginia 

mikheev mikheev grover moens 
description ltg system muc 
muc 
fairfax virginia 

mikheev mikheev moens grover 
named entity recognition 
eacl 
pages 
bergen norway 

muc morgan kaufmann publishers proceedings sixth message understanding conference muc 
columbia maryland 

muc morgan kaufmann publishers proceedings seventh message understanding conference muc 
fairfax virginia 

sekine satoshi sekine 
description japanese ne system met 
muc 
fairfax virginia 

viterbi viterbi 
error bounds convolutional codes asymptotically optimum decoding algorithm 
ieee transactions information theory 

pages april 
yu yu bai wu paul 
description kent ridge digital labs system muc 
muc 
fairfax virginia 

zhou zhou su jian error driven hmm chunk tagger context dependent lexicon 
emnlp vlc 
hong kong oct 
