journal machine learning research submitted published think globally fit locally unsupervised learning low dimensional manifolds lawrence saul cis upenn edu department computer information science university pennsylvania south rd street moore school philadelphia pa usa sam roweis roweis cs toronto edu department computer science university toronto king college road pratt building toronto ontario canada editor yoram singer problem dimensionality reduction arises fields information processing including machine learning data compression scientific visualization pattern recognition neural computation 
describe locally linear embedding lle unsupervised learning algorithm computes low dimensional neighborhood preserving embeddings high dimensional data 
data assumed sampled underlying manifold mapped single global coordinate system lower dimensionality 
mapping derived symmetries locally linear reconstructions actual computation embedding reduces sparse eigenvalue problem 
notably optimizations lle capable generating highly nonlinear embeddings simple implement involve local minima 
describe implementation algorithm detail discuss extensions enhance performance 
results algorithm applied data sampled known manifolds collections images faces lips handwritten digits 
examples provide extensive illustrations algorithm performance successes failures relate algorithm previous ongoing nonlinear dimensionality reduction 

problems machine learning preprocessing raw multidimensional signals images faces spectrograms speech 
goal preprocessing obtain useful representations information signals subsequent operations classification denoising interpolation visualization outlier detection 
absence prior knowledge representations learned discovered automatically 
automatic methods discover hidden structure statistical regularities large data sets studied general framework unsupervised learning hinton sejnowski 
lawrence saul sam roweis 
saul roweis main goals proposed algorithms unsupervised learning density estimation dimensionality reduction 
goal density estimation learn parameters probabilistic model predict assess novelty observations 
goal dimensionality reduction obtain compact representations original data capture information necessary higher level decision making 
trend machine learning pursue goals simultaneously probabilistic generative models raw sensory inputs hidden variables represent low dimensional degrees freedom attias dayan hyvarinen roweis roweis tipping bishop 
goal dimensionality reduction pursued non probabilistic nonparametric setting 
approach taken leading efficient eigenvector method nonlinear dimensionality reduction large data sets 
addresses longstanding problem intersection geometry statistics compute low dimensional embedding high dimensional data sampled noise underlying manifold 
types high dimensional data characterized way example images generated different views dimensional object 
applications machine learning pattern recognition low dimensional manifolds represent continuous percepts recurring theme computational neuroscience seung lee 
goal algorithm learn representations examples discover general setting priori knowledge degrees freedom underlie observed modes continuous variability 
canonical forms dimensionality reduction eigenvector methods principal component analysis pca jolliffe multidimensional scaling mds cox cox 
applications pca mds involve modeling linear variabilities multidimensional data 
pca computes linear projections greatest variance top eigenvectors data covariance matrix 
mds computes low dimensional embedding best preserves pairwise distances data points 
distances correspond euclidean distances case called classical scaling results mds equivalent pca linear transformation 
methods simple implement optimizations understood prone local minima 
virtues account widespread pca mds despite inherent limitations linear methods 
introduced powerful eigenvector method called locally linear embedding lle problem nonlinear dimensionality reduction roweis saul 
problem illustrated manifolds 
examples dimensionality reduction lle succeeds recovering underlying manifolds linear embeddings pca mds map faraway data points nearby points plane creating distortions local global geometry 
pca mds algorithm simple implement optimizations involve local minima 
methods capable generating highly nonlinear embeddings main optimization involves sparse eigenvalue problem scales large high dimensional data sets 
note mixture models local dimensionality reduction fukunaga olsen ghahramani hinton leen cluster data perform pca cluster address problem considered map high dimensional data single global coordinate system lower dimensionality 
particular models discover clusters high dimensional data model density unsupervised learning low dimensional manifolds problem nonlinear dimensionality reduction illustrated dimensional data sampled dimensional manifolds 
unsupervised learning algorithm discover global internal coordinates manifold external signals suggest data embedded dimensions 
lle algorithm described discovers neighborhood preserving mappings shown color coding reveals data embedded dimensions 
saul roweis compute dimensional embeddings rightmost panels 
describe lle algorithm providing significantly examples details implementation earlier roweis saul 
discuss number extensions enhance performance 
organization follows section describe algorithm general terms focusing main procedures geometric intuitions 
section illustrate algorithm performance problems different size dimensionality 
section discuss efficient ways implement algorithm provide details nearest neighbor search squares optimization eigenvalue problem 
sections describe number extensions lle including estimate enforce dimensionality discovered manifolds derive mappings generalize results lle examples outside training set 
section compare lle eigenvector methods clustering nonlinear dimensionality reduction belkin niyogi ng scholkopf shi malik tenenbaum weiss mention directions 

algorithm lle algorithm summarized simple geometric intuitions 
essentially algorithm attempts compute low dimensional embedding property nearby points high dimensional space remain nearby similarly located respect low dimensional space 
put way embedding optimized preserve local configurations nearest neighbors 
shall see suitable conditions possible derive embedding solely geometric properties nearest neighbors high dimensional space 
lle algorithm operates entirely recourse measures distance relation faraway data points 
suppose data consist real valued vectors inputs dimensionality sampled smooth underlying manifold 
provided sufficient data manifold sampled expect data point neighbors lie close locally linear patch manifold 
precisely smooth sampled mean data sampled dimensional manifold curvature sampling density data point order neighbors define roughly linear patch manifold respect metric input space 
conditions characterize local geometry neighborhood data point linear coefficients reconstruct data point neighbors 
lle algorithm derives name nature reconstructions local sense neighbors contribute reconstruction linear sense reconstructions confined linear subspaces 
simplest formulation lle identifies nearest neighbors data point measured euclidean distance 
sophisticated neighborhood criteria discussed section 
reconstruction errors measured cost function unsupervised learning low dimensional manifolds lle algorithm 
compute neighbors data point 
compute weights best reconstruct data point neighbors minimizing cost equation constrained linear fits 

compute vectors best reconstructed weights minimizing quadratic form equation bottom nonzero eigenvectors 
ik ij ij ik map embedded coordinates 
reconstruct weights 
linear select neighbors 
summary lle algorithm mapping high dimensional inputs low dimensional outputs local linear reconstruction weights sw data point neighbors locally linear reconstruction reconstruction weights constrained satisfy 
adds squared distances data points reconstructions 
weight summarizes contribution jth data point ith reconstruction 
compute weights minimize cost function equation subject constraints sparseness constraint invariance constraint 
sparseness constraint data point reconstructed neighbors enforcing belong set 
invariance constraint rows weight matrix sum 
reason constraint clear shortly 
optimal weights subject constraints solving set constrained squares problems discussed section 
saul roweis effects shearing reconstruction weights 
shown optimal weights linearly reconstruct black point terms gray neighbors resulting reconstructions indicated white squares 
note optimal weights linear reconstructions invariant shear transformations 
note constrained weights minimize reconstruction errors obey important symmetries particular data point invariant rotations rescalings translations data point neighbors 
invariance rotations rescalings follows immediately form equation invariance translations enforced sumto constraint rows weight matrix 
consequence symmetries reconstruction weights characterize geometric properties depend particular frame 
note optimal weights linear reconstructions data point general invariant local affine transformations 
see counterexample 
suppose data lie near manifold dimensionality approximation imagine exists linear mapping consisting translation rotation rescaling maps high dimensional coordinates neighborhood global internal coordinates manifold 
design reconstruction weights reflect geometric properties data invariant exactly transformations 
expect characterization local geometry input space equally valid local patches manifold 
particular weights reconstruct input dimensions reconstruct embedded manifold coordinates dimensions 
informally imagine pair scissors cutting locally linear patches underlying manifold arranging low dimensional embedding space 
patch involves translation rotation rescaling angles data points patch preserved 
follows patch arrives low dimensional destination weights provide optimal reconstruction data point neighbors 
lle constructs neighborhood preserving mapping idea 
third final step algorithm high dimensional input mapped low dimensional output representing global internal coordinates manifold 
done choosing dimensional coordinates output minimize embedding cost function 

naturally invariant global rotations translations homogeneous rescalings inputs invariance local transformations far reaching implications 
unsupervised learning low dimensional manifolds cost function previous locally linear reconstruction errors fix weights optimizing outputs note embedding computed directly weight matrix original inputs involved step algorithm 
embedding determined entirely geometric information encoded weights goal find low dimensional outputs reconstructed weights high dimensional inputs embedding cost function equation defines quadratic form outputs subject constraints problem posed cost function unique global minimum 
unique solution outputs result returned lle low dimensional embedding high dimensional inputs embedding cost function minimized solving sparse nn eigenvalue problem 
details eigenvalue problem discussed section 
show bottom non zero eigenvectors easily computed cost matrix provide ordered set embedding coordinates 
note reconstruction weights data point computed local neighborhood independent weights data points embedding coordinates computed nn global operation couples data points precisely data points lie connected component graph defined neighbors 
algorithm discovers global structure integrating information overlapping local neighborhoods 
note th coordinate output lle corresponds st smallest eigenvector cost matrix regardless total number outputs requested 
lle coordinates ordered nested dimensions added embedding space existing ones change 
implementation algorithm straightforward 
simplest formulation lle exists free parameter number neighbors data point equivalent neighborhood determining parameter radius ball drawn point 
neighbors chosen optimal weights outputs computed standard methods linear algebra detailed section 
algorithm involves single pass steps finds global minima reconstruction embedding costs equations 
learning rates annealing schedules required optimization random initializations local optima affect final results 

examples embeddings discovered lle easiest visualize data sampled dimensional manifolds 
example input lle consisted data points sampled manifolds shown panel 
resulting embeddings show algorithm neighbors data point faithfully maps manifolds plane 
example bottom row shows right conditions lle learn mapping sphere plane 
algorithm succeed case neighborhood north pole excluded data sampled uniformly manifold coordinates corresponds increasing density approaches north pole input space 
example suggests lle recover conformal mappings mappings locally preserve angles distances 
conjecture motivated invariance reconstruction weights equation translations rotations scalings local neighborhoods 
remains open problem prove manifolds saul roweis generally discovered lle sampling conditions 
survey known theoretical results algorithms nonlinear dimensionality reduction tenenbaum de silva tenenbaum bengio brand huang donoho grimes section 
shows dimensional manifold living higher dimensional space 
generated examples shown middle panel translating image single face larger background random noise 
noise independent example 
consistent structure resulting images describes dimensional manifold parameterized face center mass input lle consisted grayscale images image containing face superimposed background noise 
note easy visualize manifold translated faces highly nonlinear high dimensional vector space pixel coordinates 
bottom portion shows components discovered lle neighbors data point 
contrast top portion shows components discovered pca 
clear manifold structure example better modeled lle 
minor edge effects due selecting constant number neighbors data point 
neighbors boundary points lie away neighbors interior points 
applied lle data set containing different images single person face 
data set consisting frames digital movie contained grayscale images resolution 
shows components images discovered lle nearest neighbors 
components appear correlated highly nonlinear features image related pose expression 
applied lle images lips animation talking heads graf 
data set contained color rgb images lips resolution 
shows components images discovered lle nearest neighbors 
components appear capture highly nonlinear degrees freedom associated opening mouth lips teeth 
shows particular neighborhood lip images embedded dimensional space 
dimensionality reduction images useful faster efficient animation 
particular low dimensional outputs lle index original collection high dimensional images 
fast accurate indexing essential component example video synthesis large library stored frames 
data set lip images largest data set applied lle 
lle scales relatively large data sets generates sparse intermediate results eigenproblems 
shows sparsity pattern large sub block weight matrix data set lip images 
computing bottom eigenvectors embedding took hours high workstation specialized routines finding eigenvectors sparse symmetric matrices 

implementation algorithm described consists steps nearest neighbor search identify nonzero elements weight matrix constrained squares fits compute values weights singular value decomposition perform embedding 
discuss steps detail 
unsupervised learning low dimensional manifolds successful recovery manifold known structure 
shown results pca top lle bottom applied grayscale images single face translated dimensional background noise 
images lie intrinsically dimensional manifold extrinsic dimensionality equal number pixels image 
note lle nearest neighbors maps images corner faces corners dimensional embedding pca fails preserve neighborhood structure nearby images 
saul roweis images faces mapped embedding space described coordinates lle nearest neighbors 
representative faces shown circled points different points space 
bottom images correspond points top right path illustrating particular mode variability pose expression 
data set total grayscale images resolution 
step neighborhood search step lle identify neighborhood data point 
simplest formulation algorithm identifies fixed number nearest neighbors data point measured unsupervised learning low dimensional manifolds high resolution images lips mapped embedding space discovered coordinates lle nearest neighbors 
representative lips shown different points space 
inset shows lle coordinates entire data set corresponding images 
typical neighborhood lip images mapped embedding space described coordinates lle 
rectangle left plot locates neighborhood shown right space lip images 
saul roweis nz sparsity pattern weight matrix data set lip images 
sub block matrix shown reduce blurring zero nonzero elements printing 
positive weights indicated blue negative weights red 
roughly elements zero ink density image reflect accurately 
euclidean distance 
criteria choose neighbors 
example identify neighbors choosing points ball fixed radius 
locally derived distance metrics priori knowledge estimated curvature pairwise distances nonparametric techniques box counting deviate significantly globally euclidean norm 
number neighbors data point 
fact neighborhood selection quite sophisticated 
example take points certain radius maximum number take certain number neighbors outside maximum radius 
general specifying neighborhoods lle presents practitioner opportunity incorporate priori knowledge problem domain 
results lle typically stable range neighborhood sizes 
size range depends various features data sampling density manifold geometry 
criteria kept mind choosing number neighbors data point 
algorithm expected recover embeddings dimensionality strictly 
neighbors span space dimensionality 
unsupervised learning low dimensional manifolds number neighbors observed margin generally necessary obtain topology preserving embedding 
exact relation faithfulness resulting embedding remains important open question 
second algorithm assumption data point nearest neighbors modeled locally linear curved data sets choosing large general violate assumption 
unusual case indicating original data low dimensional data point reconstructed perfectly neighbors local reconstruction weights longer uniquely defined 
case regularization added break degeneracy 
shows range embeddings discovered algorithm data set different numbers nearest neighbors results stable wide range values break small large 
nearest neighbor step lle simple implement time consuming large data sets performed optimizations 
computing nearest neighbors scales worst case dn linearly input dimensionality quadratically number data points distributions data especially concentrated thin submanifold input space constructions trees ball trees compute neighbors logn time friedman gray moore moore omohundro 
karger ruhl specifically addresses problem computing nearest neighbors data low dimensional manifolds 
efficient approximate methods possible come various guarantees accuracy indyk 
implementation lle needs check graph formed linking data point neighbors connected 
efficient algorithms tarjan exist purpose 
graph disconnected weakly connected lle applied separately data graph strongly connected components neighborhood selection rule refined give strongly connected graph 
disconnected weakly connected case data different connected components interpreted lying distinct data manifolds 
theory situations detected fact zeros eigenvalue spectrum perona lle 
practice straightforward compute connected components apply lle separately component 
reduces computational complexity algorithm avoids possible confounding results different components 
step constrained squares fits second step lle reconstruct data point nearest neighbors 
optimal reconstruction weights computed closed form 
consider particular data point nearest neighbors reconstruction weights sum 
write reconstruction error jk jk 
simple regularizer penalize sum squares weights favors weights uniformly distributed magnitude 
discussed section 
corresponding eigenvectors constant values connected component different values different components 
yields zero embedding cost equation 
saul roweis effect neighborhood size lle 
embeddings dimensional manifold top panels computed different choices number nearest neighbors reliable embedding dimensions obtained wide range values 
small top left large bottom right lle succeed unraveling manifold recovering underlying degrees freedom 
identity exploited fact weights sum second identity introduced local gram matrix jk 
construction gram matrix symmetric definite 
reconstruction error minimized analytically lagrange multiplier enforce constraint 
terms inverse gram matrix optimal weights jk lm lm 
solution written equation appears require explicit inversion gram matrix 
practice efficient numerically stable way minimize error yields unsupervised learning low dimensional manifolds degeneracy reconstruction weights 
neighbors input dimensions weights minimize reconstruction error equation unique solution 
consider example dimensional data point neighbors lie corners diamond 
case different settings weights lead zero reconstruction error 
possible settings shown 
adding regularizer penalizes squared magnitude weights favors left solution 
note example particular symmetries chosen ease visualization degeneracy arises points general position 
result simply solve linear system equations jk rescale weights sum 
unusual cases arise gram matrix equation singular nearly singular example example neighbors input dimensions data points general position 
case squares problem finding weights unique solution see gram matrix conditioned solving linear system adding small multiple identity matrix jk jk jk tr jk tr denotes trace 
amounts adding regularization term reconstruction cost measures summed squared magnitude weights 
consider effect term limit 
regularization term acts penalize large weights exploit correlations level precision data sampling process 
may introduce robustness noise outliers 
form regularization example compute embeddings 
synthetic manifolds regularization essential neighbors input dimensions 
real data sets requiring dimensionality reduction larger 
computing reconstruction weights typically expensive step lle algorithm 
computation scales number operations required solve kk set linear equations data point 
linear number data points number input dimensions 
weight matrix stored sparse matrix nk nonzero saul roweis elements 
inputs need memory step algorithm fills weights purely local computation 
step eigenvalue problem final step lle compute low dimensional embedding reconstruction weights high dimensional inputs note information captured weights construct embedding actual inputs appear final step algorithm need remain memory weights computed low dimensional outputs minimizing cost function equation fixed weights cost function minimized outputs reconstructed nearly reconstructed weighted linear combinations neighbors computed inputs 
note assignment neighbors locations inputs algorithm dynamically recompute neighbors locations outputs optimize embedding cost function equation rewrite quadratic form involving inner products outputs square nn matrix appears equation ji ki 
matrix sparse symmetric definite 
optimization equation performed subject constraints problem posed 
note translate outputs constant displacement affecting cost equation 
remove translational degree freedom requiring outputs centered origin 
rotate outputs affecting cost equation 
remove rotational degree freedom fix scale constrain unit covariance outer products satisfy dd identity matrix 
constraint output covariance equal identity matrix embodies assumptions different coordinates embedding space uncorrelated second order second reconstruction errors coordinates measured scale third scale order unity 
note assumptions mild 
free rotate homogeneously rescale outputs covariance diagonal order unity 
restricting covariance identity matrix introduces additional assumption embedding coordinates scale 
restrictions optimal embedding trivial global rotation embedding space minimizing equation subject constraints equations 
done ways straightforward find bottom eigenvectors unsupervised learning low dimensional manifolds cost matrix 
bottom top eigenvectors corresponding smallest largest eigenvalues 
equivalence optimization normalized quadratic form computation largest smallest eigenvectors version ritz theorem horn johnson 
optimization performed introducing lagrange multipliers enforce constraints equations 
setting gradients respect vectors zero leads symmetric eigenvalue problem exactly derivations principal component analysis bishop 
eigenvector computation lle form eigenvector computation image segmentation normalized cuts shi malik 
bottom eigenvector matrix discard unit vector equal components represents free translation mode eigenvalue zero 
discarding eigenvector enforces constraint equation outputs zero mean components eigenvectors sum zero virtue orthogonality bottom 
remaining eigenvectors constitute embedding coordinates lle 
note bottom eigenvectors sparse symmetric matrix performing full matrix diagonalization bai 
operations involving exploit representation product sparse matrices giving substantial computational savings large values particular left multiplication subroutine required sparse performed requiring just multiplication multiplication matrix needs explicitly created stored sufficient just store multiply sparser matrix efficient implementation multiplication achieved update followed update final step lle typically computationally expensive 
special optimizations computing bottom eigenvectors scales dn linearly number embedding dimensions quadratically number data points specialized methods sparse symmetric eigenproblems bai reduce complexity subquadratic large problems consider alternative methods optimizing embedding cost function direct descent conjugate gradient methods press iterative partial minimizations lanczos iterations stochastic gradient descent lecun 
note th coordinate output lle corresponds st smallest eigenvector matrix regardless total number outputs computed order calculated 
efficiency convenience compute bottom eigenvectors equation time yielding nested set embeddings successively higher dimensions 

extensions section describe useful extensions basic lle algorithm including handling input form pairwise distances convex reconstructions estimation manifold underlying dimensionality 
saul roweis lle pairwise distances lle algorithm described takes input high dimensional vectors settings user may access data form measurements dissimilarity distance data points 
simple variation lle applied input form 
way matrices pairwise distances analyzed lle just easily mds cox cox distance approaches dimensionality reduction tenenbaum 
derive reconstruction weights data point need compute gram matrix jk nearest neighbors defined equation 
matrix inferring dot products pairwise distances exactly manner mds 
particular consider point neighbors denote symmetric square matrix size records pairwise squared distances points 
slight abuse notation allow indices range 
positive values refer neighbors zero index refers point 
purpose computing gram matrix equation assume loss generality points centered origin 
case dot products exactly terms pairwise squared distances ik terms earlier notation elements matrix store dot products 
elements local gram matrix terms dot products note kk square matrix dimension larger 
terms gram matrix reconstruction weights data point equation 
rest algorithm proceeds usual 
note variant lle fact require complete nn matrix pairwise distances 
data point user needs specify nearest neighbors distances neighbors pairwise distances neighbors 
information conveniently stored sparse matrix 
step algorithm remains identify nearest neighbors identified example smallest non missing elements row distance matrix 
natural wonder variant lle succeed user specifying fewer elements matrix pairwise distances 
shows just preserving pairwise distances nearest neighbors general sufficient recover underlying manifold 
consider dimensional data set points integer coordinates satisfying lie sites planar square lattice 
suppose points coordinates colored black odd coordinates colored white 
degenerate point embedding maps black points origin white points unit away exactly preserves distance point nearest neighbors 
embedding completely fails preserve structure underlying manifold 
unsupervised learning low dimensional manifolds preserving just distances nearest neighbors sufficient recover underlying manifold 
shown trivial point embedding exactly preserves distances point nearest neighbors revealing dimensional structure data 
convex reconstructions rows weight matrix computed second step lle constrained sum may positive negative 
simple geometric terms constraint forces reconstruction data point lie subspace spanned nearest neighbors optimal weights compute projection data point subspace 
additionally constrain weights nonnegative forcing reconstruction data point lie convex hull neighbors 
expensive compute squares solution nonnegativity constraints additional cost usually negligible compared steps lle 
conjunction sum constraint constraint nonnegativity limits weights strictly range 
constraint advantages disadvantages 
hand tends increase robustness linear fits outliers 
hand degrade reconstruction data points lie boundary manifold outside convex hull neighbors 
points negative weights may helpful 
general prescription convex versus linear reconstructions exist cover applications lle 
certain applications may straightforward certify neighborhoods free outliers minimizing dangers unbounded weights may simpler choose nearest neighbors require convex reconstructions 
useful heuristic inspect histogram reconstruction weights obtained nonnegativity constraints 
certain data points large positive negative reconstruction weights may wise re assign neighbors constrain linear reconstructions convex 

particular solve problem quadratic programming minimize jk jk equation subject 
required optimization convex solutions lie edge constraint region judge 
saul roweis applications lle images data sets warning signals arise 
general experience extra constraint convexity especially necessary sampled data 
estimating intrinsic dimensionality data sampled underlying manifold naturally interest estimate manifold intrinsic dimensionality recall pca solves problem linear subspaces dimensionality estimated number eigenvalues sample covariance matrix comparable magnitude largest eigenvalue 
analogous strategy lle perona immediately suggests estimate number eigenvalues comparable magnitude smallest nonzero eigenvalue cost matrix equation 
practice procedure contrived examples data lies essentially linear manifold data sampled especially uniform way lowest nonzero eigenvalues equal nearly equal due symmetry 
generally reliable 
plots eigenvalue spectrum cost matrix equation data sets intrinsic dimensionality 
eigenvalues reveal distinguishing signature plots 
useful rely classical methods pettis estimating intrinsic dimensionality data set 
way estimate dimensionality examining eigenvalue spectra local covariance matrices 
performing essence local pca neighborhood data point ask analyses yield consistent estimate intrinsic dimensionality 
estimate obtained box counting 
suppose consider points neighbors lie distance data uniformly sampled manifold number neighbors scale small intrinsic dimensionality 
robust variations basic idea developed estimate intrinsic dimensionality finite data sets noise degrees freedom brand 
methods prior final step lle set number embedding coordinates computed algorithm 
enforcing intrinsic dimensionality lle normally computes ordered set embedding coordinates assuming particular number 
applications manifold intrinsic dimensionality may known priori user may wish bias results lle embedding particular dimensionality easily visualized 
circumstances second step lle modified simple way suppress spurious noisy degrees freedom force desired intrinsic dimensionality data point idea project neighbors dimensional subspace maximal variance performing squares reconstruction 
subspace computed dominant eigenvectors gram matrix equation 
effect projection limit rank solving reconstruction weights 
note far better way limit rank gram matrix simply reducing number nearest neighbors 
reconstruction weights computed rank limited gram matrix minimum norm solution squares problem 
unsupervised learning low dimensional manifolds eigenvalues third step lle reliable indicators intrinsic dimensionality 
plots show smallest nonzero eigenvalues embedding cost matrix equation data sets 
gap eigenvalues reveals true dimensionality regularly sampled data plane sphere 
similar signature randomly sampled data bottom manifolds 
example method shows results applying lle scanned images handwritten digits usps data set hull 
digits zero taken zip codes buffalo postal envelopes 
images preprocessed downsampling resolution quantizing grayscale intensity levels 
inputs lle raw pixel values 
results neighbors projected saul roweis embeddings handwritten digits nearest neighbors digit 
inputs grayscale images handwritten numerals zero taken usps data set hull resolution maximum manifold dimensionality enforced singular value decomposition local gram matrices described section 
top panels show coordinates discovered lle 
digit classes labelled clustered just dimensions 
classes overlap dimensions typically separated bottom panels show versus third fifth seventh lle coordinates 
dimensional subspace performing squares reconstructions computing weights interestingly resulting embedding provides low dimensional clustering handwritten digits lle labels distinguishing different digits 
note digit classes confounded certain projections separated 
enforcing dimensionality essential effect interesting see number yield accurate clustering higher order coordinates discovered lle playing important role second 
unsupervised learning low dimensional manifolds 
embeddings mappings lle provides embedding fixed set training data algorithm applied 
need generalize results lle new locations input space 
example suppose asked compute output corresponding new input principle rerun entire lle algorithm original data set augmented new input 
large data sets high dimensionality approach prohibitively expensive 
purposes generalization useful derive explicit mapping high low dimensional spaces lle require expensive eigenvector calculation query 
natural question construct mapping results lle previous run algorithm 
section describe possible solutions non parametric parametric problem 
non parametric model non parametric solution relies natural mapping low high dimensional spaces lle 
particular compute output new input identify nearest neighbors training inputs ii compute linear weights best reconstruct neighbors subject sum constraint iii output sum outputs corresponding neighbors non parametric mapping embedding space input space derived manner compute input new output identify nearest neighbors training outputs compute analogous reconstruction weights output 
note direction exist option rerunning lle augmented data set 
evaluate usefulness non parametric mapping investigated lle front statistical pattern recognition 
dimensionality reduction form feature extraction simplify accelerate algorithms machine learning 
compared features produced lle pca benchmark problem handwritten digit recognition 
raw data images handwritten digits usps database described section 
half images examples digit training half testing 
test images lle features computed nearest neighbor interpolation training images described pca features computed subtracting mean training images projecting principal components 
simple classifiers implemented nearest neighbor classifier chosen optimize leave performance training set log linear classifier obtained multiclass logistic regression softmax regression bishop 
parameters softmax regression set maximize conditional log likelihood training set 
features classifiers coordinates discovered lle entire unlabeled training set images projection principal components training data computed running pca training set subtracting sample mean finding leading eigenvectors sample covariance matrix 
lle run nearest neighbors data point compute reconstruction weights 
shows results experiments 
small numbers features lle leads significantly lower error rates pca training test sets test error rates shown 
results suggest nearest neighbor interpolation saul roweis number features test error rate digit classification nn pca lle number features test error rate digit classification softmax regression pca lle comparison lle pca features classification handwritten digits usps database hull 
nearest neighbor nn log linear classifiers evaluated see text details 
plots show error rates test set lle features computed nearest neighbor interpolation 
lle features initially lead better performance crossover error rates occurs number features approaches neighborhood size lle 
test error rate nn applied directly high dimensional digit images feature extraction nearest neighbors 
effective way apply lle test examples 
note crossover error rates occurs number features approaches neighborhood size preprocessing lle 
regime appears lle extract information locally linear fits performance saturates pca continues improve 
neighbors lle extend number meaningful features extracted case lose benefits nonlinearity assuming sampling density data held fixed underlying manifold locally linear fixed scale 
summary non parametric mapping derived nearest neighbor interpolation underlying intuitions lle 
provides simple way generalize new data assumptions local linearity met 
open problem establish asymptotic conditions non parametric mappings yield nearly result rerunning lle augmented data set original plus new inputs 
straightforward implement approach generalization disadvantage requires access entire set previously analyzed inputs outputs potentially large demand storage 
non parametric mapping change discontinuously query points move different neighborhoods inputs outputs concerns motivate parametric model discussed section 
unsupervised learning low dimensional manifolds parametric model methods supervised learning function approximation derive compact mappings generalize large portions input embedding space 
particular take input output pairs lle training data invertible function approximator learn parametric mapping spaces 
discuss approach similar intuitions non parametric counterpart 
results lle consider learn probabilistic model joint distribution input embedding spaces 
joint distribution map inputs outputs vice versa computing expected values 
represent joint distribution propose mixture model mclachlan basford specifically tailored data lies low dimensional manifold 
individual components mixture model represent densities locally linear neighborhoods manifold 
mixture models widely purpose past beymer poggio bregler omohundro hinton leen roweis saul vlassis treatment necessarily brief 
model consider mixture conditional linear models gaussian noise distributions 
describes step generative process high low dimensional vectors discrete hidden variable sampled prior distribution select particular neighborhood manifold 
dimensional vector sampled conditional gaussian distribution mean vector full covariance matrix dimensional vector sampled conditional gaussian distribution mean vector diagonal covariance matrix dd loading matrix describes locally linear mapping low high dimensional observations 
model similar unsupervised mixture factor analyzers rubin thayer ghahramani hinton low dimensional variable observed hidden gaussian distributions nonzero mean vectors full covariance matrices 
distribution exp exp parameters model need estimated data prior probabilities mean vectors full covariance matrices diagonal covariance matrices loading matrices training examples model consist input output pairs dimensional data set dimensional locally linear embedding parameters model learned expectation maximization em algorithm maximum likelihood estimation dempster 
em algorithm iterative procedure attempts maximize total log likelihood observed input output pairs training set 
re estimation formulae model appendix shows model mixture components learned results lle manifold 
gaussian distributions depicted planes centered points normal vectors perpendicular subspaces spanned rows saul roweis mixture linear models learned data sampled surface dimensional manifold 
mixture component parameterizes density locally linear neighborhood manifold 
mixture model components trained input output pairs lle shown top row 
thick lines indicate increasing coordinates manifold light squares normals indicate subspaces modeled individual mixture components 
parameters mixture model estimated em algorithm described section appendix subspaces accurately model locally linear neighborhoods manifold data generated 
note mixture models high dimensional spaces typically plagued poor local minima unsupervised mixture factor analyzers treating hidden variable discover solution quality 
clamping latent variables outputs lle done learning easier 

discussion conclude tracing origins comparing lle known algorithms nonlinear dimensionality reduction mentioning open problems research 
early motivation motivation lle arose extended line mixture models 
number researchers shown mixtures locally linear models parameterize distributions data sets sampled underlying manifolds bregler omohundro leen ghahramani hinton hinton saul 
density models exhibited peculiar degeneracy objective functions measuring squares reconstruction error log likelihood invariant arbitrary rotations reflections local coordinate systems linear model 
words learning algorithms favor consistent alignment local linear models yielded internal representations changed unpredictably traversed connected paths manifold 
unsupervised learning low dimensional manifolds lle designed overcome shortcoming discover single global coordinate system lower dimensionality 
initially imagined coordinate system obtained patching local coordinate systems individual components mixture model hinton revow 
difficulties approach led consider non parametric setting local coordinate systems defined data point nearest neighbors 
main novelty lle lies believe appeal particular symmetries 
reconstruction weights lle capture intrinsic geometric properties local neighborhoods properties invariant translation rotation scaling 
appeal symmetries directly motivated degeneracy observed earlier mixture models 
lle non parametric method studies roweis verbeek fact shown possible learn probabilistic mixture model individual coordinate systems aligned consistent way unifying nonlinear dimensionality reduction density estimation single framework 
approaches rely lle initialize certain parameter estimates overcome difficult problem local maxima 
related eigenvector methods proposed brand teh roweis decouple processes density estimation manifold learning consecutive steps 
local density model degeneracies described fit data 
second internal coordinate systems model post aligned way change likelihood achieves goal learning global structure 
related ongoing core lle uses linear methods squares optimization matrix diagonalization obtain highly nonlinear embeddings 
element nonlinearity introduced step algorithm nearest neighbor search viewed highly nonlinear thresholding procedure 
optimizations straightforward implement involve local minima lle compares favorably implementation cost purely linear methods pca classical mds 
methods lle address problems nonlinear dimensionality reduction may yield superior results 
lle generates sparse eigenvalue problem opposed dense eigenvalue problems pca mds 
advantages scaling computations large high dimensional data sets 
lle illustrates general principle elucidated earlier studies martinetz schulten tenenbaum overlapping local neighborhoods collectively analyzed provide information global geometry 
formulation lle terms reconstruction weights eigenvectors arose somewhat completely unrelated line signal processing saul allen 
lle properly viewed belonging family proposed algorithms eigenvector methods solve highly nonlinear problems dimensionality reduction clustering image segmentation belkin niyogi ng scholkopf shi malik tenenbaum weiss 
algorithms discussed detail avoid pitfalls plague nonlinear approaches autoencoder neural networks demers cottrell kramer self organizing maps durbin kohonen latent variable models bishop principal curves surfaces hastie stuetzle verbeek variants multidimensional scaling cox cox buhmann littman young 
approaches especially saul roweis hill climbing methods guarantees global optimality convergence eigenvector methods tend involve free parameters learning rates initial conditions convergence criteria architectural specifications tuned user set cross validation 
third steps lle similar normalized cut algorithm image segmentation shi malik related laplacian methods clustering ng dimensionality reduction belkin niyogi 
heart algorithms sparse eigenvalue problem derived weighted graph representing neighborhood relations 
belkin niyogi related lle laplacian methods argued approaches understood terms unified framework clustering dimensionality reduction see brand huang bengio 
extensions normalized cut algorithm clustering image segmentation directed graphs yu shi probabilistic settings meila shi interesting explore problems manifold learning 
likewise gives indication lle useful certain types clustering algorithm unsupervised dimensionality reduction handwritten digits largely preserves separation different classes 
different equally successful approach nonlinear dimensionality reduction isomap algorithm tenenbaum tenenbaum 
isomap nonlinear generalization mds embeddings optimized preserve geodesic distances pairs data points say distances manifold data sampled 
distances estimated computing shortest paths large sublattices data 
lle isomap algorithm steps construct graph data point connected nearest neighbors ii compute shortest distance pairs data points paths connect nearest neighbors iii embed data mds preserve distances 
similiar aims isomap radically different philosophy lle laplacian spectral methods discussed 
particular isomap attempts preserve global geometric properties manifold characterized geodesic distances faraway points lle attempts preserve local geometric properties manifold characterized linear coefficients local reconstructions 
lle isomap somewhat different intuitions break tend different errors 
embeddings lle optimized preserve geometry nearby inputs collective neighborhoods inputs overlapping coupling faraway inputs severely attenuated data noisy sparse weakly connected 
common failure mode lle arising manifold undersampled map faraway inputs nearby outputs embedding space 
contrast embedding cost isomap dominated geodesic distances faraway inputs 
embeddings biased preserve separation faraway inputs expense distortions local geometry 
depending application algorithm may appropriate 
difficult example lle shown data generated volume dimensional 
example settings parameters 
failures detected computing pairwise distances outputs testing nearby outputs correspond nearby inputs 
note modify embedding cost function equation include repulsive attractive terms words push non neighbors apart keep neighbors close 
creates sparse eigenvalue problem 
unsupervised learning low dimensional manifolds difficult data set lle 
top data generated volume dimensional 
bottom left dimensional embedding obtained carefully tuned parameter settings number nearest neighbors regularization coefficient 
bottom right typical result lle algorithm fails discover faithful embedding 
different colors symbols reveal embedding different parts 
lle algorithm lead reasonable results 
arguably case data best described belonging collection manifolds different dimensionality 
appear hybrid strategy sort identifying weakly connected components varying number neighbors data point required lle give consistently faithful embeddings 
important differences lle isomap worth mentioning 
terms computational requirements lle involve need solve large dynamic programming problems computing geodesic distances data points 
creates sparse matrices structure exploited savings time space 
efficiency gain especially important attempting diagonalize large matrices 
subsampling data saul roweis certain landmark points isomap optimization relatively sparse expense approximating original objective function 
section conjectured appropriate conditions lle recover conformal mappings mappings locally preserve angles necessarily distances 
mappings generally recovered isomap embeddings explicitly aim preserve distances inputs 
noting de silva tenenbaum proposed variant isomap able recover conformal mappings assumption data distributed uniformly known density low dimensional embedding space 
new algorithm called isomap uses observed density high dimensional input space estimate correct local neighborhood scaling factor conformal mapping 
certain formal guarantees established isomap tenenbaum donoho grimes isomap de silva tenenbaum reformulation lle uses hessian information donoho grimes 
strongest results show asymptotic limit infinite sampling isomap hessian lle recover manifolds locally isometric subsets lower dimensional euclidean space 
isomap subsets open convex hessian lle need open connected 
contrast particularly interesting showing principles isomap lle give rise algorithms provably different properties 
notwithstanding results theoretical understanding algorithms nonlinear dimensionality reduction far complete 
important issues lle non asymptotic behavior finite data sets suitability manifolds conformally isometrically mapped subsets lower dimensional euclidean space 
eigenvector algorithm nonlinear dimensionality reduction kernel pca scholkopf 
approach builds observation pca formulated entirely terms dot products data points 
kernel pca substitutes inner product hilbert space normally euclidean dot products pca 
amounts performing pca nonlinear mapping original inputs different possibly infinite dimensional space intrinsically low dimensional structure easier discover 
williams pointed connection kernel pca metric mds 
lle kernel pca appeal explicitly notion data lies manifold 
viewing elements cost matrix equation kernel evaluations pairs inputs lle seen kernel pca particular choice kernel scholkopf smola see 
related version lle proposed decoste visualize effects different kernels 
focused application lle high dimensional data sampled believed sampled low dimensional manifold 
lle applied data existence underlying manifold obvious 
particular focused real valued signals images relationships categorial discrete valued quantities analyzed lle 
previous roweis saul example applied lle documents text viewed nonlinear alternative traditional method latent semantic analysis deerwester 
application certain limitations algorithm kept mind 
manifolds suitable lle asymptotic limit infinite data 
handle manifolds sphere torus admit uniformly continuous mapping plane simon 
likewise embed data sets unsupervised learning low dimensional manifolds intrinsic dimensionality parts space example structure better described fractal 
needed settings 
closely related issue cope poorly nonuniformly sampled data investigated 
success lle hinges sufficiently dense sampling underlying manifold ham suggests bootstrapping methods self consistency constraints improve algorithm performance smaller data sets 
summary provided thorough survey lle algorithm details implementation assortment possible uses extensions relation eigenvector methods clustering nonlinear dimensionality reduction 
lle course unsupervised learning algorithm require labeled inputs types feedback learning environment 
oft criticism unsupervised algorithms attempt solve harder problem necessary particular task cases wrong problem altogether 
view lle belongs new class unsupervised learning algorithms removes force argument 
new algorithms strong parametric assumptions distinguished simple cost functions global optimizations potential exhibit highly nonlinear behavior 
expect algorithms broadly useful areas information processing particularly tool simplify accelerate forms machine learning high dimensional spaces 
authors graf lecun frey providing data experiments 
platt hoppe zemel suggested method section enforcing intrinsic dimensionality recovered manifold jaakkola pointed certain properties convex reconstructions discussed section 
tenenbaum niyogi donoho grimes useful discussions hinton revow sharing preprint nonlinear dimensionality reduction locally linear models suggesting role uniform continuity constraint mappings learnable lle 
grateful anonymous reviewers comments helped improve nearly section 
roweis acknowledges support national sciences engineering research council canada institute robotics intelligent systems iris 
appendix em algorithm mixture linear models section update rules em algorithm section 
algorithm estimate parameters section generative model input output pairs lle denoted derivation special case full derivation em algorithm mixtures factor analyzers ghahramani hinton repeated 
step em algorithm uses bayes rule compute posterior probabilities 
saul roweis step uses posterior probabilities re estimate parameters model 
simplify updates step introduce notation zn zn zn zn zn zn elements mn matrices number mixture components number examples 
terms notation step consists updates performed order shown zn zn zn zn ii zn zn 
total log likelihood log computed iteration marginal likelihood input output pair denominator equation 
updates equations derived standard way auxiliary function em algorithms dempster 
lead monotonic improvement total log likelihood converge stationary point model parameter space 
attias 
independent factor analysis 
neural computation 
bai demmel dongarra ruhe van der vorst 
templates solution algebraic eigenvalue problems practical guide 
society industrial applied mathematics philadelphia 
belkin niyogi 
laplacian eigenmaps spectral techniques embedding clustering 
dietterich becker ghahramani editors advances neural information processing systems pages cambridge ma 
mit press 
bengio vincent 
learning eigenfunctions similarity linking spectral clustering kernel pca 
technical report departement informatique recherche universite de montreal 
beymer poggio 
image representation visual learning 
science 
bishop williams 
gtm generative topographic mapping 
neural computation 
unsupervised learning low dimensional manifolds bishop 
neural networks pattern recognition 
oxford university press oxford 
brand 
charting manifold 
becker thrun obermayer editors advances neural information processing systems cambridge ma 
mit press 
brand huang 
unifying theorem spectral embedding clustering 
bishop frey editors proceedings ninth international workshop intelligence statistics key west fl january 
bregler omohundro 
nonlinear image interpolation manifold learning 
tesauro touretzky leen editors advances neural information processing systems pages cambridge ma 
mit press 
graf 
sample synthesis photo realistic talking heads 
proceedings computer animation pages 
ieee computer society 
cox cox 
multidimensional scaling 
chapman hall london 
dayan hinton neal zemel 
helmholtz machine 
neural computation 
de silva tenenbaum 
unsupervised learning curved manifolds 
proceedings msri workshop nonlinear estimation classification 
springer verlag 
decoste 
visualizing kernel feature spaces kernelized locally linear embedding 
proceedings eighth international conference neural information processing iconip shanghai china november 
deerwester dumais landauer furnas harshman 
indexing latent semantic analysis 
journal american society information science 
demers cottrell 
nonlinear dimensionality reduction 
hanson cowan giles editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
donoho grimes 
isomap recover natural parameterization families articulated images 
technical report department statistics stanford university august 
donoho grimes 
hessian eigenmaps locally linear embedding techniques highdimensional data 
proceedings national academy arts sciences 
durbin 
analogue approach traveling salesman problem elastic net method 
nature 
saul roweis van der vorst 
jacobi davidson style qr qz algorithms reduction matrix pencils 
siam journal scientific computing 
friedman bentley finkel 
algorithm finding best matches logarithmic expected time 
acm transactions mathematical software 
fukunaga olsen 
algorithm finding intrinsic dimensionality data 
ieee transactions computers 
ghahramani hinton 
em algorithm mixtures factor analyzers 
technical report crg tr revised february department computer science university toronto may 
gray moore 
body problems statistical learning 
leen dietterich tresp editors advances neural information processing systems pages cambridge ma 
mit press 
ham lee saul 
learning high dimensional correspondences low dimensional manifolds 
submitted publication 
hastie stuetzle 
principal curves surfaces 
journal american statistical association 
hinton dayan revow 
modeling manifolds handwritten digits 
ieee transactions neural networks 
hinton revow 
mixtures factor analyzers segmentation pose estimation 
unpublished 
hinton sejnowski editors 
unsupervised learning map formation foundations neural computation 
mit press cambridge ma 
horn johnson 
matrix analysis 
cambridge university press cambridge 
hull 
database handwritten text recognition research 
ieee transaction pattern analysis machine intelligence may 
hyvarinen 
independent component analysis presence gaussian noise maximizing joint likelihood 
neurocomputing 
indyk 
dimensionality reduction techniques proximity problems 
proceedings eleventh acm siam symposium discrete algorithms soda pages 
jolliffe 
principal component analysis 
springer verlag new york 
judge 
inequality restrictions regression analysis 
journal american statistical association march 
leen 
dimension reduction local principal component analysis 
neural computation 
unsupervised learning low dimensional manifolds karger ruhl 
finding nearest neighbors growth restricted metrics 
proceedings fourth acm symposium theory computing stoc pages 

intrinsic dimension estimation packing numbers 
becker thrun obermayer editors advances neural information processing systems cambridge ma 
mit press 
buhmann 
data visualization multidimensional scaling deterministic annealing approach 
pattern recognition 
kohonen 
self organization associative memory 
springer verlag berlin 
kramer 
nonlinear principal component analysis autoassociative neural networks 
aiche journal 
lecun bottou orr 
muller 
efficient backprop 
orr muller editors neural networks tricks trade 
springer 
littman dean buja 
visualizing embedding objects euclidean space 
newton editor computing science statistics proceedings th symposium interface pages 
interface foundation north america 
martinetz schulten 
topology representing networks 
neural networks 
mclachlan basford 
mixture models inference applications clustering 
marcel dekker 
meila shi 
learning segmentation random walks 
solla leen 
muller editors advances neural information processing systems pages cambridge ma 
mit press 
moore connolly gray ii schneider szalay wasserman 
fast algorithms efficient statistics point correlation functions 
proceedings mpa mpe conference mining sky 
ng jordan weiss 
spectral clustering analysis algorithm 
dietterich becker ghahramani editors advances neural information processing systems pages cambridge ma 
mit press 
omohundro 
construction algorithms 
technical report tr international computer science institute december 
omohundro 
efficient function constraint classification learning 
lippmann moody touretzky editors advances neural information processing pages san mateo ca 
morgan kaufmann 
perona 
grouping dimensionality reduction locally linear embedding 
dietterich becker ghahramani editors advances neural information processing systems pages cambridge ma 
mit press 
saul roweis pettis bailey jain dubes 
intrinsic dimensionality estimator near neighbor information 
ieee transactions pattern analysis machine intelligence pami 
simon 
embedding images non flat spaces 
technical report wu cs washington university december 
press teukolsky vetterling flannery 
numerical recipes art scientific computing 
cambridge university press 
roweis 
em pca spca 
kearns jordan solla editors advances neural information processing systems pages cambridge ma 
mit press 
roweis saul 
nonlinear dimensionality reduction locally linear embedding 
science 
roweis saul hinton 
global coordination locally linear models 
dietterich becker ghahramani editors advances neural information processing systems pages cambridge ma 
mit press 
rubin thayer 
em algorithms ml factor analysis 
psychometrika 
saul allen 
periodic component analysis eigenvalue method representing periodic structure speech 
leen dietterich tresp editors advances neural information processing systems pages cambridge ma 
mit press 
saul 
maximum likelihood minimum classification error factor analysis automatic speech recognition 
ieee transactions speech audio processing 
scholkopf smola 
learning kernels support vector machines regularization optimization 
mit press cambridge ma 
scholkopf smola 
muller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
seung lee 
manifold ways perception 
science 
shi malik 
normalized cuts image segmentation 
ieee transactions pattern analysis machine intelligence pami pages august 
young 
nonmetric individual differences multidimensional scaling alternating squares method optimal scaling features 
psychometrika 
tarjan 
depth search linear graph algorithms 
siam journal computing 
tarjan 
data structures network algorithms 
cbms volume 
society industrial applied mathematics 
unsupervised learning low dimensional manifolds teh roweis 
automatic alignment hidden representations 
becker thrun obermayer editors advances neural information processing systems cambridge ma 
mit press 
tenenbaum 
mapping manifold perceptual observations 
jordan kearns solla editors advances neural information processing systems pages cambridge ma 
mit press 
tenenbaum de silva langford 
global geometric framework nonlinear dimensionality reduction 
science 
tipping bishop 
mixtures probabilistic principal component analysers 
neural computation 
verbeek vlassis 
coordinating mixtures probabilistic principal component analyzers 
technical report ias uva computer science institute university amsterdam netherlands february 
verbeek vlassis 
segments algorithm finding principal curves 
pattern recognition letters 
vlassis 
supervised dimension reduction intrinsically lowdimensional data 
neural computation 
weiss 
segmentation eigenvectors unifying view 
proceedings ieee international conference computer vision iccv pages 
williams 
connection kernel pca metric multidimensional scaling 
leen dietterich tresp editors advances neural information processing systems pages cambridge ma 
mit press 
yu shi 
grouping directed relationships 
figueiredo jain editors proceedings third international workshop energy minimization methods computer vision pattern recognition volume lecture notes computer science pages 
springer 

