lstm recurrent networks learn simple context free context sensitive languages felix gers schmidhuber 
ch 
ch idsia switzerland www 
idsia 
ch previous learning regular languages exemplary training sequences showed long short term memory lstm outperforms traditional recurrent neural networks rnns 
demonstrate lstm superior performance context free language cfl benchmarks recurrent neural networks rnns show works better previous hardwired highly specialized architectures 
best knowledge lstm variants rnns learn simple context sensitive language csl abc index terms recurrent neural networks long short term memory context free languages context sensitive languages 
recurrent neural networks rnns remarkably general sequence processing devices siegelmann sontag 
principle applicable tasks reach hidden markov models hmms discrete symbolic grammar learning algorithms lee sakakibara 
instance rnns hmms limited discrete state spaces 
rnns hmms typically ignore information conveyed size temporal delays relevant events rhythmic patterns 
hmm designer principle deal finite set time delays possible observations devoting separate internal state possible delay general cumbersome inefficient 
rnns deal noisy input sequences osborne briscoe 
rnns perform gradient descent general continuous space potentially noise resistant algorithms distributed internal memories mapping real valued input sequences real valued output sequences noise tends learning harder compare maass maass sontag 
standard rnns pearlmutter plagued major practical problem gradient total output error respect previous inputs quickly vanishes time lags relevant inputs errors increase hochreiter bengio simard frasconi hochreiter schmidhuber 
standard rnns fail learn presence time lags exceeding discrete time steps relevant input events target signals hochreiter schmidhuber 
long short term memory lstm method hochreiter schmidhuber affected problem 
lstm learn bridge minimal time lags excess discrete time steps hochreiter schmidhuber enforcing constant error flow constant error cecs special units loss short time lag capabilities 
multiplicative gate units learn open close access constant error flow 
lstm learning algorithm efficient previous rnn algorithms real time recurrent learning rtrl robinson fallside williams back propagation time bptt williams peng werbos local space time computational complexity time step weight 
research lstm concentrated improving structure adaptive gates surrounding cecs 
focus lstm variant gets schmidhuber provides gates direct access cec states learn selectively reset memory contents produce stable results presence input streams 
previous showed lstm outperforms traditional rnn algorithms numerous tasks involving real valued discrete inputs targets gets schmidhuber cummins hochreiter schmidhuber including tasks require learn rules regular languages rls describable deterministic yc output eating sc peephole ec output gate net peephole memorizing cell forgetting state forget gate ip input squashing input gate ne lstm memory block cell 
finite state automata dfa casey siegelmann blair pollack lehmann zeng goodman smyth 
remained unclear lstm superiority carries tasks involving context free languages discussed rnn literature sun giles chen lee wiles elman grunwald wiles rodriguez wiles elman rodriguez wiles 
recognition requires functional equivalent stack 
conceivable lstm just right bias rls fail 
focus common benchmarks rnn literature anb study questions lstm learn functional equivalent pushdown automaton 
training sequences size generalize 
stable solutions 
lstm outperform previous approaches 
apply lstm context sensitive language csl 
include include rls 
focus classic example csl cfl section 
general csl recognition requires linear bounded automaton special turing machine tape length linear input size 
language simplest generated tree adjoined grammar recognized called embedded push automaton vijay shanker finite state automaton access counters incremented decremented 
knowledge rnn able learn csl 
lstm lstm forget gates introduced peephole connections gers schmidhuber 
forget gates shown essential problems involving continual long input strings gers 
peephole connections allow gates access cec memory block 
proved essential numerous symbolic real valued counting tasks gers schmidhuber 
lstm variant forget gates clearly superior traditional lstm method choice 
basic unit lstm network memory block containing memory cells adaptive multiplicative gating units shared cells block 
memory cell core recurrently self connected linear unit called constant error carousel cec activation called cell state 
cecs enforce constant error flow overcome fundamental problem previous rnns prevent error signals decaying quickly back time 
adaptive gates control input output cells input output gate learn reset cell state contents date forget gate 
gates units outputs multiplicatively influence connections linear unit holding cell state 
peephole connections connect cec gates 
errors cut leak memory cell gate serve change incoming weights 
effect cecs part system errors flow back forever gates learn nonlinear aspects sequence processing 
lstm updates efficient significantly affecting learning power cecs permit lstm bridge huge time lags discrete time steps relevant events traditional rnns fail learn presence step time lags despite requiring complex update algorithms 
lstm local space time schmidhuber efficient rnn algorithms real time recurrent learning rtrl robinson fallside williams zipser 
lstm computational complexity memory block time step weight sj sj number cells block 
sj typically small constant fact equal computational complexity 
back propagation time bptt williams peng werbos computational complexity rtrl worse local time 
needs store activation values observed sequence processing stack potential unlimited size 
essentially lstm complexity bptt williams zipser truncates errors steps works bptt experience lstm fact bptt 
forward pass cell output yc calculated current cell state sc sources input input cell net inputs input gates forget gates output gates 
consider discrete time steps 
single step involves update units forward pass computation error signals weights backward pass 
indexes memory blocks indexes memory cells block cells denotes th cell th memory block weight connection unit unit index ranges source units specified network topology index mj ranges source units cecs th memory block 
gates ft logistic sigmoid range 
discrete time step phase update scheme computes order 
input gate activation yin forget gate activation cell input cell state sc 
output gate activation cell output yc 
step la lb 
input gate activation yin forget gate activation computed follows ym sc yin fin net wm ym wc sc net peephole connections input gate forget gate incorporated equation including cecs containing cell states memory block source units 
step lc 
state memory cell sc calculated adding squashed gated input cell state previous time step sc multiplied gated forget gate activation wcm ym sc sc yin step 
output gate activation computed follows ym sc yt fot equation includes peephole connections output gate cecs memory block cell states sc step 
cell output yc computed follows cy assuming network topology standard input wet hidden layer consisting memory blocks standard output layer equations output units nets wm ym net ranges units feeding output units fk output squashing function 
gradient backward pass essentially lstm backward pass efficient fusion slightly modified truncated bptt customized version rtrl details see hochreiter schmidhuber gers 
iterative gradient descent minimizing objective function usual mean squared error time steps sequences 
appendix lists pseudo code entire algorithm see gers full derivation 
bptt rtrl lstm learning algorithm local space time 
experiments network sequentially observes exemplary symbol strings language input symbol time 
traditional approach rnn literature formulate task prediction task 
time step target predict possible symbols including string symbol symbol occur step possible symbols predicted 
network sequentially observes exemplary symbol strings language input symbol time referred input sequences 
input sequence begins start symbol 
empty string consisting st considered part language 
string accepted predictions correct 
rejected 
prediction task equivalent classification task classes accept reject system prediction errors strings outside language 
system learned language string size able correctly predict strings size symbols encoded locally dimensional binary vectors non zero component equals number language symbols plus start symbol input string symbol output input units output units 
signifies symbol set decision boundary network output 
cfl anb sun wiles elman wiles rodriguez 
strings input sequences form input output vectors dimensional 
prior occurrence sequence beginnings possible step 
input target cfl rodriguez wiles 
second half string mirror language completely predictable half 
task involves intermediate time lag length 
input output vectors 
prior occurrence symbols possible step 
input target csl input output vectors dimensional 
prior occurrence symbols possible step 
input target training testing learning testing alternate epoch training sequences freeze weights run test 
strings processed correctly training necessary test frozen weights weight changes executed 
apart ensuring learning training set test determines generalization performance optimize say validation set 
training test sets incorporate legal strings length anb training strings random order 
exemplars class 
training stopped training sequences accepted training sequences 
generalization set largest accepted test set 
weight changes sequence 
apply momentum algorithm plaut nowlan hinton learning rate momentum parameter 
results averages inde trained networks different weight initializations initializations identical experiment 
cfl anb study training sets 
test sets sequences length cfl training sets set rodriguez wiles sequences length sequences length csl study kinds training sets 
case asks major generalization step impossible glance similar training sequences sizes differ learn process sequences arbitrary size 
test sets sequences length network topology experimental parameters input units fully connected hidden layer consisting memory blocks cell 
cell outputs fully connected cell inputs gates output units direct shortcut connections input units 
task selected topology minimal number memory blocks solved task extensive parameter optimization 
larger topologies led disadvantages increase computational complexity 
gates cell output unit biased 
bias weights input gate forget gate output gate initialized respectively 
critical values empirically experiments 
bias initialization cell states initially close zero training progresses biases progressively negative allowing serial activation cells active participants network computation 
forget gates start closed cells initially remember 
tried different bias configurations results qualitatively supports claim precise initialization critical see hochreiter schmidhuber gers schmidhuber cummins additional evidence vein 
weights initialized randomly range 
cell input squashing function identity function 
squashing function output units sigmoid function range 
cfl anb memory block cell 
peephole connections adjustable weights peephole unit unit bias connections 
cfl blocks cell resulting adjustable weights peephole unit unit bias connections 
csl topology language input output units resulting adjustable weights peephole unit unit bias connections 
layer lstm topology single input output 
recurrence limited hidden layer consisting single lstm memory block single cell 
unit unit connections shown bias peephole connections 
table previous results cfl anb showing left right number hidden units state units values training number training sequences number solutions trials largest accepted test set 
hidden train 
train 
sol best test units set str 
tri 
sun wiles elman wiles rodriguez previous results cfl anb published results anb language summarized table 
rnns trained plain bptt tend learn just reproduce input wiles elman wiles rodriguez 
sun 
highly specialized architecture neural pushdown automaton generalize sun das giles sun 
cfl rodriguez wiles bptt rnns hidden nodes 
training strings training set strings 
best network generalized sequences length 
learned complete training set 
csl knowledge previous rnn learned csl 
lstm results cfl solved training sets table 
small training sets sufficient perfect generalization tested maximum 
note long sequences kind require stable finely tuned control network internal counters casey 
performance better previous approaches largest set learned specially designed neural push automaton sun das 
required training sequences length test sequences 
training set sun training set augmented stepwise sequences misclassified testing final accepted set random sequences length exact generalization performance unclear 
applying brute force search weights best network rodriguez 
improves performance acceptance 
train 
train 
si 
generalization set str 
set table results language showing left right values training average number training sequences best generalization achieved percentage correct solutions best generalization average networks parenthesis 
train 
train 
si 
generalization set str 
set table results language showing left right values training average number training sequences best generalization achieved percentage correct solutions best generalization average networks parenthesis 
lstm generalized best previous result see table generalized slightly larger training set 
contrast wiles observe networks forgetting solutions training progresses 
previous approaches lstm reliably finds solutions generalize 
fluctuations generalization performance different training sets table may due fact optimize generalization performance validation set 
simply stopped epoch sequences training set learned 
cfl training set solved training sequences best network generalized strings length symbols processed correctly average generalization set rn strings length symbols processed correctly learned training sequences average 
training set solved 
training sequences best network generalized strings length symbols processed correctly 
average generalization set strings length symbols processed correctly learned training sequences average 
previous approach rodriguez wiles lstm easily learns complete training set reliably finds solutions generalize 
csl lstm learns training sets trials training set generalizes table 
small training sets perfect generalization tested maximum sequences length 
absence short training sequences lstm learned see bottom half table 
modified training procedure presenting exemplary string providing possible symbols targets symbol occurs current exemplar 
led slightly longer training durations significantly change results 
gb input target scl yc yl time cfl anb test run network solutions 
top network output yk middle cell state sc cell output yc bottom activations gates input gate yin forget gate output gate 
analysis solutions discovered lstm 
cfl anb shows test run network solution 
cell state sc increases symbols fed network decreases step size symbols fed 
sequence beginnings symbols observed step size smaller due closed input gate triggered sc 
results overshooting initial value sc sequence turn triggers opening output gate turn leads prediction sequence termination 
cfl behavior typical network solution shown 
network learned establish control counters 
symbol pairs treated separately different cells ct respectively 
cell tracks difference number observed symbols 
opens string predicts final cell ct treats embedded bmb substring similar way 
values stored manipulated cell output gate remains closed 
prevents cell disturbing rest network protects cec incoming errors 
csl network solutions combination counters instantiated separately memory blocks 
second cell counts input symbol 
counts input causes input gate close forget gate reset cell state sc 
second memory block respectively 
opening output gate block indicates string prediction triggered peephole connection 
network generalize short strings training strings language see table 
gate activations show activations slightly drift input stays constant 
solutions take state drift account lo aaa aa bbb bbb bbb input target scl yc esc yc 

yq time cfl test run network solution 
top network output yk middle cell state sc cell output yc bottom activations gates input gate yin forget gate ye output gate 
case sequences shorter longer observed training examples 
imposes limit generalization directions longer shorter strings 
solutions drift generalize better 
improvements 
better results obtained increased training time stepwise reduction learning rate done rodriguez 
distribution lengths sequences training set affects learning speed generalization 
set containing long sequences improves generalization longer sequences 
omitting sequence rn typically learned effect 
training sets short long sequences learned quickly uniformly distributed ones 
related tasks 
bal regular language related anb sense requires learn counter counter needs counting 
task equivalent continual spike timing task gers schmidhuber learned lstm 
hand hardwired solution learning second order rnn worked values grunwald 
tasks peephole connections mandatory 
output gates remain closed substantial time periods input sequence presentation compare figures period triggered peephole connections 
input ta ba ba ba ba ba cccc target 
scl esc yc yq time csl test run network solution system scales sequences length 
top network output yk middle cell state sc cell output yc bottom activations gates input gate yin forget gate output gate 
long short term memory lstm clearly outperforms previous rnns regular language benchmarks previous research context free language cfl benchmarks 
learns faster generalizes better 
lstm rnn learn context sensitive language 
studied may learnable certain discrete symbolic grammar learning algorithms sakakibara lee osborne briscoe exhibit task specific bias designed solve numerous sequence processing tasks involving noise real valued inputs internal states continuous output trajectories lstm solves easily hochreiter schmidhuber gers 
findings reinforce perception lstm general promising adaptive sequence processing device wider field potential applications alternative rnns 
acknowledgment supported snf long short term memory 
peephole lstm forget gates pseudo code init network reset cecs sc partials ds activations forward pass input units current external input roll activations cell states loop memory blocks indexed step la input gates em ev fi step lb forget gates net ff ej em step lc cecs cell states loop sj cells block indexed em wcm sc cy yin step output gate activation sj em ev sc cell outputs loop qj cells block indexed yc sc loop memory blocks output units net ym yk net partial derivatives loop memory blocks indexed loop sj cells block indexed osc cells jv osc ds input gates cj jv jv yj fn loop peephole connections cells indexed 
osc ds forget gates dsn ow dsm dsm sc net loop peephole connections cells indexed jv jv loops cells memory blocks backward pass error injected errors injection error ek output units net loop memory blocks indexed output gates internal state error loop sj cells block indexed esc loop memory blocks weight updates output units ym loop memory blocks indexed output gates ol ou input gates esc loop peephole connections cells indexed forget gates es loop peephole connections cells indexed cells loop qj cells block indexed eq ds loop memory blocks bengio simard frasconi 

learning long term dependencies gradient descent difficult 
ieee transactions neural networks 
blair pollack 

analysis dynamical recognizers 
neural computation 
casey 

dynamics discrete time computation application recurrent neural networks finite state machine extraction 
neural computation 
das giles sun 

learning context free grammars capabilities limitations recur rent neural network external stack memory 
proceedings fourteenth annual conference cognitive science society pp 

san mateo ca morgan kaufmann publishers 
gers schmidhuber 

recurrent nets time count 
proc 
ijcnn int 
joint conf 
neural networks 
como italy 
gers schmidhuber cummins 

learning forget continual prediction lstm 
neural computation 
hochreiter 

untersuchungen zu 
diploma thesis institut informatik lehrstuhl prof brauer technische nchen 
see www informatik 
de hochreiter schmidhuber 

long short term memory 
neural computation 
lehmann 

computation recurrent neural networks counters iterated function systems 
antoniou slaney eds advanced topics artificial intelligence proceedings th australian joint conference artificial intelligence vol 

berlin heidelberg springer 
lee 

learning context free languages survey literature tech 
rep 
tr 
center research computing technology harvard university cambridge massachusetts 
maass 

effect analog noise discrete time analog computations 
neural computation 
maass sontag 

analog neural nets gaussian common noise distribution recognize arbitrary regular languages 
neural computation 
osborne briscoe 

learning stochastic categorial grammars 
proceedings assoc 
comp 
linguistics comp 
nat 
lg 
learning conll workshop pp 

madrid 
citeseer nj nec com osborne earning html pearlmutter 

gradient calculations dynamic recurrent neural networks survey 
ieee transactions neural networks 
plaut nowlan hinton 

experiments learning back propagation tech 
rep nos 
cmu cs 
pittsburgh pa carnegie mellon university 
robinson fallside 

utility driven dynamic error propagation network tech 
rep 
cued infeng tr 
cambridge university engineering department 
rodriguez wiles 

recurrent neural networks learn implement symbol sensitive counting 
advances neural information processing systems vol 

mit press 
rodriguez wiles elman 

recurrent neural network learns count 
connection science 
sakakibara 

advances grammatical inference 
theoretical computer science 
schmidhuber 

neural bucket brigade local learning algorithm dynamic feedforward recurrent networks 
connection science 
schmidhuber 

learning complex extended sequences principle history compression 
neural computation 
siegelmann 

theoretical foundations recurrent neural networks 
unpublished doctoral tion rutgers new brunswick rutgers state new jersey 
siegelmann sontag 

turing computability neural nets 
applied mathematics letters 
grunwald 

recurrent network performs contextsensitive prediction task 
proceedings th annual conference cognitive science society 
erlbaum 
sun giles chen lee 

neural network pushdown automaton model stack learning simulations technical report 
cs tr 
university maryland college park 
wiles 

learning context free task recurrent neural network analysis stability 
proceedings fourth biennial conference australasian cognitive science society 
vijay shanker 

descriptions trees tree adjoining grammar 
computational linguistics 
werbos 

generalisation backpropagation application recurrent gas market model 
neural networks 
wiles elman 

learning count counter case study dynamics activation landscapes recurrent networks 
proceedings seventeenth annual conference cognitive science society pp 
pages 
cambridge ma mit press 
williams peng 

efficient gradient algorithm line training recurrent network trajectories letter 
neural computation 
williams zipser 

gradient learning algorithms recurrent networks computational complexity 
chauvin rumelhart eds back propagation theory architectures applications pp 

hillsdale erlbaum 
zeng goodman smyth 

discrete recurrent neural networks grammatical inference 
ieee transactions neural networks 

