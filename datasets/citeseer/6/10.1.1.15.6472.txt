communication optimization aspects parallel programming models hybrid architectures rolf gerhard published international journal high performance computing applications 
sage publications 
summary hpc systems clusters shared memory nodes 
parallel programming combine distributed memory parallelization node inter connect shared memory parallelization inside node 
hybrid mpi openmp programming model compared pure mpi compiler parallelization parallel programming models hybrid architectures 
focuses bandwidth latency aspects programming paradigms separate optimization communication computation 
benchmark results hybrid pure mpi communication 
analyzes strength parallel programming models clusters smp nodes 
keywords 
openmp mpi hybrid parallel programming threads mpi hpc 
motivation today systems high performance computing hpc clusters smp symmetric multi processor nodes hybrid architectures shared memory systems inside node distributed memory parallel dmp system node boundaries 
hybrid systems range small clusters dual cpu pcs largest systems asci systems usa earth simulator japan nodes equipped vector cpus aggregated peak performance ranked number top lists 
achieve minimal parallelization high performance computing center university stuttgart stuttgart germany de www de people erlangen erlangen germany gerhard uni erlangen de overhead hybrid programming model proposed openmp automatic compiler thread parallelization inside smp node message passing mpi node interconnect 
hybrid mpi openmp programming model clusters smp nodes applications small bene reported climate model calculations gordon bell prize sc losses reported compared pure mpi model shown discrete element modeling algorithm 
hybrid model smp node executing multi threaded mpi process 
pure mpi programming processor executes singlethreaded mpi process cluster smp nodes treated large mpp massively parallel processing system 
major drawbacks hybrid programming model simple usage hybrid approach mpi routines invoked outside parallel regions threads master thread sleeping mpi routines executed 
discuss phenomenon hybrid mpi openmp programming strategies 
sect 
overview hybrid programming models 
sect 
shows di erent methods combine mpi openmp 
rules hybrid programming discussed sect 
pure mpi hybrid architectures sect 

sect 
presents benchmark results communication models 
sect 
compares mpi programming models compiler parallelization 
sect 
analyze optimization communication computation separation mpi model combination openmp clusters 
computing applications programming models hybrid architectures available programming models depend type cluster hardware 
node interconnect allows cache coherent non cache coherent non uniform memory access ccnuma memory access inside smp node cluster interconnect implemented instructions programming models need shared memory access cluster 
includes openmp cluster usage nested parallelism inside openmp openmp cluster extensions primarily rst touch mechanism data distribution extensions 
cluster extensions may bene availability software shared virtual memory svm distributed shared memory dsm 
nasa ames hybrid approach developed 
parallelization organized levels upper level process lower level process multi threaded openmp 
processes fortran wrapper system shared memory module shm allows fork processes initialize shared memory segment associate portions segment cray pointer arrays process barrier synchronization processes 
system named multi level parallelism mlp allows exible dynamic simple way load balancing start parallel region inside mlp process number threads number cpus may adapted 
mlp proprietary method nasa ames programming style shm non proprietary 
node interconnect requires di erent methods accessing local cluster wide memory remote direct memory access rdma methods available node access memory node interaction cpu node programming methods available systems programmed array fortran uni ed parallel upc 
array fortran access array process thread done additional trailing array subscript square brackets addressing process thread 
language extensions program clusters smp nodes add message passing overhead overhead additional copies 
key issue widespread usage upc fortran availability portable compiling systems wide range platforms clear development path achieve optimal performance mpi early mpich implementation 
approach rdma hardware sided communication cray library mpi 
methods allow store fetch data memory process spmd environment distributed memory model 
library ported vendors systems 
programming models available rdma class usable numa class interconnects 
third class hardware supports numa access rdma 
pure message passing available node interconnect 
programming models designed class hardware major advantage applicable mentioned classes 
focuses type hardware 
commonly accepted standard message passing nodes message passing interface mpi 
major programming styles pure mpi mpp model uses cpu mpi process hybrid models mpi node interconnect openmp automatic semi automatic compiler thread parallelization inside smp node 
inside node mainly di erent smp parallelization strategies coarse grain spmd style parallelization similar distribution processes message passing program applied method allows similar computational eciency pure mpi parallelization eciency communication major factor comparison hybrid approach pure mpi solution 
focused communication aspects 
ne grained smp parallelization done incremental ort parallelizing loops inside mpi processes 
ef ciency hybrid solution depends eciency computation amdahl law considered levels parallelization communication shown nas parallel benchmarks 
di erent smp parallelization strategies hybrid model studied 
high performance fortran hpf available clusters smps 
hpf hybrid mpi openmp compared pure mpi 
hybrid parallel programming timeline diagrams showing di erent mpi thread support categories mpi 
mpi thread parallelization combination mpi thread parallelization addressed mpi forum sect 
mpi threads 
hybrid programming mpi routine mpi init substituted call mpi init threads input argument named required de ne application requests mpi library output argument provided mpi library tell application thread support available 
mpi libraries may support thread categories higher categories supersets lower ones mpi thread single thread support 
mpi thread master thread allowed call mpi routines 
threads may run application code master thread calls mpi routine 
mpi thread serialized multiple threads may mpi calls thread may execute mpi routine time 
mpi thread multiple multiple threads may call mpi restrictions 
constants mpi threads monotonically increasing 
mpi thread single mpi thread intermediate levels thread support addressed standard mpi process may multi threaded master thread may call mpi routines threads exist parallel threads created parallel region destroyed mpi routine called 
mpi library supporting class return provided mpi thread single thread support lack de nition mpi standard may solved revision mpi standard 
de nition relaxed sense thread may exist call mpi routines threads master thread sleep blocked openmp synchronization 
mpi library supports library return provided mpi thread single 
usually application distinguish openmp parallelization automatic parallelization needs allow calls mpi routines outside openmp parallel regions de ned parallel region team threads sleeping destroyed 
usually category chosen mpi routines called outside parallel regions 
summarize cases case mpi process may multi threaded master thread may call mpi routines outside parallel regions case openmp outside parallelized code automatic parallelization 
de ne additional constant thread value mpi thread single mpi thread 
diagrams illustrate di erent mpi thread support categories 
rules hybrid programming thread de nes simple hybrid programming model mpi openmp mpi routines may called outside parallel regions 
new cache coherence rules openmp guarantee outcome mpi routine visible threads subsequent parallel region outcome threads parallel region visible subsequent mpi routine 
programming model mpi thread achieved computing applications surrounding call mpi routine omp master omp master directives inside parallel region 
careful omp master imply automatic barrier synchronization automatic cache ush entry master section exit 
application wants send data computed previous parallel region wants receive data bu er previous parallel region data received previous iteration barrier implied cache ush necessary prior calling mpi routine prior master section 
data bu er parallel region exit mpi routine master section barrier necessary exit master section 
barriers done master thread executing mpi routine threads sleeping going back case 
rules mpi thread serialized achieved omp single directive implied barrier exit speci ed 
problems taken account 
problems arise serialized communication threads thread arbitrary thread omp single master thread omp master mpi thread multiple allows direct message passing thread node thread node 
reasons thread available nearly clusters hybrid portable parallelization parallelization scheme 
evaluates hybrid model comparing non hybrid pure mpi model described section 
pure mpi hybrid architectures pure mpi model cluster viewed hybrid communication network typically fast communication paths inside smp node slower paths nodes 
important implement mapping communication paths application hybrid communication network cluster 
mpi standard de nes virtual topologies purpose optimization algorithm implemented mpi implementations 
cases important choose ranking mpi comm world 
hitachi sr mpi library allows di erent ranking schemes round robin ranks node ranks node number nodes sequential rank node ranks node user decide scheme may better communication needs application 
pure mpi programming model implies additional message transfers due higher number mpi processes higher number boundaries 
consider example dimensional cartesian domain decomposition 
domain may transfer boundary information neighbors cartesian directions 
bringing model cluster way smp nodes node execute domains belonging cube 
domain communication occurs node node internode communication intra node communication domains inside cube 
domain neighbors inside cube neighbors outside inter node intra node communication amount transferred bytes equivalent 
compare pure mpi model hybrid model assuming domains pure mpi model cube combined super domain hybrid model amount data transferred node interconnect models 
implies pure mpi model total amount transferred bytes inter node plus intra node twice number bytes hybrid model 
ratio shown topology fig 

symmetric case intra node internode communication transfer volume 
benchmark results benchmark results compare communication behavior hybrid mpi openmp model pure mpi model named mpp mpi model 
domain decomposition scenario discussed section compare bandwidth models ratio total communication time presuming pure mpi model total amount transferred data twice amount hybrid model 
benchmark hybrid parallel programming cyclic cyclic avg 
lmax average lmax hybrid mb node mb mpp mb process mb mpp hybrid measured mpp hybrid assumed hybrid concluding table comparing hybrid mpp communication needs 
done hitachi sr nodes nodes available mpi parallel applications 
node cpus 
ective communication benchmark 
accumulates communication bandwidth values communication done mpi process 
determine bandwidth process maximum time needed processes benchmark models application behavior node slowest communication controls real execution time 
compare models benchmark patterns accumulated bandwidth average ring random patterns major benchmark pattern benchmark cyclic dimensional cyclic communication pattern neighbors mpi process additional pattern measured benchmark benchmark measures set di erent message sizes pattern 
pattern subcategories columns table average average bandwidth di erent message sizes byte mb lmax bandwidth measured mb messages 
rows tab 
hybrid accumulated bandwidth hybrid model measured threaded mpi process node mpi processes parentheses bandwidth node mpp accumulated bandwidth pure mpi model mpi processes sequential ranking mpi comm world parentheses bandwidth process mpp hybrid ratio accumulated mpp bandwidth accumulated hybrid bandwidth mpp hybrid considerations sect 
assume ratio transferred data mpp hybrid hybrid compare time models take consideration measured bandwidth values assumption total amount transferred bytes larger mpp model 
ratio hybrid hybrid hybrid mpp mpp mpp hybrid mpp hybrid note comparison done especially optimized topology mapping pure mpi model 
result shows pure mpi communication model faster communication hybrid model 
reasons hybrid model communication done master thread threads inactive 
thread able saturate total available inter node bandwidth due software implementation problems hardware design 
shows similar experiment 
hybrid mpi openmp communication scheme left thread sends inter node messages 
message size times size pure mpi scheme 
cpu communicates vertical internode horizontal intra node direction 
total computing applications parallel communication cartesian topology 
communication time hybrid model ms greater pure mpi communication ms pure mpi total amount transferred data doubled due additional communication 
shows measured transfer time message sizes left diagram ratio transfer time hybrid model transfer time inter node plus intra node communication right diagram 
note hybrid measurements message size re ect inter node data exchange threads communicated master thread message size chosen times larger ranges kb gb 
diagrams show message sizes greater kb pure mpi model faster hybrid model experiment 
smaller message sizes ratio hybrid depends mainly latencies underlying protocols may di er due larger message sizes hybrid model 
fig 
shows behavior ibm sp system power cpus smp node 
intra node communication done ring cpus 
inter node communication aggregated messages hybrid test case times longer pure mpi experiment 
left diagram shows experiment smp nodes cpus left logarithmic scale execution time right scale ratio execution time hybrid experiment time pure mpi experiment 
right diagram shows ratio experiments di erent numbers nodes 
node network adapters 
ratio values larger show master thread able saturate adapters 
similar communication behavior expected platforms inter node network saturated single processor smp node 
reasons node network adapter cpu saturate adapters node internal local mpi copying user space system bu er overlapped real inter node communication 
earth simulator inter node network saturated thread application bu ers located global memory application gb internode ping pong mpi bandwidth reported maximum rate link smp node crossbar switch gb application bu ers allocated global memory additional copying local global memory executed single thread inter node bandwidth reduced global memory inter node ping pong bandwidth 
case parallel usage multiple threads hybrid mpi openmp processes pure mpi saturate inter node network 
shown ratio hybrid pure mpi transfer time may major reason application running faster pure mpi model hybrid model 
comparison comparison focuses bandwidth latency aspects achieve major percentage physical inter node network bandwidth various parallel programming models 
hybrid mpi openmp versus pure mpi benchmark results section show advantages pure mpi model advantages hybrid model 
hybrid model communication overhead inside node 
message size boundary information process may larger total amount communication data reduced 
reduces latency overheads inter node communication 
number mpi processes reduced 
may cause bet hybrid parallel programming benchmark results comparing hybrid mpi openmp pure mpi hitachi sr cpus smp node 
benchmark results comparing hybrid mpi openmp pure mpi ibm sp power cpus smp node 
computing applications ter speedup amdahl law may cause faster convergence parallel implementation multigrid numeric computed partial grid 
reduce mpi overhead communicating thread mpi communication routines relieved unnecessary local concatenation data better done copying data scratch bu er thread parallelized loop derived mpi datatypes 
mpi reduction operations split inter node communication part local reduction part user de ned operations local thread parallelization operations may cause problems threads running mpi routine may communicate 
hybrid programming done di erent ways domain decomposition internode parallelization mpi intra node parallelization openmp cases coarse grained parallelization 
intra node parallelization implemented ne grained parallelization mainly loop parallelization 
second case allows automatic intra node parallelization compiler amdahl law considered independently 
comparing hybrid mpi openmp programming schemes want compare di erent hybrid programming schemes scheme master thread communicates outside parallel regions 
computation parallelized cpus smp node inside parallel regions 
scheme communication master thread done parallel computation threads 
application restructured allow overlap communication computation 
multiple scheme threads may communicate compute parallel 
application threads sleep master thread communicating mpi communication time hybrid tab 
counts eighth node cpus sr active plus idling cpus communicating 
hybrid programming style factor hybrid reduced eighth 
implemented dedicating thread communication threads node computing full load balancing different mixes computation communication threads 
reserved threads communicating compared hybrid programming schemes named vector mode task mode 
show performance ratio multiple multiple execution bounds number threads smp node thread reserved communication 
general threads reserved communication 
wall clock execution time programming scheme 
divided fractions communication time consumed master thread comp wall clock computation time consumed threads parallel 
parts fraction overlapped communication multiple scheme 
computation fraction divided comp non comp overlap sum comp non comp overlap scheme thread reserved communication threads computation 
multiple scheme may value results section chosen larger number cpus needed saturate communication network 
natural lower bound overlap expect overhead multiple scheme expect comp non fraction parallelized threads comp overlap parallelized remaining threads execution time multiple comp non max comp overlap hybrid parallel programming performance ratio epsilon comm percent comp non max min epsilon mark performance ratio epsilon comm percent comp non max min epsilon mark performance ratio epsilon comm percent comp non max min epsilon mark performance ratio plotted equation 
performance ratio comp non max comp overlap best performance ratio achieved cpus busy communication computation 
case terms max equal comp overlap comp non performance ratio best case max comp non best ratio achieved satis es comm best comp non value max upper bound fractions comp non comp overlap comp non comm best multiple scheme better scheme 
upper bound expresses chance performance gain load balancing done way rst thread communicating computing threads computing idle time due bad balancing 
hand risks multiple scheme 
performance loss emerge threads reserved communication needed threads idle 
term reduces performance comp non comp overlap comp non comp non comp non comp non comp non min schemes performance comm equiv comp non 
proof directly 
reality performance win may worse normally separation computational parts overlapped communication computational parts need information neighbor processes causes overhead 
case vector processing additional ort may necessary achieve long vector size multiple programming scheme 
example comp non max comm best 
comm equiv performance models equal 
smaller computing applications ratio decrease min 
fig 
shows performance ratio di erent 
experimental results reserved communication threads basic principles discussed fig 
hold real world applications sparse matrix mvm smp clusters 
selected hybrid parallel implementation sparse mvm described ref 

course scalability mvm strongly depends sparsity pattern consider matrix representation point discretisation di erential operator dimensional cartesian grid periodic boundary conditions 
conversion cartesian coordinates fi kg linear index de ned follows fi kg 


block wise parallelisation proc mpi processes done direction loc proc communication scheme independent problem sizes involves nearest neighbor communication 
easily control communication computation costs function problem size mpi processes scheme comp non non loc comp overlap overlap loc approach dominant contributions total computing time considered representing mpi communication costs overlap mvm non measuring non overlapping part total mvm computation time 
eqs 
performance ratio easily calculated function problem size non loc overlap loc mvm non loc max loc proc proc proc proc comm mvm overlap performance ratio sparse mvm algorithm measurements performed maximum proc hitachi sr nodes 
dotted line plotted eq 
overlap 
matrix dimension sparse matrix mvm step follows mat loc proc overlap loc furthermore sparsity pattern described parallel mvm implementation introduced non overlap holds adjustable parameter comm overlap left eq 

results hitachi sr system testcase xed varied loc di erent number mpi processes total communication cost remained constant local workload process changed 
performance measurements scheme multiple scheme done smp nodes hitachi sr munich smp nodes ibm sp power nersc 
corresponding performance ratios plotted fig 
fig 
function local workload node 
hitachi sr nd consistent eq 
xed loc weak dependence number nodes 
complex interplay communication computation costs described theory experimental results multiple scheme favored low hybrid parallel programming loc proc proc proc sp proc performance ratio sparse mvm algorithm measurements performed maximum proc ibm sp power nodes 
matrix dimension sparse matrix mvm step follows loc proc inset shows speed sp node di erent number threads 
ate local workloads crossover point occurs loc higher local workload xed communication cost small paid separate thread spent communication 
compare measurements fig 
performance ratio predicted eq 
overlap chosen 
choice xes crossover point loc eq 
represents realistic number overlap estimated pro ling run proc loc 
total problem sizes cover orders magnitude large range number nodes proc considered nd qualitative agreement theoretical approach measurements range workloads 
regarding maximum performance gain achieved multiple mode theoretical approach gives approximation position overestimates absolute value 
results ibm sp results hold qualitatively ibm sp system cf 
fig 
features discussed detail position maximum independent number nodes absolute values decrease increasing node counts small values loc effect may depend position nodes network comprises nodes intended examined 
second contrast hitachi nd decreasing 
fact easily understood take account memory bandwidth smp node scale number processors 
seen inset fig 
speed achieved node threads thread 
words spending thread communication increase time overlapping computations multiple model assumed eq 

ective value eq 
approximately zero giving lower bound min 
please note case hitachi memory bandwidth node saturate single processor bandwidth processors 
speed demonstrated processors hitachi node 
course detailed description communication computation going simple approach eqs 
required improve quantitative agreement theoretical model measurements 
mpi versus compiler parallelization comparing pure mpi di erent mpi hybrid parallel programming models compare mpi models numa rdma models 
access data node mpi data copied local memory location called halo shadow message passing loaded cpu 
usually necessary data transferred large message short messages 
transfer speed dominated asymptotic bandwidth network reported cyclic lmax tab 
node mb process mb 
numa rdma data loaded directly remote memory location cpu 
may imply short accesses access latency bound 
numa rdma latency usually times shorter message passing latency total transfer speed may worse 
reports ccnuma system latency implies computing applications access method copies remarks bandwidth message size sided mpi internal mpi bu er lat size application receive bu er mb mb kb mb sided mpi application receive bu er formula probably better lat array page transfer extremely poor fortran parts page needed upc hpc word access byte lat openmp byte mb cluster latency hiding pre fetch extensions latency hiding bu ering see sided communication table memory copies remote memory local cpu register 
bandwidth mb byte data 
ect eliminated compiler implemented remote pre fetching strategy described method compilers 
hess shown reach openmp top dsm system speed ups ranging mpi speed communication fraction high 
remote memory access optimized bu ering pipelining data transferred 
approach may hard automate current openmp compiler research studies bandwidth optimization smp clusters easily implemented directive optimization technique application thread de ne remote data simulation step compiled openmp code pre fetch remote part data bandwidth optimized transfer method 
table summarizes comparison 
parallelization compilation major advantages openmp programming application incrementally parallelized single source serial parallel compilation 
cluster smp nodes openmp major disadvantages openmp memory model knows private shared variables strategy achieve contiguous data transfer access shared array variables remote memory 
especially second problem dicult reach asymptotic network bandwidth 
mentioned problems solved tiny additional directives proposed migration memory pinning directives additional directives allow contiguous transfer boundary information simulation step 
directives optimization features modify basic openmp model done directives de ne full hpf user directed data distribution 
lack current openmp standard absence strategy combining automatic parallelization openmp parallelization implemented non standardized way nearly openmp compilers 
problem solved adding directives de ne scopes compiler allowed automatically parallelize code similar parallel region de ne auto parallel region 
usual rules nested parallelism apply compiler de ne handle nested parallelism 
openmp parallel programming model smp clusters usable ne grained loop parallelization coarse grained domain decomposition 
clear path mpi openmp cluster programming model performance worse pure mpi hybrid mpi openmp 
important compilation strategy allows development optimizing compilers combination processor memory access network hardware 
mpi approaches especially hybrid mpi openmp approach clearly hybrid parallel programming separate remote local memory access optimization 
remote access optimized mpi library local memory access improved compiler 
separation realized project openmp compiler 
separation local remote access optimization may essential chance achieving zero latency remote pre fetching tab 
direct compiler generated instructions remote data access 
pre fetching done macros library calls input local openmp compiler 
parallel applications hybrid systems important achieve high communication bandwidth processes node node interconnect 
architectures standard programming models smp mpp systems longer 
identi es major problems 
thread node perform inter node communication calling mpi routines outside parallel regions achieve full inter node bandwidth 
benchmark cartesian topology hybrid mpi openmp model needs communication time communicating pure mpi model additional intra node message transfers necessary pure mpi 
problem may reasons thread di erent hardware links time mpi library able saturate network due additional internal memory copying limited memory bandwidth thread 
communication percentage area ect hybrid model causes additional overhead 
openmp synchronization overhead larger execution time hybrid programs surprise 
second compare model hybrid model overlap communication computation 
implementing full load balancing decided funnel communication master thread reserve thread communication threads computation 
cluster way smp nodes exhibit theoretically total application run times faster model 
factor depends fraction communication fraction computation overlapped communication 
experiment demonstrate factor 
unfortunately fast hybrid overlapping funnel reserve model allow sophisticated sharing constructs openmp 
openmp reduced interface basic thread usage 
sharing done application program ranks threads 
third look compiler parallelization hybrid platforms 
problems arise achieve communication speed area hardware inter node bandwidth cluster capable compiler deliver computational optimizations smp local compiler 
problems unsolved 
programs relevant communication fraction mpi parallelization pure combined openmp method rst choice 
include examination mixing mpi openmp inside smp nodes bene saturation inter node network hybrid programming 
acknowledgments authors acknowledge colleagues people supported projects suggestions helpful discussions 
especially alice david eder matthias productive discussions limits hybrid programming bob discussions mlp schulz benchmarks thomas matthias uller uwe john levesque discussions openmp cluster extensions vectorization 
research resources stuttgart munich resources national energy research scienti computing center supported oce science department energy 
part supported project 
biographies rolf studied mathematics physics university stuttgart 
working high performance computing center stuttgart 
led projects dfn rpc remote pro computing applications cedure call tool mpi glue rst metacomputing mpi combining di erent vendor loosing full mpi interface 
dissertation developed controlled logical clock global time trace pro ling parallel distributed applications 
member mpi forum 
january april invited center high performance computing dresden university technology 
currently head department parallel computing 
research interests include hybrid parallel programming models mpi pro ling benchmarking parallel communication involved teaching projects 
gerhard studied physics university germany 
working eld exact diagonalization strongly correlated electron lattice systems got doctorate degree theoretical physics university 
started hpc consultant scienti computing center erlangen 
member competence network technical scienti high performance computing 
currently head hpc group 
research interests include optimization strategies new processor architectures exact diagonalization density matrix renormalization group methods strongly correlated quantum systems parallel computers hybrid parallel programming techniques large scale application programs 
involved research projects condensed matter physics 
advanced simulation computing asc accelerated strategic computing initiative asci 
www doe gov asc home htm eduard marc gonzalez jesus xavier navarro jose oliver research platform openmp extensions proceedings st european workshop openmp lund sweden sep 
www lth se papers pdf siegfried thomas brandes high level data mapping clusters smps proceedings th international workshop high level parallel programming models supportive environments hips san francisco usa april springer lncs pp 
nagel svm fortran technical report ib germany 
www fz de zam docs printable ib ib ib ps frank daniel mpi versus mpi openmp ibm sp nas benchmarks proc 
supercomputing dallas tx 
citeseer nj nec com mpi html www sc org papers pap pap pdf william carlson jesse draper david culler kathy yelick eugene brooks karen warren upc language speci cation ccs tr may www super org upc www edu projects seas edu pdf 
robert james taft jens early experiences processor single system image origin proceedings nd international cray user group conference summit netherlands may www org 
earth simulator 
www es go jp el chauvin upc benchmarking issues proceedings international conference parallel processing pp 
projects seas edu upc bench pdf gropp lusk skjellum high performance portable implementation mpi message passing interface standard parallel computing sep pp 
hager risc optimization techniques hitachi sr architecture accepted publication high performance computing science engineering munich springerverlag berlin heidelberg 
jonathan harris extending openmp numa architectures proceedings second european workshop openmp 
www ed ac uk proceedings html hybrid parallel programming performance hybrid messagepassing shared memory parallelism discrete element modeling proc 
supercomputing dallas tx 
citeseer nj nec com performance html www sc org papers pap pap pdf matthias hess gabriele matthias uller roland experiences openmp compiler directed software dsm pc cluster workshop openmp applications tools arctic region supercomputing center university alaska aug 
alice rolf karl benchmark design characterization balanced high performance architectures proceedings th international parallel distributed processing symposium ipdps workshop massively parallel processing april san francisco usa 
richard stephen thomas john dennis spectral element dynamical core atmospheric general circulation models proceedings sc nov denver usa 
www sc org papers pap pap pdf john merlin distributed openmp extensions openmp smp clusters proceedings second european workshop openmp 
www ed ac uk proceedings html message passing interface forum 
mpi message passing interface standard rel 
june www mpi forum org 
message passing interface forum 
mpi extensions message passing interface july www mpi forum org 
hans erich jack dongarra horst simon universities mannheim tennessee top supercomputer sites www top org 
matthias uller compiler generated prefetching architectures distributed memory high performance computing science engineering transactions high performance computing center stuttgart ager krause eds springer pp 
project jesus research ac upc es hpc 
reid array fortran parallel programming acm fortran forum volume pp www array org ftp cc rl ac uk pub reports ps gz 
openmp group www openmp org 
rolf alice ective communication file bandwidth benchmarks advances parallel virtual machine message passing interface proceedings th european pvm mpi users group meeting santorini greece lncs dongarra eds springer pp www de mpi eff www de mpi eff io 
sato satoh tanaka design openmp compiler smp cluster proceedings st european workshop openmp lund sweden sep pp 
citeseer nj nec com sato design html sato earth simulator sc baltimore nov 
www sc org alex scherer lu thomas gross willy zwaenepoel transparent adaptive parallelism openmp proceedings seventh conference principles practice parallel programming ppopp may pp 
shi hu tang shared virtual memory survey technical report center high performance computing institute computing technology chinese academy sciences www ict ac cn dsm tr ps 
satoru hiroshi yamada tsuda yuji sasaki kazuo kobayashi takashi shin ichi itoh ops global atmospheric simulation computing applications spectral transform method earth simulator sc baltimore nov 
www sc org pap pap pdf smith mark bull development mixed mode mpi openmp applications proceedings workshop openmp applications tools san diego july 
www cs uh edu hager fast sparse matrix vector multiplication computers proceedings th int conference high performance computing computational science porto portugal june part pp 
fe pt hitoshi tamura mpi benchmark program library application earth simulator proceedings th international symposium high performance computing zima 
eds science city japan may lncs springer pp 
hybrid parallel programming part published 
addendum proof comp non comp non comp non comp non comp non comp non nm comp non comp non comp non comp non addendum proof comp non comp non nm comp non comp non addendum calculation min detail comp non comp overlap comp non comp non computing applications comp non comp non comp non comp non comp non comp non min addendum proof theorem schemes performance comp non 
comp non comp overlap comp non comp non comp non comp non comp non comp non 
