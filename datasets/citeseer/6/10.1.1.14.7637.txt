svm incremental learning adaptation optimization christopher diehl applied physics laboratory johns hopkins university laurel md chris diehl edu gert ece department johns hopkins university baltimore md gert jhu edu objective machine learning identify model yields generalization performance 
involves repeatedly selecting hypothesis class searching hypothesis class minimizing objective function model parameter space evaluating generalization performance resulting model 
search computationally intensive training data continuously arrives needs tune hyperparameters hypothesis class objective function 
framework exact incremental learning adaptation support vector machine svm classifiers 
approach general allows learn individual multiple examples adapt current svm changes regularization kernel parameters evaluate generalization performance exact leave error estimation 
svm techniques classification regression provide powerful tools learning models generalize sparse high dimensional settings 
success attributed vapnik seminal statistical learning theory provided key insights factors affecting generalization performance 
svm learning viewed practical implementation vapnik structural risk minimization induction principle involves searching hypothesis classes varying capacity find model best generalization performance 
svm classifiers form learned data 
minimizing min subject constraints 

focus case generally preferred due improved robustness outliers offered hinge loss quadratic loss 
simplify matters learning nonlinear svms quadratic program typically expressed dual form min ij lagrange multiplier offset ij 
resulting dual form svm nonlinearly transformed training test examples appear dot product terms employ positive definite kernel function implicitly map higher possibly infinite dimensional feature space compute dot product 
search svm generalizes involves repeatedly selecting kernel function regularization parameter solving quadratic program evaluating generalization performance 
typically parameterized kernel function utilized varies smoothly respect continuous parameters 
search task finding set parameters maximize generalization performance 
incrementally vary regularization kernel parameters expect resulting svm change dramatically 
minimize computational burden search require process utilizes current svm solution simplify solution quadratic program search 
new data available integrate examples quadratic program modify necessary 
critical capability online active incremental batch learning scenarios 
incremental techniques developed facilitate batch svm learning large data sets widespread svm community 
incremental svm learning particularly attractive line setting active learning 
techniques approximate require passes data reach convergence 
procedure exact adiabatic incremental learning svm classifiers single pass data introduced extended svm regression larger set increments :10.1.1.21.1720
investigations incremental learning largely focused line learning opposed larger model selection problem 
dynamic kernel adaptation approximate procedures model selection leave oneout loo approximations bounds 
incremental svm learning procedure reverted perform decremental exact loo model selection 
extend incremental svm learning paradigm general framework incremental learning adaptation optimization allows learn individual multiple examples adapt current svm changes regularization kernel parameters evaluate generalization performance exact leave oneout error estimation 
section addressing problem learning svm solution training examples svm solution training examples new batch training examples 
illustrate basic approach problem utilized achieve remaining objectives 
final section examine initial experimental results demonstrating potential approach 
ii 
incremental decremental learning karush kuhn tucker conditions karush kuhn tucker kkt conditions uniquely define solution dual parameters minimizing form ij partial derivatives training examples partitioned different categories set margin support vectors margin set error support vectors violating margin remaining set reserve vectors exceeding margin 
incremental learning new training examples assigned directly construction enter solution 
new examples initially elements specially designated set unlearned vectors eventually margin error support vectors 
adiabatic increments increment unlearned example solution goal simultaneously preserve kkt conditions previously seen training data 
kkt conditions maintained varying margin vector coefficients response perturbation imparted incremented new coefficient 
process elements different categories may change state incremental learning proceeds sequence adiabatic steps amplitude determined bookkeeping category membership conditions 
prior perturbation svm solution partial derivatives respect equal ij 
perturbation partial derivatives ij ik il 
expressing conditions differentially obtain ik il 
perturbation unlearned vector coefficients objective determine necessary changes margin vector coefficients bias preserve kkt conditions learned data 
perturbation process controlled perturbation parameter varies svm solution perturbed initial unlearned final learned result 
solution initializes previous solution prior presentation new examples 
perturbation perturbation parameter incremented smallest value pmin leads category change example 
unlearned vectors reached categories new old data satisfy kkt conditions 
adiabatic increments expressed product corresponding coefficient sensitivities 
substituting expressions dividing yields differential kkt conditions expressed terms coefficient sensitivities ik il 
principle perturbation coefficients freely chosen 
unlearned vector coefficients change unlearned examples change categories natural choice 
corresponding coefficient sensitivities obtained solving system equations 
coefficient sensitivities known compute margin sensitivities error reserve unlearned vectors 
necessary computing pmin table lists possible category changes occur incremental decremental learning 
smallest applicable conditions determines category change perturbation step pmin determine min compute minimum min set examples undergo category change table summary bookkeeping conditions common initial category new category condition margin reserve error margin reserve margin incremental decremental learning margin error unlearned margin unlearned error regularization parameter perturbation margin error kernel parameter perturbation margin error unlearned margin unlearned reserve unlearned error pmin min min set possible category changes 
pmin known update coefficients margin vectors unlearned vectors 
noting category change recompute coefficient margin sensitivities determine perturbation 
process repeats 
initial perturbation special consideration case svm solution initially exists 
scenario training examples initially unlearned 
implies equality condition immediately satisfied 
incrementing unlearned vector coefficients immediately violate condition 
equal number examples class equality condition generally preserved 
margin vector coefficients allow preservation equality condition initial svm solution 
margin vectors provide degree freedom counterbalance changes unlearned vector coefficients 
way preserve condition bootstrap process selecting example class learning initial svm 
accomplished selecting example modifying bias example margin note possible unlearned vector set empty incremental perturbation terminate 
vector 
second example opposite class successfully incremented solution 
option simply proceed initial perturbation disregard condition margin vector set longer empty 
point incrementally correct violation manner guarantees condition satisfied perturbation process complete 
explore approach detail addressing problem adapting svm solution changes kernel parameters 
solving coefficient sensitivities obtain coefficient sensitivities solve system equations represented 
expressed matrix vector form 
sn 
sn 
sn 
sn 
sn sns 

require inverse compute sensitivities rv 
perturbation process proceeds easily adapt examples added removed margin vector set 
adding example expands 

sn sn sn sn sn rv sn sn sn sn sn sn sn removing margin vector contracts ij ij ik kj kk index refers term 
potential degenerate solution arise regularization parameter set low examples error vectors 
occur problem easily corrected increasing decremental learning cross validation adiabatic increment process fully reversible 
decremental provides flexibility perform leave error estimation generally fold crossvalidation accomplished simply relevant example computing corresponding class label resulting svm 
subset training examples objective decrement coefficients retaining kkt conditions data 
perturbation process examples currently margin vectors contract inverse corresponding kkt conditions unlearned vectors longer enforced 
complete compute coefficient sensitivities margin sensitivities equations 
decremental scenario coefficient sensitivities examples equal 
note determining pmin perturbation need track category changes remaining margin error reserve vectors 
iii 
parameter optimization key condition enables incremental learning hinge loss fact partial derivatives linear functions parameters 
allows perturb svm solution manner introduced previous section 
general svm solution perturbed respect parameters linearly related partial derivatives implies example svm perturbed respect regularization parameter 
suggests general svm perturbed respect nonlinear parameters kernel instance radial basis gaussian kernel 
limit utility algorithm general parameter optimization propose slight modification strategy allows address nonlinear case 
regularization parameter perturbation perturbing svm respect regularization parameter amounts simply incrementing decrementing error vector coefficients 
replacing unlearned vector set error vector set equations obtain differential kkt conditions ik il 
implies coefficient sensitivities equal rv 
table ii category reassignment kernel parameter change previous example category reserve vector reserve unlearned margin vector unlearned unlearned error vector unlearned error error vector coefficients change new natural choice perturbation coefficients 
subtle difference perturbation process arises computing required margin vector error vector 
incremental learning fixed varies perturbations 
margin vector error vector positive hold sk sk 
implies sk sk indicated table kernel parameter perturbation perturb svm solution vary kernel parameters requires slightly different strategy 
incremental learning regularization parameter perturbation general define method perturbing svm solution transforming kernel 
approach involve modifying kernel parameters incrementally correcting previous solution kkt conditions satisfied new kernel parameter settings 
kernel parameters changed recomputing partial derivatives margin error reserve vectors 
category example necessary 
table ii presents mapping old new category function reserve error vectors partial derivatives change sign category change occurs 
examples need modified examples unlearned 
perturbation margin vectors unlearned vector coefficients change 
margin vector modify margin vector coefficients bias manner preserves kkt conditions current margin vector set perturbation coefficients unlearned vectors defined 
examples decremented example margin vector reserve vector 
examples incremented example margin vector error vector 
perturbation process begins kkt condition holds 
order preserve condition constraint satisfied 
general constraint satisfied rescaling perturbation coefficients way signs coefficients preserved 
perturbation intentionally violate kkt condition correct violation incrementally margin vector set longer empty 
shall see easily resolved 
prior perturbation svm solution partial derivatives respect equal ij perturbation partial derivatives equal ij ik il solve coefficient sensitivities define constraint sensitivity want perturbation process complete differencing equations dividing leads ik il 
expressing system equations matrix vector form obtain 
implies coefficient sensitivities equal re rv 
comparing equation see additional term introduced force 
equations coefficient margin sensitivities hand perturbation process proceeds 
table lists possible category changes tracked perturbation process 
note constraint sensitivity equation updated prior perturbation ensure final perturbation complete 
iv 
experimental results order assess benefits offered incremental framework conducted experiments pima indians dataset uci machine learning repository 
rbf kernel exp fixed kernel width varied increased decreased regularization parameter range indicated table iii 
fixed regularization parameter varied decreased increased kernel width range indicated table iv 
tables list number floating point operations kernel evaluations perturbations required full retraining incremental approach 
note center row table list statistics incremental approach corresponding fully re trained svm serves initial svm series perturbations 
full retraining corresponds incremental learning session entire training data starting empty set 
table iii find regularization parameter increased computational savings offered incremental approach increases 
components cost terms floating point operations incremental approach worth noting 
increases number error vectors decreases 
leads steady decline number kernel evaluations required 
time number margin vectors increasing adds expense repeated computation margin sensitivities leads increases cost 
examining table iv see kernel width decreased computational savings offered incremental approach decreases 
due increasing number margin vectors result 
regularization parameter perturbation cached rows kernel matrix provide benefit modifying kernel width 
recompute needed rows kernel matrix case full retraining 
relative fraction unlearned vectors result margin vectors increases computational cost incremental approach comparable full retraining 
framework incremental learning adaptation support vector machine number floating point operations determined flops command matlab 
table iii comparison costs full retraining incremental approach fixed varying pima indians floating point operations kernel evaluations perturbations full retraining incremental full retraining incremental full retraining incremental table iv comparison costs full retraining incremental approach fixed varying pima indians initial floating point operations kernel evaluations perturbations full retraining incremental full retraining incremental full retraining incremental classifiers aims simplify model selection task perturbing svm solution regularization kernel parameters adjusted 
empirical results uci benchmark data suggest regularization parameter perturbation offer significant computational savings computational benefits kernel parameter perturbation may limited 
general benefits incremental adaptation substantial small perturbation kernel regularization parameters may expect optimization step model selection 
acknowledgments diehl supported fy jhu apl ir program applied mathematics 
supported iis national science foundation office naval research 
blake merz 
uci repository machine learning databases 
university california irvine dept information computer sciences www ics uci edu mlearn mlrepository html 
chris burges david crisp 
uniqueness svm solution 
solla leen 
uller editors advances neural information processing systems 
morgan kaufmann 
colin campbell nello cristianini alex smola 
query learning large margin classifiers 
proceedings th international conference machine learning pages 
morgan kaufmann san francisco ca 
gert tomaso poggio 
incremental decremental support vector machine learning 
advances neural information processing systems 
mit press 
chapelle vapnik bousquet mukherjee 
choosing multiple parameters support vector machines 
machine learning 
kluwer academic 
cristianini campbell shawe taylor 
dynamically adapting kernels support vector machines 
neurocolt technical report nc tr 
royal holloway college university london uk 
shai fine 
incremental learning selective sampling parametric optimization framework svm 
advances neural information processing systems 
mit press 

cristianini campbell 
kernel algorithm fast simple learning procedure support vector machines 
proceedings th international conference machine learning 
morgan kaufmann 
thorsten joachims 
making large scale svm learning practical 
sch olkopf burges smola editors advances kernel methods support vector learning 
mit press 

lee 
lin 
automatic model selection support vector machines 
www csie ntu edu tw cjlin papers ps gz 
martin 
line support vector machines function approximation 
www lsi upc es dept html html 
osuna freund girosi 
improved training algorithm support vector machines 
proceedings ieee workshop neural networks signal processing pages 
platt 
fast training support vector machines sequential minimum optimization 
advances kernel methods support vector learning pages 
cambridge ma mit press 
pontil verri 
properties support vector machines 
neural computation volume pages 
mit press 
vladimir vapnik 
statistical learning theory 
springer verlag new york 
