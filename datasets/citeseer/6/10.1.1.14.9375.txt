classifying large data sets svm hierarchical clusters yu department computer science university illinois urbana champaign il usa uiuc edu support vector machine svm promising method classification regression analysis solid mathematical foundation conveys salient properties methods provide 
despite prominent properties svm favored large scale data mining pattern recognition machine learning training complexity svm highly dependent size data set 
real world data mining applications involve millions billions data records multiple scans entire data expensive perform 
presents new method clustering svm cb svm specifically designed handling large data sets 
cb svm applies hierarchical micro clustering algorithm scans entire data set provide svm high quality samples carry statistical summaries data summaries maximize benefit learning svm 
cb svm tries generate best svm boundary large data sets limited amount resources 
experiments synthetic real data sets show cb svm highly scalable large data sets generating high classification accuracy 
categories subject descriptors pattern recognition design methodology classifier design evaluation keywords support vector machines hierarchical cluster 
supported part national science foundation nsf iis univ illinois ibm faculty award 
permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigkdd washington dc usa copyright acm 
yang department computer science university illinois urbana champaign il usa cs uiuc edu jiawei han department computer science university illinois urbana champaign il usa cs uiuc edu support vector machine svm promising method data classification regression :10.1.1.117.3731
success practice drawn solid mathematical foundation conveys salient properties margin maximization classification boundary function svm maximizes margin machine learning theory corresponds maximizing generalization performance set training data 
see section details 
nonlinear transformation feature space kernel trick svm handles nonlinear classification efficiently kernel trick implicitly transforms input space high dimensional feature space 
success svm machine learning naturally leads possible extension classification regression problems mining huge amount data 
despite prominent properties svm favored data mining pattern recognition machine learning training complexity svm highly dependent size data set real world data mining applications involve millions billions data records 
example shows unscalable standard svm large data set 
example 
forest cover type data set uci kdd archive composed data instances attributes quantitative binary attributes 
shows training time svm different numbers training data randomly sampled original data set 
graphs infer take years svm train data 
libsvm version run svm rbf kernel gave fairly results 
ran pentium iii mhz mb memory 
researchers proposed various revisions svm increase training efficiency mutating approximating 
feasible large data sets multiple scans entire data set expensive perform losing benefits svm simplifications 
see section discussions related 
svm known quadratic number data points 
refer discussions complexity svm 
kdd ics uci edu databases html www csie ntu edu tw cjlin libsvm non scalability svm 
axis data points training time hours presents new approach scalable reliable svm classification 
method called clustering svm cb svm specifically designed handling large data sets 
size data set large svm tends perform worse training entire data training fine quality samples data set 
selective sampling active learning techniques svm try sample training data intelligently maximize performance svm normally require scans entire data set section :10.1.1.16.4036
cb svm similar idea applies hierarchical micro clustering algorithm scans entire data set provide svm high quality samples carry statistical summaries data summaries maximize benefit learning svm 
cb svm scalable terms training efficiency maximizing performance svm 
key idea cb svm hierarchical micro clustering technique get finer description closer boundary coarser description farther boundary efficiently processed follows cb svm constructs micro cluster trees positive negative training data respectively 
tree node higher level summarized representation children nodes 
constructing trees cb svm start training svm root nodes 
generates rough boundary root nodes selectively decluster data summary near boundary lower finer levels tree structure 
hierarchical representation data summaries perfect base structure cb svm perform selective declustering effectively 
cb svm repeats selective declustering leaf level 
cb svm linear classification regression analysis large data sets including streaming data data large data warehouses especially random sampling hurts performance infrequently occurring important data irregular patterns incoming data causes different probability distributions training testing data 
discuss section 
experiments network intrusion data set section example shows random sampling hurt show cb svm scalable large data sets generating high classification accuracy 
best knowledge proposed method currently svm large data sets tries generate best results limited amount resources 
remainder organized follows 
provide overview svm section 
section introduce hierarchical micro clustering algorithm large data sets originally exploited zhang :10.1.1.152.7115
section cb svm algorithm applies hierarchical algorithm standard svm svm scalable large data sets 
section demonstrates experimental results artificial real data sets 
discuss related section conclude study section 
svm overview machine learning theory optimal class boundary function hypothesis limited number training data set label considered gives best generalization performance denotes performance unseen examples training data 
performance training data regarded evaluation measure hypothesis hypothesis ends overfitting tries fit training data hard 
problem easy classify boundary function complicated needs boundary overfit 
problem hard classifier powerful boundary 
svm excellent example supervised learning tries maximize generalization maximizing margin supports nonlinear separation advanced kernels svm tries avoid overfitting underfitting 
margin svm denotes distance boundary closest data feature space 
svm problem computing margin maximized boundary function specified quadratic programming qp problem number training data denoted vector variables component corresponds training data 
soft margin parameter controlling influence outliers noise training data 
kernel linear boundary function scalar product data points 
nonlinear transformation feature space performed replacing advanced kernel polynomial kernel rbf kernel advanced kernel attractive computational short cut expensive creation complicated feature space 
advanced kernel function operates input data effect computing scalar product images usually higher dimensional feature space infinite dimensional space allows implicitly hyperplanes highly complex spaces 
characteristic svm boundary function described support vectors svs data closest boundary 
qp problem computes vector element specifies weight data svs data corresponding greater zero 
words data svs contribute boundary function computing svm boundary function viewed finding svs corresponding weights describe class boundary 
attempts revise original qp formulation solved qp solver efficiently 
see section details 
revise original qp formulation svm 
try provide smaller high quality data set beneficial computing svm boundary function effectively applying hierarchical clustering algorithm 
cb svm algorithm substantially reduces total number data points training svm trying keep high quality svs describes boundary best 

hierarchical micro clustering algorithm large data sets hierarchical micro clustering algorithm apply cb svm section originally exploited zhang named birch :10.1.1.152.7115:10.1.1.152.7115
concept micro cluster similar denotes statistically summarized representation group data close belong cluster 
hierarchical micro clustering algorithm characteristics 
constructs micro cluster tree called cf clustering feature tree scan data set limited amount resources available memory time constraints incrementally dynamically clustering incoming multidimensional data points 
single scan data allow backtracking localized inaccuracies may exist depending order data input 
cf tree captures major distribution patterns data provides information cb svm perform 
handles noise outliers data points part underlying distribution effectively product clustering 
improved hierarchical clustering algorithms developed including cure chameleon 
chameleon shown powerful discovering arbitrarily shaped clusters high quality complexity worst case number data points 
cure produces highquality clusters complex shapes complexity linear number objects parameter setting general significant influence results 
cf tree birch carries spherical shapes hierarchical clusters captures statistical summaries entire data set 
provides efficient effective structure cb svm run 
clustering feature cf tree start defining basic concepts 
dimensional data points cluster centroid radius cluster defined average distance member points centroid 
concepts clustering feature cf tree core hierarchical micro clustering algorithm clustering incremental expensive computations 
cf triple summarizes information cf tree maintains cluster 
definition clustering feature 
zhang dimensional data points cluster cf vector cluster defined triple number data points cluster linear sum data points ss square sum data points theorem cf additivity theorem :10.1.1.152.7115
zhang assume cf vectors disjoint clusters :10.1.1.152.7115
cf vector cluster formed merging disjoint clusters refer proof :10.1.1.152.7115
cf definition additivity theorem know cf vectors clusters stored calculated incrementally accurately clusters merged 
centroid ra cluster computed cf cluster 
cf summary cluster set data points 
managing cf summary efficient saves spaces significantly sufficient calculating information building hierarchical micro clusters facilitate computing svm boundary large data set 
cf tree cf tree height balanced tree parameters branching factor threshold cf tree height showing right side 
nonleaf node consists entries form pointer th child node cf subcluster represented child 
leaf entry entry leaf node child pointer 
leaf nonleaf node represents cluster subclusters represented entries 
threshold constraint leaf entries satisfy radius entry leaf node tree size function larger smaller tree branching factor determined page size leaf nonleaf node fit page 
cf tree compact representation data set entry leaf node single data point subcluster absorbs data points radius threshold specific 
algorithm description cf tree built dynamically new data objects inserted 
ways inserts data correct subcluster merges leaf nodes manages nonleaf nodes similar tree sketched follows 
identifying appropriate leaf starting root descends cf tree choosing child node centroid closest 

modifying leaf leaf entry absorb new data object violating threshold condition updates just cf vector entry 
add new entry 
adding new entry causes node split split choosing farthest pair entries seeds redistributing remaining entries closest criteria 

modifying path leaf updates cf vectors nonleaf entry path leaf 
node split leaf causes insertion new nonleaf entry parent node parent node split new entry inserted higher level node 
likewise occurs recursively root 
due limited number entries node highly skewed input cause subclusters cluster split different nodes vice versa 
infrequent undesirable anomalies handled original birch algorithm refinement additional data scans 
perform refinement infrequent localized inaccuracy impact performance cb svm 
determination threshold choice threshold crucial building tree right size fits available memory small run memory data scanned 
original birch algorithm initially sets low iteratively increases tree fits memory 
zhang proved rebuilding tree larger requires re scan data inserted tree far extra pages memory height tree :10.1.1.152.7115
heuristics updating provided :10.1.1.152.7115
due space limitations keep focus skip details 
experiments set initial threshold intuitively number data points dimensions value range dimension proportional tree fits memory 
outlier handling construction cf tree leaf entries contains far fewer data points average considered outliers 
low setting outlier threshold increase classification performance cb svm especially number data relatively large compared number dimensions type boundary functions simple related having low vc dimension machine learning theory non trivial amount noise training data may separable simple boundary function prevents svm boundary converging quadratic programming 
reason enabled outlier handling low threshold experiments section type data targetting large number data points relatively low dimensions type boundary functions linear dimension vc number dimensions 
see section details 
analysis cf tree fits memory nodes maximum size memory size node 
height tree independent size data set 
assume memory unbounded number leaf entries equal number data points due small threshold insertion node tree requires examination entries cost entry proportional dimension 
cost inserting data points case rebuilding tree due poor estimation additional re insertions data inserted added cost 
cost number 
consider dependence size data set computation complexity algorithm experiments original birch algorithm shown linear scalability algorithm respect number objects quality clustering data 

clustering svm cb svm section cb svm algorithm trains large data set hierarchical micro clusters cf tree construct accurate svm boundary function 
key idea cb svm viewed similar selective sampling active learning selecting data maximizes benefit learning 
selective sampling svm selects accumulates low margin data round close boundary feature space low margin data higher chances svs boundary round :10.1.1.16.4036
idea decluster entries near boundary get finer samples nearer boundary coarser samples farther boundary 
way induce svs description boundary fine possible keeping total number training data points small possible 
selective sampling needs scan entire data set round select closest data point cb svm runs cf tree constructed single scan entire data set carrying statistical summaries facilitates constructing svm boundary efficiently effectively 
sketch cb svm algorithm follows 
construct cf trees positive negative data set independently 

train svm boundary function centroids root entries entries root node cf trees 
root node contains entries train entries nodes second levels trees 

decluster entries near boundary level children entries declustered parent entries accumulated training set non declustered parent entries 

construct svm centroids entries training set repeat step accumulated 
cf tree suitable base structure cb svm perform selective declustering efficiently 
clustered data provides better summaries svm random samples entire data set random sampling susceptible biased skewed input may generate undesirable outputs especially probability distributions training testing data similar common practice 
network intrusion data set uci kdd repository experiment section example having substantially different distributions training testing data set due fact real world patterns network intrusions irregular 
cb svm description loss generality consider linearly separable cases convenience explanation 
positive tree negative tree cf trees built positive data set negative data set respectively 
train svm boundary function centroids root entries 
note entry cluster contains cf information efficiently compute center point radius cluster 
shows example svm boundary root clusters corresponding positive tree 
boundary function root entries determine low margin clusters close boundary needs declustered finer level 
support clusters example svm boundary trained root entries positive negative trees 
clusters center points svs boundary circles bold lines 
distance boundary centroid support cluster distance boundary centroid cluster consider cluster satisfies constraint low margin cluster 
radius cluster declustering low margin clusters 
clusters satisfy constraint chances subclusters support clusters boundary illustrated clusters initially satisfied constraint support clusters declustered finer levels results right picture 
subclusters parent clusters satisfy constraint support clusters boundary surfaces parent clusters farther svs boundary 

declustering hard constraint 
radius cluster distance boundary centroid cluster respectively 
separable set positive negative clusters svm boundary set subclusters possibilities support clusters boundary distance boundary centroid support cluster 
example illustrated separable case hard constraints svm 
practice soft constraints necessary cope noise training set 
soft constraints generates svs having different distances boundary 
declustering condition soft constraints svm replace maximum distance include clusters subclusters possibilities support clusters soft boundary subclusters parent clusters satisfy constraint support clusters soft boundary surfaces parent clusters farther distant sv boundary 
declustering soft constraint 
soft constraints svm subclusters possibilities support clusters boundary maximum distance boundary centroids support clusters 
figures describe cb svm algorithm soft constraints declustering 
cb svm analysis building cf tree number data points costs number dimensions number entries node height tree number 
cf tree built training time cb svm dependent number entries number data points 
assume training time algorithm 
note known quadratic linear number dimensions 
refer discussion complexity svm 
number leaf entries leaf entries support entries svs training data entries nodes 
assume average rate number support entries training entries 
number support entries training entries normally standard svms large data sets 
number training entries average theorem training complexity cb svm 
number leaf entries cf tree equal number set input positive data set negative data output boundary function notation clustering algorithm builds hierarchical cluster tree data set return root entries tree return children entries entry set return low margin entries set close boundary see algorithm 


loop 
svm train 



exit 

return cb svm training data points cb svm trains asymptotically times faster standard svms cf tree average rate svs height tree proof 
approximate number iterations cb svm height cf tree training complexity cb svm cf tree training complexity th itera tion cb svm 
number training data points th iteration number data points node number svs data 
assume approximation accumulate training time iterations input boundary function entry set output set low margin entries algorithm 
return maximum distance support vectors boundary 
return data margin smaller 
return replace trains asymptotically times faster theorem states cb svm trains times faster standard svm cf tree 
training time cb svm asymptotically equal standard svm training data points svs 
rate svs variant depending type problems type kernels number dimensions number data points svm parameters 
especially large data sets 
performance difference cb svm standard svm goes higher data set larger 

experimental evaluation section provide empirical evidence analysis cb svm synthetic real data sets discuss results 
experiments done pentium iii mhz machine mb memory 
synthetic data set data generator verify performance cb svm realistic environments providing visualized results perform binary classifications dimensional data sets generated follows 

randomly created clusters cluster center point randomly chosen range dimension independently radius randomly set range number points cluster randomly set range 
labeled clusters axis value cluster cluster labeled axis value center threshold value 
removed clusters assigned positive negative lie threshold axis 
way drive clusters linearly separable 
synthetic data set dimensional space 
positive data negative data 
characteristics cluster determined data points cluster generated independent normal distribution mean center standard deviation radius 
class label data inherited label parent cluster 
note due properties normal distribution maximum distance point cluster center unbounded 
words point may arbitrarily far belonging cluster 
refer points belongs cluster located farther surface outsiders 
due outsiders data set completely linearly separable realistic 
svm parameter setting libsvm version svm implementation svm linear kernel 
enabled shrinking heuristics fast training 
svm advantage standard svms parameter semantic meaning denotes upper bound noise rate lower bound sv rate training data 
experiments set low usually performs size data set large noise relatively small 
results discussion large data set shows example data set generated parameters table 
data generated clusters left side right side positive negative respectively 
shows randomly sampled data original data set 
random sampling hurt svm performance ways know random sampling reflects unstable data distribution original data set includes non trivial amount unnecessary data points training svm 
dashed ellipses indicating densely sampled areas reflects original data distribution close boundary 
practice areas boundary tends dense cluster centers dense www csie ntu edu tw cjlin libsvm parameter values number clusters range range range table data generation parameters cross boundary multiple classes 
unnecessary data increases training time svm contributing svs boundary 
random sampling hurts probability distributions training testing data different random sampling reflects distribution training data significant regions testing data 
instance network intrusion detection data set kdd cup testing data probability distribution training data 
collected different times periods task realistic 
see section details 
shows training data points iteration cb svm 
set set outlier threshold standard deviation 
generated cf tree cb svm iterated times 
note training data points cb svm actual data summary clusters tend narrowly focused data points random sampling 
areas far boundary contribute sv sparse data points clusters representing areas declustered process cb svm 
show intermediate data points cb svm generated second iterations respectively 
data points centroids root entries sparse 
shows dense points boundary declustered second level cf tree 
kdd ics uci edu databases kddcup kddcup html data distribution iteration data distribution second iteration intermediate results cb svm 
positive data negative data original cb svm samples number data points svm training time sec 
sampling time sec 
false predictions fp fn table performance results synthetic data set training data testing data 
fp false positive fn false negative sampling time cb svm time cf tree shows better data distribution svm declustering support entries leaf level 
fair evaluation generated testing set clusters different probability distributions randomly re assigning number points cluster 
report number false predictions false negative false positive testing data set data size big compared number false prediction accuracy show difference 
table shows performance results testing data set 
cb svm clustering samples outperforms standard svm number random samples 
number data points cb svm table denotes number training data points iteration shown 
training time cb svm table indicates svm training time data equal random samples generated similar number data points 
sampling time cb svm denoting time construction data points takes longer random sampling involves construction cf tree 
long construction time cf tree partly caused non optimized implementation hierarchical micro clustering algorithm 
data size grows random sample size generates similar accuracies cb svm increases svm training time dominating sampling time cb svm fixed total training time svm random sampling ends longer cb svm 
see section 
results discussion large data set generated larger data set parameters table verify performance cb svm compared parameter values number clusters range range range table data generation parameters large data set rate data errors time time asvm cb svm table performance results large data set training data testing data 
rate sampling rate time training time time sampling time asvm selective sampling random sampling asvm selective sampling active learning svm large data sets 
table shows performance results random sampling asvm cb svm large data set 
run svm entire data set take years finish training 
note due simple linear boundary large amount training data random sampling increase performance svm point sample size increases 
asvm cb svm showed error rates lower random sampling highest performance 
training time cb svm total time time shorter asvm random sampling highest performance 
asvm showed similar results basic idea similar implies large data sets svm performs better fine quality samples large amount random samples 
asvm takes longer cb svm large data sets fit memory needs scan entire data set round select closest data point generating cost undergo rounds needs get training data 
experiment ran asvm starting positive negative sample adding samples round gave fairly results 
commonly set 
high performance converges slower ends larger amount training data achieve accuracy low asvm may need undergo rounds :10.1.1.16.4036:10.1.1.16.4036
underwent rounds resulting times data scans sample training data took seconds total training 
discuss asvm section 
real data set section experiment network intrusion detection data set uci kdd archive kdd cup data set consists millions training data thousands testing data 
previously noted cb svm works large data sets including streaming kdd ics uci edu databases kddcup kddcup html data data warehouse analysis especially random sampling hurts performance due infrequent occuring important data irregular patterns data incoming causes different probability distributions training testing data 
network intrusion data set application cb svm testing data probability distribution training data includes specific attack types training data 
datasets contain total training attack types additional types test data 
collected different times periods task realistic 
intrusion experts believe novel attacks variants known attacks signature known attacks sufficient catch novel variants 
experiments data set show method clustering samples significantly outperforms random sampling having number samples 
experiment setup data object consists features continuous features symbolic features 
normalized continuous feature values zero dividing maximum values 
created independent zero predicate feature value symbolic features indicates existence value 
way combining multivariable features may best way svm 
sophisticated techniques pre processing features improve performance 
set cf tree total number features data set times larger synthetic data set range value 
outlier threshold data set tuned lower value outliers network intrusion data set valuable information 
tuning outlier threshold involves heuristics depending type data set type boundary function 
definition justification heuristics specific types problems subsequent 
svm implementation way optimizing parameters experiments synthetic data sets 
linear kernel showed performance accuracy experiment implies classification network intrusion data set separable linear function 
briefly discuss usage nonlinear kernel cb svm section 
results task distinguish normal connections attacks 
table shows performance results random sampling asvm cb svm network intrusion data set 
running svm larger amount samples improve performance reason discussed section 
asvm cb svm generated better results random sampling total training time cb svm faster asvm 
run asvm parameters section 
discussion related aspects related svm fast implementations svm approximations line svm incremental decremental svm dynamic environments selective sampling active learning svm random sampling techniques svm 
algorithms implementation techniques developed training svms efficiently running time rate data errors time time asvm cb svm table performance results network intrusion data set training data testing data 
rate sampling rate time training time time sampling time asvm selective sampling standard qp algorithms grows fast 
effective heuristics speed svm training divide original qp problem small pieces reducing size qp problem 
chunking decomposition sequential minimal optimization known techniques 
cb svm algorithm runs top techniques handle large data sets condensing training data statistical summaries large data groups coarse summary unimportant data fine summary important data 
svm approximation attempted improve computational efficiency svm altering qp formulation extent keeps similar semantic original svm faster solved qp solver 
new formulations proven efficient reliable large data sets 
line svms incremental decremental svms developed handle dynamically incoming data efficiently 
svm model incrementally constructed maintained newer data higher impact svm model older data 
words data higher chance svs svm model older data 
analysis archive data treat data equally generate undesirable outputs 
selective sampling active learning intelligently sample small number training data entire data set maximizes degree learning learning maximally minimum number data points 
core active learning technique select data intelligently degree learning maximized data 
common active learning paradigm iterates training testing process follows construct model training initially data test entire data set model analyzing testing output select data entire data set maximize degree learning round accumulate data training data set train construct model repeat model accurate 
idea selective sampling svm select data close boundary feature space round data near boundary higher chances svs round higher chance move boundary :10.1.1.16.4036
iterate exists data nearer boundary svs 
active learning system needs scan entire data set round select data generates cost large data sets 
random sampling techniques developed reduce training time svm large data sets idea selective sampling samples data near boundary higher probabilities 
need scan entire data set round samples add 
method random sampling developed nonlinear svm random sampling technique kernel trick 
best knowledge proposed method currently svm large data sets tries generate best results limited amount resource 

proposes new method called cb svm clustering svm integrates scalable clustering method svm method effectively runs svm large data sets 
existing svms feasible run data sets due high complexity data size frequent accesses large data sets causing expensive operations 
cb svm applies hierarchical micro clustering algorithm scans entire data set provide svm high quality micro clusters carry statistical summaries data summaries maximize benefit learning svm 
cb svm tries generate best svm boundary large data sets limited amount resource philosophy hierarchical clustering progressive deepening conducted needed find high quality boundaries svm 
experiments synthetic real data sets show cb svm scalable large data sets generating high classification accuracy 
cb svm may perform high dimensional spaces micro clustering high dimensional spaces may meaningful 
developing effective indexing structure high dimensional classification problems documents classification interesting direction 
cb svm currently limited usage svm linear kernels hierarchical micro clusters isomorphic new high dimensional feature space space transformed nonlinear kernel 
constructing effective indexing structures nonlinear kernels interesting direction high practical value especially pattern recognition large data sets classifying forest cover types pictures huge amount satellite data 

agarwal 
shrinkage estimator generalizations proximal support vector machines 
proc 
th int 
conf 
knowledge discovery data mining edmonton canada 
dai watanabe 
random sampling technique training support vector machines 
proc 
th int 
conf 
algorithmic learning theory washington 
beyer goldstein ramakrishnan shaft 
nearest neighbor meaningful 
lecture notes computer science 
burges 
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
poggio 
incremental decremental support vector machine learning 
proc 
advances neural information processing systems vancouver canada 

chang 
lin 
training nu support vector classifiers algorithms 
neural computation 
bengio 
svmtorch support vector machines large scale regression problems 
journal machine learning research 
fung mangasarian 
proximal support vector machine classifiers 
proc 
th int 
conf 
knowledge discovery data mining san francisco ca 
greiner grove roth 
learning active classifiers 
proc 
th int 
conf 
machine learning bari italy 
guha rastogi shim 
cure efficient clustering algorithm large databases 
proc 
acm sigmod int 
conf 
management data wa 
dai 
random sampling technique training support vector machines 
ieee int 
conf 
data mining san jose ca 
jin tung han 
mining top local outliers large databases 
proc 
th int 
conf 
knowledge discovery data mining san francisco ca 
joachims 
making large scale support vector machine learning practical 
scholkopf burges editor advances kernel methods support vector machines 
mit press cambridge ma 
joachims 
text categorization support vector machines 
proc 
th european conference machine learning chemnitz germany 
karypis 
han kumar 
chameleon hierarchical clustering dynamic modeling 
computer 
kivinen smola williamson 
online learning kernels 
proc 
advances neural information processing systems cambridge ma 

lee mangasarian 
reduced support vector machines 
siam int 
conf 
data mining chicago il 
platt 
fast training support vector machines sequential minimal optimization 
scholkopf burges editor advances kernel methods support vector machines 
mit press cambridge ma 
schohn cohn 
active learning support vector machines 
proc 
th int 
conf 
machine learning stanford ca 
smola sch 
tutorial support vector regression 
technical report 
liu sung 
incremental learning support vector machines 
proc 
workshop support vector machines international joint conference articial intelligence stockholm sweden 
tong koller 
support vector machine active learning applications text classification 
proc 
th int 
conf 
machine learning stanford ca 
vapnik 
statistical learning theory 
john wiley sons 
yu han chang 
pebl positive example learning web page classification svm 
proc 
th int 
conf 
knowledge discovery data mining edmonton canada 
zhang ramakrishnan livny :10.1.1.152.7115
birch efficient data clustering method large databases 
proc 
acm sigmod int 
conf 
management data montreal canada 
