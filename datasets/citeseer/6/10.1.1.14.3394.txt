published spring verlag lecture notes artificial intelligence um user modeling proceedings ninth international conference modeling multitasking users malcolm slaney paul maglio ibm almaden research center harry road san jose ca malcolm ieee org ibm com almaden ibm com 
describes algorithm cluster segment sequences low level user actions sequences distinct high level user tasks 
algorithm uses text contained interface windows evidence state user computer interaction 
window text summarized latent semantic indexing lsi 
hierarchical models built expectation maximization represent users macro models 
user actions task modeled micro model gaussian mixture model represent lsi space 
algorithm performance demonstrated test web browsing behavior demonstrates value temporal constraint provided macro model 
problem statement design interfaces effectively support human computer interaction understand complex behavior computer users exhibit carrying jobs 
apple computer example popularized approach interaction design observing users perform analyzing users interact specific software components 
type interaction design optimizes interfaces individual users performing individual tasks 
approach contrast supposes individual users constantly shift tasks seamlessly interleaving low level activities pursuit high level goals 
describe method modeling users engaged sequence different tasks 
models multitasking users adaptive attentive user interfaces monitor user behavior order anticipate user needs 
sorts systems aim automatically provide users additional information just helpful 
relying user model keeps close tabs users shifting tasks systems potentially provide precisely targeted information 
approach modeling multitasking users applications creation adaptation individual user interfaces 
instance want understand behavior computer system large number employees number applications perform tasks optimize system cost 
sort large scale interface optimization requires understanding behavior large number users move task task day 
describe method discovering building hierarchical model user activities unlabeled data 
trace user activities segment user data learn multiple micro models corresponding separate task task defined set actions represented single micro model 
macro model controls switching micro models individual tasks 
models take forms including discrete markov chains continuous hidden markov models hmm 
demonstrate time series clustering algorithm simple web browsing example 
related user modeling focused mainly building single model user activities 
instance davison hirsch describe system called ipam builds table predicts command list past commands 
point time predict command indexing table commands 
table updated real time user enters new commands correct unix command predicted upwards time 
approach micro model type predictions ipam macro model captures relationship set tasks different probabilities 
horvitz colleagues describe system uses bayesian model infer software user goals 
developed language link user computer actions elemental features inference engine 
text showing active window judge state interactive system 
learn model user behavioral state interact video browser 
sequence low level events fast forward play system learns user states browse interesting 
approach supervised train system small set labeled data 
user states goal system learns corresponding pattern low level events 
approach hand completely unsupervised 
real application somebody probably look micro models assign names 
builds hidden markov experts ideas proposed weigend 
goal automatically cluster time series data build models predict different portions data different experts 
goal simply label data extend weigend novel text feature capture user state 
hierarchical segmentation algorithm cluster time series data hierarchy models 
illustrates basic model 
set high level models states determines highlevel behavior signal 
instance macro state correspond speaker speaker segmentation task user task user interface interaction recognition task type multimedia content 
assume system move macro state time control transition probabilities assumed learned data 
macro state controls execution micro model outputs feature vectors transition output probabilities 
macro state system hidden change feature output probabilities captured different micro models 
macro state controls micro fig 
hierarchy models 
general model macro model states si macro state micro model mi arbitrary complexity generate output data 
specific form hierarchical model micro models implemented state fully connected markov models 
model micro model implemented gaussian mixture models gmm 
form described 
model generates features observe 
micro models take different forms 
discrete hierarchical model shown 
case interested example modeling user interaction computer interface discrete set features 
user performing create database task step particular dialog boxes tabbed windows 
file open dialog box probability user go name database dialog box depend create database macro model active 
model system set hidden macro states described markov model 
micro model represented markov model micro state output equal state label 
model generates signals switch models 
described way model simple hidden markov model 
shows model structure described 
case macro states system described markov model macro state controlling single micro model implemented gaussian mixture models gmm 
gmm micro model corresponds output probabilities conventional hmm 
type model data little structure continuous features speaker segmentation modeling labeling multimedia data 
shows overview hierarchical segmentation algorithm cluster segment summarize user actions 
text user screen point time input algorithm 
latent semantic indexing lsi described section encodes interface text multidimensional feature vector function time 
best segmentation user signal parameters micro model estimated expectation maximization em algorithm described section 
text features lsi generate random segmentation build model viterbi search new segmentation fig 
block diagram hierarchical clustering segmentation algorithm 
text features lsi actions need encoded manner allows associate probabilities user events 
consider text screen due user actions indication user trying 
print dialog box contains words printing help page gives information options user 
ways describe action 
application words open document open file latent semantic indexing lsi gives feature set spans differences 
lsi form feature vector summarizes text contained window displayed user 
lsi information retrieval cluster documents determining similarity document semantic query 
lsi creates bag words model document 
term frequency histogram formed counting times word occurs document regardless words precede follow word 
data input decomposition svd finds low dimensional sub space approximates original histogram space 
feature space lsi solves difficult problems associated semantic information retrieval synonyms polysemy 
words meaning synonyms 
information retrieval want synonym retrieve information 
conversely words multiple meanings polysemy 
example apple story grocery store different meaning apple story computer store 
changes semantic space angles distance 
simple sentence semantic content second sentence contains twice words semantic space vector magnitude twice large 
computing svd projecting raw histogram data low dimensional subspace normalize document vector vector length 
normalized vector semantic feature describes know user actions point time 
gmms form micro models describe task 
gmm models probability distribution semantic events form task sum gaussian bumps 
simple model topic repre sented gaussian follow topic gaussian equal probability 
richer micro models possible 
training em algorithm train hierarchy models 
initial set models compute best macro micro level models 
data corresponding macro state train micro model corresponding macro state 
re 
note normalizing document vectors unit length pattern discrimination easier lose information done 
resulting vectors lie unit sphere better match diagonal covariance gmms model distribution 
peat procedure reach stable solution 
training micro model involves estimating output probabilities gmm 
segmentation training algorithm straightforward application em 
describe approach segmenting discrete signal multidimensional points finding nc clusters nm states micro model 
topology shown 
details class algorithms weigend 
assumption assume macro model nc states self loop probability probability transition state nc simple model encourages temporal continuity macro state sequence 
equivalent saying average user spends time steps performing task task equally follow task 
richer models represented different markov macro models change algorithm shown 
initialization choose nc points region points define initial segmentation 
segment points choose new segmentation 
segment build micro model mi captures transition probabilities 
initial mi models 
see section details 
step models viterbi algorithm find path lattice maximum likelihood 
path decode signal decide macro micro states generate portion signal 
termination test exit loop signal temporal cluster assignments change iterations 
degenerate check sure models cluster portion signal 
data assigned model mi find cluster mj largest temporal support 
concatenate segments assigned mj split signal random point 
model mi portion signal split point model mj portion signal split point 
see section step cluster mi concatenate chosen portions signal 
build new model mi captures transition output probabilities data state 
return step 
micro model mi step trained step take forms 
time varying markov model simple gmm describe rest 
practice performance algorithm depends models initialized happens model degenerate longer wins time series data step 
initialization clusters means algorithm initialized random data point data set 
temporal micro models complicated need data 
best success segmenting initial training data nonoverlapping random length segments segment train macro models describe data clusters 
initializing models random tran sition probabilities space random models large model closer data rest wins data points initial step segmentation 
degenerate models occasionally saw cases macro model captures data time series 
data retrain model needs reassigned 
tried approaches address problem 
look segment winning model 
segment calculate negative log likelihood nll data assigned model 
normalize nll number data points compare different sized segments 
choose segment highest nll worst fit data assign missing cluster 
split model largest support perturbing model different directions 
common approach data clustering relatively easy perform perturbation major axis modeled data effectively split cluster major axis 
chose perturb transition matrix small random amount 
find cluster largest support data points split random point time 
train new model original cluster section data train model missing cluster second half 
approach successful random segment sizes came answer twice 
attempt approaches fails model remains degenerate 
random segmentation approach finds way locally degenerate situation 
learning model structure common means build necessary number models decide models clusters necessary 
means single model learned data 
model cluster added stage splitting largest cluster retraining models procedures described section 
alternatively start random models learn right number models 
suggests approach single global model split removing small portion data best fits model 
small portion train new model original model trained remaining data 
data closely clustered iteration picks centroid data builds new model leaves remainder data modelled poorly global model 
approach simulations compared approach 
performance evaluation approach described report completely unsupervised reason macro model labels generated learned models agree macro labels generating test data 
want compare structure learned model labeled data 
information theoretic approaches finding optimal mapping small numbers states brute force approach enumerate possible label permutations choose gives smallest decoding error 
illustrative example practice expect text content windows dialog boxes displayed user form input signal instrumented systems capture data 
illustrate behavior algorithm multi tasking web browsing example 
log web proxy collected sequence uniform resource locators urls user looked information series different topics perl hash molecular biology hmm sudden oak disease 
google find appropriate pages topics 
pages selected topic order topics revisited order 
url logged text web browser gather contents page 
simple heuristics file suffix remove uninteresting urls images code simple heuristics average distance space characters decide url pointed web page containing text 
text web page single document lsi analysis 
experiment user visited total web pages 
lsi reduce semantic feature vector dimensions total number distinct words removing words documents dimensional space 
resulting feature vectors shown 
note clusters distinguishable drawing lines clusters errors simple discriminators gmms 
modeled data assuming fully connected state markov model 
expected model state semantic topic properly segment data topic 
micro model implemented component gmm 
gmm estimates probability point semantic space seen macro state 
shows original reconstructed segmentation macro state sequences 
em algorithm converged correct answer portion signal assigned correct model labels permuted 
reconstruction converged reached stable segmentation iterations 
summarizes learned gmm micro models 
macro model provides important temporal constraint 
simplest form self loop probability suggests user stays task number time steps moving new task 
constraint model locally optimal decision free predict time step user jumps different task 
time series constraint important smooths noisy data 
user visit web page independent task google home page see fig 
raw data lsi feature space experiment described section 
data marked represents perl pages represents molecular biology represents oak disease 
note classes separable simple decision surfaces 
dialog box common tasks print dialog 
want ignore common information free windows tell task 
shows typical segmentation learned hierarchical em algorithm temporal constraint 
self loop probability set state probability 
example state labels match original labels single point errors 
labels incorrect simple component gmms represent cluster probability 
sophisticated model proper distinctions discriminate data 
switching task model user behavior macro model constrains solution allows simple gmm perform errors see 
dif original state sequence reconstruction iteration fig 
original reconstructed sequence user states 
original data different topics encoded perl molecular biology oak disease 
labels reconstructed state sequence arbitrary permuted example 
ideal permutation reconstructed sequence matches original exactly 
ferences results shown figures due power global decision versus local decision 
iteration fig 
micro models implemented component dimensional gmms summarized ellipses shown 
important lsi directions shown plot 
reported simulation results single type hierarchical model 
remains done 
thorough test evaluation requires collecting data web browsing ordinary computer try characterize realistic user tasks 
attentive user interfaces built sort complex mul original state sequence reconstruction iteration fig 
state sequence temporal constraint 
equivalent clustering raw data clusters actions points time misclassified 
user model evaluated comparison simpler text user models 
data large numbers computer users large corporations aggregated analyzed understand people really spend time possibly informing design corporate applications systems 
capturing user behavior models take account multitasking develop tools support people naturally 
acknowledgments appreciate support received myron flickner daniel russell anonymous reviewers 

brin page 
anatomy large scale hypertextual web search engine 
proceedings seventh world wide web conference 

budzik hammond 
user interactions everyday applications context just time information access 
proceedings th international conference intelligent user interfaces new orleans pp 


cypher 
structure users activities 
norman draper eds user centered system design lawrence erlbaum associates hillsdale new jersey pp 


davison hirsh 
predicting sequences user actions 
proceedings aaai icml workshop predicting ai approaches time series analysis aaai press pp 


deerwester dumais landauer furnas harshman 
indexing latent semantic analysis 
jasis pp 


horvitz lumiere project bayesian user modeling inferring goals needs software users 
proc 
th conf 
uncertainty ai madison wi pp 


jelinek 
statistical methods speech recognition 
mit press cambridge ma 

laurel editor 
art human computer interface design 
addison wesley 

maglio barrett 
intermediaries personalize information streams 
communications acm pp 

maglio campbell barrett selker 
architecture developing attentive information systems 
knowledge systems pp 


jean francois 
hmm approach learning adapting sound models speaker indexing 
speaker odyssey crete greece june 

rhodes 
margin notes building contextually aware associative memory 
proceedings th international conference intelligent user interfaces new orleans pp 


shi weigend 
markov gated experts time series analysis regression 
proceedings ieee international conference neural networks houston tx pp 



learning video browsing behavior user interactions 
proceedings eleventh international world wide web conference honolulu hawaii usa 

