reducing rule covers deterministic error bounds vikram jayant vikram dsl serc ernet dsl serc ernet database systems lab serc indian institute science bangalore india output boolean association rule mining algorithms large manual examination 
dense datasets impractical generate frequent itemsets 
closed itemset approach handles information overload pruning uninteresting rules observation rules derived rules 
propose new framework generalized closed closed itemset framework 
allowing small tolerance accuracy itemset supports show number redundant rules far previously estimated 
scheme integrated levelwise algorithms apriori pass algorithms armor evaluate performance measuring reduction output size response time 
experiments show incorporating closed itemsets provides significant performance improvements variety databases 
output boolean association rule mining algorithms large manual examination 
dense datasets impractical generate frequent itemsets 
approaches manage gigantic output closed itemset approach attractive identities supports frequent itemsets derived completely frequent closed itemsets :10.1.1.10.7611
usefulness approach critically depends presence frequent itemsets supersets exactly support 
causes closed itemset approach sensitive noise database minor changes result significant increase number frequent closed itemsets 
example adding poster earlier version appeared proc 
intl 
conf 
data engineering icde march bangalore india 
select transactions mushroom dataset uc irvine repository caused number closed frequent itemsets support threshold increase factor times 
selection transactions break exact equalities itemset supports 
order overcome limitation propose generalized closed closed itemset framework robust database contents 
scheme output exact supports frequent itemsets estimate supports frequent itemsets deterministic user specified tolerance factor 
side effect allowing tolerance itemset supports supports borderline infrequent itemsets may estimated causing incorrectly identified frequent 
typical tolerance factors minimum support threshold major issue 
extra quick database pass check borderline cases 
provide theoretical arguments show closed itemset scheme works substantiate observations experimental evidence 
experiments run variety databases real synthetic sparse dense 
experimental results show small tolerances produce exponentially fewer rules datasets support specifications closed itemsets fewer total number frequent itemsets 
scheme ways post processing step mining process integrated solution :10.1.1.37.1102
show scheme integrated levelwise algorithms pass mining algorithms 
chose classical apriori algorithm representative levelwise algorithms proposed armor representative class pass mining algorithms :10.1.1.58.6945
integration apriori yields new algorithm apriori armor yields armor 
experimental results show integrations result significant reduction response time especially dense datasets 
note integration scheme pass mining algorithms novel important contribution pass algorithms advantages apriori levelwise algorithms 
include significantly cost significantly better performance shown ability provide approximate supports frequent itemsets pass 
ability essential requirement mining data streams infeasible perform pass complete stream 
generalized closed itemsets addition standard boolean association rule mining inputs set database columns set database rows minsup minimum support frequent closed itemset mining problem takes input user specified tolerance factor 
outputs set itemsets refer frequent closed itemsets supports 
frequent closed itemsets required satisfy properties supports frequent itemsets derived output error 
output precisely frequent closed itemsets 
note set closed itemsets may unique database mining parameters 
interested obtaining set itemsets satisfies properties ensure frequent itemsets supports estimated sufficiently accurately 
equal support pruning key concept closed itemset framework lies generalized openness propagation property stated corollary theorem supports itemsets said approximately equal equal denoted support support iff support 
refer allowable error itemset counts tolerance count 
term tolerance reserved allowable error itemset supports equal tolerance count normalized database size 
theorem supersets itemset support support support support 
corollary itemsets support support itemset support support 
result suggests general technique incorporate mining algorithms refer equal support pruning itemset immediate superset equal support prune avoid generating candidates supersets support pruned itemsets say equal subsets remaining unpruned itemsets referred generators 
note generalization notion generators proposed reduce :10.1.1.37.1102:10.1.1.37.1102
generating closed itemsets simple technique generate closed itemsets generators 
technique involve additional database scan required algorithm generating frequent closed itemsets :10.1.1.37.1102:10.1.1.37.1102
technique directly carry closed itemset case 
closed itemset generator theorem enables determine information gathered performing equal support pruning 
refer pruned 
theorem closed itemset generator item fag pruned equal support pruning technique 
immediate supersets pruned technique 
order generate closed itemset generator sufficient compute pruned performing equal support pruning 
note proofs theorems available subset proper subset equal support pruned equal support pruning technique 
necessary include items pruned pruned 
pruned value itemset needs propagated supersets 
generating closed itemsets technique outlined produce correct results epsilon supports frequent itemsets derivable approximately output 
corollary considers itemset superset equal support 
superset say yn equal support approximation error accumulates 
naive interpretation generalized openness propagation property indicate itemset proper superset equal support 
valid general case necessarily true 
theorem reveals upper bound difference supports theorem yn supersets itemset support support support support 
approach solve problem approximation error accumulation ensuring itemset pruned equal support pruning technique maximum possible cumulative error approximation exceed 
itemset having immediate superset yn equal support encountered prune superset long sum differences supports pruned superset tolerance 
performing procedure stage sum differences support counts pruned superset denoted debt 
recall section pruned supersets included pruned 
pruned needs propagated unpruned supersets necessary propagate debt 
itemset pruned referred corresponding closed itemset equal support 
natural extension closed itemset concept pruned closed itemset corresponding rule generation frequent closed itemsets associated supports possible generate association rules approximate supports confidences 
stated theorem theorem closed itemsets associated supports estimated confidence support rule actual confidence support 
minsup 
shown earlier suffices consider rules adjacent frequent closed itemsets itemset lattice rules inferred transitivity 
result carries frequent closed itemsets 
incorporation levelwise algorithms previous sections closed itemset framework theory supporting 
section show framework integrated levelwise algorithms 
chose classical apriori algorithm representative levelwise mining algorithms 
integration scheme apriori yields apriori algorithm mining frequent closed itemsets 
apriori algorithm obtained combining equal support pruning technique described section subset pruning apriori 
pseudocode apriori algorithm shown works follows code lines algorithm excluding lines consists classical apriori algorithm 
function line takes set itemsets input determines counts database making scan 
itemset set candidate itemsets frequent generators frequent generators produced far associated counter count store support count algorithm execution 
itemset fields addition counter pruned described section 
debt integer value check accumulation approximation error itemset supports 
prune function applied line candidates generated line 
responsibility perform equal support pruning ensuring approximation error supports itemsets accumulated 
pseudo code function shown performs task removes itemset subset equal count provided debt remains tolerance 
code lines excluding line analogous close algorithm generating frequent closed itemsets :10.1.1.37.1102:10.1.1.37.1102
line contain equivalent generators close algorithm 
function applied line ensures pruned value itemset appended pruned values immediate subset pseudo code function shown 
necessity performing function explained section showed pruned value itemset propagated supersets 
lines closed itemsets output 
incorporation pass algorithms section show closed framework incorporated pass mining algorithms 
mentioned novel important contribution pass algorithms typically faster level wise algorithms tweaked data streams 
selected apriori minsup tol input database set items minimum support minsup tolerance count tol output generalized closed itemsets 
set itemsets 


ck count supports ck 
gk frequent itemsets ck 
prune gk tol 
gk tol 
ck gk 
gk 
itemset 
output pruned count apriori algorithm prune gk tol input frequent itemsets gk generators tolerance count tol output remove non generators gk 
itemset gk 
jxj subset 
debt count count 
debt debt tol 
gk gk fxg 
pruned pruned 
debt debt pruning non generators gk tol input frequent itemsets gk generators tolerance count tol output propagate pruned value generators gk 
itemset gk 
jxj subset 
debt debt tol 
pruned pruned pruned 
debt debt propagate pruned value supersets proposed armor representative class pass mining algorithms :10.1.1.58.6945
integration closed framework armor yields armor pass algorithm mining frequent closed itemsets 
armor algorithm review structure armor algorithm details available :10.1.1.58.6945
algorithm database conceptually partitioned disjoint blocks data read disk processed partition partition 
pass algorithm starts set itemsets candidates 
processing partition set candidates denoted updated new candidates may inserted existing ones removed 
algorithm ensures stage database scanned far frequent itemsets called frequent itemsets available 
algorithm maintains partial counts itemsets partial count itemset count database scanned far point inserted second pass complete counts candidates obtained pass determined 
pass new insertions candidates longer frequent removed stage 
details incorporation armor complete supports candidate itemsets available pass 
candidate partial support available representing support portion database processed far starting point candidate inserted rule follow integrating closed itemset framework simple processing partition pass find partial support itemset equal superset prune proper superset ensuring approximation error accumulate tolerance limit 
itemset immediate superset yn equal partial support encountered prune superset long sum differences partial supports pruned superset 
processing partitions supports longer equal regenerate pruned supersets follows frequent itemset insert new candidate partial support new candidate set equal performance study previous sections described closed itemset framework apriori armor algorithms 
conducted detailed study assess utility framework reducing output size response time mining operations 
experiments cover range databases mining workloads including real datasets uc irvine machine learning database repository synthetic datasets ibm almaden generator real dataset blue martini software 
due lack space show representative samples results 
complete results available 
resulting pseudo code shown due lack space available algorithms coded experiments conducted mhz pentium iii workstation running red hat linux configured mb main memory local gb scsi rpm disk 
data structures optimizations arrays store itemset counters database passes apriori apriori ensure fairness 
chose tolerance count values ranging zero corresponding exact closed itemset case 
higher values tolerance uninteresting inclusion useful studying effect increasing tolerance output size 
experiment output size reduction pruned tolerance count chess minsup pruned tolerance count minsup output size reduction report experimental results 
experiment measure output size reduction obtained closed itemset framework percentage frequent itemsets pruned result frequent closed itemsets 
results experiment shown figures 
axis graphs represents tolerance count values axis represents percentage frequent itemsets pruned 
graphs see pruning achieved significant cases 
example chess dataset minimum support percentage pruned itemsets zero tolerance closed itemset case 
example tolerance count corresponding maximum error itemset supports percentage pruned itemsets increases 
pruning achieved significant sparse datasets generated ibm almaden generator 
example dataset minimum support percentage pruned itemsets zero tolerance increases tolerance count corresponding maximum error itemset supports 
interesting trend notice cases percentage pruned itemsets increases dramatically low tolerances plateaus tolerance increased 
trend significant indicates maximum benefit attainable closed itemset framework obtained low tolerances 
reason trend follows length itemset increases subsets increase exponentially number 
means chance subsets equal subsets exponentially high 
long generators get pruned low tolerances 
accounts initial steep rise curve 
shorter generators get pruned slower pace regard increase tolerance 
accounts gradual upward slope curve initial exponential increase 
experiment response time reduction performance gain tolerance count chess minsup performance gain tolerance count minsup response time reduction second experiment measure performance gain obtained closed itemset framework 
measured percentage reduction response time apriori apriori 
results experiment shown figures axis graphs represents tolerance count values axis represents performance gain apriori apriori 
graphs see performance gain apriori apriori significant 
fact curves follow trend experiment 
expected bottleneck apriori frequent itemsets mining algorithms lies counting supports candidates 
improvement pruning result corresponding reduction response time 
experiment response times armor third experiment measure response times armor compare apriori 
results experiment shown figures axis graphs represents tolerance count values axis plotted log scale represents response times seconds 
graphs see response times armor order magnitude faster apriori 
notice response times faster increase tolerance count values 
experiment expected candidates pruned higher tolerances 
reduction response time response time seconds tolerance count chess minsup apriori armor response time seconds tolerance count minsup apriori armor response times armor steep experiment due fact armor efficient apriori responsive change number candidates 
show response times armor graphs ran main memory datasets support specifications evaluation 
datasets dense armor described designed sparse datasets memory intensive :10.1.1.58.6945
experiment scale experiment response time seconds tolerance count minsup apriori apriori armor scale experiment fourth final experiment studied scalability armor apriori apriori measuring response times database having records 
results experiment shown 
results analyzed figures show performances algorithms linear database size 
behaviour due reasons number database passes algorithms depends pattern density number transactions 
rate transactions processed pass depend number transactions distribution transactions derived number candidate itemsets efficiency data structure holding counters candidates 
related number post mining schemes discover redundancy association rules proposed 
schemes applied frequent itemsets mined 
inefficient infeasible number frequent itemsets large especially dense databases 
techniques pruning uninteresting rules mining previously :10.1.1.10.7611
studies closed itemset approach sufficient rule considered uninteresting redundant additional predictive power rule fewer items 
techniques closed itemset approach hand tighter requirement rule considered redundant rule redundant identity support derived non redundant rule 
follow tighter approach 
mentioned relax requirement deriving exact supports sufficient supports estimated deterministic user specified tolerance factor 
strategy relaxing requirement deriving exact supports considered 
authors develop notion algorithm called mine 
bound approximation error approach increases linearly itemset length contrast constant bound featured approach 
authors provide bounds approximation error 
focus highly correlated dense data sets show techniques profitably applied sparse data sets 
attempt incorporate scheme pass mining algorithms mentioned earlier essential mining data streams 
close algorithm mining frequent closed itemsets levelwise algorithm apriori :10.1.1.37.1102
apriori significantly differs close zero tolerance case require additional database scan mine closed itemsets respective generators 
achieved utilizing technique described section 
technique bypasses additional processing required algorithm test freeness 
proposed generalized closed itemset framework closed itemset framework order manage information overload produced output frequent itemset mining algorithms 
framework provides order magnitude improvement earlier closed itemset concept 
achieved relaxing requirement exact equality supports itemsets supersets 
framework accepts supports itemsets equal difference supports user specified tolerance factor 
algorithms apriori classical levelwise apriori algorithm armor pass mining algorithm mining frequent closed itemsets 
apriori shown perform significantly better apriori solely frequent closed itemsets fewer frequent itemsets 
armor shown perform order magnitude better apriori workloads experimental evaluation 
aggarwal yu 
online generation association rules 
intl 
conf 
data engineering icde february 
agrawal srikant 
fast algorithms mining association rules 
proc 
intl 
conf 
large databases vldb september 
bayardo agrawal gunopulos 
constraint rule mining large dense databases 
intl 
conf 
data engineering icde february 


frequent closures concise representation binary data mining 
pacific asia conference knowledge discovery data mining pakdd april 


approximation frequency queries means free sets 
european conference principles practice knowledge discovery databases pkdd september 
dong li 
interestingness discovered association rules terms 
pacific asia conference knowledge discovery data mining pakdd 

online association rule mining 
proc 
acm sigmod intl 
conf 
management data june 
klemettinen mannila ronkainen toivonen verkamo 
finding interesting rules large sets discovered association rules 
intl 
conf 
information knowledge management cikm november 
liu hsu ma 
pruning summarizing discovered association rules 
intl 
conf 
knowledge discovery data mining kdd august 
manku motwani 
approximate frequency counts streaming data 
proc 
intl 
conf 
large databases vldb august 
pasquier bastide taouil lakhal :10.1.1.37.1102
discovering frequent closed itemsets association rules 
proc 
intl 
conference database theory icdt january 
pei mine hyper structure mining frequent patterns large databases 
intl 
conf 
data mining icdm december 

generalized closed itemsets improving conciseness rule covers 
technical report tr dsl indian institute science 
:10.1.1.58.6945
efficiency association rule mining algorithms 
conference knowledge discovery data mining pakdd may 
taouil pasquier bastide lakhal 
mining basis association rules closed sets 
intl 
conf 
data engineering icde february 
zaki 
generating non redundant association rules 
intl 
conf 
knowledge discovery data mining kdd august 
zaki hsiao 
charm efficient algorithm closed itemset mining 
siam international conference data mining 

