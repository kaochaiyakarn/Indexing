comparing hmm scfg arun jagota rune 
christian pedersen 
center computer science engineering university california santa cruz ca mail jagota cse ucsc edu brics department computer science university aarhus ny munkegade dk denmark 
mail brics dk 
stochastic models commonly bioinformatics hidden markov models modeling sequence families stochastic context free grammars modeling rna secondary structure formation 
comparing data common task bioinformatics natural consider compare stochastic models 
rst study problem comparing hidden markov model stochastic context free grammar 
describe compute emission collision probability probability independently generate sequence 
consider related problem nding run hidden markov model derivation grammar generate sequence maximal joint probability generalization cyk algorithm parsing sequence stochastic context free grammar 
illustrate methods experiment rna secondary structures 
basic chain structure key dna rna proteins allows view strings sequences nite alphabets obviously nite length 
furthermore sequences completely random exhibit various kinds structures di erent contexts 
family homologous proteins similar amino acid residues equivalent positions rna sequence pairs complementary subsequences form base pairing helices 
natural consider applying models formal language theory model di erent classes biological sequences 
completely random biological sequences possess inherent stochastic traits due mutations family homologous sequences lack knowledge computing power correctly model aspects rna secondary structure formation 
better stochastic models giving probability distribution sequences high supported program mathematics molecular biology partially supported ist programme eu contract number ist alcom ft basic research computer science www brics dk funded danish national research foundation probability re ects sequence belong class sequences modeled formalisms distinguishing sequences belonging class modeled 
widely grammatical models bioinformatics hidden markov models stochastic context free grammars models proposed 
types stochastic models originally developed tools speech recognition see :10.1.1.131.2084
identify hidden markov models stochastic version regular languages stochastic context free grammars stochastic version context free languages see formal languages 
depth treatment biological uses hidden markov models stochastic context free grammars chap :10.1.1.131.2084

stochastic models commonly model families biological sequences common task bioinformatics comparing data natural ask compare stochastic models 
described compare hidden markov models computing emission collision probability probability distributions models probability models independently generate sequence 
having emission probability pair probability distributions distributions easy compute hellinger distance distributions 
study problem comparing hidden markov model stochastic context free grammar 
develop recursions emission probability distributions model grammar recursions lead set quadratic equations 
quadratic equations generally hard solve show nd approximate solution simple iteration scheme 
furthermore show solve equivalent maximization problem problem nding run hidden markov model derivation grammar generate sequence maximal joint probability 
essence parsing hidden markov model grammar algorithm viewed generalization cyk algorithm parsing sequence stochastic context free grammar 
cases complexity algorithm identical complexity cyk algorithm 
discuss undecidability natural extensions results 
structure follows 
sect 
brie introduce hidden markov models stochastic context free grammars terminology 
sect 
consider problem computing emission probability probability distributions hidden markov model stochastic context free grammar sect 
develop algorithm parsing hidden markov model stochastic context free grammar 
sect 
experiment sect 
discuss problems occurring trying extend methods sect 

hidden markov models hidden markov model generative model consisting states transitions states state silent non silent 
model generates string nite alphabet probability pm pm describes probability distribution set nite strings 
hidden markov model left right model states numbered transitions state state satisfy run begins special start state continues state state state transition probabilities probability transition state special state reached 
state silent non silent state 
time non silent state entered symbol emitted symbol emission probabilities probability emitting symbol state entering silent state emitted 
run follows markovian path states generates string concatenation emitted symbols 
probability pm generating probability path generating probability pm path generating depends subsequence non silent states path 
length di erent number non silent states probability pm zero 
pm pm 
pm 
pm pm partial run run starts state ends state necessarily special start states 
ease presentation methods sect 
introduce concept partial run semi including meaning non silent state symbol emitted probability partial run semi including generating probability generating path immediate predecessor transition string model cient algorithms determining pm run maximal probability generating known see :10.1.1.131.2084
stochastic context free grammars context free grammar describes set nite strings nite alphabet called language 
consists set non terminals set terminals empty string set production rules production rule means rewritten applying rule 
string derived non terminal rewritten sequence production rules language described derived special start non terminal derivation sequence production rules rewrites derivation called parsing stochastic context free grammar context free grammar production rules assigned probabilities pg sum probabilities possible production rules non terminal 
resulting stochastic grammar describes probability distribution set nite string nite alphabet probability pg deriving referred probability generating sum probabilities possible derivations probability derivation pg pg product probabilities production rules sequence applied derive stochastic context free grammar transformed equivalent stochastic context free grammar chomsky normal form describes language production rules split sets set pn non terminal production rules form xy set terminal production rules form non terminal non terminals special start non terminal special start non terminal symbol empty string 
stochastic context free grammar chomsky normal form compute pg inside algorithm parse cyk algorithm time jp 
jsj jp number production rules jp jp jsj length see 
comparing scfg hmm section consider problem comparing stochastic contextfree grammar chomsky normal form hidden markov model generate strings alphabet 
precisely consider problem computing emission probability probability distributions strings quantity pg 
pm similar de nition emission probability hidden markov models 
quantity referred collision probability probability distributions probability strings picked random probability distributions collide identical 
initially assume acyclic hidden markov model left right hidden markov model states self loop transitions 
interesting class hidden markov models model type generate strings arbitrary length ideas approach computing emission probability class models applicable left right general hidden markov models 
initial discussion assuming acyclic hidden markov models essential production special start non terminal 
complex hidden markov models recursions developed allow non terminals productions 
adds complications computations hand allowing simpler transformation arbitrary grammar relaxed chomsky normal form non terminals productions 
acyclic hidden markov models approach closely mimicking inside algorithm computing probability stochastic contextfree grammar generates string 
inside algorithm computing probability string derived stochastic context free grammar track kept probability substring derived non terminal grammar 
algorithm computing emission probability keep track probability deriving string non terminal generated partial run state state semi including dynamic programming algorithm maintain arrays purposes 
sum probabilities partial runs semi including emit symbols illustrated fig 

states partial runs possibly silent states 
probability independently deriving single symbol string non terminal generating single symbol string partial run semi including illustrated fig 

probability independently deriving string nonterminal generating string partial run semi including illustrated fig 

purpose arrays deal ciently partial runs consisting silent states 
symbols string sense non silent main new problem encountered modifying inside algorithm parse acyclic hidden markov models 
array similar array maintained inside algorithm 
array inside algorithm tells probability deriving substring string parsed non terminals similarly array tell probability independently generating sequence partial run pairs states time deriving non terminals evident start start symbol start start states emission probability probability deriving string generated genuine run start assuming state silent partial run start semi including genuine run having described required arrays specify recursions computing argue recursions split computations smaller parts dependencies recursions acyclic obtain array holds probabilities getting state state emitting symbols 
array holds probabilities getting state state emitting symbol time generating symbol terminal production rule non terminal array holds probabilities getting state state emitting string time generating string non terminal 
illustration individual purposes recursions arrays 
hollow circles denote silent states solid circles denote non silent states hatched circles denote states type 
arrows indicate partial runs arbitrary length straight arrows indicate single transitions states 
dynamic programming algorithm computing 
array probabilities getting state state path consists silent states 
general path broken transition path preceding path going silent states 
obtain recursion rst case takes care initialization 
silent 
ordering states referred summation ordering consistent partial ordering states acyclic transition structure 
immediately observes entry array requires time time compute 
entire array computed time 
observe need sum states transition summation 
observation reduce time requirements nm transition part recursions 
array holds probabilities getting state state partial run generating exactly symbol time generating symbol terminal production rule 
general partition partial run initial path consisting silent states remaining partial run starting non silent state emitting symbol 
obtain recursion initialization handled special case initial path silent states empty 
pg 


non silent 
nd time requirements computing array observes non terminal occurs entries 
terminal production rule part recursions 
recursions requires time compute reduce time requirements handling rst sum parentheses observing need sum states transition similar way time requirements computing array reduced second sum trick applied 
computing array requires time jp 

array hold probabilities getting state state partial run generating string time generating string non terminal purpose arrays handle special cases single symbol emitted partial run cases really recurse non terminal ignore completely consider terminal production rules 
general case string symbol generated need apply non terminal production rule xy part string derived part string derived special start non terminal start non terminal generate empty string string derived string derived non empty 
need consider partial runs length zero dividing partial run partial runs 
leads recursion 
pg 
xy pg xy 
non silent 
reason requiring non silent state sum ensure unique decomposition partial runs allowed silent erroneously include partial runs times summation 
computation array observe non terminal production rule part recursions recursion requires time compute 
obtain total time requirements jpn 
computing array 
adding time requirements computing arrays leads time requirements jp 
computing emission probability correspondence time requirements jp 
jsj parsing string grammar cf 
sect 

previously stated order recursions dynamic programming algorithm need argue recursions refer elements sense smaller 
recursion entries arrays depends 
easy observation pairs states indexing array right hand side recursions closer ordering states pair states indexing array left hand side recursions 
point recursing smaller elements exactly run problems trying extend method allow cycles allowing self loops left right models problem crops 
easily modify remain valid equations entries arrays 
needed change strict inequalities summation boundaries include equality 
speci cally assume non silent states self loop transitions self loop transitions silent states meaningless easily eliminated need change pg 


non silent 
pg 
xy pg xy 

array refers smaller elements 
array entry array states closer ordering strictly larger ordering sum 
array choose equal sum 
compute need know turn depend 
stated holds equations entries array 
pair states system equations variable equation restriction variables attain non negative values non terminal assume solve systems order distance pair states ordering increasing rst consider systems systems successor systems systems linear equations 
equation entry unknown quantities occurrences corresponding production rules xy occurrences coe cients pg xy 
pg xy 
respectively coe cients known values sum lead number terms form 

system equations quadratic 
state self loop transition need solve system quadratic equations variable equation non terminal general systems quadratic equations hard solve see construction proving requires equations terms having coe cients sign 
immediately observe system equations left hand side terms coe cients opposite sign right hand side terms 
hardness proof relate system quadratic equations obtain 
able nd literature algorithms solving systems type derivable 
dependencies positive approximate solution simply initializing entries terms depending arrays iteratively update entry turn 
process converge true solution 
conjecture convergence rapid realistic grammars models 
general hidden markov models ordering states modify hold general hidden markov models ordering constraints states removed summations 
longer ordering states entries depend 
separate systems equations entries array independent blocks pair states indexing entry array 
means obtain just aggregate system quadratic equations jv 
variables equations entries array jv number non terminals number states entries arrays get systems linear equations 
array computed simple dynamic programming 
parsing hmm scfg stochastic context free grammar modeling rna secondary structures hidden markov model representing family rna sequences method previous section determine likelihood family common secondary structure 
established want nd structure 
words equivalent cyk algorithm nding parse string nding parse hidden markov model desirable 
want compute 
pm derivation run string generated run preferably way allows easily extract derivation run witnessing maximum 
basic principles compute value similar computing emission probability previous section 
general technique nd probabilities optimal combinations derivation non terminal partial run pair states yielding string 
furthermore optimal combination split pair optimal combinations derivation non terminal partial run states illustrated fig 

computing maxima easier computing sums 
main reason consider combinations derivations partial runs arbitrary lengths 
probability optimal combination particular choice non terminal pair states depend product probabilities larger probabilities 
similar principle algorithms nding shortest paths graphs positive edge weights cf 
chapters 
surprisingly methods described section bear strong algorithms particular nature problem prevent formulating simply shortest path problem chosen graph 
description method employ arrays amax bmax cmax similar arrays previous section 
amax entries hold maximum probability partial run state state semi including emitting symbols 
just path maximum weight graph de ned transitions leaving non silent state compute amax array standard pairs shortest paths graph algorithms time log nm cf 

bmax entries hold maximum probability combination derivation consisting terminal production rule partial run generating string consisting symbol 
imposes restriction path preventing standard graph algorithms 
need combine transitions non silent states preceding succeeding partial runs emitting symbols entries amax compute bmax entries directly equation bmax max pg 


amax 
amax time jp 


possibly dominating term time requirements computing value see 
specify cient way compute bmax entries 
non silent state optimal choice preceding partial run emitting symbols empty run 
bmax max pg 


amax entries bmax non silent 
having computed entries proceed compute entries silent bmax max non silent 
bmax computing bmax array way reduces time requirements jp 

ready specify determine value compute entries cmax array 
entry cmax holds maximum probability combination derivation partial run yielding string 
equation computing entry cmax closely follows fig 

cmax max pg 
amax bmax max pg xy 
cmax 
cmax xy exception harm considering combination derivation partial run times working maxima sums 
restriction type state denoted maximize general case 
actual implementation want retain type restriction speed program 
slightly shortest path problem 
gives number inequalities entries cmax similar lemma cmax large terms right hand side 
technique similar relaxation technique discussed pp 
time cmax pg xy 
cmax 
cmax xy increase cmax value 
means start initializing cmax 
amax bmax keep updating cmax iterating possible relaxations changes algorithm algorithm computing optimal parse hidden markov model stochastic context free grammar initialization pq pq insert 
amax bmax main loop entry xed time pq empty fix entry highest probability xed pq insert combine entry feasible xed entries uy pq 
pq 
occur 
process eventually terminate entry depend discussed 
worst case time requirements scheme quite excessive 
question order relaxations compute cmax ciently remains 
propose approach similar dijkstra algorithm computing single source shortest paths graph cf 
sect 

assume set entries determined correct value performed relaxations combining correct values entries initialized entries amax bmax arrays mentioned preceding paragraph 
cmax entry maximum value claim current value cmax correct increased 
fact entry correct value larger cmax 
reason relaxation combining entries assumed performed involve entry current value cmax 
entry relaxation value relaxations lead values cmax 
insert cmax perform relaxations combining cmax entry idea formalized algorithm 
time requirements algorithm 
extent course depends choice data structures implementing priority queue pq set key observation possible relaxation combination particular entries performed entry smaller value inserted algorithm performs jpn 
relaxations 
observe relaxation need perform operation pq operations pq performed jv 
times 
critical operation assume pq implemented fibonacci heap 
limits time requirements operations pq jpn 
jv 

log jv 

set need able insert element ciently ef ciently iterate elements particular non terminal state 
having set aside time jp 

priority queue operations operations need cient 
turns su cient maintain simply dimensional boolean array indexed 
insertion constant time operation 
allow cient way iterate elements particular nonterminal state short iterating elements non terminal state test membership individual element 
turns su ciently cient 
situations especially elements test membership numerous elements membership test associated relaxation involving particular pair elements relaxation performed test succeeds 
furthermore relaxation test membership twice element relaxation combines 
total time spend iterating elements jpn 

time requirements algorithm jpn 
jv 

log jv 

combined time complexity computing amax bmax arrays leads time complexity jp 
jv 

log jv 
determining optimal parse general hidden markov model stochastic context free grammar having computed amax bmax cmax easy nd optimal parse standard backtracking techniques 
compared time requirements jp 
jsj nding optimal parse string cyk algorithm 
description assumptions structure hidden markov model natural question ask gained respect time requirements restricting attention left right models 
lot surprising considering close complexity cyk algorithm 
observe cmax depend entries cmax ordering respect implicit partial ordering states left right model 
separate systems choice solved time prede ned order 
priority queue needs hold jv elements time reducing time complexity nding optimal parse jp 
jv 

log jv 
importantly need priority queue jv elements jv usually small feasible replace fibonacci heap implementation implementations worse asymptotic complexities smaller overhead 
just implement pq array scanning entire array time operation performed complexity parsing left right model increases jp 
jv 
involved constants small 
seq 
gg cca seq 
max 
gg 
alignment predicted secondary structure sequences seq seq construct trna model sequence secondary structure maximal parse trna aligned states trna emitting symbols 
positions indicated sequence maximal parse match sequences 
results algorithm implemented program comparison stochastic random models currently working adding computation emission probability implementation 
implementation available www cse ucsc edu tar gz 
illustrating experiment program parse trna pro le hidden markov model part test suite sam software distribution available www cse ucsc edu research sam html 
model built alignment seq seq shown fig 
symbol emission probabilities position little symbols position alignment rest probability distributed evenly symbols position alignment 
match state probability positions gaps alignment choosing transition match state 
grammar stochastic context free grammar general rna secondary structure 
grammar predicting secondary structure seq seq shown fig 

evident fig 
maximal parse poor job nding common structural elements sequences 
part explained fact half base pairs structures sequences shared structure sequence 
shared base pairs structure maximal parse 
observe structure maximal parse dense structures seq seq bases part uninterrupted helices constituting structure 
probably indication main problem maximal parse predict secondary structure 
position choose symbol construct sequence structure high probability maximal parse question highly probable structures nding matching sequences really looking highly probable sequences nding matching structure 
supported positions maximal parse disagrees seq seq especially seq seq agree positions sequence obtained changing symbols positions symbols shared seq seq roughly times probable hidden markov model 
problem maximum discriminating states exhibit complementarity states exhibit complementarity 
state probabilities emitting gets lower probability paired state identical paired state probability emitting emitting having framework algorithm easy modify details accommodate scoring combinations pairs states derivations introducing base pairs captures complementarity better 
furthermore probability capture emitting states exhibit better complementarity pair 
better idea common structure obtained looking probability states emit symbols base paired pairs non silent states similar 
dependencies energy rules commonly rna secondary structure prediction captured context free grammar combine computation emission probability discussed computation equilibrium partition function obtain probabilities base pairing positions including randomness base pairing captured partition functions variability family sequences captured pro le hidden markov model 
discussion considered problem comparing hidden markov model stochastic context free grammar 
methods viewed natural generalizations methods analyzing strings means stochastic context free grammars idea comparing hidden markov models 
natural question extend results comparing stochastic context free grammars 
determine emission probability just maximal joint probability pair parses stochastic context free grammars determine languages context free grammars disjoint simply assigning uniform probability distribution derivations variables asking computed probability zero 
wellknown result formal language theory states undecidable languages context free grammars disjoint theorem 
generalize methods methods comparing stochastic context free grammars precision allows determine true probability zero 
emission probability compute distance probability distributions hidden markov models 
having demonstrated compute emission probability hidden markov model stochastic context free grammar thing required compute distance emission probability grammar 
stated problem computing emission probability stochastic context free grammars undecidable hope computing emission probability stochastic context free grammar easier 
stochastic context free grammars construct aggregate grammar start symbols chosen equal probability half 
easy see emission probability sum emission probabilities plus twice emission probability computing emission probability stochastic context free grammar hellinger distances probability distributions context free grammar hidden markov model hard computing emission probability stochastic context free grammars 
emission probability measure comparing stochastic models 
emission probability interesting uses 
allows model prior training model distribution sequences hidden markov model prior belief distribution sequences stochastic context free grammar want construct 
secondly allows compute probability stochastic models independently generated sequence models generate sequence 
combine models assumption independence 

asai 
prediction protein secondary hidden markov model 
computer applications biosciences 

baker 
trainable grammars speech recognition 
speech communications papers th meeting acoustical society america pages 

churchill 
stochastic models heterogeneous dna sequences 
bulletin mathematical biology 

cormen leiserson rivest 
algorithms 
mit press 

durbin eddy krogh mitchison 
biological sequence analysis models proteins nucleic acids 
cambridge university press 

phillips safra 
characterized approximation problem 
information processing letters :10.1.1.131.2084

knudsen hein 
rna secondary structure prediction stochastic context free grammars evolutionary history 
bioinformatics 

krogh 
methods improving performance hmm application gene nding 
proceedings th international conference intelligent systems molecular biology ismb pages 

krogh brown mian haussler 
hidden markov models computational biology applications protein modeling 
journal molecular biology 

pedersen nielsen 
metrics similarity measures hidden markov models 
proceedings th international conference intelligent systems molecular biology ismb pages 

mccaskill 
equilibrium partition function base pair binding probabilities rna secondary structure 
biopolymers 

rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee volume pages 

eddy 
language rna formal grammar includes 
bioinformatics 

sakakibara brown hughey mian underwood haussler 
stochastic context free grammars trna modeling 
nucleic acids research 


linguistics dna 
american scientist 

von krogh 
hidden markov model predicting helices protein sequences 
proceedings th international conference intelligent systems molecular biology ismb 


languages machines 
computer science 
addison wesley publishing 

hasegawa kobayashi yokomori 
tree adjoining grammars rna structure prediction 
theoretical computer science 

zuker 
nding suboptimal rna molecule 
science 
