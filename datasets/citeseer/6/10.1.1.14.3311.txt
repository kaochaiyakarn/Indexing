refining kernels regression uneven classification problems kandola john shawe taylor dept computer science royal holloway university london surrey tw ex 
uk kernel alignment proposed method measuring degree agreement kernel classification learning task 
extend notion kernel alignment common learning problems regression classification uneven data 
modified definition alignment novel theoretical justification improving alignment lead better performance regression case 
experimental evidence provided show improving alignment leads reduction generalization error standard regressors classifiers 
kernel methods increasingly data modelling conceptual simplicity performance tasks see example 
kernel function chosen trial error heuristics quantitative measure agreement important theoretical practical point view 
kernel alignment proposed method measuring degree agreement kernel classifica tion task 
extends kernel alignment machine learning problems regression classification uneven datasets 
novel inductive kernel alignment optimization algorithm 
structure follows 
section give formal definition alignment 
section extends theory alignment case regression providing definition alignment case novel justification improving alignment lead better performance regression case 
section considers case uneven datasets natural extension classi fication case considered 
section presents novel inductive algorithm kernel target alignment section presents experimental results methods 
finish discussion 
kernel alignment quantitative measure agreement kernels learning task important theoretical practical point view 
introduce concept alignment measures degree agreement kernel target 
definition alignment empirical alignment kernel kernel respect sample quantity ki kernel matrix sample kernel ki 
definition inner products gram matrices known frobenius inner product 
consider classification scenario yy vector outputs sample yy yy ky yy yy alignment shown possess con properties 
notably effi ciently computed training kernel machine takes place eigenvalue decomposition kernel matrix training data information 
complete eigendecomposition kernel matrix expensive computational step avoided large kernel matrices 
approximation strategy full eigenvalue decomposition gram schmidt decomposition making alignment optimization tractable large kernel matrices 
kernel alignment regression problem regression approximate un known real valued function observation limited sequence typically noise corrupted input output data pairs 
formally consider dataset xi yi drawn unknown probability distribution xi represents set inputs yi represents single output represents number training examples 
regression function learnt training set performance measured independent test set 
algorithm give method improve alignment kernel fixed set target variables acting eigenvalues 
algorithm performs transduction provides nonparametric way perform kernel selection require specify family kernel functions directly acts entries kernel matrix 
apply transductive algorithm case regression rank matrix yy needs modified transformation represents mean training set target values 
classification alignment motivated facts 
firstly alignment measure concentrated expected value 
suggests optimize value training set expect see corresponding increases testing set alignment 
expectation verified classification case 
proof concentration special fact labels binary regression alignment concentrated provided range output values bounded proof omitted 
second observation case classification value alignment high parzen window estimator give generalization 
justified adapting kernel improve alignment target training set result better generalization performance 
argument applied regression case modification 
complex analysis suggesting improving alignment regression improve generalization 
key result optimizing alignment dimensional linear projection data equivalent performing ridge regression value alignment corresponds objective ridge regression optimization 
furthermore alignment minus kernel matrix provides lower bound projected alignment 
optimizing alignment kernel decreases upper bound ridge regression objective mn xi yi forms adaptable part upper bound generalization error 
theorem feature example matrix ex pressed possibly kernel defined feature space 
solution optimization lw ww yy gives weight vector solves ridge regression problem regularization parameter proof observe ww yy yy argmax ww xl argmax implicitly observed invariance rescaling consider optimizing square root numerator denominator constrained fixed value introduce la multipliers obtain problem xy iz xx 
varying correspond obtaining different values constrained denominator 
pair optimization minimizes 
result invariant ing choose giving xy just negative ridge regression op timization 
step show projected alignment lower bounded alignment matrix 
theorem feature example matrix ex pressed possibly kernel defined feature space 
solution optimization ww yy satisfies yy yy 
proof loss generality take lying space spanned columns consider creating orthonormal basis space spanned columns wl 
wm write perpendicular projection matrix space spanned columns observe xy similarly iix tx iix ma xw yy xx wi wi xy wi wi xll giving yy yy required 
kernel alignment uneven datasets uneven datasets unequal number class labels target vector exist commonplace real world applications 
consider problem document classification particular query unreasonable expect large number documents match particular query giving rise uneven set class labels 
justification kernel alignment relation performance parzen windows estimator implicit assumption equal number positive negative class labels equal weights positive negative examples 
apply parzen window argument uneven datasets rank matrix yy needs modified transformation yi positive represents number positive negative labels dataset respectively 
gives slightly modified definition alignment 
proof concentration hold provided number positive negative examples remains generalization bound standard parzen window estimator unequal weights positive negative examples 
inductive kernel alignment section consider inductive algorithm kernel alignment optimization 
transductive algo rithms considered far relied eigenvalue decomposition full kernel matrix constructed training test data points 
reassembled complete kernel matrix entire set data obtained 
describe implement analogous inductive proce dure 
dataset needs randomly split training test set kernel matrix constructed training data 
eigenvalue decomposition kernel matrix written vav diagonal matrix 
effect decomposition find sequence subspaces feature space capture greatest variance data 
reweight directions optimize alignment training set kernel matrix labels method described section 
difference project new data subspace feature space spanned eigenvectors principal axes coordinate system 
rescale coordinate resulting feature vector compute inner products transformed space 
pseudo matlab code procedure algorithm 
data construct kernel matrix yy maximum number runs split data training test set threshold small eigenvalues number remaining eigenvalues endfor diag 
compute alignment train svm parzen window endfor algorithm inductive alignment algorithm complete eigendecomposition kernel matrix expensive computational step 
companion inductive approximation strategy gram schmidt decomposition making optimization feasible large kernel matrices 
experiments demonstrate performance transductive inductive algorithms regression uneven datasets range datasets considered 
medline dataset commonly text process ing uneven classification problem 
dataset contains documents queries obtained national library medicine 
focus query 
contains negative positive examples 
words tion removed documents porter stemmer applied words 
terms documents weighted variant tfidf scheme 
log log dr tf represents term frequency df document frequency total number documents 
order test performance alignment algorithm regression automobile miles gallon dataset considered 
contains miles travelled gallon fuel consumed various cars 
input variables measure characteristics car number cylinders discrete displacement horsepower weight acceleration model year discrete 
goal dis cover relationship cars characteristics 
removing small number entries missing values original dataset datapoints remain 
different learning algorithms implemented uneven datasets 
parzen window estimator support vector classifier svc 
regression case implemented ridge regression rr motivated analysis section 
fold cross validation procedure find optimal value capacity control parameter 
having selected optimal parameter svc re trained times random data splits 
similar procedure select ridge regression parameter error results different algorithms tables values 
measure popular statistic information retrieval community comparing performance algorithms uneven data 
computed pr represents preci sion measure proportion selected items system classified correctly represents recall proportion target items system selected 
table presents results medline dataset support vector classifier svc parzen window pw estimator bag words kernel 
matrices adaption matrices optimization transductive alignment algorithm 
index represents percentage training points 
table apparent training alignment increases aligned matrix data partitions 
similar affect observed alignment test set 
reduction svc mean generalization error pw error training sets 
quoted value derived svc increases data partitions aligned matrix 
table presents results obtained inductive alignment algorithm applied medline dataset 
apparent training alignment increases adapted matrix data partitions 
similar affect observed test set alignment 
reduction svc mean generalization error pw error training sets 
comparing results obtained applying transductive inductive alignment gorithms medline datasets see tables note similar behaviour algorithms 
consistent improvement train test set alignment improved svc pw error measures 
table represents alignment training test datasets associated ridge regression rr generalization error dataset 
clear training alignment increases adapted matrix data partitions 
similar affect observed test alignment 
reduction rr mean generalization error training sets 
dataset similar trend observed 
datasets sig decrease rr errors datasets 
table uneven medline dataset alignment values svc pw test error values obtained svc bag words kernel runs transductive algorithm 
train align test align svc error pw error svc oo ls table uneven medline dataset alignment values svc kernel runs inductive algorithm 
train align test align svc error go pw error values bag words fi svc table regression dataset alignment values ridge regression rr error linear kernel runs 
train align test align ii ii ii assess performance regression algorithm high noise datasets 
discussion addressed problem measuring degree agreement kernel learning tasks 
extended notion kernel align ment originally 
alignment sion analysis classification uneven datasets motivated demonstrated 
novel inductive algorithm framework kernel alignment kernel combination kernel selection 
algorithms tested performance 
computational cost performing eigenvalue decomposition kernel matrix prohibitive large kernel matrices 
examples considered small moderate size computational cost problem 
larger kernel matrices arise typically real world datasets method prohibitive 
companion proposed faster approach performing gram schmidt optimization kernel defined feature space interesting com pare performance approach 
performance algorithms evaluated high noise datasets 
tasks left 
acknowledgments acknowledge financial support epsrc 
gr eu project 
ist neurocolt working group 
cristianini elisseeff shawe taylor kandola 
kernel target alignment 
proceedings neural information processing systems 
cristianini shawe taylor lodhi 
la tent semantic kernels 
journal intelligent infor mation systems 
cristianini shawe taylor 
tion support vector machines 
cambridge university press cambridge uk 
joachims 
text categorization support vector machines 
proceedings european con ference machine learning ecml 
kandola shawe taylor cristianini 
efficient kernel optimisation approach 
submitted proceedings neural information processing systems 
