support vector networks corinna cortes vladimir vapnik labs research usa 
support vector network new learning machine group classification problems 
machine conceptually implements idea input vectors non linearly mapped high dimension feature space 
feature space linear decision surface constructed 
special properties decision surface ensures high generalization ability learning machine 
idea network previously implemented restricted case training data separated errors 
extend result non separable training data 
high generalization ability support vector networks utilizing polynomial input transformations demonstrated 
compare performance network various classical learning algorithms took part benchmark study optical character recognition 
keywords pattern recognition efficient learning algorithms neural networks radial basis function classifiers polynomial classifiers 
years ago fisher suggested algorithm pattern recognition 
considered model normal distributed populations sigma sigma dimensional vectors mean vectors variance matrices sigma sigma showed optimal bayesian solution quadratic decision function sq sign gamma sigma gamma gamma gamma gamma sigma gamma gamma ln sigma sigma case sigma sigma sigma quadratic decision function degenerates linear function lin sign gamma sigma gamma gamma sigma gamma gamma sigma gamma estimate quadratic decision function determine free parameters 
estimate linear function free parameters determined 
daytime phone 
mail corinna research att com daytime phone 
mail research att com output hidden units weights hidden units dot products weights hidden units dot products dot product perceptron output 
input vector weights output unit 
output hidden units simple feed forward perceptron input units layers hidden units output unit 
gray shading vector entries reflects numeric value 
case number observations small say estimating parameters reliable 
fisher recommended case sigma sigma linear discriminator function sigma form sigma sigma gamma sigma constant fisher recommended linear decision function case distributions normal 
algorithms pattern recognition associated construction linear decision surfaces 
rosenblatt explored different kind learning machines perceptrons neural networks 
perceptron consists connected neurons neuron implements separating hyperplane perceptron implements piecewise linear separating surface 
see 
problem finding algorithm minimizes error set vectors adjusting weights network rosenblatt time rosenblatt suggested scheme weights output unit adaptive 
fixed setting weights input vectors non linearly transformed feature space layer units 
space linear decision function constructed sign ff adjusting weights ff th hidden unit output unit minimize error measure training data 
result rosenblatt approach optimal coefficient sixties 
construction decision rules associated construction linear hyperplanes space 
algorithm allows weights neural network adapt order locally minimize error set vectors belonging pattern recognition problem back propagation algorithm discovered 
solution involves slight modification mathematical model neurons 
neural networks implement piece wise linear type decision functions 
article construct new type learning machines called network 
support vector network implements idea maps input vectors high dimensional feature space non linear mapping chosen priori 
space linear decision surface constructed special properties ensure high generalization ability network 
example obtain decision surface corresponding polynomial degree create feature space coordinates form coordinates coordinates gamma gamma coordinates 
hyperplane constructed space 
problems arise approach conceptual technical 
conceptual problem find separating hyperplane generalize dimensionality feature space huge hyperplanes separate training data necessarily generalize technical problem computationally treat high dimensional spaces construct polynomial degree dimensional space may necessary construct hyperplanes dimensional feature space 
conceptual part problem solved case optimal hyperplanes separable classes 
optimal hyperplane defined linear decision function maximal margin vectors classes see 
observed construct optimal hyperplanes take account small amount training data called support vectors determine margin 
shown training vectors separated errors optimal hyperplane expectation value probability committing error test example bounded ratio expectation recall fisher concerns small amounts data quadratic discriminant function 
optimal margin optimal hyperplane example separable problem dimensional space 
support vectors marked grey squares define margin largest separation classes 
value number support vectors number training vectors pr error number support vectors number training vectors note bound explicitly contain dimensionality space separation 
follows bound optimal hyperplane constructed small number support vectors relative training set size generalization ability high infinite dimensional space 
section demonstrate ratio real life problems low optimal hyperplane generalizes dimensional feature space 
delta optimal hyperplane feature space 
show weights optimal hyperplane feature space written linear combination support vectors support vectors ff linear decision function feature space accordingly form sign support vectors ff delta delta dot product support vectors vector feature space 
decision function described layer network 

non linear transformation support vectors feature space input vector feature space input vector classification classification support vector network unknown pattern conceptually done transforming pattern high dimensional feature space 
optimal hyperplane constructed feature space determines output 
similarity layer perceptron seen comparison 
input vector vectors support lagrange multipliers classification comparison classification unknown pattern support vector network 
pattern input space compared support vectors 
resulting values nonlinearly transformed 
linear function transformed values determine output classifier 
optimal hyperplane generalizes technical problem treat high dimensional feature space remains 
shown order operations constructing decision function interchanged making non linear transformation input vectors followed dot products support vectors feature space compare vectors input space dot product distance measure non linear transformation value result :10.1.1.103.1189
see 
enables construction rich classes decision surfaces example polynomial decision surfaces arbitrarily degree 
call type learning machine support vectors network technique support vector networks developed restricted case separating training data errors 
article extend approach support vector networks cover separation error training vectors name emphasize crucial idea expanding solution support vectors learning machines 
support vectors learning algorithm complexity construction depend dimensionality feature space number support vectors 
impossible 
extension consider support vector networks new class learning machine powerful universal neural networks 
section demonstrate generalizes high degree polynomial decision surfaces order high dimensional space dimension 
performance algorithm compared classical learning machines linear classifiers neighbors classifier neural networks 
section devoted major points derivation algorithm discussion properties 
details derivation relegated appendix 
optimal hyperplanes section review method optimal hyperplanes separation training data errors 
section introduce notion soft margins allow analytic treatment learning errors training set 
optimal hyperplane algorithm set labeled training patterns gamma said linearly separable exists vector scalar inequalities delta delta gamma gamma valid elements training set 
write inequalities form delta optimal hyperplane delta unique separates training data maximal margin determines direction jwj distance projections training vectors different classes maximal recall 
distance ae ae min fx delta jwj gamma max fx gamma delta jwj optimal hyperplane arguments maximize distance 
follows ae jw delta means optimal hyperplane unique minimizes delta constraints 
constructing optimal hyperplane quadratic programming problem 
note inequalities right hand side vector normalized 
vectors delta termed support vectors 
appendix show vector determines optimal hyperplane written linear combination training vectors ff ff 
ff support vectors see appendix expression represents compact form writing show find vector parameters ff ff ff solve quadratic programming problem gamma respect ff ff subject constraints dimensional unit vector dimensional vector labels symmetric theta matrix elements ij delta inequality describes nonnegative quadrant 
maximize quadratic form nonnegative quadrant subject constraints 
training data separated errors show appendix relationship maximum functional pair maximal margin ae ae large constant inequality valid accordingly assert hyperplanes separate training data margin ae training set separated hyperplane margin patterns classes arbitrary small resulting value functional turning arbitrary large 
maximizing functional constraints reaches maximum case constructed hyperplane maximal margin ae finds maximum exceeds large constant case separation training data margin larger impossible 
problem maximizing functional constraints solved efficiently scheme 
divide training data number portions reasonable small number training vectors portion 
start solving quadratic programming problem determined portion training data 
problem possible outcome portion data separated hyperplane case full set data separated optimal hyperplane separating portion training data 
vector maximizes functional case separation portion coordinates vector equal zero 
correspond non support training vectors portion 
new set training data containing support vectors portion training data vectors second portion satisfy constraint determined set new functional constructed maximized continuing process incrementally constructing solution vector covering portions training data finds impossible separate training set error constructs optimal separating hyperplane full data set note process value functional monotonically increasing training vectors considered optimization leading smaller smaller separation classes 
soft margin hyperplane consider case training data separated error 
case may want separate training set minimal number errors 
express formally introduce non negative variables 
minimize functional phi oe small oe subject constraints delta gamma sufficiently small oe functional describes number training errors minimizing finds minimal subset training errors data excluded training set separate remaining part training set errors 
separate remaining part training data construct optimal separating hyperplane 
idea expressed formally minimize functional cf oe subject constraints monotonic convex function constant 
sufficiently large sufficiently small oe vector constant minimize functional constraints determine hyperplane minimizes number errors training set separate rest elements maximal margin 
note problem constructing hyperplane minimizes number errors training set general np complete 
avoid npcompleteness problem consider case oe smallest value oe optimization problem unique solution 
case functional describes sufficiently large problem constructing separating hyperplane minimizes sum deviations training errors maximizes margin correctly classified vectors 
training data separated errors constructed hyperplane coincides optimal margin hyperplane 
contrast case oe exists efficient methods finding solution case oe 
call solution soft margin hyperplane 
appendix consider problem minimizing functional cf subject constraints monotonic convex function 
simplify formulas describe case section 
function optimization problem remains quadratic programming problem 
training error defined pattern inequality holds 
appendix show vector optimal hyperplane algorithm written linear combinations support vectors ff find vector ff ff solve dual quadratic programming problem maximizing ffi gamma ffi subject constraints ffi ffi elements optimization problem constructing optimal hyperplane ffi scalar describes coordinate wise inequalities 
note implies smallest admissible value ffi functional ffi ff max max ff ff find soft margin classifier find vector maximizes gamma ff max constraints 
problem differ problem constructing optimal margin classifier additional term ff max functional 
due term solution problem constructing soft margin classifier unique exists data set 
functional quadratic term ff max maximizing subject constraints belongs group called convex programming problems 
construct soft margin classifier solve convex programming problem dimensional space parameters solve quadratic programming problem dual space parameters ffi experiments construct soft margin hyperplanes solving dual quadratic programming problem 
method convolution dot product feature space algorithms described previous sections construct hyperplanes input space 
construct hyperplane feature space transform dimensional input vector dimensional feature vector choice dimensional vector function oe oe dimensional linear separator bias constructed set transformed vectors oe oe oe oe classification unknown vector done transforming vector separating space 
oe sign function delta oe properties soft margin classifier method vector written linear combination support vectors feature space 
means ff oe linearity dot product implies classification function unknown vector depends dot products oe delta ff oe delta oe idea constructing support vectors networks comes considering general forms dot product hilbert space oe delta oe hilbert schmidt theory symmetric function expanded form oe delta oe oe eigenvalues eigenfunctions oe du oe integral operator defined kernel 
sufficient condition ensure defines dot product feature space eigenvalues expansion positive 
guarantee coefficients positive necessary sufficient theorem condition satisfied du functions satisfy theorem dot products 
aizerman braverman consider convolution dot product feature space function form exp gamma ju gamma vj oe call potential functions 
convolution dot product feature space function satisfying condition particular construct polynomial classifier degree dimensional input space function delta different dot products construct different learning machines arbitrarily types decision surfaces :10.1.1.103.1189
decision surface machines form ff image support vector input space ff weight support vector feature space 
find vectors weights ff follow solution scheme original optimal margin classifier soft margin classifier 
difference matrix determined uses matrix ij general features support vector networks constructing decision rules support vector networks efficient construct support vector networks decision rule solve quadratic optimization problem gamma ffi simple constraints ffi matrix ij determined elements training set function determining convolution dot products 
solution optimization problem efficiently solving intermediate optimization problems determined training data currently constitute support vectors 
technique described section 
obtained optimal decision function unique optimization problem solved standard techniques 
support vector network universal machine changing function convolution dot product implement different networks 
section consider support vector networks machines polynomial decision surfaces 
specify polynomials different order functions convolution dot product delta radial basis function machines decision functions form sign ff exp jx gamma oe implemented convolutions type exp gamma ju gamma vj oe decision function unique expansion support vectors 
case support vector network machine construct centers approximating function weights ff incorporate knowledge problem hand constructing special convolution functions 
support vector networks general class learning machines changes set decision function simply changing form dot product 
support vector networks control generalization ability control generalization ability learning machines control different factors error rate training data capacity learning machine measured vc dimension 
exist bound probability errors test set form probability gamma inequality pr test error frequency training error confidence interval valid 
bound confidence interval depends vc dimension learning machine number elements training set value factors form trade smaller vc dimension set functions learning machine smaller confidence interval larger value error frequency 
general way resolving trade proposed principle structural risk minimization data set find solution minimizes sum 
particular case structural risk minimization principle occam razor principle keep fist term equal zero minimize second 
known vc dimension set linear indicator functions sign delta jxj fixed threshold equal dimensionality input space 
vc dimension subset sign delta jxj jwj cw set functions bounded norm weights dimensionality input space depend cw point view optimal margin classifier method execute principle 
keep term equal zero satisfying inequality minimizes second term minimizing functional deltaw 
minimization prevents fitting problem 
case training data separable may obtain better generalization ability minimizing confidence term expense errors training set 
soft margin classifier method done choosing appropriate values parameter support vector networks algorithm control trade complexity decision rule frequency error changing parameter general case exists solution zero error training set 
support vectors network control factors generalization ability learning machine 
examples dot product 
support patterns indicated double circles errors cross 
experimental analysis demonstrate support vector network method conduct types experiments 
construct artificial sets patterns plane experiment nd degree polynomial decision surfaces conduct experiments real life problem digit recognition 
experiments plane dot products form delta construct decision rules different sets patterns plane 
results experiments visualized provide nice illustrations power algorithm 
examples shown 
classes represented black white bullets 
indicate support patterns double circle errors cross 
solutions optimal sense nd degree polynomials exist errors 
notice numbers support patterns relative number training patterns small 
experiments digit recognition experiments constructing support vector networks different databases bit mapped digit recognition small large database 
small examples patterns labels postal service digit database 
postal service database contains training patterns test patterns 
resolution database theta pixels typical examples shown 
database report experimental research polynomials various degree 
large database consists training test patterns mixture nist training test sets 
resolution patterns theta yielding input dimensionality 
database constructed th degree polynomial classifier 
performance classifier compared types learning machines took part benchmark study 
experiments separators class constructed 
hyper surface dot product pre processing data 
classification unknown patterns done maximum output classifiers 
experiments postal service database postal service database recorded actual mail pieces results database reported researchers 
table list performance various classifiers collected publications experiments 
result human performance reported bromley sackinger 
result cart carried pregibon michael riley bell labs murray hill nj 
results best layer neural network optimal number hidden units obtained specially corinna cortes bernard respectively 
result special purpose neural network architecture layers lenet obtained lecun 
experiments postal service database pre processing centering de smoothing incorporate knowledge invariances national institute standards technology special database 
classifier raw error human performance decision tree cart decision tree best layer neural network special architecture layer network table performance various classifiers collected publications experiments 
see text 
degree raw support dimensionality polynomial error vectors feature space theta theta theta theta theta table results obtained dot products polynomials various degree 
number support vectors mean value classifier 
problem hand 
effect smoothing database pre processing support vector networks investigated :10.1.1.103.1189
experiments chose smoothing kernel gaussian standard deviation oe agreement :10.1.1.103.1189
experiments database constructed polynomial indicator functions dot products form 
input dimensionality order polynomial ranged 
table describes results experiments 
training data linearly separable 
notice number support vectors increase slowly 
degree polynomial support vectors rd degree polynomial degree polynomial 
dimensionality feature space degree polynomial times larger dimensionality feature space nd degree polynomial classifier 
note performance change increasing dimensionality space indicating fitting problems 
labeled examples errors training set nd degree polynomial support vector classifier 
relatively high number support vectors linear separator due non separability number includes support vectors training vectors non zero value 
training vector misclassified number mis classifications training set averages classifier linear case 
nd degree classifier total number mis classifications training set 
patterns shown 
remarkable experiments bound generalization ability holds consider number obtained support vectors expectation value number 
cases upper bound error probability single classifier exceed test data actual error exceed single classifier 
training time construction polynomial classifiers depend degree polynomial number support vectors 
worst case faster best performing neural network constructed specially task lenet 
performance neural network raw error 
polynomials degree higher outperform lenet 
experiments nist database nist database benchmark studies conducted just weeks 
limited time frame enabled construction type classifier chose th degree polynomial pre processing 
choice experience postal database 
table lists number support vectors classifiers gives performance classifier training test set 
notice polynomials degree free parameters commit errors training set 
average frequency training errors class 
misclassified test patterns classifier shown 
notice upper bound holds obtained number support vectors 
combined performance classifiers test set error 
result compared participating classifiers benchmark cl cl cl cl cl cl cl cl cl cl supp 
patt 
error train error test table results obtained th degree polynomial classifier th nist database 
size training set size test set patterns 
misclassified test patterns labels classifier 
patterns label false negative 
patterns labels false positive 
linear classifier lenet lenet test error nearest neighbor results benchmark study 
study 
classifiers include linear classifier nearest neighbor classifier prototypes neural networks specially constructed digit recognition lenet lenet 
authors contributed results support vector networks 
result benchmark 
conclude section citing describing results benchmark quite long time lenet considered state art 
series experiments architecture combined analysis characteristics recognition error lenet crafted 
support vector network excellent accuracy remarkable high performance classifiers include knowledge geometry problem 
fact classifier image pixels encrypted fixed random permutation 
suggest improvement performance network expected construction functions dot product reflect priori information problem hand 
introduces support vector network new learning machine classification problems 
support vector network combines ideas solution technique optimal hyperplanes allows expansion solution vector support vectors idea convolution dot product extends solution surfaces linear non linear notion soft margins allow errors training set 
algorithm tested compared performance classical algorithms 
despite simplicity design decision surface new algorithm exhibits fine performance comparison study 
characteristics capacity control easiness changing implemented decision surface render support vector network extremely powerful universal learning machine 
constructing separating hyperplanes appendix derive method constructing optimal hyperplanes soft margin hyperplanes 
optimal hyperplane algorithm shown section construct optimal hyperplane delta separate set training data minimize functional phi delta subject constraints delta standard optimization technique 
construct lagrangian delta gamma ff delta gamma ff ff vector non negative lagrange multipliers corresponding constraints 
known solution optimization problem determined saddle point lagrangian dimensional space minimum taken respect parameters maximum taken respect lagrange multipliers point minimum respect obtains fi fi fi fi gamma ff fi fi fi fi ff ff equality derive ff expresses optimal hyperplane solution written linear combination training vectors 
note training vectors ff effective contribution sum 
substituting obtain ff gamma delta ff gamma ff ff delta vector notation rewritten gamma dimensional unit vector symmetric theta matrix elements ij delta find desired saddle point remains locate maximum constraints kuhn tucker theorem plays important part theory optimization 
theorem saddle point lagrange multiplier ff corresponding constraint connected equality ff delta gamma equality comes non zero values ff achieved cases delta gamma words ff cases inequality met equality 
call vectors delta support vectors 
note terminology equation states solution vector expanded support vectors 
observation kuhn tucker equation optimal solution relationship maximal value separation distance ae delta ff delta ff gamma ff substituting equality expression obtain ff gamma delta delta account expression section obtain ae ae margin optimal hyperplane 
soft margin hyperplane algorithm consider case describe general result monotonic convex function 
construct soft margin separating hyperplane maximize functional phi delta constraints delta gamma lagrange functional problem delta gamma ff delta gamma gamma non negative multipliers ff ff ff arise constraint multipliers enforce constraint 
find saddle point functional minimum respect variables maximum respect variables ff 
conditions minimum functional extremum point fi fi fi fi gamma ff fi fi fi fi ff fi fi fi fi kc gamma gamma ff gamma denote ffi ck gamma rewrite equation ffi gamma ff gamma equalities find ff ff ffi ff substituting expressions ffi lagrange functional obtain ffi ff gamma ff ff delta gamma ffi gamma kc gamma gamma find soft margin hyperplane solution maximize form functional constraints respect non negative variables ff vector notation rewritten ffi gamma ffi gamma kc gamma gamma defined 
find desired saddle point find maximum constraints ffi obtains vector satisfy conditions ffi conditions conclude maximize ffi ff max max ff ff substituting value ffi obtain gamma ff gamma max kc gamma gamma find soft margin hyperplane find maximum quadratic form constraints find maximum convex function constraints 
experiments reported solved quadratic programming problem 
case technique brings problem solving quadratic optimization problem minimize functional gamma constraints general solution case monotone convex function obtained technique 
soft margin hyperplane form ff ff ff solution dual convex programming problem maximize functional gamma ff max gamma ff max gamma cf gamma ff max constraints denote convex monotone functions inequality valid uf second term square brackets positive goes infinity ff max goes infinity 
consider hyperplane minimizes form delta subject constraints second term minimizes square value errors 
lead quadratic programming problem maximize functional gamma non negative quadrant subject constraint 
aizerman braverman 
theoretical foundations potential function method pattern recognition learning 
automation remote control 
anderson bahadur 
classification multivariate normal distributions different covariance matrices 
ann 
math 
stat 
boser guyon vapnik :10.1.1.103.1189
training algorithm optimal margin classifiers 
proceedings fifth annual workshop computational learning theory volume pages pittsburg 
acm 
bottou cortes denker drucker guyon jackel lecun sackinger simard vapnik miller 
comparison classifier methods case study handwritten digit recognition 
proceedings th international conference pattern recognition neural network 
bromley sackinger 
neural network nearest neighbor classifiers 
technical report tm 
courant hilbert 
methods mathematical physics 
interscience new york 
fisher 
multiple measurements taxonomic problems 
ann 

lecun 
une procedure apprentissage pour 
la de intelligence artificielle des sciences de la des neurosciences pages paris 
lecun boser denker henderson howard hubbard jackel 
handwritten digit recognition back propagation network 
advances neural information processing systems volume pages 
morgan kaufman 
parker 
learning logic 
technical report tr center computational research economics management science massachusetts institute technology cambridge ma 
rosenblatt 
principles neurodynamics 
books new york 
rumelhart hinton williams 
learning internal representations backpropagating errors 
nature 
rumelhart hinton williams 
learning internal representations error propagation 
james mcclelland david rumelhart editors parallel distributed processing volume pages 
mit press 
vapnik 
estimation dependences empirical data addendum 
new york springer verlag 

