nanothreads vs fibers support fine grain parallelism windows nt platforms vasileios panagiotis polychronopoulos theodore high performance information systems laboratory department computer engineering informatics university patras rio patras edp tsp gr www gr 
support parallel programming essential efficient utilization modern multiprocessor systems 
focuses implementation multithreaded runtime libraries fine grain parallelization applications windows operating system 
implemented introduce runtime libraries 
standard windows user level fibers second nanothreads 
follow nanothreads programming model 
systematic evaluation comparing implementations conducted levels user level thread packages runtime libraries applications level 
results demonstrate nanothreads outperform windows fibers 
performance gains thread creation context switching mechanisms reflected runtime libraries 
experiments fine grain applications demonstrate higher speedup case nanothreads compared fibers 
years significant technological advances area workstations servers 
systems low cost multiprocessor configurations running conventional operating systems windows nt 
performance systems comparable expensive small scale unix multiprocessors software inadequate utilize existing hardware efficiently 
parallel processing systems primitive stage due lack appropriate tools efficient implementation parallel applications 
parallelization sequential application requires explicit knowledge underlying thread architecture 
furthermore user detect potential parallelism 
show existing support provided windows inadequate efficient implementation wide range parallel applications 

eds lncs pp 

springer verlag berlin heidelberg nanothreads vs fibers support fine grain parallelism hand high multiprocessors running unix operating system provide adequate convenient tools user order build parallel applications executed lowest possible overhead 
advanced tools consist automatic parallelizing compiler optimized multi threading runtime library appropriate operating system support 
directives inserted manually automatically compile time sequential code specify existing parallelism 
modified code analyzed directives interpreted appropriate runtime api functions 
final code compiled linked runtime library produce parallelized executable 
implementation multithreaded runtime libraries user level threads running windows 
libraries specially designed provide user necessary support efficient parallelization applications 
library called fibers runtime uses standard windows fibers second called nanothreads library nt uses nanothreads custom user level threads package 
runtime library ported windows operating system original implementation irix operating system 
libraries implemented nanothreads programming model 
libraries export api user providing functionality 
rest term runtime library refer specified 
compare implementations terms runtime overhead performance gains parallelization execution real applications 
rest organized follows section provides necessary background 
section runtime libraries introduced necessary details implementations 
performance study experimental results section 
section related summarize section 
background section outline multithreading support provided windows operating system kernel level user level threads nanothreads programming model 
windows multithreaded architecture windows operating system supports multiple kernel level threads powerful thread management api 
threads operating system smallest kernel level objects execution processes may consist threads 
thread create threads share address space system resources having independent execution stack thread specific data 
kernel level threads scheduled system wide basis kernel order executed processor 
vasileios threads windows allows programmers exploit benefits concurrency parallelism 
threads kernel level state resides operating system kernel responsible scheduling 
kernel level threads windows provides user level threads called fibers 
smallest user level objects execution 
fibers run context threads schedule unknown operating system 
thread schedule multiple fibers 
fiber state information associated associated thread 
state information maintained fiber stack subset registers fiber data provided creation 
saved registers set registers typically preserved function call 
fiber scheduled switching fiber 
operating system schedules threads run 
thread running fibers preempted currently running fiber preempted 
illustrates windows multithreaded architecture 
fibers kernel threads processes physical cpus user space kernel space fig 

threads architecture windows operating system fibers reside completely user space efficient utilization multiprocessor system requires multiple threads control active time 
presence processors means simultaneous execution corresponding number threads 
windows thread api provides extensive functionality threads overhead insufficient fine grain parallelization applications 
application uses hundreds ready execute threads reserves significant part process address space 
furthermore large number context switches occurs resulting excessive scheduling overhead 
hand fiber management occurs entirely user space consequently overhead significantly lower kernel level threads 
cost suspending kernel level thread order magnitude fiber switching performed user space 
similarly order magnitude difference cost creation fibers kernel level threads 
application programmer responsible management fibers allocating memory scheduling kernel threads preempting 
means user manage scheduling nanothreads vs fibers support fine grain parallelism fibers scheduling threads controlled entirely operating system 
fibers programming difficult 
obviously appropriate runtime system provides support programming fibers take advantage potential benefits 
nanothreads programming model nanothreads programming model npm exploits multiple levels loop functional parallelism 
integrated compilation execution environment consists parallelizing compiler multithreaded runtime library appropriate operating system support 
model applications decomposed fine grain tasks executed dynamic multiprogrammed environment 
parallelizing compiler analyzes source program produce intermediate representation called hierarchical task graph 
directed acyclic graph various hierarchy levels determined nesting loops correspond different levels exploitable parallelism 
nodes level hierarchy connected directed arcs represent data control dependencies 
input output dependencies 
arc nodes implies nodes executed sequentially 
node instantiated task uses user level thread execute associated code 
task ready executed dependencies may unresolved 
dependencies satisfied task ready execution 
runtime library controls creation tasks ensuring generated parallelism matches number processors allocated application operating system overhead runtime library low management parallelism affordable 
words runtime library implements dynamic program adaptability available resources adjusting granularity generated parallelism 
runtime library environment operates operating system distributes physical processors running applications 
operating system provides virtual processors applications kernel abstraction physical processors applications execute 
virtual processors provide user level contexts execution tasks user level threads 
main objective npm application scheduling user level threads virtual processors mapping user level virtual processor scheduling virtual physical processors mapping kernel level tightly coordinated order achieve high performance 
runtime libraries implementation section description runtime libraries implementation 
libraries primarily designed provide support parallel execution tasks backend parallelizing compiler 
compiler takes vasileios input sequential code code annotated special directives indicate presence code executed parallel multiple processors 
case frequently technique today adoption openmp standard 
compiler translates annotated code parallel code takes advantage runtime libraries 
compiler runtime libraries programmer directly development applications exported api 
rest section term user refer parallelizing compiler application programmer 
stated implemented multithreaded runtime libraries windows nt environment library uses custom user level threads named nanothreads library uses standard windows user level fibers 
custom user level threads package provides similar functionality non preemptive thread management provided fibers windows api 
additionally package enables passing multiple arguments thread function 
libraries designed provide user suitable interface exploitation application parallelism 
light weight user level threads support fine grain parallelism programming model allows exploitation multiple levels parallelism 
currently exported api implements nanothreads programming model described section 
programming models fork join required openmp standard implemented easily existing infrastructure 
runtime libraries export api user providing functionality 
api responsible task management handling ready task queues control dependencies tasks initialization environment 
implementation details described rest section 
detailed description exported api 
task management 
task fundamental object runtime libraries manage 
tasks blocks application code executed parallel 
responsibility runtime libraries instantiate userlevel threads 
task execute represented user supplied function take multiple arguments 
task management implementation different runtime libraries different user level thread packages instantiation tasks 
libraries task represented compact structure called task structure describes task execute input output dependencies virtual processor task runs pointer associated user level thread stack 
creation task involves creation execute task 
stack initialized information user function arguments 
order execute user function just switch task user level thread 
contrary create fiber task 
case information resides task structure pointer refers associ nanothreads vs fibers support fine grain parallelism ated fiber 
fiber created order execute helper function pass argument pointer task structure 
function pushes user function arguments stack calls user function 
switching associated fiber helper function executed having result user function executed 
way user directly execute function multiple arguments fiber bypassing fibers disadvantage argument user function 
exported api provides functions creation tasks user specifies function arguments task input output dependencies 
task creation procedure involves initialization task structure associated user level thread fiber 
allocate space task structure stack allocate space task structure call win api function create initialize new fiber 
optionally reuse allocated space distinguishing point libraries 
maintain global queue called reuse queue insert stacks executed tasks 
need create new task extract allocated task structure stack reinitialize new task information 
find executed queue reuse allocate space new task structure stack 
reuse task structures fibers due limitations fibers implementation 
space allocated fiber released termination execution 
queue management 
libraries maintain ready queues ready execution tasks inserted 
task ready executed input dependencies satisfied ancestors graph 
global queue processors equal access local queues owner processor access 
configuration flexible preserves affinity task scheduling optionally allow processor empty local queue steal processor local queue maintain load balancing better system utilization 
dependency management 
inside task structure keep information input output dependencies 
input dependencies represented counter structure output dependencies maintained keeping pointer successor task 
task finishes execution satisfies input dependency successors 
time task creates subtask additional care taken preserve input dependencies creator task 
reason increase creator task input dependencies declare successor subtask 
way maintain multiple levels nested tasks structure making runtime libraries capable exploit multiple levels parallelism 
vasileios runtime initialization 
runtime libraries environment initialized user calls libraries routines 
initialization routine provided instantiation environment library function application executes 
routine takes arguments user parameters maximum number processors application run thread scheduling policy routine initializes ready queue environment creates kernel level thread processor application run 
kernel level threads play role virtual processors execute application created tasks bound specific processors 
additionally library created kernel level threads initialize internal structures order support fibers call win api function 
task scheduling 
virtual processor enters task dispatching routine searches ready tasks execute creation execution task finished 
maintain affinity tasks task selected set satisfied successors previously ran task 
satisfied successors selected remainders inserted local ready queue 
satisfied successors virtual processor search task local ready queue global ready queue 
selecting task order maximizes exploited task affinity 
virtual processor find task method search processor local queue 
technique maximize load balancing executing processors 
experimental evaluation section reports systematic evaluation implementation runtime libraries 
addition performance native windows threads order show inefficiency compared user level threads 
specifically measurements conducted levels user level threads packages runtime libraries manually parallelized applications built runtime libraries 
subsection evaluation user level threads primitives cost 
subsection reports performance multithreaded runtime libraries 
subsection applications parallelized libraries measure performance delivered final user 
experiments conducted compaq processor mhz pentium pro system running windows advanced server equipped mb main memory 
runtime libraries developed microsoft visual compiler 
time measurements collected pentium pro processor time stamp counter register 
nanothreads vs fibers support fine grain parallelism evaluating user level threads section measure cost thread primitives windows fibers nanothreads implementation 
primitives include thread creation thread context switching 
additionally cost corresponding kernel level threads primitives 
experiments conducted processor setting process affinity mask processor 
simple microbenchmark measure creation time suspended kernel level threads fibers nanothreads 
illustrates results various numbers threads 
measured times clock cycles logarithmic scale 
time required creation number nanothreads half time needed create number fibers 
difference nanothreads fibers due smaller stack allocated nanothreads initialization independent nature nanothreads versus fibers depend creator kernel level thread 
experiment didn stack reuse mechanism nanothreads package 
expected creation time user level thread packages order magnitude windows kernel level threads 
due heavy nature kernel level threads require creation initialization user kernel context internal kernel structures 
second experiment evaluate cost context switching thread classes 
reason microbenchmark create number suspended kernel level threads fibers nanothreads execute empty function 
user level threads measure time takes execute threads peer peer scheduling 
case kernel level threads measurement includes time resume initially suspended threads finish execution 
fact benchmark execution time indicates cost context switching thread run system 
clock cycles thread creation win threads win fibers nanothreads number threads clock cycles context switching win threads win fibers nanothreads number threads fig 

threads primitives overhead comparison vasileios measurement includes overhead read new thread state memory means cache misses page faults occur 
case cost differs pure context switching cost measured threads switching times 
implies thread states reside cache memory 
experiment represents realistic measurement refers context switching cost occurs execution real applications 
illustrates measured time microbenchmark execution case 
measured time case order magnitude corresponding time fibers orders magnitude kernel level threads time 
results interesting show nanothreads context switching mechanism faster compared fibers 
user level thread packages preserve state context switching measured substantial difference costs 
difference due compact implementation nanothreads package uses stack save state registers fibers separate context structure 
additionally fibers preserve state exception handling 
large difference user level threads packages kernel level threads case full processor context preserved consecutive kernel level threads context switching 
experiments demonstrate nanothreads provide significantly creation context switching costs standard windows fibers 
low overhead suitable fine grain parallelization applications 
furthermore experiments show inefficiency kernel level thread fine grain parallel applications 
evaluating runtime libraries section evaluate runtime overhead libraries impose creation execution tasks 
fact want see advantages faster primitives nanothreads complex runtime library fibers 
measure runtime overhead built microbenchmark creates number tasks run empty function 
measure time create task structures finish 
time interval task include task structure allocation initialization time creation userlevel thread task insertion ready queue time task selected execution finish 
additionally order evaluate influence stack reuse mechanism runtime library measure time mechanism turned 
experiment conducted processors execution created tasks 
results case illustrated 
additionally time needed creation execution number tasks instantiated windows kernel level threads 
measured times show clearly creation execution tasks runtime library faster runtime library nanothreads vs fibers support fine grain parallelism clock cycles total runtime overhead win threads reuse number tasks fig 

runtime library overhead creation execution number tasks kernel level threads cases 
addition reuse mechanism library lowers total runtime overhead 
mechanism improves task creation procedure allowing new task created allocated stack influence important total execution time microbenchmark 
runtime overhead library order magnitude lower library orders magnitude kernel level threads 
results expected due lower costs nanothreads primitives measured previous section 
performance evaluation applications section investigate difference performance runtime library reflected execution parallel applications 
interested fine grain parallelism advantages coarse grain parallelism 
experiments select known applications order measure performance runtime libraries 
applications parallelized runtime libraries variable granularity 
parallelization hand currently source code parallelizing compiler available 
additionally implemented applications windows kernel level threads just illustrate fine grain parallelism 
applications cmm performs complex matrix multiplication decomposes dense matrix blocking raytrace renders dimensional scene 
cmm application previously evaluation nanothreads model applications come splash benchmark suite 
examine behavior cmm levels granularity matrices size variable chunk size outer loop 
vasileios speedups measured chunk size equal implementations 
number processors show bars correspond execution chuck size fine grain execution chunk size coarser grain 
coarser granularity case observe user level thread implementations scale better kernel level threads implementation 
particularly achieve higher speedups 
fine granularity case gains increased resulting higher speedups 
cases user level thread implementations achieve similar performance 
cmm application parallelized chunk size granularity coarse implementations parallelize outermost loop 
speedup cmm win threads chunk chunk chunk win threads chunk chunk chunk processors speedup block lu win threads win threads raytrace car speedup win threads processors processors fig 

performance applications variable granularity second application divides matrix blocks 
task represent computation required block different block sizes observe performance application different granularity 
execute matrix size block sizes express coarse grain fine grain case respectively 
speedups case illustrated form speedup measurements exhibit larger divergences cmm due finer granularity 
execution matrix nanothreads vs fibers support fine grain parallelism size block size thousands tasks created 
coarse grain case observe speedups runtime library implementations higher kernel level thread implementation 
fine grain case kernel level threads implementation scale processors 
contrary user level implementations scale implementation achieves better speedup implementation mainly due lower runtime overhead 
application raytrace differs applications due coarser granularity nature 
raytrace creates tasks associated task quite large terms execution time 
consequently task management overhead considered negligible compared total execution time 
execute raytrace car scene input 
speedups raytrace execution illustrated 
see implementations scale implementation achieves approximately better speedup implementations 
summarizing experiments clear exploitation finegrain parallelism minimize runtime overhead 
kernel level threads inefficient purpose due high overhead 
runtime libraries user level thread packages provide adequate support low overhead efficient exploitation fine grain parallelism 
furthermore shown application experiments runtime library suitable corresponding 
related windows thread api provides infrastructure high performance multithreaded programming runtime systems developed 
runtime systems concern platform windows nt 
visual kap kuck associates commercial optimizing preprocessor windows nt provides multiprocessing runtime library 
system structured high performance multithreaded programming 
library top support windows nt provides 
cases support multilevel parallelism nested parallel loops outermost parallelized 
furthermore windows kernel level threads experiments section inappropriate exploitation fine grain parallelism 
illinois intel multithreading library supports various types parallelism extends degree available support multithreading providing capability express nested loop dag 
support multiple levels general unstructured parallelism 
uses windows nt fibers user level threads defines threads advantages lower overhead creation context switching result support fine grain parallelism 
important difference system provides defined vasileios programming model necessary functionality multiprogramming support arbitrary forms parallelism 
unix platforms variety custom user level thread packages case windows nt platforms 
user level thread package 
information implementation technical details porting 
development continues extent supports ongoing projects 
development multithreaded runtime libraries nanothreads fibers respectively 
overhead runtime libraries low management fine grain parallelism affordable 
systematic comparison libraries showed nanothreads efficient lightweight user level threads fibers 
experiments applications application benchmarks indicate provides efficient parallelization better scalability 
main objective system effective integration fine grain parallelism exploitation multiprogramming 
concentrated implementation kernel interface provides lightweight communication path active user applications windows operating system 
interface support requests resources user level execution environment inform actual resource allocation availability 
implementation kernel interface relies shared memory communication mechanism kernel application vice versa 
mechanism scheduling policies major part series modifications performed windows kernel implemented operating systems 
progress implementing necessary mechanism integrating kernel interface multiprogramming support runtime system context windows 
acknowledge contribution research staff project upc university patras cnr research infrastructure led research results reported 
partially supported research program 

giordano 
hoppe navarro polychronopoulos nano threads programming model specification esprit project deliverable 
nanothreads vs fibers support fine grain parallelism 
polychronopoulos supporting fine grain parallelism windows nt platforms technical report high performance information systems laboratory university patras january 

giordano mango graphical parallelizing environment user compiler interaction proc 
international conference supercomputing rhodes greece june 

grey saito polychronopoulos intel illinois multi threading library multithreaded support ia multiprocessors systems intel technology journal 

keppel tools techniques building fast portable thread packages technical report uw cse university washington seattle june 

kuck associates visual kap openmp user manual version available www kai com 

microsoft microsoft developer network library available msdn microsoft com library 

mukherjee ghosh machine independent interface lightweight threads operating systems review acm special inter est group operating systems pages january 

polychronopoulos fine grain multiprogramming conscious solaris operating system proc 
international conference parallel distributed processing techniques applications pdpta csrea press vol 
iv pp 
las vegas nevada june july 

polychronopoulos achieving multiprogramming scalability parallel programs intel smp platforms linux kernel inproc parallel computing conference delft netherlands august 

openmp architecture review board openmp application program interface ver 
available www openmp org 

polychronopoulos nanothreads user level threads architecture technical report university illinois urbana champaign 

polychronopoulos scheduling user level threads distributed shared memory multiprocessors proc 
th europar conference europar toulouse france september 

advanced windows professional developer guide win api windows nt windows microsoft press 

mani chandy ishii system structured high performance multithreaded programming windows nt inproc ofthe nd usenix windows nt symposium pp 
seattle washing ton august 

woo singh gupta splash programs characterization methodological proc 
th international symposium computer architecture santa margherita ligure italy june 

