school informatics university edinburgh institute perception action behaviour learning state confusion perceptual aliasing grid world navigation paul crook hayes informatics research report edi inf rr school informatics august www informatics ed ac uk learning state confusion perceptual aliasing grid world navigation paul crook hayes informatics research report edi inf rr school informatics institute perception action behaviour august appears proceedings intelligent mobile robots due unavoidable fact robot sensors limited manner entirely possible find unable distinguish differing states world 
confounding states referred perceptual aliasing serious effects ability reinforcement learning algorithms learn stable policies 
simple grid world navigation problems demonstrate experimentally effects 
step backup reinforcement learning algorithms performed surprisingly better expected results confirm algorithms eligibility traces preferred 
keywords perceptual aliasing partially observability pomdp grid world navigation reinforcement learning copyright university edinburgh 
rights reserved authors university edinburgh retain right reproduce publish non commercial purposes 
permission granted report reproduced non commercial purposes long copyright notice reprinted full reproduction 
applications material addressed instance copyright permissions school informatics university edinburgh buccleuch place edinburgh eh lw scotland 
learning state confusion perceptual aliasing grid world navigation due unavoidable fact robot sensors limited manner entirely possible find unable distinguish differing states world 
confounding states referred perceptual aliasing serious effects ability reinforcement learning algorithms learn stable policies 
simple grid world navigation problems demonstrate experimentally effects 
step backup reinforcement learning algorithms performed surprisingly better expected results confirm algorithms eligibility traces preferred 

consider robot learning navigate way goal point building 
design robot equipped finite number sensors limited computational resources interpret sensory information 
due limitations chance multiple states world example long corridors building indistinguishable robot 
problem identified active vision systems whitehead ballard coined phrase perceptual aliasing 
current pace technological advance possible augment sensory information processing better range techniques robot deal situations 
addition perceptual aliasing controlled correctly form useful tool 
mapping external world agent internal state chosen correctly state distinctions irrelevant task ignored reducing state space explored 
perceptual aliasing especially problematic reinforcement learning algorithms 
reinforcement learning algorithms learn associate rewards ac paul crook hayes institute perception action behaviour school informatics university edinburgh forrest hill edinburgh 
eh ql uk dai ed ac uk tions specific states 
perceptual aliasing results confounding true state world difficult impossible algorithms learn stable policies 
systems contain perceptual aliasing examples partially observable environments 
looking partially observable environments shown reinforcement learning augmented memory ability create models world chrisman mccallum solve tasks contain perceptually aliased states 
long term goal test active perception provide effective alternative approaches 
stage wish better understand problems caused perceptual aliasing 
study fundamentals problem consider simulated agents fixed perception moving deterministic grid worlds sutton grid world 
depending sensory input allow agent faces similar problems encountered real mobile robot 
presents results applying various reinforcement learning algorithms commonly robotics grid world navigation problems 
problems rendered partially observable selecting observations agent state 
aim experiments test hypotheses step reinforcement learning algorithms able learn policies stable optimal task involves perceptually aliased states reinforcement algorithms eligibility traces learn optimal memoryless stable policies task 
confirm results observed singh partially observable environments reinforcement learning algorithms eligibility traces see section 
explanation term proceedings intelligent mobile robots th british conference mobile robotics uwe bristol preferable step backup 
furthermore running multiple trials gather useful insights performance algorithms tested 

background whitehead ballard considered reinforcement learning active perceptual systems specifically active vision systems 
simulate problems involved considered blocks world problem robotic arm uncover grasp green block randomly sorted piles blocks 
system select blocks focus attention characteristics blocks formed state world seen learning algorithm 
reinforcement learning algorithm learn coordinate objects focus attention actions robotic arm 
whitehead ballard step backup learning failed learn optimal policy performing slightly better selecting actions random 
failure concluded due inability learning reinforcement learning algorithm step backup learn stable policies presence perceptual aliasing perceptual aliasing case caused design active perception system 
littman considered learning state action policies memory partially observable environments environments containing perceptual aliasing 
introduced useful concepts satisficing optimal memoryless policies 
memoryless policy returns action solely current sensation 
standard reinforcement learning algorithms sarsa learning sutton barto exactly basis 
policy satisficing independent initial state agent policy guaranteed reach goal 
performance policy measured total steps agent takes reach goal possible initial states 
optimal policy achieves minimum total steps goal 
optimal memoryless policy policy achieves minimum number total steps achieved memoryless policy 
hill climbing branch bound techniques littman showed possible find optimal memoryless policies various grid world navigation problems including variation sutton grid world 
singh showed reinforcement learning eligibility traces find optimal memoryless policies grid world navigation problems 
proceedings intelligent mobile robots th british conference mobile robotics uwe bristol sutton grid world 
values indicate observations obtained agent fixed perception sampling surrounding squares adjacent squares agent 
arrows show example optimal memoryless policy singh 
filled black squares obstacles walls goal indicated asterisk 
agent perceptions adjacent squares agent fixed perception sampling surrounding squares 
effects perceptual aliasing perceptual aliasing causes distinct effects labelled whitehead chp local global impairment 
local impairment agent unable distinguish states world select actions inconsistent true underlying state 
example seen 
states labelled indicate observations obtained agent observe squares adjacent agent illustrated 
agent believes state distinct locations 
squares directly goal near middle obstacle left goal near middle obstacle left hand side board 
original problem sutton states optimal action move north state directly goal second case optimal action move south state just left goal third state matter agent moves north south 
seen agent unable distinguish locations decides best served moving north 
appropriate occurrences optimal state just left goal 
situations optimal stateaction policies learnt absence memory optimal memoryless policy arbitrarily worse optimal policy achieved absence perceptual aliasing singh 
global impairment bucket brigade update employed step backup reinforcement learning algorithms inaccurate estimates state values occur perceptually aliased states lead errors state values non perceptually aliased states 
problem best illustrated considering 
world deliberately designed agent observe squares adjacent distinguish states 
able uniquely identify remaining states 
states optimal action move right 
agent updating state values regard states separate states stores state value represent states updates averaged 
averaging results state having value greater true value 
agent state mistakenly select action move left state believing state nearer goal state 
updating state basis state value state propagates error potentially causing states state select action move left propagating inconsistent state values 
errors state values affecting agent policy 

experiments conduct experiments grid world navigation problems sutton grid world ii simple example devised whitehead pp illustrate problems perceptual aliasing causes step learning 
types agent agent state representation location grid world cartesian coordinates absolute position agent 
ii agent state representation formed observing squares adjacent current location adjacent squares agent see 
importance agents detail observe absolute position agent unique state representation location grid worlds adjacent squares agent state representation multiple states worlds 
sutton grid world sutton grid world shown 
consists grid containing various obstacles goal top right hand corner indicated asterisk 
agent world choose physical actions move north south east west 
state transitions deterministic action moves square appropriate direction 
agent tries move obstacle wall allowed move location state remain unaltered receives reward action succeeded 
agent receives reward action move directly goal state reward moving directly goal state 
agent reaches goal state relocated uniformly random start state 
adjacent squares agent multiple locations appear state state discussed section 
simple example state state state state state state state proceedings intelligent mobile robots th british conference mobile robotics uwe bristol goal state wall simple example world illustrate problems caused perceptual aliasing whitehead pp 
simple example world consists grid shown goal far right hand side 
agent world select physical actions move east move west 
state transitions world deterministic actions move agent square appropriate direction 
agent allowed move past far left hand far right hand world tries location state remain unaltered 
reaching goal state agent receives reward 
non goal states yield zero reward 
arrangement wall gaps play part actions agent allowed execute 
encode state seen adjacent squares agent 
agent state appears unique states labelled state state appear 
learning algorithms action selection selection learning action selection algorithms random action selection sarsa learning sarsa replacement traces watkins accumulating traces 
details learning algorithms see sutton barto respectively 
random action selection algorithm name suggest selects uniformly available actions provides baseline comparison methods 
learning algorithms continuously update policies 
actions selected greedily current policy probability 
cases actions value ties broken random 
remaining cases action executed select randomly available actions 
cases random selection uniform possibilities 
values learning algorithms learning rate discount rate probability random action started decayed linearly reaching zero th step 
remained zero 
range values tried eligibility trace decay rate 
state action values learning algorithms initiated zero 
evaluation adopted evaluation method singh 
learning steps policy evaluated greedily determine total number steps required reach goal possible starting state 
agent limited maximum steps reach goal starting state 
policy evaluated world starting states fails reach goal maximum total steps 
run consists action steps evaluation current policy occurring steps 
combination agent world learning algorithm value repeated times giving samples data point 
mean total steps reach goal mean total steps reach goal random action sarsa learning sarsa watkins action learning steps action learning steps plot sutton grid world mean total steps policies evaluated versus action learning steps 
bars indicate confidence intervals 
simplify plots data points shown steps 
top graph shows results position agent suffers perceptual aliasing 
insert shows enlargement steps visible data plotted step 
lower plot shows results adjacent squares agent aliases multiple locations 

results sutton grid world shows mean total steps learning algorithms 
top plot shows results absolute position agent experiences perceptual aliasing 
learning algorithms quickly converge optimal solution action learning steps 
indicates learning algorithms problem learning task proceedings intelligent mobile robots th british conference mobile robotics uwe bristol number policies class number policies class sarsa action learning steps sarsa non satisficing satisficing memoryless optimal action learning steps number policies class number policies class learning action learning steps watkins action learning steps plots show categorisation policies versus action learning steps learning algorithms 
learning solve sutton grid world adjacent squares agent 
ceptual aliasing 
lower plot shows results adjacent squares agent aliases multiple locations world 
mean total steps sarsa watkins rapidly approach optimal memoryless solution sarsa reaching convergence steps watkins action learning steps 
values tried shown converged similar number mean total steps 
expected lower values take longer converge lowest value converging action learning steps 
mean total steps policies learnt step backup algorithms learning sarsa appear gradually converging level majority policies satisficing 
convergence extremely slow compared sarsa watkins 
indicated confidence intervals significant variation policies learnt learning sarsa 
obtain better idea quality policies learnt identified policy cat egories tracked number policies fall category time see 
policy categories optimal better memoryless optimal memoryless optimal satisficing 
define categories specifically sutton grid world terms total steps measured policy evaluated 
optimal policy defined takes minimum possible total steps reach goal starting positions 
sutton grid world steps 
littman showed optimal memoryless solution sutton grid world steps 
littman definition satisficing policy reaches goal possible start states 
measure satisficing stricter requirement agent limited actions start state run truncated 
accordingly policy fails reach goal state start location steps classed non satisficing irrespective total steps policy 
remaining policies succeed reaching goal start states classified optimal total steps equals better memoryless optimal total steps lies exclusive memoryless optimal total steps equals satisficing total steps exceeds proceedings intelligent mobile robots th british conference mobile robotics uwe bristol number policies changing satisficing non satisficing visa versa number policies changing satisficing non satisficing visa versa sarsa action learning steps sarsa action learning steps number policies changing satisficing non satisficing visa versa number policies changing satisficing non satisficing visa versa learning action learning steps watkins action learning steps plots stability policy classification versus action learning steps learning algorithms 
height bars indicate number policies changed satisficing non satisficing visa versa previous policy evaluation 
learning solve sutton grid world adjacent squares agent 
goal reached total physical policy category start states actions optimal better memoryless optimal memoryless optimal satisficing non satisficing table policy categories sutton grid world 
categories summarised table 
shows variation types policies learnt number action learning steps executed 
combination parameters learning algorithm repeated times 
height shaded areas plots indicate number policies falling policy category measured particular number action learning steps 
examining top left hand plot shows policies learnt sarsa initially policies non satisficing grey shading 
action learning steps small number policies satisficing total steps exceeds classified satisficing policies black shading 
number policies classified satisficing gradually increases action learning steps policies satisficing non satisficing 
results learning top right similar final tallies satisficing policies non satisficing 
learnt policies better satisficing stage 
satisficing policies reasonable mean total steps sarsa learning 
comparison sarsa watkins action learning steps learnt small number policies classified memoryless optimal white shaded areas lower plots 
steps distribution policies sarsa memoryless optimal satisficing non satisficing 
similarly action learning steps distribution policies watkins memoryless optimal satisficing existed possibility learning algorithms proportion population policies continually categorised satisficing individual policies stable switching back forth satisficing non satisficing solutions 
possibility mind examined stability proceedings intelligent mobile robots th british conference mobile robotics uwe bristol policies learnt learning algorithms plots stability shown 
derived counting number individual policies change classification consecutive policy evaluations 
change classification counted policy changes non satisficing satisficing classifications visa versa 
sarsa learning stable policies changing classification time 
large changes classification seen initially sarsa watkins expect 
number changes fairly low level sarsa remains relatively high watkins 
related observation watkins generates large number satisficing policies sarsa 
sarsa learns memoryless optimal non satisficing polices virtually zero satisficing policies 
quick investigation reveals memoryless optimal policies generated watkins reasonably stable number changes memoryless optimal policies classification comparable figures sarsa 
large number polices average flip satisficing classification 
accounts changes reported watkins plot 
appears policy update rule watkins learns significant proportion unstable non memoryless optimal policies 
simple example results simple example world shown 
top plots show mean total steps confidence intervals learning algorithms random action selection 
left hand plot shows results absolute position agent experience perceptual aliasing 
absence state aliasing learning algorithms learn optimal solution world 
adjacent squares agent top right hand plot aliases state state simple world 
agent sarsa learning perform worse agent selecting actions random large confidence intervals suggest worth investigating occurring individual policies 
sarsa watkins learn optimal solution steps 
total steps optimal solution problem 
due simple nature world total steps optimal memoryless policy 
types policy identical reduce number categories just see table 
appears solu goal reached total physical policy category start states actions optimal satisficing non satisficing table policy categories simple example tion move east states policies evaluated greedily satisficing policies exist 
ties actions value broke random 
possible image policy states preference moving east moving west agent policy performs limited random walk ultimately reaching goal 
policy reached goal steps starting state satisficing exceed total steps 
practice occurred categories shown plots 
middle plots show number policies falling category sarsa qlearning 
see sarsa learning reach plateau just policies learnt classified optimal just action learning steps 
learning algorithms plot change classification policies test policies stable 
lower plots suggest optimal policies learnt stable 
plots categorisation policies learn sarsa watkins shown policies classified optimal steps little variation initial level remainder steps 

discussion results successfully replicates singh showing sarsa find optimal memoryless solutions tasks containing perceptual aliasing 
fact result generalises watkins suggesting method uses eligibility traces find optimal memoryless solutions 
surprise result sarsa learning learn satisficing policies sutton grid world optimal policies simple example 
remarkable whitehead example order illustrate extent perceptual aliasing interfere learning claimed step learning learn optimal policy task 
instances policies learnt appear stable 
examination issue indicates ex proceedings intelligent mobile robots th british conference mobile robotics uwe bristol mean total steps goal number policies class number policies changing satisficing non satisficing visa versa random action sarsa learning sarsa watkins action learning steps sarsa non satisficing optimal action learning steps sarsa action learning steps mean total steps goal number policies class number policies changing satisficing non satisficing visa versa action learning steps learning action learning steps learning action learning steps top plots mean total steps policies evaluated versus action learning steps 
simplify plots data points shown action learning steps 
left hand graph shows results position agent suffers perceptual aliasing 
right hand plot shows results adjacent squares agent aliases states 
middle plots show categorisation policies versus action learning steps sarsa learning 
bottom plots show stability policy classification versus action learning steps sarsa learning 
height bars indicate number policies changed optimal non satisficing visa versa previous policy evaluation 
plots simple example grid world exception top left adjacent squares agent 
proceedings intelligent mobile robots th british conference mobile robotics uwe bristol important determining reinforcement learning algorithms step backup sarsa learning learn policies stable satisficing partially observable environments 
experiments probability selecting exploratory action starts reaches zero steps 
remaining steps agent follows current policy trying exploratory actions 
lack exploration appears avoid destructive effects global impairment allowing policies achieve stable solutions 
effect exploration nicely illustrated shows categorisation policies learnt simple example world adjacent squares agent learning fixed 
fixed value policies stable continuous oscillations seen number optimal policies 
contrast plateau seen 
secondary point note number optimal policies ramps slowly decreases zero 
contrasts fixed initially lower value number optimal policies learnt increases rapidly 
observed oscillations reinforce whitehead argument learning step backup algorithm unable converge stable solutions partially observable environments provided possibility selecting exploratory action 
exploration ceased possible step backup algorithms converge satisficing policies 
number policies class learning non satisficing optimal action learning steps plot shows categorisation policies versus steps learning fixed simple example world adjacent squares agent 
initial hypotheses confirmed second reinforcement algorithms eligibility traces learn optimal memoryless policies question raised stability solutions policies learnt watkins appear flipping satisficing non satisficing see 
hypothesis step reinforcement learning algorithms able learn policies stable optimal task involves perceptually aliased states needs modified light discussion reflect importance selecting exploratory actions 
interesting result apparent policies learnt sarsa learning converge satisficing solutions slowly 
main aim experiments illuminate problems occur applying reinforcement learning partially observable environments 
interested doing order clear ground moving look active perception address issues 
results interest right reinforcement learning robotics real limited sensor arrays certainly create possibility perceptual aliasing states 
important observation exists possibility state aliasing needs designed task careful selection learning algorithm 
example probably worth avoiding reinforcement learning algorithms step backup 
fact reinforcement learning algorithm uses truncated returns subject detrimental effects global impairment whitehead 
demonstrated results reinforcement learning algorithms eligibility traces quickly learn reasonable solutions 

main focus test conjecture active perception allow reinforcement learning algorithms enhanced memory internal world models find optimal solutions navigation problems involve perceptual aliasing 
plan investigate initially equipping grid world agents form self directed perceptual system 
ultimately prove approach mobile robot navigating building robot input state formed images captured board camera pan tilt zoom robot learning algorithm direct control 
paul crook epsrc funding possible 
reviewers useful comments 
proceedings intelligent mobile robots th british conference mobile robotics uwe bristol chrisman 

reinforcement learning perceptual aliasing perceptual distinctions approach 
tenth national conference artificial intelligence pages 
aaai mit press 


adaptive agents reinforcement learning internal memory 
meyer 
eds animals animats proceedings sixth international conference simulation adaptive behavior sab pages 
mit press cambridge ma 
littman 

memoryless policies theoretical limitations practical results 
cliff eds animals animats proceedings third international conference simulation adaptive behavior sab pages 
mit press cambridge ma 
singh 

eligibility traces find best memoryless policy partially observable markov decision processes 
proceedings fifteenth international conference machine learning icml pages 
morgan kaufmann san francisco ca 
mccallum 

overcoming incomplete perception utile distinction memory 
proceedings tenth international conference machine learning icml pages 
morgan kaufmann san francisco ca 
singh jaakkola jordan 

learning state estimation partially observable markovian decision processes 
proceedings eleventh international conference machine learning icml pages 
morgan kaufmann san francisco ca 
sutton 

integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning icml pages 
morgan kaufmann san francisco ca 
sutton barto 

reinforcement learning 
mit press cambridge ma 
whitehead 

reinforcement learning adaptive control perception action 
phd thesis university rochester department computer science rochester new york 
whitehead ballard 

learning perceive act trial error 
machine learning 
proceedings intelligent mobile robots th british conference mobile robotics uwe bristol 
