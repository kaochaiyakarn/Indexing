reinforcement learning relational mdps martijn van department computer science university twente netherlands cs utwente nl new method reinforcement learning relational domains 
logical language employed states actions decreasing size state action space signi cantly 
probabilistic transition model abstracted process estimated speed learning 
theoretical experimental analysis new representation 
insights concerning problems opportunities logical representations reinforcement learning obtained context growing interest abstraction reinforcement learning contexts 
general agreement nowadays intelligent agents adaptive capable learning 
reinforcement learning rl sutton barto powerful paradigm behavior learning learning agent performs actions reach particular goal state gets rewards doing 
goal agent maximize reward intake 
example agent learn navigate maze quickly rewarding step reward minimize number steps needed 
starting grow early rl young research eld 
algorithms calculating value functions understood relatively days 
ort put scaling larger problems discovering task structure hierarchy temporal abstraction factored representations 
form abstraction obtained allowing powerful representation languages 
issues relational representations addressed decision theoretic planning model free rl currently large gap rl traditionally dominated propositional attribute value representations logical languages traditionally agent literature planning community see 
new method rl relational representation allows rl methods applied environments naturally represented terms objects relations 
model relationally factored markov decision processes environments representational device called carcass 
extend approach learned world models speed learning 
relational languages representation rl creates opportunities generates interesting new problems address 
structured follows 
section introduce basic notions section model discussed proposed abstraction carcass 
section highlights rl algorithm 
section discuss model extension section theoretical experimental analysis method di erent domains 
selection related discussed ends directions research 
preliminaries remainder terminology logical languages markov decision processes 
section brie introduce main notions 
details see sutton barto bergadano 
furthermore examples concerning blocks worlds standard intuitive environment blocks oor blocks moved move denoting block moved top goal reach certain con guration towers blocks 
valid relations block cl block clear moved 
logic rst order alphabet set predicate symbols arity set constants atom predicate symbol followed bracketed tuple terms term variable constant 
conjunction set possibly negated atoms 
set variables conjunction denoted vars 
substitution conjunction set assignments terms variables fx vars terms 
term atom conjunction called ground contains variables 
background knowledge bk set horn clauses conjunction head atom vars vars 
conjunctions horn clauses implicitly existentially quanti ed 
conjunction entails conjunction denoted resulting substitution exists proof bk axioms 
herbrand base hb set ground atoms constructed predicate symbols constants 
rst order interpretation subset herbrand base 
markov decision processes markov decision process mdp tuple hs ri set states set actions transition function reward function set actions applicable state denoted 
transition state caused action occurs probability reward received 
de nes proper probability distribution states actions 
deterministic policy speci es action executed agent state value functions mdp hs ri policy discount factor state value function represents value state policy expected rewards 
shown state 

similar state action value function de ned 
policy optimal optimal value functions denoted relationally factored mdps rmdp environments described terms objects relations objects 
rst consequence mdp described terms discrete states factored propositions need relationally factored mdp rmdp 
section de ned representational device useful reinforcement learning 
rst de ne de nition relationally factored mdp rmdp tuple hp ri nite domain objects nite set predicate templates nite set action templates 
state space de ned unique subset set rst order interpretations action set hb transition function reward function de ned section 
example rmdp clear fa fg ri blocks world blocks oor state blocks top fon clear remember interpretation set ground relations true state 
action moving block top stack oor move 
rewards action resulting usually implicit background theory restricts example blocks world block top transition goal state case reward 
size state space rmdp grow extremely large modest number relations 
example blocks world blocks jsj blocks jsj 
considering fact small generate huge state spaces type abstraction needed aggregate states actions similar 
de nition hp ri rmdp bk set horn clauses 
state conjunction heads bk 
action atom set ground states aggregated state fs sg 
example blocks world containing blocks state cl aggregates states unstack state blocks oor 
set states partitions state space state belongs state 
possible ensure syntactic way ground state proves unique state 
choose di erent solution 
de ne ordering states decision list fashion 
simple ordering set states induces partition equivalence classes states goal rl learning optimal policy 
abstractions states actions de ne value functions states actions de ne policies rmdp 
action rules constituents policies 
rst de ne action rule explain procedural semantics de nition action rule rule state action var var 
assume substitutions map variable unique domain object 
implementation enforced extra atoms bk 
semantics action rule agent state sk choosing action perform nondeterministically chosen action assume action chosen probability total number di erent substitutions 
set state action pairs aggregated transition rule de ned ak sk set ground actions applicable state relative transition rule de ned sk ak fa ak set actions similar abstraction level 
discern actions arbitrary action chosen set 
policy consists set action rules rules ordered fashion 
carcass abstraction section de ne formalization skeleton rmdp carcass compact abstraction relational conjunctions aggregation state action spaces 
carcass de nes relational abstraction terms pairs state set actions applicable state 
carcass restricts state action space learner considerably 
de nition hp ri rmdp 
carcass denoted structure sa sa set state set applicable actions ordering states bk set predicate de nitions set horn clauses 
sa var var 
set states denoted 
conjunctions heads bk 
gure see example carcass blocks world 
states block names states model exactly possible con gurations consisting blocks 
state number actions possible 
note second state easily replace actions move cl move cl cl move move move cl cl move move cl cl move move move cl cl move carcass blocks world 
linear ordering states bk substitutions 
simpli cations making state ordering 
supplying background knowledge de nitions state abstractions powerful 
example possible de ne predicate relating top tower denoted tower height carcass learn function state action pairs 
rst discuss learning representation give general algorithm 
algorithm main rl loop require environment initialized values initialized 
episodes initialize start state episode maximum number steps reached current ground state nd sk 
exploration exploration strategy chooses arg max take random action ak observe new state reward nd new state max 
consider carcass bk standard ordering gure current state fon cl cl cl state belongs assume value action move highest choose execute 
state belongs state rst state ordering model proof generates possible substitutions variables fa cg fa dg 
abstraction distinguish take random 
example take action move 
performing action arrive new state receive reward 
update value move learning update rule 
algorithm depicted algorithm 
learning function deduce policy max model rl states carcass partition states rmdp essentially de nes new discrete po mdp having state set fj kg 
means learn transition reward model new state space 
model ecient value function learning 
estimated probabilities associated rewards transitions states calculated counters updated action number transitions state state executing action number times agent executed action state sum rewards received agent executing action state making transition state quantities calculate approximate transition reward model underlying rmdp algorithm prioritized sweeping ps method updating values shown 
ps line algorithm replaced call algorithm 
qlearning updates values experienced trace ps updates values state action space 
transition model updates state propagated states action leading state 
priority queue pq orders states magnitude change states updated updates performed order 
algorithm prioritized sweeping wiering version wiering require visited state promote top pq umax pq remove top state pq predecessor states max promote priority pq keep values general decision problem longer markovian abstractions 
markov property holds output action depend previous actions visited states history depends current state state action time environment property hold said partially observable 
general state abstraction condition abstraction maintains markov property level dearden boutilier 
states action pr pr carcass abstracts actions need di erent condition 
illustration see example cl cl cl consider actions move move move 
fon notice things aggregate move move aggregates move move move shows actions dependent abstractions speci ground state 
shows actions stem di erent actions 
alters way look transitions opposed condition equation 
condition states actions carcass 
hp ri rmdp 
states 
markov property aggregated rmdp maintained ak ak choices 
condition ensures dynamic behavior markov say rewards 
rewards transitions averaged ground transitions aggregated 
experiments analysis section show results experiments 
important notions shown 
rst example carcass gure 
abstracts purely block names con gurations blocks described states 
need states con gurations number states drastically decreased 
omit cl atoms examples shorter notation ground states 
episode steps ground learning ps learning ground state space vs ps state space blocks world averaged runs 
representation just block names transitions consistent respect ground model condition holds 
gure model rl method compared standard learning ground state space 
goal reach stack state 
episodes value function policy learned ground learning experience high variance takes time learn value function stateaction pairs 
ect apparent number blocks increases 
note learned policy deterministic level non deterministic ground level 
gure see general carcass blocks worlds adopted kersting de raedt 
carcass applies blocks worlds arbitrary size shows general nature carcass 
abstraction appears useful learning move stack position blocks worlds arbitrary size 
di erent numbers stacks discriminated goal state reachable 
due abstraction level interesting things occur 
example consider rst state gure 
aggregates states towers height tower height 
action move moves top rst tower top second action move puts oor 
intuitively actions di erent things 
show due abstraction reinforcement learner exactly 
states states structure list tower heights move move move carcass blocks worlds arbitrary size 
states ordered number 
dashed blocks denote presence zero blocks 
example state accompanied available actions 
di erent types situations 
case 
actions move move decrease height rst tower tower left transition 
case 
case rst tower disappears towers height left actions result transition 
case 
actions result transition state basically con guration left rst tower height decreased 
conclude intuitively actions move move quite di erent transition probabilities exactly get value actions equally 
optimal policy choose 
similar analysis applies state delete state shows careful de ning abstractions simple carcass easily mistaken resulting learning problem 
example shows equation non markov problem outcome actions depend history actions 
consider blocks world fon fon fa move fa move doing actions state results fon fon fon fon fs equation see amounts 
experiments episodes ps resulted stack unstack policy move move move move move see kersting de raedt argued abstraction generates non markovian decision problem 
learning methods ps learning developed markovian problems 
step look cope kind problems 
eligibility traces rl sutton barto result better learning carcass easily extended 
general case solutions 
careful construction abstractions preferably automatically avoiding partial observability 
equation guideline similar state abstraction dearden boutilier 
de ning principled methods dealing partial observability means probabilistic belief states memory store past states 
performed experiments game tic tac toe 
size ground state space patterns board described compactly carcass 
background knowledge provide relations describe lines winning moves fork positions example empty board learner move corner center midline square 
depending speci carcass compact stateaction space contains approximately state action pairs values learning reasonably fast case playing reasonable stochastic opponent 
related interest grown richer powerful representation languages mdps 
roughly types methods exist transition function reward functions known form 
especially concerned 
mention 
model methods proposed past years 
boutilier rst proposing relational representation mdps boutilier situation calculus representation language 
yoon 
yoon introduced method upgrading policies small larger similar problems 
fern extended method introducing approximated policy iteration fern 
methods description logics representation language 
model free approaches assume knowledge transition reward function 
existing methods relational rl majority focuses learning relational value function approximation 
early dealt batch learning sampled states actions dzeroski induces relational regression trees online fashion driessens 
methods combine learning representation value function 
upgrades statistical learning methods originally developed propositional attribute value representations relational case 
examples nearest neighbor techniques driessens ramon kernel methods 
model free methods focus function approximation 
noticing underlying model functions rmdp state spaces de ned value functions learned 
rq learning introduced morales qlearning separated state action spaces 
van introduced state action spaces function learning learned transition model 
independently kersting de raedt kersting de raedt introduced logical mdps similar rst part 
new model relational mdps de ned new representational tool carcass rl relational domains 
carcass allows declarative speci cation stateaction space 
learned policies comprehensible general 
focusing relational abstraction mdp model relational abstraction function example driessens relate existing results mdp literature connect various existing methods solving mdps 
shown usefulness approach highlighted problems 
general deal partial observability principled manner 
criteria discussed section characterize general markovian abstraction level 
principle ground level lose advantages abstraction 
furthermore ordering states practical best method representing partitions 
replacing unordered possibly overlapping partition increase generality approach allow easier speci cation 
research explore practical theoretical consequences relational abstractions mdps 
includes convergence issues model extensions larger domains 
furthermore investigating methods learning state space representation 
long term goal closing gap rl programming languages logic agents 
bergadano 

inductive logic programming machine learning software engineering 
mit press 
boutilier reiter price 

symbolic dynamic programming rst order mdp 
bernhard nebel editor proc 
th international joint conference arti cial intelligence ijcai pages san francisco ca august 
morgan kaufmann publishers dearden boutilier 

abstraction approximate decision theoretic planning 
arti cial intelligence 
driessens ramon 

relational instance regression relational reinforcement learning 
proc 
twentieth international conference machine learning icml washington dc 
driessens ramon blockeel 

speeding relational reinforcement learning incremental rst order decision tree algorithm 
de raedt flach editors proc 
ecml european conference machine learning volume lnai pages 
springer verlag 
dzeroski de raedt blockeel 

relational reinforcement learning 
shavlik editor proc 
th international conference machine learning icml pages 
morgan kaufmann 
fern yoon givan 

approximate policy iteration policy language bias 
proc 
neural information processing conference nips 
driessens ramon 

graph kernels gaussian processes relational reinforcement learning 
proc 
international conference inductive logic programming ilp 
kersting de raedt 

logical markov decision programs 
proc 
ijcai workshop learning statistical models relational data 


learning optimal dialogue management rules reinforcement learning inductive logic programming 
proc 
north american chapter association computational linguistics naacl pittsburgh june 
morales 

scaling reinforcement learning relational representation 
proc 
workshop adaptability multi agent systems sydney 
van wiering dastani ch 
meyer 

characterization agents 
proc 
international conference integration knowledge intensive multi agent systems 
van 

relational representations reinforcement learning review open problems 
de jong oates editors proc 
icml workshop development representations 
van 

ecient reinforcement learning relational aggregation 
proc 
sixth european workshop reinforcement learning nancy france 
sutton barto 

reinforcement learning 
mit press cambridge 
wiering 

explorations ecient reinforcement learning 
ph thesis der wiskunde informatica en universiteit van amsterdam 
yoon fern givan 

inductive policy selection rst order mdps 
proc 
international conference uncertainty arti cial intelligence uai 
