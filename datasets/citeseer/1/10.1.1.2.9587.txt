kernel recursive squares algorithm engel interdisciplinary center neural computation hebrew university jerusalem israel alice nc huji ac il lab 
information decision systems massachusetts institute technology cambridge ma mit edu ron meir dept electrical engineering technion institute technology haifa israel ee technion ac il copyright authors 
report may copied distributed entirety academic purposes 
copies include cover page notice 
non academic requires permission authors 
non linear kernel version recursive squares rls algorithm 
kernel rls krls algorithm performs linear regression feature space induced mercer kernel recursively construct minimum error regressor 
sparsity solution achieved sequential sparsification process admits kernel representation new input sample feature space image sufficiently approximated combining images previously admitted samples 
sparsification procedure crucial operation krls allows operate line effectively regularizing solutions 
theoretical analysis sparsification method reveals close affinity kernel pca data dependent loss bound quantifying generalization performance krls algorithm 
demonstrate performance scaling properties krls compare stateof art support vector regression algorithm synthetic real data 
additionally test krls signal processing problems traditional squares methods commonplace time series prediction channel equalization 
keywords line learning kernel methods non linear regression sparse representations recursive squares signal processing table contents line sparsification sparsification procedure 
properties sparsification method 
line sparsification approximate pca 
comparison sparsification schemes 
kernel rls algorithm generalization bound experiments non linear regression 
time series prediction 
mackey glass time series 
santa fe laser time series 
channel equalization 
discussion list tables kernel rls algorithm 
right bound number operations time step line pseudo code 
time step computational cost bounded assume kernel evaluations require time 
results synthetic friedman data sets 
columns left right list average test set error standard deviation average percentage support dictionary vectors average cpu time seconds respective algorithm 
results real world comp activ boston data sets 
step step iterated prediction results mackey glass time series mg mg 
results non linear channel equalization experiment 
list figures scaling properties krls svmtorch respect sample size top noise magnitude bottom sinc linear function 
error bars mark standard deviation mean 
multi step iterated predictions mackey glass time series top bottom 
santa fe competition laser training series data set 
krls predicting steps dashed line laser time series 
true continuation shown solid line 
note steps prediction error hardly noticeable 
celebrated recursive squares rls algorithm popular practical algorithm extensively signal processing communications control 
algorithm efficient line method finding linear predictors minimizing mean squared error training data 
consider classic system identification setup assume access recorded sequence input output samples 
xt yt arising unknown source 
classic regression function approximation framework input output pairs xi yi assumed independent identically distributed iid samples distribution 
signal processing applications inputs typically consist lagged values outputs yi case autoregressive ar sources samples signal arma ma models respectively 
prediction problem attempts find best predictor yt yt zt xt 
context interested line applications predictor updated arrival new sample 
line algorithms useful learning scenarios input samples observed sequentially time data mining time series prediction reinforcement learning 
cases clear advantage algorithms need scratch new data arrive 
applications additional requirement real time operation meaning algorithm computational cost time step bounded constant independent time assumed new samples arrive roughly constant rate 
standard approaches prediction problem usually assume simple parametric form xt vector parameters fixed finite dimensional mapping 
classic squares approach attempts find value minimizes squared error yi xi rls algorithm recursively solve squares problem new sample xt yt number computations performed rls derive new minimum squares estimate independent making suitable real time applications 
kernel machines relatively new class learning algorithms utilizing mercer kernels produce non linear non parametric versions conventional supervised unsupervised learning algorithms 
basic idea kernel machines mercer kernel function applied pairs input vectors interpreted inner product high dimensional hilbert space feature space allowing inner products feature space computed making direct feature vectors 
idea commonly known kernel trick extensively years notably classification regression :10.1.1.11.2062
focusing regression kernel algorithms proposed prominently support vector regression svr gaussian process regression gpr 
kernel methods alternative parametric approach 
solutions attained methods non parametric nature typically form ik xi xi training data points 
representer theorem assures practical cases need look expression form 
number tunable parameters kernel solutions equals size training dataset introduce form regularization 
instance svr regularization attained called insensitive error tolerant cost function conjunction additional regularization penalty term encouraging flat solutions 
effect form regularization svr solutions typically sparse meaning variables vanish svr solution 
svr generally regularization networks sparsity achieved elimination 
means outset algorithms consider training samples potential contributing members expansion solving optimization problem eliminate samples coefficients vanish 
alternative approach obtain sparsity construction 
algorithm starts empty representation coefficients vanish gradually adds samples criterion 
constructive sparsification normally line case algorithm free choose training samples step construction process :10.1.1.126.8183
due intractability finding best subset samples algorithms usually resort employing various greedy selection strategies step sample selected maximizes amount increase decrease addition induces fitness error criterion 
nutshell major obstacles applying kernel methods line algorithms kernel methods require random multiple access training samples ii computational cost time space super linear size training set iii prediction query time scales linearly training set size 
clearly non methods described suitable line applications 
applications algorithm time step single training sample simple decision add sample representation discard 
proposed solution problem line constructive sparsification method sequentially admitting kernel representation samples approximately represented linear combinations previously admitted samples 
sparsification method construct line svr algorithm main contribution report krls kernel version rls algorithm capable efficiently recursively solving non linear leastsquares prediction problems particularly useful applications requiring comment approaches sparsification context kernel methods general kernel regression particular suggested 
compare method section 
line real time operation 
addition discuss detail line kernel sparsification procedure possesses merit context kernel learning methods signal processing 
line sparsification sparse solutions kernel algorithms desirable main reasons 
storing information pertaining entire history training instances sparsity allows solution stored memory compact form easily 
sparser solution kernel algorithm time memory consumed learning operation query phases kernel machine 
second sparsity related generalization ability considered desirable property learning algorithms see signal processing :10.1.1.11.2062
ability kernel machine correctly generalize learned experience new data shown improve number free variables decreases long training error increase means sparsification may regularization instrument 
classic svm framework sparsity achieved making error tolerant cost functions conjunction additional regularization term encouraging flat solutions penalizing squared norm weight vector 
sv classification shown expected number svs bounded perr number training samples perr expectation error probability test sample 
spite claims contrary shown theoretically empirically solutions provided svms maximally sparse 
stands reason sufficiently large training set learned additional training samples contain new information cause linear increase size solution 
take somewhat different approach sparsification observation dimension feature space usually high infinite effective dimensionality manifold spanned training feature vectors may significantly lower 
consequently solution optimization problem conforming conditions required representer theorem may expressed arbitrary accuracy set linearly independent feature vectors approximately span manifold 
mentioned kernel methods typically output predictor function training set 
implies time estimate form discussed fact linear predictor hilbert space xi naive approach admit training points 
xt expression form solve squares problem coefficients lead algorithm complexity grows data points added additionally result severe overfitting 
essentially problem points considered increases dimensionality space spanned 
xt increases 
note classic linear rls algorithm complexity update step depends dimensionality input samples 
linear rls dimension input samples change time dimension 
xt may increase time step depending data points specific kernel 
point xt satisfies xt ai xi estimate time subsequent time need non zero coefficient xt absorbed terms 
idea low dimensional data happens belong low dimensional subspace feature space 
kernels high dimensional infinite dimensional 
example gaussian kernel dim 
case xt xi feature vector xt linearly independent xi solution propose relax requirement xt exactly written sum xi consider approximate linear dependency 
new sample xt distinguish cases 
case sample approximately dependent past samples 
sample considered effect estimate sample feature vector approximately dependent past samples admitted dictionary rls point view approach line projection feature vectors encountered training low dimensional subspace spanned small subset training samples dictionary 
section describe line sparsification algorithm detail 
section proves discusses desirable theoretical properties algorithm section illuminates connection kernel pca 
section overview current sparsification methods compare method 
sparsification procedure line prediction setup assumes sequentially sample stream input output pairs xi yi assume time step having observed training samples xi collected dictionary consisting subset training samples dt xj mt mt construction xj linearly independent feature vectors 
new sample xt 
test xt approximately linearly dependent dictionary vectors 
add dictionary 
consequently training samples time approximated linear combinations vectors dt 
avoid adding training sample xt dictionary need find coefficients 
amt satisfying approximate linear dependence ald condition def min mt aj xj xt accuracy parameter determining level sparsity 
ald condition holds xt approximated squared error linear combination current dictionary members 
performing minimization simultaneously check condition satisfiable obtain optimal coefficient vector expanding note may written entirely terms inner products feature vectors min mt xi xj mt aj xj xt xt xt employ kernel trick replacing inner product feature space vectors kernel defined pairs vectors input space 
substitution obtaining min kt kt xt kt xi xj kt xt xi xt xt xt 
mt 
solving yields optimal ald condition kt xt kt xt respectively 
expand current dictionary augmenting xt dt dt xt mt mt 
expanded dictionary xt may exactly represented set zero 
consequently time step mi xi ai xj res res res denotes residual component vector 
choosing sufficiently small approximation error xi mi ai xj correspondingly small 
corresponding approximation terms kernel matrices kt kt xi xj 
full kernel matrix ai note due sequential nature algorithm mi 
practice freely substitution kt understanding resulting expressions approximate 
properties sparsification method study properties sparsification method 
show mild conditions data kernel function dictionary finite 
discuss question approximation really showing sensitivity parameter controls true kernel matrix approximated 
recall data points 
assumed belong input set theorem holds regardless dimensionality essentially says long compact set dictionary vectors finite 
theorem 
assume continuous mercer kernel ii compact subset banach space 
training sequence xi number dictionary vectors finite 
proof claim continuous 
sequence 
points zi zi zi zi zi zi writing terms kernels zi zi zi zi zi 
continuous zi continuous 
recall ideas functional analysis 
see precise definitions applications learning 
norm cover scale cover collection balls radius union contains 
covering number scale minimal number balls radius needed cover set 
packing number set maximal number points set separated distance larger 
compact continuous conclude compact implying finite cover exists 
recall covering number finite packing number finite implying maximal number separated points finite 
observe construction algorithm points xi xj dictionary obey xi xj separated 
packing number scale finite conclude number separated balls finite implying dictionary finite 
theorem implies initial period computational cost time step algorithm independent time depends dictionary size 
property framework practical line real time learning 
precise values size cover maximal size dictionary obtained making assumptions kernel 
proposition assumes bounds size dictionary function 
proposition 
assume lipschitz continuous mercer kernel ii compact subset exist constant depends kernel function training sequence xi number dictionary vectors satisfies proof compact contained bm bm ball radius 
covering number scale bounded 
kernel lipschitz continuous function implying exists positive constant 
suppose point admitted dictionary 
theorem know point admitted dictionary 
claim points admitted dictionary 

size dictionary exceed packing number scale 
packing number scale upper bounded covering number scale comment similar results obtained embedded bound covering number available 
furthermore tighter bounds obtained considering eigenvalues kernel function 
behavior size dictionary fundamentally different pursued 
establishing dictionary finite turn study effect approximation level approximation kernel matrix order simplify analysis consider line version algorithm finite data set size case dictionary constructed usual manner optimal expansion coefficient matrix computed entire data set 
essentially notation quantity depending incomplete dictionary redefined depend entire dictionary 
order remind change omit time index quantity 
instance denote dictionary size full kernel matrix respectively 
defining matrices 
xt 
xm res res 
res may write samples concisely res 
optimal expansion coefficients sample xi ai xi ai pre multiplying transpose get ka res res 
cross term res transpose vanish due orthogonality residuals subspace spanned dictionary vectors 
defining residual kernel matrix ka easily seen line line cases diag line case res res positive semidefinite 
consequence line case may bound norm justifying approximation 
recall matrix matrix norm defined ru 
proposition 
line case 
proof th eigenvalue recall linear algebra maxi 
diag ri tr positive semi definite maxi 
corollary similar bound placed covariance residuals res res maxi 
considering line case clear dictionary stops growing remaining samples algorithm behaves exactly line counterpart 
theorem know compact dictionary finite bound similar proposition holds finite time 
specifically line case random positive constant accounting size residuals point dictionary reaches final size 
corresponding bound residual covariance res line sparsification approximate pca line sparsification method described close connections kernel pca 
pca optimal unsupervised dimensionality reduction method mean squared error sense signal processing machine learning fields involving high dimensional data 
pca remove noise data 
applications noisy data projected principal directions subspace spanned eigenvectors data covariance matrix eigenvectors ordered non increasing eigenvalues implicit assumption variance remaining directions due noise 
shown solving eigenvalue problem kernel gram matrix kt essentially equivalent performing pca data feature space section show sparsification method essentially approximate form pca feature space 
describe optimal dimensionality reduction procedure 
covariance def matrix time ct ct positive eigenvalues 
corresponding orthonormal set eigenvectors 
forming orthogonal basis subspace contains 
xt 
defining projection operator ps span set 
known projecting training data subspace spanned eigenvectors entails mean squared error ps xi immediate consequence optimal dimensional projection respect mean squared error criterion consists eigenvectors 

similarly define projection operator span dictionary vectors def sparsification method 
pd denote projection operator 
ald condition place bound mean squared error due projection pd xi pd xi pd xi mt inequality due fact mt dictionary vectors error zero 
size dictionary known priori second inequality provides useful bound 
preceding paragraph showed mean squared error projection performed line sparsification procedure bounded 
show projection performed sparsification method essentially keeps important eigenvectors optimal pca projection 
theorem 
th normalized eigenvector empirical covariance matrix eigenvalue satisfies pd proof may expanded terms feature vectors corresponding data points 
known respective expansion coefficient vector eigenvector kernel matrix eigenvalue may write ui iui ui 
substituting pd pd pd pd ui 
recalling pd projection operator span dictionary pd xi ai pd pd ka pd ka ui inequality due proposition 
rui ui theorem establishes connection sparsification algorithm kernel pca implies eigenvectors eigenvalues significantly larger projected entirety span dictionary vectors quality approximation improves linearly ratio 
comparison kernel pca projects data solely span eigenvectors led regard sparsification method approximate form kernel pca caveat method diagonalize extract individual orthogonal features kernel pca 
computationally method significantly cheaper memory tm time exact kernel pca memory time number extracted components 
comparison sparsification schemes approaches sparsification kernel solutions proposed literature 
mentioned svms classification regression achieve sparsity utilizing error insensitive cost functions 
solution produced svm consists linear combination kernel evaluations training sample typically large fraction combination coefficients vanish 
approach major disadvantages 
training set samples learning svm algorithm maintain matrix size update full set coefficients 
means result turns sparse training algorithm able take full advantage sparsity terms efficiency 
consequence current state art svm algorithm scales super linearly 
second svms solution sparsity depends level noise training data effect especially pronounced case regression 
svm solutions known non maximally sparse 
due special form svm quadratic optimization problem constraints limit level sparsity attainable 
shortly svms machine learning community realized solutions provided svms classification regression may significantly sparser altering solutions weight vectors 
shown additional sparsity may attained allowing small changes svm solution little degradation generalization ability 
burges idea reduced set feature space vectors approximate weight vector original solution 
burges method reduced set feature vectors apart size virtually unconstrained algorithmic complexity finding reduced set solutions high posing major obstacle widespread method 
suggested restricting reduced set subset training samples help alleviate computational cost associated original reduced set method 
backed empirical results problems including handwritten digit recognition 
reduced set methods achieve sparsity elimination meaning best post processing stage kernel solution obtained main algorithm svm level sparsity deemed unsatisfactory user 
thorough account reduced set methods see chapter 
ultimately reduced set methods identification approximate linear dependencies feature space vectors subsequent elimination 
underlying principle class sparse greedy methods aim greedily constructing non redundant set feature vectors starting initially empty set full solution see chapter 
methods closely related kernel matching pursuit algorithm :10.1.1.126.8183
noted reason greedy strategies resorted due general hardness result regarding problem finding best subset samples sparse approximation framework positive results concerning convergence rates sparse greedy algorithms 
greedy methods represent opposite extreme reduced set methods axis sparsification methods 
orthogonal dimension sparsification methods may measured differentiating supervised unsupervised sparsification 
supervised sparsification geared optimizing supervised error criterion mean squared error regression tasks unsupervised sparsification attempts faithfully reproduce images input samples feature space 
examples supervised sparsification unique aims achieving sparsity bayesian approach prior favoring sparse solutions employed greedy sparsification method suggested noted support vector machines may cast probabilistic bayesian framework see 
specific gaussian process regression similar kernel matching pursuit :10.1.1.126.8183
examples unsupervised sparsification :10.1.1.32.8744
randomized greedy selection strategy reduce rank kernel matrix uses purely random strategy nystr method achieve goal 
incomplete cholesky factorization algorithm yield reduced rank approximation employing low rank approximations essentially equivalent low dimensional approximations feature vectors corresponding training samples 
principal component analysis pca known deliver optimal unsupervised dimensionality reduction mean squared reconstruction error criterion natural turn kernel pca sparsification device 
unsupervised methods mentioned closely related kernel pca 
sparse variant kernel pca proposed gaussian generative model 
general idea cases project entire feature space low dimensional manifold spanned eigenvectors sample covariance feature space corresponding leading non zero eigenvalues 
sparsification methods discussed constructive nature progressively building increasingly richer representations time applicable line setting 
setting input samples randomly accessible stream data sample may observed time 
imposes additional constraint sparsification method attempts represent entire training history representative sample 
point time algorithm decide add current sample representation discard 
problem line sparsification studied kernel methods community address sparsification algorithm 
method closely related sparsification method csat opper context learning gaussian processes 
csat opper method incrementally constructs dictionary input samples data projected projection performed feature space 
method criterion decide sample added dictionary distance new sample span previously stored dictionary samples method takes account estimate regressor classifier new point difference target value 
consequently dictionary constructed method depends function estimated sample noise method disregards completely 
defer discussion differences methods section 
kernel rls algorithm rls algorithm incrementally train linear regression model parameterized weight vector form feature vector associated input assume standard preprocessing step table kernel rls algorithm 
right bound number operations time step line pseudo code 
time step computational cost bounded assume kernel evaluations require time parameter initialize cost 

get new sample xt yt 
compute kt xt 
ald test kt xt kt xt add xt dictionary dt dt xt compute 
compute pt compute dictionary unchanged dt dt qt pt pt compute pt compute output dt performed order absorb bias term weight vector redefining see details 
simplest form rls algorithm minimize time step sum squared errors xi yi yt defined vector yt 
yt ordinarily minimize respect obtain wt yt yt pseudo inverse classic rls algorithm matrix inversion lemma allows minimize loss recursively line recomputing matrix step 
mentioned feature space may high dimensionality rendering handling manipulation matrices prohibitive 
fortunately easily verified may express optimal weight vector wt xi 
substituting slightly abusing notation kt yt 
theoretically minimizer computed recursively classic rls algorithm 
problem approach threefold 
large datasets simply maintaining memory estimating coefficient vector evaluating new points prove prohibitive terms space time 
second order model produced size vector dense general equal number training samples causing severe overfitting 
third cases eigenvalues matrix kt decay rapidly means inverting numerically unstable 
making sparsification method described preceding section overcome shortcomings 
basic idea smaller smaller set dictionary samples dt expansion weight vector wt entire training set 
wt ta reduced coefficients 
loss def vector yt kt yt minimizer kt yt 
line scenario time step faced cases 
xt ald dt 
case dt dt consequently mt mt kt kt 
xt ald dt 
xt added dictionary dt dt xt mt mt kt grows accordingly 
derive krls update equations cases 
case changes time steps ata yt yt 
note kt unchanged 
defining pt apply matrix inversion lemma obtain recursive formula pt pt pt pt ata pt pt simply add wt vector orthogonal xi 
substitute 
defining qt pt derive krls update rule pt pta yt pt pt yt qt kt yt kt xt equality qt kt xt 
case kt kt recursive formula easily derived kt kt kt xt kt xt kt xt 
note equals computed ald test need recompute kt xt 
furthermore 
xt exactly representable 
pt pt vector zeros appropriate length 
krls update rule yt yt yt yt kt xt yt kt xt final equality kt kt xt algorithm pseudo code form described table 
generalization bound section data dependent generalization bound generality applicable krls algorithm regression classification algorithms 
currently available bounds classification regression assume bounded loss function 
assumption acceptable classification clearly case regression 
generalization error bound unbounded loss functions established boundedness assumption replaced moment condition 
theorem slightly revised version theorem 
quote general theorem apply specific case squared loss studied 
function consider loss function viewed function context may take function 
xn yn set iid samples drawn distribution 
set ex yi xi 
furthermore lf class functions defined 
order establish useful generalization bounds introduce classic complexity measure class functions 
sequence independent identically distributed valued random variables prob 
empirical rademacher complexity defined rn sup xi rademacher complexity rn rn rn expectation taken respect marginal product distribution 
xn 
theorem 
class functions mapping domain xi yi xi yi independently selected probability measure assume exists positive real number positive log ex sup cosh 
probability samples length satisfies log rn lf note condition replaces standard uniform boundedness approaches 
order result useful need upper bound rademacher complexity rn lf 
assume initially 
quote result see eq 

lemma 
consider class functions fa 
rn fa xi 
context assume simplicity sequence inequalities notation indicate denoting expectation respect product distribution sample es keeping mind rn ese sup ese sup ese sup ab es ab es yi xi xi yi xi ab yi xi ab yi ab ab nab ne ab abe xi replaced ab yi xi fact xi jensen inequality derived bound rn formulated terms parameter order remove dependence standard trick union bound 
ai pi sets positive numbers lim sup ai pi example pi 
apply theorem value ai replacing pi 
simple utilization union bound algebraic manipulations described proof theorem allow establish bound parameter eliminated 
theorem 
class functions form fw xi yi xi yi independently selected probability measure assume holds 
fix number set max probability samples length fw satisfies fw yi fw xi log log log essential feature bound theorem holds uniformly particular weight vector obtained krls algorithm 
observe price paid eliminating parameter extra term order log log 
note dual representation xi bound phrased terms coefficients 
experiments section experimentally demonstrate potential utility efficacy krls algorithm range machine learning signal processing applications 
exploring scaling behavior krls simple non linear static regression problem 
move test krls known benchmark regression problems synthetic real 
point highly efficient svm package svmtorch compare algorithm 
move domain time series prediction tsp 
common approach tsp problem identifying generally non linear auto regressive model series 
approach essentially reduces tsp problem regression problem caveat samples longer assumed iid 
numerous learning architectures algorithms thrown problem mixed results see 
successful general purpose algorithms tested tsp svm svms inherently limited line batch mode training poor scaling properties 
argue krls appropriate tool domain support claim test krls known difficult time series prediction problems 
apply krls non linear channel equalization problem svms reported perform 
tests run mb mhz pentium linux workstation 
non linear regression report results experiments comparing krls algorithm coded state art svr implementation svmtorch 
best parameter values fold cross validation procedure looked see chapter definition 
minimum average root mean squared error rmse range parameter values spaced logarithmically 
kernel section gaussian kernel exp similar procedure find value kernel width parameter svmtorch performed best 
value svmtorch krls 
results reported averaged independent randomly generated training sets 
krls learning dimensional sinc linear function sin defined domain kernel width parameter 
svr parameters krls parameter learning performed random set samples corrupted additive iid zero mean gaussian noise 
testing performed independent random sample noise free points 
depicts results tests 
fixed noise level noise std varied number training samples training set drawn independently 
plotted test set error top left number support dictionary vectors percentage training set top center cpu time top right algorithm 
seen solution produced krls significantly improves svr solution terms generalization performance sparsity maximum dictionary samples training set sizes samples 
terms speed krls outperforms svmtorch entire range training set sizes orders magnitude 
fact looking asymptotic slopes cpu time graphs observe svmtorch exhibits super linear dependence sample size slope krls scales linearly slope required real time algorithm 
second test fixed training sample size varied level noise range 
note svmtorch suffers incorrect estimation noise level parameter respects 
notably presence high noise sparsity solution deteriorates drastically 
contrast krls produces sparse solution complete disregard level noise 
terms generalization krls solution svr solution 
tested algorithm additional synthetic data sets friedman due 
training test sets samples long introduced noise gaussian standard deviation 
data sets simple preprocessing step performed consisted scaling input variables unit hyper cube minimum maximum values 
results summarized table 
tested krls real world data sets boston housing comp activ delve task boston data set samples dimensions predict median value owner occupied homes various boston neighborhoods census data 
comp activ data set samples dimensions www cs toronto edu delve data datasets html test set errors svs training samples test set errors svs noise std percent svs torch krls time sec training samples percent svs torch krls time sec noise std cpu times training samples cpu times noise std scaling properties krls svmtorch respect sample size top noise magnitude bottom sinc linear function 
error bars mark standard deviation mean 
parameters friedman rmse std sv cpu svmtorch krls parameters friedman rmse std sv cpu svmtorch krls parameters friedman rmse std sv cpu svmtorch krls table results synthetic friedman data sets 
columns left right list average test set error standard deviation average percentage support dictionary vectors average cpu time seconds respective algorithm 
task predict users cpu utilization percentage multi processor multi user computer system measures system activity 
preprocessing step friedman data sets performed 
generalization performance checked boston comp activ left test samples 
results summarized table 
parameters comp activ rmse std sv cpu svmtorch krls parameters boston rmse std sv cpu svmtorch krls table results real world comp activ boston data sets 
time series prediction digital signal processing rich application domain classic rls algorithm applied extensively 
line real time constraints imposed signal processing algorithms making developed kernel machine learning algorithms irrelevant tasks 
development krls aimed filling gap tasks recursive line operation krls particularly useful essential 
important studied problem machine learning signal processing communities prediction time series 
tested krls synthetically generated real world time series studied mackey glass time series laser time series santa fe time series prediction competition 
mackey glass series synthetically generated numerical integration time delay differential equation 
laser time series taken real measurements intensity far infrared nh laser 
series exhibit chaotic behavior making task multi step prediction exponentially difficult function prediction horizon 
short term multi step prediction feasible depending intrinsic predictability dynamics 
mackey glass series possible accurate predictions time steps laser series useful limit 
time series experiments gaussian kernel 
mackey glass time series experiment mackey glass chaotic time series 
time series may generated numerical integration time delay differential equation proposed model white blood cell production dy dt ay 
dynamics chaotic conducted tests value corresponding weakly chaotic behavior difficult case 
eq 
numerically integrated euler method uniformly distributed initial conditions xt 
training sets samples long generated sampling series time unit intervals respective test sets consisting subsequent samples 
embedding dimension fixed embedding delay samples xt 

parameters algorithm selected searching minimum step rms iterated prediction error averaged independent training validation sets 
parameters mackey glass series svmtorch krls mackey glass series svmtorch parameters unchanged krls remained 
test results svmtorch krls series averaged independent trials table 
mackey glass series krls svm perform comparably terms prediction accuracy terms sparsity 
mackey glass series krls significantly outperforms svm prediction accuracy case remains clarified 
fig 
shows examples test sets series iterated predictions produced algorithms 
mg rmse std rmse std sv cpu svmtorch krls mg rmse std rmse std sv cpu svmtorch krls table step step iterated prediction results mackey glass time series mg mg mackey glass krls torch mackey glass krls torch multi step iterated predictions mackey glass time series top bottom 
santa fe laser time series experiment chaotic laser time series data set santa fe time series competition particularly difficult time series predict due www psych stanford edu time series santafe html chaotic dynamics fact intensity collapse events occur training set 
accurate prediction events crucial achieving low prediction error test set 
training data consists samples test data subsequent samples see fig 

task predict test series minimum mean squared error 
looking competition results difficulty task apparent achieved prediction accuracies significantly better simply predicting mean training series 
winning entry achieved normalized mean squared error nmse mean squared error divided series variance utilizing complex highly specialized neural network architecture adapted temporal version backpropagation algorithm 
final steps predicted sequence hand picked adjoining initial step prediction similar sequence taken training set 
second place entry approach local linear models achieved nmse test set 
santa fe competition laser training series data set attempted learn series krls algorithm 
naive approach mackey glass series minimize step prediction error defining training samples xi yi xi yi yi 
yi model order number training samples predicting test series performed iterated prediction successive step predictions fed back predictor inputs prediction subsequent series values 
unfortunately naive approach works series really interested step prediction 
goal provide multi step predictions sophisticated method called 
method follows run krls minimize step prediction mse naive approach 
pass training set complete compute optimal step estimates 

continue running krls new data set length original data xt yt 
yt 

assumed equal zero 
step estimates 
real data step estimates obtained previous step 
th step iterative process appended training set consists samples form xt 
yt 
yt xt 
may iterate process prespecified prediction horizon reached 
assuming dictionary ceases grow early stage result approximation minimizer mean squared error entire long dataset equal weight step step prediction accuracy 
idea somewhat complex scheme improve stability iterative multi step predictor respect small errors predicted values fed back predictor inputs predictions subsequent time steps 
note scheme relies heavily recursiveness krls implementation scheme batch algorithm svm considerably difficult costly 
free parameters algorithm tuned follows 
normalized series values lie range 
performed hold tests determine values ald threshold parameter model order prediction horizon iterations described 
training sets samples samples samples 
respective held sets 
parameters optimized respect mean squared error multi step iterated prediction held sets 
insignificant differences prediction accuracy observed parameter value incurring lower computational cost preferred preference high values low values 
values 
nmse prediction error competition test set samples achieved krls slightly better winning entry competition 
krls prediction true continuation shown fig 

channel equalization svms applied non linear channel equalization problems considerable success 
reservations authors concerning svms application domain due inability svms trained line 
suggest krls viable alternative performs similarly terms error rates may trained line produces solutions sparser svm solutions time especially large amounts data 
briefly state channel equalization problem 
binary signal fed generally non linear channel 
receiver channel signal corrupted additive iid usually gaussian noise observed 
yt 
aim channel equalization construct inverse filter reproduces possibly delay original signal low error rate possible 
order attain experiment matlab version krls entire training session took minutes 
implementation typically runs times faster 
true continuation krls prediction krls predicting steps dashed line laser time series 
true continuation shown solid line 
note steps prediction error hardly noticeable 
goal known random binary signal 
ut sent channel corresponding noisy observations 
yt adapt equalizer 
just time series prediction problem cast regression problem easy see channel equalization may reduced classification problem samples xi ui xi yi yi 
yi model order equalization time lag 
krls minimizes mse choice learning algorithm equalization questionable equalization aims reducing bit error rate ber number misclassifications 
show performance krls solutions measured ber criterion svm classification solutions 
experiment replicate setup simulation 
nonlinear channel model defined xt ut ut yt xt white gaussian noise variance 
svm third degree polynomial kernel parameters 
testing performed samples long random test sequence 
results table entry result averaging repeated independent tests 
results svm reported significantly better especially currently explanation discrepancy 
remarkable result experiment differences solutions provided respective algorithms 
krls uses dataset dictionary svm uses data support vectors 
krls outperforms svm 
table results non linear channel equalization experiment svm krls ber sparsity ber sparsity ber sparsity discussion non linear kernel version popular rls algorithm 
algorithm requires key ingredients expressing rls related operations terms inner products feature space calculated kernel function input space data dimension samples feature space remains bounded 
mentioned section approach closest probably csat opper introduced line algorithm sparse gaussian process gp regression 
gp regression posterior moments evaluated method described requires additional parameter estimated measurement noise variance needed method 
distinction sparse gp regression sparsification method apparent parameter lower value true noise variance 
case fixed dictionary sparse gp regression favor fitting dictionary points price increased error points 
limit gp regression fit dictionary points ignore completely points 
means compensate erroneous noise model sparse gp regression need increase size dictionary sacrificing sparsity observed behavior svr 
comparison sparsification method weights points equally maintains level sparsity irrespective level noise 
sparsification algorithm differs considers error performed estimating new sample target value yt sample added dictionary method considers error incurred approximating sample point xt 
essentially distinction supervised unsupervised sparsification 
unsupervised path able prove finiteness dictionary derive pca properties framework 
summarize main contributions 
computationally efficient line algorithm possessing performance guarantees similar rls algorithm 
essentially means information content sample fully extracted sample disposed second iteration previously learned training set cause change line gradient algorithms usually benefit data recycling 
due unsupervised nature sparsification mecha nism sparsity solution immune increase noise training set size 
formally proved relationship line sparsification approach kernel pca known optimality properties 
able data dependent generalization bounds dictionary obtained 
experiments indicate algorithm compares favorably state art svr algorithm svmtorch generalization performance computation time 
cases krls algorithm produces sparser solutions higher robustness noise 
usefulness krls demonstrated rls algorithm traditional application domain signal processing 
algorithm compares favorably current state art algorithms results important research direction employ line sparsification method conjunction kernel trick algorithms recursive nature kalman filter 
non linear kernelized version kalman filter may able circumvent inherent problem handling non linearities partially resolved extended kalman filter 
far krls algorithm concerned directions modification improvement open 
exploration connection krls maximum likelihood estimation order 
directions include utilization specialized kernels tailored specific problems time series prediction optimization kernel function tuning hyper parameters noise estimation spirit adaptive rls exponentially weighted version krls adaptive model order identification 
anthony bartlett 
neural network learning theoretical foundations 
cambridge university press 
burges 
simplified support vector decision rules 
international conference machine learning pages 
burges sch lkopf 
improving accuracy speed support vector machines 
advances neural information processing systems volume 
mit press 
bengio 
svmtorch support vector machines large scale regression problems 
journal machine learning research 
cristianini shawe taylor 
support vector machines 
cambridge university press cambridge england 
csat opper 
sparse representation gaussian process models 
advances neural information processing systems 
csat opper 
sparse line gaussian processes 
neural computation 
downs gates masters 
exact simplification support vector solutions 
journal machine learning research december 
engel meir 
sparse online greedy support vector regression 
th european conference machine learning 
pontil poggio 
regularization networks support vector machines 
advances computational mathematics 
fine 
efficient svm training low rank kernel representation 
jmlr special issue kernel methods pages 
friedman 
multivariate adaptive regression splines 
annals statistics 
gibbs mackay 
efficient implementation gaussian processes 
draft 
haykin 
adaptive filter theory 
prentice hall rd edition 
herbrich 
learning kernel classifiers 
mit press cambridge ma 
kailath hassibi 
linear estimation 
prentice hall 
ljung 
system identification theory user 
prentice hall new jersey second edition 
mackey glass 
oscillation chaos physiological control systems 
science 
mallat 
wavelet tour signal processing 
academic press new york 
meir zhang 
generalization bounds bayesian mixture algorithms 
technical report technion 
submitted publication 

ller smola tsch sch lkopf kohlmorgen vapnik 
support vector machines time series prediction 
gerstner 
nicoud editors proceedings icann international conference artificial neural networks volume lncs pages berlin 
springer 
natarajan 
sparse approximate solutions linear systems 
siam journal computing 
osuna girosi 
reducing run time complexity support vector machines 
international conference pattern recognition 
kailath 
state space approach adaptive rls filtering 
ieee signal processing magazine 

statistical signal processing 
addison wesley 
mika burges 
mller smola 
input space vs feature space kernel methods 
ieee transactions neural networks 
sch lkopf smola 
learning kernels 
mit press cambridge ma 
sch lkopf smola 
muller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 

support vector machine techniques nonlinear equalization 
ieee transactions signal processing 
smola bartlett 
sparse greedy gaussian process regression 
advances neural information processing systems pages 
mit press 
smola sch lkopf 
sparse greedy matrix approximation machine learning 
proc 
th international conference machine learning pages 
morgan kaufmann san francisco ca 

bayesian methods support vector machines evidence predictive class probabilities 
machine learning 
liu sung 
incremental learning support vector machines 
proceedings international joint conference artificial intelligence ijcai 
tipping 
sparse bayesian learning relevance vector machine 
journal machine learning research june 
tipping 
sparse kernel principal component analysis 
advances neural information processing systems pages 
mit press 
van der wellner 
weak convergence empirical processes 
springer verlag new york 
vapnik smola 
support vector method function approximation regression estimation signal processing 
neural information processing systems pages 
vapnik 
nature statistical learning theory 
springer verlag new york 
vincent bengio :10.1.1.126.8183
kernel matching pursuit 
machine learning 
wahba 
spline models observational data 
siam 
weigend gershenfeld eds 
time series prediction 
addison wesley 
williams 
prediction gaussian processes linear regression linear prediction 
jordan editor learning inference graphical models 
kluwer 
williams seeger 
nystr method speed kernel machines 
advances neural information processing systems pages 
zhang 
sequential greedy approximation certain convex optimization problems 
ieee transactions information theory 

