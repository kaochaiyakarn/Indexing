classes fast maximum entropy training joshua goodman microsoft research redmond washington usa microsoft com research microsoft com maximum entropy models considered promising avenues language modeling research 
unfortunately long training times maximum entropy research difficult 
novel speedup technique change form model classes 
speedup works creating maximum entropy models predicts class word second predicts word 
factoring model leads fewer nonzero indicator functions faster normalization achieving speedups factor best previous techniques 
results typically slightly lower perplexities 
trick speed training machine learning techniques neural networks applied problem large number outputs language modeling 

maximum entropy models promising techniques language model research 
techniques allow diverse sources information combined 
source information set constraints model determined algorithm generalized iterative scaling gis model satisfies constraints smooth possible 
training maximum entropy models extremely time consuming weeks months 
show word training time significantly reduced factor 
particular change form model 
predicting words directly predict class word belongs predict word conditioned class 
technique general applied problem large number outputs predict language modeling just example 
furthermore technique applies maximum entropy models machine learning technique predicting probabilities slowed large number outputs including uses decision trees neural networks 
give brief maximum entropy techniques language modeling 
go describe speedup 
describe previous research speeding maximum entropy training compare technique 
give experimental results showing increased speed training slight reduction perplexity resulting models 
conclude short discussion results applied machine learning techniques problems 
quick language models general maximum entropy language models particular 
language models assign probabilities word sequences wn 
typically done trigram approximation wn wi wi wi wi wi maximum entropy models language modeling necessarily gram approximation principle condition arbitrary length contexts 
general form conditional maximum entropy model follows exp 
wi wi real valued constants learned way optimize perplexity training data 
wi normalizing constant sum probabilities simply set equal exp wi represent large set indicator functions value 
instance fj tuesday wi wi meet implicitly 
positive value probability tuesday context meet raised 
making indicator functions type capture information captured trigram 
similarly bigram indicator function fj tuesday wi 
unigram indicator function simple fj tuesday wi wi 
principle set indicator functions depends wi including grams caching skipping grams word triggers optimal values learned 
algorithm generalized iterative scaling optimizing values set training data slow 
requires iterations iteration involves loop words training data 
give rough sketch algorithm inner loop detail 
inner loop code time consuming part lines 
notice inner loop contains loops words vocabulary lines 
notice iteration observed indicators fori training data word vocabulary unnormalized exp unnormalized word vocabulary foreach fj wi observed fj unnormalized indicator fj re estimate observed iteration sum line typically bounded number different types indicator functions 
particular system typically types indicators unigram bigram trigram typically types word history non zero indicator function 
means sum line loop line bounded number types indicator function 
inner loop lines typically bounded number types indicator function times vocabulary size 
means decreasing vocabulary size leads decrease runtime inner loop 
certain types indicator functions triggers optimizations line summing non zero fj change exact analysis run time intuition run time roughly proportional vocabulary size 

class speedup describe speedup 
assign word vocabulary unique class 
instance cat dog class animal tuesday wednesday class weekday 
observe wi class wi wi class equality holds word single class easily proven 
conceptually says decompose prediction word history prediction class history probability word history class 
true probabilities equality exact 
probabilities true instance results estimating model smoothed results computing maximum entropy model equality exact approximation 
approximation typically lower perplexity models 
decomposition basis technique 
create single maximum entropy model create different models predicts class word context class wi second predicts word class context wi class wi 
process training models completely separate 
classes system inner loop training code predicting class bounded factor factor vocabulary size 
model computed relatively quickly 
consider unigram bigram trigram indicators class model 
example unigram indicator fj tuesday wi class wi class wi weekday 
bigram indicator fj tuesday wi class wi wi weekday wi anda trigram indicator fj tuesday wi class wi wi weekday wi wi meet 
notice important fact word class wi wi class wi 
means modify loops lines loop words class wi 
class words run time inner loop bounded factor 
explicitly perform computations unigram words class wi set unnormalized probabilities leading contribution lines 
consider hypothetical example word vocabulary classes words class 
inner loop standard training algorithm require time proportional 
alternatively class speedup 
inner loop learning class model inner loop running word class model bounded factor leading hypothetical improvement 
extend result levels predicting super class noun class weekday word tuesday 
decomposition reduce maximum number indicator functions overhead level improvements extending levels 

previous research maximum entropy studied 
gives classic generalized iterative scaling algorithm form suitable joint probabilities opposed conditional probabilities somewhat dense classic maximum entropy models language modeling despite fact uses conditional probabilities discussion joint probabilities 
previously simple form classes maximum entropy language models 
conditioning variables indicator functions fj wi class wi 
predicting outputs lead speedups 
word classes course extensively language modeling including 
previous research focused improving perplexity reducing language model size knowledge increasing speed 
note previously model form similar reducing language model size factor perplexity 
noteworthy previous attempts speed maximum entropy models unigram caching improved iterative scaling iis cluster expansion 
unigram caching observation bigram trigram indicators practice string york francisco occurred training data bigram indicators case 
hand possible unigram indicators typically 
means typically vast majority indicator functions non zero context unigram indicators notice unigram indicators independent context meaning computation easily shared 
unigram caching effect unigram indicators pre computed computations inner loop rearranged depend non unigram indicators take non zero value 
practice number non zero indicators tends proportional vocabulary size number non zero bigrams trigrams similar indicator functions history bounded vocabulary size 
implemented unigram caching leads considerable speedups na implementation 
times speedup speedup unigram caching 
technique unigram caching extra overhead involved unigram caching technique drastically reduces number unigrams usually best technique unigram caching 
improved iterative scaling different update technique 
introduces additional overhead slows time iteration iterative scaling algorithm allows larger steps taken time leading fewer iterations faster performance 
introduces additional memory overhead coding complexity 
main benefits improved iterative scaling come certain models total number indicator functions true certain time highly variable 
learning speed generalized iterative scaling inversely proportional value max max wi gis uses maximum value slow learning iis slows learning case case basis 
models sum different different particular models caching triggering techniques lead different numbers active indicators 
models gram style models fixed maximum number non zero indicators 
models iis lead little reduction number iterations iterative scaling additional overhead iteration lead slowdown 
technique consider powerful cluster expansion introduced expanded 
cluster expansion regarded natural extension unigram caching grams 
consider simple trigram model 
straightforward rearrangement equations trigrams common bigram computation shared 
technique extended handle cases limited interaction hierarchical constraints achieves speedups factor 
concludes cluster expansion limited usefulness number interacting constraints large cluster expansion little computing exact maximum entropy solution believe applies 
particular simple model combining bigram back back back constraints probably show small gains techniques technique gains 
theory speedup conjunction iis unigram caching cluster expansion 
conjunction unigram caching experiments typically leads small speed improvements actual slowdowns unigram caching introduces overhead parts algorithm 
similarly suspect cluster expansion speedup limited 
tested algorithm iis principle reason combined guess combination 

results ran experiments different learning techniques simple gis gis unigram caching gis level clustering gis level clustering 
ran different sizes training data 
model skipping model types indicator functions variables filled specific instances indicator functions 
fj unigram wi fj class bigram wi class wi fj class skip bigram wi class wi fj bigram wi wi fj skip bigram wi wi fj class trigram wi class wi class wi fj class bigram skip bigram wi class wi wi fj bigram class skip bigram wi wi class wi indicator functions matching cases training data 
word classes top splitting algorithm attempted minimize entropy loss described 
different numbers classes different purposes 
twolevel splitting approximately classes 
level splitting approximately classes level classes second level 
cases optimized number classes running iteration training varying numbers classes picking fastest 
classes indicator functions typically classes factoring indicator classes classes 
linearly interpolated maximum entropy model trigram model smooth avoid zero probabilities 
technique interpolated trigram model reduced perplexity versus maximum entropy model technique interpolated trigram model able run baseline perplexity words version speedups slow 
subsets wall street journal data building classes scratch size common words training data words fewer unique words training data 
time relative baseline baseline gis unigram caching clustered clustered level training size speedup results shows results giving relative speeds 
notice achieve speedup factor unigram caching result 
believe largest speedup reported 
notice smallest data size methods result minor slowdowns compared unigram caching training data size increases speedup technique rapidly increases 

discussion discussed speedup technique context training 
cases testing 
particular test situation needs probabilities words particular context speedup helpful 
hand maximum entropy models rescore best lists speedup just testing training 
case say rescoring lattices speedup helpful long lattices allow words context 
notice speedup technique applied variety problems variety learning methods 
particular particular specific language modeling speedup technique predicting probabilities large number outputs possible words 
problem predicting probabilities large number outputs benefit methods 
similarly little specific maximum entropy models technique 
instance consider training neural network learn probabilities outputs 
step training require back propagating zeros 
alternatively place outputs classes 
network trained learn class probabilities 
step training require back propagating zeros 
learn neural networks predicting probability outputs class neural network class predicting probability output class 
network learn conditional probabilities outputs class class correct 
step training need train network corresponding correct class meaning zeros need back propagated 
presumably number hidden units smaller networks predicting values class outputs class smaller number hidden units network predicting outputs directly 
similarly ways train decision trees handle large numbers outputs train decision trees outputs leaf train binary decision tree possible output normalize 
cases method applied 
generally learning algorithm slowed training time large number outputs benefit approach 
similarly algorithm slowed test time large number outputs situation outputs needed benefit 
technique extremely promising 
approximation exact technique theoretically empirically reduces perplexity adds little complexity coding leads largest reported speedups factor speedups largest needed large complex problems applied independently form model applied learning algorithms problem domains 
hopeful maximum entropy modeling applied language modeling fields 

darroch ratcliff 
generalized iterative scaling log linear models 
annals mathematical statistics pp 

rosenfeld ronald 
adaptive statistical language modeling maximum entropy approach ph thesis carnegie mellon university april 
ney hermann essen ute kneser reinhard structuring probabilistic dependencies stochastic language modeling computer speech language vol 
pp 

brown peter peter desouza robert mercer vincent della pietra jennifer lai class gram models natural language computational linguistics december vol 

goodman joshua language model size reduction pruning clustering icslp beijing october 
della pietra stephen vincent della pietra john lafferty inducing features random fields cmu technical report cmu cs may lafferty suhm 
cluster expansions iterative scaling maximum entropy language models maximum entropy bayesian methods kluwer academic publishers 
khudanpur efficient training methods maximum entropy language modeling icslp beijing october 
goodman joshua putting language model combination icassp istanbul june 
