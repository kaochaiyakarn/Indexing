note semi supervised learning markov random fields wei li andrew mccallum mccallum cs umass edu computer science department university massachusetts amherst february describes conditional probability training markov random fields combinations labeled unlabeled data 
capture similarities instances learning appropriate distance metric data 
likelihood model training procedures 
obtaining labeled data expensive unlabeled data usually plentiful cheap 
models proposed semi supervised learning combination labeled unlabeled data train learner supervised task 
results models including mixtures multinomials trained em nigam svm transduction joachims show incorporating unlabeled data improves times hurts accuracy nigam 
characterizes situations unlabeled data hurts accuracy occurring natural clustering data correspondence class labels 
words poor performance results instances differing class labels near large distance spans containing instances correspond class boundaries 
contends having distance metric key requirement success semi supervised learning 
models distance metric implicit treated right distance metric task specific learned 
consider set inputs reasonable expect differing classification tasks performed set 
example set news articles classify topic style 
different distance metrics appropriate task 
previous mccallum minka addresses issue limited labeled data large quantities unlabeled data bootstrap learning distance metric multinomial naive bayes classifier 
classifier metric represented polya trees dirichlet trees prior multinomial mixture components 
polya trees distribution multinomials traditional dirichlet represent differing variance different dimensions emphasize dimensions playing 
model demonstrated promise discriminative conditionally trained models tend perform better classification generative models 
seek discriminative model learns distance metric integral part training 
outlines model conditionally trained semi supervised learning distance metric learning integral part training 
approach markov random fields tied parameters parameters impacting classification individual instances dependent features parameters encouraging instances related relevant features share class label 
purely supervised model classification consider traditional log linear conditionally trained known maximum entropy classifier case binary class label logistic regression 
class label finite set classes arbitrary input various feature functions return real values 
model defines conditional likelihood exp kfk zx zx exp kfk partition function 
parameters 
trained maximize penalized log likelihood set training labeled data dl xl yl 
notation xl labeled training set yl labeled training set 
penalized log likelihood log yl xl log yi xi second sum gaussian prior parameters handle sparsity training data 
separate default feature independent allows model represent class prior probability 
standard optimization techniques conjugate gradient limited memory bfgs byrd malouf find parameters maximize function 
gradient fk xi yi xi xi note model corresponds undirected graphical model known markov random field observed random variables xi connected undirected edge corresponding random yi cliques graph exactly pairs clique potential exp kfk xi yi 
semi supervised model classification consider case addition labeled data dl xl yl available unlabeled data du xl xl 
notation xu unlabeled set notation yu unknown labels xu 
furthermore xl xu yl yu 
wish unlabeled data improve parameter estimation 
classification model previous section unlabeled data impact 
see considering maximizing likelihood log yl xl xu log yl yu xl xu yu log yl xl yu xu log yl xl yu xu yu log yl xl noting xu appear gradient unlabeled data difference conditional likelihood training allow unlabeled training data xu impact labels yl 
accomplished introducing dependencies xu yl 
way way add new features weights corresponding cliques undirected graphical model assess aspects pairs examples xi yi xj yj pairs including labeled data unlabeled data hypothesized labels 
corresponding new complete data likelihood model yl yu xl xu exp kfk xi yi fk xi xj yi yj zx zx exp kfk xi partition function 
yu fk xi xj new cases reasonable simplify expression ignoring individual values yi yj determining classes equal 
yij iff yi yj complete data likelihood simpler model yl yu xl xu exp kfk xi yi fk xi xj yij zx parameter estimation new semi supervised model maximizes incomplete penalized log likelihood log yl xl xu log yl yu xl xu yu log exp zx yu kfk xi yi fk xi xj yij features fk parameters parameterize distance measure compare examples 
example document classification task feature cases iff document xi document xj con tain word airplane yij corresponding feature iff word appeared yij 
scheme parameters cor respond weights dimension input space course features fk capture arbitrarily complex features input including conjunctions conjunctions induced mccallum 
maximum likelihood parameter estimation model corresponds simultaneously learning classifier distance metric maximize penalized conditional likelihood labels labeled unlabeled inputs 
number labeled points small model severely overfit priors important 
relative importance features instance classify independently instances distance measure encourage nearby points agree class label controlled different values consider enforcing constraints class label proportions labeled data training blum mitchell 
situations may desirable include structure class label output space class hierarchy dag distance measures relations different pairs classes compatibility measured differently class specific distance metrics 
discuss scenario section 
note distinct means variances gaussian priors different features fk fk simple convenient way inject domain knowledge 
approximate classification exact inference model intractable sum necessary calculating zx includes exponential number apply approximate inference algorithms problem loopy belief propogation trp particular schedule loopy belief propogation better convergence properties 
graphical model densely connected loops algorithms may 
approach tries approximate joint classification graph mincut algorithm blum chawla 
method constructs graph data instances edges assigned weights similarities 
nodes partitioned disjoint subsets removing edges weights 
model graph partitioning algorithm approximate classification 
similarities instances want consider individual classification scores 
construct graph slightly different way 
nodes correspond data points xl xu class labels types weighted edges pair data node class node connected weight exp kfk score assigning class label edge weight pair data nodes xi xj measures probability share class label 
simplified case yij set weight exp fk xi xj fk xi xj 
graph partitioning algorithm similar standard agglomerative clustering algorithm iteratively merges closest clusters 
case find similar node cluster criteria add node cluster 
correlation clustering bansal method usually better practice mccallum wellner 
algorithm described initialize clusters ci contains class node yi 
add data nodes xl clusters contain corresponding class nodes 
xu empty find heaviest edge data node class node 
ci cluster maximizes average edge weight members ci ci algorithm puts data nodes clusters cluster ci members assigned class label yi 
see greedy algorithm sense membership data node change determined 
step choose cluster node independent classification scores similarities existing members 
ordering consider nodes affect clustering result 
shown step pick node highest classification score 
tried merging node cluster highest average edge weight works worse significantly 
parameter estimation learn parameters model need maximize incomplete penalized log likelihood log yl xl xu log yl yu xl xu 
note yu includes exponential number xu experiment various approximations avoid calculating sum 
labeled data learn parameters maximizing penalized log likelihood log yl xl log exp kfk xi yi yu fk xi xj yij apply standard optimization techniques limited memory bfgs need provide values objective function gradient fk xi yi xl fk xi fk xi xj yij xl fk xi xj ij involves calculating joint probability yl xl marginals yi xl yi yj xl 
efficient algorithm exact inference tried various approximations obtain values 
voted perceptron optimization requires gradient 
order avoid calculating marginals gradient approximate feature expectation xl fk xi single value xi arg maxy yp xl 
find approximate graph partitioning algorithm described treating training instances unlabeled data 
similarly xl fk xi xj ij replaced xi xj ij 
second solution calculate joint marginal probabilities approximate inference algorithms apply limited memory bfgs train parameters 
choose trp perform inference 
approach approximate yl xl simpler function form yl xl easy perform exact inference 
calculate accurate values new log likelihood want maximize gradient 
instance pseudo likelihood training approximates joint probability set random variables product conditional probabilities variable 
case define yl xl yl xl yn xl exp kfk xi yi fk xi xj yij indicates yi yl yn normalization factor 
explore strategy call local joint training construct approximating function 
difficult calculate joint probability large number instances problem small set variables 
pair nodes xi xj calculate probability yi yj xi xj normalizing potentials locally approximate global probability yl xl product yl xl yi yj xi xj exp fk xi yi fk xj yj xj fk xi xj yij better understand effect approximating joint probability product pair wise probabilities consider simple case variables 
assume independence similarly 
independence assumptions approximate joint follows general case approximation roughly raises joint probability power multiplies log likelihood constant 
values different parameters maximize similar assuming independences 
expect reasonable approximation 
independence assumption violated double count evidence just naive bayes classification 
pseudo likelihood training local joint training easily evaluate approximating log likelihood functions gradients allow learn parameters efficiently limited memory bfgs 
easy extend local joint training method incorporate unlabeled instances 
approximate yl xl xu product pair wise probabilities 
half labeled pairs pairs instance labeled contribute product 
calculate local probabilities yl xl xu yl xl xu 
consider triples unlabeled example yl yl xl xl xu yl yl xl xl xu experiment results experimental results document classification task newsgroup dataset 
preprocessing documents includes removing headers stopwords 
features fk term frequencies fk xi xj min fk xi fk xj shared term frequencies 
gaussian means priors set idf respectively 
idf inverse number documents term occurs 
means information retrieval area combined tf term frequency measure document similarities 
small set labeled instances impose relatively tight variances parameters reduce overfitting 
currently performance larger variances worse 
large data sets calculating affinity measure pairs instances prohibitively expensive 
canopies method mccallum efficiently pruning set pairs measured success related situations including pasula 
enables large number documents training classification 
method local joint training unlabeled instances similarity measure tf idf 
experiments keep approximately pairs examples 
method testing significantly hurts performance 
training models discussed previous section conduct experiments simplified features fk xi xj yij care classes equal 
easy extend model consider individual values yi yj additional parameters associated 
case different classes different distance metrics 
currently experimenting complicated model 
results reported graph partitioning algorithm test time classification 
experimented trp perform inference classification algorithm 
complete comparison various training methods binary problem pc vs mac table 
furthermore compare performances traditional models classify instance independently naive bayes nb maximum entropy support vector machines svm 
different amounts labeled instances shown column table perform trials 
labeled instances randomly sampled remaining instances data 
average classification accuracies standard deviations parentheses listed table 
see results different training methods labeled data 
voted perceptron vp trp pseudo likelihood pl didn improve independent classifiers local joint training lj signifi cantly better performance 
show impact learning distance metric conduct experiment idf learns fixes idf 
performance fixed distance metric metric learned training 
unlabeled data local joint training performance generally better statistically significantly 
test time classification model approximated graph partitioning 
evaluate loss performance due approximation compare clustering result produced graph partitioning algorithm correct clustering data node placed cluster class node 
criterion difference average inter intra cluster similarities 
shows correct clustering worse 
means graph partitioning algorithm misses perfect classification configuration algorithm bad approximation 
nb svm vp trp pl idf lj related table classification accuracies semi supervised learning important research topic machine learning community 
review various techniques field seeger 
focused exploring structure described unlabeled data change classification boundaries 
common assumption methods give classification close data points 
szummer jaakkola szummer jaakkola propose markov random walk representation unlabeled examples resisting rapid change class labels high density regions 
fixed affinity function density calculation 
chapelle chapelle introduce framework design cluster kernels distance points smaller cluster 
blum chawla blum chawla graph partitioning algorithm classification minimizing number close pairs assigned different class labels 
discuss ways define similar examples learn distance metric 
related zhu zhu 
differences approach clear model provides strong advantages 
uses gaussian random fields discrete random fields continuous state space providing easier infer ence efficient closed form solution configuration 
similar parameters model different weights associated different features measure similarities instances 
weights depend class labels 
think reasonable distance metrics 
imagine features meaningful particular class 
need consider hypothesized labels judge instances close 
difference approach zhu parameter estimation 
maximizing conditional probability labels instances try minimize average label entropy unlabeled data 
done efficient way 
zhu try maximize data likelihood 
basic model classification data manifold potentials data points potentials class prototypes 
extended incorporate external classifier produces labels unlabeled data 
relative importances kinds information determined arbitrarily 
hand capture data structure individual features model learn sets parameters simultaneously 
automatically adjust values appropriately balance 
presents conditionally trained markov random fields tied parameters affect classification individual instances features capture similarities pairs instances 
unlabeled data training classification 
graph partitioning algorithm approximate inference describe training procedures 
currently exploring directions improve performance 
example similarity features fk xi xj yij shared term frequencies instances 
experimenting options 
furthermore simplified features care classes equal 
extend include individual class values 
experiments show local joint training method best performance 
idea approximating joint probability product locally normalized probabilities applied larger pieces 
continue working approach try improve parameter estimation 
bansal bansal blum chawla 
correlation clustering 
blum chawla avrim blum chawla 
learning labeled unlabeled data graph 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
blum mitchell avrim blum tom mitchell 
combining labeled unlabeled data training 
colt proceedings workshop computational learning theory morgan kaufmann publishers 
byrd byrd nocedal schnabel 
representations quasi newton matrices limited memory methods 
mathematical programming 
chapelle chapelle weston 
cluster kernels semi supervised learning 
neural information processing systems 
joachims thorsten joachims 
transductive inference text classification support vector machines 
ivan bratko saso dzeroski editors proceedings icml th international conference machine learning pages bled sl 
morgan kaufmann publishers san francisco 
malouf robert malouf 
comparison algorithms maximum entropy parameter estimation 
mccallum minka andrew mccallum tom minka 
semi supervised learning distance metrics learned dirichlet trees unpublished 
mccallum wellner andrew mccallum ben wellner 
object consolidation graph partitioning conditionally trained distance metric 
mccallum andrew mccallum kamal nigam lyle ungar 
efficient clustering high dimensional data sets application matching 
knowledge discovery data mining pages 
mccallum andrew mccallum 
efficiently inducing features conditional random fields 
nineteenth conference uncertainty artificial intelligence uai 
nigam kamal nigam andrew mccallum sebastian thrun tom mitchell 
text classification labeled unlabeled documents em 
machine learning 
nigam kamal nigam 
unlabeled data improve text classification 
phd thesis pittsburgh 
pasula pasula russell 
identity uncertainty citation matching 
seeger seeger 
learning labeled unlabeled data 
szummer jaakkola martin szummer tommi jaakkola 
partially labeled classification markov random walks 
advances neural information processing systems volume 
zhu zhu john lafferty zoubin ghahramani 
learning labeled unlabeled data label propagation 
zhu zhu zoubin ghahramani john lafferty 
semisupervised learning gaussian fields harmonic functions 
proceedings icml 
zhu zhu john lafferty zoubin ghahramani 
semisupervised learning gaussian fields gaussian processes 

