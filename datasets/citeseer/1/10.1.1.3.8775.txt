robust subspace computation norm ke takeo kanade august cmu cs school computer science carnegie mellon university pittsburgh pa linear subspace important applications computer vision structure motion motion estimation layer extraction object recognition object tracking 
singular value decomposition svd algorithm standard technique compute subspace input data 
svd algorithm sensitive outliers uses norm metric handle missing data 
propose norm metric compute subspace 
show robust outliers handle missing data 
algorithms optimize norm metric weighted median algorithm quadratic programming algorithm 
views contained document authors interpreted representing official policies expressed implied carnegie mellon university government 
keywords subspace robust norm norm weighted median quadratic programming svd measurements observation data lie lower dimensional subspace original high dimensional data space 
subspace especially linear subspace important applications computer vision structure motion sfm motion estimation layer extraction object recognition object tracking :10.1.1.18.5663
compute subspace measurement matrix constructed factorized compute subspace 
construct data item reshaped column vector reshaped column vectors stacked form measurement matrix mk 
compute subspace need factorize measurement matrix dk dimension input data space number input data items dimension linear subspace 
columns matrix bases linear subspace want compute 
input data contain noises real cases 
depending distribution noises maximum likelihood estimation mle subspace equivalent minimize reconstruction error function 
example noise distribution modelled gaussian distribution mle equivalent minimize cost function uv matrix norm norm 
known norm sensitive outliers input data 
formulate subspace computation problem probabilistic estimation problem 
cost functions different assumptions noise model 
show cost function norm metric robust outliers computationally 
probabilistic view subspace computation shown principal subspace computed maximum likelihood estimation turn computed em 
similar way formulate subspace computation maximum likelihood estimation problem different noise model 
show maximizing likelihood equivalent minimization cost function 
format cost function determined distribution noise data 
general observed datum local measurement dimensional column vector contaminated additive noise unobservable fixed unknown true value corresponding observed measured additive noise 
know resides dimensional linear subspace pd uv projection subspace defined columns assuming local measurements independent log likelihood total measurements log mk log goal subspace computation measurement data find true values maximize likelihood measurements subject condition reside low dimensional subspace defined eq 

gaussian noise noise follows zero mean normal distribution common standard deviation 
assuming elements vector independent probabilistic distribution conditioned exp norm vector data log likelihood written positive constant 
maximizing data log likelihood equivalent minimizing term eq 
called cost function energy function substituting eq 
eq 
rewriting eq 
matrix format uv measurement matrix th column projection matrix th column projection value th data item subspace defined assumption identical independent distributed gaussian noise model transfers maximum likelihood problem max minimization problem norm cost function convex svd algorithm closed form solution compute global minimum 
laplacian noise assume noise follows laplacian distribution normal distribution mk exp norm vector maximum likelihood observed data minimizing norm cost function written matrix form uv measurement matrix th column 
norm cost function norm cost function general non convex general case general noise follows distribution model different model parameters different data points data likelihood mk exp ij ij ij distance function ij related parameter noise distribution 
maximum likelihood observed data minimizing weighted cost function ij ij ij notice data item weighed different component ij euclidean distance cx cost function simplified weighted sum ij ij ij ij ij written matrix format uv denotes component wise multiplication 
low rank approximation context cost function studied robust pca 
norm cost function weighted cost function general non convex due weight matrix summary maximum likelihood ml solution matrix factorization subspace computation depends noise distribution assumed 
noise follows independent identical gaussian distribution ml solution obtained minimizing norm cost function 
noise follows independent identical laplacian distribution ml solution achieved minimizing norm cost function 
general noise distributions longer identical ml solution comes minimizing non convex weighted cost function weights set problem dependent distance function 
cases norm weighted cost function deal outliers shown sections 
noise distributions generalized exponential family corresponding cost functions derived 
norm subspace computation gaussian distribution assumed noise model 
gaussian noise model problem estimating subspace equivalent minimize norm cost function dk dimension subspace defined singular value decomposition svd popular approach minimize 
theory svd explains svd minimize theorem 
svd matrix add dd dk diag orthonormal matrix 
min theorem states columns eq 
defines subspace minimizes norm cost function defined eq 
similarly svd gives closed form solution norm cost function eq 

problem norm cost function sensitive outliers 
single influential outliers resulted subspace completely different desired solution 
detecting outliers necessary 
parametric approaches deal outliers 
parametric approaches define global parametric model inliers follow 
outliers items follow parametric model 
specifically parametric approaches parametric model fit data outliers identified data violate fit model 
general scheme give data item weight range degree data item violates global parametric model 
zero weight indicates outlier 
robust estimator weight data item objective function eq 
rewritten min ij ij ij th element global parametric model subspace model th row th column contribution cost function data element controlled robust estimator distance data element current subspace model residual ij example geman mcclure robust function parameter controls convexity robust estimator 
robust estimator solve eq 
changes convexity cost function 
general minimums eq 
iterative procedures derive local minimum 
iteration data item weighted distance current parametric model new model recomputed weighted data 
dimension data high afford computing subspace model multiple times gradient decent compute local minima 
convergence iterative process depends model initialization 
reasonably initialization available parametric method highly effective takes global data account detecting outliers 
parametric approaches effective detecting structure outliers outliers influential initial model possible extreme outliers 
hand presence influential extreme outliers hard removal extreme outliers obtain initial model starting point iterative procedure 
norm subspace computation section discuss potential advantages norm metric subspace computation 
minimizing norm metric corresponds maximum likelihood estimation laplacian noise model 
show norm metric robust norm simple illustrative line fitting example 
algorithms compute subspace norm metric alternating weighted median algorithm alternating convex quadratic programming 
algorithms efficient weighted median fast algorithm convex quadratic programming see studied efficient software package available 
extensive experiments approaches subspace computation part 
robust norm metric example important advantage norm robust outliers norm statistical estimation 
seen simple example try find subspace data items 
words example fit line data points 
suppose dimensional points response variable corrupted gaussian noise 
want fit line kx points parameter slope need estimate 
words want compute dimensional subspace dimensional data 
specifically linear model kx parameter estimate noise corrupts response variable norm formulation assumed gaussian noise ml estimation parameter minimizing norm cost function sum squared difference kx squared solution minimize cost function norm formulation assumed laplacian distribution ml estimation parameter minimizing follow norm cost function kx data points norm norm fit line data points 
data points inliers 
result norm cost function similar norm 
global minimum eq 
obtained known result see result 
global minimum norm cost function kx weighted median weight th item corresponding th data point removed accumulation eq 
weight equal zero 
results fig shows results outlier data 
see norm norm cost functions give similar estimation outliers data results different 
fig outliers norm cost function gives results norm cost function gives erroneous estimation 
alternative minimization shown assuming laplacian noise distribution maximum likelihood estimation matrix factorization corresponds minimizing norm cost function norm metric robust outliers norm metric 
section algorithms maximize likelihood minimize norm cost function uv fit line data points 
data points outliers 
norm cost function gives erroneous result shown dash line norm cost function gives correct result shown solid line 
measurement matrix column observed measured data columns bases subspace estimated min 
eq global minimum computed weighted median cost function matrix factorization eq general non convex unknown 
requires iterative scheme achieve local minimum 
known weighted median compute global minimum eq 
fact suggests scheme minimizes cost function alternatively time optimizing argument keeping fixed 
alternative minimization scheme widely subspace computation norm distance metric bregman divergences 
alternative optimization written arg min uv arg min alternative minimization weighted median alternative minimization weighted median applied robust svd unpublished document 
algorithm contains mistakes correctly handle case rank measurement matrix rank 
correct algorithm handle case rank 
initialization set cycle columns times optimize th column columns fixed initialize randomly set ik kj convergence arg min uv arg min vu algorithm iterative weighted median minimize norm cost function compute subspace 
simplify presentation consider case dimension subspace 
alternating minimization problems arg min uv arg min vu th element column vector similar definition index iteration steps 
solutions global minimums eq 
obtained known weighted median algorithm result 
subspace dimension contain column 
algorithm cycles columns optimizing column fixing 
problem broken subproblems eq 

algorithm shown fig 
alternative minimization convex quadratic programming approach cycles principal vectors subspace bases optimizing principal vector fixing 
subsection convert subspace computation problem alternative convex optimization problem updates principal vectors time iteration 
alternative convex optimization potentially faster achieve better local minimum alternative weighted median approach 
show solve eq 

eq 
solved similarly 
cost function eq 
written th column th column problem eq 
decomposed sub problems optimizing sub problem general formula arg min ax problem reduced simple convex quadratic programming problem global minimum computed efficiently min ax column vector ones 
small positive constant 
convergence cost function decrease alternative minimization step 
cost function lower bound alternative minimization procedure converge 
carefully design algorithm converge local minimum 
investigating converge local minimum theory 
convergence achieved difference parameters adjacent iterations small 
specifically algorithm subspace base holds angle vectors th subspace base small positive number 
handling missing data missing data handled weighted median algorithm convex programming algorithm discarding constraints corresponding missing data 
see reason rewrite eq 
ij ij element located th row th column th row th column ij missing discard corresponding cumulative item ij 
weighted median algorithm discarding item affect result weighted median eq 
quadratic programming algorithm discarding item removes corresponding equation equation set eq 
long total number missing elements column measurement matrix equation set constrained quadratic programming solvable 
general original dimensions larger subspace dimension allows large number missing data column summary summary norm formulation subspace computation requires minimization uv decomposed alternative minimization problem 
alternating problem divided independent sub problems 
sub problem turn reduced simple convex quadratic problem global minimum computed efficiently 
notice global minimum sub problem derived convex quadratic programming original problem min uv general non convex 
example consider measurement matrix consists data points dimensional column space data points dimensional row space shown eq 
rank matrix means data points lie subspace 
suppose observe data points outlier measurements 
shown red italic elements eq data point contains outlier measurement 
apply algorithm alternative minimization weighted median compute subspace 
eq shows reconstructed matrix subspace fig 
shows reconstruction error 
see errors small outlier measurements successfully recovered 
apply svd algorithm norm compute dimensional subspace 
eq shows reconstructed matrix subspace fig 
shows reconstruction error 
svd algorithm sensitive outlier measurements see erroneous reconstructed matrix 
comparison purpose plot reconstruction errors coordinate frame shown fig 
weighted median algorithm norm achieves better results svd algorithm norm 
study problem robust subspace computation 
probabilistic view point subspace computation formulated maximum likelihood estimation problem turn leads low rank matrix approximation 
different noise models subspace computation formulated minimizing matrix reconstruction error respectively norm norm general weighted reconstruction error function 
norm norm reconstruction error element measurement matrix 
black weighted median norm metric red svd algorithm norm metric 
un weighted norm error function convex global minimum computed svd algorithm 
sensitive outliers 
norm error function general weighted error function robust outliers non convex 
alternative minimization algorithms minimize non convex function 
study alternative minimization algorithms minimize norm error metric weighted median quadratic programming 
weighted median algorithm robust simple compute subspace bases potentially easier trapped bad local minima 
quadratic programming compute subspace bases iteration step potentially efficient quadratic programming studied tuned algorithm 
alternative minimization requires initialization algorithm converge solution 
currently random initialization 
study initialize alternative algorithm 
testing algorithms real data part 
michael black allan jepson :10.1.1.18.5663
eigentracking robust matching tracking articulated objects view representation 
eccv pages 
collins dasgupta schapire 
generalization principal components analysis exponential family 
dietterich becker ghahramani editors advances neural information processing systems cambridge ma 
mit press 
csisz ar 
information geometry alternating minimization procedures 
statistics decisions supplement issue 
errors common coordinate frame zoom zoom reconstruction error 
dempster laird rubin 
maximal likelihood form incomplete data em algorithm 

gabriel zamir 
lower rank approximation matrices squares weights 
technometrics november 
golub van loan 
computation 
johns hopkins university press baltimore maryland nd edition 
douglas li liu stanley 
robust singular value decomposition www niss org tr pdf 
irani 
multi frame optical flow estimation subspace constraints 
iccv 
ke takeo kanade 
subspace approach layer extraction 
cvpr 
ke takeo kanade 
robust subspace approach layer extraction 
ieee workshop motion video computing motion 
mangasarian david 
robust linear support vector regression 
ieee transactions pattern analysis machine intelligence 
roweis 
em algorithms pca spca 
nips 
nathan tommi jaakkola 
weighted low rank approximations 
icml appear 
cormen leiserson andr rivest 
algorithms 
mit press mcgraw hill new york ny 
tipping bishop 
probabilistic principal component analysis 
technical report ncrg neural computing research group aston university 
tipping bishop 
mixtures probabilistic principal component analysers 
neural computation 
tomasi kanade 
shape motion image streams orthography factorization method 
ijcv 
torre black 
robust principal component analysis computer vision 
iccv 
turk pentland 
eigenfaces recognition 
journal cognitive neuro science 

