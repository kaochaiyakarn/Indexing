hmm passage models document classification ranking lip university paris case place jussieu paris cedex france 
lip fr patrick gallinari lip university paris case place jussieu paris cedex france 
patrick gallinari lip fr th march hugo zaragoza microsoft research street cambridge cb nh microsoft com application hidden markov models supervised document classification ranking 
consider family models take account fact relevant documents may contain irrelevant passages originality model explicitly segment documents considers possible segmentations final score 
model generalizes multinomial naive bayes derived general model different access tasks 
model evaluated reuters test collection compared multinomial naive bayes model 
shown robust respect training set size improve performance ranking classification specially classes training examples 
statistical sequence models variety settings field natural language processing part speech tagging language translation proposed models handling different information access ia tasks ranging document classification summarization information extraction 
models capture word dependencies perform inference sequences documents short passages 
allow extend classical paradigms information retrieval ir considering sequences text elements classical bag words representation 
opens way new models applications go traditional ir 
need extended ir relevance paradigms models arises partly due evolution document collections user needs 
increasing amount textual resources comes internet source lacks homogeneity length content formats style corpora traditional ir models developed 
example case web pages web directories faqs mails newsgroup discussions document sources heterogeneous documents loosely structured differently formatted ungrammatical documents may contain different sections relevant different topics irrelevant information headings signatures users hand wish complete complex tasks information new tasks filtering detection associations document summarization information extraction need addressed simultaneously document retrieval 
earlier introduced general probabilistic model ia sequence models showed potential handling variety tasks ranging document retrieval information extraction 
rd bcs european annual colloquium information retrieval hmm passage models document classification ranking proposed model defined levels relevance document term labels associated words document classes associated entire documents 
term label reflects relevance term regard specific topic document class defines category document belongs 
distinction permits build quite complex document classes 
suppose terms may labeled respect semantic classes example case information extraction tasks 
example mail containing conference announcement may judged relevant presents conference topic interested ii gives date location conference iii includes program 
may impossible learn combination classes separately may consider sub topics term labels information construct document class 
investigating benefits sequence models performing classical document classification ranking respect category 
tasks available knowledge set documents labeled relevant irrelevant particular information need 
documents labeled parts relevant documents necessarily relevant 
consider possible term labels relevant irrelevant category consider document may relevant irrelevant segments 
question estimate probability relevance terms category ii define probability relevance documents respect relevance terms 
implicit assumption relevance document depends solely relevance smaller units type information need relevance smaller units may depend wide range factors distribution context problem similar estimation weights probabilistic approaches 
problem ii open solution allow different priori knowledge nature documents task 
going term labels document classes perform form soft line segmentation 
soft alternative segmentations considered parallel weighted probabilistic manner 
line segmentation process bound computation retrieval function done posteriori preprocessing indexing model training 
related key concepts development sequence models text processing text segmentation retrieval regard passages probabilistic ir models 
review related different aspects 
sequence models mainly hmms proposed handling different tasks ir 
propose generative model hmms document retrieval highlighting 
generative methods estimate conditional distribution model represents model parameters estimated 
applied similar approach document modeled hmm ad hoc task large document collections trec trec 
field hmms named entity extraction experiments show simple ergodic models state associated topic reach surprisingly performances 
uses hmms extracting information form simple binary relations entities limited domain biology 
model carefully hand crafted task 
consider extraction document entities document title citations 
build discrete hmm extract information state represents particular label 
tool document indexing search engines 
recurrent neural networks formalism sequence modeling routing 
passages document parts retrieving documents advocated authors 
early subject shown sentences helps determine relevance documents proposes combine score document sections computing document score performs tests second trec collection trec 
introduces different types passages discusses role long documents retrieval performed tests tipster collection 
provides thorough discussion evaluation passage retrieval performed tests trec collection note works focus ad hoc retrieval rely adaptation classical ir document ranking techniques account text passages documents 
text segmentation considered perspective 
uses decision trees extraction keywords key phrases words text rd bcs european annual colloquium information retrieval hmm passage models document classification ranking framing extraction problem classification problem pre segmented text 
large amount dedicated text segmentation coherent passages 
models introduced considered extensions multinomial naive bayes classifiers machine learning community text classification 
closed query tasks document ranking routing extensively studied ir community 
classical benchmark reuters collection 
different machine learning techniques tested problems 
papers compare different techniques reuters collection 
terminology notation define document sequence words 
set documents considered 
assume unknown process generating documents 
consider sets documents interested indexes sets 
say document relevant belongs set 
deal set independent manner drop index consider set time 
refer relevant set complement irrelevant set 
mapping function map document simplified representation 
denote document representation 
different documents belonging different relevance sets mapped representation determine relevance document deterministic manner 
denote probability represents document belongs set 
consider documents preprocessed translated sequence terms terms mean normalized form words 
set terms lexicon 
represent document sequence term corresponding th word document 
consider bag words representation documents models ordering words document 
proposed model model proposed relies observation documents sequential structure exploited ir 
documents generic structure case example journal conference papers composed title documents composed distinct parts distribution relevant terms may different part weight parts differently position 
documents carry small parts relevant information middle irrelevant information 
priori information type easily taken account sequence models encoded deterministic stochastic grammars 
fig top successive parts document modeled different successive states different parts supposed generated successively different states 
training term statistics different states evaluated different parts documents 
possible know structure documents 
relevance computed summation authorized paths sequence model structure encodes priori knowledge sequential structure document 
simpler case illustrated fig bottom 
document supposed contain blocks relevant information separated passages non relevant information 
generative viewpoint user writes document mind different items interested writes different passages come back previous item 
time writes particular item mind switches specific vocabulary term distribution 
interested computing relevance regard particular item distribution terms relevant item modeled particular state denoted distribution items modeled common garbage state denoted 
note examples sophisticated models easily developed line needed priori information available 
build idea computing relevance document focusing relevant passages show different ways 
task consider types passages relevant rd bcs european annual colloquium information retrieval hmm passage models document classification ranking part part part part irrelevant relevant irrelevant sequence models information retrieval 
irrelevant 
consider binary classification case documents relevant irrelevant 
generative model consider documents belonging classes relevant irrelevant generated different generative models respectively 
new document probability belonging relevance set written modeling respectively may write alternatively usual log odds score written follows concentrate estimation generative model sequence terms document 
estimated accurately maximum likelihood dividing number relevant documents number irrelevant ones 
hidden markov models develop particular family generative sequence models order hidden markov models hmms 
hmms set states emit symbols different probabilities 
case state supposed emit set words model generates sequence words constitutes document 
recall document represented sequence terms term associated th word document 
note state generating rd bcs european annual colloquium information retrieval hmm passage models document classification ranking sequence states corresponding document 
hmm governed probabilities emission words lexicon states probabilities state transition initial state probabilities exit probabilities states generate words 
hmm generate documents different lengths 
probability particular observed sequence produced hmm call computed follows sum possible state sequences produce nonzero probability model 
simplify equations omit conditioning model 
simplest case state generate documents model seen multinomial model seen 
model irrelevant documents usually priori structure documents 
denote probability model stay state generate word exit denote exit probability 
hmm model relevant documents proportionality symbol indicates equality constant term independent document 
complexity computation cost classifying document order number states relevant model 
look special cases model 
irrelevant relevant irrelevant iri models constrain relevant model states call relevant irrelevant states 
furthermore constrain possible state sequences form indicates repetitions preceding symbol 
consider relevant documents contain single relevant segment arbitrary length similarly 
note word document labeled irrelevant model 
may strange effect practice simplifies equations 
model easily modified allow irrelevant passages size allowing transitions initial final states relevant state 
denote sequence states 
number states sequence 
denote rewrite equation assumption terms irrelevant sections relevant documents follow distribution terms irrelevant documents reasonable approximation greatly simplifies computations 
rewrite rd bcs european annual colloquium information retrieval hmm passage models document classification ranking note new expression depends ratio conditional probabilities terms relevant sections length sequence length relevant passage 
score takes account probability possible document segmentations probability close 
segmentation considers irrelevant passage relevant probability word passage lower quotient product close 
score segmentation contribute final score 
hand segmentations relevant labels fall truly relevant regions document account main part final score 
fixed sized window constrain model relevant sections fixed size documents previous equation simplifies case dependence constants disappears needs estimate emission probabilities model 
furthermore note computational complexity model reduced 
transition probabilities values related average length relevant irrelevant sections documents 
remaining consider terms equal 
general true general lead decrease performance generative model chosen accurate 
fact generative model chosen implicitly assumes exponential model document length terms appropriate model document length 
setting see previous models independent constants 
general iri model obtain fixed sized window model quantities depend sums ratios relevant irrelevant word probabilities easily computed training corpus 
parameter estimation word probability emissions estimated usual multinomial naive bayes model smoothed maximum likelihood estimator lexicon size 
set documents case relevant irrelevant documents 
note approximation model relevant documents assumed completely relevant 
fact em algorithm appropriately estimate emission probabilities computationally expensive proved reasonable approximation 
rd bcs european annual colloquium information retrieval relationship multinomial model hmm passage models document classification ranking multinomial model referred naive bayes gained attention machine learning due successful application wide range tasks 
model theoretically simple sound yields results general 
extended applied success new difficult ir problems multi class document classification learning unlabelled data 
briefly model show model generalization model 
consider document representation far term corresponding th word document 
consider furthermore mapping documents classes consider class generated single word probability distribution component resulting generative model 
consider furthermore document length evenly distributed classes probability relevant document written constant independent class 
probability relevance document see model similar model section models transition probabilities considered equal 
sense model extends naive bayes model sophisticated generative models relevant class 
note quite different multi class extension multinomial naive bayes model considered irrelevant documents generated mixture classes model continues operate bag word representation documents take account ordering words notion relevance passages 
results experimental setting reuters text categorization test collection evaluation purposes 
corpus heterogeneous reasons chosen 
classes large small ranging documents zero 
classes quite ambiguous depend basically presence absence key words 
sets classes partly overlap meaning documents belong classes form partitions larger classes classes disjoint rest 
certain classes deal specific topics containing documents technical language cover broader areas general terms expressions 
modapte split reuters test collection 
eliminated classes zero training test documents 
leads set classes documents set evaluations 
choosing adequate evaluation measure difficult 
documents belong classes reuters evaluate models break points independent classification decision 
break point defined point precision equals recall precision recall curve 
point linearly interpolated closest points necessary 
macro average break points obtained averaging break points class considered 
micro average break points obtained weighting average relative size class 
reuters corpus documents belong small set classes difference micro macro averages tells models performance degrades small classes 
rd bcs european annual colloquium information retrieval model earn acq money fx grain crude trade interest ship wheat corn micro average macro average hmm passage models document classification ranking table micro macro average break point break points largest classes 
see text notations 
report probability classification averaged documents micro averaged classes macro 
number correctly classified relevant documents highest scoring class attributed document 
document belongs classes highest scoring classes attributed document possible real data regard upper bound optimistic approximation reach perfect classification decision 
measure interesting models multinomial naive bayes model designed classify documents respect set classes ranking documents respect single class 
micro macro measures give insight performance systems task document classification 
large classes dominate micro average result small classes macro average 
evaluation evaluate fixed window iri models varying window size words baseline multinomial model 
refer models rn particular size window 
models data preprocessed manner words stemmed porter algorithm list words eliminate common empty words 
words appearing times training corpus eliminated 
feature selection carried models 
table presents break point micro macro averages break points classes illustration purposes 
note baseline multinomial model yields reasonably high micro average performances task low macro average 
best result reported task far know linear kernel svm models micro macro 
implies break points small classes poor 
results hmm models vary depending window size better baseline model 
model greater micro average higher macro average baseline model 
model performs similarly 
performance decreases window size increased micro macro averages 
see greater gain performance clearly macro averages indicating model outperforming baseline model specially small classes 
drawn 
firstly hmm robust multinomial model respect size training corpus 
basic problem small classes difficulty estimating term relevance 
usually resolved applying feature selection techniques rely heuristics difficult apply consistent manner 
performance hmms small classes indicates rd bcs european annual colloquium information retrieval micro macro multinomial hmm passage models document classification ranking table micro macro comparison feature selection critical models 
secondly small window sizes perform better larger ones 
may counterintuitive fact denotes relevant documents words fact probably irrelevant relevant 
document score possible segmentations small window sizes advantage model 
trivially small window sizes lead performances lower baseline model table 
looking micro macro results find hmm models perform consistently better baseline model 
model outperforms baseline model performance measure fact model highest micro macro higher baseline model respectively 
model obviously candidate document classification perform document ranking 
means model produces higher precision values rest certain recall values precision recall curves produces deteriorate quickly break point 
indicates model candidate low recall applications 
macro deteriorates fast window size increased micro average quite stable 
supports previous claim small classes require small window sizes 
new family generative models information retrieval probabilistic sequence models 
motivated models light increasing complexity textual data tasks showing extend classical notion document retrieval 
developed hmm implementation task document ranking classification detailed possible derivations stating explicitly assumptions 
showed proposed models generalizes classic multinomial naive bayes models account existence non relevant passages relevant documents 
evaluated models reuters data set compared baseline multinomial naive bayes model 
show systematic improvement performances baseline model 
furthermore performance measures gain insights weaknesses strengths proposed models 
amini zaragoza gallinari learning sequence extraction tasks 
content multimedia information access riao beeferman berger lafferty statistical models text segmentation 
machine learning bikel schwartz weischedel algorithm learns name 
machine learning callan characteristics text 
dumais platt heckerman sahami inductive learning algorithms representations text categorization proceedings acm cikm 
rd bcs european annual colloquium information retrieval hmm passage models document classification ranking freitag mccallum information extraction hmms shrinkage proceedings aaai workshop machine learning information extraction 
harman darpa tipster project sigir forum 
hearst plaunt subtopic structuring full document access sigir 
joachims text categorization support vector machines learning relevant features 
proceedings ecml th european conference machine learning pp 
chemnitz germany 
kaszkiel zobel davis sacks 
efficient passage ranking document databases acm trans 
information systems 
leek information extraction hidden markov models master thesis university california san diego miller leek schwartz bbn trec hidden markov models information retrieval 
proceedings trec mccallum nigam comparison event models naive bayes text classification aaai workshop learning text categorization 
mccallum nigam thrun mitchell text classification labeled unlabeled documents em machine learning mittendorf schauble document passage retrieval hidden markov models proceedings th annual international acm sigir conference research developement information retrieval si gir nigam mccallum thrun mitchell text classication labeled unlabeled documents em 
machine learning 
ponte croft text segmentation topic 
proceedings european conference research advanced technology libraries 
rabiner tutorial hidden markov models applications speech recognition proceedings ieee reuters collection www research att com lewis reuters html salton mcgill modern information retrieval mcgraw hill book turney learning algorithms keyphrase extraction information retrieval wermter recurrent neural networks text routing international conference neural networks internation conference artificial neural networks icann wilkinson effective retrieval structured documents proceedings th annual international acm sigir conference research developement information retrieval sigir yang evaluation statistical approaches text categorization information retrieval zaragoza mod les apprentissage num pour acc information phd thesis university paris rd bcs european annual colloquium information retrieval 
