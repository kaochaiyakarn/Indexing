improving storage system availability graid andrea arpaci dusseau arpaci dusseau computer sciences department university wisconsin madison dusseau cs wisc edu design implementation evaluation graid gracefully degrading quickly recovering raid storage array 
graid ensures files file system remain available unexpectedly high number faults occur 
graid recovers failures quickly restoring live file system data hot spare 
graceful degradation live block recovery implemented prototype storage system underneath unmodified file systems demonstrating powerful file system functionality implemented narrow block interface 
tree falls forest hears sound 
george berkeley storage systems comprised multiple disks backbone modern computing centers storage system entire center grind halt 
downtime clearly expensive example line business world millions dollars hour lost systems available 
storage system availability formally defined mean time failure mtbf divided sum mtbf mean time recovery mttr mtbf mtbf mttr 
improve availability increase mtbf decrease mttr 
surprisingly researchers studied components availability 
increase time failures large storage array data redundancy techniques applied 
keeping multiple copies blocks sophisticated redundancy schemes parity encoding storage systems tolerate small fixed number faults 
decrease time recovery hot spares employed failure occurs spare disk activated filled reconstructed data returning system normal operating mode relatively quickly 
narrow interface file systems storage curtailed opportunities improving mtbf mttr 
raid storage array disk fails repaired entire array corrupted 
availability cliff result storage system laying blocks oblivious semantic importance relationship files corrupted inaccessible just extra disk failure 
time consuming restore backup entire array remains unavailable disks operational 
storage array information blocks live file system recovery process restore blocks disk 
unnecessary slows recovery reduces availability 
ideal storage array fails gracefully th disks system th data unavailable 
ideal array recovers intelligently restoring live data 
effect important data disappear failure data restored earlier recovery 
strategy data availability stems berkeley observation falling trees file isn available process tries access recovered truly failure 
explore concepts provide storage array graceful failure semantics design implementation evaluation graid raid system degrades gracefully recovers quickly 
graid exploits semantic intelligence disk array place file system structures disks fault contained manner analogous fault containment techniques hive operating system distributed file systems 
unexpected double failure occurs graid continues operation serving files accessed 
graid utilizes semantic knowledge recovery specifically blocks file system considers live restored hot spare 
aspects graid combine improve effective availability storage array 
note techniques complementary existing appears third usenix symposium file storage technologies fast schemes storage administrator configures graid array utilize raid level single disk fail data loss additional failures lead proportional fraction unavailable data 
prototype implementation graid refer alexander 
alexander example semantically smart disk system 
built underneath narrow block scsi storage interface disk system understands file system data structures including super block allocation bitmaps inodes directories important structures knowledge central implementing graceful degradation quick recovery 
intricate understanding file system structures operations semantically smart arrays tailored particular file systems alexander currently functions underneath unmodified linux ext vfat file systems 
important contributions semantic disk technology 
deepen understanding build semantically smart disk systems operate correctly imperfect file system knowledge 
second demonstrate technology applied underneath widely varying file systems 
third demonstrate semantic knowledge allows raid system apply different redundancy techniques type data improving availability 
key aspects alexander implementation graceful degradation 
selective metadata replication alexander replicates naming system meta data structures file system high degree standard redundancy techniques data 
small amount overhead excess failures render entire array unavailable 
entire directory hierarchy traversed fraction files missing proportional number missing disks 
second fault isolated data placement strategy 
ensure semantically meaningful data units available failure alexander places semantically related blocks blocks file storage array unit fault containment disk 
observing natural failure boundaries array failures semantically related groups blocks unavailable leaving rest file system intact 
unfortunately fault isolated data placement improves availability cost related blocks longer striped drives reducing natural benefits parallelism raid techniques 
remedy alexander implements access driven diffusion improve throughput frequently accessed files spreading copy blocks hot files drives system 
alexander monitors access data determine files replicate fashion finds space replicas pre configured performance reserve opportunistically unused portions storage system 
evaluate availability improvements possible graid trace analysis simulation find graid excellent job masking arbitrary number failures processes enabling continued access important data 
evaluate prototype alexander microbenchmarks trace driven workloads 
find construction graid feasible imperfect semantic knowledge powerful functionality implemented block storage array 
find run time overheads graid small cpu costs compared standard array high 
show access driven diffusion crucial performance live block recovery effective disks utilized 
combination replication data placement recovery techniques results storage system improves availability maintaining high level performance 
rest structured follows 
section extended motivation section discuss design principles graid 
section trace analysis simulations discuss semantic knowledge section 
section prototype implementation 
evaluate prototype section discuss alternative methods implementing graid commercial feasibility semantic disk approach section 
section related conclude section 
extended motivation case graceful degradation raid redundancy techniques typically export simple failure model 
fewer disks fail raid continues operate correctly degraded performance 
disks fail raid entirely unavailable problem corrected restore tape 
raid schemes small disks working users may observe failed disk system 
graceful degradation raid system absolutely tolerate fixed number faults excess failures catastrophic data amount proportional number disks available system continues available allowing access data failed data restored 
matter users applications entire contents volume matters particular set files available 
question realistic expect catastrophic failure scenario raid system 
example raid system high mtbf reported disk manufacturers believe second appears third usenix symposium file storage technologies fast disk failure highly occur failed disk repaired 
multiple disk failures occur primary reasons 
correlated faults common systems expected 
raid carefully designed orthogonal manner single controller fault component error render fair number disks unavailable redundant designs expensive may higher storage arrays 
second gray points system administration main source failure systems 
large percentage human failures occur maintenance maintenance person typed wrong command wrong module introducing double failure page 
evidence suggests multiple failures occur 
example ibm array controller product includes directions attempt data recovery multiple disk failures occur raid storage array 
organization data stored file servers raid 
servers single disk failed indicator informed administrators problem 
problem discovered second disk array failed full restore backup ran days 
scenario graceful degradation enabled access large fraction user data long restore 
think best approach dealing multiple failures employ higher level redundancy enabling storage array tolerate greater number failures loss data 
techniques expensive way data mirroring bandwidth intensive os write redundant store 
graceful degradation complementary techniques 
storage administrators choose level redundancy believe necessary common case faults graceful degradation enacted worse expected fault occurs mitigating ill effect 
need semantically smart storage implementing new functionality semantically smart disk system key benefit enabling wide scale deployment underneath unmodified scsi interface os modification working smoothly existing file systems software base 
desire evolve interface file systems storage reality current interfaces survive longer anticipated 
bill joy said systems may come go protocols live forever 
new mechanism graid deployed non intrusive existing infrastructure semantic disks ensure just 
design graid expectations section discuss design graid 
background information file systems data layout strategy required enable graceful degradation important design issues arise due new layout process fast recovery 
file system background semantic knowledge system specific discuss graid design implementation widely differing file systems linux ext microsoft vfat file system 
inclusion vfat represents significant contribution compared previous research operated solely underneath unix file systems 
ext file system intellectual descendant berkeley fast file system ffs 
disk split set block groups akin cylinder groups ffs contains bitmaps track inode data block allocation inode blocks data blocks 
information file including size block pointers file inode 
vfat file system descends world pc operating systems 
consider linux vfat implementation fat general applies variants 
vfat operations centered file allocation table contains entry block file system 
entries locate blocks file linked list fashion file block address look entry fat find block file forth 
entry hold file marker setting indicates block free 
unix file systems information file inode vfat file system spreads information fat directory entries fat track blocks belong file directory entry contains information size permission type information 
graceful degradation ensure partial availability data multiple failures raid array graid employs main techniques 
fault isolated data placement strategy graid places set blocks unit fault containment storage array 
simplicity discussion assume file semantically related set blocks single disk unit fault containment 
generalize easily generalized failure boundaries observed scsi chains 
refer physical disk file belongs home site file 
particular disk fails fault isolated data placement ensures files disk appears third usenix symposium file storage technologies fast foo bar inode foo inode bar data bar data bar data root data foo inode root foo bar inode foo inode bar data bar data bar data root data foo inode root inode foo inode bar data bar data bar foo data root bar data foo inode root inode root inode root inode root inode foo inode foo inode foo foo data root foo data root foo data root bar data foo bar data foo bar data foo comparison layout schemes 
figures depict different layouts file foo bar unix file system starting root inode directory tree file data 
vertical column represents disk 
simplicity example assumes data redundancy user file data 
left typical file system layout non graid disk system blocks pointers spread file system single fault render blocks file bar inaccessible 
middle fault isolated data placement files directories 
scenario access inode file access data indirect pointer blocks constrained disk 
right example selective meta data replication 
replicating directory inodes directory blocks graid guarantee users get files available 
requisite pointers removed rightmost simplicity 
color codes white user data light shaded inodes dark shaded directory data 
home site unavailable files remain accessible files 
second technique selective meta data replication graid replicates naming system meta data structures file system high degree directory inodes directory data unix file system 
graid ensures live data reachable orphaned due failure 
entire directory hierarchy remains traversable fraction missing user data proportional number failed disks 
graid lays logical file system blocks way availability single file depends disks possible 
traditional raid array dependence set normally entire set disks group leading entire file system unavailability unexpected failure 
unix centric example typical layout fault isolated data placement selective meta data replication depicted 
note techniques graid meaningful subset file system laid single graid array 
example file system striped multiple graid arrays single array meaningful view file system 
scenario graid run logical volume manager level viewing arrays single disk techniques remain relevant 
graid treats file system block type differently traditional raid taxonomy longer adequate describing graid behaves 
finer grained notion raid level required may employ different redundancy techniques different types data 
example graid commonly employs way mirroring naming system meta data uses standard redundancy techniques mirroring parity encoding raid user data 
note value administrative control determines number failures graid degrade gracefully 
section explore data availability degrades varying levels namespace replication 
design considerations layout replication techniques required enable graceful degradation introduce host design issues 
highlight major challenges arise 
semantically related blocks fault isolated data placement graid places logical unit file system data file fault isolated container disk 
blocks graid considers related determines data remains available failure 
basic approach file grouping single file including data blocks inode indirect pointers treated logical unit data technique user may find files directory unavailable may cause frustration confusion 
groupings preserve meaningful portions file system volume failure 
directory grouping graid ensures files directory placed unit fault containment 
automated options possible allowing users specify arbitrary semantic groupings graid treats unit 
load balance fault isolated placement placing blocks file disks blocks isolated single home site 
isolated placement improves availability introduces problem load balancing space time components 
terms space total utilized space disk maintained roughly level fraction disks fail roughly fraction data unavailable 
balancing addressed foreground data allocated background migration 
files directories larger amount free space single disk handled potentially appears third usenix symposium file storage technologies fast expensive reorganization reserving large extents free space subset drives 
files larger single disk split disks 
pressing performance problems introduced fault isolated data placement 
previous indicates striping data disks better performance compared sophisticated file placement algorithms 
graid additional copies user data spread drives system process call access driven diffusion 
standard graid data placement optimized availability access driven diffusion increases performance files frequently accessed 
surprisingly access driven diffusion introduces policy decisions graid including place replicas performance files replicate create replicas 
meta data replication level degree meta data replication graid determines resilient excessive failures 
high degree replication desirable 
unfortunately meta data replication comes costs terms space time 
space overheads trade offs obvious replicas imply resiliency 
difference traditional raid graid amount space needed replication naming system meta data dependent usage volume directories induces greater amount overhead 
time overheads higher degree replication implies lowered write performance naming system meta data operations 
observed lack update activity higher levels directory tree lazy update propagation employed reduce costs 
fast recovery main design goal graid ensure higher availability fast recovery failure critical 
straightforward optimization available graid recover live file system data 
assume restoring data live mirror hot spare straightforward approach graid simply scans source disk live blocks examining appropriate file system structures determine blocks restore 
process readily generalized complex redundancy encodings 
graid potentially prioritize recovery number ways restoring certain important files importance domain specific files indicated users manner similar hoarding database coda 
exploring graceful degradation section simulation trace analysis evaluate potential effectiveness graceful degradation impact different semantic grouping techniques 
quantify space overheads level replication way way way ext kb ext kb vfat kb vfat kb table space overhead selective meta data replication 
table shows space overheads selective metadata replication percentage total user data level naming system meta data replication increases 
leftmost column percentage space overhead meta data replication shown 
columns depict costs modest way paranoid way schemes 
row shows overhead particular file system ext vfat block size set kb kb 
graid 
demonstrate ability graid provide continued access proportional fraction meaningful data arbitrary number failures 
importantly demonstrate graid hide failures users replicating important data 
simulations file system traces collected hp labs cover days activity gb data spread logical volumes 
space overheads examine space overheads due selective meta data replication typical redundancy 
calculate cost selective meta data replication percentage overhead measured volumes hp trace data 
calculate highest selective meta data replication overhead percentage possible assuming replication user data user data mirrored overheads cut half 
table shows selective meta data replication induces mild space overhead high levels meta data redundancy linux ext vfat file systems 
way redundancy meta data space overhead incurred worst case vfat kb blocks 
increasing block size ext uses space due internal fragmentation larger directory blocks overheads decrease vfat 
phenomenon due structure vfat fixed sized file system block size grows file allocation table shrinks blocks contain directory data grow 
static availability examine graid availability degrades failure different semantic grouping strategies 
strategy file grouping keeps information associated single file failure boundary disk second directory grouping allocates files directory 
analysis place entire gb files directories hp trace simulated disk sys appears third usenix symposium file storage technologies fast number failed disks static data availability directory way directory way file way directory way static data availability 
percent entire directories available shown increasing disk failures 
simulated system consists disks loaded gb hp trace 
different strategies semantic grouping shown file directory 
line varies level replication namespace meta data 
point shows average deviation trials trial randomly varies disks fail 
tem remove simulated disks measure percentage directories available 
assume user data redundancy graid level 
shows percent directories available directory available files accessible subdirectories files may 
observe graceful degradation works quite amount available data proportional number working disks contrast traditional raid disk crashes lead complete data unavailability 
fact availability degrades slightly expected strict linear fall due slight imbalance data placement disks directories 
modest level namespace replication way leads data availability failure 
conclude file grouping files directory disappear failure leading user dissatisfaction 
dynamic availability simulating dynamic availability examine users applications oblivious operating degraded mode 
specifically run portion hp trace simulator number failed disks record percent processes observed failure run 
experiment find namespace replication certain files needed processes replicated 
experiment set degree namespace replication full replication vary level replication contents popular directories usr bin bin lib 
number failed disks dynamic process availability popular replication way way way dynamic data availability 
plots percent processes run unaffected disk failure busy hour hp trace 
degree namespace replication set aggressively 
line varies amount replication popular directories way implies directories replicated way way show happens modest extreme amount replication 
means deviations trials shown 
shows replicating contents directories percent processes run ill effect lower expected results 
directories replicated percentage processes run completion disk failure better expected 
reason clear substantial number processes ps require executable libraries available run correctly 
popular directory replication excellent availability failure possible 
fortunately popular files read directories wide scale replication raise write performance consistency issues 
space overhead due popular directory replication minimal reasonably sized file system trace directories account mb total file system size 
semantic knowledge move construction graid prototype underneath block scsi interface 
enabling technology underlying graid semantic knowledge 
understanding file system utilizes disk enables graid implement graceful degradation failure quick recovery 
exact details acquiring semantic knowledge disk raid system described just assume basic understanding file system layout structures available storage system 
specifically assume graid static knowledge file system layout including regions disk block types contents specific block types fields inode 
appears third usenix symposium file storage technologies fast file system behaviors extend understanding disks presenting techniques handle general file system behaviors 
previous required file system mounted synchronously implementing complex functionality disk relax requirement 
describe assumptions general file system behavior believe modern file systems adhere behavioral guidelines 
blocks file system dynamically typed file system locate different types blocks physical location disk lifetime file system 
example unix file system block data region user data block indirect pointer block directory data block 
second file system delay updates disk delayed writes file system facilitate batching small writes memory suppressing writes files subsequently deleted 
third consequence delayed writes order file system writes data disk arbitrary 
certain file systems order writes carefully remain general assumptions ordering 
note assumptions practical reasons linux ext file system exhibits aforementioned behaviors 
accuracy information assumptions general file system behavior imply storage system accurately classify type block 
block classification straightforward type block depends location disk 
example berkeley fast file system ffs regions disk store inodes fixed file system creation traffic regions known contain inodes 
type information spread multiple blocks 
example block filled indirect pointers identified observing corresponding inode specifically inode indirect pointer field contains address indirect block 
formally identify indirect block semantic disk look inode block indirect pointer field 
relevant inode block written disk disk infers indirect block observes block written uses information classify treat block indirect block 
due delayed write reordering behavior file system possible time disk writes block freed original inode reallocated inode different type normal data block 
disk know operations took place memory reflected disk 
inference semantic disk block type wrong due inherent staleness information tracked 
implementing correct system despite potentially inaccurate inferences challenges address 
implementation making graid discuss prototype implementation known alexander 
alexander uses data placement selective meta data replication provide graceful degradation failure employs access driven diffusion correct performance problems introduced availability oriented layout 
currently alexander replicates namespace system meta data administrator controlled value stores user data raid raid manner refer systems graid levels respectively 
currently pursuing graid level implementation uses avoid small write problem exacerbated fault isolated data placement 
section implementation graceful degradation live block recovery complexity discussion centered graceful degradation 
simplicity exposition focus construction alexander underneath linux ext file system 
section discuss differences implementation underneath vfat 
graceful degradation overview basic operation graceful degradation alexander 
indirection map similar scsi raid system alexander presents host systems linear logical block address space 
internally alexander place blocks facilitate graceful degradation 
control placement alexander introduces transparent level indirection logical array file system physical placement disks indirection map imap similar structures 
systems imap maps live logical file system block replica list physical locations 
unmapped blocks considered free candidates graid 
reads handling block read requests graid level straightforward 
logical address block alexander looks imap find replica list issues read request replicas 
choice replica read various criteria currently alexander uses randomized selection 
appears third usenix symposium file storage technologies fast anatomy write depicts control flow sequence write operations alexander 
inode block written alexander observes contents inode block identifies newly added inode 
selects home site inode creates physical mappings blocks inode home site 
inode block aggressively replicated 
alexander observes write data block inode mapped write goes directly physical block 
third alexander gets write unmapped data block defers block alexander observes corresponding inode fourth creates relevant mappings observes blocks deferred issues deferred write relevant home site 
writes contrast reads write requests complex handle 
exactly alexander handles write request depends type block written 
depicts common cases 
block static meta data block inode bitmap block unmapped alexander allocates physical block disks replica reside writes copies 
note alexander easily detect static block types inode bitmap blocks underneath unix file systems simply observing logical block address 
inode block written graid scans block newly added inodes understand inodes new graid compares newly written block old copy process referred block differencing 
new inode graid selects home site lay blocks belonging inode records inode hashtable 
selection home site done balance space allocation physical disks 
currently graid uses greedy approach selects home site disk space utilization 
write unmapped block data region data block indirect block directory block allocation done graid knows file block belongs actual home site 
case graid places block deferred block list write disk learns file block associated 
crash inode write block inaccessible file system anyway memory deferred block list reliability concern 
graid looks newly added block pointers inode indirect block written 
newly added block pointer refers unmapped block adds new entry imap mapping logical block physical block home site assigned corresponding inode 
newly added pointer refers block deferred list graid removes block deferred list issues write appropriate physical block 
writes deferred blocks written corresponding owner inode blocks 
inode written subsequent data writes mapped sent disk directly 
block type interest graid looks data bitmap block 
data bitmap block written graid scans looking newly freed data blocks 
freed block removes logical physical mapping exists frees corresponding physical blocks 
block currently deferred list freed block removed deferred list write suppressed data blocks written file system deleted corresponding inode written disk generate extra disk traffic similar optimizations file systems 
removing blocks deferred list important case freed blocks alexander may observe owning inode 
deferred block stays deferred list bounded amount time inode owning block written bitmap block indicating deletion block written 
exact duration depends delayed write interval file system 
block reuse discuss intricate issues involved implementing graceful degradation 
issue block reuse 
existing files deleted truncated new files created blocks part file may reallocated file 
needs place blocks correct home site reuse blocks needs detected acted 
graid handles block reuse manner inode block indirect block written graid examines valid block pointer see physical block mapping matches home site allocated corresponding inode 
graid changes mapping block correct home site 
possible write block context new file went old home site needs copied old physical location new location 
blocks copied added pending copies list background thread copies appears third usenix symposium file storage technologies fast blocks new home site frees old physical locations copy completes 
dealing imperfection difficulty arises semantically smart disks underneath typical file systems exact knowledge type dynamically typed block impossible obtain discussed section 
alexander handle incorrect type classification data blocks file data directory indirect blocks 
example graid understand contents indirect blocks uses pointers place file blocks home site 
due lack perfect knowledge fault isolated placement file compromised note data loss corruption issue 
goal dealing imperfection conservatively avoid possible eventually detect handle cases 
specifically block construed indirect block written assume valid indirect block 
live pointer block graid take action 
cases consider 
case pointer refer unmapped logical block 
mentioned graid creates new mapping home site corresponding inode indirect block belongs 
indirect block pointer valid mapping correct mapping 
indirect block misclassified consequently pointer invalid graid detects block free observes data bitmap write point mapping removed 
block allocated file bitmap written graid detects reallocation inode write corresponding new file creates new mapping copies data contents new home site discussed 
second case potentially corrupt block pointer point mapped logical block 
discussed type block reuse results new mapping copy block contents new home site 
indirect block pointer valid new mapping correct block 
indirect block misclassification alexander wrongly copies data new home site 
note data accessible original file block belongs blocks incorrect home site 
fortunately situation transient inode file written graid detects reallocation creates new mapping back original home site restoring correct mapping 
files accessed properly laid infrequent sweep inodes looks rare cases improper layout 
optimizations graid eventually move data correct home site preserving graceful degradation 
reduce number times misclassification occurs alexander assumption contents indirect blocks specifically contain number valid unique pointers null pointers 
alexander leverage assumption greatly reduce number misclassifications performing integrity check supposed indirect block 
integrity check reminiscent conservative garbage collection returns true pointers byte words block point valid data addresses volume non null pointers unique 
clearly set blocks pass integrity check corrupt data contents happened exactly evade conditions 
test run data blocks file system indicates small fraction data blocks pass test blocks pass test reallocated file data block indirect block misclassified 
access driven diffusion issue graid address performance 
fault isolated data placement improves availability cost performance 
data accesses blocks large file directory grouping files directory longer parallelized 
improve performance alexander performs access driven diffusion monitoring block accesses determine hot diffusing blocks replication disks system enhance parallelism 
access driven diffusion achieved logical physical levels disk volume 
logical approach access individual files monitored considered hot diffused 
file replication fails capture sequentiality multiple small files example single directory 
pursue physical approach alexander replicates segments logical address space disks volume 
file systems allocating contiguous logical blocks single file files directory replicating logical segments identify exploit common access patterns 
implement access driven diffusion alexander divides logical address space multiple segments normal operation gathers various statistics utilization access patterns segment 
background thread selects logical segments benefit access driven diffusion diffuses copy drives system 
subsequent reads writes go replicas background updates sent original blocks 
imap entry block indicates copy date 
amount disk space allocate replicas presents important policy decision 
initial policy alexander implements reserve certain minimum amount space specified sys appears third usenix symposium file storage technologies fast tem administrator replicas opportunistically free space available array additional replication 
approach similar autoraid mirrored data autoraid identify data considered dead file system written contrast graid semantic knowledge identify blocks free 
live block recovery implement live block recovery graid understand blocks live 
knowledge correct block live considered dead lead data loss 
alexander tracks information observing bitmap data block traffic 
bitmap blocks tell liveness state file system reflected disk 
due reordering delayed updates uncommon observe write data block corresponding bit set data bitmap 
account graid maintains duplicate copy bitmap blocks sees write block sets corresponding bit local copy bitmap 
duplicate copy synchronized file system copy data bitmap block written file system 
conservative bitmap table reflects superset live blocks file system perform live block recovery 
note assume pre allocation state bitmap written disk subsequent allocation locking linux modern systems ensures 
technique guarantees live block classified dead possible disk consider block live far longer situation arise example file system writes deleted blocks disk 
implement live block recovery alexander simply uses conservative bitmap table build list blocks need restored 
alexander proceeds list copies live data hot spare 
aspects alexander host aspects implementation required successful prototype discuss length due space limitations 
example preserving logical contiguity file system important block allocation developed mechanisms enable placement 
directory grouping requires sophistication implementation handle deferral writes parent directory block written 
just time block allocation prevents misclassified indirect blocks causing spurious physical block allocation 
deferred list management introduces tricky issues memory 
alexander preserves sync semantics returning success inode block writes deferred block writes waiting inode complete 
number structures alexander maintains imap reliably committed disk preferably performance buffered small amount non volatile ram 
important component missing alexander decision popular read directories usr bin replicate widely 
alexander contains proper mechanisms perform replication policy space remains unexplored 
initial experience indicates simple approach monitoring frequency inode access time updates may effective 
alternative approach allows administrators specify directories treated manner 
interesting issue required change design behavior linux ext partial disk failure 
process tries read data block unavailable ext issues read returns failure process 
block available recovery process issues read ext issue read works expected 
process tries open file inode unavailable ext marks inode suspicious issue request inode block alexander recovered block 
avoid change file system retain ability recover failed inodes alexander replicates inode blocks namespace meta data data blocks file 
alexander fat surprised similarities implementing graid underneath ext vfat 
example vfat overloads data blocks user data blocks directories alexander defer classification blocks manner similar ext implementation 
instances vfat implementation graid differed interesting ways ext version 
example fact pointers file located file allocation table number aspects graid simpler implement vfat indirect pointers worry 
ran occasional odd behavior linux implementation vfat 
example linux write disk data blocks allocated freed avoiding obvious common file system optimization 
indicative untuned nature linux implementation served indicator semantic disks wary assumptions file system behavior 
evaluating alexander performance evaluation alexander 
focus primarily linux ext variant appears third usenix symposium file storage technologies fast 
misplaced blocks time sec misplaced blocks remapping remapping close close errors placement 
plots number blocks wrongly laid alexander time running busy hour hp trace 
experiment run disks total number blocks accessed trace 
include baseline measurements vfat system 
wish answer questions alexander correctly 
time overheads introduced 
effective access driven diffusion 
fast live block recovery 
benefits expect graid 
complex implementation 
platform alexander prototype constructed software raid driver linux kernel 
file systems mount pseudo device normal disk 
environment excellent understanding issues involved construction real hardware graid system limited ways 
importantly alexander runs system host os applications interference due competition resources 
second performance characteristics microprocessor memory system may different actual raid system 
experiments utilize mhz pentium iii rpm ibm disks 
alexander correctly 
alexander complex simple raid systems 
ensure alexander operates correctly put system numerous stress tests moving large amounts data system problems 
extensively tested corner cases system pushing situations difficult handle making sure system degrades gracefully recovers expected 
example repeatedly crafted microbenchmarks stress mechanisms detecting block reuse handling imperfect information dynamically typed blocks 
constructed benchmarks write user data blocks disk slowdown versus raid operational overheads ext fat create read overwrite unlink time overheads 
plots time overheads observed graid level versus raid level series microbenchmarks 
tests run disk systems 
experiment operations enacted file creations operation kb file 
contain worst case data data appears valid directory entries indirect pointers 
cases alexander able detect blocks indirect blocks move files directories proper fault isolated locations 
verify alexander places blocks appropriate disk instrumented file system log block allocations 
addition alexander logs events interest assignment home site inode creation new mapping logical block re mapping blocks different receipt logical writes file system 
evaluate behavior alexander certain workload run workload alexander obtain time ordered log events occurred file system alexander 
process log line look number blocks wrongly laid time 
ran test hours hp traces hours examined number blocks misplaced temporarily quite low blocks 
report detailed results hour trace observed greatest number misplaced blocks hours examined 
shows results 
parts 
bottom part shows normal operation alexander capability react block reuse remapping copying blocks correct 
shows alexander able quickly detect wrongly placed blocks remap appropriately 
number blocks misplaced temporarily total number blocks accessed trace 
top part shows number misplaced blocks experiment assuming remapping occur 
expected blocks remain misplaced 
dip trace occurs appears third usenix symposium file storage technologies fast run time blocks written seconds total meta unique data raid graid graid graid graid table performance postmark 
table compares performance graid level raid postmark benchmark 
row marked graid indicates specific level metadata replication 
column reports benchmark run time second column shows number disk writes incurred 
third column shows number disk writes metadata blocks fourth column indicates number unique metadata blocks written 
experiment run disks 
misplaced blocks assigned file accidentally correcting original 
time overheads introduced 
explore time overheads arise due semantic inference 
primarily occurs new blocks written file system file creation 
shows performance alexander simple microbenchmark 
seen allocating writes slower due extra cpu cost involved tracking fault isolated placement 
reads overwrites perform comparably raid 
high unlink times fat fat writes data pertaining deleted files processed graid newly allocated data 
implementation untuned infrastructure suffers cpu memory contention host believe worst case estimates overheads 
cost graid explore overhead metadata replication 
purpose choose postmark metadata intensive file system benchmark 
slightly modified postmark perform sync deletion phase metadata writes accounted making pessimistic evaluation costs 
table shows performance alexander various degrees metadata replication 
seen table synchronous replication metadata blocks significant effect performance metadata intensive workloads file sizes postmark range bytes kb 
note alexander performs better default raid lower degrees replication better physical block allocation ext looks contiguous free chunk blocks allocate new file layout sub optimal small files 
table shows number disk writes incurred course benchmark 
percentage extra disk writes roughly accounts difference bandwidth mb file size kb access driven diffusion raid graid file access driven diffusion graid directory access driven diffusion graid file graid directory access driven diffusion 
presents performance graid level standard raid sequential workload 
experiment number files size read sequentially total volume data fixed mb 
graid performs better smaller files due better physical block layout 
formance different replication levels extra writes metadata blocks 
count number unique physical writes metadata blocks absolute difference different replication levels small 
suggests lazy propagation updates metadata block replicas idle time freeblock scheduling greatly reduce performance difference cost added complexity 
example lazy update propagation replicas updated graid incur extra disk writes 
played back portion hp traces minutes standard raid system graid disks 
playback engine issues requests times specified trace optional speedup factor speedup implies idle time requests reduced factor 
speedup factors graid delivered operation throughput raid utilizing idle time trace hide extra cpu overhead 
scaling factor operation throughput lagged slightly graid showing slowdown third trace execution caught due idle time 
effective access driven diffusion 
show benefits access driven diffusion 
trial experiment perform set sequential file reads files increasing size 
compare standard raid striping graid access driven diffusion 
shows results experiment 
see access driven diffusion sequential access larger files run rate single disk system benefit potential parallelism 
access driven diffusion performance improved reads directed appears third usenix symposium file storage technologies fast reconstruction time live volume percentage costs reconstruction graid level worst case graid level best case idealized raid level live block recovery 
shows time recover failed disk hot spare graid level mirrored system live block recovery 
lines plotted worst case live data spread entire mb volume best case compacted smallest contiguous space possible 
plotted recovery time idealized raid level 
diffused copies disks system 
note case arrange files diffused start experiment reading certain threshold number times 
investigating sophisticated policies initiate access driven diffusion left 
fast live block recovery 
explore potential improvement seen live block recovery 
presents recovery time graid varying amount live file system data 
plots lines worst case best case live block recovery 
worst case live data spread disk best case compacted single portion volume 
graph see live block recovery successful reducing recovery time particularly disk half full 
note difference worst case best case times difference suggests periodic disk reorganization speed recovery moving live data localized portion 
benefits expect graid 
demonstrate improved availability alexander failures 
shows availability performance observed process randomly accessing kb files running graid raid 
ensure fair comparison graid raid limit reconstruction rate mb shows reconstruction gb volume gb live data completes faster compared raid 
extra second failure occurs availability raid drops near zero graid continues availability 
surprisingly restore raid fails certain files linux retry inode blocks fail 
required availability ops succeed time sec raid availability raid throughput graid availability graid throughput operation failure failure recon complete failure recon second failure restore failed disk re mount recon complete availability profile 
shows operation graid level raid failures 
gb array consists data disks hot spare 
failure data reconstructed hot spare graid recovering faster raid 
failures occur raid loses files graid continues serve files 
workload consists read modify writes kb files randomly picked gb working set 
raid returns full availability 
complex implementation 
briefly quantify implementation complexity alexander 
table shows number statements required implement different components alexander 
table see core file system inferencing module ext requires lines code counted number semicolons core mechanisms graid contribute lines code 
rest spent hash table avl tree wrappers memory management 
compared tens lines code comprising modern array firmware believe added complexity graid significant 
discussion section compare semantic disk approach alternative methods implementing discuss possible concerns commercial feasibility semantic disk systems 
alternative approaches semantic disk approach different ways implementing graid trade offs 
similar modern processors innovate beneath unchanged instruction sets semantic disk level implementation facilitates ease deployment inter operability unchanged client infrastructure making pragmatic 
cost approach complexity rediscovering semantic knowledge tolerant inaccuracies 
alternative approach change interface file systems storage convey richer information layers 
instance storage system expose failure boundaries file system appears third usenix symposium file storage technologies fast semicolons total graid generic setup fault isolated placement physical block allocation access driven diffusion mirroring live block recovery internal memory management hashtable avl tree file system specific sds inferencing ext sds inferencing vfat total table code size alexander implementation 
number lines code needed implement alexander shown 
column shows number semicolons second column shows total number lines including white spaces comments 
file system explicitly allocate blocks fault isolated manner 
alternatively file system tag write logical fault container id storage system implement data placement 
techniques intrusive existing infrastructure software base conceivably complex approach 
object storage new interface considered file boundaries visible storage layer 
objectbased interface semantically smart technology relevant discover relationships objects instance inferring directory object points set file objects need collocated 
commercial feasibility definition graid semantically smart storage systems detailed knowledge file system 
embedding higher degree functionality storage system leads concerns commercial feasibility systems 
concern arises placing semantic knowledge disk system ties disk system intimately file system 
example file system disk structure changes storage system may change 
believe issue problematic 
disk formats evolve slowly reasons backwards compatibility 
example basic structure ffs file systems changed period years linux ext file system introduced roughly exact layout lifetime 
ext journaling file system backwards compatible ext disk layout new extensions freebsd file system backwards compatible 
evidence storage vendors willing maintain support software specific file system example emc storage system comes software understand format common file systems 
second concern storage system needs semantic knowledge file system interacts 
fortunately large number file systems need supported cover large fraction usage population 
semantic storage system file system support storage system detect file system conform expectations turn special functionality case graid revert normal raid layout 
detection done simple techniques observing file system identifier partition table 
final concern arises processing required disk system 
believe major issue general trend increasing disk system intelligence processing power increases disk systems contain substantial computational abilities :10.1.1.106.4722
modern storage arrays exhibit fruits moore law example emc storage server configured processors gb ram 
related graid draws related number different areas including distributed file systems traditional raid systems 
discuss turn 
distributed file systems designers distributed file systems long ago realized problems arise spreading directory tree different machines system 
example walker discuss importance directory namespace replication locus distributed system 
coda mobile file system takes explicit care regard directory tree 
specifically file cached coda sure cache directory root directory tree 
doing coda guarantee file remains accessible disconnection occur 
interesting extension reconsider host inmemory caching availability mind 
slice tries route namespace operations files directory server 
wide area file systems re emphasized importance directory tree 
example pangaea file system aggressively replicates entire tree root node file accessed 
island file system points need fault isolation context widearea storage systems island principle quite similar fault isolated placement graid 
systems past place entire file single machine similar load balancing issues :10.1.1.110.5867
problem difficult space due constraints file placement block appears third usenix symposium file storage technologies fast migration simpler centralized storage array 
traditional raid systems draw long history research classic raid systems 
autoraid learned complex functionality embedded modern storage array background activity utilized successfully environment 
afraid learned flexible trade performance reliability value delaying updates 
raid research focused different redundancy schemes 
early stressed ability tolerate single disk failures research introduced notion tolerating multiple disk failures array 
stress complementary line research traditional techniques ensure full file system availability certain number failures graid techniques ensure graceful degradation additional failures 
related approach parity striping stripes parity data achieve fault isolation layout oblivious semantics data blocks level redundancy irrespective importance meta data vs data multiple failures entire file system inaccessible 
number earlier works emphasize importance hot speed recovery time raid arrays 
semantic recovery complementary approaches 
note term graceful degradation refer performance characteristics redundant disk systems failure 
type graceful degradation different discuss systems continues operation unexpected number failures occurs 
robust system continues operate nearly correctly presence class errors robert hagmann graid turns simple binary failure model storage systems continuum increasing availability storage continuing operation partial failure quickly restoring live data failure occur 
shown potential benefits graid established limits semantic knowledge shown successful graid implementation achieved despite limits 
simulation evaluation prototype implementation graid built underneath standard block interface file system modification delivers graceful degradation live block recovery access driven diffusion performance 
conclude discussions lessons learned process implementing graid limited knowledge disk imply limited functionality 
main contributions demonstration limits semantic knowledge proof implementation despite limitations interesting functionality built inside semantically smart disk system 
believe semantic disk system careful assumptions file system behavior hope guide pursue similar course 
semantically smart disks easier build help 
way file systems reorder delay hide operations disks reverse engineering exactly doing scsi level difficult 
believe small modifications file systems substantially lessen difficulty 
example file system inform disk believes file system structures consistent state challenges disk lessened 
example small alterations ease burden semantic disk development 
semantically smart disks stress file systems unexpected ways 
file systems built operate top disks behave graid specifically may behave particularly part volume address space unavailable 
heritage os inexpensive hardware linux file systems handle unexpected conditions fairly 
exact model dealing failure inconsistent data blocks missing reappear true inodes 
semantically smart disks push new functionality storage file systems evolve accommodate 
detailed traces workload behavior invaluable 
excellent level detail available hp traces able simulate analyze potential graid realistic settings 
traces contain process information anonymize file extent pathnames included trace utilize study 
remaining challenge tracing include user data blocks semantically smart disks may sensitive contents 
privacy concerns campaign encounter may difficult overcome 
acknowledgments anurag acharya erik riedel yasushi saito john bent nathan burnett timothy brian forney insightful comments earlier drafts 
richard golding excellent shepherding anonymous reviewers thoughtful suggestions greatly improved content 
appears third usenix symposium file storage technologies fast computer systems lab providing environment computer science research 
sponsored nsf ccr ccr ccr ngs itr itr ibm emc wisconsin research foundation 
acharya saltz 
active disks programming model algorithms evaluation 
asplos viii san jose ca october 
alvarez burkhard cristian 
tolerating multiple failures raid architectures optimal storage uniform declustering 
isca pages 
anderson chase vahdat 
interposed request routing scalable network storage 
acm transactions computer systems february 
bitton gray 
disk shadowing 
vldb pages los angeles ca august 
boehm weiser 
garbage collection uncooperative environment 
software practice experience september 
burkhard menon 
disk array storage system reliability 
ftcs pages toulouse france june 
chapin rosenblum devine lahiri gupta 
hive fault containment shared memory multiprocessors 
sosp december 
chen lee gibson katz patterson 
raid high performance reliable secondary storage 
acm computing surveys june 
arpaci dusseau arpaci dusseau 
bridging information gap storage protocol stacks 
usenix june 
malone 
filesystem optimisations freebsd 
freenix monterey ca june 
emc 
enterprise information storage systems 
www emc com 
english stepanov 
loge self organizing disk controller 
usenix winter january 
ganger 
blurring line oses storage devices 
technical report cmu cs carnegie mellon university december 
ganger mckusick soules patt 
soft updates solution metadata update problem file systems 
acm tocs may 
ganger worthington hou patt 
disk subsystem load balancing disk striping vs conventional data placement 
hicss 
gibson nagle amiri butler chang gobioff hardin riedel zelenka 
costeffective high bandwidth storage architecture 
asplos viii october 
gray 
computers 
th international conference reliability distributed databases june 
gray horst walker 
parity striping disc arrays low cost reliable storage acceptable throughput 
proceedings th international conference large data bases vldb pages brisbane australia august 
gribble 
robustness complex systems 
hotos viii schloss germany may 
hagmann 
reimplementing cedar file system logging group commit 
sosp november 
holland gibson siewiorek 
fast line failure recovery redundant disk arrays 
ftcs france 

hsiao dewitt 
chained declustering new availability strategy multiprocessor database machines 
th international data engineering conference 
ibm 
recovering multiple disk failures 
www pc ibm com html 
ji felten wang singh 
file system highly available scalable internet services 
th usenix windows symposium august 
katcher 
postmark new file system benchmark 
technical report tr network appliance oct 
keeton wilkes 
automating data dependability 
proceedings th acm sigops european workshop pages saint france september 
kistler satyanarayanan 
disconnected operation coda file system 
acm tocs february 
mckusick joy leffler fabry 
fast file system unix 
acm tocs august 
menon mattson 
comparison alternatives disk arrays 
isca gold coast australia may 
microsoft 
www microsoft com december 

doubly distorted mirrors 
sigmod washington dc may 
park balasubramanian 
providing fault tolerance parallel secondary storage systems 
technical report cs tr princeton november 
patterson gibson katz 
case redundant arrays inexpensive disks raid 
sigmod june 
patterson 
availability maintainability performance new focus new century 
key note fast january 
popek walker chow edwards kline thiel 
locus network transparent high reliability distributed system 
sosp december 
reddy banerjee 
gracefully disk arrays 
ftcs pages montreal canada june 
riedel gibson faloutsos 
active storage largescale data mining multimedia 
proceedings th international conference large databases vldb new york new york august 
riedel swaminathan 
framework evaluating storage system security 
fast pages monterey ca january 
rosenblum ousterhout 
design implementation log structured file system 
acm transactions computer systems february 
rowstron druschel 
storage management caching past large scale persistent peer peer storage utility 
sosp banff canada october 
ruemmler wilkes 
disk shuffling 
technical report hpl hewlett packard laboratories 
saito karlsson mahalingam 
taming aggressive replication pangaea wide area file system 
osdi boston ma december 
savage wilkes 
afraid frequently redundant array independent disks 
usenix pages san diego ca january 
arpaci dusseau arpaci dusseau 
semantically smart disk systems 
fast san francisco ca march 
ts tweedie 
directions ext filesystem 
freenix monterey ca june 
wang anderson patterson 
virtual log file systems programmable disk 
osdi new orleans la february 
wilkes golding staelin sullivan 
hp autoraid hierarchical storage system 
acm transactions computer systems february 
wolf 
placement optimization problem practical solution disk file assignment problem 
sigmetrics pages berkeley ca may 

