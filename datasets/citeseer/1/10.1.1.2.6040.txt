letter communicated john platt new support vector algorithms bernhard sch lkopf alex smola gmd berlin germany department engineering australian national university canberra australia robert williamson department engineering australian national university canberra australia peter bartlett australian national university canberra australia propose new class support vector algorithms regression classification 
algorithms parameter lets effectively control number support vectors 
useful right parameterization additional benefit enabling eliminate free parameters algorithm accuracy parameter regression case regularization constant classification case 
describe algorithms give theoretical results concerning meaning choice report experimental results 
support vector sv machines comprise new class learning algorithms motivated results statistical learning theory vapnik 
originally developed pattern recognition vapnik chervonenkis boser guyon vapnik represent decision boundary terms typically small subset sch lkopf burges vapnik training examples called support vectors 
order sparseness property carry case sv regression vapnik devised called insensitive loss function max penalize errors chosen priori 
algorithm henceforth call svr seeks estimate functions address microsoft research street cambridge neural computation massachusetts institute technology sch lkopf smola williamson bartlett independent identically distributed data 
space input patterns live applies inputs set goal learning process find function small risk test error dp probability measure assumed responsible generation observations see equation loss function example choices smola sch lkopf 
particular loss function minimize equation depends specific regression estimation problem hand 
necessarily coincide loss function learning algorithm 
additional constraints regression estimation satisfy instance sparse representation terms training data 
sv case achieved insensitive zone equation 
second minimize equation directly place know sample equation try obtain small risk minimizing regularized risk functional emp 
term characterizes model complexity emp yi xi measures insensitive training error constant determining trade 
short minimizing equation captures main insight statistical learning theory stating order obtain small risk needs control training error model complexity explain data simple model 
minimization equation equivalent constrained optimization problem see minimize new support vector algorithms sv regression desired accuracy specified priori 
attempted fit tube radius data 
trade model complexity points lying outside tube positive slack variables determined minimizing expression 
subject xi yi yi xi 
understood boldface greek letters denote dimensional vectors corresponding variables shorthand implying variables asterisks 
lagrange multiplier techniques show vapnik leads dual optimization problem 
maximize subject yi xi xj 
resulting regression estimates linear setting generalized nonlinear kernel method 
precisely method section shall omit exposition point 
sch lkopf smola williamson bartlett motivate new algorithm shall propose note parameter useful desired accuracy approximation specified 
cases want estimate accurate possible having commit specific level accuracy priori 
describe modification svr algorithm called svr automatically minimizes 
theoretical results svr concerning connection robust estimators section asymptotically optimal choice parameter section 
extend algorithm handle parametric insensitivity models allow account prior knowledge heteroscedasticity noise 
bridge connecting theoretical part article second definition margin sv classification sv regression algorithms maximize section 
view close connection algorithms surprising possible formulate sv classification algorithm 
done including theoretical analysis section 
conclude experiments discussion 
sv regression estimate functions see equation empirical data see equation proceed follows sch lkopf bartlett smola williamson 
point xi allow error 
captured slack variables penalized objective function regularization constant chosen priori vapnik 
size traded model complexity slack variables constant minimize subject xi yi yi xi 
constraints introduce multipliers obtain lagrangian new support vector algorithms yi xi xi yi 
minimize expression find saddle point minimize primal variables maximize dual variables setting derivatives respect primal variables equal zero yields equations xi 
sv expansion equation nonzero correspond constraint equations precisely met corresponding patterns called support vectors 
due karush kuhn tucker kkt conditions apply convex constrained optimization problems bertsekas 
write constraints xi yi corresponding lagrange multipliers solution satisfies xi yi substituting conditions leads optimization problem called wolfe dual 
stating explicitly carry modification 
boser 
substitute kernel dot product corresponding dot product feature space related input space nonlinear map 
implicitly carry computations feature space maps high dimensionality 
feature space structure reproducing kernel hilbert space wahba girosi sch lkopf minimization understood context regularization operators smola sch lkopf ller 
method applicable algorithm cast terms dot products aizerman braverman boser sch lkopf smola ller :10.1.1.103.1189
choice research topic sch lkopf smola williamson bartlett right shall touch williamson smola sch lkopf sch lkopf shawe taylor smola williamson typical choices include gaussian kernels exp polynomial kernels 
rewriting constraints noting appear dual arrive svr optimization problem maximize subject yi xi xj 
regression estimate takes form cf 
equations xi computed account equations substitution xj understood cf 
equations equalities points respectively due kkt conditions 
give theoretical results explaining significance parameter observation concerning helpful 
pay increase cf 
equation 
happen example data noise free perfectly interpolated low capacity model 
case interested corresponds plain loss regression 
term errors refer training points lying outside tube term fraction errors svs denote relative numbers tube called slab region parallel hyperplanes 
new support vector algorithms errors svs divided 
proposition define modulus absolute continuity function function sup bi ai supremum taken disjoint intervals ai bi ai bi satisfying bi ai 
loosely speaking condition conditional density asks absolutely continuous average proposition 
suppose svr applied data set resulting nonzero 
statements hold upper bound fraction errors 
ii 
lower bound fraction svs 
iii 
suppose data see equation generated distribution continuous expectation modulus absolute continuity density satisfying lim 
probability asymptotically equals fraction svs fraction errors 
proof 
ad 
constraints equations imply fraction examples 
examples outside tube certainly satisfy grow reduce 
ad ii 
kkt conditions implies 
equation equality cf 
equation 
svs examples result follows vapnik 
ad iii 
strategy proof show asymptotically probability point lying edge tube vanishes 
condition means sup ep function approaches zero 
class sv regression estimates behaved covering numbers anthony bartlett chap 
pr sup practice alternatively equation equality constraint 
sch lkopf smola williamson bartlett sample estimate proportion points satisfy may depend 
discretizing values union bound applying equation shows supremum converges zero probability 
fraction points edge tube surely converges 
fraction svs equals errors 
combining statements ii shows fractions converge surely 
control number errors note equation implies vapnik 
constraint equation implies equation equivalent conclude proposition holds upper lower edges tube separately 
note argument number svs edges standard svr tube asymptotically agree 
intuitive albeit somewhat informal explanation terms primal objective function see equation 
point solution note emp emp greater equal fraction errors points outside tube certainly contribute change remp changed 
points edge tube possibly contribute 
inequality comes 
note contradict freedom choose 
case pay increase cf 
equation 
briefly discuss svr relates svr see section 
algorithms insensitive loss function svr automatically computes 
bayesian perspective automatic adaptation loss function interpreted adapting error model controlled hyperparameter 
comparing equation substitution kernel dot product understood equation note svr requires additional term fixed encourages turn 
accordingly constraint see equation appears svr needed 
primal problems equations differ term optimization grow arbitrarily large zero empirical risk obtained zero 
sense svr includes svr 
note general case kernels vector feature space 
proposition 
svr leads solution svr set priori value solution new support vector algorithms proof 
minimize equation fix minimize remaining variables 
solution change 
connection robust estimators insensitive loss function patterns outside tube enter empirical risk term patterns closest actual regression zero loss 
mean outliers determine regression 
fact contrary case 
proposition resistance sv regression 
support vector regression insensitive loss function see equation local movements target values points outside tube influence regression 
proof 
shifting yi locally change status xi yi point outside tube 
dual solution feasible satisfies constraints point 
primal solution transformed movement yi feasible 
kkt conditions satisfied 
bertsekas optimal solution 
proof relies fact outside tube upper bound 
precisely case loss function increases linearly tube cf 
huber requirements robust cost functions 
inside various functions derivative smaller linear part 
case insensitive loss proposition implies essentially regression generalization estimator mean random variable throws away largest smallest examples fraction category section shown sum constraint equation implies proposition applied separately sides 
estimates mean average extremal ones remaining examples 
resistance concerning outliers close spirit robust estimators trimmed mean 
fact get closer idea trimmed mean throws away largest smallest points computes mean remaining points quadratic loss inside tube 
leave huber robust loss function 
sch lkopf smola williamson bartlett note parameter related breakdown point corresponding robust estimator huber 
specifies fraction points may arbitrarily bad outliers related fraction arbitrary distribution may added known noise model estimator failing 
add simple modification loss function white weighting slack variables tube target function equation respectively estimate generalized quantiles 
argument proceeds follows 
asymptotically patterns multipliers bound cf 
proposition 
changes upper bounds box constraints applying different types slack variables respectively 
equality constraint equation implies give fractions points outside tube lie sides tube respectively 
asymptotically optimal choice analysis employing tools information geometry murata amari smola murata sch lkopf ller derive asymptotically optimal class noise models sense maximizing statistical efficiency 

denote density unit variance family noise models generated 
assume data generated distribution continuous 
assumption sv regression produces estimate converging underlying functional dependency asymptotically optimal estimation model sv regression described smola murata sch lkopf ller argmin dt dt section assumes familiarity concepts information geometry 
complete explanation model underlying argument smola murata sch lkopf ller downloaded svm gmd de 
prototype generating class densities normalization assumptions ease notation 
new support vector algorithms see note assumptions stated probability deviation larger pr converge pr dx 
fraction samples asymptotically svs proposition iii 
algorithm generating fraction svs correspond algorithm tube size 
consequence noise model compute optimal equation compute corresponding optimal value 
exploits linear scaling behavior standard deviation distribution optimal 
result established smola murata sch lkopf ller smola proved shall merely try give flavor argument 
basic idea consider estimation location parameter insensitive loss goal maximizing statistical efficiency 
cram rao bound result murata 
efficiency 
gi fisher information information geometrical quantities computed loss function noise model 
means consider distributions unit variance say compute optimal value holds class distributions equation arrives dt 
minimum equation yields optimal choice allows computation corresponding leads equation 
consider example arbitrary polynomial noise models unit variance written exp denotes gamma function 
table shows optimal value different polynomial degrees 
observe lighter tailed sch lkopf smola williamson bartlett table optimal various degrees polynomial additive noise 
polynomial degree optimal distribution smaller optimal tube width increases 
reasonable long tails distribution data outliers appears reasonable early cutoff influence data basically giving data equal influence 
extreme case laplacian noise leads tube width regression 
conclude section caveats asymptotic statement second nonzero sv regression need necessarily converge target measured functions just third proportionality established estimation location parameter context quite sv regression 
parametric insensitivity models return algorithm described section 
generalized svr estimating width tube priori 
far retained assumption insensitive zone tube slab shape 
go step parametric models arbitrary shape 
useful situations noise depends understood set positive functions input space consider quadratic program minimize subject xi yi xi yi xi xi 
new support vector algorithms calculation analogous section shows wolfe dual consists maximizing expression subject constraints modified constraints linear xi 
experiments section simplified version optimization problem drop term objective function equation equation 
render problem symmetric respect edges tube 
addition 
leads wolfe dual constraint cf 
equation xi 
note optimization problem section recovered constant function 
advantage setting sides tube computation straightforward instance solving linear system conditions described equation 
general statements harder linear system zero determinant depending functions evaluated xi linearly dependent 
occurs instance constant functions 
case pointless different values constraint see equation implies sums bounded min 
conclude section giving proof generalization proposition optimization problem constraint see equation proposition 

suppose run algorithm data set result upper bound fraction errors 
xi ii 
upper bound fraction svs 
xi observe similarity semiparametric sv models smola sch lkopf modification expansion led similar additional constraints 
important difference setting lagrange multipliers treated equally different signs semiparametric modeling 
sch lkopf smola williamson bartlett iii 
suppose data equation generated distribution continuous expectation modulus continuity satisfying lim 
probability asymptotically fractions svs errors equal asymptotic distribution svs margins regression classification sv algorithm proposed case pattern recognition boser generalized regression vapnik :10.1.1.103.1189
conceptually take view case simpler providing posterior justification started article regression case 
explain introduce suitable definition margin maximized cases 
glance variants algorithm conceptually different 
case pattern recognition margin separation pattern classes maximized svs examples lie closest margin 
simplest case training error fixed done minimizing subject yi xi note pattern recognition targets yi 
regression estimation hand tube radius fitted data space target values property corresponds function feature space 
svs lie edge tube 
parameter occur pattern recognition case 
show seemingly different problems identical cf 
vapnik pontil rifkin naturally leads concept canonical hyperplanes vapnik suggests different generalizations estimation vector valued functions 
definition margin 
normed spaces define margin function fas inf 
zero continuous functions example 
note margin related albeit identical traditional modulus continuity function measures largest difference function values obtained points distance observations characterize functions margin strictly positive 
lemma uniformly continuous functions 
notations positive uniformly continuous 
new support vector algorithms proof 
definition uniformly continuous 
similarly uniformly continuous find implies 
holds uniformly take infimum get 
specialize particular set uniformly continuous functions 
lemma lipschitz continuous functions 
exists proof 
take infimum example sv regression estimation 
suppose endowed dot product 

generating norm 
linear functions see equation margin takes form see note distance smallest parallel due cauchy schwartz case 
fixed maximizing margin amounts minimizing done sv regression simplest form cf 
equation slack variables training data equation consists minimizing subject xi yi 
example sv pattern recognition see 
specialize setting example case 
equal margin defined vapnik canonical hyperplane vapnik 
way data set oriented hyperplane uniquely expressed linear function see equation requiring min 
vapnik gives bound vc dimension canonical hyperplanes terms 
optimal margin sv machine pattern recognition constructed data sch lkopf smola williamson bartlett toy problem 
separate sv classification algorithm constructs linear function satisfying equation 
maximize margin minimize 
follows boser minimize subject yi xi :10.1.1.103.1189
decision function classification takes form sgn 
parameter superfluous pattern recognition resulting decision function sgn change minimize subject yi xi 
understand constraint see equation looks different equation multiplicative additive note regression points xi yi required lie tube radius pattern recognition required lie outside tube see correct side 
points tube yi xi xi yi 
far interpreted known algorithms terms maximizing 
consider guide constructing general algorithms 
example sv regression vector valued functions 
assume linear functions wx matrix new support vector algorithms consequence lemma matrix norm compatible horn johnson matrix norm induced exists unit vector wz equality holds 
see argument example setting 
hilbert schmidt norm ij compatible vector norm problem minimizing subject separate constraints output dimension separates regression problems 
smola williamson mika sch lkopf shown specify invariance requirements imply regularizers act output dimensions separately identically scalar fashion 
particular turns assumption quadratic homogeneity permutation symmetry hilbert schmidt norm admissible 
sv classification saw svr differs svr uses parameters cases useful reparameterization original algorithm worthwhile ask similar change incorporated original sv classification algorithm brevity call svc 
primal optimization problem minimize cortes vapnik subject yi xi 
goal learning process estimate function see equation probability misclassification independent test set risk small 
parameter dispose regularization constant substitute parameter similar regression case proceed follows 
primal problem svc implicitly loss function risk equals probability misclassification 
sch lkopf smola williamson bartlett consider minimization subject cf 
equation yi xi 
reasons shall constant appears formulation 
understand role note constraint see simply states classes separated margin cf 
example 
derive dual consider lagrangian yi xi multipliers 
function respect primal variables maximized respect dual variables 
eliminate compute corresponding partial derivatives set obtaining conditions iyi 
sv expansion see equation nonzero correspond constraint see precisely met kkt conditions cf 
vapnik 
substituting equations incorporating kernels dot products leaves quadratic optimization problem maximize xi xj ij new support vector algorithms subject iyi 
resulting decision function shown take form sgn xi 
compared original dual boser vapnik differences :10.1.1.103.1189
additional constraint similar regression case 
second linear term boser 
longer appears objective function 
interesting consequence quadratically homogeneous 
straightforward verify obtains exactly objective function starts primal function difference constraints extra factor right hand side 
case due homogeneity solution dual scaled straightforward see corresponding decision function change 
may set 
compute consider sets identical size containing svs xi yi respectively 
due kkt conditions equality 
terms kernels xj xj xj 
regression case parameter natural interpretation removed formulate define term margin error 
denote points errors lie margin 
formally fraction margin errors emp yi xi 
sch lkopf smola williamson bartlett denote argument sgn decision function equation sgn position modify proposition case pattern recognition proposition 
suppose real analytic kernel function run svc data result 
upper bound fraction margin errors 
ii 
lower bound fraction svs 
iii 
suppose data see equation generated distribution contains discrete component 
suppose kernel analytic non constant 
probability asymptotically equals fraction svs fraction errors 
proof 
ad 
kkt conditions implies 
inequality equality cf 
equations 
fraction examples 
examples satisfy grow reduce 
ad ii 
svs contribute left hand side 
ad iii 
follows condition apart set measure zero arising possible singular components class distributions absolutely continuous written integrals distribution functions 
kernel analytic nonconstant constant open set constant 
functions constituting argument sgn sv decision function see equation essentially functions class sv regression functions transform distribution distributions lim 
time know class functions behaved covering numbers get uniform convergence supf converges zero probability sample estimate proportion points satisfy 
lim lim sup 
sup converges zero probability 
shows surely fraction points exactly margin tends zero fraction svs equals margin errors 
new support vector algorithms combining ii shows fractions converge surely 
equation means sums coefficients positive negative svs respectively equal conclude proposition holds classes separately 
note argument number svs sides margin asymptotically agree 
connection standard sv classification somewhat surprising interpretation regularization parameter described result proposition 
sv classification leads sv classification set priori leads decision function 
proof 
minimizes function fixes minimize remaining variables change 
obtained solution minimizes function subject constraint 
recover constraint rescale set variables 
leaves constant scaling factor objective function 
case regression estimation see proposition linearity target function slack variables leads outlier resistance estimator pattern recognition 
exact statement differs regression respects 
perturbation point carried feature space 
precisely corresponds input space depends specific kernel chosen 
second referring points outside tube refers margin error points points misclassified fall margin 
shorthand zi xi 
proposition resistance sv classification 
terms svs bound suppose expressed coefficients dual solution 
local movements margin error zm parallel change hyperplane 
proof 
slack variable zm satisfies kkt conditions bertsekas imply 
sufficiently small transforming point zm results slack nonzero sch lkopf smola williamson bartlett updating keeping primal variables unchanged obtain modified set primal variables feasible 
show obtain corresponding set feasible dual variables 
keep unchanged need satisfy substituting zm equation note sufficient condition hold iyi 
assumption nonzero provided sufficiently small equal 
cases feasible solution kkt conditions satisfied 
bertsekas hyperplane parameters solution 
note assumption restrictive may 
sv expansion solution contains multipliers bound conceivable especially discarding requirement coefficients bounded obtain expansion see equation terms subset original vectors 
instance problem solve directly input space suffices linearly independent svs bound order express holds overlap classes svs upper bound 
selection methods proposed probably adapted sch lkopf shawe taylor cristianini 
practice researchers far cross validation 
clearly done svc 
shall propose method takes account specific properties svc 
parameter lets control number margin errors crucial quantity class bounds generalization error classifiers covering numbers measure classifier capacity 
connection give generalization error bound svc terms 
number complications doing best possible way indicate simplest 
result proposition bartlett 
suppose probability distribution training set equation drawn 
probability function class new support vector algorithms probability error classification function sgn independent test set bounded emp ln ln sup fx covering number fx respect usual metric set vectors 
obtain generalization bound svc simply substitute bound emp proposition estimate covering numbers terms margin 
best available bounds stated terms functional inverse slightly complicated expressions 
proposition williamson 
denote br ball radius origin hilbert space covering number class functions br scale satisfies log inf constant 
log consequence fundamental theorem due 
obtains log log 
apply results svc rescale length obtaining margin cf 
equation 
combine propositions 
yields result 
proposition 
suppose svc kernel form 
data points xi feature space live ball radius centered origin 
consequently probability training set see equation svc decision function sgn sch lkopf smola williamson bartlett xi cf 
equation probability test error bounded emp log ln log ln notice general vector feature space 
note set functions proposition differs support vector decision functions see equation comes term 
leads minor modification details see williamson 
better bounds obtained estimating radius optimizing choice center ball cf 
procedure described sch lkopf burges :10.1.1.117.3731
order get theorem form case complex argument necessary see shawe taylor williamson anthony sec 
vi indication 
conclude section noting straightforward extension svc algorithm include parametric models margin xi constraint see equation complete analogy regression case discussed section 
experiments regression estimation 
experiments optimizer loqo 
serendipitous advantage primal variables recovered dual variables wolfe dual see equation double dual variables fed optimizer 
toy examples 
task estimate noisy sinc function examples xi yi xi drawn uniformly yi sin xi xi drawn gaussian zero mean variance stated radial basis function rbf kernel exp 
standard deviation error bars results obtained trials 
risk test error regression estimate computed respect sinc function noise sin dx 
results table figures 
available online www princeton edu 
new support vector algorithms sv regression top bottom 
larger allows points lie outside tube see section 
algorithm automatically adjusts top bottom 
shown sinc function dotted regression tube 
sch lkopf smola williamson bartlett sv regression data noise top bottom 
cases 
tube width automatically adjusts noise top bottom 
new support vector algorithms sv regression vapnik data noise top bottom 
cases 
choice specified priori ideal case 
upper regression estimate biased lower match external noise smola murata sch lkopf ller 
sch lkopf smola williamson bartlett svr different values error constant 
notice decreases errors allowed large large range test error risk insensitive changes 
new support vector algorithms svr different values noise tube radius increases linearly largely due fact enter cost function linearly 
due automatic adaptation number svs points outside tube errors noise free case largely independent sch lkopf smola williamson bartlett svr different values constant 
top decreases regularization decreased large 
little overfitting occurs 
bottom upper bounds fraction errors lower bounds fraction svs cf 
proposition 
bound gets looser increases corresponds smaller number examples relative cf 
table 
new support vector algorithms table asymptotic behavior fraction errors svs 
fraction errors fraction svs notes sv regression largely independent sample size 
fraction svs fraction errors approach respectively number training examples increases cf 
proposition 
gives illustration parametric insensitivity models proposed section 
proper model estimate gets better 
parametric case sin due dp corresponds standard choice svr cf 
proposition 
relies assumption svs uniformly distributed experimental findings consistent asymptotics predicted theoretically got fraction svs errors respectively 
boston housing benchmark 
empirical studies svr reported excellent performance widely boston housing regression benchmark set stitson 
due proposition difference svr standard svr lies fact different parameters versus specified priori 
accordingly goal experiment show svr better svr useful parameter select 
consequently interested kept remaining parameters fixed 
adjusted width exp sch lkopf 

input dimensionality original value corrected case maximal value 
performed runs time set examples randomly split training set examples test set examples cf 
stitson 
table shows wide range note sense obtained performances close best performances achieved selecting priori looking test set 
validation techniques select optimal values performances state art stitson report mse svr anova kernels bagging regression trees 
table shows real world application control fraction svs errors 
sch lkopf smola williamson bartlett table results boston housing benchmark top svr bottom svr 
automatic mse std errors svs mse std errors svs note mse mean squared errors std standard deviations thereof trials errors fraction training points outside tube svs fraction training points svs 
classification 
regression case difference svc svc lies fact select different parameter priori 
able obtain identical performances 
words svc reproduce excellent results obtained various data sets svc overview see sch lkopf burges smola 
certainly worthwhile project restrict showing toy examples illustrating influence see 
corresponding fractions svs margin errors listed table 
discussion new class sv algorithms parameterized quantity lets control number svs errors 
described svr new regression algorithm shown facing page 
svr different values gaussian kernel width exp 
kernel wide results underfitting tube rigid gets larger needed accomodate fraction points increases significantly 
bottom seen speed uniform convergence responsible asymptotic statement proposition depends capacity underlying model 
increasing kernel width leads smaller covering numbers williamson faster convergence 
new support vector algorithms useful practice 
gave theoretical results concerning meaning choice parameter 
applied idea underlying sv regression develop sv classification algorithm 
just regression counterpart algorithm interesting prac sch lkopf smola williamson bartlett toy example prior knowledge dependence noise 
additive noise multiplied function sin 
top function parametric insensitivity tube section 
bottom svr standard tube 
new support vector algorithms table fractions errors svs margins class separation toy example depicted 
fraction errors fraction svs margin note upper bounds fraction errors lower bounds fraction svs increasing allowing errors increases margin 
tical theoretical point view 
controlling number svs consequences run time complexity evaluation time estimated function scales linearly number svs burges training time chunking algorithm vapnik complexity increases number svs possible data compression applications characterizes compression ratio suffices train algorithm svs leading solution sch lkopf generalization error bounds algorithm directly optimizes quantity give generalization bounds 
turn perform structural risk minimization 
asymptotically directly controls number support vectors give leave generalization bound vapnik 
toy problem task separate circles disks solved sv classification parameter values ranging top left bottom right 
larger select points allowed lie inside margin depicted dotted lines 
kernel gaussian exp 
sch lkopf smola williamson bartlett regression pattern recognition case enabled dispose parameter 
regression case accuracy parameter pattern recognition regularization constant abolished regression case open problem 
note algorithms fundamentally different previous sv algorithms fact showed certain parameter settings results coincide 
believe practical applications convenient specify fraction points allowed errors quantities hard adjust priori accuracy intuitive interpretation 
hand desirable properties previous sv algorithms including formulation definite quadratic program sparse sv representation solution retained 
optimistic applications new algorithms prove quite robust 
reduced set algorithm osuna girosi approximates sv pattern recognition decision surface svr 
svr give direct handle desired speed 
includes experimental test asymptotic predictions section experimental evaluation sv classification realworld problems 
formulation efficient chunking algorithms sv case studied cf 
platt 
additional freedom parametric error models exploited 
expect new capability algorithms useful situations noise problems financial data analysis general time series analysis applications ller haykin 
priori knowledge noise available incorporated error model try estimate model directly data example variance estimator seifert gasser wolf quantile estimator section 
acknowledgments supported part australian research council dfg ja ja 
ben david elisseeff jaakkola ller platt von sachs vapnik discussions almeida pointing white 
jason weston independently performed experiments sum inequality constraint lagrange multipliers declined offer coauthorship 
aizerman braverman 

theoretical foundations potential function method pattern recognition learning 
automation remote control 
new support vector algorithms anthony bartlett 

neural network learning theoretical foundations 
cambridge cambridge university press 
bartlett 

sample complexity pattern classification neural networks size weights important size network 
ieee transactions information theory 
bertsekas 

nonlinear programming 
belmont ma athena scientific 
boser guyon vapnik 

training algorithm optimal margin classifiers 
haussler ed proceedings th annual acm workshop computational learning theory pp 

pittsburgh pa acm press 
burges 

tutorial support vector machines pattern recognition 
data mining knowledge discovery 
cortes vapnik 

support vector networks 
machine learning 
girosi 

equivalence sparse approximation support vector machines 
neural computation 
horn johnson 

matrix analysis 
cambridge cambridge university press 
huber 

robust statistics 
new york wiley 
haykin 

support vector machines dynamic reconstruction chaotic system 
sch lkopf burges smola eds advances kernel methods support vector learning pp 

cambridge ma mit press 
ller smola tsch sch lkopf kohlmorgen vapnik 

predicting time series support vector machines 
sch lkopf burges smola eds advances kernel methods support vector learning pp 

cambridge ma mit press 
murata amari 

network information criterion determining number hidden units artificial neural network models 
ieee transactions neural networks 
osuna girosi 

reducing run time complexity support vector machines 
sch lkopf burges smola eds advances kernel methods support vector learning pp 

cambridge ma mit press 
platt 

fast training svms sequential minimal optimization 
sch lkopf burges smola eds advances kernel methods support vector learning pp 

cambridge ma mit press 
pontil rifkin 

regression classification support vector machines 
verleysen ed proceedings esann pp 

brussels facto 
sch lkopf 

support vector learning 
munich oldenbourg verlag 
sch lkopf bartlett smola williamson 

support vector regression automatic accuracy control 
niklasson bod ziemke eds proceedings th international conference artificial neural networks pp 

berlin springer verlag 
sch lkopf smola williamson bartlett sch lkopf burges smola 

advances kernel methods support vector learning 
cambridge ma mit press 
sch lkopf burges vapnik 

extracting support data task 
fayyad uthurusamy eds proceedings international conference knowledge discovery data mining 
menlo park ca aaai press 
sch lkopf shawe taylor smola williamson 

support vector error bounds 
ninth international conference artificial neural networks pp 

london iee 
sch lkopf smola ller 

nonlinear component analysis kernel eigenvalue problem 
neural computation 
sch lkopf sung burges girosi niyogi poggio vapnik 

comparing support vector machines gaussian kernels radial basis function classifiers 
ieee trans 
sign 
processing 
seifert gasser wolf 

nonparametric estimation residual variance revisited 
biometrika 
shawe taylor bartlett williamson anthony 

structural risk minimization data dependent hierarchies 
ieee transactions information theory 
shawe taylor cristianini 

margin distribution bounds generalization 
computational learning theory th european conference pp 

new york springer 
smola 

learning kernels 
doctoral dissertation technische universit berlin 
gmd research series birlinghoven germany 
smola sch lkopf 

semiparametric support vector linear programming machines 
kearns solla cohn eds advances neural information processing systems pp 

cambridge ma mit press 
smola murata sch lkopf ller 

asymptotically optimal choice loss support vector machines 
niklasson bod ziemke eds proceedings th international conference artificial neural networks pp 

berlin springer verlag 
smola sch lkopf 

kernel method pattern recognition regression approximation operator inversion 
algorithmica 
smola sch lkopf ller 

connection regularization operators support vector kernels 
neural networks 
smola williamson mika sch lkopf 

regularized principal manifolds 
computational learning theory th european conference pp 

berlin springer verlag 
stitson gammerman vapnik vovk watkins weston 

support vector regression anova decomposition kernels 
sch lkopf burges smola eds advances kernel methods support vector learning pp 

cambridge ma mit press 
new support vector algorithms vapnik 

estimation dependences empirical data russian 
nauka moscow 
english translation springer verlag new york 
vapnik 

nature statistical learning theory 
new york springer verlag 
vapnik chervonenkis 

theory pattern recognition russian 
nauka moscow 
german translation theorie der akademie verlag berlin 
wahba 

support vector machines reproducing kernel hilbert spaces randomized 
sch lkopf burges smola eds advances kernel methods support vector learning pp 

cambridge ma mit press 
white 

parametric statistical estimation artificial neural networks condensed discussion 
cherkassky friedman wechsler eds statistics neural networks 
berlin springer 
williamson smola sch lkopf 

generalization performance regularization networks support vector machines entropy numbers compact operators tech 
rep neurocolt series 
london royal holloway college 
available online www neurocolt com 
received december accepted may 
