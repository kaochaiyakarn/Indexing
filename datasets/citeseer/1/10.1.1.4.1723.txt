kernel mutual information arthur ralf herbrich alex smola april introduce new functions kernel covariance kc kernel mutual information kmi measure degree independence continuous random variables 
guaranteed zero random variables pairwise independent shares property addition approximate upper bound mutual information measured near independence kernel density estimate 
show bach jordan kernel generalised variance kgv upper bound kernel density estimate looser 
suggest addition regularising term kgv causes approach kmi motivates regularisation 
performance kc kmi veri ed context instantaneous independent component analysis ica recovering arti cial real musical signals linear mixing 
authors jean yves discovering gap original reasoning kmi proof francis bach michael jordan providing kernel ica code web various helpful comments advice ica methods applications provided excellent explanation geometric properties canonical correlation 
bene ts helpful discussions christophe andrieu olivier bousquet arnaud doucet john fisher graepel james chih jen lin erik miller peter rayner matthias seeger 
version contains changes formatting minor corrections background section compared originally posted april 
contents ica linear mixing problem statement 
review ica methods 
preprocessing 
maximum likelihood 
kl divergence 
semi parametric entropy estimates kl divergence 
kernel covariance kernel canonical correlation de nitions 
normalised covariance kernel covariance 
concepts similar kernel covariance 
canonical correlation 
approximations mutual information mutual information approximated multivariate gaussian random variables 
mutual information multivariate gaussian random variables mutual information discretised univariate parameters 
multivariate gaussian approximation discretised mutual information kernel density estimate discretised mutual information 
exact expression kernel density estimate 
upper bound kernel density estimate 
practical choice 
alternative upper bound kernel density estimate 
multivariate kc kmi 
contents implementation issues cient contrast computation 
gradient descent stiefel manifold 
experimental results measurement performance 
experiments performance assessment 
general mixtures arti cial data 
performance di cult arti cial problems 
audio signal demixing 

ica stationary random processes 
nonlinear mixtures 
models dependent random variables 
proofs de nitions standard linear algebra results 
miscellaneous de nitions 
matrix inner products projections 
properties determinant 
properties matrix inverse 
eigenvalues eigenvectors 
properties symmetric matrices 
properties positive semi de nite matrices 
derivatives 
normalised covariance equivalent eigenvalue problem 
solution unconstrained 
alternative form unconstrained solution 
solution restricted speci basis 
canonical correlation de nition properties 
derivation projection directions 
properties canonical correlation 
geometric interpretation incorporating sample 
contents link gaussian mutual information 
approximate mutual information discretised distributions 
approximate mutual information gaussians :10.1.1.29.2911
ratio determinants gaussian mutual information 
approximation mutual information near independence 
discussion bach jordan derivation kgv 
computation kernel canonical correlations 
discussion kgv proof 
miscellaneous proofs 
ect norm sums rows 
centered kernel matrix singular 
proof centering matrix idempotent 
basic results information theory 
information theory discrete spaces 
information theory continuous spaces 
cumulants characteristic functions gram charlier expansion 
contents chapter problem separating mixtures signals recover original signals prior mixing studied challenge signal processing 
methods solution generally depend nature signals manner mixed particular criterion known contrast function required determine demixing successful 
assume original signals unknown probability distributions combined scalar mixing process demixing achieved ensuring recovered signals statistically independent 
framework instantaneous ica 
measure statistical independence random variables mutual information random vectors zero random vectors independent 
may interpreted kl divergence dkl jjf joint density product marginal densities quantity generalises readily distributions random variables 
propose quantities mutual information may contrast functions ica 
rst call kernel covariance kc shown zero random variables independent 
second function kernel mutual information kmi upper bound parzen window estimate mutual information zero random variables independent 
functions bear strong resemblance kernel canonical correlation kcc kernel generalised variance kgv introduced bach jordan demonstrate kgv thought looser upper bound parzen window estimate 
important advantage derivation described addresses behaviour contrast functions nite kernel sizes relying limiting argument kernel size approaches zero proof may case require re nement see appendix 
addition approach allows apply established methods selecting kernel size function number observations see instance 
ica framework practical applications 
earliest best known separation multiple audio signals recorded room basic ica superseded algorithms take better account mixing process signal properties reverberation room statistical properties human speech movement sources see instance 
successful application instantaneous linear ica removing eye blinks electronic artefacts eeg recordings isolate weaker signals arising various mental activities study 
ica determine brain regions visualisation applied event related fmri experiments 
compared generalised linear models depend particular form response assumed ica permits identi cation additional regions activation relating stimulus 
applications ica described notably basis function determination natural images nancial data analysis :10.1.1.107.3613
kcc applied nd correlations documents english french identical meanings revealing features best represent semantic information pairs documents common 
chapter 
instantaneous linear ica problem introduced chapter contains review methods previously address 
kc kcc contrast functions variable case derived chapter behaviour independence investigated 
chapter contains principal results study 
multivariate gaussian approximation mutual information variables holds near independence introduced section 
upper bounds parzen window estimate quantity derived section constitute kmi kgv contrast functions 
generalisation variables section 
procedure apply kernel contrast functions ica includes method reducing computational cost gradient descent technique described chapter 
show chapter performance kmi kc contrasts ica competitive kgv kcc contrasts respectively kernel methods outperform traditional ica algorithms 
chapter ica linear mixing chapter describe goal instantaneous independent component analysis ica review approaches problem 
discussion draws numerous existing surveys ica related methods including see discussion older literature topic :10.1.1.107.3613
section describing general framework linear instantaneous ica independent particular method solve 
main methodologies previously ica described section 
problem statement discussion general description problem wish solve 
samples dimensional random vector drawn independently identically distribution vector related random vector dimension scalar mixing process bs matrix full rank refer ica problem instantaneous way describing dual assumptions observation depends sample instant samples drawn independently identically 
components assumed mutually independent model codi es assumption sources generated unrelated phenomena instance component eeg signal brain due electrical noise nearby equipment 
mutual independence de nition de nition mutual independence 
suppose random vector dimension say components mutually independent follows easily random variables pairwise independent mutually independent reverse hold pairwise independence guarantee mutual independence 
square means number sources equal number sensors 
full rank required ensure sources mixed exactly identical coe cients imply sources coincide perfectly 
discussed may consider number sources number sensors change basis may di cult presence noise :10.1.1.29.2911:10.1.1.29.2911
chapter 
ica linear mixing goal recover mixing model fact components mutually independent 
wish estimate inverse matrix recovered vector mutually independent components measured de nition 
turns problem described indeterminate certain respects 
instance measure independence change ordering elements swapped addition components may scaled di erent constant amounts retaining independence respect remaining components 
modify problem de nition slightly choose vb ps permutation matrix scaling matrix words matrix non zero elements diagonals 
associated random vector pss clearly independent components 
mutual independence generally di cult determine 
case scalar mixing able nd unique optimal unmixing matrix pairwise independence elements equivalent recovering mutually independent terms permutation scaling 
due theorem 
theorem mutual independence linear ica 
random vectors dimension related underlying densities contain delta functions 
contain gaussian component 
properties components pairwise independent components mutually independent ps permutation matrix full rank scaling matrix equivalent 
third identi ability limitation addition due permutation scaling 
illustrated case gaussian random variables equal variance 
combined pure rotation matrix cos sin sin cos clearly density factorised manner de nition regardless mutual independence components invert likewise deterministic constant element sample inverted measure independence 
theorem general statement concepts 
theorem independence gaussianity 
random vectors components mutually independent components pairwise independent 
furthermore entries jth column 
gaussian deterministic 
practical instantiation framework provided sample true distribution determination independence empirically basis sample 
selection criteria accomplish goal described sections application test performance kernel covariance kernel mutual information 
note point elements sample drawn independently instance generated random process non zero correlation 
review ica methods outputs di erent times entirely di erent set approaches brought bear instance independent sources modeled generalised autoregressive processes inverse cosh noise distribution allows separation maximum likelihood principles 
alternatively sources may modeled stationary random processes case recovered spectral covariance matrices diagonal nonstationary sources may jointly covariance matrices time point see subject demixing random processes convolutive mixing :10.1.1.29.2911:10.1.1.29.2911
elegant overview ica methods links 
study concentrates entirely case brie address random processes time dependencies chapter describing possible extensions 
review ica methods section describe various approaches previously ica 
problem nding inverse broken smaller steps greater ease solution 
remove mean observations estimate zero mean 
decompose demixing matrix wq whitening matrix orthogonal matrix described section 
generally simpler determine separately compute entirety small loss accuracy results procedure 
remaining parts section introduce main methods determine independent sources recovered possible choices rst approach maximises likelihood section second applies mutual information speci density model section third draws various density estimation techniques compute mutual information section 
describe gradient descent may instance accomplished natural gradient relative gradient techniques result similar algorithms despite di erent motivating principles 
algorithm computing follows performing gradient descent stiefel manifold described section 
preprocessing steps described section may discussion ica instance :10.1.1.107.3613
whiten observations operation qt new observations unit empirical covariance matrix 
estimate demixing matrix wq orthogonal matrix say may written rotation matrix arbitrary row column permutations 
determination remains di cult degrees freedom involved problem opposed degrees freedom estimation remainder section describe way estimate wish population random variable unit covariance matrix may write qt qt tt particular possible separate gaussian processes correlated time 
reasoning population random variables practice empirical estimates 
chapter 
ica linear mixing 
tt tt covariance matrix tt positive de nite real symmetric 
written form tt diagonal matrix real values eigenvalues contains orthonormal eigenvectors means 
trivially seen pre whitening process described statistically cient means estimating see practice performance penalty incurred pre whitening small 
maximum likelihood computation remaining portion rely high order statistics output random vector statistics indicate components pairwise independent 
term contrast function denote expression speci es statistical dependencies elements function high order statistics empirical estimate expression 
solve optimising empirical contrast function 
idea expectation nonlinear function measure independence rst proposed jutten summary early research choice function case mathematically rigorous methods set 
rst approach discuss measuring independence maximum likelihood method 
equivalent infomax algorithm described objective nd orthogonal matrix causes distribution closely approach certain model written independent sources recovered minimising kl divergence dkl jj dkl jj respect see de nition de nition kl divergence theorem properties 
equivalently solve maximising dkl jj 
model 
di erential entropy 
course leaves open nature model chosen 
strategy property signals ought gaussian estimate yields increasingly independent ect know theorem component gaussian non gaussianity property remaining problem de nition 
mind bell sejnowski set densities derivatives generalised logistic sigmoids corresponds assumption elements super gaussian 
practice particular choice contrast known perform poorly sub gaussian signals case stationary points contrast function greater obtained independent addressed second contrast function sub gaussian cases 
associated density models exp sech super gaussian exp exp sub gaussian 
review ica methods model applied separation source signals removal noise eeg recordings 
show method corresponds maximum likelihood approach described instance concise explanation link connection independently :10.1.1.29.2911:10.1.1.29.2911
assume whitened observations generated model distribution accordance model expected log likelihood whitened random vector maximised respect log log dkl jj rst term need considered changes 
comparing note dkl jj dkl jj completes proof 
applying method described know proposed contrast function works model incorrect 
fact shown independently kl divergence exhibits zero gradient respect elements independent :10.1.1.29.2911:10.1.1.29.2911
su cient condition stability point ensures saddle point instance taken score functions de ned guaranteed solution independence global optimum give speci instances errors model formulation cause global minimum contrast far independence 
expect penalised source density model incorrect contrast practice computed empirical expectation samples 
maximum likelihood estimate shown asymptotic ratio output unwanted signal power desired signal power minimised source density model correct 
ects incorrect source density models reduced parametric estimate adapted observations alternative methods adapting source density models include designed near zero kurtosis uses regularised splines approximate departure sources gaussianity :10.1.1.29.2911:10.1.1.29.2911
kl divergence introduce alternative approach determining independence summarised 
case criterion suggested directly theorem pairwise independence components maximised adjustment recover natural permutation scaling 
recall random variables mutually independent joint density written product marginals 
way looking say kl divergence joint density product marginals zero words dkl note model requires approximate density estimate qb 
words det chapter 
ica linear mixing contrast proposed 
discuss link method maximum likelihood algorithms introduced previous section 
decompose contrast dkl jj dkl dkl terms non negative maximum likelihood score minimised model correct required 
correct density model maximum likelihood kl divergence solutions coincide 
advantage kl divergence assumptions regarding source model hand remains problem empirically estimating marginals 
describe methods compute empirical estimate part criterion varies choice simpli cation 
theorem change kl divergence instantaneous linear mixing 
random vectors related kl divergence product marginals written dkl log jdet wj result proved theorem 
recall previous section orthogonal log jdet wj 
addition invariant respect remains estimate entropies 
method computing entropies cumulant expansions gaussian density described 
methods empirically estimating entropies include edgeworth expansion gram charlier expansion 
expansion definition subject constraint whitened contrast approximated dkl constant term respect th order cumulant de ned appendix 
note stability analysis described previous section applied cumulant contrasts 
related contrast function jade algorithm di ers slightly function simplify calculation gram charlier expansion yields dkl 



non cumulant method approximating kl divergence proposed particular interest permits design contrast takes speci features source distributions account aside cumulants providing alternative link kl divergence contrasts derived maximum likelihood 
case observation densities modeled densities maximum entropy subject constraints functions 
describe certain known properties sources denotes expectation respect expectation respect 
right hand equality applies quantities practice estimated samples drawn gloss question ambiguities permutation scaling dealt instance convention source ordering :10.1.1.29.2911:10.1.1.29.2911

review ica methods assumed addition close gaussian distribution highest entropy mean variance 
orthonormal respect metric induced gaussian orthogonal quadratic polynomials rise faster quadratically function argument 
circumstances may write exp accordance 
orthogonality relations required 
gaussian random variable zero mean unit covariance 
single function ica results simple ica algorithms 
desirable property measured function 
properties minimising shown equivalent minimising xg constant minimisation achieved maximising 
write particular row replacing log row allows recover maximum likelihood contrast compare 
shown local maximum minimum chosen recover independent component 
denoting estimate extremum observations proof trace asymptotic covariance matrix minimised log corresponds approximately minimum squares estimate know source densities contrast functions proposed achieve results terms robustness outliers small asymptotic covariance log cosh exp 
rst function recommended general super gaussian sources robust outliers second robust highly supergaussian situations third equivalent kurtosis contrast 
method performing ica quickly robustly contrasts 
semi parametric entropy estimates kl divergence results section describe ways compute entropies context kl divergence contrast 
methods su ciently di erent cumulant nonlinearity methods merit separate discussion represent promising directions research 
kernel density estimate proposed context ica re ned decrease computational cost 
divide operations required modify arbitrary function 
satisfy orthogonality constraints required 
accomplished course derivation expression 
sense 
related method rediscovered independently uses binning fft density estimation method described 
chapter 
ica linear mixing support grid size spacing 
assumed odd notational convenience 
proposed approximation di erential entropy bl blx 

log 

log 
entropy associated discretised approximation distribution 

continuous random variable kernel density estimate sample xm size simple kernel density estimate determines degree smoothing valid probability density see detailed discussion start section 
unfortunately lend computationally cient numerical integration apply binning simplest version involves setting ective number samples grid point proportional number observations fall closer neighbours 
accurate estimate density mean square sense assigning weights grid points immediate vicinity observation weights function distance observation question 
turns equivalent replacing 

cardinal splines order simple binning corresponds 
positive coe cients sum symmetric functions original kernel 
single coe cient sum kernel ect spline kernel 
shall see section method pham bears certain similarities approach 
alternative estimate entropy proposed require density estimate intermediate step order statistics 
estimate represents modi cation proposed fact asymptotically equivalent addition asymptotically cient 
writing sample terms ordered smallest largest entropy approximated log satisfy lim estimate consistent 
estimate smoothed augmenting data gaussian clusters points observation grid search jacobi angles parameterising nd global optimum 
results indicate estimate highly resistant outliers performs better kgv data sets described 
hand performance method sources remains problematic 
relation derives theorem appendix 
chapter kernel covariance kernel canonical correlation chapter focus formulation measures independence contrast functions random variables 
reasoning uses established principles going back study list desirable properties measure statistical dependence random variables include 
de ned random variables constant probability 

independent 
borel measurable functions 
shown measure satisfying constraints sup corr nite positive variance borel measurable 
similar kernel canonical correlation kcc introduced shall see restrictive choice propose di erent measure kernel covariance kc omits second fourth properties context ica rst third properties adequate 
comparing contrast functions described section kernel methods section supremum function class xed nonlinearity shall see guarantees global minimum contrast independence traditional methods contingent accuracy source density model 
section useful de nitions 
section introduce normalised covariance demonstrate quantity measure independence computed rkhs 
related approaches area spectral methods clustering section 
introduce canonical correlation associated interpretation accomplished rkhs section 
de nitions de ning terms concepts needed describe contrast functions 
vectors respectively bounded subset nx bounded subset ny random vectors de ne vectors chapter 
kernel covariance kernel canonical correlation random vectors feature spaces fx fy mappings fx fy feature spaces may reproducing kernel hilbert spaces subspaces associated gaussian laplace kernels consider feature spaces nx ny occasion case feature input spaces coincide 
de ne variance covariance matrices xy xx yy xx xy xy yy assume samples data xm ym fx fy de ning matrices 
xm 
ym obtain empirical estimates xy xx yy vector ones 
results obtained noting multiplying equivalent subtracting column mean column xh xm de ne centered sample matrices xh yh greater conciseness 
writing yields empirical estimate xx see appendix proof idempotent 
interpretation rows xh projection rows space perpendicular discussion appendix proved rank matrix xh rank 
de ne gram matrices inner products centered observations case fx fy reproducing kernel hilbert spaces associated kernels argument kernel speci es kernel pertains fx fy 
normalised covariance kernel covariance gram matrices mm 
mm 
gram matrices centered variables mm hk mm mm hk mm result taken 
normalised covariance kernel covariance section de ne normalised covariance describe properties version 
quantity non form widely partial squares method regression applied problems dimension regressors greater number regressors highly situation encountered chemometrics 
version highly robust nonlinear iterative partial squares algorithm derived case variables space regressors mapped rkhs fx study map outputs fy noted feature spaces commonly kernel methods high dimension large ratio maximum minimum eigenvalues associated gram matrices imply feature space representations observations highly collinear 
partial squares algorithm suited 
giving general description normalised covariance applies feature spaces fx fy reproducing kernel hilbert spaces 
random vectors fx de ned previous section 
wish nd vectors fx fy respectively project covariance projections stationary point respect theorem yields equivalent eigenvalue problem 
theorem stationary points normalised covariance 
stationary points respect cov fx fy solutions eigenvalue problem xy xy proof may appendix 
eigenvalues pairs solution eigenvalue solution eigenvalue replace xy empirical estimate obtain empirical estimate stationary points subscripts associated gram matrices may point redundant required distinguish di erent gram matrices sections 
course terms input output case meaningless require dependency quantities 
chapter 
kernel covariance kernel canonical correlation clear expression eigenvectors non zero eigenvalues written linear combinations centered observations 
de ne coe cient vectors linear combinations xc yd argument thought speci instance representer theorem see 
remainder section consider case fx fy reproducing kernel hilbert spaces intent demonstrating maximum eigenvalue may measure independence 
replacement sides matrix alter non zero eigenvalues light obtain equivalent generalised eigenvalue problem terms centered gram matrices mm mm mm mm mm mm contrast function associated maximum eigenvalue explicitly de ned 
de nition kernel covariance 
feature spaces fx fy reproducing kernel hilbert spaces associated kernels 
kernel covariance de ned fx fy sup fx fy je fx fx kfk fx fy fy kgk fy empirical estimate quantity follows simply de nition 
de nition empirical kernel covariance 
training sample drawn independently identically empirical estimate kernel covariance emp fx fy sup fx fy particular replace observe kernel covariance simply maximum stationary point respect normalised covariance theorem 
demonstrate link kernel covariance independence 
theorem kernel covariance independence 
absolutely bounded continuous functions respective bounded sets nx ny kernel correlation fx fy independent 
proof classical result 

concepts similar kernel covariance proof 
rst show fx fy independent 
quite simple fx fy sup fx fy je sup fx fy je second step independence prove converse 
simplify discussion describe case write vector notation 
strictly positive functions fx fy compactly supported 
case fx fy 
limits lim lim supremum covariance functions fx fy zero follows lim algebra yg constitute borel sets fx fy implies independent 
noting kernel covariance generalised random variables section 
general expression zero random variables pairwise independent 
concepts similar kernel covariance kernel covariance turns similar certain respects number kernel algorithms appropriate choice spaces fx fy contrast ica methods seek maximise kernel covariance correct choice feature space elements 
describe similarities rewrite suitable form 
lemma kernel covariance alternative form 
square empirical kernel covariance de nition written emp fx fy mm mm matrix norm maximum eigenvalue 
proof may appendix 
describe examples demonstrated perform optimisation contrast 
rst algorithm related kernel covariance kernel principal component analysis kpca 
data sample size kpca entails covariance matrix xx feature space fx words solve xx contains eigenvectors columns diagonal matrix eigenvalues 
empirical estimate covariance matrix written factor empirical covariance absorbed eigenvalues 
chapter 
kernel covariance kernel canonical correlation eigenvector non zero eigenvalue written linear combination centered observations eigenvalue problem rewritten hk mm hc hc entry diagonal equivalently hk mm hc know hc non zero de ne second feature space fy kernel consider maximum eigenvalue max associated eigenvector obtain max hk mm hc mm hcc mm mm gram matrix fy mm cc words largest eigenvalue obtained kernel pca maximising kernel covariance optimal choice elements fy constraint kc inequality keep constraint set convex 
second algorithm relates kernel covariance part spectral clustering kernel target alignment framework 
consider case observe data sample size matrix distances observations de ned kernel mm case spectral clustering wish assign point classes points class similar de ned kernel di erent points opposite class 
writing vector containing proposed labels mg point choose ensure mm cc similar 
measure similarity alignment written mm cc mm cc mm mm hcc cc mm cc mm mm inner product de nition alignment simply angle matrices see de nition 
numerator rewritten mm cc mm denominator constant respect optimisation discrete labels intractable relaxed problem solved replace mk nd yields maximum eigenvalue mm return kernel pca framework described earlier 
signs elements provide estimate bearing mind set added account imbalances proportion points class 
alternatively labels wish build kernel mm maximise alignment detail regarding approach may 
canonical correlation results section taken 
particular extensive discussion properties canonical correlation analysis may 
canonical correlation section simply summarises properties derivations relevant requirements ica 
start de ning canonical correlation general case interpretation fx fy reproducing kernel hilbert spaces 
random vectors fx fy de ned section related variance covariance matrices 
set indices determination addressed shortly nd vectors respectively project correlation projections stationary point respect related interpretation attempting nd linear combination elements best predictor linear combination predictable subject uncorrelated linear combinations 
de ne linear variates respective linear combinations random variables correlation corr xy xx yy de nitions lead theorem 
theorem stationary points correlation projections 
stationary points respect known canonical correlations solutions generalised eigenvector problem xy xy xx yy alternative form xx yy proof may appendix 
describe correlation relations canonical variates 
result proved appendix 
theorem correlation relations canonical variates 
xx yy full rank writing xx yy xy diag min fn fx fy coincide solutions pairs note linear variates corresponding di erent roots uncorrelated 
rst canonical correlation largest 
replacing xy xx yy clear projection directions non zero written linear combinations centered observations introduce theorem taken useful describing behaviour canonical variates high dimensional spaces 
chapter 
kernel covariance kernel canonical correlation theorem geometric interpretation canonical variates 
sample matrices de ned vectors empirical estimates linear variates sample 
projection column space projection column space represent centered sample matrices de ned 
proof theorem appendix 
discuss properties canonical correlation fx fy reproducing kernel hilbert spaces 
investigated 
procedure derive section obtain expression kernel canonical correlations kcc terms gram matrices mm mm mm mm mm mm analogy de nition de ne candidate contrast function basis 
de nition kernel canonical correlation 
feature spaces fx fy reproducing kernel hilbert spaces associated kernels 
rst kernel canonical correlation de ned fx fy sup fx fy corr sup fx fy case kernel covariance may specify empirical estimate fx fy max similar de nition recover 
pointed rst canonical correlation similar function maximised alternating conditional expectation algorithm case may replaced linear combination functions note numerator contrast de nition de nition suggests rst kernel canonical correlation useful contrast function proposed bach jordan 
problem kernel canonical correlation form described discussed describe problem main ways solved 
lemma regularisation kernel canonical correlation independent data 
suppose gram matrices mm mm full rank 
non zero solutions regardless proof may appendix 
result come surprise theorem respective dimensions fx fy greater case associated gaussian laplace kernel rank 
words columns span projection column space column space gives resp 
exactly argument justify regularised contrast function fx fy sup fx fy cov var kfk fx var kgk fy 
canonical correlation requires additional parameter complicates model selection problem 
stated regularisation responsible ensuring consistency estimate obtained observations respect regularised population quantity 
comparing de nition kernel covariance de nition note kc di ers kcc contain variance terms var var 
light derivation theorem relies covariance numerator kc kcc contrasts ought perform similarly measure independence observation borne experimental results section 
alternative solution problem described lemma projection directions compute canonical correlations expressed terms restricted set basis functions respective subspaces fx fy spanned entire series mapped observations 
basis functions chosen kernel pca instance 
chapter 
kernel covariance kernel canonical correlation chapter approximations mutual information chapter investigate mutual information contrast function measuring independence 
section introducing mutual information multivariate gaussian random variables closed form solution exists 
describe discrete approximation mutual information continuous univariate random variables discuss relates continuous mutual information 
show discrete mutual information may approximated gaussian mutual information near independence 
general discussion basic principles information theory may appendix 
section deriving parzen window estimate gaussian mutual information random variables 
give upper bound quantity de nes kernel mutual information kmi 
show zero random variables independent 
give detail computation certain data dependent normalising factors kmi 
demonstrate regularised kernel generalised variance kgv proposed upper bound gaussian mutual information looser kmi 
comparison original kgv proof appendix 
section derive generalisations kc kmi univariate random variables 
demonstrate kc kmi zero associated random variables pairwise independent suited application contrast functions ica 
mutual information approximated multivariate gaussian random variables mutual information multivariate gaussian random variables introducing gaussian mutual information relation canonical correlation 
detailed general discussion principles may 
xg yg gaussian random variables respectively mean vector subscripts emphasise gaussian notation introduced reasoning clearer subsequent sections 
chapter 
approximations mutual information covariance matrix mutual information written yg log jcj jc xx jc yy de nition theorem appendix 
useful representation theorem proved appendix 
theorem gaussian mutual information function canonical correlations 
mutual information gaussian random variables xg yg may written yg log min lx canonical correlations stationary points 
simplify ratio determinants jcj jc xx jc yy xx xy yx yy jc xx jc yy jc xx yy yx xy jc xx jc yy yx xy may rearrange slightly obtain expression shown possess useful mathematical properties subsequent discussion 
jcj yy yx xx xy yy yy yx xx xy yy yy yy yx xx xy xx xy yy mutual information discretised univariate parameters section describe discretised approximation mutual information continuous univariate random variables bounded intervals words 
results proofs section taken 
consider grid size range indices denote point grid complete sequences grid coordinates 
assume spacing points axes respectively 

de ne random multinomial random variables distribution grid write complete matrix probabilities xy corresponds probability small interval surrounding grid position 
dx 
dy 

dxdy discretisation denote vector similar de nition 
mutual information de ned log 
mutual information approximated gaussian random variables known represents upper bound discretisation nitely ne 
may write appropriate choice small near independence 
making substitution yields proof may appendix 
multivariate gaussian approximation discretised mutual information results section proof novel 
de ning equivalent multidimensional representation previous section equivalent 
precise de ne functions 

dx dxdy speci instance second formula 
dxdy summary xy diag 
results possible de ne covariances xy xy xx yy de nitions may de ne gaussian random variables xg yg covariance structure mutual information 
prove appendix mutual information gaussian case yg log xy xy may approximated yg chapter 
approximations mutual information appendix 
expression identical approximate mutual information discretised approximation basis possible show multivariate gaussian mutual information approximation mutual information continuous univariate variables arbitrary probability distribution 
kernel density estimate discretised mutual information section describe kernel density estimate approximate mutual information 
proceeding motivate discussion reviewing parzen window estimate properties discussed discussion pertains general case multivariate application requires univariate random variables 
sample size point assumed generated unknown distribution density associated parzen window estimate density written kernel function legitimate probability density function correctly normalised 
may rescale kernel vx term needed preserve theorem describes convergence parzen window estimate true probability density 
theorem convergence parzen window estimate 
km parzen window chosen sample size window size chosen varying increasing associated normalisation constant density estimate converges mean square sense subject kernel km legitimate probability density sup km lim xk km lim lim mv 
properties de nition give indication choose kernel size function number observed samples 
common choices log method requires initial parameter choice particular sample size written case slight abuse index notation obtained cross validation 
parzen window deliberately written way rkhs kernel section fact function link described detail section 
scale component factor 
kernel density estimate discretised mutual information exact expression kernel density estimate return problem described sections 
sample length ym kernel density parzen window estimates kernel argument specify kernel simplify notation 
require approximations terms gaussian mutual information described 
de ne vectors matrix xy expectations computed kernel expressions xy limit 

small implication 

de nes kernel size approximations 
dx 



dxdy 

normalisation condition ds 


proceeding de ne matrices kernel inner products simplify notation 
lm xm 
xm lm ym 
ym chapter 
approximations mutual information write manner indicate rst subscript speci es grid sample rows kernel matrix second subscript grid sample columns 
analogy may de ne matrices ll mm ll mm useful results follow lm 
lm 
rede ne estimates matrices compute gaussian mutual information 
xy 

lm lm lm lm 
diag lm diag lm lm diag lm lm substituting terms seen factors 



cancel 
furthermore matrices inverted full rank required 
comparing may proceed analogy proof strategy appendix obtain kernel density approximation discretised mutual information log min lx xy 

lm lm lm lm 
diag lm 
diag lm lm lm lm lm diag lm lm diag lm lm form mislead supposing expressions numerator denominator expressible riemann integrals limit grid increasingly ne 
correct solutions constant set grid points new grid points added 
section derive upper bound independent discretisation 
upper bound kernel density estimate compute upper bound kernel density estimate discretised mutual information theorem 

kernel density estimate discretised mutual information theorem ect norm sums rows 
symmetric matrix positive elements arbitrary vector elements diag bc proof appendix 
apply result computation upper bound 
de ne quantities min min ll diag ll 



diag lm lm second nal lines 
de ning lm lm lm lm ll ll nd new quantity normalised covariance correlation notation 
nd upper bound independent discretisation geometric properties rkhs interpretation 
de ne representations fx fy grid points feature space representations de ned de nition centering matrix may rewrite xh yh theorem proved appendix 
theorem eigenvalue problem normalised covariance restricted projections xh yh centered matrices respective feature spaces points stationary points respect solutions eigenvalue problem pq pr de ne projection operators pq pr chapter 
approximations mutual information see immediately problem described similar normalised covariance problem additional requirement solutions projected basis spanned columns added scaling factor particular non zero solutions enumerate potentially non zero solutions bear mind projection operations increase nullspace 
insight prove corresponding non zero eigenvalues obtained solutions problems come pairs equal value opposite sign necessary take absolute value 
decomposing solutions 


component perpendicular column space 
component perpendicular column space addition write pq 
pr 
system equations pq 


pr 


considering components span columns obtain pq pq 
pr pr 
pq pr pq 
pr 
conclude solutions written complete proof take account geometric properties feature spaces fx fy particular reproducing kernel hilbert space induced gaussian laplace kernel angle vectors rkhs vectors equal magnitude located positive quadrant 
pq 
pr 
sign required 
drawing results section see log log log motivates introduce new contrast function described de nition gram matrix expression kernel normalised covariance 
de nition kernel mutual information 
sample size consisting points corresponding feature space representations 
upper implied scaling factor case included computation explicit 

kernel density estimate discretised mutual information bound kernel density approximation mutual information near independence kernel mutual information de ned fx fy log non zero solutions mm mm mm mm mm mm fact mm mm positive semide nite may proceed analogy proof strategy appendix obtain convenient form fx fy log mm mm mm mm mm mm mm mm log mm mm kmi inherits important property kernel covariance 
theorem population kmi zero independence 
random variables independent fx fy 
theorem follows theorem bearing mind kernel correlation largest eigenvalue practical choice section upper bound mutual information incorporated constants de ned 
readily obtained practice propose easily computable replacements 
base reasoning fact upper bound regardless grid spacing 
grid ne respect spacing observations coe cients corresponding grid points nearest samples non zero assuming kernel continuous 
state idea formally 
assuming grid su ciently ne de ne arg min jx qj long spacing grid smaller spacing samples analogous de nition de ne gram matrices mm mm grid points vectors entries corresponding recall upper bound written mm mmd mmc mmd closely spaced follows reader reminded maximum choice approaches upper bound approach chapter 
approximations mutual information lm lm ll ll mm mm mm replace hc hd obtain centered gram matrices 
light need compute minima entire grid merely grid points closest observations 
speci cally de ne new constants min mg min mg min mg min mg leftmost term denominator may lower bounded mm diag mm diag mm diag 
xm xm 


diag lm lm may replace constants de nition new constants 
brie return alternative kernel density estimate described section 
aside fact approximate mutual information entropy important di erence respective approaches contrast computed limit nitely small grid size removes need binning 
retain original kernel spline kernel cases 
allows greater freedom choose kernel density appropriate characteristics sources 
alternative upper bound kernel density estimate bach jordan propose related quantities contrast functions ica kernel canonical correlation kcc discussed section kernel generalised variance kgv 
section demonstrate quantity may derived nding upper bound 
multivariate kc kmi 
derivation kernel density estimate takes completely di erent approach proof uses limit kernel nitely small 
event may problems limiting argument see appendix discussion 
simpler case kgv kernel generalised variance fx fy log case stationary points respect mm mmd mm mm generalised eigenvalue solutions 
starting expression theorem show lm lm diag lm lm replacing right hand term left hand term denominator likewise term get lm lm lm lm lm lm lm lm express solution projection respective grid matrices theorem similar reasoning previous section prove 
contrast function induced stationary points practice interest compute recalling result lemma nd fx fy log words approximation loose helpful result provides guideline loosely may bound terms denominator obtaining applicable empirical contrast function 
follows regularised kernel canonical correlations compute regularised empirical estimate kgv constitute tighter approximation 
particular replacement diag lm lm lm lm ll recover expression correct choice yields regularised kgv 
expect performance kgv kmi similar ica case seen section 
multivariate kc kmi describe contrast function may generalised random variables 
de ne continuous univariate random variables xn assumed bounded intervals fx xn sample size emphasise regularised empirical estimate kgv practice 
chapter 
approximations mutual information joint distribution de ne associated feature spaces fx fxn corresponding kernel class case kernels may di erent identi ed argument 
generalisation concept normalised covariance 
derivation takes similar form appendix bach jordan deal canonical correlations normalised covariances changes proof strategy respects 
vectors unit magnitude respective feature spaces variable normalised covariance feature space vectors xn respectively project 
de ning jth covariance projections ij ij eigenvalues 
yn yn zero ij words pair projections uncorrelated 
feature spaces su ciently rich implies variables pairwise independent theorem fact need determine eigenvalue largest magnitude zero costly compute guaranteed eigenvalue positive 
gives multivariate contrast fx fxn max max instructive compare kcc contrast variables uses smallest eigenvalue matrix correlations diagonal terms equal zero correlation matrix positive eigenvalues 
generalisation covariance rewritten form closely resembles normalised covariance expression rewriting equivalent eigenvalue problem 
cn cn 
eigenvectors retain unit norm due components having unit norm 
may nd empirical estimate quantity writing projection vector contains observations columns follows argument obtain section 
making replacement reasoning applied deriving write terms gram matrices observations giving kn kn 
kn kn 

kn 
require pairwise independence recover independent sources case linear ica see theorem 

multivariate kc kmi hk matrix represents centered feature space representation sample gram matrix describe generalisation kernel mutual information variables 
analogy variable case de nition propose contrast fx fxn log mn min ng min mg additional scaling factor justi ed numerical theoretical grounds 
de ned necessary true near independence reasonable choice contrast function wish zero pairs feature space projections uncorrelated kernel correlation indicate point variables independent 
may shown minor adaptation corresponding proof appendix 
may rewrite factor solution 


bearing mind determinant left hand matrix product eigenvalues 
left hand matrix symmetric trace equal sum eigenvalues theorem 
means mn mn assuming loss generality eigenvalue corresponds max max rewrite log log max log mn log max mn mn mn log log max mn log mn mn log max mn log mn max mn rst reason introducing factor generally causes results fx fxn de ned independence 
scaling factor provide justi cation due course 
chapter 
approximations mutual information penultimate line uses jensen inequality substitute nal line 
function mn log mn max mn convex respect max tangent max max 
convex function greater equal tangent nd log max mn log mn max mn log max max left hand side convex global minimum max 
follows likewise minimised fx fxn point corresponds point pairs covariances zero 
brie outline contrast function relates kl divergence definition section zero random variables pairwise independent 
case gaussian random vector xg segmented kl divergence joint distribution xg product marginal distributions written terms covariance matrices dkl xg xg log jcj jc ii xg xg xg ii results allow generalise reasoning section substitute kernel density estimates xn apply bounding technique section obtain contrast second reason choosing scale details generalisation scope 
chapter implementation issues chapter describe manner kc kmi contrasts may solve instantaneous linear ica problem described section 
implementation comprises components cient computation kc kmi contrasts low rank approximations gram matrices gradient descent space orthogonal matrices follows whitening determination inverse mixing matrix see section 
results summarised detailed discussion low rank decomposition case easier absence variance term kcc kgv contrasts 
cient contrast computation note kc requires determine eigenvalue maximum magnitude mn mn matrix see kmi determinant mn mn matrix speci ed 
reasonable sample size cost computations prohibitive 
describe computational complexity problem may substantially reduced 
note positive semi de nite matrix written lower triangular known cholesky decomposition 
eigenvalues gram matrix decay su ciently rapidly may approximation gram matrix matrix error due approach may measured maximum eigenvalue determined incomplete cholesky decomposition smaller pivots skipped symmetric permutation rows columns course process increase accuracy numerical stability approximation 
method applied decrease storage computational requirements interior point methods svms faster computation kmi kcc contrasts pseudocode algorithms may 
incomplete cholesky decomposition accomplished compute approximate centered gram matrices hk hz hz show low rank decomposition may ciently compute kernel covariance 
substituting chapter 
implementation issues get zn zn 
zn zn 

zn 
may sides diag increasing nullspace generalised eigenvalue problem eliminate diag zn sides 
making changes left zn zn 


tractable eigenvalue problem having dimension procedure may easily recast determinant matrix 
brie consider choose rank precision depends density kernel 
gaussian kernels densities exponential decay rates shown required precision relates rank log demonstrates slow increase rank sample size 
case kgv kcc form contrast causes eigenvalues approximately discarded serves target precision ensure retain constant rank regardless adopt threshold simulations gaussian kernel motivation purely reduction computational cost 
gradient descent stiefel manifold describe method minimise kernel contrast functions possible choices orthogonal demixing matrix whitening process having accomplished 
manifold described matrices known stiefel manifold 
gradient descent functions de ned manifold described 
clear intuitive explanation procedure kindly provided authors constitutes basis description 
particular contrast function kc kmi wish gradient descent whitened mixed observations see section 
nave gradient descent algorithm involve computing derivative updating chosen minimise projecting resulting matrix back stiefel manifold 
particularly cient update largely canceled subsequent projection operation 
attempt nd direction steepest descent stiefel manifold perform update constraint remain manifold 
achieve rst describe set perturbations retain orthogonality choose direction notation diag de nes matrix blocks diagonal zeros 
matrix need square diagonal case de ned manner consistent asymmetry 
gradient descent stiefel manifold steepest descent ascent set nally give expression shifts geodesic direction 

perturbation small norm orthogonal matrix 
remains stiefel manifold 
constraint hold require 

implies words 
skew symmetric 
nd particular 
gives direction steepest change solve 
max arg max 
subject tr 
const 
yields 
max wg proof may 
parameterise displacement geodesic direction 
max initial matrix resulting exp qw 
max ica algorithm implemented modifying matlab code kc kmi contrast functions 
consequently determine approximation gradient contrast making small perturbations possible jacobi rotation recomputing contrast perturbation 
gradient descent accomplished golden search direction steepest descent 
interesting note performance empirically improve kernel algorithms grid search jacobi angles parameterise careful tuning tolerance golden search reduce di erence 
impractical large numbers sources 
note procedures compute hessian stiefel manifold implementations newton method conjugate gradient descent 
addition adaptive algorithm gradient descent stiefel manifold proposed 
application methods improve performance algorithm scope 
geodesic represents shortest path manifold points equivalently acceleration involved moving points geodesic perpendicular manifold constant velocity maintained 
chapter 
implementation issues chapter experimental results chapter examine performance contrast functions kc kmi compares kgv kcc methods address problem linear instantaneous ica 
objective nd estimate wq inverse mixing matrix reader referred section detailed description ica problem require measure distance approximation true inverse amari divergence introduced section 
separate range arti cial signals mixed randomly generated matrices including cases observations corrupted noise 
results compared obtained fast ica jade extended infomax algorithm kcc kgv 
arti cially mix range audio signals representing number musical genres attempt separate 
chapter general observations regarding results give suggestions study 
measurement performance amari divergence de ned index ica algorithm performance 
de nition amari divergence 
matrices ac amari divergence jb max jb jb max jb measure strictly speaking distance metric general matrices possesses certain useful properties shown 
lemma properties amari divergence 
amari divergence matrices properties equality equal permitted scaling permutation thereof described 

arbitrary permutation matrices arbitrary non zero scaling factors 
rp sp amari divergence invariant respect scaling column swaps 
amari divergence generally invariant respect row swaps respect scaling rows columns di erent amounts 
nal property lemma particularly useful context ica causes performance measure invariant output ordering ambiguity see theorem 
chapter 
experimental results experiments performance assessment describe experiments conducted verify performance algorithms 
main purpose compare performance reported tried far possible generate test distributions manner described near gaussianity experiment certain additional investigations performed 
list distributions experiments respective table 
distributions represent broad range behaviours note negative predominate borne mind evaluating performance 
kgv kcc matlab implementations downloadable modi ed implement kmi kc 
precision incomplete cholesky decomposition approximate gram matrices kernel contrasts set choice represents tradeo accuracy computation speed 
speci ed kernel contrast results re ned polishing step kernel size halved convergence gradient descent procedure smaller kernel 
polishing usually caused measurable improvement results 
follow providing results fastica jade extended infomax algorithm additional check algorithm performance 
case fast ica default kurtosis nonlinearity sources speci cally unsuited cases signal alternative choice nonlinearity predominantly negative table choice see section 
jade infomax algorithms likewise default con gurations 
brief investigation form taken various kernel contrast functions selection data table 
contours kgv kc kmi amari divergence plotted describes demixing samples distributions combined product known jacobi rotations 
kernel contrasts demonstration computed gaussian rbf kernel kg exp kx observe contrasts exhibits local minima locations distant independence possesses basin attraction vicinity correct answer 
note contrasts smooth choice kernel size global minima fairly symmetric 
reasons gradient descent algorithm described section converge rapidly global optimum reasonable initialisation point 
solution method di ers generally jade speci ed initialise kernel contrast functions kc kcc kgv kmi bach jordan separating large numbers signals 
initialisation accomplished unit kernel contrast de ation costly polynomial kernel 
signals process repeated times starting di erent initialising matrices 
jade computationally costly initialisation method reliable certain cases sources near gaussian large number outliers exist due noise cause jade 
general mixtures arti cial data describe ica experiments performed distributions table amari divergence measure closeness estimated mixing matrix true matrix 
kernels include gaussian rbf kernel laplace kernel kl exp kx 
experiments performance assessment est 
est 
amari divergence est 
est 
kernel mutual information est 
est 
kernel covariance est 
est 
kernel generalised variance contour plots kernel contrast functions 
top left amari divergence 
top right kernel mutual information 
bottom left kernel covariance 
bottom right kernel generalised variance 
signals length respective distributions choice random combined orthogonal rotation matrix 
matrix expressed product jacobi rotations subscript angle denotes axis rotation occurs 
estimate took values range 
red plot located coordinates corresponding optimal estimate gaussian kernel size cases kgv 
chapter 
experimental results table labels distributions respective 
distributions zero mean unit variance 
label de nition kurtosis student distribution dof double exponential uniform students distribution dof exponential mixture double exponentials symmetric mixture gauss multimodal symmetric mixture gauss transitional symmetric mixture gauss unimodal 
mixture gauss multimodal 
mixture gauss transitional 
mixture gauss unimodal symmetric mixture gauss multimodal symmetric mixture gauss transitional symmetric mixture gauss unimodal 
mixture gauss multimodal 
mixture gauss transitional 
mixture gauss unimodal combined independent sources random mixing matrices condition numbers whitened resulting observations estimating orthogonal de mixing matrix rst results obtained de mixing independently generated samples distribution distribution table investigated 
results samples length table samples length table 
average performance sample case best case kgv followed kmi laplace gauss kernels 
addition kc kcc methods outperform remaining algorithms despite single eigenvalue 
notable extended infomax method performs badly example appears due small number observations 
sample case hierarchy algorithm performance retained kgv kcc outperform kmi kc smaller margin 
notable di erence kmi laplace kernel gaussian kernel large distributions heavy tailed laplace kernel yields best performance double exponential exponential distributions surprising sense parzen window estimate heavy tailed distribution accomplished ciently heavy tailed kernel 
note distribution symmetric unimodal mixture gaussians accurately possible samples 
second experiment consisted de mixing data drawn independently distributions chosen random table 
results table 
note kmi gaussian kernel outperforms kgv nal experiments kmi laplace kernel yields best performance experiments 
hand kgv performs best rst third case number samples small 
behaviour akin large performance lead kgv table compared smaller di erence table suggests kgv performs better smaller number samples expense performance large sample sizes 
detailed look results averages illustrated shows rare cases instances experiment amari divergences substantially greater mean occasionally corresponds poor convergence simple orthogonal matrices mix sources lower variance estimate making problem slightly easier estimating truly random mixing matrix 

experiments performance assessment jade initialise optimisation process kernel contrasts 
ect pronounced cases absent cases 
suggests due local minima contrast caused insu cient observations relative number sources necessarily artefact due poor initialisation 
extended infomax algorithm unable separate signals sample signal case amari error spread uniformly range 
kernel clearly gives superior performance comes increased computational cost eigenvalues associated gram matrices decay slowly gaussian kernel necessitating higher rank incomplete cholesky decomposition maintain performance 
chapter 
experimental results table average amari divergence runs mixtures samples length drawn corresponding distributions table 
laplace kernel size associated gaussian kernel kmi kc cases size kcc kgv gaussian kernels size nal row gives average experiments 
jade imax kcc kc gauss kc kgv kmi gauss kmi 
experiments performance assessment table average amari divergence runs mixtures samples length drawn corresponding distributions table 
laplace kernel size associated gaussian kernel kmi kc cases size kcc kgv gaussian kernels size nal row gives average experiments 
jade imax kcc kc gauss kc kgv kmi gauss kmi chapter 
experimental results table illustration demixing randomly chosen samples length sample distributions drawn independently replacement table 
kc kmi gaussian kernel size experiments 
kcc kgv signals length remaining signals 
cases 
initial guess kernel methods jade cases fast ica due stable output 
rep jade imax kcc kc kc kgv kmi kmi 
experiments performance assessment run index amari divergence run index jade kmi gauss kgv jade kmi gauss kgv amari divergence demixing randomly chosen signals left hand plot right hand plot experiments respectively 
parameters cases identical table 
performance di cult arti cial problems third experiment investigated ects noise added observations selected generating distributions table randomly replacement 
combining signals randomly generated matrix condition number added points chosen equal probability signals random locations various quantities 
results shown left hand plot 
tanh gauss nonlinearities fast ica algorithm resistant outliers 
observe kmi kc resistant outliers kgv kcc contrasts rate increase kc kmi amari divergences function number corrupted points amari divergences high noise levels signi cantly lower 
addition kernel methods perform substantially better remaining methods including designed robustness outliers 
expected jade performed poorly highly noise sensitive cumulants 
additional experiment carried data test sensitivity kcc kgv choice 
replaced recommended samples 
observe right hand plot greatly reduces performance kcc kgv respect kc kmi remain superior fast ica jade extended infomax methods 
fourth experiment concerns ect near gaussianity sources performance various algorithms 
case sources generated members exponential power family exp jxj parameters chosen ensure density appropriately normalised zero mean unit variance 
parameter determines nature distribution super gaussian sub gaussian laplace distribution 
results left hand plot 
zero kurtosis kmi kgv perform best followed kcc kc 
kernel methods exhibit decisive performance lead chapter 
experimental results num 
outliers num 
outliers amari divergence gauss tanh jade imax kcc kc kgv kmi gauss tanh jade imax kcc kc kgv kmi ect outliers performance ica algorithms signals length drawn independently replacement table corrupted random observations outliers sign probability 
point represents average independent experiments 
number corrupted observations signals horizontal axis 
plots kernel contrast functions left hand plot kcc kgv right hand plot 
regions positive near zero kurtosis traditional algorithms notably jade competitive kurtosis negative 
kgv kcc somewhat better vicinity zero kurtosis kmi kc recover rapidly kurtosis increases 
fth experiment addresses ects low kurtosis performance contrast functions ica contrasts rely implicitly choice nonlinearity kurtosis index signal independence 
signals drawn single distribution consisting asymmetric mixture gaussians amplitudes gaussians adjusted give positive negative kurtosis 
results right hand plot 
kernel contrasts una ected near zero kurtosis opposed jade fast ica rely explicitly kurtosis extended infomax method 
surprisingly tanh nonlinearity perform kurtosis pow nonlinearity fast ica 
audio signal demixing nal experiment involved demixing brief extracts various musical sources combined randomly generated matrix manner arti cial signals described previous section 
total di erent extracts taken ica benchmark set provided 
consist second segments sampled khz precision bits represent wide variety musical genres 
adjacent samples musical signal certainly generated independently identically ica algorithms applied successfully problem sense constitutes reasonable benchmark algorithm 
random permutation time indices reduce statistical dependence adjacent samples music improve performance 
summary results table kmi performs best extracts kgv best extracts 
case possible combination di erent 
experiments performance assessment kurtosis amari divergence kurtosis amari divergence tanh jade imax kcc kc kgv kmi pow jade imax kcc kc kgv kmi left hand plot ect near gaussianity performance algorithms signals length drawn range generalised exponential distributions see text 
point represents average independent experiments 
laplace kernel precision kc kmi gaussian kernel size precision kcc kgv 
nonlinearity fast ica due performance 
right hand plot ect near zero kurtosis performance algorithms signals length drawn range mixtures gaussians 
point represents average independent experiments 
gaussian kernel precision kernel contrast functions kcc kgv 
chapter 
experimental results table illustration demixing music segments length taken collection music samples representing average experiments 
details kgv kmi parameters may section 
jade imax kgv kmi amari divergence kgv sources amari divergence num 
results kmi sources histograms amari divergences kgv kmi plotted results obtained unmixing randomly selected music signals 
associated mean values table comparison ica methods 
settings kmi kgv described section 
extracts investigated total experiments results averaged 
gaussian kernel kgv laplace kernel kmi 
cases polishing step applied re ne result 
experiment music segments drawn randomly replacement available extracts results averaged repetitions 
kernel contrast parameters case laplace kernel size increased 
addition polishing step applied kgv kmi caused drop performance contrasts laplace kernel kmi motivated music generally super gaussian 
results table quite similar kgv kmi instructive compare distribution outcomes obtained experiment 
histograms distributions case 
plot reveals markedly di erent distributions amari divergences kgv results tightly grouped mean kmi yields results smaller divergences larger number outliers 
surprising polishing step caused minor increase performance case 
hand larger dimension problem global minimum harder nd diversion local minima 
chapter experiments appear demonstrate ectiveness kernel methods compared jade fast ica extended infomax algorithms 
kernel methods proved resistant noise observations near zero kurtosis near gaussianity 
kmi kgv yield best performance experiments including conducted audio signals 
expected recall section methods assume particular models source densities 
contrast functions exhibit stable extrema independence regardless accuracy underlying density models variance values extrema larger models decrease accuracy 
contrast kernel methods nonlinearities adapt sources determined observations kmi kgv upper bounds parzen window estimates source densities 
parzen window interpretation facilitates kernel choice properties source densities known instance seen laplace kmi performs particularly separating samples generated double exponential exponential distributions performs better gaussian kernel kmi separating heavy tailed music extracts 
computational cost performance higher particularly true laplace kernel occasionally requires retention high rank gram matrix approximation due slow decay associated eigenvalues 
multimodal nature cost function optimised kernel methods necessitates initialisation method pointed earlier methods exist nd starting guesses kernel framework reasonable cost 
choice kgv kmi alternatively kc kcc di cult 
methods proposed appear little data available table cases table 
kgv kcc yield better performance case sources distribution table gap smaller experiment 
kmi kc particular di culty separating student larger number degrees freedom student distribution approaches gaussian 
small sample size near gaussianity cause variance best possible estimate rise making ica problem harder 
appear var kfk fx regularised kcc kgv see section simply kfk fx de nition section allows closely approach minimum variance estimate di cult cases mechanism achieved remains unclear 
variance reduction ect seen 
chapter 
hand kcc kgv appear susceptible noise observations particularly apparent small see 
absence kernel contrasts greatly simpli es model selection especially observations known corrupted noise 
kmi kc perform better cases described table generally di erent sources separated 
performance obtained kgv kcc low sample sizes set drop accuracy number samples sources increases 
apparent tradeo easily explained analysis section requires investigation 
number extensions readily apparent 
instance behaviour kmi studied detail univariate random variables discussion section guarantees zero sources pairwise independent 
particular interest prove section upper bound gaussian mutual information manner described section random variables 
incidentally require link gaussian mutual information discrete mutual information described section variable case extended greater number random variables 
calculations easier removal discretisation step section negated subsequent limiting argument section 
optimisation procedure ica faster instance implementing newton method conjugate gradient descent stiefel manifold simple gradient descent 
guarantee empirical estimates kmi kc converge population expressions requires application concentration inequalities 
possible rademacher averages bound deviation kc expected value applying theorem 
theorem rademacher bound deviation function expectation class uniformly bounded functions probability measure exists universal constant probability exp random draw sup rm rm sup 
application result predicting kmi performance clear kmi product multiple kc type quantities 
generally necessary investigate methods model selection choosing kernel size type kc kmi 
presently known performance ectively tuned simple cross validation bounds derived concentration inequalities properties parzen window estimates discussed 
performance algorithm context ica entails higher computational cost methods investigated 
interest compare semi non parametric entropy approximations proposed ica adapt source density estimates observations 
real life problems neatly linear ica framework outline ways kernel contrasts improve performance di cult signal separation problems 

ica stationary random processes ica stationary random processes rare practice encounter signals depend previous outputs 
real signals drawn random processes statistical dependencies observations di erent times 
random processes may stationary meaning statistical properties instance mean correlation change time may nonstationary 
cases time dependence greatly assists separating signals independent components idea independence di erent random processes hold samples drawn time samples drawn di erent times 
simple computationally cient criterion described separate linearly mixed independent stationary random processes second order moments 
achieved jointly covariance matrices parameterised various delay values stationarity process causes expectation depend time delay large number delays chosen signals inseparable power spectra sources proportional noted expected cross signal interference estimate sources high indeterminate situation approached spectral overlap increases 
problem avoidable re ning decorrelation solution proposed kernel contrasts depend higher order moments obviously ine ective sources merely gaussian random processes 
require extend kmi random vectors random variables dimension vectors corresponding number di erent lags considered previously words need show kmi remains upper bound dkl xn jjf xn near independence dimension greater remind reader kc kcc known valid contrast functions circumstance perform 
alternative derive general features signals determine independence features 
done cohen class time frequency kernels permits separation sources identical spectra 
cohen class kernels applied describe identifying properties audio samples case train support vector machine separate polynomial phase signals 
nonlinear mixtures di culty modeling nonlinear mixtures considerably greater complexity problem far higher number larger 
way dealing problems assume speci functional form nonlinearity 
alternative simpli ed nonlinearity ith component observation vector ith unknown nonlinearity ith row mixing matrix situation corresponds instance observations distorted sensors 
situation equivalent approach whittle approximation proposed :10.1.1.29.2911:10.1.1.29.2911
likelihood ith independent stationary random processes ng approximated modeling dft series independent gaussians 
gaussians assigned zero mean variances equal power kth frequency 
independence attained inter signal correlation zero frequency components signals addition methods designed speci cally separating signals generated gaussian processes generalised arma models non gaussian noise proposed 
chapter 
analysed shown xed source density model kind described section performs exceptionally badly necessary estimate densities sources computing contrast accomplished kernel density estimate details step di er studies cited 
comparison methods kmi interest 
various orts solve general case problem requires constraints determinate generally case source recovered nonlinear distortion analogue scaling indeterminacy theorem linear mixing case 
example indeterminacy 
unconstrained apply decomposition random variable obtain jt jt 
components output independent uniform distribution 
constraint proposed applies observations require 
conformal mapping complex variable removes ambiguity cases 
di erent approach proposed shown enforcing temporal decorrelation single time step su cient test recovered independent processes simply result decomposition 
rule transforms return independent signals unrelated sources suggests time dependencies crucial rle play general nonlinear mixing 
scheme suggested demixing achieved mapping observations reproducing kernel hilbert space nding low dimensional basis feature space approximately spans subspace formed observations enforcing second order temporal decorrelation projections basis step 
remains unclear guarantees exist ensure recovered independent sources identical aside nonlinear distortion indeterminacy described original sources multiple signals recovered correct ones recognised expected statistical properties 
noted algorithm time dependency samples ignored lending support hypothesis 
applicability kmi clear case post nonlinear mixtures follow better understanding relation 
models dependent random variables kgv model source distribution tree structured graphical model incorporates dependencies components independence assumption ica thought method estimating high dimensional multivariate density densities lower dimension linear mixing 
undirected spanning tree vertices mg source model assumed writing wt signal minimum possible loss due encoding respect choice expressed min dkl jj methods attaining minimum compared 
rst approach kgv replace terms resulting expression minimised respect derivative exist non zero 
basis generally low dimension irrespective number observations 

models dependent random variables second case decomposition section applied pairwise mutual information terms entropies estimated parzen windows expression minimised 
second method generally performs better kgv ective 
gives indication tightness upper bound mutual information provided kgv minimising kgv setting involves maximising upper bounds pairwise mutual information terms 
kmi theory tighter upper bound kgv interesting compare performance kgv setting 
chapter 
appendix proofs de nitions standard linear algebra results results mainly taken results 
proofs rarely provided certain proofs included yield insight main body discussion 
miscellaneous de nitions de nition trace 
trace matrix sum diagonal elements tr de nition rank 
rank matrix maximum number independent columns rows de nition subspaces matrix 
consider matrix fundamental subspaces column space space spanned columns row space space spanned rows left nullspace orthogonal complement column space nullspace orthogonal complement row space theorem rank product matrices 
column space ab spanned column space row space ab spanned row space consequently rank ab min frank rank matrix inner products projections de nition matrix inner product 
matrices space matrices subspace thereof inner product function axioms inner product space satis ed appendix proofs definitions ha bi hb ai ci ha ci hb ci ha ai equality 
function ha bi tr de nition matrix norm 
matrix follows de nition matrix inner product matrix norm written kak ha ai tr 
de nition angle matrices 
possible de ne angle matrices cos ha bi kak kbk emphasise cauchy schwarz inequality holds inner product norm de ned 
theorem projection vector column space 
matrix dimensional vector require rank 
writing squares projection columns dimensional vector pseudoinverse de nition 
proof 
squares estimate subspace spanned columns follows orthogonal columns words follows de nition obtain 
properties determinant theorem determinant product matrices 
matrices det ab det det theorem determinant matrix submatrix 
de ned jmj jaj jdj 
standard linear algebra results theorem determinant matrix containing unit submatrices 
matrices ji ji cbj theorem determinant scaled matrix 
matrix jaj de nition singular matrix 
matrix singular determinant zero 
theorem determinant matrix full rank 
matrix rank jaj 
theorem determinant inverse 
invertible matrix jaj theorem determinant partitioned matrix 
assuming existence jmj jaj ca jdj bd properties matrix inverse de nition matrix pseudoinverse 
matrix matrix 
called pseudoinverse generalised inverse aga square full rank pseudoinverse nite number exist matrix pseudoinverse 
pseudoinverse commonly written theorem pseudoinverse solution linear system 
matrix matrix linear system ax consistent 
gb solution ax theorem inverse matrix product 
invertible matrices ab theorem inverse partitioned matrix 
assuming existence beca eca ca assuming existence cf bd appendix proofs definitions theorem sherman morrison woodbury formulae 
nonsingular uv 
au uv 
uv eigenvalues eigenvectors de nition eigenvalues eigenvectors 
square matrix dimension az eigenvalue corresponding eigenvector 
consequence de nition eigenvalues diagonal matrix elements diagonal 
theorem characteristic polynomial 
roots eigenvalue problem au roots characteristic polynomial ja ij theorem similar matrices 
matrices invertible matrices am said similar 
follows eigenvalue spectrum 
proof 
proof taken strang 
simply apply de nition eigenvalue decomposition au mbm theorem singular value decomposition 
matrix rank may written matrix columns eigenvectors aa non zero eigenvalues matrix columns eigenvectors non zero eigenvalues diagonal matrix containing singular values aa set non zero eigenvalues obtained squaring singular values 

standard linear algebra results properties symmetric matrices theorem eigenvalues symmetric matrix 
eigenvalues symmetric matrix real 
theorem determinant symmetric matrix 
symmetric matrix eigenvalues 
property jaj theorem trace symmetric matrix 
symmetric matrix eigenvalues 
property tr theorem spectral decomposition theorem 
symmetric matrix eigenvalues 
written diag ee vector eigenvalues properties positive semi de nite matrices de nition positive de nite semide nite matrices 
symmetric matrix individual entries positive de nite equality 
matrix positive semide nite non negative complex conjugate symmetric positive de nite equality 
theorem eigenvalues positive de nite matrix 
matrix positive de nite eigenvalues positive 
matrix positive semide nite eigenvalues nonnegative 
theorem cholesky decomposition 
positive de nite positive semide nite matrix may decomposed upper triangular standard lu decomposition procedure 
known cholesky decomposition 
derivatives theorem derivative linear function 
matrix vector derivative ax respect dimensional vector theorem derivative quadratic form 
symmetric matrix ax scalar valued function 
derivative function respect dimensional vector ax appendix proofs definitions normalised covariance equivalent eigenvalue problem solution unconstrained section constitutes proof theorem nd vectors fx fy random vectors respectively project covariance projections stationary point respect results established may instance 
lagrangian problem written xy xy wish stationary points covariance occur zero derivatives expression section true long slater condition holds feasible region feasible region required interior point true constraints 
set derivatives respect zero compute saddle points 
yields xy xy demonstrate premultiplying rst expression second expression xy xy xy xy write solution single eigenvalue equation xy xy completes proof 
alternative form unconstrained solution section derive form eigenvalue solution lemma 
initial form kernel covariance may written max mm mm mm mm introducing new variables mm mm max mm mm lagrangian written mm mm 
normalised covariance equivalent eigenvalue problem slater condition section nd zero derivatives stationary points mm mm respect constrained previous section get system equations mm mm mm mm substituting rst second obtain mm mm mm mm words mm mm mm mm mm nal equality premultiplying mm making change variables mm norm represents largest eigenvalue matrix 
solution restricted speci basis section prove theorem deriving stationary directions normalised covariance case restricted linear combinations speci set basis vectors 
words columns vectors respective feature spaces fx fy hats distinguish solutions unconstrained case section 
knowledge result new constrained solutions context canonical correlation discussed projection directions restricted certain linear combinations observed samples constraints case inequalities coe cients represent restriction speci basis 
constrained empirical estimate theorem cov replace xy empirical estimate centered sample matrices de ned 
lagrangian written derivation solution takes form appendix obtain generalised eigenvalue written consistency section 
may rearranged samples mapped respective feature spaces 
neglect constant factor required section ect reasoning appendix 
appendix proofs definitions may changing values solution row space matrix non zero doing substituting de nitions obtain pq pr de ne projection operators pq pr canonical correlation de nition properties derivation projection directions section derive projection directions canonical correlation 
derivation standard procedure taken 
random variables de ned respectively fx fy nd vectors inputs outputs respectively project correlation projections stationary point respect stationary points index 
linear variates written correlation variates corr xy xx yy covariance submatrices de ned assumed nullspace xx nullspace yy constraints important empirical estimates covariance matrices 
note multiplied arbitrary constants relations hold 
normalise xx yy obtain equivalent lagrangian xy xx yy zero derivatives stationary points xx respect constrained slater condition section 
words xy xx xy yy appendix show premultiplying rst expression second expression xy xx xy xy yy xy 
canonical correlation definition properties write solution single generalised eigenvalue equation xy xy xx yy alternative form xx xy xy yy xx yy properties canonical correlation section nd number canonical correlations random vectors describe properties correlations summarised 
expression assume xx yy respective dimensions full rank xy rank min fn expand solution obtain xx yy xy xy xx xy yy xy xx xy yy xy yy xy xx xy theorem applying spectral decomposition theorem theorem reveals number non zero eigenvalues min 
rst expressions nd xx xy yy xy xx xx xy yy yy xy xx xx xx xx xy yy xx xy yy xx similarly yy xx xy yy xx xy yy yy may notation compact de ning xx xy yy singular value decomposition theorem columns xx eigenvectors bb satisfy xx xx xx columns yy eigenvectors satisfy yy yy yy singular value decomposition satis es xx diag yy xy xx xx xy yy yy diag dimensions follow simplifying assumption fx coincide fy appendix proofs definitions geometric interpretation incorporating sample interpretation kernel canonical correlation taken included crucial understanding kernel cca requires regularisation seen appendix 
sample xm ym size points mapped respective feature spaces fx fy data matrices de ned empirical covariances 
recall equations describing solution canonical correlation problem xy xx xy yy explicitly incorporate empirical expressions covariance matrices get centered sample matrices described 
making substitutions linear variates obtain xb xa ya yb span columns may write steps repeated case 
xb ya xb ya pseudoinverse account fact may full rank 
premultiplying respectively gives xb ya comparing theorem observe projection column space projection column space link gaussian mutual information section demonstrate link canonical correlation coe cients theorem gaussian mutual information 
proof expansion discussion appendix 
note xx yy positive de nite written xx yy ss xx yy xx xx xx 
approximate mutual information discretised distributions matrix square roots symmetric :10.1.1.29.2911
making replacement writing assuming full rank yields xx xy yx yy ss xx xy yx yy xx xy yx yy decomposition necessary preserve symmetry left hand matrix turn guarantees eigenvalues real 
note eigenvalues trying solve changed procedure covariance matrices full rank min pairs eigenvalues 
note xx xy yx yy symmetric determinant may written product eigenvalues xx xy yx yy accordance theorem 
result write ratio determinants xx xy yx yy jc xx jc yy xx xy yx yy xx yy xx xy yx yy xx xy yx yy approximate mutual information discretised distributions section derive approximate mutual information near independence discrete random variables 
proof taken appendix 
de ne joint distribution associated marginal distributions 
writing appropriate choice small near independence nd log log appendix proofs definitions log small approximate mutual information gaussians ratio determinants gaussian mutual information result stated proof appendix 
recall mutual information xg yg yg log jcj jc xx jc yy log xy xy jd expansion xy matrix quanti es departure independence means xy 
theorems obtain jcj jc xx jc yy xy xy jd xy 
xy xy 
xy 
xy 
xy 
reasoning glossed fact rank respectively invertible see expansions recall normalisation requires say ratio determinants unde ned de ne limit matrices full rank 

approximate mutual information gaussians consequently nal column may written negative sum remaining columns 
may get problem adding small diagonal term resulting matrices invertible may arbitrarily close drops zero 
may limiting case determine ratio determinants 
inverses may greatly simpli ed results analogous results theorem expand inverses 
know xy xy xy 
xy 
xy 


analogous result xy xy xy xy xy xy 
xy 
lim xy xy lim xy xy xy xy appendix proofs definitions approximation mutual information near independence result proved appendix 
wish approximate ratio determinants jcj jc xx jc yy xy xy xy xy near independence 
start expansion follows xy xy xy write symmetric may write ee theorem 
log jcj log log jej ji log log tr theorem penultimate step 
discussion bach jordan derivation kgv computation kernel canonical correlations section prove lemma show regularised empirical estimate kernel canonical correlates needed associated high dimension 
restate mm mm mm mm mm mm equivalent mm mm mm mm mm mm 
discussion bach jordan derivation kgv gram matrices full rank 
recall centering matrix solutions correspond solutions mm mm mm mm mm mm ij mm mm mm mm mm mm ij 
roots roots roots 
avoid problem regularised empirical estimate discussed bach jordan 
discussion kgv proof section describe possible problems derivation kernel generalised variance appendix 
assume bounded intervals recall de nition kernel canonical correlations written corr fx fy approximate expansions hx 
hx hy 
grids de ned section 
substituting approximations get replacing random vectors observations obtain section derive kgv 
clear manner population expression note superscripts sans serif indicates inner products random vectors sample appendix proofs definitions relates gaussian approximation discretised mutual information section address problem 
restating argument logarithm form appendix words jcj jc xx jc yy xy xy jd comparing expression observe link gaussian approximation discrete mutual information kgv shown demonstrating xy appropriate conditions similar results terms certain constants related grid spacing neglected see 
consider case kernels gaussian hx exp hy exp bearing mind impulse function limiting case lim exp lim compute covariance structure vectors require expressions expectations expectation individual entries matrix hx hy dxdy convolution product kernels underlying unknown density random variables input space evaluated kernels normalised expectation probability density superscript indicates represents density smoothed 
similarly hx hx dx 
discussion bach jordan derivation kgv assumes 

note exp exp probability density integral equal 
hx dx light observations relations ought hold limit 

long 

grid size small allow approximations 
dx 


xy dxdy 

kernel size small kernel functions approach delta functions squared kernel functions 
ignoring moment problem dealing factors 

case assumed decrease slowly kernel sizes impact limiting argument write population expression kernel generalised variance limit small kernel size lim fx fy lim log lim log penultimate line retains terms remain large limit approach zero words smaller terms set zero expression squared kernel 
problem reveals need enforce opposite assumption 

see section 
appendix proofs definitions miscellaneous proofs ect norm sums rows section prove theorem symmetric matrix positive elements arbitrary vector elements diag bc rst give expansion diag expand bc comparing terms equal bearing mind complete proof 
centered kernel matrix singular section show mm singular 
see rst theorem yields mm jhj mm jhj jhj theorem rank column expressed negative sum remaining columns 
proof centering matrix idempotent de ne hh 
basic results information theory basic results information theory material taken 
omit proofs intent simply give overview useful results de nitions 
information theory discrete spaces signal instance piece music lm may interpreted output random process 
output characterised entropy quantity introduced shannon describe lower bound far signal compressed 
de nition entropy 
entropy discrete random variable possible values ng probabilities log log interpreted average number bits required describe random variable average uncertainty random variable 
measured bits logarithm base nats base subsequent discussion omit subscript log units important context 
joint entropy measure average number bits required represent random variables conditional entropy entropy random variable random variables instance average number bits needed represent known 
discrete random variable possible values ng probabilities discrete random variable possible values mg probabilities de nitions apply 
de nition joint entropy random variables 
log de nition conditional entropy random variables 
xj log ij important note mean conditional entropy taken joint distribution conditional distribution 
simple result de nitions yj xj de ne relative entropy kullback leibler divergence 
de nition kullback leibler divergence 
probability measures de ned nite set dkl jjq log appendix proofs definitions relations entropies related quantities random variables represented entire area circle indicated relevant arrow xjy area shaded red 
number interpretations 
expected log likelihood ratio data generated distribution hypothesis describes average number extra bits sent set code words designed distribution transmit data generated distribution compared average number bits needed code words designed achieve zero probability error 
crucial property kl divergence theorem 
theorem positivity kl divergence 
dkl jjq dkl jjq de nition mutual information random variables 
mutual information takes equivalent forms xj yj dkl jjp log mutual information represents reduction average number bits needed represent saving representing knowledge relative know 
words interpreted average number bits needed transmit information common 
alternatively represents kl divergence penalty incurred assuming independent 
summary various relations 
information theory continuous spaces basic de nitions quantities analogous de ned case discrete spaces continuous spaces 
de ning di erential entropy 

basic results information theory de nition di erential entropy 
continuous random variable de ned support set density di erential entropy de ned log dx distinguished entropy lower case 
interpretation represents logarithm equivalent side length smallest set contains probability detail see chapter 
describe link di erential entropy entropy de nition discrete random variable de nition 
theorem entropy di erential entropy 
support continuous random variable intervals size 
discrete random variable value ng probability 

mean value theorem requires exist 



dx 
consequently shown lim 
log 
analogy joint conditional entropies previous section de ne similar di erential quantities 
de nition joint di erential entropy 
random vector density de ned log dx de nition conditional di erential entropy 
random vectors de ned respectively joint density xjy log xjy dxdy kl divergence continuous random variables mutual information de ned manner consistent previous section 
de nition kl divergence terms densities 
densities de ned dkl jjg log dx theorem gives useful properties kl divergence 
theorem properties kl divergence continuous spaces 
kl divergence densities properties dkl jjg equality dkl jjg invariant respect invertible transform dkl jjg dkl jjg dkl jjg appendix proofs definitions dkl jjg invariant respect random permutation components de nition mutual information continuous random vectors 
random vectors de ned respectively joint density mutual information log dxdy xjy yjx dkl jjf case convention dictates lowercase 
relations di erential entropies kl divergence analogous discrete case 
nice consequence de nition mutual information continuous random variables de ned limit discrete mutual information discretisation parameter 
approaches zero need log 
term 
discretised random variables sense theorem lim 
gaussian case joint di erential entropy gaussian distribution theorem 
theorem di erential entropy gaussian random variables 
random vector gaussian probability density jcj exp joint di erential entropy log jcj useful result gaussian distribution easily shown maximum di erential entropy distribution covariance matrix stated theorem 
theorem gaussian random variables highest di erential entropy 
random vector zero mean covariance matrix log jcj equality gaussian density 
basis theorem may gaussian di erential entropy upper bound entropy discrete random variable 
involves nding continuous piecewise constant random variable entropy discrete variable theorem bound continuous variable 
theorem upper bound entropy gaussian approximation 
consider discrete random variable possible values ng probabilities 
log 
cumulants characteristic functions gram charlier expansion investigate ect entropy applying invertible transforms random variables 
classic result see instance 
theorem ect invertible transform entropy 
consider random vectors related invertible transform jf jacobian log jdet jf proof 
densities related jdet jf making replacement de nition di erential entropy nd log dy jdet jf log jdet jf dy jdet jf log dy jdet jf log det jf dy substitution dx dy jdet jf complete proof 
cumulants characteristic functions gram charlier expansion section introduce cumulants hermite polynomials gram charlier expansion material taken 
rst de ne characteristic function random vector 
de nition characteristic function 
characteristic function random vector 
exp 
cumulant generating function logarithm characteristic function 
case univariate taylor expansion 
ln 
cumulants 
rst cumulants functions relevant moments 






appendix proofs definitions nal expression known kurtosis 
zero mean case 

known application kurtosis measuring non gaussianity kurtosis zero gaussian distributions generally kurtosis measure distribution kurtosis negative distribution called sub gaussian tends peaked possess shorter tails gaussian instance uniform distribution kurtosis positive distribution called super gaussian peaked long tails laplacian distribution 
de ne hermite polynomials 
de nition hermite polynomials 
hermite polynomials de ned recursion xh kh biorthogonal mth order derivatives gaussian distribution 
de nitions specify approximations arbitrary densities expanding gaussian density 
consider edgeworth gram charlier expansions respectively proposed computing ica contrast functions 
de nition edgeworth expansion gaussian density 
probability density zero mean unit variance gaussian random variable mean variance 
may expand 


de nition gram charlier expansion gaussian density 
probability density zero mean unit variance gaussian random variable mean variance 
may expand coe cients functions cumulants rst non zero coe cients gram charlier approximation obtained computing power series expansion cumulant generating function inverse fourier transform 
close gaussian low order terms expansions need retained 
note care taken truncating gram charlier expansion discard terms similar magnitude terms edgeworth expansion decrease uniformly 
exist non gaussian distributions zero kurtosis simply heuristic 
bibliography 
pham jutten 
blind source separation post nonlinear mixtures 
rd international conference ica bss 
ahmed 
signal separation 
phd thesis department engineering university cambridge 

amari 
natural gradient works ciently learning 
neural computation 

amari yang new learning algorithm blind signal separation 
advances neural information processing systems volume pages 
mit press 
bach jordan 
kernel independent component analysis matlab code version 
www cs berkeley edu kernel ica index htm bach jordan 
kernel independent component analysis 
technical report ucb csd university california berkeley 
bach jordan 
kernel independent component analysis 
journal machine learning research 
bach jordan 
tree dependent component analysis 
uncertainty arti cial intelligence volume 
barker 
mutual information gaussian processes 
siam journal applied mathematics 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neural computation 

cardoso moulines 
blind source separation technique second order statistics 
ieee transactions signal processing 
amin 
blind source separation time frequency signal representations 
ieee transactions signal processing 
knutsson 
learning canonical correlations 

fourier transform applications 
mcgraw hill new york 
breiman friedman estimating optimal transformations multiple regression correlation 
journal american statistical association 
mike brookes 
matrix manual 
www ee ic ac uk hp staff matrix intro html watson 
fmri activation visual perception task network areas detected general linear model independent components analysis 
neuroimage 
bibliography 
cardoso 
blind separation real signals jade matlab code version 
ftp tsi enst fr pub jfc algo jade 
cardoso 
infomax maximum likelihood blind source separation 
ieee letters signal processing 

cardoso 
blind signal separation statistical principles 
proceedings ieee 

cardoso 
high order contrasts independent component analysis 
neural computation 

cardoso 
easy routes independent component analysis 
international conference component analysis signal separation volume san diego california 
institute neural computation 
cichocki 
amari 
adaptive blind signal image processing 
john wiley sons new york 
comon 
independent component analysis new concept 
signal processing 
cover thomas 
elements information theory 
john wiley sons new york 
cristianini shawe taylor 
optimizing kernel alignment 
technical report nc tr neurocolt www neurocolt com 
cristianini shawe taylor kandola 
spectral kernel methods clustering 
nips volume cambridge ma 
mit press 

analyse des 
rev inst 
internat 
stat 
das sen restricted canonical correlations 
linear algebra applications 
davy doucet rayner 
optimized support vector machines nonstationary signal classi cation 
ieee signal processing letters 
duda hart stork 
pattern classi cation 
wiley new york second edition 

probability theory examples 
duxbury press belmont california 
edelman arias smith 
geometry algorithms orthogonality constraints 
siam journal matrix analysis applications 
fine 
cient svm training low rank kernel representations 
journal machine learning research dec 
hugo jaakko srel 
fastica matlab code version 
www cis hut fi projects ica fastica girolami 
alternative perspective adaptive independent component analysis algorithms 
neural computation 

theory applications correspondence analysis 
academic press london 
davy doucet rayner 
nonstationary signal classi cation support vector machines 
ieee workshop statistical signal processing proceedings pages 
ieee signal processing society 
bibliography 
mller 
kernel nonlinear blind source separation 
technical report fraunhofer ida 
berlin germany 
harville 
matrix algebra statistician perspective 
springer new york 
hastie tibshirani 
independent components analysis product density estimation 
nips volume cambridge ma 
mit press 
haykin 
neural networks comprehensive foundation 
macmillan new york nd edition 
herbrich 
learning kernel theory algorithms 
mit press cambridge ma 
james 
nonstationary signal processing application reverberation cancellation acoustic environments 
phd thesis department engineering university cambridge 
jutten 
separability nonlinear mixtures temporally correlated sources 
ieee signal processing letters 

unit contrast functions independent component analysis statistical analysis 
proc 
ieee neural networks signal processing workshop pages 

new approximations di erential entropy independent component analysis projection pursuit 
nips volume pages cambridge ma :10.1.1.29.2911
mit press 

fast robust xed point algorithms independent component analysis 
ieee transactions neural networks 
karhunen oja 
independent component analysis 
john wiley sons 
oja 
independent component analysis general nonlinear hebbian learning rules 
signal processing 
pajunen 
nonlinear independent component analysis existence uniqueness results 
neural networks 
plumbley 
optimization orthogonality constraints modi ed gradient method 
unpublished note 
jutten 
blind separation sources adaptive algorithm neuromimetic architecture 
signal processing 
eriksson 
adaptive score functions maximum likelihood ica 
journal vlsi signal processing systems 

kernel multivariate analysis 
master thesis technical university berlin 
lai fyfe 
kernel nonlinear canonical correlation analysis 
international journal neural systems 

lee girolami bell sejnowski 
unifying framework independent component analysis 
computers mathematics applications 

lee girolami sejnowski 
extended infomax algorithm mixed sub gaussian super gaussian sources matlab code version 
www cnl salk edu ica code ext ica download html bibliography 
lee girolami sejnowski 
independent component analysis extended infomax algorithm mixed sub gaussian super gaussian sources 
neural computation 
mackay 
maximum likelihood covariant algorithms independent component analysis 
technical report cavendish laboratory university cambridge 
mangasarian 
nonlinear programming 
siam philadelphia 
thomas reiter horst bischof 
kernel canonical correlation analysis 
technical report tr pattern recognition image processing group tu wien 
mendelson 
methods analysis glivenko cantelli classes 
helmbold williamson editors proceedings colt pages 
miller fisher iii 
independent components analysis direct entropy minimization 
technical report ucb csd computer science division university california berkeley 
papoulis 
probability random variables stochastic processes 
mcgraw hill new york 
pearlmutter 
music samples illustrate context sensitive generalisation ica 
www cs unm edu bap demos html pearlmutter parra 
context sensitive generalisation ica 
proc 
iconip 
pearson editor 
handbook applied mathematics 
van nostrand reinhold new york 

pham 
blind separation instantaneous mixture sources independent component analysis 
ieee transactions signal processing 

pham 
fast algorithms mutual information independent component analysis 
ieee transactions signal processing 
submitted 

pham 
mutual information approach blind separation stationary sources 
ieee transactions information theory 

pham 
cardoso 
blind instantaneous mixtures non stationary sources 
ieee transactions signal processing 
:10.1.1.29.2911:10.1.1.29.2911
pham 
blind separation mixture independent sources quasi maximum likelihood approach 
ieee transactions signal processing 

pham jutten 
separation mixture independent sources maximum likelihood approach 
proc 
eusipco pages 

measures dependence 
acta math 
acad 
sci 


kernel partial squares regression reproducing kernel hilbert spaces 
journal machine learning research 
herbrich smola 
generalised representer theorem 
proceedings annual conference computational learning theory 
smola 
mller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
bibliography smola 
mller 
kernel principal component analysis 
burges smola editors advances kernel methods support vector learning pages 
mit press cambridge ma 
silverman 
density estimation statistics data analysis 
chapman hall new york 
strang 
linear algebra applications 
harcourt brace jovanovich new york third edition edition 
taleb jutten 
source separation post nonlinear mixtures 
ieee transactions signal processing 
van suykens de de moor 
kernel canonical correlation squares support vector machines 
proceedings international conference arti cial neural networks icann 
springer verlag 

test normality sample entropy 
journal royal statistical society series 
shawe taylor cristianini 
inferring semantic representation text cross language correlation analysis 
nips volume cambridge ma 
mit press 
vlassis 
cient source adaptivity independent component analysis 
ieee transactions neural networks 
wold ruhe wold dunne iii 
problem linear regression 
partial squares pls approach generalized inverse 
siam journal scienti statistical computations :10.1.1.29.2911
yang 
amari 
adaptive line learning algorithms blind separation maximum entropy minimum mutual information 
neural computation 
yang 
amari cichocki 
information theoretic approach blind separation sources non linear mixture 
signal processing 

zhu 
zhang 
adaptive rls algorithm blind source separation natural gradient 
ieee signal processing letters 
