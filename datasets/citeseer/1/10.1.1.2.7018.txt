overview ninth text retrieval conference trec ellen voorhees donna harman national institute standards technology gaithersburg md ninth text retrieval conference trec held national institute standards technology nist november 
conference sponsored nist information technology oce defense advanced research projects agency darpa ito advanced research development activity arda oce department defense 
trec latest series workshops designed foster research text retrieval 
workshop series goals encourage research text retrieval large test collections increase communication industry academia government creating open forum exchange research ideas speed transfer technology research labs commercial products demonstrating substantial improvements retrieval methodologies real world problems increase availability appropriate evaluation techniques industry academia including development new evaluation techniques applicable current systems 
previous trecs ad hoc main task large test collections built 
recognition sucient infrastructure exists support researchers interested traditional retrieval task ad hoc main task discontinued trec trec resources focused building evaluation infrastructure retrieval tasks called tracks 
tracks included trec cross language retrieval filtering interactive retrieval query analysis question answering spoken document retrieval web retrieval 
table lists groups participated trec 
groups including participants di erent countries represented 
diversity participating groups ensures trec represents di erent approaches text retrieval 
serves research described detail remainder volume 
section provides summary retrieval background knowledge assumed papers 
section presents short description track complete description track track overview proceedings 
nal section looks forward trec conferences 
text retrieval text retrieval called information retrieval document retrieval concerned locating documents relevant user information need 
traditionally emphasis text retrieval research provide access natural language texts set documents searched large topically diverse 
documents free text specially structured access computers standard database technologies ective solutions problem 
prototypical retrieval task researcher doing literature search library 
environment retrieval system knows set documents searched library holdings anticipate table organizations participating trec australian national university csiro ntt data university oregon health sciences university labs research pam wood bbn technologies queens college cuny chapman university new mexico state university chinese university kong kong cl research rmit university csiro carnegie mellon university groups rutgers university groups oy research cwi netherlands seoul national university dipartimento di informatica pisa cambridge icsi dublin city university southern methodist university university state university new york bu alo fujitsu laboratories sun microsystems communications syracuse university ibm watson research center groups trans ez iit aat ncr imperial college university alberta informatique cdc university california berkeley irit sig university cambridge johns hopkins university university glasgow university iowa kaist university maryland college park katholieke universiteit nijmegen university massachusetts kdd laboratories waseda university university melbourne korea university universite de montreal limsi groups university north carolina chapel hill microsoft groups universite de mitre university padova labs university project usc isi national taiwan university xerox research centre europe llc particular topic investigated 
call ad hoc retrieval task re ecting arbitrary subject search short duration 
examples ad hoc searches web surfers internet search engines lawyers performing patent searches looking precedences case law analysts searching archived news reports particular events 
retrieval system response ad hoc search generally list documents ranked decreasing similarity query 
document routing ltering task topic interest known stable document collection constantly changing 
example analyst wishes monitor news feed items particular subject requires solution ltering task 
ltering task generally requires retrieval system binary decision retrieve document document stream system sees 
retrieval system response ltering task unordered set documents accumulated time ranked list 
text retrieval traditionally focused returning documents contain answers questions returning answers 
emphasis re ection retrieval systems heritage library systems diculty question answering 
certain types questions users prefer system answer question forced number cat 
description provide information bengal cat breed 
narrative item include information bengal cat breed including description origin characteristics breeding program names breeders carrying 
discuss bengal clubs relevant 
discussions bengal tigers relevant 
sample trec topic web track 
wade list documents looking speci answer 
encourage research systems return answers document lists trec introduced question answering task 
test collections text retrieval long history retrieval experiments test collections advance state art trec continues tradition 
test collection abstraction operational retrieval environment provides means researchers explore relative bene ts di erent retrieval strategies laboratory setting 
test collections consist parts set documents set information needs called topics trec relevance judgments indication documents retrieved response topics 
documents document set test collection sample kinds texts encountered operational setting interest 
important document set re ect diversity subject matter word choice literary styles document formats operational setting retrieval results representative performance real task 
frequently means document set large 
trec test collections created previous years ad hoc main tasks gigabytes text documents 
document sets various tracks smaller larger depending needs track availability data 
trec document sets consist newspaper newswire articles government documents federal register patent applications computer science abstracts computer selects zi davis publishing included 
high level structures document tagged sgml document assigned unique identi er called docno 
keeping spirit realism text kept close original possible 
attempt correct spelling errors sentence fragments strange formatting tables similar faults 
topics trec distinguishes statement information need topic data structure retrieval system query 
trec test collections provide topics allow wide range query construction methods tested include clear statement criteria document relevant 
format topic statement evolved trec stable past years 
topic statement generally consists sections identi er title description narrative 
example topic taken year web track shown gure 
di erent parts trec topics allow researchers investigate ect di erent query lengths retrieval performance 
titles topics specially designed allow experiments short queries title elds consist words best describe topic 
title eld di erently topics year web track topics described 
description eld sentence description topic area 
narrative gives concise description document relevant 
participants free method wish create queries topic statements 
trec distinguishes major categories query construction techniques automatic methods manual methods 
automatic method means deriving query topic statement manual intervention whatsoever manual method 
de nition manual query construction methods broad ranging simple automatically derived query manual construction initial query multiple query reformulations document sets retrieved 
methods require radically di erent amounts human ort care taken comparing manual results ensure runs truly comparable 
trec topic statements created person performs relevance assessments topic assessor 
usually assessor comes nist ideas topics interests searches document collection nist prise system estimate number relevant documents candidate topic 
nist trec team selects nal set topics candidate topics estimated number relevant documents balancing load assessors 
standard procedure topic creation changed topics 
topics web track participants concerned queries users type current web search engines quite di erent standard trec topic statements 
participants literal queries submitted web search engine know criteria documents judged 
compromise standard trec topic statements retro tted actual web queries 
nist obtained log queries submitted excite search engine december sample queries deemed acceptable government sponsored evaluation assessors 
assessor selected query sample developed description narrative query 
assessors instructed original query ambiguous cats develop description narrative consistent interpretation original musical cats playing 
searched web document collection estimate number relevant documents topic 
title eld topics contains literal query seed topic 
trec topics description narrative elds topics title eld contains spelling grammatical errors original excite query 
relevance judgments relevance judgments turns set documents topics test collection 
set relevance judgments retrieval task retrieve relevant documents irrelevant documents 
trec uses binary relevance judgments document relevant document 
de ne relevance assessors assessors told assume writing report subject topic statement 
information contained document report entire document marked relevant marked irrelevant 
assessors instructed judge document relevant regardless number documents contain information 
relevance inherently subjective 
relevance judgments known di er judges judge di erent times 
furthermore set static binary relevance judgments provision fact real user perception relevance changes interacts retrieved documents 
despite idiosyncratic nature relevance test collections useful abstractions comparative ectiveness di erent retrieval methods stable face changes relevance judgments 
relevance judgments early retrieval test collections complete 
relevance decision document collection topic 
size trec document sets complete judgments utterly infeasible documents take hours judge entire document set topic assuming document judged just seconds 
jack xu excite released log ftp site ftp excite com pub jack 
trec uses technique called pooling create subset documents pool judge topic 
document pool topic judged relevance topic author 
documents pool assumed irrelevant topic 
judgment pools created follows 
participants submit retrieval runs nist rank runs order prefer judged 
nist chooses number runs merged pools selects runs participant respecting preferred ordering 
selected run top documents usually topic added topics pools 
retrieval results ranked decreasing similarity query top documents documents relevant topic 
documents retrieved top run pools generally smaller theoretical maximum number selected runs documents usually maximum size 
pooling produce test collection questioned documents assumed relevant 
critics argue evaluation scores methods contribute pools de ated relative methods contribute non contributors highly ranked documents 
zobel demonstrated quality pools number diversity runs contributing pools depth runs judged ect quality nal collection 
trec collections biased runs 
test evaluated run contributed pools ocial set relevant documents published collection set relevant documents produced removing relevant documents uniquely retrieved run evaluated 
trec ad hoc collection unique relevant documents increased run point average precision score average 
maximum increase run 
average increase trec ad hoc collection somewhat higher 
similar investigation trec ad hoc collection showed automatic run mean average precision score percentage di erence scores group uniquely retrieved relevant documents 
investigation showed quality pools signi cantly enhanced presence recall oriented manual runs ect noted organizers test collection evaluation information retrieval systems workshop performed manual runs supplement pools 
lack appreciable di erence scores submitted runs guarantee relevant documents strong evidence test collection reliable comparative evaluations retrieval runs 
di erences scores resulting incomplete pools observed smaller di erences result di erent relevance assessors 
evaluation retrieval runs test collection evaluated number ways 
trec ad hoc tasks tasks involve returning ranked list documents evaluated trec eval package written chris buckley research 
package reports di erent numbers run including recall precision various cut levels plus single valued summary measures derived recall precision 
precision proportion retrieved documents relevant recall proportion relevant documents retrieved 
cut level rank de nes retrieved set example cut level de nes retrieved set top documents ranked list 
trec eval program reports scores averages set topics topic equally weighted 
alternative weight relevant document equally give weight topics relevant documents 
evaluation retrieval ectiveness historically weights topics equally users assumed equally important 
precision reaches maximal value relevant documents retrieved recall reaches maximal value relevant documents retrieved 
note theoretical maximum values obtainable average set topics single cut level di erent topics di erent numbers relevant documents 
example topic fewer relevant documents precision score documents retrieved regardless documents ranked 
similarly topic relevant documents recall score documents retrieved 
single cut level recall precision re ect information number relevant documents retrieved 
varying cut levels recall precision tend inversely related retrieving documents usually increase recall degrading precision vice versa 
numbers reported trec eval recall precision curve mean non interpolated average precision commonly measures describe trec retrieval results 
recall precision curve plots precision function recall 
actual recall values obtained topic depend number relevant documents average recall precision curve set topics interpolated set standard recall values 
particular interpolation method appendix de nes evaluation measures reported trec eval 
recall precision graphs show behavior retrieval run entire recall spectrum 
mean average precision single valued summary measure entire graph cumbersome 
average precision single topic mean precision obtained relevant document retrieved zero precision relevant documents retrieved 
mean average precision run consisting multiple topics mean average precision scores individual topics run 
average precision measure recall component re ects performance retrieval run relevant documents precision component weights documents retrieved earlier heavily documents retrieved 
geometrically mean average precision area underneath non interpolated recall precision curve 
reformatted output trec eval submitted run appendix addition ranked results participants asked submit data describes system features timing gures allow primitive comparison amount ort needed produce corresponding retrieval results 
system descriptions included printed version proceedings due size available trec web site trec nist gov 
trec tracks trec track structure begun trec 
tracks serve purposes 
tracks act new research areas rst running track de nes problem really track creates necessary infrastructure test collections evaluation methodology support research task 
tracks demonstrate robustness core retrieval technology techniques frequently appropriate variety tasks 
tracks trec attractive broader community providing tasks match research interests groups 
table lists di erent tasks trec number groups submitted runs task total number groups participated trec 
tasks tracks ered trec diverged trec progressed 
helped fuel growth number participants created smaller common base experience participants participant tends submit runs fewer tracks 
section describes tasks performed trec tracks 
see track reports proceedings complete description track 
web track purpose web track build test collection closely mimics retrieval environment world wide web 
creating collection variety web retrieval strategies investigated 
task track traditional ad hoc retrieval task documents collection web pages 
web track coordinated david hawking colleagues csiro australian national university 
obtained snapshot web internet archive produced subsets spidering 
gigabyte subset known wt main task web track 
separate large task web track gigabyte vlc wt collection set queries selected electronic monk altavista query logs 
see web track report proceedings details large web task 
table number participants task total number distinct participants trec 
trec task ad hoc routing interactive spanish confusion database merging filtering chinese nlp speech cross language high precision large corpus query question answering web total participants topics main web task trec topics 
described earlier topics created especially track 
way relevance judgments relevant relevant highly relevant 
addition assessors asked select best document documents pool topic 
ocial results task scored con ating relevant highly relevant categories additional information collected assessing develop evaluation schemes web retrieval 
groups submitted runs main task web track 
twelve runs manual query construction techniques runs automatic runs original excite query remaining runs automatic runs part topic addition excite query 
runs description eld topic ective corresponding run original excite query 
remember description eld corrected spelling errors original query 
topics sort error original excite query serious 
examples serious errors word queries 
order systems ranked average ectiveness di ered depending evaluation relevant highly relevant documents highly relevant documents 
nding implies retrieving highly relevant documents di erent task retrieving generally relevant documents extent di erent retrieval techniques 
research needed determine precisely techniques better task 
motivation distinguishing highly relevant generally relevant documents widespread belief web users better served systems retrieve highly relevant documents 
reasoning step argued web search engines evaluated ability retrieve best page 
results web track demonstrate best page single relevant document unstable reliable evaluation strategy 
web track assessing complete nist gave set relevant documents highly relevant generally relevant additional assessors asked select best page 
cases de nition best left assessor 
signi cant disagreement assessors best page topic assessors disagreed topics topics assessors picked page topics fewer pages relevant set 
furthermore traditional retrieval system evaluation best document evaluation robust changes caused di erent assessors select best documents 
kendall tau correlation system rankings produced di erent assessors averaged best document evaluation compared traditional evaluation 
cross language clir track clir task ad hoc retrieval task documents language topics di erent language 
goal track facilitate research systems able retrieve relevant documents regardless language document happens written 
trec cross language track chinese documents english topics 
chinese version topics developed cross language retrieval performance compared equivalent monolingual performance 
document set approximately megabytes news articles taken hong kong commercial daily hong kong daily news 
documents available trec topics developed nist assessors 
assessment pools created group rst choice cross language run rst choice monolingual run top documents run 
runs di erent groups submitted track 
thirteen runs monolingual runs 
run cross language run manual run 
ectiveness cross language runs frequently reported percentage monolingual ectiveness 
clir track cross language run submitted bbn group better monolingual run measured mean average precision better submitted monolingual runs 
bbn produced monolingual run better best cross language runs ectiveness english chinese cross language retrieval remains high 
spoken document retrieval sdr track sdr track fosters research retrieval methodologies spoken documents recordings speech 
task track ad hoc task documents transcriptions audio signals 
sdr track run trecs track general structure year 
participants worked di erent versions transcripts news broadcasts judge ects errors transcripts retrieval performance 
transcripts manually produced assumed perfect 
trec transcripts combination human transcripts closed captioning transcripts automatically combined nist rover algorithm automatic transcripts 
baseline transcripts produced automatic speech recognizer available participants 
trec baseline transcript transcript trec track 
transcript produced nist installation bbn rough ready byblos speech recognizer 
recognizer transcripts produced participants recognizer systems 
recognizer transcripts di erent participants available participants perform retrieval runs recognizer transcripts recognizer transcripts cross recognizer runs 
di erent versions transcripts allowed participants observe ect recognizer errors retrieval strategy 
di erent recognizer runs provide comparison di erent recognition strategies ect retrieval 
document collection trec audio portion tdt news corpus collected linguistic data consortium ldc 
corpus contains hours audio representing news shows segmented approximately documents 
di erent versions new topics numbers created track 
standard version topics sentence description terse version topics just words 
trec track trec track focused unknown boundary condition retrieving documents audio stream system story boundaries 
groups participated track submitting total runs 
retrieval results excellent systems nd relevant passages produced variety recognizers full unsegmented news broadcasts terse longer standard queries 
participants retrieval transcripts created recognizer comparable retrieval human transcripts 
table submitted trec query track 
participant description hum variants microsoft ok okapi run query expansion research sab variants smart sun microsystems sun variants nova univ massachusetts variants inquery univ melbourne variants mg query track task query track ad hoc task old trec document topic sets 
focus track absolute retrieval ectiveness variability topic performance 
variety research example see shown di erence retrieval ectiveness retrieval system di erent topics greater average di erence retrieval ectiveness systems topic 
development query speci processing strategies hampered result available topic sets size trec collections small isolate ects caused di erent topics 
query track designed means creating large set di erent queries existing trec topic set rst step query speci processing 
groups participated trec query track group running di erent variants retrieval system 
consists query topics trec topics query category queries 
di erent query categories 

short words selected reading topic statement 

sentence sentence normally line developed reading topic statement possibly relevant documents 

sentence rel sentence developed reading handful relevant documents 
topic statement category query 
relevant documents trec disk construct queries 
developed trec query track remaining developed trec track 
result running version retrieval system documents trec disk 
eighteen submitted total runs submitted track 
table gives short description table gives average minimum maximum mean average precision score computed di erent 
seen wide range scores system individual query formulations pronounced ect retrieval performance 
question answering qa track purpose question answering track encourage research systems return actual answers opposed ranked lists documents response question 
participants received set fact short answer questions searched large document set extract construct answer question 
participants returned ranked list document id answer string pairs question answer string believed contain answer question document supported answer 
answer strings limited bytes bytes depending run type 
question guaranteed answer collection 
individual question received score equal reciprocal rank rst correct response returned responses table average minimum maximum mean average precision scores query track system computed di erent submitted track 
contains queries created trec topics 
average minimum maximum average minimum maximum hum sun ok contained correct answer 
score submission mean individual questions reciprocal ranks 
question answering systems credit retrieving multiple di erent correct answers recognizing know answer 
changes trec track initial running track trec 
trec document set larger consisting news articles trec disks 
test set questions larger consisting questions 
substantial di erence way questions created 
questions created especially track tended back formulations sentence document collection questions selected query logs 
questions taken log questions submitted available nist microsoft 
questions created nist sta excite log web track queries selected suggestions 
cases questions created document set 
questions created nist assessors searched document set nd questions answers test document set 
questions selected candidate questions answer document set 
separate pass nist assessors subset questions answers asked create equivalent re questions 
example question tall empire state building re high empire state building height empire state building empire state building tall total question variants added set nal question set questions 
human assessors read string decided answer string contained answer question 
response judged incorrect 
assessor decided answer supported document returned string 
answer supported document response judged supported 
supported response judged correct 
ocial scoring track treated supported answers incorrect 
groups submitted runs track runs byte limit runs byte limit 
best performing system southern methodist university able extract correct answer time integrating multiple natural language processing techniques abductive reasoning 
score slightly worse result trec scores absolute terms represents signi cant improvement question answering systems 
trec task considerably harder trec task switch real questions tend far ambiguous questions constructed trec task 
smu system answer third best system questions vs questions 
interactive track interactive track rst tracks introduced trec 
inception highlevel goal track investigation searching interactive task examining process outcome 
main problems studying interactive behavior retrieval systems searchers topics generally larger ect search results retrieval system 
task trec track question answering task 
di erent types questions nd xs example name committees regulating nuclear industry compare speci xs example people graduate mba harvard business school mit sloan human searchers maximum minutes nd answer question support answer set documents 
total questions track 
document set set documents question answering track news articles trec disks 
track de ned experimental framework speci ed minimum number searchers order searchers assigned questions set data collected 
framework provide comparison systems sites allow groups estimate ect experimental manipulation free clear main additive ects searcher topic 
groups participated interactive track groups performing minimum number searches 
responses submitted trec topics groups correct answer suggesting minute time limit task challenging 
percentage answer roughly types questions nd questions comparison questions 
filtering track ltering task retrieve just documents document stream match user interest represented query 
main focus track adaptive ltering task 
task ltering system starts just query derived topic statement trec system received relevant documents processes documents time date order 
system decides retrieve document obtains relevance judgment modify query judgment desired 
simpler tasks part track 
batch ltering task system topic relatively large set known relevant documents 
system creates query topic known relevant documents decide retrieve document test portion collection 
routing task system builds query topic statement set relevant documents uses query rank test portion collection 
ranking collection similarity query routing easier problem making binary decision document retrieved batch ltering requires threshold dicult set appropriately 
document set trec ltering task ohsumed test collection 
test documents documents set documents available training particular task allowed training 
topic sets queries ohsumed test collection set mesh headings treated topic statements subset mesh headings 
track existing test collection relevance judgments nist track 
research appropriate evaluation methods ltering runs produce ranked list evaluated usual ir evaluation measures major thrust ltering track 
earliest ltering tracks linear utility functions evaluation metric 
linear utility function system rewarded number points retrieving relevant document penalized di erent number points retrieving irrelevant document 
utility functions attractive directly re ect experience user ltering system 
unfortunately drawbacks functions evaluation measures 
utilities average best possible score topic function number relevant documents topic worst possible score essentially unbounded 
topics relevant documents dominate average single poorly performing topic eclipse topics 
furthermore dicult know set relative worth relevant irrelevant documents 
example utility functions trec track rewarded systems points retrieving relevant document penalized systems points retrieving irrelevant document 
de nes dicult ltering task utility function average behavior trec ltering systems worse baseline retrieving documents 
trec track bounded utility function measure introduced new precision oriented measure 
bounded utility function rewarded systems points relevant document penalized systems point irrelevant document addition set limit worst possible score topic receive 
idea precision oriented measure penalize systems retrieving sucient number documents trec documents 
number relevant retrieved max number retrieved runs di erent groups submitted ltering track 
runs adaptive ltering runs runs batch ltering runs runs routing runs 
trec trec adaptive ltering systems obtained average utilities retrieving adequate number documents 
addition adaptive ltering scores relatively close scores easier routing task demonstrated comparing scores new measure average precision documents retrieved routing runs 
trec trec see changes tracks ered 
spoken document track met goals discontinued 
new track focus content access digital video continue trec interest multimedia retrieval 
query track cease track evolve station 
trec web site serve repository query statements retrieval results produced queries 
allow research track continue time pressures trec deadlines 
remaining tracks continue speci task involved track change 
web track include called navigational topics addition informational topics trec traditionally 
cross language track focus retrieving arabic documents english french arabic topics 
addition fact short answer questions rst years question answering track track feature pilot study questions require information multiple documents combined form correct answer 
task interactive track involve observing subjects live web accomplish speci task 
observations track inform de nition task metrics evaluation plan year track suggested sigir workshop interactive retrieval trec held sigir 
ltering track continue focus adaptive ltering new collection data released reuters see reuters com corpus 
peter bailey nick craswell david hawking 
engineering multi purpose test collection web retrieval experiments 
information processing management 
appear 
david banks paul fan zhang 
blind men elephants approaches trec data 
information retrieval 
nicholas belkin bruce croft 
information ltering information retrieval sides coin 
communications acm december 
chris buckley 
trec eval ir evaluation package 
available ftp ftp cs cornell edu pub smart 
cleverdon mills keen 
factors determining performance indexing systems 
volumes cran eld england 
harabagiu moldovan pasca mihalcea rus 
falcon boosting knowledge answer engines 
voorhees harman 
william hersh paul 
sigir workshop interactive retrieval trec 
sigir forum spring 
hersh buckley leone 
ohsumed interactive retrieval evaluation new large test collection research 
bruce croft van rijsbergen editors proceedings th annual international acm sigir conference research development information retrieval pages 
kato 
overview ir tasks rst workshop 
proceedings workshop research japanese text retrieval term recognition pages 
salton editor 
smart retrieval system experiments automatic document processing 
prentice hall englewood cli new jersey 
linda schamber 
relevance information behavior 
annual review information science technology 
sparck jones van rijsbergen 
report need provision ideal information retrieval test collection 
british library research development report computer laboratory university cambridge 
karen sparck jones 
information retrieval experiment 
butterworths london 
bob travis andrei broder 
need query web search vs classic information retrieval 
www com sh slides sh pro html 
ellen voorhees 
variations relevance judgments measurement retrieval ectiveness 
information processing management 
ellen voorhees donna harman 
overview eighth text retrieval conference trec 
voorhees harman editors proceedings eighth text retrieval conference trec pages 
nist special publication 
electronic version available trec nist gov pubs html 
ellen voorhees donna harman 
overview sixth text retrieval conference trec 
information processing management january 
voorhees harman editors 
proceedings ninth text conference trec 
xu weischedel 
trec cross lingual retrieval bbn 
voorhees harman 
justin zobel 
reliable results large scale information retrieval experiments 
bruce croft alistair mo van rijsbergen ross wilkinson justin zobel editors proceedings st annual international acm sigir conference research development information retrieval pages melbourne australia august 
acm press new york 
