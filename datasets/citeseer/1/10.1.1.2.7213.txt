image rendering handheld cameras quad primitives 
koch christian university kiel germany institute computer science applied mathematics email rk mip informatik uni kiel de novel approach surface patches image rendering 
image sequences acquired freely moving portable multi camera rig extrapolate novel views complex real scenes realtime 
cameras calibrated image sequence dense depth maps computed camera view improved multiview depth estimation technique 
depth maps approximated quad surface patches efficient rendering 
displaying virtual worlds major goals computer graphics 
image base rendering aims images real scenes render new views 
bring quality photographs virtual environments 
today ccd cameras possible capture large complex scenes thousands images short time 
visual components immediately geometrical representation scene form depth maps 
novel approach render complex scenes captured uncalibrated handheld cameras real time 
significant improvements multi view stereo depth estimation allow points graphics primitives triangles 
rendering performance depends density points adopted complexity scene interactively 
editing scene possible 
removing unwanted objects image sequences normally means remove parts image sequence 
object selected removed giving position size 
section related discussed motivation 
rendering stage step complete system designed capture arbitrary scenes virtual environments 
section describe necessary steps precomputations complete chain briefly 
section construction usage points image rendering explained detail 
section show results examples issues concerning quality performance discussed 
concluded section 
motivation previous roots image base rendering date back adelson introduced plenoptic function 
mcmillan bishop published uses plenoptic function images rendering 
levoy hanrahan introduced lightfield rendering enhanced directions 
scene geometry lightfield assumed plane gortler started approximative shape lumigraph 
texture mapping described debevec method rendering known geometry textures image sequences 
methods share problem relying globally consistent model possible obtain 
different methods published years obtain models 
specialized hardware laser range scanner calibrated multi camera systems need controlled environments markers 
structure motion approach pollefeys methods estimate camera calibration sparse geometrical representation scene unknown vmv munich germany november scenes time 
calibration stereo algorithms compute dense depth maps 
discussed plenoptic modeling approach local depth maps correct interpolation ray 
due occlusions errors calibration depth estimation possible construct consistent global model useable example 
proposed geometry 
halfway ray interpolation triangular surface mesh constructed fly fusing depth information selected cameras 
triangle textured cameras 
regions large variations depth distortions visible triangles created connect foreground background 
connection reflect real scene objects grouped loosely depth mesh serving warping surface convex hull scene 
whitted turner points triangles graphics primitives avoid connectivity problem associated complex topology 
lately different researchers took develop approach 
rusinkiewicz levoy multiresolution substitution large meshes approach 
representing textured surfaces splats published pfister improved zwicker 
point base rendering techniques avoid connections primitives suits purpose rendering geometry depth maps topology information 
transfering meshes points surface splats done arbitrary density avoid holes point representation 
true ideal depth maps depth pixel exactly known 
depth maps real image sequences complex scenes far ideal 
holes occlusions remain depth estimation errors result noise 
argued triangles useful interpolate errors holes depth maps 
significant improvements calculation depth maps allows points rendering primitives synthetic footage 
gives chance avoid distortions interpolating triangles quads gain image quality 
prerequisites section explain necessary steps preparation image rendering handheld camera sequences 
modules operate offline need performed acquired scene image capture camera calibration multi view depth estimation generation splats described section 
image capture arbitrary cameras capture scene improvements achieved multi camera systems described 
system consists uncalibrated cameras mounted pole looking direction operating synchronously 
scanning viewpoint surface obtained single walk rigid coupling exploited camera calibration 
camera calibration images acquired hand held multi camera system arbitrary camera motion 
want avoid putting markers scene control scene content 
camera views uncalibrated calibration camera track estimated image sequence 
tracking calibration extended sfm approach pollefeys koch multi camera configuration 
details 
approach intrinsic extrinsic camera parameters image estimated 
standard description projective camera consists matrices dimensional vector contains intrinsic camera parameters focal length aspect ratio image center describes rotation camera space translation vector camera center 
projection matrix projects homogeneous point image image point zp projective depth 
defined projective matrix sfm approach automatically tracks salient features intensity corners images 
calibration results projection matrix camera sequence sparse point cloud tracked feature points 
multi viewpoint depth estimation calibrated image sequence hand obtain dense depth maps multi viewpoint disparity estimation 
calibration epipolar geometry pairs images known restrict correspondence search dimensional search 
extend method koch multi viewpoint depth estimation multi camera configuration 
method ideally suited exploit grid linked depth maps cameras rig 
pair cameras allows fill holes occlusions enhance precision depth maps 
results dense depth maps local scene geometry 
homogeneous image regions possible extract sufficient scene depth 
shows scene complex geometry real view 
depth maps give idea quality improved multi view stereo 
left overview dino scene inside national history museum london 
right images multi camera system 
splats steps image capturing camera calibration depth estimation data sets consisting image projection matrix depth map real camera 
depth map multi view stereo 
left original algorithm described right improved multi view stereo exploiting grid cameras 
representing splats quads point rendering textured models point representation textured easily chosen 
respect api opengl small number possibilities gl points gl point polygons 
standard points gl textured color 
display texture point color corresponding area image 
means point drawn color looked set appropriately prohibits acceleration structures vertex arrays results enormous amount function calls 
worse size points specified screen space means points size screen independent original distance 
overcome drawbacks called point introduced 
point sprite defined homogeneous point size defined object space 
rendering point sprite expanded quad width height size 
projection screen space size depends distance object space 
furthermore texture applied point sprite texture coordinate generation manipulation limited 
standard quads best choice case 
quad specified vertices points stored vertex array reducing number function calls 
quads textured polygons allows efficient projective texture mapping need calculate store texture coordinate explicitly 
creating quads discrete sampled images spacing pixels quads arranged image plane regular grid 
avoid gaps projected visualization non overlapping splats enlargement splat chosen 
typically fraction pixel enlargement choice 
overlap adjacent quads 
ensures virtual views differing real view slightly gaps quads filled 
value depends differences depth quads maximum angle real camera virtual camera 
gaps filled real camera generate new view 
sparse sampled scenes useful increase quads placed image borders exceed borders enlargement 
texturing texture coordinates clamped resulting fragments drawn repeated color values 
errors remain invisible 
spacing pixels pixel rows center quads left setting quads image plane spacing enlargement 
right quads image plane approximate geometry 
done camera pk projection matrix combined described 
quad homogeneous coordinates corners image plane calculated 
distance zi new quad camera center determined depth map center quad 
median filter ad size applied remove depth outliers 
image plane corresponding distance zi depth map calculated point euclidean scene point zi basic setup reprojection illustrated 
different level details created varying spacing repeating procedure 
obtain different detail starting steps si si 
points level detail stored separate vertex arrays saved disk avoid recalculation start program 
camera selection samples samples real cameras evaluated respect virtual camera cv 
step image rendering select real cameras giving information new view 
described different evaluated real camera 
shown 
visibility check verifies real camera shares viewing volume virtual camera projecting selected quads virtual camera 
visible virtual new view real camera marked invalid 
viewing angle angle viewing directions cameras 
viewing angle virtual camera real camera exceeds view real camera axis marked invalid 
cameras marked invalid ranked orthogonal distance viewing direction virtual camera 
closer viewing direction real camera better ranked 
cameras marked invalid proper view interpolation 
remaining ranked cameras selected 
number active cameras current level detail manipulated user interactively control quality performance rendering 
rendering texturing quads ranked cameras traversed procedure followed image real camera texture setup texture matrix draw quads current lod render textured quads original image real camera texture 
done binding current texture opengl 
automatic texture coordinate generation means texture coordinates vertex generated vertex 
linear combination vertex components evaluated component texture coordinate 
direct mapping vertex serves 
intermediate texture coordinate multiplied current texture matrix result taken address texture equivalent 
difference maps points image coordinates xmax ymax texture coordinates suffices decompose adopt fx xmax fx cx fy cy ymax fy cx xmax cy ymax ymax 
setting current texture matrix results homogeneous texture coordinates generated thefly 
maps current texture projectively quads 
drawing precalculated quads lod done sending vertex array corresponding current real camera gl pipeline 
quality performance implemented proposed system opengl tested standard pc linux ghz athlon gb memory geforce ti mb memory 
section discuss evaluation quality performance give examples 
visual quality measuring visual quality rendering algorithms difficult 
decided synthetic virtual scene produce ground truth material 
avoids errors camera calibration depth estimation 
scene vrml reconstruction castle leuven belgium modeled structure motion visualized standard tools 
screenshots different heights simulating walk multi camera system taken 
depth maps taken buffer render hardware projection matrices computed motion parameters virtual camera 
ideal footage chose views removed sequence tried interpolate remaining cameras 
better comparison image subtraction display differences critical region magnified 
due variations depth artefacts located 
mean absolute difference mad value image pixels easy comparison 
best result geometry shown 
multipass texturing errors give mad 
quads nearest camera results holes occlusions activating cameras scene holes filled shown 
spacing enlargement gives mad 
shows critical regions points meshes serves far better geometry approximation 
decreasing spacing result slightly increased quality spacing gives mad 
shows rendered image spacing mad heavily increased computational costs 
compared spacing polygon count increases scene quad pixel 
input view synthetic scene right magnified closeup critical regions left 
geometry multipass texturing results mad quads spacing cameras gives mad 
spacing spacing slightly better mad achieved 
significant enhancements result spacing mad 
computational costs grow factor compared spacing 
evaluation real scenes previous section demonstrated rendering approach gives results synthetic scenes especially ideal depth maps 
main target image base rendering images real cameras view generation 
test approach selected scene high complexity lots occlusions 
dinosaur sequence camera acquisition dinosaur showing results sfm calibration camera positions pyramids feature points 
taken entrance hall national history museum london gives overview hall 
cameras transversal scan alongside skeleton dinosaur recorded 
due difficult lighting conditions tripod dolly avoid motion blur 
positions orientations cameras point cloud calibration shown 
scene divided distinct layers depth background windows skeleton occluding parts background 
view adaptive meshing creates single mesh connecting foreground background results distortions 
point approach resolution quality reconstruction relies quality depth maps mainly 
fine grained structures rib bones modeled distortions avoided background visible bones connected foreground 
problems arise regions depth maps depth estimation failed 
rendering regions filled cameras remain black 
activating cameras leads problem 
matches depth estimation result located quads 
cameras active misplaced quads disturb rendering 
filtering depth maps reduce artefacts significantly small objects removed filtering 
rendered image cameras 
misplaced quads due errors depth estimations results visible critical regions 
performance image rendering typically involves large amounts data 
handle images textures loaded demand 
typically cameras active create new view 
texture checked resides main memory loaded activated 
amount memory textures limited lru mechanism removes textures necessary 
required time load bind texture significantly reduced texture compression 
compressed textures original size saves time loading disk graphic hardware 
preprocessing phase images stored compressed textures quads precomputed stored vertex arrays 
camera ranking data camera 
possible load vertex arrays demand 
techniques able handle image sequences large size 
synthetic castle sequence consists images size 
images depth maps mbyte space 
preprocessing textures mbyte 
creating quads spacing different level details produces mbyte vertex arrays spacing drops mbyte 
framerate achieved rendering depends mainly number quads draw 
total number quads affected number quads current level detail nl number active cameras nc nl nc 
number quads finest lod depends im age size spacing xy nl nl 
example quads 
active cameras resulting framerate fps quads 
activating cameras gain overview large scene easily compensated reducing level detail 
cameras fps setting level detail 
limitations usage quads represent points limited spacing 
case pixels combined quad values depth map represented vectors size values 
possible decrease spacing single depth value substituted quad means data geometry grows factor 
scene results vertices mbyte data camera 
techniques point displacement maps elevation maps pixel precision required 
depth maps uncalibrated cameras sufficient spacing 
missing accuracy noise normally require filtering allows subsample depth maps 
method generate new virtual views handheld camera sequences 
geometry approximated points primitives 
contrast geometry meshes topology assumed reduces distortions complex scenes 
synthetic data results show significantly improved rendering quality 
multi camera systems improved multi view depth estimation methods increased demands quality local depth maps satisfied 
method combining meshes points currently developement increase quality performance 
homogeneous regions depth maps local static mesh created rendered 
regions large variation depth filled points avoiding topological connections distinct objects 
funded european project ist 
images dinosaur scene supplied oliver grau bbc research 
adelson bergen 
computation models visual processing chapter plenoptic function elements early vision 
mit press 
paul debevec yu george 
efficient view dependent imagebased rendering projective 
technical report csd 

koch 
image interactive rendering view dependent geometry 
accepted eurographics computer graphics forum 
eurographics association 
steven gortler grzeszczuk richard szeliski michael cohen 
lumigraph 
computer graphics annual conference series 
reinhard koch mark pollefeys 
plenoptic modeling rendering image sequences taken hand held camera 
proceedings dagm 
koch 
plenoptic modeling scenes sensor augmented multi camera rig 
international workshop digital communication proceedings sept 
koch pollefeys van gool 
multi viewpoint stereo uncalibrated video sequences 
proc 
eccv number lncs 
springer 
koch pollefeys van gool niemann 
calibration handheld camera sequences plenoptic modeling 
proceedings iccv greece 
marc levoy pat hanrahan 
light field rendering 
computer graphics annual conference series 
leonard mcmillan gary bishop 
plenoptic modeling image rendering system 
computer graphics annual conference series 
pfister matthias zwicker jeroen van baar markus gross 
surfels surface elements rendering primitives 
kurt editor siggraph computer graphics proceedings pages 
acm press acm siggraph addison wesley longman 
marc pollefeys reinhard koch luc van gool 
self calibration metric reconstruction spite varying unknown internal camera parameters 
international journal computer vision 
rusinkiewicz levoy 
multiresolution point rendering system large meshes 
siggraph computer graphics proceedings pages 
acm press 
turner whitted marc levoy 
points display primitive 
technical report computer science department university north carolina 
matthias zwicker pfister jeroen van baar markus gross 
surface splatting 
siggraph computer graphics proceedings pages 
acm press 
