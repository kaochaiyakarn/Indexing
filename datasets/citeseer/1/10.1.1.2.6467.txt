journal machine learning research submitted published sufficient dimensionality reduction amir cs huji ac il naftali tishby tishby cs huji ac il school computer science engineering interdisciplinary center neural computation hebrew university jerusalem israel editors isabelle guyon andr elisseeff dimensionality reduction empirical occurrence data fundamental problem unsupervised learning 
studied problem statistics known analysis cross classified data 
principled approach problem represent data low dimension minimal loss mutual information contained original data 
introduce information theoretic nonlinear method finding informative dimension reduction 
contrast previously introduced clustering approaches extract continuous feature functions directly occurrence matrix 
sense automatically extract functions variables serve approximate sufficient statistics sample variable 
method different dimensionality reduction methods specific arbitrary metric embedding 
interpretation method generalized multi dimensional non linear regression fitting regression function dimensional data extract regression functions expectation values capture information variables 
presents new learning paradigm unifies aspects supervised unsupervised learning 
resulting dimension reduction described conjugate dimensional differential manifolds coupled maximum entropy projections 
riemannian metrics manifolds determined observed expectation values extracted features 
geometric interpretation iterative information projection algorithm finding features prove convergence 
algorithm similar method association analysis statistics feature extraction context information theoretic geometric interpretation new 
algorithm illustrated various synthetic occurrence data 
demonstrated text categorization information retrieval proves effective selecting small set features improving performance original feature set 
keywords feature extraction maximum entropy information geometry mutual information association analysis 

classic scenario machine learning statistics identifying source distribution sample generated 
sample xn generated unknown distribution basic task statistical estimation infer value sample 
commonly assumed source distribution belongs parametric family distributions indices parameters may take infinite set values 

unusual notation parameters clear 
amir naftali tishby 
tishby useful approach problem extract small set statistics features functions samples values inferring source parameters 
statistics said sufficient capture relevant information sample identity known certain regularity assumptions non trivial sufficient statistics exist belongs exponential family pitman 
furthermore sufficient statistics written additive functions samples 
belong exponential family exist statistics capture information sample set suggest way quantifying information information theoretic notions show features maximize information extracted 
due link statistical concept sufficiency call method sufficient dimensionality reduction sdr 
alternative view setup generalization problem nonlinear regression 
regression problems assumes relevant information variables captured unknown function normally taken conditional expectation conditional expectations minimize square loss may capture structure variables 
information captured single conditional expectation value 
problem find functions regressors capture information structure variables 
shown problem cast finding dimension reduction captures mutual information way contingency table 
related long line statistics started fisher canonical correlation decomposition haberman log linear models frequency tables 
formalism closely related association models developed goodman see goodman thorough review 
link ideas feature extraction machine learning provide novel information theoretic geometric interpretations algorithms analysis 
furthermore variational formulation problem enables extensions multivariate dimension reduction discriminative feature extraction 
deeply related new information theoretic method relevant clustering information bottleneck method tishby discrete set clusters captures mutual information contingency table generated 
method turn related rate distortion theory lossy compression hand channel coding letter cost hand see cover thomas 
classical coding theorems bottleneck method relevant distortion function channel cost properties induced solely occurrence statistics 
feature extraction problem turns equivalent special matrix factorization problem data joint distribution approximated exponent low rank matrix 
related rich literature factorization positive matrices lee seung hofmann 
describe alternating projection algorithm solving special factorization problem compare performance linear non linear methods 
algorithm repeated application projections csiszar roles constraints lagrange multipliers alternated repeatedly 
feature learning algorithm similar entropy min max principle different setting appeared zhu 

organized follows formulate feature extraction problem finding observable functions variable informative respect vari sufficient dimensionality reduction able 
defining information observations minimal mutual information joint distributions consistent observations problem formulated max min mutual information variational principle 
show problem equivalent finding model special exponential form closest empirical distribution contingency table kl divergence maximum likelihood sense 
iterative algorithm solving problems prove convergence solution 
interesting information geometric formulation algorithm suggested provides covariant formulation extracted features 
provides interesting sample complexity relations cramer rao inequalities selected features 
conclude demonstrating performance algorithm artificial data real life document classification retrieval problem 

problem formulation illustrate motivation consider die unknown outcome distribution 
suppose mean outcome die roll 
information provide probability obtain rolling die 
possible answer question suggested jaynes maximum entropy maxent principle 
denote possible outcomes die roll observation feature function example expected outcome die observation specific outcome 
result rolls xn empirical expected value ni xi 
maxent argues probable outcome distribution maximum entropy distributions satisfying observation constraint distribution depends course actual value observed expectation maxent principle considered problematic implicit assumptions distribution underlying micro states justified 
example fact uniform assumption possible sequences die outcomes consistent observations 
maxent tell features expected values provide maximal information desired unknown case probability obtain 
question meaningful additional random variable denotes parameterizes set possible distributions measures feature quality respect variable 
die case consider parameter probability obtain 
optimal measurement observation case obviously expected number times occurred expectation single feature 
interesting question general procedure finding features 
important step formulating procedure define information measurement feature vector relevant variable information related question predict identity sample generated source distribution 
answer consider set distributions agree expectations margin distributions 
assumption marginals known limiting reliably estimated relatively small joint samples 
assumption simplifies analysis algorithm considerably 
notice true distribution set possible finite sample errors expectations 
distribution set relate expected prediction error shannon mutual information tishby log cover thomas measure mutual information rely prediction process provides bound error rate prediction scheme distribution 
information captured precisely expected values measured features larger mutual information joint distribution consistent measurements distributions produce observations 
natural take minimal possible value distributions consistent observations measure information observations 
distribution minimizes information largest possible prediction error distributions set 
show principle minimum information coincides finding joint distribution maximum entropy subject observations margin constraints hidden assumptions maxent principle 
formally define im measurement im min 
set set distributions satisfy constraints defined desired informative features precisely measurements provide maximal information plugging definition im obtain max min problem arg max min 
notice variational principle define generative statistical model fact model independent approach 
show resulting distribution necessarily special exponential form interpreted generative model class 
need assumption validity model empirical data 
data distribution fact needed order estimate expectations marginals candidate features 
practice expectations estimated finite samples empirical distributions 
machine learning view method requires statistical queries underlying joint distribution kearns 
section show problem finding optimal functions dual problem extracting optimal features capture information variable fact problems solved simultaneously 

approximate bayesian prediction error see slonim tishby 

nature solution sufficient dimensionality reduction show problem formulated equation equivalent problem minimizing kl divergence empirical distribution special family distributions exponential form 
simplify notation omit suffix distributions 
pt stands pt minimizing mutual information equation linear constraints expectations equivalent maximizing joint entropy log constraints additional requirement marginals changed 
due concavity entropy convexity linear constraints exists unique maximum entropy distribution compact domains exponential form cover thomas exp normalization partition function exp functions uniquely determined lagrange multipliers expectation values marginal constraints 
important notice distribution equation maximizes entropy unique freedom choice functions exponent 
freedom restricted linear transformations vector functions respectively long original variables remain show 
distributions form equation viewed distribution class parametrized infinite family functions note treat symmetrically 
henceforth denote class 
discussion shows candidate feature minimum information equation information distribution argued precisely information measurement variable define set information minimizing distributions follows easily shown distribution satisfying constraints see della pietra 

alternately defined 

note unique distribution closure boundary set exponential forms 
address point details 
tishby finding optimal informative features amounts finding distribution maximizes information arg max optimal parameters equation 
easily show dkl dkl kullback liebler divergence equation important consequences 
shows maximizing equivalent minimizing dkl arg min dkl 
second equation shows information larger information original data empirical distribution 
supports intuition model maintains properties original distribution captured selected features 
problem equation minimization subset 
proposition shows fact equivalent minimizing function 
closest distribution data satisfies conditional expectation constraints 
proposition arg min dkl arg min dkl proof need show distribution minimizes right hand side 
generally functional derivative dkl parameters obtains conditions clearly distribution satisfies constraints definition equation conclude 
problem equivalent minimization problem arg min dkl 
equation symmetric respect removing asymmetry formulation equation 
sufficient dimensionality reduction notice minimization problem viewed maximum likelihood fit class known association model statistical literature see goodman context quite different 
interesting write matrix factorization problem matrix exponent element element distribution matrix th column ones matrix th row ones vectors th column th row 
vectors clearly related margin distributions general possible eliminate absorb completely marginals 
consider general form 
maximum likelihood formulation problem guarantees quality fit justifies class principles 
prefer information theoretic model independent interpretation approach 
shown interpretation provides powerful geometric structure analogies 

iterative projection algorithm previous section showed equivalence max min problem equation kl divergence minimization problem equation 
iterative algorithm provably converges local minimum equation solves max min problem 
minimization problem solved number optimization tools gradient descent iterative procedures described goodman 
follows demonstrate information geometric properties optimization procedure constructing general framework general convergent algorithms generated 
algorithm described information geometric notion projections csiszar 
projection distribution set distributions defined distribution minimizes kl divergence dkl 
denote distribution ipr ipr arg min dkl important property projection called pythagorean property see cover thomas ch 

set convex distribution holds dkl dkl ipr dkl ipr focus case set determined expectation values 
dimensional feature function distribution consider set distributions agree expectation values denote 
clearly convex due linearity expectations 
projection case exponential form ipr exp input joint empirical distribution tishby output feature functions result solution variational problem equation local minimum equation matrix factorization equation initialization initialize randomly choosing random 
iterate set set pt ipr pt pt pt 
pt ipr pt pt pt 
halt convergence small change pt achieved 
iterative projection algorithm 
vector lagrange multipliers corresponding expectation constraints 
addition special exponential form pythagorean inequality tight equality csiszar 
property linked notion geodesic lines curved manifold distributions 
describing algorithm additional notations needed pt distribution iterations 
functions pt 
functions pt 
full parameter set pt 
iterative projection algorithm outlined described graphically 
projections exponential distributions applied iteratively fixed expectations fixed expectations 
interestingly projection functions modified lagrange multipliers vice versa second projection 
iterations viewed alternating mappings sets ddimensional functions 
direct goal variational problem 
proceed prove convergence algorithm 
show step reduces dkl pt 
proposition dkl pt dkl pt proof holds sufficient dimensionality reduction iterative projection algorithm dual roles projections determine dual lagrange multipliers iteration 
iterations converge diagram commutative 

pt projection pt set 
pythagorean property equality dkl pt dkl pt dkl pt pt multiplying summing values obtain dkl pt dkl pt dkl pt dkl pt pt dkl pt pt 
elimination dkl pt sides gives dkl pt dkl pt dkl pt pt 
non negativity dkl pt pt desired inequality obtained dkl pt dkl pt note equality obtained iff dkl pt pt 
analogous argument proves dkl pt dkl pt 
easily provable proposition states stationary points algorithm coincide extremum points target function dkl 
proof uses properties projections algorithm characterization extremum point equation 
tishby proposition pt pt corresponding satisfies dkl order see algorithm converges generally local minimum dkl note dkl monotonously decreasing bounded series converges 
difference series see equation dkl pt dkl pt dkl pt pt dkl pt pt converges zero 
infinity get pt pt pt 
limit stationary point iterations proposition follows local minimum dkl 
implementation partial projections description iterative algorithm assumes existence module calculates linear expectation constraints 
general closed form solution projection available successive iterations asymptotically converge solution 
straightforward show projection algorithm uses partial projection ipr module converges minimum 
ipr implemented generalized iterative scaling gis procedure see darroch ratcliff described 
briefly address computational complexity algorithm 
time complexity naturally depends number iterations needed till convergence 
experiments described ran algorithm iterations produced satisfactory results 
projection steps performed gis iterative 
gis iterations projection stopping condition ratio empirical model expectations close 
step gis linear complexity size variable number features 
sdr iteration perform projections iteration performs operations 
run time algorithm influenced implementation projection algorithm 
gis known slow converge simplicity presentation implementation 
methods speed calculation factor suggested literature malouf handle large datasets features 
expected sdr comparable svd algorithms computational complexity 
parameters initialized randomly different initial conditions affect results noticeably 
initializing parameters svd log generated better initial distribution improve final results convergence time 

information geometric interpretation iterative algorithm exponential form provide elegant information geometric insight interpretation method 
values variable mapped ddimensional differential manifold described values variable mapped 
order take limit fact pt pt continuous functions pt projection continuous parameters 
sufficient dimensionality reduction input distributions functions output ipr initialize iterate pt zt pt exp log pt zt normalization constant generalized iterative scaling algorithm 
dimensional manifold described 
empirical samples variables mapped dimensional manifolds empirical expectations respectively 
geometric embeddings generated feature vectors fact curved riemannian differential manifolds conjugate local metrics see amari 
differential geometric structure manifolds revealed normalization partition function exponential form 
note relations second derivatives logz logz log log 
matrices positive definite known fisher information matrices sets parameters 
information geometry formalism amari define natural coordinates manifolds geodesic projections equivalent previously defined projections 
natural coordinates manifold diagonalize fisher matrices locally locally uncorrelated 
intrinsic geometric properties manifolds local curvature geodesic distances invariant respect transformations including nonlinear coordinates 
iterative algorithm formulated covariant projections fixed convergence point invariant local coordinate transformations long coupling manifolds preserved 
formulation suggests sdr reduced statistical description terms characterized way invariant transformation particular tishby invariant permutations rows columns original occurrence matrix 
fact illustrated section 
information geometric formulation algorithm application study geometric invariants requires analysis 
cramer rao bounds uncertainty relations special exponential form provides interesting uncertainty relation conjugate manifolds way deal finite sample effects 
general parametric family parameter vector fisher information matrix log ji log log provides bounds covariance matrix estimator parameter vector sample 
denoting estimator sample size covariance matrix estimators cov symmetric definite parameters linearly independent diagonalized fisher information matrix 
particular diagonal elements matrices satisfy cramer rao bound ji var special exponential form inequality yields particularly nice uncertainty relation finite sample estimates feature vectors exponential form fisher information matrices dual ji cov ji cov cramer rao inequalities diagonal terms var var var var fisher information feature just variance adjoint variable vice versa 
fact know bound tight precisely exponential families equations equalities 
intriguing uncertainty relations conjugate features strictly true exponential form hold approximately true variances 
provide way analyze finite sample fluctuations estimates features 
information geometric structure problem allows interpret algorithm alternating geodesic projections riemannian manifolds 
manifold allow invariant formulation feature extraction problem geometric quantities depend choice local coordinates specific choice functions 
invariants metric tensors manifolds provide way define measure geodesic projections 
hand tensors just fisher information matrices exponential forms provide bounds finite sample fluctuations feature functions 

applications sufficient dimensionality reduction derivation sdr features suggests efficient identifying source distribution sample xn just empirical expectations xi 
show sdr algorithm extract non trivial structure artificial data real life problems 
illustrative problems section simple scenarios regression linear non linear extract information variables 
sdr shown find appropriate regressors uncover underlying structures data 
construction sdr features assumption knowledge function expected values required estimating assuming sdr approximation valid replace functions 
clear lagrange multipliers expected values correspondence functions problem symmetric interested regressor averages provide optimal information depicts plots running sdr distribution sin normal distribution mean standard deviation 
distribution exponential form single sufficient statistic sdr single feature turns nearly identical scaling translation 
averages corresponding reveal periodic structure data appropriate regressors problem 
numerical value plotting sdr features 
variables assigned meaningful numerical values terms documents approach 
extract description invariant representation cases plotting points allows analysis functional relation statistics assuming order domains 
illustrate consider scrambled version distribution sin variables undergone random permutation 
scrambled distribution shown scatter plots 
plots illustrate structure differential manifolds described previous section 
scatter plots clearly dimensional curves correct order recovered traversing curves resulting distribution shown 
clearly demonstrates original continuous ordinal scale recovered 
similar procedures recovering continuous structure distributions statistics 
shows distribution zx sin cos tishby sdr feature extracted distribution sin 
middle distribution 
left sdr feature blue scaled translated red dots 
bottom expected value function top feature 
right expected value function reordering sdr 
left right scrambled version sin scatter plot vs curved manifold 
scatter plot vs manifold 
reordering curves 
sufficient dimensionality reduction distribution sufficient statistics sdr represent dimensional curves shown 
statistics capture information distribution curves reveal underlying parameterization 
specifically original order domains reconstructed 
sdr analysis 
left distribution 
middle scatter plot manifold 
right scatter plot manifold 
document classification retrieval analysis large databases documents presents challenge machine learning addressed different methods see yang liu review :10.1.1.11.9519
important applications domain include text categorization information retrieval document clustering 
works probabilistic information theoretic framework analysis hofmann slonim tishby documents terms considered stochastic variables 
probability term appearing document doc doc doc doc denoted doc obtained normalized term count vector document doc doc doc doc number occurrences term document doc 
joint distribution doc calculated multiplying document prior doc uniform proportional document size 
stochastic relationship analyzed assumed underlying generative model linear mixture model hofmann maximum entropy model nigam 

optimal model predicting properties unseen documents 
current sdr finding term features mean values infer information document identity 
approach nonlinear significantly differs linear approaches lsi deerwester plsi hofmann :10.1.1.108.8490
approach extends works maximum entropy nlp berger della pietra 
works set features predetermined algorithmically chosen large set predefined features 
sdr assumptions feature kl tishby left kl original distribution model different values 
right information model 
horizontal line marks information original distribution 
space positivity range performs completely unsupervised search optimal 
document indexing sdr output sdr algorithm represent document index transformation reduces representation document number terms corpus dimensions 
dimensionality reduction serves purposes extract relevant information eliminate noise 
second low dimensional vectors allow faster efficient information retrieval important feature real world applications 
resulting indices document retrieval categorization 
natural index expected value document doc doc sdr formulation assumes knowledge expected values sensible represent document set values 
alternatively set lagrange multipliers doc document training data 
new document training data find doc single projection doc linear constraints imposed 
document classification application newsgroups database lang test sdr generate small features sets document classification 
start illustrative example shows sdr features capture information document content 
chose different newsgroups subjects alt atheism comp graphics preselected terms documents subject information gain criterion yang liu 
projection algorithm run resulting doc matrix values 
shows dkl seen larger values original distribution approached kl sense mutual information sense equation 
gain insight nature sdr features look obtained case 
histogram values shown 
seen values roughly symmetrical 
shows terms highest lowest values 
clearly sufficient dimensionality reduction val val histogram values 
islam correction gamma lossless terms high right low left values terms high correspond comp graphics subject ones low correspond alt atheism 
single feature maps terms continuous scale assigns positive weights class negative weights negligible weights terms possibly irrelevant classes 
performed classification different pairs newsgroups 
doc index input support vector machine svm light joachims trained classify documents subjects 
baseline results obtained running svm classifier original doc vectors 
training testing sets consisted documents subject terms 
experimented values test classify relatively features 
shows fraction baseline performance achieved number features 
seen features baseline performance achieved 
classification sdr index achieves performance comparable baseline uses significantly features compared features baseline classifier 
importantly features obtained wholly unsupervised manner discovered document properties relevant classification 
information retrieval application automated information retrieval commonly done converting documents queries vector representation measuring similarity cosine angle vectors deerwester 
sdr offers natural procedure converting documents dimensional indices doc 
retrieval performed standard cosine similarity measure necessarily optimal procedure 
sdr baseline accuracy tishby fraction baseline performance achieved number sdr features 
results averaged newsgroup pairs 
databases test information retrieval med documents terms cran documents terms cisi documents terms 
database precision calculated function recall levels 
performance summarized mean precision recall levels see hofmann 
compared performance sdr indices indices generated algorithms raw tf uses original normalized term count vector doc index 
dimension index number terms 
latent semantic indexing lsi lsi sdr dimensionality reduction mechanism uses singular value decomposition svd obtain low rank approximation term frequency matrix doc usv doc matrix 
new document vector represented dimensional vector log lsi sdr related performing low rank approximation log doc tried perform lsi input matrix log doc 
order avoid log zero thresholded matrix locally linear embedding lle roweis saul lle performs non linear approximation manifold vectors doc lie 
maps vectors dimensional space preserving neighborhood structure original highdimensional space 
lle shown perform image document data 
experiments described followed procedure roweis saul dot product document vectors neighborhood metric nearest neighbors values experimented gave optimal performance 
version lle index new vectors 
list terms obtained www cs utk edu lsi corpora html 
code available www cs toronto edu roweis lle code html minor changes 
sufficient dimensionality reduction mean precision function index dimension indexing algorithms sdr circles lsi triangles diamonds lle squares databases left right med cran cisi 
horizontal line mean precision raw tf algorithm 
raw tf lsi lle sdr med cran cisi table mean precision results information retrieval databases different indexing algorithms 
numbers brackets number features optimal performance obtained extension roweis document query data indexed simultaneously 
note gives lle advantage indexing algorithms test data queries training procedure 
databases indexing dimensions 
shows mean precision function index dimension indexing algorithms 
table gives peak performance databases algorithms included inferior lsi 
seen sdr performs uniformly better algorithms index sizes 
sdr achieves high performance small number features methods perform poorly 
suggests sdr succeeds capturing low dimensional manifold documents reside 
fact performs poorly shows sdr equivalent svd low rank approximation log doc 

discussion current suggests information theoretic feature extraction mechanism features calculating means empirical samples means turn tishby inferring unknown variable relevant property statistical data 
main goal procedure reveal possible continuous low dimensional underlying structure explain dependencies variables 
applications suggest extracted continuous features successfully prediction relevant variable 
interesting question sense resulting prediction close optimal 
words infer extracted features compared optimal inference procedure 
prediction error optimal procedure bayes error devroye 
true underlying distribution exponential form correct dimensionality sdr equivalent maximum likelihood estimation parameters features consistent devroye achieves asymptotically optimal error case 
interesting case source distribution special exponential form sdr interpreted induction principle similar better motivated maximum entropy principle 
fact solves inverse problem maxent finding optimal set constraints observables capture mutual information empirical joint distribution 
obvious methods exploit prior knowledge true distribution better cases 
assuming empirical conditional expectations single variable functions known source joint distribution similar statistical queries machine learning see kearns max min mutual information principle reasonable induction method 
briefly address important issues procedure raises 
information individual features low rank decomposition calculated svd suggests clear ordering features associated singular values 
svd solutions nested optimal features subset 
due non linear nature sdr solution necessarily nested true solutions linear methods hofmann lee seung 
set optimal sdr features information single feature quantified ways 
simplest measure im defined equation reflects information obtained measuring 
measure im im information lost result measuring 
measures reflect different properties feature research required test usefulness example assigning confidence measurement different features 
finite samples basic assumption problem formulation access true expectation values estimated uniformly finite sample standard uniform convergence conditions 
words standard learning theoretical techniques give sample complexity bounds dimensions reduced dimension continuous assumptions fat shattering dimension features 
source distribution close exponential form mutual information captured features cramer rao bounds provide simpler method analyzing sufficient dimensionality reduction finite sample effects discussed section 
bound prediction errors terms empirical covariance matrices obtained features 
uniqueness solution optimal features unique sense dot product appears distribution invertible matrix applied equivalent 
note resulting functions may depend initial point iterations information extracted optimum 
remove ambiguity orthogonalization scaling feature functions example applying singular value decomposition svd log see becker additional normalization schemes 
de correlate appropriate linear transformation 
notice procedure different direct application svd log 
coincide original joint distribution exponential form equation 
cases svd approximations lsi included preserve information features dimension reduction 
information theoretic interpretation information maxmin principle close formal structure problem channel capacity channel uncertainty see narayan 
suggests interesting interpretation features channel characteristics 
channel enables reliable transmission expected values exploit channel optimal way 
channel decoder case provided vector decoding performed dot product vectors 
intriguing interpretation algorithm obviously requires analysis 
relations methods dimension reduction clustering algorithms fundamental component unsupervised large scale data analysis 
sdr dimension reduction method reduces description distribution components 
large family algorithms similar purpose linear factorization 
example hofmann lee seung suggest finding positive matrices size size qr 
procedure relies fact rows columns lie dimensional plane 
svd lsi method deerwester performs linear factorization required positive 
approach equivalent approximating kl divergence sense matrices size respectively 
exponent written qr qr 
matrix powers element element linear methods said approximate terms 
important note methods linear exponential assume different models data success failure application dependent 
information retrieval experiments current shown document analysis linear methods successful exponential factorization improve performance 
tishby information theoretic approach feature selection zhu 
context texture modeling vision 
approach similar arrive min max entropy formulation 
current assume specific underlying parametric model define finite feature set features chosen greedy procedure 
sense algorithm similar feature selection mechanism della pietra 

works search approximate sufficient statistics see wolf george geiger directly calculating information statistic parameter interest 
results formalism different usually necessitates modeling assumptions computation feasible 
sdr finds mapping variables dimensional space considered embedding algorithm 
related non linear embedding algorithms notably multi dimensional scaling cox cox locally linear embedding roweis saul isomap tenenbaum 
algorithms try preserve properties points high dimensional space distance neighborhood structure embedded space 
sdr formulated way requires original points reconstructed optimally embedded points quality reconstruction measured 
relations information bottleneck closely related idea information bottleneck ib method originated tishby aims clustering rows preserves information 
defined sense ib method considered special case sdr features functions restricted finite set values correspond clusters 
clustering may correct answer problems relationship variables comes hidden low dimensional continuous structures 
cases clustering tends quantize data arbitrary way low dimensional features simpler easier interpretation 
hand motivation sdr essentially ib find low dimensional complexity representations variable preserve mutual information variable 
algorithm case quite different solves fact symmetric problem find low dimensional representations variables mutual information captured possible 
closely related symmetric version information bottleneck discussed friedman 

ib quality procedure described terms information curve fraction mutual information captured function reduced dimension 
interesting open question point extend sdr algorithm deal different dimensions simultaneously 
move continuously complex representations done ib mutual information constraint complexity representation 
reasons believe extension may soften notion dimension possible sdr 

research feature extraction method information theoretic account notion measurement 
proposed method new dimensionality reduction technique 
sufficient dimensionality reduction nonlinear aims directly preserving mutual information empirical occurrence matrix 
achieved information variation principle enables calculate simultaneously informative feature functions random variables 
addition obtain exponential model approximation data precisely features dual sets sufficient statistics 
described alternating projection algorithm finding features proved convergence local optimum 
fact algorithm extracting optimal sets constraints statistical data 
experimental results show method performs language related tasks better linear models extensively past 
performance enhanced smaller feature sets obtaining better accuracy 
initial experiments image data show promising results 
taken results suggest underlying structure non negative data may captured better sdr linear mixture models 
immediate extension multivariate case 
natural measure information case multi information see friedman 
generalize bivariate sdr optimization principle defining optimal measurement large set variables 
acknowledgments noam slonim gal chechik amir helpful discussions comments help experimental data 
research supported israeli academy science ministry science 
supported foundation 
amari 
methods information geometry 
oxford university press 
becker 
analysis sets way contingency tables association models 
journal american statistical association 
berger della pietra della pietra 
maximum entropy approach natural language processing 
computational linguistics 
cover thomas 
elements information theory 
wiley 
cox cox 
multidimensional scaling 
chapman hall 
csiszar 
divergence geometry probability distributions minimization problems 
annals probability 
darroch ratcliff 
generalized iterative scaling log linear models 
ann 
math 
statist 
deerwester dumais landauer furnas harshman 
indexing latent semantic analysis 
journal american society information science 
tishby della pietra della pietra lafferty 
inducing features random fields 
ieee transactions pami 
devroye lugosi 
probabilistic pattern recognition 
springer 
fisher 
precision discriminant functions 
annals 
friedman slonim tishby 
multivariate information bottleneck 
proceedings seventeenth conference uncertainty artificial intelligence uai 
geiger maloney 
features sufficient statistics 
advances neural information processing systems volume 
goodman 
simple models analysis association cross classifications having ordered categories 
journal american statistical association 
goodman 
analysis cross classified data having ordered unordered categories association models correlation models asymmetry models contingency tables missing entries 
annals statistics 
haberman 
analysis data 
academic press 
hofmann 
unsupervised learning probabilistic latent semantic analysis 
machine learning 
jaynes 
information theory statistical mechanics 
physical review 
joachims 
text categorization support vector machines learning relevant features 
th european conference machine learning number 
kearns 
efficient noise tolerant learning statistical queries 
acm symposium theory computing pages 
lang 
newsweeder learning filter news 
icml pages 
narayan 
reliable communication channel uncertainty 
ieee transactions information theory 
lee seung 
learning parts objects non negative matrix factorization 
nature 
malouf 
comparison algorithms maximum entropy parameter estimation 
sixth conf 
natural language learning pages 
nigam lafferty mccallum 
maximum entropy text classification 
ijcai workshop machine learning information filtering pages 
pitman 
sufficient statistics intrinsic accuracy 
proc 
cambridge phil 
soc 
sufficient dimensionality reduction roweis saul 
nonlinear dimensionality reduction locally linear embedding 
science 
roweis saul hinton 
global coordination local linear models 
advances neural information processing systems pages 
slonim tishby 
agglomerative information bottleneck 
advances neural information processing systems vol 

mit press 
slonim tishby 
document clustering word clusters information bottleneck method 
acm sigir pages 
tenenbaum de silva langford 
global geometric framework nonlinear dimensionality reduction 
science 
tishby pereira bialek 
information bottleneck method 
proc 
th annual allerton conference communication control computing pages 
wolf george 
maximally informative statistics 

url lanl arxiv org abs physics 
yang liu 
re examination text categorization methods 
acm sigir pages 
zhu wu mumford 
minimax entropy principle application texture modeling 
neural computation 

