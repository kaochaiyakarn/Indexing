creating web page recommendation system haystack jonathan submitted department electrical engineering computer science partial fulfillment requirements degrees bachelor science computer science engineering master engineering electrical engineering computer science massachusetts institute technology september jonathan 
rights reserved 
author mit permission reproduce distribute publicly electronic copies thesis document part 
author 
department electrical engineering computer science september certified 
david karger associate professor thesis supervisor accepted 
arthur smith chairman department committee graduate students creating web page recommendation system haystack jonathan submitted department electrical engineering computer science september partial fulfillment requirements degrees bachelor science computer science engineering master engineering electrical engineering computer science driving goal thesis create web page recommendation system haystack capable tracking user browsing behavior suggesting new interesting web pages read past behavior 
course thesis salient subgoals met 
haystack learning framework unified example di erent types binary classifiers black box access single interface regardless text learning algorithms image classifiers 
second tree learning module capable hierarchical descriptions objects labels classify new objects designed implemented 
third haystack learning framework existing user history faculties leveraged create web page recommendation system uses history user visits web pages produce recommendations unvisited links user specified web pages 
testing recommendation system suggests tree learners url tabular location web page link taxonomic descriptions yields recommender significantly outperforms traditional text systems 
thesis supervisor david karger title associate professor acknowledgments supervisor professor david karger suggesting topic providing guidance way 
kai shih helped thesis discussing tree learning algorithms providing test data user study administered 
additionally haystack research group invaluable helping familiarize haystack system 
david huynh vineet sinha particular aid teaching accomplish various coding tasks haystack system helping debug haystack dependent code wrote 
patient providing welcome thesis 
contents haystack 
web page recommendation 
related daily 
news 
montage 
newsweeder 
background notation 
probability 
model user interaction 
labeling links 
hierarchical correlation 
original bayesian tree algorithm 
user experience configuring agent 
agent 
analysis tree learning algorithms weakness original algorithm 
fixing weakness buckets 
developing tree learner intuitively 
attempt new tree learning algorithm 
improving learner 
improving algorithm 
assessing algorithm 
weaknesses tree learning algorithms 
design tree learning package describing location hierarchy 
interface 
interface 
interface 
implementation tree learning package prevents magnitude underflow 
implementing 
implementing interface 
implementing interface 
implementing bayesian tree learner 
implementing continuous tree learner 
implementing beta system tree learner 
haystack learning architecture featurelist class 
interface 
interface 
creating general classifier interfaces 
incorporating text learning 
incorporating tree learning 
creating web page recommendation system haystack extracting resources 
implementing 
implementing 
getting list links rank 
ranking links 
listening changes haystack database 
putting 
testing recommender system experimental setup 
results 
url feature 
layout feature 
url layout features 
text url layout features 
composite versus url layout 
justification negative samples 
summary results 
criticism experimental setup 

tree learning 
haystack learning framework 
recommender agent 
contributions 
list figures agent setting interface collection pages track accessed 
web pages dragged dropped collection pages track 
collection recommended links visiting pages cnn 
example active feedback 
note page placed uninteresting category 
interface 
interface 
interface 
public methods constructors 
public methods constructors 
public methods constructors 
public methods fields class 
private method called update probabilities recompute mode 
private method called divide node 
private method called multiply node 
class signature 
class signature 
featurelist class 
interface 
interface 
interface 
class 
class 
class 
interface 
class definition 
class definition 
interface 
class definition 
precision recall curves discrete beta buckets nave nave 
precision recall curves discrete beta buckets nave nave 
precision recall curves discrete ul beta ul buckets ul nave nave 
precision recall curves discrete ul beta ul discrete nave nave 
precision recall curves discrete ul beta ul discrete beta discrete beta 
precision recall curves discrete beta compared curves algorithms negative samples added 
precision recall curves discrete beta algorithms compared curves learners negative samples added 
list tables short hand notation various described classifier 
chapter chapter introduces context thesis providing basic information haystack information management project subfield web page recommendation driving application thesis 
haystack haystack project started increasing need help users interact growing corpora data objects mails papers calendars media files 
opposed traditional user interfaces distinct applications interact di erent types files di erent user interfaces constrained way users view manipulate data haystack attempts unify interaction single application 
haystack allows users organize data semantic content superficial file type 
unification achieved creating semantic user interface data object annotated arbitrary metadata subject predicate object format 
example factoring polynomial time listed having author ben member set papers seminar near list user favorite files 
semantic user interface job render data objects context user browsing provide appropriate options menus objects 
addition providing uniform semantic user interface haystack provides architecture incorporating agents autonomous programs perform diverse activities gathering weather data automatically classifying data objects categories extracting text documents 
agent architecture excellent setting creating modules large amount utility system 
allows system incorporate arbitrarily rich functionality simply plugging new agents 
web page recommendation keeping underlying goal helping user manage large amounts data goal thesis create agent haystack searches new web pages user find interesting 
doing agent decrease ort user expend navigating interesting web documents help user find greater quantity quality pages 
recommendation web pages area active research 
web page recommendation textual analysis see chapter 
kai shih david karger proposed new feature learning web pages 
claim url web pages bears correlation interesting features page page advertisement user interested page 
argue nature correlation hierarchical pages similar urls www cnn com law www cnn com law review equally interesting uninteresting user 
course correlation necessarily related url hierarchical description page 
example consider articles www cnn com world east sars www cnn com health conditions sars 
articles sars disease outbreak may interesting user reason 
learning may able improved incorporating traditional textual methods 
addition url location link web page table fourth element second fifth element correlated user interest link contents 
web pages useful visitors web editors incentive localize links web pages users interested user entire web page examine links 
example visitors cnn home page may busy look approximately links may look headline news stories links appear favorite 
thesis seeks implement web page recommending capability shih daily application exploring benefits traditional textual analysis implementing idea learning hierarchy url tabular location links 
addition seeks provide robust interfaces recommendation web pages allow new algorithms plugged recommender agent requiring agent rebuilt scratch 
chapter related chapter describes kai shih daily application motivation thesis web page recommending systems built 
daily daily application kai shih developed recommend new interesting web pages 
user gives application list web pages track uses history user clicks provide set positive samples serves training data 
url tabular position links recommends top stories page tracking 
addition daily cleans pages displayed blocking ads static portions web pages table links various sections news website 
algorithm daily uses learn trends urls tabular locations links tree learning algorithm uses tree shaped bayesian network see chapter details 
high level functionality agent created thesis essentially emulates web page recommending functionality daily generalized version shih algorithm tree learning algorithms developed thesis necessary agent created thesis exclusively tree learning algorithms 
news billsus pazzani news recommends news stories emphasis placed development speech interface 
learn stories interesting uses known nave bayes nearest neighbor algorithms text stories 
similar agent built thesis customizes recommendations individual user 
di erent uses textual analysis news stories requires active feedback 
contrast recommender agent thesis learns user preferences binary classifier including tree learners text 
uses passive feedback allowing active feedback haystack categorization scheme users required exert ort agent learn 
interesting aspect news attempts di erentiate short term user interests long term interests captures trends users tastes 
agent built thesis attempt distinction 
montage corin anderson eric horvitz developed montage system tracks users routine browsing patterns assembles start page contains links content pages system predicts users visit 
important ideas learning process montage note content users find interesting varies depending context web access 
particular application uses urls websites visited time day access occurs 
result recommendations may change time day changes 
interesting aspect montage start page considers probability user visiting page current context navigation ort required reach page 
pages equally visited montage fetches require browsing ort reach 
compared agent built thesis montage similar uses features web pages text recommending content users 
underlying goal recommender agent exactly montage 
montage strives assist users routing browsing various web pages frequently visited recommender agent searches links new stories visited 
newsweeder ken lang newsweeder project netnews filtering system recommends documents leveraging content collaborative filtering emphasis content 
newsweeder asks users active feedback uses conjunction textual analysis improve recommendations users 
lang introduces minimum description length heuristic classifying new documents text reports compares favorably traditional textual analysis techniques 
newsweeder di ers agent thesis requires active feedback simply allowing uses textual analysis learn preferences 
chapter background chapter introduces notation terminology concepts essential learn order fully understand rest thesis 
notation custom notation adopted simplify probabilistic analysis tree learning algorithms thesis 
section introduces notation representing probabilities values associated tree learner discrete bayesian tree learner described section 
trees described roots top children tree node parent higher 
variables usually represent classes node may belong variables usually represent specific nodes tree 
expressions form represent event specified node case specified class case 
value represent probability evidence subtree rooted node conditioned hypothesis class value represent prior probability root tree belongs class probability section provides overview basic probability concepts necessary understand algorithms thesis 
david heckerman written excellent tutorial learning bayesian networks original tree learning algorithm specific example 
tutorial consulted ideas improve tree learning 
essential understand bayes rule form pr pr pr pr pr 
particular rule implies probability event occurring occurred computed probability occurring divided probability occurring 
implication original discrete bayesian algorithm introduced section 
second important general understanding beta distribution example discretized continuous bayesian tree learner implemented thesis 
tree learner beta distribution probability distribution node children see section 
lot said derivation beta distribution expectation set prior understanding essential understanding thesis 
basic characteristics beta distribution discussed 
basically beta distribution distribution positive values interval 
shape beta distribution quasi bell curve centered expectation computed number positive samples negative samples distribution 
expected fraction beta distribution samples belonging positive class simple closed form 
beta distribution important parameters establishing prior distribution represents number virtual negative samples represents number virtual positive samples expected fraction beta distribution samples belonging positive class simple closed form prior 
beta distribution commonly applications positive values exactly interval expectation simple formula prior set intuitively adding virtual samples 
model user interaction section model user interaction motivates development tree learning provides concrete model judge aspects various tree learning algorithms 
labeling links suppose user browses front page cnn home page day 
time user click subset links appear page day 
goal web page recommendation determine links user click day new links available 
user provided feedback regarding new links may feedback similar old links links part page having similar urls 
compute probability new page interesting intuitive fraction similar old links visited 
example similar urls clicked reasonable believe probability new link clicked 
question remains positive negative examples added 
strategy simply labeling nodes clicked versus clicked 
strategy unvisited links negative samples user visits repeated visits page ignored 
strategy simple intuitive aspects web pages merit discussion 
simple setup ignores fact nodes 
example link section heading www cnn com world may persist indefinitely user clicks link labeled time user visited link accident 
provide simple example cause problems assume user clicks section headings particularly interesting 
new section heading appears heavily supported similar links clicked 
hand user reads health articles posted user interested new health article new section heading 
issue motivates practice adding multiple pieces evidence link time page sampled 
example suppose page sampled day link clicked day positive label added page clicked negative label added 
health labels positive negative samples mount static section links may positive 
implementation passive evidence gathering thesis time agent runs visited links gathered positive samples classifiers equal number negative samples randomly picked pages tracked subset links visited time agent ran 
choice retains positive samples keeping total number samples manageable number samples proportional number times human clicked links 
aside goal recommend pages user wants read clicks proxy perfect solution user clicks necessarily directly lead interesting web pages 
instance user may mistakenly open page link intermediate point navigating interesting page 
avoid requiring active feedback deemed best proxy passive feedback 
hierarchical correlation new nodes nearby nodes tree pieces evidence 
new nodes appear immediate neighbors hierarchical correlation assumption provide prior knowledge likelihood new node class 
hierarchical correlation assumption states children parents class 
reason assumption hierarchical correlation expected hold tabular location link feature web editors incentive localize stories similar degrees interest users 
clustering links decrease users costs searching want read making user return website 
nearby links close tabular layout hierarchy web page reasonable assume hierarchical correlation assumption holds 
reason correlation assumption expected hold url feature similar 
subject matter article correlated url evidenced various news sites cnn organize stories sections appear urls stories 
url convey information merely subject 
instance page section humorous articles humorous articles may similar urls assorted subject matter 
original bayesian tree algorithm section describes original tree learning algorithm developed shih 
essentially shih algorithm solves problem binary classification computing node tree probability probability evidence subtree rooted type trait type lacks trait 
represent prior probabilities root types respectively 
probability new node trait computed bayes rule probability evidence underneath root root root new node trait divided probability evidence tree pr evidence pr evidence pr evidence 
shih algorithm computes modeling tree bayesian network children children ij probability child class parent class ij thought probability mutation generations tree causes child class parent class 
leaf nodes base case recursive definition example set leaf represents positive sample leaf represents negative sample 
thesis algorithm generalized handle arbitrary number classes just versus redefining probabilities node children classes ji class asset algorithm yields cient incremental update rule 
new leaf incorporated tree way update values node tree recompute values recursively equation suggests 
key insight leads fast incremental update algorithm values tree remain new piece evidence added factors changed follows fact evidence subtrees changed 
changed lie path newly added leaf root 
changed need exactly factor changed factor corresponding child changed 
changes bottom parent leaf modified incorporate new factor leaf grandparent divides old values multiplies new ones 
removing leaves accomplished analogously 
details regarding incremental update rule implemented including pseudocode see section describes implementation discrete bayesian tree learner implemented thesis 
chapter user experience chapter describes experience user configuring recommender agent developed thesis 
configuring agent running haystack user open settings recommender agent searching recommender agent 
point user interface shown 
pertinent setting collection pages track 
order users recommended web pages provide agent starting point points searches links new pages 
collection pages track just collection haystack web pages added haystack drag drop interface 
example shows screenshot collection pages track cnn homepage dragged 
agent collection pages track formed recommender agent recommending links appear pages 
collection recommended links maintained recommender agent updated periodically agent setting interface collection pages track accessed 
day 
view recommendations user search collection recommended links keep collection side bars shown 
recommender agent able recommendations passive feedback obtained listening user visits web pages users option providing active feedback categorizing web pages view 
functionality shown 
web pages dragged dropped collection pages track 
collection recommended links visiting pages cnn 
example active feedback 
note page placed uninteresting category 
chapter analysis tree learning algorithms delving design implementation recommender agent new tree learning algorithms developed thesis discussed chapter 
chapter potential fault original discrete bayesian algorithm see section identified explained 
consequently alternative algorithms variant original proposed rectify weakness 
weakness original algorithm bayesian tree learning algorithm traits 
fast incremental update procedure provide performance setting positive examples 
troubling aspects cause algorithm produce undesirable results 
user interaction model section accepted providing negative examples important producing results 
intuition claim negative samples tree learner recommend types links visited frequently opposed types links visited highest probability rarely appearing links visited rare occasions appear breaking news alert 
second negative examples discrete bayesian algorithm described section ranking quality node relation depends systematically number nearby samples 
illuminate problem toy example analyzed intuition problem 
consider example root node children fraction class supported data gathered far 
unknown node appears child compute probability equation follows pr evidence pr evidence pr evidence represents event new node type 
probability evidence formula pr evidence children children plugging terms children pr evidence 
reasonable simplifying assumption reduces pr evidence find pr evidence simply need multiply extra factor terms resulting pr evidence pr evidence reduce pr evidence ready examine ect algorithm 
note place come ect exponents value simply di erence number positive samples negative samples ratio 
example illustrate yielding pr data compare case pr data example second case higher fraction positive examples declared equally samples 
little inspection clear second case samples predicted contain new positive node case 
fact cases assuming dominates term predicted probability new node mutation probability far 
predicted probability new node runs contrary intuition predicted probability 
toy example parent generation children clear underlying cause problem extends algorithm applied tree 
fact thinking assumptions algorithm underlying model sheds light root problem application binary probability space incapable capturing phenomenon inherently continuous 
words discrete bayes algorithm tries determine node children bad children 
notion excellent average 
create ordered list recommendations places excellent leaves ahead leaves necessary tree learner able distinction confident node di erent predicting node excellent 
order binary discrete tree learner avoid problem underlying process modeling exhibit data corresponds model example children nodes tree roughly roughly binary discrete tree learner model process mutation probability set 
appeared empirically case web page recommendation 
fixing weakness buckets problem identified discrete bayes tree potential solution simply parameter space continuous reflect true underlying process clicks correlated urls page layout 
problem daunting task model continuous parameter space bayesian net ciently infinite number classes store probabilities 
making complete transition continuous parameter space unnecessary applications 
alternative discretize parameter space finite number buckets represent instance cetera assuming users care document 
generalized multiclass version discrete bayesian tree learning algorithm transition easy new classes correspond bucket 
thing left determine conditional probabilities children parent nodes 
natural choice assume children parameters distributed beta distribution centered parent bucket 
beta distribution choice assigns positive weight exactly buckets interval shaped bell curve centered expectation 
prior easily placed distribution desired push distribution children slightly priori determined beta distribution 
developing tree learner intuitively addition fixing weakness buckets alternatives making tree learner amenable solving problems continuous parameter space 
section describes algorithm developed intuitively 
developed probability model runs linear time small constant directly addresses problem discrete bayes algorithm 
attempt new tree learning algorithm vector contain histogram set observations node example positive examples negative examples contained set leaves vector values representing prior beliefs ratio distribution evidence normalized entries sum example equal node tree expected percent positive examples 
define constant weight 
weight thought number imaginary observations represents 
define predicted distribution classes new leaves represents total number samples node instance continuing example values equation provides suitable solution observations known correlated observations sets 
learning hierarchy natural nearby nodes children parents ect beliefs probability distribution especially observations observations parent children node assumptions data collected single node assumed come distribution distributions parents children believed correlate may di erences expected slight possibly large 
assuming severity di erences distributions neighbors independent locations child incorporated equal weight follows defined average distribution neighbors words children children 
definition circular node tree simultaneously equations collected solved system equations 
improving learner learner previous section satisfy intuition learn hierarchy troubling caveat 
consider scenarios root little evidence child positive evidence root little evidence children positive evidence 
intuitively second scenario cause root positive child nodes supporting hypothesis positive 
framework allow children lumped sum normalized number neighbors 
alternative algorithm lump term neighbors term term weights distribution 
intuitively model maintains nice feature real observations node outweigh ect neighbors term adds feature higher number neighbors diminishes ect priori determined term 
setting distribution weight expressed neighbors neighbors thought expected value distribution node leaves beneath thought priori determined expected value distribution arbitrary node tree number virtual neighbors represents 
defined solved system equations previous algorithm 
improving algorithm problem previous algorithm distribution node changed children added contain evidence 
example consider parent node fraction positive samples weighting term reaches equilibrium value attached positive class 
new node evidence added child parents predicted fraction positive samples may increase child inherit distribution parent parent perceive evidence higher fraction positives correct 
sense new node added evidence added tree 
exact problems cause undesirable behavior adding empty node 
imaginary data weighting term ects neighboring nodes empty leaf node 

data single node allowed reflect back propagates tree 
solution problem weight average neighboring nodes nodes little evidence little influence neighbor distributions 
solution define distribution node usual redefined neighbors neighbors weight term ensures neighbors small amount data branches carry weight weighted average neighboring nodes 
reasonable definition fraction distribution real data node neighbors 
equations defined neighbors 
strategy solves problem imaginary data regarded real data weights nodes branches contain data data flows tree may case 
incorporating solution second problem solve problem completely 
prevent data reflection solve second problem listed new set distributions defined distribution sees coming neighbor value declared influenced nodes branch nodes reach visit 
defined neighbors wz hz neighbors wz defined similarly neighbors 
distribution arbitrary node defined redefined neighbors neighbors problems solved 
positive attribute definitions yield algorithm solved linear time solved explicitly ciently 
computing distinct requires time equations applied directly pair neighbors tree tree traversed postorder compute bottom values preorder second compute top values 
knowledge exists compute directly equations 
pseudocode algorithm omitted sake simplicity implementing algorithm straightforward equations 
assessing algorithm genesis intuition specific problems learning user interests hierarchy urls tabular locations algorithm may choice tree learning contexts proposed tree learning algorithm positive attributes matter confident neighbors node significant number observations node outweigh influence neighbors 
aggregate ect neighbors grouped term weight exceed constant number real observations node 
branches tree evidence ect induced distribu tions nodes tree 
computed distribution nodes neighbors distribution closer distribution neighbors neighbors 
algorithm cient trained scratch linear time relatively small constant 
weaknesses tree learning algorithms tree learning algorithms mentioned thesis su er weaknesses resulting rigid tree structure tree learning framework imposes 
assume likelihood node completely encapsulated node immediate neighbors 
probably true general urls tabular locations 
urls times parent receive negative samples children positive distributions 
example suppose user clicks links www cnn com world clicks links form www cnn com world 
algorithms deduce link form www cnn com world bad direct descendant node received lot negative evidence 
way recognize grandchildren world node usually recommend 
problem occurs web editors place meaningless dynamic information urls 
example cnn places date urls probably significant factor user wants read story 
problem occurs layout web pages 
home pages add remove banners elements web pages move tables page 
underlying structure layout remains 
instance headline news story may location day day extra banner may put page making layout position 
extent problems solved heuristically example deleting dates urls 
ideal tree learning algorithm able deal problems gracefully needing heuristics 
potential solution variant nearest neighbors defining distance nodes edit distance hierarchical descriptions 
ciently implementing algorithm challenge systematic problems straightforward nearest neighbors implementation choose equally distant neighbors 
chapter design tree learning package functionality tree learning package tree learning framework developed shih see section 
maximize utility tree learning architecture thesis designed independent document recommender system 
fact reason learn hierarchies documents web pages objects documents photographs animals provided hierarchical description 
recommender system merely example tree learning framework learn hierarchical data 
high level tree learning architecture presents interface user learner adds evidence associated hierarchical description object tree learner 
user query nodes determine probability queried node specified class 
describing location hierarchy locations nodes hierarchies described diverse ways 
string www cnn com business technology square cam bridge ma table third element second describe location node tree hierarchical name node abstracted class provided java 
constructor takes array objects representing downward path root leaves node hierarchy 
purpose class opposed url directly tree learner flexible form hierarchical description node may take words provides layer abstraction hierarchy described 
left user tree learner transform features particular taxonomies objects tree learning architecture 
example users may change names www cnn com business technology square cambridge ma com cnn business ma cambridge technology square 
interface usually tree learning classification evidence objects added nodes merely labels corresponding class nodes belong 
tree learning architecture flexible bayesian algorithms full power 
bayesian tree represents probability evidence subtree rooted type piece evidence tree need simple label values defined 
example suppose fire engine red probability sports car red probability 
leaf representing evidence red fire engine red sports car red 
encapsulate notion evidence interface written see 
method returns probability object exhibit evidence class cls cls evidence object type class abstractly represent identity classes node may belong 
public interface public double cls interface 
specialized purpose doing tree learning classes child interface created 
noted tree learning algorithms easily generalized evidence framework 
example intuitive algorithm developed section may produce undesirable results evidence labels variable confidence 
reason hindsight may desirable change interface represents labels variable confidence 
interface interface tree learner objects similar machine learning interfaces containing methods adding new training data testing new objects 
hand tree learning framework di erent machine learning interfaces requires objects training testing hierarchical description url taxonomic name organism 
interface appears 
public interface public double name cls public void name ev public void name ev public iterator classes interface 
method associates evidence ev object implements interface entity described name may contain sequence words com cnn law 
method functions analogously 
method find probability object description name class cls evidence tree 
implementations interface update incrementally need explicitly trained extends interface 
interface part generalized haystack learning framework described detail chapter 
classes method returns iterator classes tree learner learning 
note interface flexible form hierarchical description object may take di erent classes objects underlying implementation may need impose constraints ectively implementable 
details left implementer 
interface object implementing interface required implementation useful methods constructing bayesian similar implemented thesis 
interface appears 
public interface public double cls public double parent child public iterator classes interface 
method returns prior probability node member class cls bayesian tree learners prior distribution root 
method returns probability class child child conditioned hypothesis parent class parent 
classes method returns iterator classes described 
implementations methods hidden interface allow easy existing implementations better di erent implementation developed 
chapter implementation tree learning package chapter describes implementation tree learning package including implementations interface described section 
tools tree learner implementations described followed sketches tree learners built tools 
prevents magnitude underflow initially implementation discrete bayesian algorithm described section double datatype represent probabilities node tree 
soon evident magnitude probabilities tree quickly infinitesimal double su ce smallest positive nonzero value double probability group evidence product individual probabilities piece evidence decrease probability evidence root node order magnitude 
pieces evidence double longer distinguish probability root evidence zero 
options explored solving problem 
java class considered problem 
carries arbitrary precision amount memory required store probabilities roughly number pieces evidence added tree 
follows nodes pieces evidence added tree number bits probabilities nodes factors probabilities 
potential solution standard machine learning practice log likelihoods 
storing probability evidence node logarithm probability evidence stored magnitude values stored underflow minimum positive value double roughly pieces evidence estimated number elementary particles known universe added tree piece evidence decreased likelihood evidence tree factor 
examination formula probability evidence node revealed method 
consider formula probability evidence underneath node type children classes pr children set children node classes set classes 
compute log likelihoods formula changes log log children classes pr children log classes pr performing computations raw probabilities unavoidable terms second summation reside log near thesis realized log likelihoods 
log stored child node parent values log approximated log address problems potential solutions problem precision underflow class created 
class analogous java classes immutable datatype representing number containing methods standard arithmetic operators 
allow arbitrary precision 
object mantissa stored double normalized interval exponent stored integer value 
value represented memory required constant bits mantissa plus log bits exponent measures magnitude number 
amount memory consumed probabilities stored nodes roughly log number evidence objects added tree 
reasonable java bit long datatype store exponent 
allowed slightly faster arithmetic operations exponent allowed slightly easier intuitive implementation 
guarantee class overflow outweighed slight loss performance addition subtraction operations pieces evidence added cause long overflow 
implementing natural implementation describes transition prior probabilities tree learner see section matrix 
class created 
class public method signatures shown 
implementation children log classes log log pr log classes log log pr approximately computed finding max classes log log pr letting log classes log log pr precision may lost values sum may su er magnitude underflow 
public class implements public matrix classes public double cls public double parent child public iterator classes public methods constructors 
essentially wrapper custom immutable matrix class created 
classes argument constructor contains identities classes represented row column 
entry classes represents identity class entries appear column row entries represent mutation probabilities 
particular node type node child probability type stored entry row column 
convenience creator constructor normalizes columns sum 
entries obvious classes method 
get priors matrix arithmetic required 
natural distribution prior probabilities node discrete bayesian tree distribution node children distribution absence evidence 
class distribution deep nodes tree far away evidence 
distribution priors vector mx matrix containing mutation probabilities 
column vector steady state computation placed matrix class 
matrix class immutable implementation matrix basic matrix operators provided convenience methods making simple matrices 
addition facilitate computation priors steadystate method provided returns column vector mx stochastic matrix steady state computed 
steadystate method works identity mx constraint entries sum added substituting 
row column matrix changed 
reason substitution performed stochastic underspecified columns sum subset rows uniquely determines row 
matrix computed substituting row called value steady state ax 

computed gaussian elimination 
class direct utility discrete bayesian algorithm useful having consistent set priors tree learning algorithms 
default set parameters created algorithms determine prior probability classes set classes learn algorithms may explicitly mutation probabilities 
implementing interface basic implementations tree learners training data written see section discussion interface 
class definition appears 
cls argument constructor specifies class evidence supports 
confidence parameter constructor value returned method queried class evidence supports 
public class implements serializable public cls double confidence public cls public double nc public methods constructors 
example confidence equals think evidence object fact object looks pencil returns queried pencil queried eraser class objects 
note classes matter probabilities sum probability returned probability evidence class way 
similar allow confidence specified assumed 
object equal node tree learner keeps set evidence associated keep copy object added 
hand objects equal supports class 
pieces added set copy added 
example leaf tree receives interesting user categorizes object interesting object added represent semantic idea fact described object fact member specified category 
hand reason evidence objects general identity defined class support tree learning applications may sense add multiple identical distinct objects object 
inspired haystack categorization scheme 
users haystack categorize documents circumstance copies categorization evidence added class built reflect 
addition basic implementa tions implement interface analogous implementations written 
class definition built capture visit document 
similar parent class public class extends public date public calendar public methods constructors 
hardcoded support true class represent interesting 
stores date time access notion equality calendar returned method 
implementing interface implementations interface created reflect algorithms explored tree learning 
implementation discrete bayesian algorithm implemented 
weakness discovered discrete bayesian algorithm see section alternative implementations created algorithms sections 
implementing bayesian tree learner implement interface class created see 
implementation interface requires user pass provide prior transition probabilities computing probability evidence node tree 
parameter specifies type feature extractor tree learner provide see section 
mode parameter specifies incremental changes tree called value public class extends public static final int recompute public static final int incremental public public int mode public double name nc public void name ev public void name ev public boolean public void train public iterator classes public methods fields class 
recompute incremental 
recompute mode incremental performs extra recomputations ensure loss precision adding removing evidence user concerned loss precision probabilities insertions deletions tree necessary risk perfectly contradictory evidence occurs pieces evidence may query disagree value node confidence assertions fully incremental mode case divide zero error occurs typically query evidence removed 
see recompute works recall form probabilities node equation replicated convenience children classes pr 
note changes values ected ancestor 
su ces simply walk tree ancestor ancestor recompute suggested equation code cached values descendants way 
note imprecision results repeatedly adding removing evidence operation undone evidence needs removed divided simply removed products containing changed factors recomputed 
examine time private void private method called update probabilities recompute mode 
complexity method assume number pieces evidence node bounded constant 
amount compute probabilities node bc branching factor tree number classes 
assuming number classes constant changes 
ancestor needs probabilities recomputed total time complexity update probabilities recompute mode bh height tree 
constant branching factor assumed height tree log number pieces evidence added total time complexity log recompute update mode cient 
assumptions probably hold domains thesis urls tabular layout positions 
realistic assumption urls height constant 
fact time complexity polynomial fact realistic assumption breadth unbounded number children particular node may comprise constant fraction total number nodes tree time complexity linear applications may slow 
fix problem note changes node ect parent broken steps 
initial value divided parent product 
careful inspection equation shows possible 
value parent simply product factors coming children factor independent 
divided computing classes pr dividing value just changed recursively divided parent 
get recursion correct note correctly divide parents tree old values recursion start root downwards suggested 
second updated new value multiplied private void perform actual dividing node 
private method called divide node 
analogous procedure 
main di erence fact node path root multiplied product parent divided child nodes updated parents recursion successive parent incorporates new changes changed child 
compare code code 
private void perform actual multiplying node private method called multiply node 
methods operate described tree learner inserts removes evidence appropriate node immediately recomputes cached probabilities described 
method simply returns probability node specified belongs specified class 
works computing probability evidence tree pr evidence classes represents root node represents prior probability root class probability evidence subtree rooted evidence type adds new piece evidence corresponding notion specified tree path specified class recomputes probability evidence tree making formula pr evidence pr evidence pr evidence 
note train methods irrelevant class regardless update mode ready queried immediately called 
simply returns true train empty body 
implementing continuous tree learner problems discussed section alternative implementations interface written 
subsection gives overview implementation details regarding bucket bayesian tree described section 
class implements interface multiclass underneath emulates bayesian tree continuous parameter space 
outline class appears 
basic default constructor takes arguments specifying prior probabilities classes specifying type feature extractor see section 
parameter second constructor specifies number public class extends public params public params int int mode public void add featurelist features boolean cls public double classify featurelist features public boolean public void train public double name nc public void name ev public iterator classes public void name ev class signature 
buckets continuous interval divided 
default value largest value practical recommender application 
mode parameter specifies update mode underlying see section 
internal object mutation matrix defined described section distribution child nodes particular bucket centered parent node beta distribution 
add classify methods general haystack learning framework 
override implementations handle binary evidence 
see chapter details methods 
noted details essential understanding tree learning package context general haystack learning framework 
train methods interface vacuous implementation contained class 
implementations straight forward dispatches internal 
dichotomy internal nodes tree evidence nodes 
internal nodes bucket classes leaf nodes containing evidence know classes bad example 
internal tree nodes separate class bucket learn reasonable interface internal nodes leaf nodes needs developed 
natural way internal tree follows declared bucket represents fraction leaf children positive 
positive leaf node appears parent node deduces bucket positive evidence appears probability negative leaf node appears parent node deduces negative evidence appears probability 
example buckets buckets centered positive evidence return queried bucket class queried second bucket class 
note positive negative probabilities leaf leaf leaf parent uses linear combination probabilities leaf leaf 
gracefully handles case newly added evidence full confidence example 
method works adding test buckets leaves newly object fit tree computing probability new object fall bucket quality measured continuous scale versus 
way expected value new object bucket weighted average weights determined probability new object falls bucket 
hindsight may simpler just add binary test evidence tree compute pr new leaf positive evidence pr evidence 
implementing beta system tree learner alternative class implementation developed discussion section 
class definition appears 
params argument public class extends public static final double default alpha public static final double default beta public params public params double alpha double beta public double name cls public void name ev public void name ev public boolean public void train public iterator classes class signature 
constructors specifies parameters tree mutation probabilities algorithm weights specified method classes method 
parameters specify respectively definitions discussed section 
parameter specifies type feature extractor see section details feature extractors 
trained method needs look computed value queried node exists tree simple formula computing correct probability return corresponding node exists tree 
methods simply collect evidence nodes tree performing computations 
method returns true exactly evidence added removed tree time train called 
train method executes final algorithm described section 
chapter haystack learning architecture haystack existing machine learning architecture developed mark rosen master thesis 
interfaces developed specific textual learning algorithms precisely vector space learning algorithms 
tree learning uses hierarchical description object frequency vector existing machine learning architecture generalized haystack applications able classifiers black box manner regardless types features classifiers 
devising general interface machine learning classes haystack important consideration disparate models algorithms learning variety features 
example vector space models features bag words representation commonly text classification algorithms 
hand simple agent filing documents author may simply author attribute documents feature 
tree learning algorithms hierarchical description object feature 
disparity determined top level interface machine learning classes classifiers support universal methods adding training data classifying test data giving instructions get features arbitrary resource object 
features corresponding resource objects abstractly represented 
featurelist class feature extraction framework finalized featurelist class created encapsulate features relevant particular algorithm see 
note way featurelist created calling public final class featurelist implements serializable public featurelist resource ex public int public iterator iterator featurelist class 
constructor forces featurelist instances created limits possibility developer mistakenly give algorithm wrong kind features see section 
developers simple means getting features object knowing gotten doing preprocessing resource 
featurelist class meant wrapper features algorithm may find important learning 
instance text learning algorithm may expect featurelist contains element may expect featurelist contains just 
resource features wrapped list opposed just providing single feature creating feature class learning algorithms may feature 
example composite algorithm recommending web pages performing textual learning performing tree learning 
interface interface methods appear created order encapsulate machine learning class instructions getting features relevant learning algorithm 
method returns array public interface public object resource public featurelist resource interface 
containing features resource constructor featurelist class developers call method explicitly 
method returns featurelist containing features resource relevant feature extractor 
machine learning class provide corresponding getting features uses resource access haystack data store 
examples implementations interface tree learners appear sections 
addition interface created factories build objects 
interface order capture machine learning sense interface appears written 
contains methods train interface public boolean public void train public interface 
machine learning class execution test data tests machine learning algorithm ready perform function classifier just trained provides developer means accessing resource extract featurelist containing features particular needs 
supplied provides hook haystack data store means agents help get features 
alternative scheme force machine learning algorithms general methods adding data testing data resource object parameter circumventing need feature extractors 
framework loses level abstraction failing decompose tasks learning extracting features 
machine learning algorithms need know resources learning need know mappings features labels generally idea give classes information need 
algorithm needs know resources learning feature extractor include resource object feature 
example underlying type need known follows 
call get 
feature extractor black box manner get features resources added training data 
add training data 
call train method untrained classifier queried behavior unspecified exception may thrown 
feature extractor test resources get relevant features test data 
call testing methods extracted feature lists call classify creating general classifier interfaces framework place creating general interfaces various types classifiers binary multi class cetera easy 
interface appears illustrative example classifiers structured 
add method adds training data having specified feature list specified public interface extends public void add featurelist features boolean cls public double classify featurelist features interface 
class label classify method returns double representing class positive true negative false magnitude measures confidence classifier prediction correct 
addition interface interfaces created 
discussion omitted similarity interface 
lack need methods removal training samples included interfaces 
existing learning interfaces haystack support remove operation 
incorporating text learning mark rosen developed existing textual learning framework haystack see methods adding labeled training data feature parameter bag words class labels objects 
incorporate rosen interfaces labels data transformed typesafe type 
additionally interfaces supported operations general framework level apparent major changes existing code classes need 
abandoning rosen text classifier interfaces text classifier classes created 
example class black box access methods interface implement general haystack learning interface 
existing implementations simply extend consequently implementers interface 
similar classes text classification interfaces 
substantive method belonging class method methods just call obvious corresponding method interface add method calls method 
method returns utility class written capable returning corresponding specified resource 
works connecting agents haystack designed purpose extracting text indexing text bag words representation respectively text extraction agent lucene agent 
incorporating tree learning tree learning package developed haystack learning framework generalized incorporated framework 
accomplished class appears 
just class class black box calls methods interface implement general interfaces 
order implement general interfaces tree learner needs extend class implement methods interface 
substantive method belonging class public class implements protected public void add featurelist features boolean cls public void add featurelist features boolean cls double confidence public void add featurelist features cls public void add featurelist features cls double confidence public double classify featurelist features public featurelist features public manager class 
method methods just call obvious corresponding method interface add method calls method 
method returns extractor gets objects resource objects 
currently implementations pertinent tree learners getting hierarchical description url getting hierarchical description tabular location links 
described sections 
leaf name extractor specified sole constructor 
chapter creating web page recommendation system haystack chapter describes driving application thesis web page recommendation system haystack built haystack learning framework specifically interface existing functionality haystack web spider capable finding links pages 
extracting resources interface underlying implementation factory class objects suitable tree learning interface created 
tree learning module uses hierarchical descriptions objects underlying features put feature list returned feature extractor objects type 
hierarchical features thesis url tabular location link url content classes created getting corresponding resource url tabular location 
implementing implementing class getting hierarchical description resource url simple require interaction rest haystack system class relied agents haystack 
nontrivial methods needed rearrange information urls constructing returned 
methods class appear 
public class implements public static public static resource resource public static resource public featurelist resource public object resource class 
singleton design pattern class instances carry meaningful state method returns sole instance 
static methods provide important functionality 
purpose method map urls represent web page string 
instance urls www cnn com business index html www cnn com business represent web page regarded examining history visits web pages recommender agent may recommend pages visited recommend page twice 
accomplish task heuristic removing su xes index html performed practice 
final detail method specified stable sense calling multiple times time feeding output previous normalization change return value 
formally property represents set urls 
second significant method important stabilization property calling passing resource ect returned 
property stated formally represents 
practice achieved calling method 
method calls urls heuristically defined represent link hierarchical description 
characters resource name delimit names hierarchy replaced 
hierarchically meaningless prefixes www erased 
components urls consist solely numerical digits removed 
instance string cnn com law fbi watch list ap changed cnn com law fbi watch list ap 
number removal ad hoc heuristic adopted sites archive stories date addition content readers may care story came yesterday today 
particular case cnn appears significant position law user browsing behavior completely irrelevant stories recommended 
final modification performed name representing host 
host names organized specific name delimited 
character money cnn com 
host name tokenized order reversed 
path url tokenized character tokens passed constructor 
summary method called prefixes removed followed numbers 
host rearranged string tokenized transformation object 
example total transformation url www cnn com law fbi watch list ap index html com cnn law fbi watch list ap 
noted normalizing urls causes risk collision 
distinct urls map normalized 
tree learning framework serious problem urls highly correlated tree anyway forced distinct 
avoided placing full original unaltered url specific name node 
name collision observed practice unaltered url added unnecessary extra node time evidence added 
methods implemented straightforward calls 
implementing implementing class see getting hierarchical description resource tabular location links complicated extracting url 
url object completely encapsulated resource nearly impossible find links resource located link resource name 
able determine look links resources feature extraction performed 
public class implements public manager public resource starts public object resource public featurelist resource class 
accomplish constructors parameters provide starting points look links resources tabular locations need determined 
starts parameter specifies list pages starting points manager parameter provides hook haystack data store collection pages track stored 
html pages search links obtained java built html parsing capability exploited 
extract current tree path maintained parsing html 
time html tag table html tag td start tag current tree path adds hierarchical level tags decrease hierarchical level increment index counter new current level 
sequence table link table table tb link link occurs links link named link named link named 
distinct categorical evidence distinct resources appear tabular location time resource name appended 
getting list links rank order able recommend links list links newly web pages compiled 
thesis accomplished creating web spider starts initial pages searches internet standard graph search algorithms 
example start node www cnn com notice neighbors appear links web page explore neighbors 
allow easy di erent implementations spiders recommender system interface created see 
search method public interface public resource search resource starts interface 
interface takes array resources starting points search newly posted links see section help spider prioritize search search algorithms similar best 
rudimentary implementation spider built 
je article code guide class built see 
performs breadth search starting urls described public class implements public int maxdepth public resource search resource starts class definition 
starts 
uses java built html parsing capability class extract links web pages 
maxdepth argument constructor specifies depth breadth search 
noted commercial news sites cnn depth larger allows examination links start page causes large performance 
keeping sight goal spider part web page recommendation system user spider may want see web pages nodes deep search graph 
unfortunately finding deep pages requires expansion irrelevant nodes 
users creating systems web page recommenders presumably mechanism ranking web pages explored spider search web 
assumed web pages receive high scores link web pages receive high scores greedy search algorithm explores nodes receive high scores search deep graph wasting lot time expanding irrelevant nodes 
motivated creation class see 
argument constructor public class implements public int public resource search resource starts class definition 
value links encounters searches 
argument gives user precise control links expanded 
search method maintains queue links seen expands link highest value step search 
ranking links list links recommender agent needs determine links recommend 
motivated creation interface see 
methods adding removing observations public interface extends comparator public void observation obs manager public void observation obs public void train manager public boolean public double resource interface 
observation class wrapper resource associated object 
method needs hook haystack implementations interface able feature extracting framework see section may need haystack data store agents 
interface simpler classifiers need decision class object belongs complex classifiers responsible extracting features resources observations 
train method needs access feature extractor interface requires access 
interface extends comparator interface instances java arrays collections classes sort lists links descending order value simple call sort methods 
method returns double corresponding value specified resource 
noted value simply monotonic function semantic implications resource declared twice resource 
haystack learning framework provides straightforward way implement interface 
class built underlying 
significant detail implementation simply uses add classify methods underlying binary classifier binary classifier scratch time train method called algorithms support incremental update 
reasons choice 
implementations support adding new data classifier trained mentioned rosen thesis 
algorithms fast accomplish tasks acceptable time frame regardless trained scratch 
choice avoids serialization classifiers haystack shut underlying classifiers need implement serializable interface 
listening changes haystack database order haystack learning framework create service recommends web pages users mechanism learning components receive training data fact particular web page visited fact user placed resource certain category 
fortunately haystack mechanism place getting information 
listening relevant changes database requires creating appropriate objects adding suitable patterns listen 
encapsulate listeners abstracted away module class see 
manager argument class public manager public synchronized report resource public static class report public observation public observation public void public void class definition 
serves broadcasts changes database registered 
creates internal objects listen addition removal relevant rdf statements data store 
methods start internal listeners 
user calls begins listen changes continue listening changes called 
user directly hear changes database 
user call returns report object methods getting lists observations added removed haystack data store 
removals result url user think page interesting anymore 
addition returning observations method clears memory statements heard 
reason statements stored immediately broadcasted back visits pages user tracking randomly pick negative evidence complement visit evidence collected 
web pages immediately contacted time new url visited performance su ers unacceptably time user navigates web page negative evidence extractor contacts pages currently tracked 
important practical detail implementation objects separate thread add observations sets add remove 
adding removing observation resource checked ensure represents content 
accomplished attempting connect url resource represents 
reason separate thread methods objects allowed return quickly stall data store happen content test waits response 
putting provide top level functionality recommender agent haystack class created extension class 
main task writing class override init cleanup shutdown methods class 
method overrides straightforward implementations 
init method prepares components recommender system haystack starts running 
recommender service loads collection pages track maintained haystack user interface haystack data store loads collection uses contain recommended links haystack data store loads serialized tools uses rank web pages new starts listeners new spider performing searches new links 
spider creates depth 
greedy spider sophisticated decided class ideal ciency yielded results experimentation agent 
cleanup method prepares shut agents shut 
accomplish stops listeners incorporates final report occurred time agent run lost 
reason operations put cleanup method shutdown method code run shutdown method guarantee components haystack operational 
components haystack needed underlying machine learning algorithms access agents data store extract features newly added removed observations 
cleanup method run tasks need performed haystack shuts shutdown method overridden 
method overridden contains steps necessary update list recommended links 
performs tasks incorporates latest report trains newly collected data added uses spider find new links ranks newly links updates collection recommended links accordingly 
detail regarding method newly recommended links titles extracted utility class written thesis 
doing user browses collection recommended links entitled recommended links user sees titles recommended links urls 
chapter testing recommender system chapter summarizes results testing done analyze quality web page recommendations various ensembles machine learning classes haystack learning framework 
results show tree learning classifiers provide substantial increase recommending performance traditional textual analysis articles 
best classifier results composite classifier created tree learners tabular layout feature url feature 
adding text composite classifier help performance 
experimental setup tests data collected kai shih user study administered order test similar application daily 
user study subjects shown sequence home pages cnn asked click check box links liked page 
data leave cross validation performed testing accuracy predicting links user click pages data pages train 
just negative samples randomly added set observations equal number positive samples added adding negative samples may algorithms slight advantage normally 
enforced negative samples added links clicked just recommender agent 
precision recall curves created results examining predicted ranking links user clicked test pages 
precision recall averaged curves user 
final detail regarding experiments missing text data links 
problem user study data year old missing data may important initial user study purposes able recovered 
text available links redirects incidentally issue current implementation text extraction agent haystack 
text available stories cnn posted associated press apparently stories removed cnn website shortly posted 
having missing data reasonable workaround ignore data regarding links text extracted 
noted exclusion training test data potentially slightly bias results classifier way practice excluding links stories extracted text slightly ected precision recall curves types classifiers 
criticism experimental setup section 
results section presents data obtained experimental setup described section 
section various combinations classifiers experiments 
provide concise consistent notation map standardized names type see table 
composite classifiers created merging binary classifiers 
example url feature combined nave nave bayes text classifier body text feature rosen nave nave bayes text classifier body text feature simple discrete url feature discrete layout feature discrete ul composite classifier url layout features discrete composite classifier url layout features plus nave bayes text classifier beta url feature beta layout feature beta ul composite classifier url layout features beta composite classifier url layout features plus nave bayes text classifier buckets url feature buckets layout feature buckets ul composite classifier url layout features buckets composite classifier url layout features plus nave bayes text classifier table short hand notation various described classifier 
layout feature 
devise theoretically sound model synthesizing classifiers mixture experts average classifiers outputs taken output composite classifier 
reasonable implementations tree learner interface nave bayes text classifier outputs determined estimation probability tested resources bad return values geometric interpretation test data dot product feature vector vectors rocchio support vector machine classifier 
output test sample meaning case probability membership class regardless underlying classifier 
notable detail di erent nave bayes text algorithms 
mark rosen implementation sophisticated appeared perform better web page recommender implementation example best web page recommending performance expected standard text classification algorithm 
values returned classify method directly correlated predicted probability documents function log likelihood sense classifier composite classifiers simple form thesis 
simple nave bayes implementation created classify method returned value semantic meaning tree learners classify methods return value minus twice predicted probability object bad 
simple nave bayes implementation composite classifiers text 
indicated table rosen classifier denoted nave simple nave bayes implementation denoted nave 
url feature test performance various tree learning algorithms web page url feature performance discrete versus beta versus buckets tested 
performances nave nave included baselines 
resulting precision recall curves see showed tree learners significantly outperformed nave nave 
discrete beta yielded top ranked links times declared interesting average article compared factor top recommendations nave 
buckets perform tree learners 
recall discrete beta buckets nave nave precision recall curves discrete beta buckets nave nave 
layout feature test performance various tree learning algorithms tabular location web page links features performance discrete versus beta versus buckets tested 
performances nave nave included baselines 
resulting precision recall curves see showed tree learners significantly outperformed nave nave 
discrete beta yielded top ranked links times declared interesting average article compared factor top recommendations nave 
analogously previous section buckets perform discrete beta 
url layout features test performance composite classifiers combined hierarchical features url position links layout performance versus beta ul versus buckets ul tested 
performances nave recall discrete beta buckets nave nave precision recall curves discrete beta buckets nave nave 
nave included baselines 
resulting precision recall curves see showed tree learners significantly outperformed nave nave 
discrete ul beta ul yielded top ranked links times declared interesting average article compared factor top recommendations nave 
analogously previous sections buckets ul perform discrete ul beta ul 
text url layout features test adding textual analysis improve recommendations higher performing classifiers previous sections discrete ul discrete beta nave nave provided baselines 
bucket tree learner slightly outperformed previous experiments omitted comparison performed section 
resulting precision recall curves see showed adding text simple composite classifiers created thesis provide recall discrete ul beta ul buckets ul nave nave precision recall curves discrete ul beta ul buckets ul nave nave 
noticeable improvement recommendation quality 
suspected reasons 
text classifier composite classifier text classifiers 
algorithm composing classifiers may simple facilitate incremental improvement text classifier 
possible text captures similar subset information url captures subjects articles adding text composite classifier uses url little benefit 
composite versus url layout comparisons successful types classifiers discrete ul compared discrete discrete beta ul compared beta beta recall discrete beta discrete ul beta ul nave nave precision recall curves discrete ul beta ul discrete beta nave nave 
tree learners compared slightly inferior performance 
resulting precision recall curves see suggest composite classifiers may slightly better highest performing single feature classifiers 
performances part roughly equal low recall curves composite classifiers ered robust performance higher precision intermediate recall values 
classifiers yield close performance decisive drawn 
justification negative samples benefits tree learning algorithms predictions links user having negative samples shown 
thesis claimed important provide negative samples tree learners order provide better recommendations 
claim motivates experiment tree learners compared identical counterparts random negative samples added 
recall discrete ul beta ul discrete beta discrete beta precision recall curves discrete ul beta ul discrete beta discrete beta 
result precision recall curves figures created tree learners url feature 
superior performance tree learners random negative samples suggests imperative provide tree learners negative samples optimal performance 
shows feature choice changed layout feature result di erent tree learners negative samples perform just tree learners received random negative samples 
results mixed drawn experiment appears safer add negative samples add 
summary results types tree learners discrete bayes beta system tree learners provide best performance discretized continuous tree learner explicitly developed solve problem discrete bayesian algorithm see section 
trend surprising possible number buckets low unable successfully model recall discrete beta discrete neg 
beta neg 
precision recall curves discrete beta compared curves algorithms negative samples added 
continuous distribution 
reason buckets learning classification time scaled quadratically number buckets 
possible bucket algorithm merely required training samples performance surpass discrete bayes tree learner 
training data subject meager precise trends browsing behavior identified dozen positive training samples 
expected systematic weakness discrete bayes algorithm show experiment 
text competitive tree learners training data provided 
various feature sets combination url layout position yield best performance 
reason url layout position correlated similar subjects placed similar areas layout position er additional information story subject front page news story headline 
recall discrete beta discrete neg 
beta neg 
precision recall curves discrete beta algorithms compared curves learners negative samples added 
training data smarter way synthesizing multiple classifiers just arithmetic mean adding textual learning top performing classifiers expected improve recommendations 
experiments show enhancements textual learning best provides little incremental value recommendation system built tree learning urls layout positions 
criticism experimental setup test results encouraging experimentation recommender agent suggests system works faults experimental setup warrant brief discussion 
training data experimental setup purer training data agent get real world setup things agent unable distinguish intermediate step locating story read destination user navigation story 
user study users explicitly select stories read 
cnn web page web pages may di erent formats may ect quality recommendations 
example com stories exhibit organized hierarchical url format tabular format useful recommendations sites 
additionally user study carried short run home pages tests unable show di erent classifiers perform long run 
classifiers increase accuracy may showing systematic problems receive lot data see section 
trends observed test results su ciently marked succeed demonstrating macroscopic result url tabular location links appears essential providing accurate recommendations user 
chapter result thesis working web page recommending agent created haystack 
choices algorithms agent parameters existence transparent users choose 
hand provides simple interface reasonably recommendations chooses agent benefit recommendations minimal ort overhead 
tree learning algorithms developed thesis showed promise ering similar performance discrete bayes algorithm solving potential long run problem identified thesis 
thesis describes design implementation working application haystack ways thesis continued improved 
tree learning tree learning improved creation new algorithms reflect underlying correlation accurately current tree learning algorithms 
correlation urls layout positions user interest appear remarkably strong evidenced success tree learners experimental trials thesis tree learners robust addressing issues dynamic hierarchical elements date placed url bear little correlation user interest page tabular layouts top level banners added page tables moved high level nesting remains unchanged 
gracefully dealing issues theoretical manner heuristically provide robust classifier 
potential solution may form nearest neighbors string edit distance distance metric level taxonomy defined character 
potential solution model urls tabular layout positions hidden markov model 
haystack learning framework general structure haystack learning framework reasonable interfaces added allow users leverage implementers power fully 
example classifiers support removal data added 
learning new classifiers scratch unnecessary context class 
additionally interface machine learning classes support incremental updates helpful allow users guarantee incremental classifiers need retraining data added removed 
interface created probabilistic classifiers return estimation probability tested resource particular class classes support methods similar method 
recommender agent modifications may increase utility recommender agent haystack system 
better method combining binary classifiers arithmetic mean improve performance recommendations composite binary classifiers 
may additional features text url layout position increase accuracy recommendations 
may possible improve evidence reporting 
currently technique adding negative samples fairly ad hoc randomly chooses negative samples pages tracked 
may desirable provide separate list recommendations page tracked users control view top recommendations 
order handle situations user tastes change may beneficial decay confidence training data ages 
providing stronger negative feedback recommended links ignored weaker positive evidence recommended links visited may desirable links receive exposure users may clicked things equal 
contributions lastly list major contributions course thesis 
criticism existing discrete bayes tree learning algorithm including identification potential systematic long run flaw 
proposition algorithms solve flaw bucket emulation bayesian network continuous parameter space intuitively developed algorithm implemented 

implementation tree learning package haystack including implementations tree learning algorithms 

development general haystack learning framework 

integration existing text learning framework newly created tree learning package general haystack learning framework 

development modular extensible lightweight web page recommending agent haystack 

demonstration capability agent testing 
bibliography corin anderson eric horvitz 
web montage dynamic personalized start page 
proceedings eleventh international conference world wide web pages 
acm press 
billsus pazzani 
hybrid user model news story classification 
je 
programming spider java 
technical report developer com 
heckerman 
tutorial learning bayesian networks 
ken lang 
newsweeder learning filter netnews 
proceedings th international conference machine learning pages 
morgan kaufmann publishers san mateo ca usa 
mark rosen 
mail classification haystack framework 
master thesis massachusetts institute technology 
lawrence shih david karger 
learning classes correlated hierarchy 

