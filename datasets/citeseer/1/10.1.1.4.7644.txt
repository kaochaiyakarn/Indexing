answer 
role context question answering jimmy lin dennis quan vineet sinha bakshi david huynh boris katz david karger mit ai laboratory lcs cambridge ma usa vineet boris ai mit edu karger lcs mit edu question answering systems proven helpful users provide succinct answers require users wade large number documents 
despite advances underlying question answering technology problem designing effective interfaces largely unexplored 
conducted user study investigate area discovered users prefer paragraph sized chunks text just exact phrase answer questions 
furthermore users generally prefer answers embedded context regardless perceived reliability source documents 
researching topic increasing amount text returned users significantly decreases number queries pose system suggesting users utilize supporting text answer related questions 
believe results serve guide developments question answering interfaces 
keywords question answering user study interface design question answering qa important widely researched technique information access deliver users exactly information need flooding documents wade 
current stateof art systems capable answering percent questions spanish explorer discovered mississippi unrestricted domain voorhees 
despite significant advances underlying technology question answering systems problem designing effective user interfaces largely unexplored 
developments question answering focused improving system performance standard set questions qa track trec conferences voorhees voorhees notable example 
batch run experiments neglect important aspect information access process human interactions actual system 
system improvements measured batch experiments may translate actual benefits users hersh problem interaction studied parallel 
notable exceptions katz current question answering systems text return users fragments text containing answer queries 
sense question answering related information access techniques document retrieval entire documents retrieved passage retrieval chunks text returned 
unique challenge advantage question answering systems promise deliver succinct answers directly satisfy users information needs phrased natural language 
naturally begs question exactly qualifies succinct answer 
text question answering system return 
questions seek explore 
believe natural response presentation style question answering systems focus leung apperley closely related overview plus detail green presentation style 
system directly answer user query provide additional contextual information 
current question answering systems extract answers textual documents text surrounding answer serves natural source context 
images sounds multimedia segments may provide better answers focus textual responses 
sample interaction question answering system focus bold paragraph context question man moon 
answer neil armstrong neil armstrong commander apollo mission moon 
july armstrong man walk moon famous statement small step man giant leap mankind setup attempt address question context text various interface conditions tested experiment 
shown responses question man reach south pole exact answer top left answer sentence middle left answer paragraph bottom left answer document right 
question answering system return user 
explored variables affect context preferences source reliability trustworthiness source text scenario size user asks single question set related questions 
related question answering systems studied extensively katz brill hovy katz moldovan lin research focused underlying answer extraction algorithms 
knowledge studies regarding effects context conducted qa systems 
context traditional information retrieval ir systems extensively studied 
effectiveness spatial temporal contextual clues park kim category labels dumais top ranking related sentences white explored empirically user studies 
furthermore interactive track trec generated interest information retrieval interfaces example belkin 
compared different views ir results question answering task 
studies unclear results gathered studying traditional information retrieval interfaces directly applied question answering 
believe context fundamentally see hearst overview ir interfaces 
different purpose question answering warrants separate research 
information retrieval systems return list documents user browse extract relevant answers research focused supporting browsing behavior reducing cognitive load 
example structural temporal contexts help users navigate collection hypertext documents category labels give users general idea documents topics 
goal question answering systems different seek directly provide information satisfies user information need obviating need browsing 
believe role context qa systems support browsing justify answer offer related information 
interface conditions amount context returned user continuous variable discretize context order support experiments 
focus plus context framework account natural language discourse principles developed different interface conditions 
case focus answer user question 
context simply text surrounding answer varied length different interface conditions 
detail see exact answer 
exact answer returned user additional context 
ex ample exact answer battle april 
exact answers named entities dates locations names noun phrases verb phrases 
answer sentence 
exact answer returned user sentence answer extracted 
answer paragraph 
exact answer returned user paragraph answer extracted sentence containing answer highlighted 
answer document 
exact answer returned user entire document answer extracted sentence containing answer highlighted 
user study conducted user study investigate effects variables user preferences regarding context interface conditions described previously reliability source size scenarios 
hypothesized trustworthiness source inversely correlated amount context required user judge particular answer user require context accept answer trustworthy source trustworthy 
hypothesized users researching topic asking multiple related questions context play important role answer related questions surrounding text 
presenting users scenarios contained single question multiple related questions explored relationship question answering document browsing 
methods graduate undergraduate students asked participate experiment 
participants ages strong backgrounds computer science 
participants experienced searching information web experience question answering systems 
experiment divided parts phase tested effects source reliability second tested effect scenario size 
starting study users brief question answering systems 
prior phase second phase users asked complete short surveys 
study concluded open ended interview participants encouraged share general thoughts 
purpose study investigate effectiveness actual question answering system isolate criteria effective interfaces study worked system answer test questions percent accuracy 
answers taken electronic version encyclopedia 
surveys short question answering systems starting phase users asked general questions impressions importance question answering various tasks point likert scale ranging important important replacing augmenting normal web search engines accessing personal documents email interacting operating system applications natural language command interface researching factual information writing report finding facts troubleshooting finding wrong computer 
addition asked users rate importance source reliability additional context respect satisfaction point scale 
purpose survey elicit users preconceived notions question answering systems 
study concluding interview users survey addition questions 
addition questions survey asked users interface condition preferred 
asked importance factors affecting question answering system speed multimedia responses presentation alternative answers 
goal exit survey determine users views question answering changed 
source reliability phase study implemented clickthrough experiment determine context user needed order accept reject answer depending perceived trustworthiness source document 
eighteen questions see examples user randomly associated trust conditions trusted answer obtained neutral generally reputable source encyclopedia 
biased answer obtained source known biased viewpoints advocacy site particular special interest group 
relatively obscure questions purposely chosen reduce chance user know answer directly 
examples questions phase 
day war 

person reach south pole 

rosenberg trial 

devil tower 

active ingredient 
examples questions phase ii scenario battle 
state battle 
won battle 
scenario won nobel peace prize 
scenario marilyn monroe real name 
marilyn monroe born 
marilyn monroe born 
marilyn monroe die 
scenario capital 
sample questions user study 
unknown answer obtained source authority established personal homepage 
focus study perceived reliability source actual source source citation 
answer source labeled trust conditions described question originate 
answer portugal source trusted source 
furthermore actual answer context change labeling 
start question exact answer indication source reliability 
user choices accept believe answer move question reject believe answer move question request information request information 
user requested information interface condition click information gave answer sentence interface condition second time gave answer paragraph interface condition third time gave answer interface condition 
entire document user choose importance qa various tasks standard error mean 
importance various factors affecting qa systems standard error mean 
accept reject answer 
information option basically undo operation reverting back previous interface condition 
phase user study major goal determine context user needed order accept reject answer source document user require judgment regarding validity system response 
scenario size second phase study participants asked directly interact sample question answering system 
goal complete series scenarios quickly possible 
scenario consisted single question set questions topic see examples 
phase user study total scenarios single question questions questions questions 
scenario randomly associated fixed interface condition users preference various interface conditions 
previous phase users request context 
scenario considered complete users entered answer question text box beneath question clicked button 
goal phase measure time number queries required complete scenario 
users told interact question answering system way wanted typing questions necessary reading contextual information desired 
results users opinions importance question answering various tasks shown 
choices believed question answering important fact finding research important interacting system application 
tests showed study alter users opinions statistically significant way 
users views factors affect satisfaction question answering system shown 
initially believed source reliability context text response equally important 
opinions changed dramatically user study 
importance source reliability showed statistically significant drop 
conversely importance context rose statistically significant amount 
shows users interface condition preferences 
discovered users liked paragraph interface condition best exact answer interface condition 
majority users remarked paragraphs formed size effect source reliability amount context required judge answer standard error mean 
percentage answers accepted various conditions 
chunk information exact answer little entire document 
noted sentence doesn give just exact answer displaying sentence containing answer provide user useful amount additional information 
example sentence answering question particular person birthday may simply born march particular pronouns posed big problem sentences pronouns taken context meaningfully interpreted 
coreference resolution technology integrated qa systems address issue 
source reliability effect source reliability amount context required judge answer shown 
bar graph shows average number times user clicked information judgment ac cumulative percentage clicks making judgment 
cept reject answer line graph shows cumulative distribution 
trusted unknown sources users needed paragraph average form judgment answer trusted sources users needed paragraph 
anova revealed difference clicks statistically significant difference biased unknown conditions ns 
users final judgments answers shown 
trusted sources users accepted nearly answers percentage near percent conditions 
anova revealed differences statistically significant tiny difference biased unknown conditions ns 
interviews confirmed source reliability important users initially thought reflected surveys 
users surprised didn care source answer came 
compelled read portion text making judgment regardless source reliability 
instructions phase clearly stated source reliability varied question answering system generally reliable count system correctly extract document 
users remarked just didn trust computer want ed check source scenario size results phase grouped scenarios multi question scenarios com completion time scenarios standard error mean 
number questions posed scenario standard error mean 
times shown number questions posed users shown 
multi question scenarios answer document interface condition resulted lower average completion time difference statistically significant ns 
small variations completion time single question scenarios statistically significant proved control 
multi question scenarios different interface conditions statistically significant impact completion time effect number questions needed complete scenario significant 
answer document interface condition users asked average half questions exact answer interface condition 
ex pected anova reveal statistical significance slight difference number questions asked single question scenarios demonstrating validity control 
discussion setup source reliability experiment compromise investigating complex dependent variables maintaining feasibility study 
retrospect may assumptions 
original intent away subtle judgments reliability simply assert judgments user pretend answer came source considered trusted biased unknown knowledge users understood sources maliciously disseminating false information 
unfortunately users interpreted source reliability external judgment computer system despite clarifications instructions 
echoed comments don trust computer saying source trusted users suggested actual citations evaluate source reliability 
added additional layer complexity study 
believe considerations negate results simply relabeling sources differently produced statistically significant effect user population 
necessary order sort complex factors play 
different interface conditions significant impact completion time scenarios users required fewer interactions complete task fewer questions 
believe significant result highlights role context plays question answering 
users additional surrounding text read 
context helps users confirm answer respond additional related questions 
potential objection validity results effect conducted study canned system returned correct answer 
state art question answering far able achieve just currently best systems successfully answer percent types questions studied voorhees 
focus studying qa interfaces goal providing longer term guidance development systems 
hope results useful design evaluations 
trend trec qa tracks number exactly people typing mistakes experimented system returning exact answers trec trec participants return byte paragraph length byte sentence length answers 
trec answers restricted bytes 
trec exact answers accepted 
forcing question answering systems return exact answers correct technological push exact answers force systems develop sophisticated natural language processing techniques studies show users prefer chunks exact answer 
identifying exact answers prevent systems displaying text final response actual user preferences kept mind deploying qa systems designing evaluations 
study revealed features users considered important question answering systems 
requested feature ability question answering system resolve pronouns ellipses questions able follow question battle current question answering systems treat question independently unable maintain state preserving prolonged interaction 
addition discovered effective question answering systems able extract short answers specific questions respond general questions meaningful way 
faced multi question scenario users attempt ask general question marilyn monroe hopes obtaining general article 
question answering system designed handle queries users received response 
interviewed asked general questions users responded hoped get important information case marilyn monroe expected get biography include birthdate real name films starred ability answer general questions aspect question answering warrants research 
ways question answering represents step information access technology 
promising deliver answers just documents question answering systems effectively fulfill users information needs 
relatively new field average approximately half question classified general queries 
queries qualitatively affect results users equally ask general questions different interface condition 
question answering research focused primarily underlying technology computer human interaction issues 
advances answer extraction technology followed similar advances interface design 
research revealed interesting technological drive actual user preferences 
question answering systems evolving providing exact answers studies shown users prefer paragraph level chunks text appropriate answer highlighting 
order design effective question answering systems believe user considerations treated equal footing underlying technology 
extended version interactive poster published chi title role context question answering systems supported darpa contract number additional funding provided mit ntt collaboration mit oxygen project packard foundation fellowship ibm 
wish mark ackerman susan dumais greg helpful comments earlier drafts users participated study 
nicholas belkin amy keller diane kelly jose perez sikora ying sun 

support question answering interactive information retrieval rutgers trec interactive track experience 
proceedings ninth text retrieval conference trec 
eric brill jimmy lin michele banko susan dumais andrew ng 

data intensive question answering 
proceedings tenth text retrieval conference trec 
susan dumais edward cutrell hao chen 

optimizing search showing results context 
proceedings sigchi conference human factors computing systems chi 
stephan green gary marchionini catherine plaisant ben shneiderman 

previews overviews digital libraries designing surrogates support visual information seeking 
technical report cs tr department computer science university maryland 
marti hearst 

user interfaces visualization 
ricardo baeza yates berthier ribeiro neto editors modern information retrieval 
addison wesley longman publishing 
william hersh andrew susan price dale kraemer benjamin chan daniel olson 

batch user evaluations give results 
analysis trec interactive track 
proceedings eighth text retrieval conference trec 
eduard hovy ulf chin yew lin deepak ravichandran 

knowledge facilitate answer pinpointing 
proceedings th international conference computational linguistics coling 
boris katz jimmy lin sue 

start multimedia information system current technology directions 
proceedings international workshop multimedia information systems mis 
boris katz 

annotating world wide web natural language 
proceedings th riao conference computer assisted information searching internet riao 
ying leung mark apperley 

review taxonomy distortion oriented presentation techniques 
acm transactions computer human interaction 
jimmy lin aaron boris katz gregory stefanie 

extracting answers web knowledge annotation knowledge mining techniques 
proceedings eleventh text retrieval conference trec 
dan moldovan marius pas ca sanda harabagiu mihai 

performance issues error analysis open domain question answering system 
proceedings th annual meeting association computational linguistics acl 
park kim 

effects contextual navigation aids browsing diverse web systems 
proceedings sigchi conference human factors computing systems chi 
ellen voorhees 

overview trec question answering track 
proceedings tenth text retrieval conference trec 
ellen voorhees 

overview trec question answering track 
proceedings eleventh text retrieval conference trec 
white ian jose 

finding relevant documents top ranking sentences evaluation alternative schemes 
proceedings th annual international acm sigir conference research development information retrieval sigir 
