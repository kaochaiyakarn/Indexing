journal machine learning research submitted revised published unified framework model clustering shi zhong zhong cse edu department computer science engineering florida atlantic university boca raton fl usa joydeep ghosh ghosh ece utexas edu department electrical computer engineering university texas austin austin tx usa editor claire cardie model clustering techniques widely shown promising results applications involving complex data 
presents unified framework probabilistic model clustering bipartite graph view data models highlights commonalities differences existing model clustering algorithms 
view clusters represented probabilistic models model space conceptually separate data space 
partitional clustering view conceptually similar expectationmaximization em algorithm 
hierarchical clustering graph view helps visualize critical important distinctions similarity approaches model approaches 
framework suggests useful variations existing clustering algorithms 
new variations balanced model clustering hybrid model clustering discussed empirically evaluated variety data types 
keywords model clustering similarity clustering partitional clustering hierarchical agglomerative clustering deterministic annealing 
clustering segmentation data fundamental data analysis step widely studied multiple disciplines years hartigan jain dubes jain ghosh 
fundamental distinction discriminative distance similarity approaches indyk scholkopf smola vapnik generative model approaches rose smyth clustering 
exceptions vapnik jaakkola haussler considered primary dichotomy vast clustering literature partitional vs hierarchical popular choice far 
shall show discriminative vs generative distinction leads useful understanding existing clustering algorithms 
discriminative approaches clustering graph partitioning karypis determines distance similarity function pairs data objects groups similar objects clusters 
parametric modelbased approaches hand attempt learn generative models data model representing particular cluster 
categories popular clustering techniques 
done shi zhong phd student university texas austin 
shi zhong joydeep ghosh 
zhong ghosh include partitional clustering hierarchical clustering hartigan jain 
partitional method partitions data objects specified priori groups optimization criterion 
widely means algorithm classic example partitional methods 
hierarchical method builds hierarchical set nested clusterings clustering top level containing single cluster data objects clustering bottom level containing singleton clusters cluster data object total number data objects 
resulting hierarchy shows level clusters merged inter cluster distance provides visualization tool 
discriminative approaches commonly distance measures euclidean distance mahalanobis distance data represented vector space 
instancebased learning literature aha provides examples scenarios customized distance measures perform better generic ones 
high dimensional text clustering strehl 
studied impact different similarity measures showed euclidean distances appropriate domain 
complex data types variable length sequences defining similarity measure data dependent requires expert domain knowledge 
example wide variety distance measures proposed clustering sequences geva kalpakis qian 
disadvantage similarity approaches calculating similarities pairs data objects computationally inefficient requiring complexity 
despite disadvantage discriminative methods graph partitioning spectral clustering algorithms karypis dhillon meila shi ng strehl ghosh gained popularity due ability produce desirable clustering results 
model clustering approaches model type specified priori gaussian hidden markov models hmms 
model structure number hidden states hmm determined model selection techniques parameters estimated maximum likelihood algorithms em algorithm dempster 
probabilistic model clustering techniques shown promising results corpus applications 
gaussian mixture models popular models vector data mclachlan basford banfield raftery fraley yeung multinomial models shown effective high dimensional text clustering vaithyanathan dom meila heckerman 
deriving bijection bregman divergences exponential family distributions banerjee 
shown clustering mixture components member vast family done efficient manner 
clustering complex data time sequences dominant models markov chains cadez ramoni hmms kokkinakis smyth oates law kwok li biswas 
compared similarity methods model methods offer better interpretability resulting model cluster directly characterizes cluster 
model partitional clustering algorithms computational complexity linear number data objects certain practical assumptions analyzed section 
existing works model clustering largely concentrate specific model application 
notable exception cadez 
proposed em framework partitional clustering mixture probabilistic models 
essentially em cluster unified framework model clustering ing emphasis clustering non vector data variable length sequences 
address model hierarchical clustering specialized model partitional clustering algorithms self organizing map som kohonen neural gas algorithm martinetz varying neighborhood function control assignment data objects different clusters 
provides characterization existing model clustering algorithms unified framework 
framework includes bipartite graph view model clustering information theoretic analysis model partitional clustering view model hierarchical clustering leads useful extensions 
listed main contributions 
propose bipartite graph view section data models provides visualization understanding existing model clustering algorithms partitional hierarchical algorithms 
partitional clustering view conceptually similar em algorithm 
hierarchical clustering points helpful distinctions similarity approaches model approaches 

conduct information theoretic analysis model partitional clustering demonstrates connections existing algorithms including means em clustering som neural gas deterministic annealing point view section 
deterministic annealing clustering wong hofmann buhmann rose gaussian models 
analysis model clustering algorithms perspective gives new insights means em clustering provides model extensions som neural gas algorithms 
benefits synthetic view demonstrated experimental study document clustering 

analysis model approaches vs similarity approaches hierarchical clustering section leads useful extensions model hierarchical clustering hierarchical cluster merging extended kullback leibler divergences 

unified framework obtain new variations model clustering balanced clustering section hybrid clustering section tailored specific applications 
variations show promising results case studies 
organization follows 
section presents unified framework model clustering synthetic view existing model partitional hierarchical clustering algorithms 
section introduces commonly clustering evaluation criteria 
section compares different models different model partitional clustering algorithms document clustering 
section describes generic balanced model clustering algorithm produces clusters high quality comparable sizes 
section proposes hybrid clustering idea combine advantages partitional hierarchical model clustering methods 
experimental results show effectiveness proposed algorithms 
section summarizes related 
section concludes 

term signifies specific application general em algorithm dempster treats cluster identities data objects hidden indicator variables tries maximize objective function equation em algorithm 
zhong ghosh 
unified framework model clustering section unifying bipartite graph view probabilistic model clustering demonstrate benefits having viewpoint 
section model partitional clustering mathematically analyzed deterministic annealing perspective reveals relationships generic model means em clustering deterministic annealing som neural gas algorithms 
model hierarchical clustering discussed section distinction model similarity hierarchical clustering 
practical issues including complexity analysis discussed section 
bipartite graph view bipartite graph view assumes set data objects sequences represented probabilistic generative models hmms corresponding cluster data objects 
bipartite graph formed connections data model spaces 
model space usually contains members specific family probabilistic models 
model viewed generalized centroid cluster typically provides richer description cluster centroid data space 
connection object model indicates object associated cluster connection weight closeness log likelihood log 
bipartite graph view model clustering 
readers may immediately notice conceptual similarity view em algorithm dempster general algorithm solving maximum likelihood estimation incomplete data 
partitional clustering cluster indices data objects treated missing data em algorithm employed estimate model parameters maximize incomplete data likelihood 
bipartite graph view equivalent em algorithm reasons 
algorithm provides 
interchangeably represent model set parameters model 
set parameters modeling dataset represented lk 
includes cluster priors soft clustering 
unified framework model clustering visualization model clustering 
second encompasses hierarchical model clustering involve incomplete data 
idea representing clusters models generalizes standard means algorithm data objects cluster centroids data space 
models provide probabilistic interpretation clusters desirable feature applications 
variety hard soft assignment strategies designed attaching connection association probability connection weights 
hard clustering probabilities 
intuitively suitable objective function sum connection weights log likelihoods weighted association probabilities maximized 
maximizing objective function leads known hard clustering algorithm kearns li biswas banerjee 
show section soft modelbased clustering obtained adding entropy constraints objective function 
similar deterministic annealing temperature parameter regulate softness data assignments 
model partitional clustering imposed logical neighborhood structure cluster models 
straightforward design model partitional clustering algorithm iteratively retrain models re partition data objects 
achieved applying em algorithm iteratively compute hidden cluster identities data objects step estimate model parameters step 
alternative techniques applied 
model partitional clustering bipartite graph view allows extensions 
examples impose structure models model space constrain partitioning data objects data space certain ways 
grid map structure imposed relationship models get som model partitional clustering algorithm clustering process relative distance different clusters conform imposed topological ordering 
example heskes multinomial model som market basket data analysis 
second extension idea introduce section balanced model clustering algorithm zhong ghosh produce balanced clusters clusters comparable number data objects improve clustering quality balance constraints clustering process 
alternatively initialize hierarchically merge clusters model space resulting model hierarchical clustering algorithm 
difference standard single link complete link hierarchical clustering algorithms hierarchy built model space suitable measure divergence models 
graph view model hierarchical clustering 
model partitional clustering section principled information theoretic analysis model partitional clustering 
derivation process similar deterministic annealing rose analysis provides common view useful generalization existing algorithms including means macqueen kokkinakis dhillon modha li biswas em clustering mclachlan basford banfield raftery cadez meila heckerman som kohonen neural gas martinetz 
resulting algorithm involves temperature parameter governs randomness posterior data assignments 
em clustering corresponds special case means clustering corresponds 
joint probability data object cluster 
aim maximize expected log likelihood log log 
note practice typically uses sample average calculate goes infinity sample average approaches expected log likelihood asymptotically 
directly maximizing objective function equation leads known hard clustering algorithm kearns li biswas call model kmeans mk means 
generalized version standard means algorithm macqueen unified framework model clustering lloyd iterates steps argmax log argmax log 
posterior probability equation conditioned current parameters simplicity confusion 
equation represents hard data assignment strategy data object assigned probability cluster gives maximum log 
equi variant spherical gaussian models vector data mk means algorithm reduces standard means algorithm 
known means algorithm tends quickly get stuck local solution 
way alleviating problem soft assignments rose 
introduce randomness softness data assignment step add entropy constraints equation 
set data objects set cluster indices 
new objective logp cluster prior entropy logp average posterior entropy mutual information parameter lagrange multiplier trade maximizing average log likelihood minimizing mutual information fix minimizing equivalent maximizing average posterior entropy maximizing randomness data assignment 
note added entropy terms change model re estimation formula equation model parameters maximize maximize solve constraint construct lagrangian lagrange multipliers partial derivative resulting known gibbs distribution geman geman 
known priori estimate data get model clustering algorithm parameterized parameter temperature interpretation deterministic annealing rose 
high temperature objective function equation smooth local solutions rose 
decreases zero posterior probabilities equation hard 
standard deterministic annealing zhong ghosh algorithm model clustering deterministic annealing input set data objects model structure temperature decreasing rate final temperature usually small positive value close 
output trained models partition data objects cluster identity vector steps 
initialization initialize model parameters set high large number 
optimization optimize objective equation iterating step equation step equation convergence 
annealing lower temperature parameter new old go step go back step 
data object set argmax deterministic annealing algorithm model clustering 
algorithm model clustering constructed shown 
note temperature em algorithm maximize objective equation cluster labels hidden variables equation equation corresponding step step respectively 
shown plugging equation equation setting reduces objective function log exactly incomplete data log likelihood objective function em clustering maximizes neal hinton 
goes equation reduces equation algorithm reduces mk means independent actual 
iterating equation equation gives soft model clustering algorithm maximizes objective function equation analysis clear mk means em clustering viewed special stages deterministic annealing algorithm respectively optimize different objective functions vs 
interesting view relationship mk means em clustering different traditional view kmeans regarded special case em clustering mitchell neal hinton 
larger indicates smoother objective function smaller number local solutions theoretically em clustering better chance finding local solutions algorithm 
justifies em clustering results initialize mk means viewed step deterministic annealing algorithm temperature decreases time 
reveals practice means initialize em clustering 
safer approach start high gradually reduce 
clustering melting achieved done slowly carefully wong 
unified framework model clustering stochastic variant mk means algorithm stochastic mk means described kearns 
posterior assignment opposed means assignment em assignment 
basic idea data object stochastically entirely fractionally assigned clusters posterior probability 
stochastic mk means viewed sampled version em clustering uses sampled step posterior probabilities 
generalized batch version som neural gas algorithms context model clustering show interpreted deterministic annealing point view 
distinct feature som topological map cluster fixed coordinate 
map location cluster exp neighborhood function 
argmax log 
batch som algorithm amounts iterating equation step parameter controlling width neighborhood function decreases gradually clustering process 
seen temperature parameter deterministic annealing process 
som viewed constrained step calculation posteriors actual log constrained topological map structure 
mechanism gives som advantage resulting clusters related pre specified topological map 
batch neural gas algorithm differs som algorithm calculated equivalent temperature parameter function cluster rank 
example takes value closest cluster centroid value second closest centroid value th closest centroid shown online algorithm converge faster find better local solutions som deterministic annealing algorithms certain problems martinetz 
model hierarchical clustering partitional clustering methods number clusters needs specified priori 
number unknown clustering problems 
prefers clustering algorithm return series nested clusterings interactive analysis seo shneiderman 
hierarchical clustering techniques provide advantage 
run means em clustering multiple times different numbers clusters returned clusterings guaranteed structurally related 
bottom hierarchical agglomerative clustering popular hierarchical method jain top methods steinbach 

researchers usually discriminate model similarity approaches hierarchical clustering algorithms 
contrast distinction model zhong ghosh hierarchical methods similarity ones 
ward algorithm centroid methods modelbased methods 
selects clusters merge maximizes resulting likelihood chooses clusters centroids closest 
methods spherical gaussians underlying models 
hand single link complete link methods discriminative methods data pairwise distances calculated form basis computing inter cluster distances 
design model hierarchical clustering algorithms needs methodology identifying clusters merge iteration 
define distance measure clusters models iteratively merge closest pair clusters 
traditional way choose clusters merging results largest log likelihood logp fraley meila heckerman 
distance method defined logp ore logp ter ore ter set parameters merging models respectively 
call measure generalized ward distance exactly ward algorithm ward equi variant gaussian models 
method efficient find closest pair needs train merged model pair clusters evaluate resulting log likelihood 
practice specific models ward distance efficiently computed fraley meila heckerman kullback leibler kl distance measure involve re estimating models commonly sinkkonen kaski ramoni 
exact kl divergence difficult calculate complex models 
empirical kl divergence juang rabiner models defined log log set data objects grouped cluster distance symmetric defining juang rabiner jensen shannon divergence lin js compared classical hierarchical agglomerative clustering hac algorithms kl divergence analogous centroid method 
shown gaussian models equal covariance matrices kl divergence reduces mahalanobis distance cluster means 
motivated observation single link complete link hac 
quantities defined section merging criteria termed distance colloquial sense may satisfy symmetry triangle inequality properties needed metric 
divergence technically correct term situations 

complex model high representational power able describe complex data non gaussian vectors variable length sequences 
unified framework model clustering algorithms propose modified kl distances 
corresponding single link define distance min log log corresponding complete link define distance max log log 
characterize high boundary density clusters building complex shaped clusters propose distance measure log log fraction smallest log log values 
value log log defines boundary cluster distance measure reduces distance contains data object kl distance measures sensitive outliers kl distance defined specific data object 
favorable property measure hierarchical algorithms distance analogous single link hac methods produce arbitrary shaped clusters 
guard outliers reap benefits single link methods set 
experimental results section demonstrate effectiveness new distance measure 
describes generic view model hac algorithm 
instances generic algorithm include existing model hac algorithms explored banfield raftery fraley gaussian models vaithyanathan dom multinomial models clustering documents ramoni 
markov chain models grouping robot sensor time series 
works ward distance equation fourth employed kl distance equation 
practical considerations general maximum likelihood estimation model parameters equation iterative optimization process estimation hmms needs appropriate initialization may get local minima 
clustering algorithms converge sequential initialization 
model parameters resulting previous clustering iteration initialize current clustering iteration guarantee objective equation decrease 
second observation ml model estimation leads singularity problem unbounded log likelihood 
happen continuous probability distribution probability density unbounded dx 
example gaussian models occurs covariance matrix singular 
discrete distributions happen upper bounded 
singularity problem dealt ways restarting clustering algorithm different initialization juang maximum posteriori estimation appropriate prior gauvain lee constrained ml estimation lower bound variance spherical gaussian models bishop 
zhong ghosh algorithm model hac input set data objects model structure output level cluster model hierarchy hierarchical partition data objects models clusters th level 
steps 
initialization start th level initialize data object cluster train model cluster max log 
distance calculation compute pairwise inter cluster distances appropriate measure measures defined equations 
cluster merging merge closest clusters assume re estimate model merged data objects max logp 
data objects merged cluster go back step 
model hierarchical agglomerative clustering algorithm 
third comment performance mk means stochastic mk means em clustering 
practice common see condition especially complex models hmms means equation dominated likelihood values close provided small 
suggests differences hard mk means stochastic mk means em clustering algorithms small clustering results similar practical applications 
look computational complexity model clustering algorithms 
consider partitional clustering involving models estimation model parameters closed form solution need iterative process gaussian multinomial iteration time complexity linear number data objects number clusters data assignment step model estimation step 
total complexity number iterations 
models estimation parameters needs iterative process hidden markov models gaussian observation density model estimation complexity clustering iteration number iterations model estimation process 
case total complexity clustering process 
theoretically number iterations large may increase bit practice maximum number iterations typically set constant empirical observations 
experiments em algorithm usually converges fast iterations clustering documents 
analysis applies mk means stochastic mk means em clustering 
model deterministic annealing algorithm additional outer loop controlled decreasing temperature parameter 
slower annealing schedule computationally expensive 
model hierarchical agglomerative clustering models clusters th level starts clusters bottom 
number inter cluster distances unified framework model clustering calculated bottom th level th level th level needs compute distances merged cluster clusters 
total number distances calculated hierarchy logic compute total number distance comparisons needed total complexity model estimation nm number iterations model estimation 
complexity reduced logn inter cluster distance comparisons clever data structure heap store comparison results jain 
clearly complexity logn high large datasets explains model hierarchical clustering algorithms popular partitional ones 
areas researchers hierarchical algorithms model specific tricks reduce computational complexity fraley meila heckerman 

clustering evaluation comparative studies clustering algorithms difficult general due lack universally agreed quantitative performance evaluation measures jain 
subjective human evaluation difficult expensive indispensable real applications 
objective clustering evaluation criteria include intrinsic measures extrinsic measures jain 
intrinsic measures formulate quality function data similarities models objective function clustering algorithm explicitly optimizes 
example data likelihood objective meila heckerman cluster text data multinomial models 
low dimensional vector data average summed distance cluster centers sum squared error criteria standard means algorithm common criterion 
extrinsic measures commonly category class labels data known course clustering process 
class predefined true data category cluster category generated clustering algorithm 
examples external measures include confusion matrix classification accuracy measure average purity average entropy mutual information ghosh 
ways compare partitions data set rand index rand fowlkes mallows measure fowlkes mallows statistics community 
measure information retrieval clustering serves way improving quality accelerating speed search 
purity cluster defined percentage majority category cluster 
entropy measures category spread uncertainty cluster normalized range dividing logk number classes 
objects cluster come category purity normalized entropy zhong ghosh 
cluster contains equal number objects category purity normalized entropy 
argued mutual information governing cluster labels governing class labels superior measure purity entropy dom strehl ghosh 
normalizing measure lie range relatively impartial choices normalization entropies 
shall follow definition normalized mutual information nmi geometrical mean nmi strehl ghosh 
corresponding sample estimate nmi log nn log log number data objects class number objects cluster number objects class cluster nmi value clustering results perfectly match external category labels close random partitioning 
simplest scenario number clusters equals number categories correspondence established external measures fruitfully applied 
example number clusters small accuracy measure intuitive easy understand 
number clusters differs number original classes confusion matrix hard read accuracy difficult impossible calculate 
situations nmi measure better purity entropy measures biased high solutions strehl strehl ghosh 
different measures explain case study measures 

case study document clustering performed extensive comparative study model approaches document clustering zhong ghosh 
section reports small subset study intent highlight unified framework proves helpful endeavor 
details data sets experimental setting comparative results relegated zhong ghosh focus experimental process 
particular compare different probabilistic model types mixtures multinomials von mises fisher distributions 
model type instantiate generic model clustering algorithms mk means stochastic mk means em deterministic annealing described section 
key observation pseudocode mk means different models model re estimation segment step 
needs changed 
code development easier experimental settings automatically kept different models ensure fair comparison 
traditional vector space representation text documents document represented high dimensional vector word counts document 
dimensionality equals number words vocabulary 

broad sense may represent individual words stemmed words tokenized words short phrases 
unified framework model clustering algorithm mk means input data objects model structure 
output trained model parameters partition data samples cluster identity vector steps 
initialization initialize model parameters cluster identity vector 
model re estimation cluster parameters model re estimated max log 
sample re assignment data sample set argmax log 
change go back step 
common mk means template document clustering case study 
models multinomial models quite popular text clustering meila heckerman standard formulation estimating model parameters mccallum nigam laplace smoothing avoid zero probabilities 
second model uses von mises fisher distribution analogue gaussian distribution directional data sense unique distribution normalized data maximizes entropy second moments distribution mardia 
long time folklore information retrieval community direction text vector important magnitude leading practices cosine similarity normalizing vectors unit length norm 
model directional data worthwhile consider 
pdf vmf distribution exp normalized data vector normalized mean vector bessel function normalization term 
measures directional variance dispersion higher peaked distribution maximum likelihood estimation simple estimation difficult due bessel function involved banerjee ghosh banerjee 
means clustering setting assumed clusters clustering results depend ignored 
case evaluate average cosine similarity displaced log likelihood objective minimized 
em clustering maximum likelihood solution derived banerjee 
including computationally expensive updates convenience simpler soft assignment scheme discussed section 
vmf models word count document vectors log idf weighted normalized 
idf stands inverse document frequency 
log idf weighting common practice information retrieval community de emphasize words occur documents 
weight word log number documents number documents contain word normalization required zhong ghosh vmf distribution directional distribution defined unit hypersphere capture magnitude information 
datasets newsgroups dataset number datasets cluto toolkit karypis 
datasets provide representation different characteristics number documents ranges number terms number classes balance 
balance dataset defined ratio number documents smallest class number documents largest class 
value close indicates un balanced dataset 
summary datasets section shown table 
additional details data characteristics preprocessing zhao karypis zhong ghosh 
data source balance ng newsgroups ohsumed hitech san jose mercury trec webace tr trec tr trec tr trec tr trec table summary text datasets 
dataset total number documents total number words number classes average number documents class 
experiments simplicity introduce abbreviations instantiated multinomial model algorithms mk means stochastic mk means em deterministic annealing referred multinomials stochastic multinomials mixture multinomials multinomial deterministic annealing respectively 
vmf algorithms corresponding abbreviated names 
soft vmf algorithm reason 
mentioned previously estimation parameter vmf model difficult needed mixture algorithm 
simple heuristic km iteration number 
set constant clusters iteration gradually increases iterations 
algorithm uses exponential schedule equivalent inverse temperature parameter km starting 
algorithm inverse temperature parameter created parameterize step 
annealing schedule set starts go 

kdd ics uci edu databases newsgroups newsgroups html 

www cs umn edu karypis cluto files datasets tar gz 
unified framework model clustering model algorithms relative convergence criterion likelihood objective changes multinomial models average cosine similarity vmf models iterative process treated converged 
situations vmf models ng dataset clustering process converges fewer iterations average 
largest average number iterations needed algorithm running ng clusters 
experiment run times time starting random balanced partition documents 
averages standard deviations normalized mutual information results reported 
nmi measure class labels document available number clusters relatively large 
recall nmi measures clustering results match existing category labels 
include results stateof art graph partitioning approach document clustering cluto karypis 
algorithm cluto toolkit default setting 
algorithm run times time randomly ordered documents 
note results regular means euclidean distance included known perform miserably high dimensional text data strehl 
discussion table shows nmi results ng datasets different number clusters dataset 
numbers table shown format average standard deviation 
boldface entries highlight best performance column 
number clusters affect relative comparison different algorithms range experimented study 
case datasets zhong ghosh 
save space show nmi results datasets specific table 
ng cluto table nmi results ng dataset unified framework allows distinguish effects assignment strategy impact probability models 
study vmf algorithms fare better multinomial ones significantly smaller datasets table indicating directional characteristic important high dimensional vector space representation text documents working normalized document vectors produces promising clustering results 
model soft algorithms perform better hard ones gain marginal large datasets separated clusters 
study run times zhong ghosh hitech tr tr tr tr cluto table nmi results hitech tr tr tr tr datasets zhong ghosh indicates algorithms soft assignment take slightly longer time hard assignments 
algorithm fastest deterministic annealing slower vmf distributions 

balanced model clustering problem clustering large scale data constraints balancing received attention data mining literature bradley tung banerjee ghosh strehl ghosh zhong ghosh 
balanced solutions yield comparable numbers objects cluster desirable variety applications zhong ghosh 
balancing global property difficult obtain near linear time techniques achieve goal retaining high cluster quality 
section show balancing constraints readily incorporated unified framework 
essentially needs perform balanced partitioning bipartite graph iteration em algorithm balanced step 
suggested approach easily generalized handle partially balanced assignments specific percentage assignment problems 
balanced model means focus balanced hard clustering posteriors 
simplicity nk binary assignment variable value indicating data object assigned cluster completely balanced mk means clustering problem written max nk log nk nk nk integer round closest integer slight changes nk holds 
problem decomposed subproblems corresponding step unified framework model clustering algorithm iterative greedy bipartitioning input log likelihood matrix nk log output partition matrix satisfies nk nk nk steps 
initialization set nk 
calculating log likelihood difference vector dv max nk dv dv 
bipartitioning sorted difference vector sort dv descending order assign top objects cluster set set indices corresponding top objects 
go back step 
iterative greedy bipartitioning algorithm 
step em algorithm respectively 
balanced data assignment subproblem max nk log nk nk nk 
integer programming problem np hard general 
fortunately integer programming problem special optimum corresponding real relaxation bradley linear programming problem 
best known exact algorithm solve linear programming problem improved interior point method complexity log nk anstreicher 
clustering algorithm scalable large database seek approximate solutions optimization problem equation obtained time better 
propose iterative greedy bipartitioning algorithm assigns data objects clusters iteration locally optimal fashion 
motivation heuristic solves balanced assignment problem equation exactly 
words just clusters simply sorts difference vector dv log log descending order assigns objects cluster second half cluster 
easy show gives bipartition maximizes objective equation 
greedy bipartition conducted iteration separates data objects cluster way objective equation locally maximized 
trivial show th iteration algorithm gives locally optimal bipartition assigns objects th cluster 
look time complexity algorithm 
length difference vector computed th iteration 
calculating difference vectors takes time sorting takes logn kn logn time 
total zhong ghosh time complexity kn logn greedy bipartitioning algorithm mn kmn logn resulting balanced clustering algorithm number clustering iterations 
greedy nature algorithm stems imposition arbitrary ordering clusters investigate effect different orderings 
experiments ordering done random experiment multiple experiments run variation results inspected 
results exhibit abnormally large variations suggest effect ordering small 
post processing refinement improve cluster quality approximate exact balanced solutions acceptable 
achieved letting results completely balanced mk means serve initialization regular mk means 
regular mk means relatively low complexity kmn extra overhead low 
experiments reported section reflect full refinement sense regular mk means refinement step run convergence 
alternatively partial refinement round ml re assignment expected give intermediate result completely balanced fully refined 
experimental results intermediate results shown bounded sides completely balanced fully refined results 
refinement step viewed second perspective results completely balanced clustering serve initialization regular mk means clustering 
point view completely balanced data assignment generates better initial clusters random initialization experimental results 
results real text data ng dataset described section types models multinomials 
model type compare balanced mk means regular mk means clustering terms balance objective value mutual information original labels different number clusters 
balance clustering defined normalized entropy cluster size distribution clustering logk log number data objects cluster value means perfectly balanced clustering extremely unbalanced clustering 
average log likelihood clustering logp multinomial models von mises fisher models argmax dimensionality document vectors 
experimental settings section 
show results ng dataset results multinomial models left column vmf models right 
row shows balance results normalized entropy second row average log likelihood values row normalized mutual information nmi values 
results shown average standard deviation runs 
cases completely balanced clustering algorithms produce worse clusterings terms nmi measures perfect balancing strict constraint 
balanced clustering algorithms refinement perform comparably significantly better regular mk means terms nmi results provide significantly balanced unified framework model clustering number clusters balance normalized entropy regular multinomials bk multinomials refined bk multinomials number clusters balance normalized entropy regular bk refined bk number clusters average log likelihood regular multinomials bk multinomials refined bk multinomials number clusters average cosine similarity regular bk refined bk number clusters normalized mutual information regular multinomials bk multinomials refined bk multinomials number clusters normalized mutual information regular bk refined bk results ng dataset balance results multinomial models vmf models log likelihood results multinomial models vmf models mutual information results multinomial models vmf models 
zhong ghosh clusterings regular mk means 
comparing multinomial models vmf ones see vmf algorithms produce balanced clusterings datasets 
note balanced variation model clustering generic applied unified framework 
simply plug different models different applications 
conciseness shown results dataset 
experimental results earlier zhong ghosh 

hybrid model clustering section presents hybrid methodology combines advantages partitional hierarchical methods 
idea reverse scatter gather approach cutting vaithyanathan dom karypis 

shall analyze advantages hybrid approach model clustering new variation called hierarchical meta clustering section 
case studies section show benefits hybrid model clustering algorithms 
key observation hybrid methodology built top generic partitional model section inherits generality 
shows generic model hybrid algorithm 
cluster data greater natural number clusters may unknown groups cluster data fine granularity partitional method discussed section 
model clustering means models 
step viewed compressing coarsening data 
second step run hac algorithm starting clusters iteratively merge clusters closest data objects cluster process stopped user 
hybrid approach returns series nested clusterings interactively analyzed user evaluated optimization criterion 
note methodology necessarily limited model clustering karypis intend show benefits model approaches 
hybrid algorithm practical substitute hierarchical agglomerative clustering algorithm keeps hierarchical structure visualization benefit reduces computational complexity nm 
reasonably set constant times obtain complexity model partitional clustering assuming 
improve partitional clustering algorithms starting clusters iteratively merge back clusters 
method proven effective graph partitioning techniques generate high quality partitions karypis 
intuitive explanation second merging step fine tunes improves initial flat clusters 
experimental results section show effectiveness hybrid clustering approach time series clustering hidden markov models 
section introduce particular variation hybrid algorithm hierarchical meta clustering algorithm 

number distance comparisons complexity estimating models nm analysis section 
recall number iterations model training 
unified framework model clustering algorithm model hybrid partitional hierarchical clustering input data objects model structure hierarchy depth output level cluster model hierarchy hierarchical partition data objects clusters th level 
steps 
flat partitional clustering partition data objects clusters model partitional clustering algorithms discussed section 
distance calculation compute pairwise inter cluster distances measures defined equations identify closest cluster pair 
cluster merging merge closest clusters assume re estimate model merged data objects argmax logp 
data objects merged cluster user specified number clusters reached go back step 
model hybrid partitional hierarchical clustering algorithm 
hierarchical meta clustering introduce composite model cluster merged cluster define likelihood data object model max 
call children composite model set data objects merged cluster log log log 
furthermore define distance composite models 

min 
immediate benefits result design 
model parameter re estimation needed merging clusters composite model simply represented parameters children 
equation seen cluster merging change likelihood means ward distance equation case 
second composite model characterize complex clusters single model represents poorly 
example rotated shape cluster accurately modeled single gaussian approximated mixture gaussians 
single gaussian model loses shape structure cluster concatenating spherical gaussian clusters gives representation shown 
composite models defined equation inter cluster distances equation get hierarchical meta clustering algorithm equivalent treating initial zhong ghosh shape cluster 
single spherical gaussian model cluster 
union spherical gaussian models cluster 
circle shows gaussian model times standard deviation 
cluster meta object applying traditional single link hierarchical clustering algorithm group meta objects 
obviously prevents different hierarchical method complete link average link cluster meta objects suitably modifying measure definition equation 
hierarchical method desirable different applications 
hierarchical meta clustering algorithms seen combination model flat clustering discriminative hierarchical methods 
compared single complex model favor strategy merging simple models form complex clusters single complex model difficult define train 
example distribution shape cluster 
furthermore impossible avoid poor local solutions define complex distribution 
helpful produce approximately balanced clusters partitional step 
may immediately clear look shape cluster example 
suppose divide data clusters flat clustering step 
means algorithm may get unbalanced clustering contains big cluster near empty clusters shown balanced solution 
merging clusters back solution leads set data objects prefer solution provides useful hierarchy disclosing shape structure cluster 
results synthetic spatial data tested hybrid algorithm synthetic difficult datasets dataset contains artificially generated data points dataset included cluto toolkit karypis contains data points 
ground truth labels datasets natural clusters dataset dataset plus noise outliers human judgment 
obvious natural clusters modeled single gaussian 
fact difficult propose model fits arbitrary shaped clusters 
unified framework model clustering synthetic datasets dataset dataset 
sight probably turn graph partitioning approaches get clustering results 
traditional means hac algorithms fail miserably datasets spectral clustering algorithms kannan ng identify natural clusters hybrid graph partitioning approach karypis karypis produces natural clusters 
hybrid graph partitioning algorithm partitions data large number clusters merges refines back proper granularity level 
section demonstrate achieve intuitive clusters efficiently proposed hybrid model approaches 
models case equi variant spherical gaussians result improved hierarchical means algorithm 
step hybrid algorithm balanced version means algorithm zhong ghosh partition data fine granularity clusters dataset clusters dataset 
varying number clusters observed final hybrid clustering results relatively insensitive 
show balanced results 
seen clustering data fine granularity get pure clusters clusters mix objects different natural clusters 
balance constraint helps restrict cluster defined empty large 
resulting stable defined clusters form basis step hierarchical merging 
currently number clusters user selected automatic methods model selection investigated 
second step compute cluster pairwise distances measure equation parameter set apply single link hierarchical meta clustering construct meta cluster hierarchy 
observe final clustering results relatively insensitive change 
show meta cluster 
heuristically select constant times true number clusters estimated empirically known prior knowledge 
hierarchical meta clustering actual clusters represented multiple simple models roughly flat clusters 
regular hybrid clustering retrain models merging clusters flat clusters 
note numbers unavoidably heuristic reality number needs estimated see discussion section 

reused symbols colors cluster represented unique combination symbol color 
zhong ghosh clusters inter cluster distance clusters results datasets balanced clustering dataset clusters dataset clusters meta cluster hierarchy dataset dataset distances hybrid clustering results dataset clusters dataset clusters 
unified framework model clustering hierarchies datasets respectively 
hierarchies evident clusters 
slicing hierarchies appropriate granularity level see hierarchical meta clustering produces decent natural clusters 
results suggest hierarchical meta clustering useful building meta cluster hierarchy hierarchical clustering clusters finding right number clusters 
results synthetic eeg time series case study datasets synthetic hidden markov model generated datasets real eeg dataset 
worth noting simplistic approach converting time series fixed length vector example regular means time delay embedding problematic general leads correlated components problems dealing time warping alignment variable length sequences synthetic dataset syn contains clusters sequences length 
sequences generated continuous hmm models hmm hmm smyth 
models hidden states priors observation parameters 
priors uniform observation distribution univariate gaussian mean variance hidden state mean variance hidden state 
state transition parameters hmm hmm respectively 
remaining sequences composed uniformly distributed random numbers seen generated special hmm model state uniform observation distribution 
second synthetic dataset syn simply subset containing time points sequence syn 
eeg dataset eeg extracted uci kdd archive bay contains measurements electrode scalp 
measurements subjects control subject alcoholic subject 
measurement sampled hz second producing sequence length 
shows sequence objects 
goal group time series subject cluster 
model cluster univariate hmm 
eeg signals believed highly correlated sleep stages human brain cells 
number sleep stages geva 
number hidden states hmms manually chosen 
hidden states synthetic datasets eeg dataset 
heuristic choose number high expected range models possess representational power characterize data 
experiments classification accuracy evaluation criterion assuming number clusters known priori 
class label cluster defined popular class cluster 
accuracy measures percentage sequences correct class labels 
accuracy criterion chosen easy understand convenient compare classification accuracy described earlier zhong ghosh data set 
discussion compare hmm clustering algorithms datasets described 
partitional algorithms instantiated generic model partitional clustering zhong ghosh alcoholic subject control subject eeg data objects alcoholic subject control subject 
algorithms discussed section 
plugging hmm models get hmm means stochastic hmms mixture hmms respectively 
instantiated hybrid algorithms hier hybrid clustering algorithm algorithm step kl distance second step hier hmms hybrid clustering algorithm hmms algorithm flat clustering step kl distance second step 
clustering accuracy results shown table 
flat clusters partitional step run algorithm times different random initializations report averages standard deviations 
boldface font indicates best performance 
results form average standard deviation 
partitional methods perform comparably consistently better datasets 
hybrid approaches clearly outperform partitional ones datasets 
hier hmms better hier short sequences syn 
sophisticated models classify eeg time sequences zhong ghosh best classification accuracy fold cross validation 
average accuracy hier algorithm eeg dataset 
methods large standard deviations indicating random initialization big effect clustering results small data size 
reduce initialization effect hmm models remains interesting task 
despite high variances run tests measure significance results observed best hybrid result column significantly outperforms partitional results level 
final comment 
initialization method train global model sequences modify train random balanced partitions sequences get initial models 
unified framework model clustering datasets approaches syn syn eeg hmms stochastic hmms hier hmms hier table clustering accuracy results synthetic datasets eeg dataset power hybrid model clustering lies suitability models regular hybrid algorithm regular means followed hierarchical clustering time series clustering 

related majority model clustering methods maximum likelihood formulation mclachlan basford banfield raftery 
early focused different types normal distributions 
example banfield raftery discussed clustering mixture constrained gaussian models fraley described efficient hierarchical algorithms special cases gaussian models 
smyth applied mixture hmms cluster synthetic sequences 
cadez 
extended probabilistic framework model partitional clustering em algorithm 
advocate mixture generative models clustering irregular data non gaussian difficult impossible handle traditional vector space sequences different lengths 
mentioned section basically em clustering emphasis applications non vector data 
partly motivates addresses model clustering algorithms including hierarchical methods 

step view iterative clustering process data assignment step supervised learning step 
compared em algorithm supervised learning step corresponds step general need maximum likelihood method 
potential difficulty step process arbitrary supervised learning method may converge 
model partitional clustering convergence guaranteed em algorithm provided maximum number iterations large 
kamvar 
interpretations classical hierarchical agglomerative clustering algorithms model standpoint 
intended fit hac algorithms standard model hierarchical clustering framework discovered corresponding model agglomerative algorithms ward single link complete link average link 
ward algorithm fits natural model interpretation match contrived model approximate 
viewpoint ward algorithm model approach assumes spherical gaussian models cluster result cluster represented mean 
discriminative algorithms data pairwise similarities distances 
explains difficulty determining suitable underlying generative models 
zhong ghosh deterministic annealing successfully wide range applications rose 
applications clustering largely restricted vector data rose hofmann buhmann 
hofmann buhmann provided unified treatment som neural gas deterministic annealing algorithms vector quantization applications gersho gray 
showed types algorithms different implementations continuation method georg vector quantization different competitive learning rules 
analyzed probabilistic model clustering demonstrated relationship model means em clustering annealing perspective 
hybrid algorithms cutting 
famous scatter gather approach reducing computational complexity 
applied hac algorithm small sampled dataset fed resulting clusters initial cluster seeds means algorithm ran means entire dataset 
similar idea explored meila heckerman hac supply initial cluster centers subsequent em clustering step 
vaithyanathan dom step hierarchical method clustering documents apply flat partitional clustering algorithm feature selection model selection methods build cluster hierarchy 
method basically generic algorithm instantiated multinomial models ward inter cluster distance 
related classic isodata algorithm hall ball performs refinement splitting merging clusters obtained standard means algorithm 
clusters merged number members cluster certain threshold centers clusters closer certain threshold 
cluster split standard deviation exceeds predefined value 
method needs user specified thresholds assumes spherical clusters 
multi level graph partitioning algorithms karypis karypis similaritybased clustering approaches hybrid flavor 
example cluto toolkit karypis allows user specify large number initial clusters merged final desired number clusters 
similarity method falls short terms theoretical complexity interpretability compared hybrid model clustering method described 

concluding remarks unified framework model clustering provides richer understanding existing model clustering algorithms 
framework applicable application domain probabilistic models exist 
related model partitional clustering algorithms including soft hard means type methods analyzed detail relationships explained deterministic annealing point view 
comparative study document clustering conducted show usefulness view 
designed inter cluster distances leading useful variations model hierarchical clustering algorithms 
clear distinction model similarity hierarchical algorithms helps gain better understanding existing hierarchical algorithms 
proposed new variations model clustering unified framework balanced clustering hybrid clustering improve clustering results certain applications 
unified framework model clustering effectiveness model clustering algorithms highlighted experimental comparisons synthetic real datasets 
question addressed choose final number clusters partitional hierarchical hybrid procedures 
old important problem universally satisfactory answer obtained 
bayesian model selection techniques schwarz banfield raftery fraley raftery investigated extensively 
simple criteria bic bayesian information criterion aic akaike information criterion overestimate underestimate number clusters severely limits practical usability 
monte carlo estimation posterior likelihood smyth accurate computationally expensive 
cross validation methods effective data criteria hold likelihood meila heckerman smyth evaluated hold validation dataset prove helpful 
inappropriate model selection methods find number clusters models description clusters 
example natural clusters synthetic datasets modeled gaussians 
attempts estimate number gaussians mixture gaussians clustering data lead high value hierarchical clustering alleviates need select user interact hierarchical set clusterings choose best 
needless say clustering problems human give best domain specific judgment 
important related issue choose suitable model family 
highly expressive models difficult train poor local solutions hard interpret 
encourages researchers start simple models combine hierarchical merging methods characterize complex clusters 
model partitional clustering algorithms online promising feature stream data mining applications 
competitive learning method widely neural network literature provides way constructing online means algorithms 
employed banerjee ghosh law kwok sinkkonen kaski online model clustering text documents sequences gene expressions respectively 
type algorithm certainly deserves investigation 
promising direction examine possibility combining model clustering methods discriminative ones 
preliminary direction 
example researchers started constructing similarity measures generative models amari jaakkola haussler tipping tsuda impact clustering performance fully understood 
possible approach combine bipartite graph partitioning model training 
partitional clustering hard assignments data objects models corresponds partitioning bipartite graph constraint partition contains exactly model vertex 
fact hard data assignment mk means algorithm equivalent constrained minimum cut bipartite graph partitions 
model hierarchical agglomerative clustering algorithms merging clusters corresponds partitioning graph clusters partition contains exactly model vertices partitions model vertex 
connections may lead way combining model method graph partitioning algorithms deserve investigation 
research may reveal new robust clustering algorithms insightful connections generative discriminative approaches 
zhong ghosh acknowledgments anonymous reviewers claire cardie helpful comments significantly improved technical quality 
research supported part ibm faculty partnership award ibm tivoli ibm nsf iis 
aha kibler albert 
instance learning algorithms 
machine learning 
georg 
numerical continuation methods 
springer verlag berlin heidelberg 
amari 
information geometry em em algorithms neural networks 
neural networks 
anstreicher 
linear programming logn operations 
siam journal optimization 
banerjee dhillon sra ghosh 
generative model clustering directional data 
proc 
th acm sigkdd int 
conf 
knowledge discovery data mining pages 
banerjee ghosh 
frequency sensitive competitive learning clustering highdimensional hyperspheres 
proc 
ieee int 
joint conf 
neural networks pages may 
banerjee ghosh 
scaling balanced clustering algorithms 
proc 
nd siam int 
conf 
data mining pages april 
banerjee dhillon ghosh 
clustering bregman divergences 
technical report tr department computer science university texas austin 
banfield raftery 
model gaussian non gaussian clustering 
biometrics september 
bishop 
neural networks pattern recognition 
oxford university press 

gentle tutorial em algorithm application parameter estimation gaussian mixture hidden markov models 
technical report university california berkeley april 
bradley bennett demiriz 
constrained means clustering 
technical report msr tr microsoft research redmond wa 
cadez smyth 
general probabilistic framework clustering individuals objects 
proc 
th acm sigkdd int 
conf 
knowledge discovery data mining pages 
unified framework model clustering cutting karger pedersen tukey 
scatter gather cluster approach browsing large document collections 
proc 
acm sigir pages 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
kokkinakis 
algorithm clustering continuous density hmm recognition error 
ieee trans 
speech audio processing may 
dhillon 
clustering documents words bipartite spectral graph partitioning 
proc 
th acm sigkdd int 
conf 
knowledge discovery data mining pages 
dhillon modha 
concept decompositions large sparse text data clustering 
machine learning 
dom 
information theoretic external cluster validity measure 
technical report rj ibm 
fowlkes mallows 
method comparing hierarchical clusterings 
journal american statistical association 
fraley 
algorithms model gaussian hierarchical clustering 
siam journal scientific computing 
fraley raftery 
clusters 
clustering method 
answers modelbased analysis 
computer journal 

gauvain 
lee 
maximum posteriori estimation multivariate gaussian mixture observations markov chains 
ieee trans 
speech audio processing april 
geman geman 
stochastic relaxation gibbs distribution bayesian restoration images 
ieee trans 
pattern anal 
machine intell november 
gersho gray 
vector quantization signal processing 
kluwer academic publisher boston ma 
geva 
brain state identification forecasting acute pathology unsupervised fuzzy clustering eeg temporal patterns 

kandel jain editors fuzzy neuro fuzzy systems medicine chapter pages 
crc press 
ghosh 
scalable clustering 
ye editor handbook data mining pages 
lawrence erlbaum assoc 
hall ball 
clustering technique summarizing multivariate data 
behavioral science 
hartigan 
clustering algorithms 
john wiley sons 
zhong ghosh heskes 
self organizing maps vector quantization mixture modeling 
ieee trans 
neural networks november 
bay 
uci kdd archive kdd ics uci edu 
irvine ca university california department information computer science 
hofmann buhmann 
pairwise data clustering deterministic annealing 
ieee trans 
pattern anal 
machine intell 
hofmann buhmann 
competitive learning algorithms robust vector quantization 
ieee trans 
signal processing june 
indyk 
sublinear time approximation scheme clustering metric spaces 
th annual ieee symp 
foundations computer science pages 
jaakkola haussler 
exploiting generative models discriminative classifiers 
kearns solla cohn editors advances neural information processing systems volume pages 
mit press 
jain dubes 
algorithms clustering data 
prentice hall new jersey 
jain murty flynn 
data clustering review 
acm computing surveys 

juang levinson sondhi 
maximum likelihood estimation multivariate mixture observations markov chains 
ieee trans 
information theory 

juang rabiner 
probabilistic distance measure hidden markov models 
technical journal 
kalpakis 
distance measures effective clustering arima time series 
proc 
ieee int 
conf 
data mining pages 
langley wagstaff yoo 
generalized clustering supervised learning data assignment 
proc 
th acm sigkdd int 
conf 
knowledge discovery data mining pages 
kamvar klein manning 
interpreting extending classical agglomerative clustering algorithms model approach 
proc 
th int 
conf 
machine learning pages 
kannan vempala 
clusterings bad spectral 
st annual ieee symp 
foundations computer science pages 
karypis 
cluto clustering toolkit 
dept computer science university minnesota may 
karypis 
han kumar 
chameleon hierarchical clustering dynamic modeling 
computer 
unified framework model clustering kearns mansour ng 
information theoretic analysis hard soft assignment methods clustering 
proc 
th conf 
uncertainty artificial intelligence pages 
kohonen 
self organizing map 
springer verlag new york 
law kwok 
rival penalized competitive learning model sequence clustering 
proc 
ieee int 
conf 
pattern recognition pages 
li biswas 
applying hidden markov model methodology unsupervised learning temporal data 
international journal knowledge intelligent engineering systems july 
lin 
divergence measures shannon entropy 
ieee trans 
information theory 
lloyd 
squares quantization pcm 
ieee trans 
information theory march 
macqueen 
methods classification analysis multivariate observations 
proc 
th berkeley symp 
math 
statistics probability pages 
mardia 
statistics directional data 
royal statistical society 
series methodological 
martinetz schulten 
neural gas network vector quantization application time series prediction 
ieee trans 
neural networks july 
mccallum nigam 
comparison event models naive bayes text classification 
aaai workshop learning text categorization pages 
mclachlan basford 
mixture models inference applications clustering 
marcel dekker new york 
meila heckerman 
experimental comparison model clustering methods 
machine learning 
meila shi 
learning segmentation random walks 
leen dietterich tresp editors advances neural information processing systems pages 
mit press 
mitchell 
machine learning 
mcgraw hill 
neal hinton 
view em algorithm justifies incremental sparse variants 
jordan editor learning graphical models pages 
kluwer academic publishers 
ng jordan weiss 
spectral clustering analysis algorithm 
dietterich becker ghahramani editors advances neural information processing systems volume pages 
mit press 
zhong ghosh oates firoiu cohen 
clustering time series hidden markov models dynamic time warping 
ijcai workshop neural symbolic reinforcement methods sequence learning stockholm sweden 
qian lin yu 
relationships local clustering time shifted inverted gene expression profiles identifies new biologically relevant interactions 
journal molecular biology 
ramoni sebastiani cohen 
bayesian clustering dynamics 
machine learning 
rand 
objective criteria evaluation clustering methods 
journal american statistical association 
rose 
deterministic annealing clustering compression classification regression related optimization problems 
proceedings ieee 
rose gurewitz fox 
constrained clustering optimization method 
ieee trans 
pattern anal 
machine intell august 
scholkopf smola 
learning kernels 
mit press 
schwarz 
estimating dimension model 
annals statistics 
seo shneiderman 
interactively exploring hierarchical clustering results 
computer 
sinkkonen kaski 
clustering conditional distributions auxiliary space 
neural computation 
smyth 
clustering sequences hidden markov models 
mozer jordan petsche editors advances neural information processing systems volume pages 
mit press 
steinbach karypis kumar 
comparison document clustering techniques 
kdd workshop text mining boston ma august 
strehl ghosh 
cluster ensembles knowledge reuse framework combining partitions 
journal machine learning research 
strehl ghosh 
relationship clustering visualization high dimensional data mining 
informs journal computing special issue web mining 
strehl ghosh mooney 
impact similarity measures web page clustering 
aaai workshop ai web search pages july 

clustering criteria multivariate normal mixtures 
biometrics 
tipping 
deriving cluster analytic distance functions gaussian mixture models 
proc 
th ieee int 
conf 
artificial neural networks volume pages 
unified framework model clustering tsuda 
uller 
clustering fisher score 
becker thrun obermayer editors advances neural information processing systems volume 
mit press 
tung ng lakshmanan han 
constraint clustering large databases 
proc 
th int 
conf 
database theory pages 
vaithyanathan dom 
model hierarchical clustering 
proc 
th conf 
uncertainty artificial intelligence pages july 
vapnik 
statistical learning theory 
john wiley new york 
ward 
hierarchical groupings optimize objective function 
journal american statistical association 
wong 
clustering data melting 
neural computation 
yeung fraley raftery ruzzo 
model clustering data transformations gene expression data 
bioinformatics july 
zhao karypis 
criterion functions document clustering experiments analysis 
technical report department computer science university minnesota november 
zhong ghosh 
hmms coupled hmms multi channel eeg classification 
proc 
ieee int 
joint conf 
neural networks pages may 
zhong ghosh 
comparative study generative models document clustering 
siam int 
conf 
data mining workshop clustering high dimensional data applications san francisco ca may 
zhong ghosh 
scalable balanced model clustering 
proc 
rd siam int 
conf 
data mining pages san francisco ca may 

