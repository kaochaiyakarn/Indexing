sample extensions lle isomap mds eigenmaps spectral clustering bengio jean fran ois pascal vincent olivier nicolas le roux marie partement informatique recherche op universit de montr montr qu bec canada iro umontreal ca unsupervised learning algorithms eigendecomposition provide embedding clustering training points straightforward extension sample examples short recomputing eigenvectors 
provides unified framework extending local linear embedding lle isomap laplacian eigenmaps multi dimensional scaling dimensionality reduction spectral clustering 
framework seeing algorithms learning eigenfunctions data dependent kernel 
numerical experiments show generalizations performed level error comparable variability embedding algorithms due choice training data 
unsupervised learning algorithms proposed eigendecomposition obtaining lower dimensional embedding data lying non linear manifold local linear embedding lle roweis saul isomap tenenbaum de silva langford laplacian eigenmaps belkin niyogi :10.1.1.111.3313
variants spectral clustering weiss ng jordan weiss embedding intermediate step obtaining clustering data capture flat elongated curved clusters :10.1.1.43.7945
tasks manifold learning clustering linked clusters spectral clustering arbitrary curved manifolds long data locally capture curvature 
common framework consider types unsupervised learning algorithms cast framework computation embedding training points obtained principal eigenvectors symmetric matrix 
algorithm 
start data set 
xn points construct neighborhood similarity matrix denote kd shorthand data dependent function produces mij kd xi xj 

optionally transform yielding normalized matrix equivalently corresponds generating kd mij kd xi xj 

compute largest positive eigenvalues eigenvectors vk 
embedding example xi vector yi th element th principal eigenvector vk alternatively mds isomap embedding ei eik 
eigenvalues positive ei ej best approximation mij coordinates squared error sense 
consider specializations algorithm different unsupervised learning algorithms 
si th row sum affinity matrix si mij 
say points nearest neighbors nearest neighbors vice versa 
denote xij th coordinate vector xi 
multi dimensional scaling multi dimensional scaling mds starts notion distance affinity computed pair training examples 
consider metric mds cox cox 
normalization step algorithm distances converted equivalent dot products double centering formula mij mij si sj embedding eik example xi 
spectral clustering sk 
spectral clustering weiss yield impressively results traditional clustering looking round blobs data means fail miserably :10.1.1.43.7945:10.1.1.43.7945
main steps embedding data points space clusters obvious eigenvectors gram matrix applying classical clustering algorithm means ng jordan weiss 
affinity matrix formed kernel gaussian kernel 
normalization steps proposed 
successful ones advocated weiss ng jordan weiss mij mij :10.1.1.43.7945
obtain clusters principal eigenvectors computed means applied unit norm coordinates obtained embedding 
laplacian eigenmaps laplacian eigenmaps proposed dimensionality reduction procedure belkin niyogi proposed semi supervised learning 
authors approximation laplacian operator gaussian kernel matrix element xi xj nearest neighbors 
solving ordinary eigenproblem generalized eigenproblem solved vj eigenvalues eigenvectors vj diagonal matrix entries eq 

smallest eigenvalue left eigenvectors corresponding small eigenvalues embedding 
embedding computed spectral clustering algorithm shi malik 
noted weiss normalization lemma equivalent result componentwise scaling embedding obtained considering principal eigenvectors normalized matrix defined eq :10.1.1.43.7945

isomap isomap tenenbaum de silva langford generalizes mds non linear manifolds 
replacing euclidean distance approximation geodesic distance manifold 
define geodesic distance respect data set distance neighborhood follows min pi pi sequence points length pl pi 
pi pi nearest neighbors 
length free minimization 
isomap algorithm obtains normalized matrix embedding derived transforming raw pairwise distances matrix follows compute matrix mij xi xj squared geodesic distances respect data apply matrix distance dot product transformation eq 
mds 
mds embedding eik 
lle local linear embedding lle algorithm roweis saul looks embedding preserves local geometry neighborhood data point :10.1.1.111.3313
sparse matrix local predictive weights wij computed wij wij xj nearest neighbor xi xi minimized 
matrix formed 
embedding obtained lowest eigenvectors smallest eigenvector uninteresting 
eigenvalue 
note lowest eigenvectors largest eigenvectors fit algorithm discussed section 
embedding constant respect 
eigenvectors eigenfunctions obtain embedding new data point propose nystr formula eq 
baker successfully speed kernel methods computations focussing heavier computations eigendecomposition subset examples 
formula justified considering convergence eigenvectors eigenvalues number examples increases baker williams seeger gin shawe taylor williams 
intuitively extensions obtain embedding new example require specifying new column gram matrix training set dependent kernel function kd arguments may required training set 
start data set obtain embedding elements add data embedding points converges eigenvalues unique 
shawe taylor williams give bounds convergence error case kernel pca 
limit expect eigenvector converge eigenfunction linear operator defined sense th element th eigenvector converges application th eigenfunction xi normalization factor 
consider hilbert space hp functions inner product dx density function 
associate kernel linear operator kp hp dy 
don know true density approximate inner product linear operator eigenfunctions empirical distribution empirical hilbert space defined note proposition applied kernel positive semi definite embedding algorithms studied restricted principal coordinates associated positive eigenvalues 
rigorous mathematical analysis see bengio 
proposition kernel function necessarily positive semi definite gives rise symmetric matrix entries mij xi xj dataset 
xn 
vk eigenvector eigenvalue pair solves kvk 
fk eigenfunction eigenvalue pair solves fk empirical distribution ek yk yk denote embedding associated new point fk xi fk xi yk fk xi yk xi ek xi eik see bengio proof justifications formulae 
generalized embedding isomap mds ek kyk spectral clustering laplacian eigenmaps lle yk 
proposition addition data dependent kernel kd positive semi definite fk th component kernel pca projection obtained kernel kd centering 
relation kernel pca sch lkopf smola ller pointed williams seeger discussed bengio 
extending new points proposition obtains natural extension unsupervised learning algorithms mapped algorithm provided write kernel function gives rise matrix eq 
generalize embedding 
consider turn 
addition convergence properties discussed section justification equation proposition proposition define fk xi eq 
take new point value fk minimizes xi tft ft xi eq 
proof direct consequence orthogonality eigenvectors vk 
proposition links equations 
obtain eq 
trying approximate data points minimizing cost xi xj tft xi ft xj 
add new point natural cost approximate xi yields 
note doing seek approximate 
investigate embeddings minimize empirical reconstruction error ignore diagonal contributions 
extending mds mds normalized kernel defined follows continuous version double centering eq 
ex ex ex original distance expectations taken empirical data extension metric mds new points proposed gower solving exactly embedding consistent distances training points general requires adding new dimension 
extending spectral clustering laplacian eigenmaps version spectral clustering laplacian eigenmaps described initial kernel gaussian nearest neighbor kernel 
equivalent normalized kernel ex ex expectations taken empirical data extending isomap extend isomap test point computing geodesic distance training points recompute geodesic distances 
reasonable solution definition eq 
uses training points intermediate points path obtain normalized kernel applying continuous double centering eq 
formula proposed de silva tenenbaum approximate isomap subset examples landmark points compute eigenvectors 
notations formula ex xi xi 
ex average data set 
formula applied obtain embedding non landmark examples 
corollary embedding proposed proposition isomap ek equal formula landmark isomap defined eq 
proof proof relies property gram matrix isomap mij construction 

eigenvector eigenvalue eigenvectors vk property orthogonality 

writing ex xi xi xi ex ex yields ek sum 
xi ex ex extending lle extension lle challenging fit framework algorithm matrix lle clear interpretation terms distance dot product 
extension proposed saul roweis unfortunately cast directly framework proposition 
embedding new point yk yk xi xi xi weight xi reconstruction nearest neighbors training set xj xi ij 
close eq 
lacks normalization see embedding limit case proposition shown 
need define kernel xi xj ij ij wij wji xi xj define kernel xi xi xi training set defined xi xj wij wji isn construction kernel verifies eq 

apply eq 
obtain embedding new point yields xi xi th lowest eigenvalue rewrites xi xi 
yk defined eq 

choice free consider eq 
approximating kernel large proposition 
done experiments described section 
note find smoother kernels verifying eq 
giving extensions lle proposition 
scope study kernel best generalization desirable smooth kernel take account reconstruction neighbors xi reconstruction xi neighbors including new point experiments want evaluate precision generalizations suggested previous section comparable intrinsic perturbations embedding algorithms 
perturbation analysis achieved considering splits data sets training comparing embeddings algorithm described section apply procedure 

training set variability minus sample error wrt proportion training samples substituted 
top left mds 
top right spectral clustering laplacian eigenmaps 
bottom left isomap 
bottom right lle 
error bars confidence intervals 

choose samples 
remaining samples split equal size subsets 
train obtain eigenvectors 
eigenvalues close estimated eigenvectors unstable rotate subspace span 
estimate affine alignment embeddings points calculate euclidean distance aligned embeddings obtained si 
sample si train si 
apply extension sample points find predicted embedding si calculate euclidean distance embedding obtained training si training set 

calculate mean difference standard error shown distance obtained step obtained step sample si repeat experiment various sizes results obtained mds isomap spectral clustering lle shown different values experiments done database synthetic face images described components available athttp isomap stanford edu 
qualitatively similar results obtained databases ionosphere www ics uci edu mlearn html www cs toronto edu roweis lle 
algorithm generates twodimensional embedding images experiments reported isomap 
number neighbors isomap lle gaussian kernel standard deviation spectral clustering laplacian eigenmaps 
confidence intervals drawn mean difference error 
expected mean difference distances monotonically increasing fraction substituted examples grows axis 
cases sample error comparable training set embedding stability corresponds substituting fraction training examples 
extension unsupervised learning algorithms spectral embedding data mds spectral clustering laplacian eigenmaps isomap lle 
extension allows apply trained model points having recompute eigenvectors 
introduces notion function induction generalization error algorithms 
experiments real highdimensional data show average distance sample sample embeddings comparable lower variation sample embedding due replacing points training set 
baker 

numerical treatment integral equations 
clarendon press oxford 
belkin niyogi 

laplacian eigenmaps dimensionality reduction data representation 
neural computation 
bengio vincent le roux 

spectral clustering kernel pca learning eigenfunctions 
technical report partement informatique recherche op universit de montr cox cox 

multidimensional scaling 
chapman hall london 
de silva tenenbaum 

global versus local methods nonlinear dimensionality reduction 
becker thrun obermayer editors advances neural information processing systems pages cambridge ma 
mit press 
gower 

adding point vector diagrams multivariate analysis 
biometrika 
gin 

random matrix approximation spectra integral operators 
bernoulli 
ng jordan weiss 

spectral clustering analysis algorithm 
dietterich becker ghahramani editors advances neural information processing systems cambridge ma 
mit press 
roweis saul 

nonlinear dimensionality reduction locally linear embedding 
science 
saul roweis 

think globally fit locally unsupervised learning low dimensional manifolds 
journal machine learning research 
sch lkopf smola ller 

nonlinear component analysis kernel eigenvalue problem 
neural computation 
shawe taylor williams 

stability kernel principal components analysis relation process 
becker thrun obermayer editors advances neural information processing systems 
mit press 
shi malik 

normalized cuts image segmentation 
proc 
ieee conf 
computer vision pattern recognition pages 
tenenbaum de silva langford 

global geometric framework nonlinear dimensionality reduction 
science 
weiss 

segmentation eigenvectors unifying view 
proceedings ieee international conference computer vision pages 
williams seeger 

effect input density distribution kernel classifiers 
proceedings seventeenth international conference machine learning 
morgan kaufmann 
