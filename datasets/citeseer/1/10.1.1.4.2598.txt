algorithm selection sorting probabilistic inference machine learning approach guo beijing university aeronautics astronautics dissertation submitted partial fulfillment requirements degree doctor philosophy department computing information sciences college engineering kansas state university manhattan kansas copyright algorithm selection sorting probabilistic inference machine learning approach guo algorithm selection problem aims selecting best algorithm computational problem instance characteristics instance 
dissertation introduce results theoretical investigation algorithm selection problem 
show rice theorem nonexistence automatic algorithm selection program description input instance competing algorithms 
describe theoretical framework instance hardness algorithm performance kolmogorov complexity show algorithm selection search 
driven theoretical results propose machine learning inductive approach experimental algorithmic methods machine learning techniques solve algorithm selection problem 
experimentally applied proposed methodology algorithm selection sorting mpe problem 
sorting instances existing order easier algorithms 
studied different presortedness measures de signed algorithms generate permutations specified existing order uniformly random applied various learning algorithms induce sorting algorithm se lection models runtime experimental results 
mpe problem instance characteristics studied include size topological type network net connectedness skewness distributions conditional probability tables cpts proportion distribution evidence variables 
mpe algo rithms considered include exact algorithm clique tree propagation stochastic sampling algorithms mcmc gibbs sampling importance forward sampling search algorithms multi restart hill climbing tabu search hybrid algorithm combining sampling search ant colony optimization 
major contribution dissertation discovery multifractal properties joint probability distributions bayesian networks 
sufficient asymmetry individual prior conditional probability distributions joint distribution highly skewed clusters high probability instantiations scales 
phase hybrid random sampling search algorithm solve mpe problem exploiting clustering property 
mpe problem decision version np complete multifractal meta heuristic applied solve np hard combinatorial optimization problems 
preface record journey dissertation research started half years ago joined dr hsu kdd group state 
remember handed charniak bayesian networks tears cha 
interests uncertain reasoning bayesian networks soon read implemented bayesian network learning algorithm 
time came choose dissertation topic knew research going area artificial intelligence bayesian networks 
group department doing real time research thought real time artificial intelligence topic 
consulting dr hsu committee members set topic real time ai 
time committee members managed open seminar class topics real time artificial intelligence spring semester 
class requested related chosen distributed anytime architecture probabilistic reasoning santos ssw 
presenting interested study real time bayesian network inference 
started reviewing various bayesian network inference algorithms realized real time constraints selection proper algorithms crucial different algorithms performances vary differently properties input instance change 
noticed real world problem solving main reason experts domain called experts selecting best problem solving technique quickly 
focused attention algorithm selection problem bayesian network inference 
formed goal building algorithm selection system act algorithm selection expert selecting best algorithm particular input instance gain best performance 
formed idea bayesian network sitting meta level algorithm selection reasoner job bayesian networks graphical models learning reasoning probabilistic expert systems 
components intelligent system representation learning inference 
dr hsu bringing ijcai uai seattle got chance directly talk class researchers field 
uai particularly impressed encouraged eric horvitz applying bayesian approach tackling hard computational problems 
idea basically came 
bayesian network meta reasoner monitor control processes hard problem solving process 
forgot exactly question asked eric talk remember confident research direction 
started literature survey related fields soon came back seattle 
noticed input instances created equal 
stances occur real world applications frequently exists theoretical world 
correspondingly called real world instances treated specially 
time started reading papers free lunch basically state structural assumptions search optimization problem algorithm perform better average blind search algorithms 
order solve problem better need adapted algorithm algorithm able take consideration structural specificities problem 
motivated study build algorithm selection meta reasoner assign best algorithm input instance examining reasoning instance structural property 
meta reasoner gain knowledge 
usual ways analytically deductive experimentally inductive 
tions theoretical direction led fields computational complexity computability kolmogorov complexity ga hardness 
formed basis theoretical aspects dissertation research 
discussions dr howell clarified misconceptions 
pure analytical approach algorithm selection proven dead realized rice theorem impossible automatic checker program turing machine read algorithms decide better problem undecidable general 
easy understand turing showed tur decide algorithm halts halting problem 
turned inductive direction doubt 
uai edmonton managed chair workshop real time decision support diagnosis systems dr hsu horvitz santos 
workshop chance talk fabio cozman research 
talk michael saskatchewan free lunch theorems stuff 
enhanced confidence experimental approaches 
fabio graduate student jaime open source bayesian network generator proved helpful experiments bayesian network inference 
thesis proposal aaai doctoral consortium 
leslie kaelbling mit assigned tutor 
talk asked bayesian network 
decision tree just agreed 
focus thesis research switched little original bayesian approach machine learning approach models decision tree naive bayes studied compared bayesian network 
met james park ucla edmonton 
mpe map inspired lot 
allowed share source code able executable 
multifractal analysis part research mainly inspired marek druzdzel skewness joint probability space bayesian networks 
verify results generated joint probability plotted excel 
looking shape joint distribution reminded concept fractal 
led discovery multifractal property joint space usefulness meta heuristic design algorithm finding probable explanation 
druzdzel led investigation cpt skew ness mpe instance feature help select best algorithm 
turned skewness important features differentiating algorithm performance space 
coming back edmonton clearer picture mind 
busy time implementing algorithms running experiments collecting analyzing data writing thesis 
draft finished early may 
central goal build algorithm selection meta reasoner real time bayesian network inference 
effort culminated development machine learning inductive methodology build general meta level intelligent algorithm selection systems 
problems remain open 
example types meta level reasoners 
need meta meta level reasoner reason selection meta level reasoner 
efforts put meta level reasoning 
thinking problem 
grown appreciate true complexity challenge building artificial intelligent system task 
find exactly challenge stimulates continue research 
finished recording long story ph research 
hope someday somebody go process doing find worth reading 
major professor dr william hsu continual source inspiration encouragement 
introducing field guiding ph study 
appreciate research freedom enjoyed supervision 
possible intelligent financial support 
acknowledge insightful comments provided committee members dr mitchell dr singh dr shing chang dr kenneth 
dr rodney howell helpful discussions took independent study advanced topics computational complexity 
dr marc berlin dr rudolf riedi rice univ proofreading multifractal part dissertation 
want dr fan comments multifractal study dr sanjoy das help ant algorithm implementations 
am indebted leslie kaelbling mit valuable comments dissertation proposal aaai doctoral consortium workshop 
past years fortune op discuss ideas authors refer dissertation 
inspirations came reading papers 
tools developed authors great help experiments 
par ticular am indebted researchers dr eric horvitz microsoft research dr marek druzdzel university pittsburgh dr heikki mannila helsinki dr eugene santos jr 
university connecticut dr fabio cozman brazil dr bart selman cornell james park ucla dr ole jesse uiuc jaime ide brazil great weka development team university waikato new zealand 
want friends developing team members system administrators kdd group wonderful experimental environment insightful comments provided cecil schmidt laura kruse benjamin perry jessica perry julie thornton eric james plummer jason 
foremost want parents believing encouraging take chances pursue dream 
contents list figures vii list tables motivations 
analytical versus experimental approaches 
problem instance characteristics algorithm performance machine learning models algorithm selection 
thesis statement 
organization 
background theoretical aspects 
computational complexity theory 
computability theory 
kolmogorov complexity algorithmic information theory experimental aspects 
experimental analysis algorithms 
algorithmic experiment setup 
bayesian networks 
representation 
inferences 
related works 
algorithm selection problem 
algorithm selection various applications 
meta reasoning techniques 
bayesian approach 
summary 
theoretical results analytical algorithm selection undecidability general automatic algorithm selection problem framework instance hardness algorithm perfor mance search 
black box optimization 
random instance random algorithm 
instance hardness algorithm performance 
kolmogorov complexity 
deceptive problems deceptively solving algorithms 
ga hardness revisited 
genetic algorithms gas 
free lunch theorems 
misconceptions ga hardness research 
alternative direction experimental approaches 
summary 
multifractal properties joint probability distribution bayesian networks motivation 
multifractal analysis 
fractals multifractals 
multifractal spectrum 
binomial multifractal cascade 
ii probabilistic roots multifractals 
thermodynamics formalism multifractals 
bayesian networks random multinomial multifractals 
case study asia alarm 
self similarity jpd asia 
skewed jpd alarm 
multifractal spectrum alarm 
quantifying clustering property 
multifractal search algorithm finding mpe 
summary 
machine learning techniques machine learning data mining 

example weather problem 
learning problem automatic algorithm selection machine learning algorithms 
decision tree learning 
naive bayes classifier 
bayesian network learning 
evaluation learning algorithms 
classification accuracy error rate 
stratified fold cross validation 
confusion matrix kappa statistic 
evaluating predicted probabilities rmse 
comparing classifiers paired test 
evaluation learning weather problem 
data preprocessing 
feature selection 
discretization numeric attributes 
iii meta learning combining multiple models 
bagging 
boosting 
stacking 
overview learning process algorithm selection 
algorithm selection sorting sorting 
sorting algorithms 
insertion sort 
shellsort 
heapsort 
mergesort 
quicksort 
instance characteristics measures presortedness 
inversions inv 
runs run 
longest ascending subsequence las rem 
random generation permutations degree presortedness markov chain approach random generation problems random generation problem inv 
random generation problem run 
random generation problem las rem 
experiment setups environment 
experimental results evaluation induction predictive algorithm selection models sorting 
training datasets 
experiment verifying sorting algorithm performance specific datasets 
iv experiment determining best feature sorting algo rithm selection 
experiment determining best model sorting algo rithm selection 
experiment evaluating sorting algorithm selection sys tem 
summary 
algorithm selection probable explanation problem mpe problem 
algorithms finding mpe 
exact algorithm clique tree propagation 
stochastic sampling algorithms 
search algorithms 
hybrid algorithm ant colony optimization mpe prob lem 
characteristics mpe problem instances 
network characteristics 
cpt characteristics 
evidence characteristics 
random generation mpe instances 
experiment setups environment 
experimental results evaluation induction algorithm se lection models finding mpe 
characteristics real world bayesian networks 
training datasets 
experiment determining exact mpe algorithm applicable 
experiment wrapper feature selection 
experiment determining best model approximate mpe algorithm selection 
experiment mpe algorithm performance specific datasets experiment evaluating mpe algorithm selection system summary 
contributions 
open questions 
theoretical aspects 
experimental aspects 
multifractal analysis study 
vi list figures instance reachability problem 
model algorithm selection problem 
cantor set 
generating cantor measure 
curve cantor measure 
binomial measure 
jpd simplest bn nodes 
steps multiplicative cascade asia network 
jpd asia network 
second half jpd illustrating self similarity property 
number instantiations order probability sum instantiations order 
curve alarm jpd 
clusters high probability instantiations clusters low probability instantiations 
average hamming distance graphs 
hybrid random sampling search algorithm finding mpe id algorithm decision tree learning 
decision tree weather problem 
naive bayes classifier weather problem 
algorithm 
vii bayesian network weather problem learned 
algorithm bagging 
algorithm boosting 
algorithm stacking 
overview machine learning approach algorithm se lection 
algorithm randomly generating permutation inv 
algorithm randomly generating permutation run 
algorithm randomly generating permutation las 
algorithm selection meta reasoner sorting 
number permutations vs presortedness measures size 
relationships inv run rem permutations size 
computational time measures running time sort algorithms dsort 
result dsort 
result dsort 
result dsort 
distribution size inv dsort 
distribution run rem dsort 
parameters attribute selection ga wrapper dsort 
result attribute selection ga wrapper dsort 
learned decision tree dsort 
time spent algorithm dsort 
working range algorithm selection system dsort 
time spent sort algorithm nearly sorted permutations time spent sort algorithm reversely ordered permutations time spent sort algorithm randomly ordered permutations viii candidate mpe algorithms 
gibbs sampling finding mpe 
forward sampling finding mpe 
random restart hill climbing finding mpe 
tabu search finding mpe 
ant mpe algorithm 
algorithm selection meta reasoner mpe problem 
learned decision tree exact mpe algorithm selection 
parameters output attribute selection ga wrapper dmp learned bn approximate mpe algorithm selection 
classification accuracies fold cross validation 
partitioning dmp number nodes 
partitioning dmp number samples 
partitioning dmp cpt skewness 
partitioning dmp evidence percentage 
partitioning dmp evidence distribution 
search history algorithms alarm network 
search history algorithms cpcs network 
search history algorithms cpcs network 
learned decision tree approximate mpe algorithm selection total mpe algorithms instances 
total mpe algorithms medium skewed instances 
total mpe algorithms highly skewed instances 
search history algorithms munin network 
ix list tables results randomly generated networks 
weather problem 
confidence limits student distribution degrees freedom 
evaluation result weather data 
discretized weather data 
bijection permutations 
permutations inversion tables 
transition matrix defined algorithm 
basic experiment setup instances vs algorithms 
statistics inv run rem values dsort 
training dataset dsort 
statistics inv run rem values dsort 
statistics inv run rem values dsort 
statistics inv run rem values dsort 
computation time classification accuracy inv run rem 
classification accuracy different learning schemes dsort reasoning time microseconds classification accuracy best models 
statistics test dataset dsort 
characteristics real world bayesian networks 
statistics attribute values dmp 
classification accuracy different learning schemes dmp format training dataset dmp 
statistics attribute values dmp 
classification accuracy different learning schemes dmp statistics attribute values dmp est 
total mpe returned algorithms algorithm selection system test instances 
mpe exactly computable bayesian networks 
mpe link munin 
xi chapter computational problem usually exist different algorithms solve exactly approximately 
different algorithms perform better dif ferent classes problem instances 
algorithm selection problem ric asks question algorithm select solve instance 
algorithm selection problem important theoretical practical reasons 
computer scientists seek better theoretical understanding problem instance hardness algorithm performance deliver better algo rithms computational task 
practice helps gain efficient computations solve problem hand 
especially crucial real time applications pressure hard soft computational deadlines 
dissertation mainly presents machine learning approach solve mentioned problem applies sorting finding probable explanation mpe probabilistic inference problem 
chapter give brief motivations directions 
describe thesis organization dissertation 
motivations analytical versus experimental approaches algorithm comparison algorithm selection central topics computational complexity theory studies amount resources needed solving compu tational problems 
field traditionally divided algorithm analysis problem complexity complexity classes 
algorithm analysis studies amount resources algorithm consumes 
problem complexity studies amount re sources needed solve computational problem 
theory complexity classes studies classification problems intrinsic computational com plexity 
amount resources needed algorithm problem usually measured functions length size input instances time complexity functions 
know practice instances size may require different amounts resources differences characteristics 
example ordered permutation sorted insertion sort algorithm linear time takes quadratic time insertion sort sort totally unordered permutation 
need study problem instance characteristics just size order select best algorithm 
classical computational complexity theory mainly relies analytical approaches worst case analysis average case analysis 
worst case analysis works provides basis algorithm selection 
cases fails 
consider quick sort example 
worst case analysis quick sort quadratic time complexity fast practice cases 
example simplex algorithm solving linear programming problem 
worst case time complexity exponential performs extremely real world applications 
worst case analysis treats instances size collectively may different terms features problem size 
average case analysis difficult apply requires reasonable estimate instances probabilities 
situations simply infeasible 
distinguish varying resource requirements instances size different features need move problem complexity instance complexity 
ways deal issue 
defines instance complexity decision problems kolmogorov complexity defined size shortest program solve deci sion problem 
second defines instance complexity restricting allowed algorithms 
example instance harder takes time specified algorithm solve 
third method considers instance classes single instances man 
defines subproblem original prob lem intuitive criteria instance easiness hardness studies worst case complexity subproblem 
algorithms designed analyzed subproblems resource requirements increasing smoothly moving larger subproblems 
resulting algorithm called optimal instance hardness measures 
method kolmogorov complexity produced results results mainly line complexity classes help practical algorithm selection 
second method depends particular algorithm 
third method lot progresses designing adaptive sorting algorithms optimal measures existing orders sorting instance 
analytical na ture method easy simple problems sorting hard applied arbitrary np hard optimization problems important ones practice 
furthermore third method provides way design adaptive algorithms optimal measures usually impossible design algorithm optimal measures 
algorithm selec tion problem different adaptive algorithms 
dissertation follows third method considering instance classes single instances rely experimental approaches analytical ones 
problem instance characteristics algorithm performance research partly motivated observation easy compute problem features indicators algorithm performance hard instances 
knowledge utilized help select best algorithm order gain efficient computations 
consider sorting example 
known permutation nearly sorted insertion sort algorithm sort linear time algorithm worst case time complexity computational complexity sorting log 
community np hard optimization problem solving researchers long noticed np complete result just worst case result instances equally hard 
algorithms exploit features input instances perform particular class instances better worst case scenario 
light main directions research study different instance features terms goodness predictive measure algorithm performance investigate relationships different instance features different algorithms performance 
aim develop unified machine learning methodology automate process knowledge discovery reasoning regard solving algorithm selection problem 
machine learning models algorithm selection motivation came inspiration automating mim human expert algorithm selection process 
real world situations algorithm selection done hand experts theoretical understanding computational complexities various algorithms familiar runtime behaviors 
automation expert algorithm selection process aspects analytical aspect experimental aspect 
theoretically aspect hard automated compiled program 
examine detail chapter 
comparatively automating experimental aspect feasible progresses experimental algorithmic joh machine learning mit uncertain reasoning techniques pea 
difficulty automatic algorithm selection largely due uncertainty input problem space lack understanding working mechanism algorithm space uncertain factors implementations run time en 
especially true np hard problems complex randomized algorithms 
viewpoint expert systems machine learning auto matic algorithm selection system acts intelligent meta level reasoner able learn uncertain knowledge algorithm selection past experiences learned knowledge models reason algorithm selection input instance order right decision 
thesis statement dissertation develop theoretical results automatic algorithm selection 
show rice theorem exist program auto matically select best algorithm descriptions input instance algorithms 
general framework problem hard ness algorithm performance search kolmogorov complexity order show difficulty analytically deriving algorithm performance input instance 
driven theoretical results propose machine learning approach build automatic algorithm selection systems experimental methods machine learning techniques 
choose problems sorting finding mpe test cases 
chosen representatives important complexity classes np complete 
sorting easy fundamental problem computer science gen eral 
mpe problem interesting important inference problem artificial intelligence ai probabilistic reasoning bayesian networks 
decision version mpe problem np complete tasks inter ests example sat vertex covering converted mpe problem coo shi 
sense mpe problem representative np hard combinatorial optimization problems 
problems apply procedures 
identify list candidate algorithms solving problem 

identify list feasible instance characteristics domain knowledge 

generate representative set test instances different characteristic values settings uniformly random 

run candidate algorithms designed instances col lect performance data produce training datasets 

apply machine learning techniques training data induce predictive gorithm selection model decision tree naive bayes classifier bayesian network 

new instance analyze quickly characteristic values learned models infer classification best algorithm solve 
studying mpe problem discovered important multifractal property joint probability distributions bayesian networks 
specif ically sufficient asymmetry individual prior conditional probability dis tributions joint distribution highly skewed clusters high probability instantiations scales 
clustering property designed phase hybrid sampling search algorithm solving mpe problem bayesian networks 
finding mpe np complete expect multifractal meta heuristic applied solving np hard combinatorial optimization problems 
summary thesis dissertation fold 
theoretically automatic algorithm selection impossible description input instance algorithm 

machine learning approach experimental methods machine learning techniques build automatic algorithm selection systems select suitable algorithm input instance characteristics 

bayesian networks skewed cpts multifractal prop erties meta heuristic design new search algorithms solving np hard optimization problems 
organization organization dissertation follows chapter gives brief main themes dissertation 
research problem clarified main motivations directions described fold thesis stated organization dissertation 
chapter introduce concepts background materials related areas relationship research 
areas include computational com plexity theory computability theory algorithmic information theory kolmogorov complexity main issues experimental algorithmics various machine learning tech niques probabilistic learning reasoning models 
formal definitions related works surveyed 
goal dissertation unify concepts developments different areas solving algorithm selection problem 
main theoretical results chapter 
rice theorem ap plied illustrate infeasibility building automatic algorithm selection system analytical methods 
general framework prob lem instance hardness algorithm performance search kolmogorov complexity developed discuss related issues 
results applied study ga hardness 
driven infeasibility analytical methods turn machine learning inductive approach experimental nature exploits various machine learning techniques 
chapter describes discovery multifractal property joint probability distributions bayesian networks 
algorithm finding mpe multifractal meta heuristic developed experimental results pre sented 
chapter reviews major issues machine learning describes proposed machine learning approach algorithm selection 
chapter apply proposed approach machine learning algo rithm selection sorting 
different presortedness measures studied 
set random generation algorithms 
algorithmic experimental design described various machine learning algorithms applied induce algo rithm selection model experimental data 
performance algorithm selection system evaluated 
chapter applies machine learning methodology algorithm selection mpe problem 
mpe instance characteristics discussed random instance generation algorithm described 
various machine learning algorithms applied induce algorithm selection model experimental data results evaluated 
chapter summarizes results dissertation points open questions directions 
chapter background automatic algorithm selection crossroads fields including computa tional complexity theory computability theory algorithmic information theory exper algorithmics machine learning artificial intelligence 
chapter review concepts related areas 
formal definitions related research 
concepts introduced partly thesis self contained importantly set formal notations thesis 
related works surveyed chapter 
theoretical aspects section review concepts computational complexity theory computability theory algorithmic information theory 
complexity theory provides fundamental concepts algorithm selection 
results computability theory algorithmic information theory chapter derive theoretical results undecidability automatic algorithm selection 
computational complexity theory computational complexity theory gj pap central field computer science 
main goal field classify computational problems intrinsic computational difficulty 
provides basics solving automatic algorithm selection problem 
central question complexity theory problem computing resources need solve 
problem instance algorithm defining terms 
distinguish problem instance 
informally problem general question answered usually having parameters 
problem described giving configuration parameters statement properties answer solution required satisfy 
instance problem obtained specifying particular values parameters 
formally problem total function strings alphabet 
represent set finite strings 
joh definition definition problem instance problem set ordered pairs strings called instance 
called answer stance string occurs component pair 
example consider reachability problem directed graph reachability instance directed graph nodes question path 
answer path 

problems reachability problem infinite set possible instances 
example graph shown want ask exists path node notice reachability problem asks question requires answer 
problems called decision problems 
decision problems counting problems search problems optimization problems counting instance reachability problem problems functions answers nonnegative integers 
search problems string relations 
optimization problems special kinds search problems objective find best maximum minimum possible solutions defined objective function 
decision problems particular important class problems 
complexity classes defined decision problems decision problems natural formal counterpart called languages provide suitable object study mathematically precise theory computation 
definition language language subset language corresponding decision problem rl 
decision problem corresponding language 
problem language set strings finite alphabet 
instance problem string 
computer science solve problem means find algorithm solves instances problem assuming time space resources provided 
goal algorithm solves problem follows instance produce answer algorithms general mechanical procedures followed step step solve problem 
formally algorithms defined model computation turing machines tms gj pap 
variants turing ma chines 
thesis consider deterministic turing machines nondeterministic turing machines 
look 
dtm composed infinite tape bounded left read write tape head finite control unit 
definition deterministic turing machine dtm tuple finite set states finite set allowable tape symbols blank symbol set input symbols transition function left right stay start state set final states 
machine starts takes step changes state prints symbol advances cursor takes step 
input machine halts final state say accepts halts final state rejects halts final state output string time halting 
possible machine halts 
turing machines provide ideal computational model solve string related problems computing string functions accepting deciding languages 
language 
turing machine string 
say decides decided turing machine called recursive language 
say accepts string halt continue forever halting 
accepted turing machine called recursive enumerable language 
turing machines thought algorithms solving string related prob lems 
order solve problems turing machine need encode problems strings 
clear finite mathematical ob jects interest represented finite string appropriate alphabet 
thesis shall move freely strings problem instances algorithms explicitly saying assuming problem instances algorithms encoded strings 
assume en coding scheme reasonable sense garey johnson gj 
fixed representation algorithm decision problem simply creating turing machine decides corresponding language 
turing machine accepts input represents instance problem rejects 
turing machine exceedingly simple amazingly powerful model com putation 
turing claimed tur process naturally called effective procedure realized turing machine 
known famous church turing thesis computable computable turing machine 
efficient computability complexity class having defined notion algorithms turing machines consider time complexity algorithms problems central algorithm selection problem 
time computation dtm program input length number steps occurring computation halt state entered 
dtm program halts inputs time complexity defined follows definition time complexity tm max takes time 
definition see time complexity function algorithm expresses time requirements giving possible input length largest amount time needed algorithm solve problem instance size 
sense worst case analysis 
different algorithms different time complexities 
particularly computer sci realized significant distinction polynomial time algorithms exponential time algorithms 
time complexity function polynomial time algorithm bounded polynomial function exists constant cp 
exponential time algorithm bounded exponential time complexity function 
researchers associate efficient algorithms terminate time polynomial length input 
algorithm called efficient polynomial time algorithm 
exponential time algorithms considered cient 
reason ruling exponential rates known universe small accommodate exponents pointed lev 
light years planck units wide 
system particles packed planck units radius collapses rapidly universe sized neutron star 
number particles 
reason computer physical limitations raw 
computer perform switches second 
frequency visible lights energy needed switching break chemical bonds holding solids 
operations second range expect best fold speedup 
grows factor better increases 
furthermore theory happen faster seconds time light takes cross diameter proton 
times faster eats increases 
intrinsic complexity computational problem measured complex ity best algorithm solve far 
problem considered tractable exists polynomial algorithm solve 
contrast problem intractable hard polynomial algorithm possibly solve 
notion tractability formally captured complexity class definition complexity class class languages decision prob lems solved polynomial time deterministic turing machine 
wide range problems known researchers continually attempting identify members 
significant addition membership list aks algorithm aks primality test 
com plexity aks algorithm log log log time polynomial means time takes run algorithm constant times number digits twelfth power times polynomial evaluated log number digits 
algorithm selection problems badly needed comparing harder problems basically candidate algorithms considered efficient 
important crucial applications require optimal computations 
polynomial time algorithms best situation 
highly theoretical importance able identify best algorithm task 
thesis shall sorting case study algorithm selection problems 
np completeness intractability shown represents class problems solved poly nomial time deterministic turing machine 
researchers identified practice large class problems verified polynomial time deterministic turing machine 
known deterministic algorithm solve class problems polynomial time 
somebody claimed instance problem answer exists time algorithm check verify claimed answer 
capture notion polynomial time verifiability introduce definition nondeterministic turing machines complexity class np follows definition nondeterministic turing machine turing machine state combinations contents current input sym bol current state 
input accepted move sequence leads acceptance 
glance ill defined concept specified machine confronted possible transition choices 
case machine takes available choices 
answers decision problem possible sequence choices leads machine state answers sequence choices leads state halt 
way understanding seeing dtm additional guessing head 
solving problem guesses answer checks 
input program possible computations possible guess 
say accepts accepting computation halts state 
define complexity class np follows definition complexity class np class languages decision prob lems accepted polynomial time nondeterministic turing machine 
clearly solve problem dtm solve special cases 
natural question problems solve solve 
important open problem theoretical computer science versus np problem coo 
problems np np complete problems hardest ones solve single np complete problem polynomial time problems np solved efficiently np notion precisely need introduce concepts reduction completeness gj 
definition reduction say language reducible function strings strings input true 
called reduction 
meaningful usually require computable deter turing machine space log time polynomial pap 
reduction transitive 
reduction language reduction composition reduction 
fact orders problems respect difficulty 
shall particularly interested problems chain 
definition completeness complexity class language complete language reduced complete problems extremely central concept tool complexity ory 
capture essence difficulty complexity class 
np complete problems complete problems class np satisfiability problem problem proven np complete cook coo 
specified follows satisfiability sat instance set boolean variables collection clauses question satisfying truth assignment 
famous cook theorem stated follows theorem cook theorem sat np complete 
proof cook theorem done transforming sat problem non deterministic turing machine 
solve sat quickly find solutions nondeterministic turing machines quickly 
definition np np problem solved nondeterministic turing machine polynomial time 
turn solve sat quickly solve np problem quickly 
proves sat np complete 
having concrete np complete problem hand people np complete problems reducing sat 
proof problem np complete consists steps 
prove np second prove np complete problem reduces thousands problems various applications tsp vertex cover clique partition hamiltonian circuit shown np complete sat gj 
recall solve np complete problem polynomial time show np conversely prove np np complete problems solved efficiently 
unfortunately true possible 
people believe np able prove disprove far 
aspect importance np completeness shown problem np complete reasonable direct efforts alternative approaches available example developing approximation algorithms studying special cases analyzing average performance algorithms developing randomized algorithms resorting heuristic methods 
np hard optimization problems far restricted discussions decision problems defining complexity classes np real world applications practical problems search problems optimization problems 
search problem consists set instances instance set solutions 
algorithm said solve search problem input instance returns answer empty returns solution 
optimization problem special kind search problem 
search consists set instances set solutions instance additionally instance candidate solution objective function assigns positive rational number pair defined 
optimization problem minimization maximization problem 
correspondingly optimal solution minimum maximum value possible solutions 
problem np complete corresponding search optimization problem usually np hard means hard np complete problem 
notion np hard formally defined turing reduction oracle turing machine refer readers gj 
remember intuition wants capture np complete harder 
practice call optimization problem np complete convenience decision version np complete 
np complete problems automatic algorithm selection significantly valuable 
np complete problem exists tractable special cases find dividing line apply different algorithms special cases 
general cases usually alternative algorithms available approximation algorithms randomized algorithms heuristic ones 
algorithms different properties perform best different classes instances 
selecting appropriate algorithm instance imperative 
secret instances np complete problem equally hard 
researchers discovered changing values parameters instance go easy hard easy phase transition process 
interesting important identify turning points treat hard instances separately specially designed algorithms 
thesis choose mpe problem decision version np complete case study building automatic algorithm selection system np hard optimization problems 
generally speaking approaches solve algorithm selec tion problem analytically experimentally 
furthermore analytical methods algorithm comparison algorithm selection 
apply worst case analysis algorithms compare complexities select best 
method fundamental 
algorithms may bad worst case time complexities perform average practice 
complex approximate randomized heuristic algorithms genetic algorithms hol gol simply resist formal analysis 
second way analyze algorithms average case complex ity 
method requires strong assumption distribution input instances hard cases 
third method considers instance classes single instances man 
defines subproblem original problem intuitive criteria instance easiness hardness study worst case complexity subproblem 
algorithms designed analyzed subproblems resource requirements increasing smoothly moving larger subproblems 
drawback method feasible simple problems sorting hard applied np hard optimization prob lems important ones practice 
method provides way design adaptive algorithms optimal measures usually impossible design algorithm optimal measures 
algorithm selection problem different adaptive algorithms 
furthermore analytical methods generally suitable making predictions empirical hardness problem instances run time performance algo rithms 
ultimate goal analytical approach select best algorithm just analyzing description algorithms input instance running 
ideally program able take input descriptions instance solved candidate algorithms return best algorithm 
show chapter inherent scheme 
thesis propose experimental machine learning methodology automatic algorithm selection analytical results serve mainly source domain knowledge guide experimental design 
methodology reasonable feasible reasons 
cases analytical algorithm selection possible theoretical results need implemented verified 
second practice algorithm selection experts seldom depend solely theoretical analysis observing algorithms run time behaviors 
third analytical method inherent limitations 
explore point chapter 
developments field experimental algorithmics machine learning ai provided set powerful techniques implementing experimental machine learning approach solve algorithm selection problem 
computability theory goal computational complexity theory identify intractable decision prob lems efficient polynomial time algorithm exists solve 
exists problems algorithm exists 
computability theory concerned identifying kind unsolvable problems 
core automatic algorithm selection stated decision problem algorithm input instance algorithm performance criteria measured real number performance better 
order study decidability problem briefly go basic concepts computation theory hs section 
recursive languages decidability previous section considered decision problems languages algorithms turing machines recognize languages methods determining string language 
talked recursive recursively enumerable languages 
quickly recall language recursively enumerable re accepted turing machine halt input halt run forever 
case say solve decision problem language language said recursive exits turing machine accepts halts inputs 
turing machine halts solves decision problem language accepts 
definition decidability follows definition decidable language decidable recursive 
intuitively problem decidable exists effective procedure algorithm computer program solves problem 
procedure exists problem undecidable 
may think existence unde problems poses limitations effectiveness analytical problem solving mathematics 
existence undecidable languages existence undecidable languages easy see simply languages turing machines pap hs 
just turing machines languages 
shown difference countable uncountable sets 
formally set countable finite exists correspondence set natural numbers 
set real numbers uncountable real numbers natural numbers 
shown cantor diagonalization argument 
turing machine encoded binary string 
easy see correspondence set natural numbers set turing machines 
say set turing machines countable 
hand set languages decision problems uncountable build correspondence set languages set real numbers uncountably infinite 
language just set strings alphabet order strings ordering give string index number starting 
example 
subset forms language 
language encoded th decimal position denote inclusion string index number 
set languages precisely set encodings real numbers uncountable 
undecidability halting problem turing showed defined decision problem halting prob lem settled effective computational procedures 
halting problem follows 
halting problem halt instance turing machine input word question eventually halt input 
give proof undecidability halt 
theorem halt undecidable 
proof 
assume halting problem recursive program takes input turing machine input halts returns halt returns loop 
construct program follows function loop return halt true loop forever program input says halts loop constructed 
says loops halt 
case gives wrong answer assumption wrong 
got undecidable problem prove undecidability problem reducing previously determined undecidable problem 
rice theorem undecidability halt show set problems undecidable 
result rice theorem ric hut 
property recursively enumerable languages set re languages possesses particular property 
property trivial empty satisfied language re languages satisfied re language 
rice theorem stated follows theorem rice theorem non trivial property recursively enumerable languages undecidable 
proof 
see ric pap hut rice theorem means arbitrary algorithm algorithm decides non trivial property language defined proof rice theorem consists reduction halting problem 
shows property checking algorithm devise algorithm solving halting algorithm 
refer interested readers ric pap hut details 
chapter result show undecidability algorithm selection problem 
kolmogorov complexity algorithmic information theory practice algorithm design special case algorithms designed special class problem instances 
best target class instances algorithms compile special information input instances 
problem theoretical interest measure mutual information input instances algorithms solve 
intuitively mutual information better algorithm perform instance 
concept information different notion entropy developed shannon classical communication information theory sha ct 
entropy defined random variable outcomes set measure information produced set specific value assigned measures uncertainty change instantiate probabilistic notion natural information transmission communication channels developed 
context instance hardness algorithm performance interested measuring information conveyed individual finite object problem instance string finite object algorithm string 
order investigate issue need look results developed kolmogorov complexity lv algorithm information theory kol cha 
concepts reviewed section chapter develop framework problem instance hardness algorithm performance 
universal turing machine universal turing machine utm computational model 
utm turing machine simulate turing machine encode action table string construct turing machine expects tape string describing action table followed string describing input tape compute tape encoded turing machine computed 
utm thought standard general purpose computer runs programs data usual way 
fundamental fact machine exists constructed effectively 
guarantees properly defined concepts utm sort invariant property lv 
kolmogorov complexity kolmogorov complexity called descriptive algorithmic complexity lv developed sol kolmogorov kol chaitin cha 
basic idea measure complexity string size bits smallest program produce 
idea comes observation random strings difficult compressed strings internal regularities 
formal definition follows definition kolmogorov complexity universal turing ma chine 
shortest program generates kolmogorov com plexity string defined min denotes starting terminates leaving definition conditional kolmogorov complexity strings shortest program generates conditional kolmogorov complexity defined min definition easy see 
defi nitions machine independent defined utm 
kolmogorov complexity measures randomness string incompressibility 
random string larger value 
larger generate program code print pro gram marginally longer example intuitively looks random shannon measure probability selected ensemble possible strings bits 
write program print shorter length generate 
write program print generate random structural regularity program shorter 
example 
fact implies internal structure regularity compressed means random incompressible 
reason randomness means incompressibility 
invariance theorem utm notion kolmogorov complexity captures intrinsic property string finite object independent choice mode description 
algorithmic information theory kolmogorov complexity measure information encoded string information string contains kol lv lv 
kolmogorov complexity string seen absolute information string 
interpretation quantity information needed generate scratch 
similarly quantifies information needed generate may say contains lot information applications definition information advantage refers individual objects objects treated elements set objects probability distribution sha 
definition algorithmic information strings 
algorithmic information defined value interpreted amount information needed produce interpreted amount information added produce definition 
string contains information 
study problem instance hardness algorithm performance stances algorithms seen strings 
concepts algorithmic information theory measure absolute information contained instance relative information contained algorithm instance 
serve naturally measures instance hardness algorithm performance 
experimental aspects section review major issues arise experimental aspects au algorithm selection 
main purpose algorithmic experiments research generate high quality representative training data set contains knowledge seek regards solving algorithm selection problem dependency relationships problem instance features algorithm formance measures 
experimental analysis algorithms past years dominant method algorithm analysis asymptotic analysis algorithm worst case average case behavior 
experimental analysis algorithms intensively related fields operations research artificial intelligence invisible algorithm data structure community 
growth interests experimental algorithmic works hoo mor joh mcg msf 
newly developed field called experimental mics studies algorithms data structures joining experimental studies traditional theoretical analysis 
researchers realized theoretical results tell full story algorithm performance 
invented algorithms especially randomized heuristic algorithms genetic algorithm ga gol complex detailed mathematical analysis reasonable 
usually algorithms practice facts known performance really fully analyzed 
main advantages experimental algorithmic analysis allows investigate algorithmic performance statistically depends problem characteristics 
central problem automatic algorithm selection 
years collection rules thumb follow number pitfalls avoid accumulating regarding experimental analysis algorithms solid science experimental algorithmics 
clear right answers questions experimental analysis algorithms 
joh johnson listed challenges theoretical computer science new century 
research discuss basic issues experimental algorithmics centering algorithm selection problem 
issues include selection algorithms features random generation problem instances measurements algorithm performance assure reproducibility platform independence results analysis results 
algorithmic experiment setup algorithmic experiments involve running algorithms problem stances collecting data analyzing result 
experiments advance hypotheses questions test ask 
central hypothesis research exists dependency problem instance tics algorithm performance knowledge exploited build algorithm selection system gain efficient computation 
designing experiments hypothesis consider issues 
algorithms features instance features algorithms basic elements exper iments choice combination important 
domain knowledge obtained analytical results plays important role making choice 
partic ular computational problem usually exists list algorithms defined studied certain extent literature 
cases exist single algorithm outperforms entire problem domain 
different algorithms different properties algorithm typically efficient subset possible instances 
knowledge algorithm works better class instances usually explicitly available 
may vague information permutation high presortedness insertion sort network large sparse clique tree propagation algorithm works fast 
cases cutting point working range algorithm needs determined precisely experiments order facilitate automatic algorithm selection 
selecting candidate algorithms straightforward 
theory algo rithm candidate 
practice domain knowledge filter algorithms obviously inferior difficult implement 
candidate algorithm easy implement special class instances efficiently solved 
selection instance features relies heavily domain knowledge 
feature easy compute usually negligible compared problem solving time relevant performance particular algorithm particular class algorithms 
generation training test instances biggest practical challenges algorithmic experiments assembling required test instances 
best test instances probably real world instances taken real world applications 
represent im portant set instances 
unfortunately rarely possible collect real world problems instances computational problems 
research taken nearly years accumulate real world bayesian networks 
applications real world instances limitations potential real world instance may implemented 
real world instances may cover problem characteristics interest 
alternative randomly generated instances 
requires develop random generator generate instances specified parametric character uniformly random 
problem quite challenge 
open question exists polynomial random generation algorithms np languages san 
naive approach exhaustively enumerate possible instances pick uniformly random 
apply ran dom generation problems simply far possible instances general 
method research polynomial time uniform random generation algorithm markov chain approach sin mm ic 
basic idea construct dynamic stochastic process finite markov chain states correspond set structures interest 
process able move state space means random local perturbations struc tures 
process designed ergodic 
allowed evolve time distribution final state tends asymptotically unique stationary distribution independent initial state 
stationary distribution designed uniform 
simulating process sufficiently steps outputting final state able generate elements uniformly random set uniform distribution 
choice combining real world instances random generation gether 
analyze real world instances get characteristic vector statistically representing distribution real world instances input char vector random generator produce random synthetic real world instances 
experiments kinds instances 
algorithm performance measurements measuring collecting algorithm performance data practical challenge 
related issues include measure ensure reproducibility results reduce variances 
running time solution quality common measures algorithm performance measures space important 
tractable problems sorting measuring actual running time cpu time wall clock important 
may hard interpret influenced uncertain factors environments 
granularity time unit may cause problems 
experiments sorting find high resolution time stamp facility ibm measure run time microsecond jdk provides millisecond resolution 
algorithms solving np hard problems measures number calls crucial subroutine usually better choice 
heuristic search algorithms need evaluate search points calling solution evaluation subroutine problem solving process 
measuring number calls evaluation subroutine ensures results independent plat forms improves reproducibility results 
issue balance time solution quality 
far effective way fairly compare different heuristic algorithms allow algorithms consume amount computation resources distinctions quality solutions obtained ru 
variance reduction techniques mcg necessary dealing ran algorithms 
carefully handled variability runs algo rithms instances may obscure results hard interpret 
instance variance reduced set randomly generated stances runs algorithms see instances 
algorithm variance reduced performing algorithm instances runs recording average performance 
bayesian networks reasoning uncertainty common issue daily life central ai 
bayesian networks bns pea nea rn known bayesian belief networks belief networks causal networks probabilistic networks currently dominant technique ai uncertain knowledge representing reasoning 
underlying principle apply probability theory building intelligent systems reasoning incomplete information uncertain knowledge 
years ago considered impractical mainstream ai com due difficulty manipulating exponentially sized joint probability distributions 
breakthrough early pearl introduced probabilistic graphic model bayesian networks published effi cient elegant message propagation inference algorithms particular classes bns kp pea pea 
bns combine representational algorithmic power graphic theory probability theory 
provide compact natural tool representing uncertain knowledge facilitate efficient inference learning algorithms 
bns proven useful wide range applications including medical diagnosis decision support systems real time monitoring intelligent user interface image analysis 
bayesian networks play important role research serve purposes different levels 
hand point view learning meta level knowledge representation model classifier just decision tree naive classifier 
hand inference problems mpe problem chosen test case machine learning algorithm selection methodology 
section introduce basic concepts representation inferences bayesian networks 
introduce bayesian networks learning chapter learning algorithms 
representation bns domain interest viewed probabilistic model consists set random variables 
random variable take particular values certain probabilities theory jpd probabilistic model contains complete information random variables dependencies 
practice obviously infeasible deal jpd directly 
bns sidestep jpd directly conditional probabilities order reduce representational computational complexity 
syntax bayesian networks bayesian network rn graph holds 
set random variables corresponds nodes network 
simplicity losing generality consider discrete variables 

set directed links connects pairs nodes 
intuitive meaning arrow node node direct influence 
node conditional probability table cpt quantifies effects parents node 
parents node nodes arrows pointing 
graph directed acyclic graph dag 
topology network thought knowledge base domain 
represents general structure dependencies variables 
semantics bayesian networks semantics bayesian networks understood ways 
see network representation 
second see encoding collection conditional independence statements 
views equivalent emphasize different aspects 
helpful understanding construct bns 
helpful designing efficient inference algorithms 
viewpoint bayesian network provides complete description domain 
encodes jpd compact manner 
entry jpd calculated information network formula chain rule 
xn xi xi numbers network probabilities interpreted beliefs probability measure belief proposition particular evidence che 
called belief networks 
interpreting avoids diffi culties associated classical frequency definition probabilities 
second viewpoint topologically bns types local connec tions decompose jpd series conditional independence statements linear diverging converging 
concept separation rn fundamental bns 
states path node node separated set nodes conditionally independent 
helpful designing efficient inference algorithms 
set nodes separates nodes node path condition holds 
path linear 
path converging 
path diverging descendant inferences main purpose building bayesian networks perform inference compute answers users queries prediction diagnosis explanation domain exact values observed evidence variables 
basically types bn inference tasks belief updating belief revision 
belief updating algorithms belief revision just minor modifications vice versa 
belief updating belief updating called probabilistic inference denoted pr 
objective calculate posterior probabilities query nodes observed values evidence nodes simple form results single node interested computing posterior marginal probabilities single query node 
mainly involves marginalization operation query nodes 
name tells posterior probabilities change smoothly incrementally new item evidence 
belief revision task belief revision amounts finding probable configuration hypothesis variables observed evidence 
resulting output optimal list instantiations hypothesis variables list may change abruptly evidence obtained 
belief revision case contains non evidence nodes known computing probable explanation mpe 
explanation observed evidence complete assignment 
xn xn consistent computing mpe finding explanation explanation higher probability 
max mp typical optimization maximization problem 
computing means finding top highest explanations 
cases contains partial subset non evidence nodes task called finding maximum posteriori hypothesis map map involves marginalization maximization 
complexity inferences bayesian networks computing pr mpe map np hard 
belong different complexity classes 
mpe essentially combinatorial optimization problem 
mpe np complete precisely decision version np complete shi par 
harder 
counting problem complexity complete functional version coo nea 
decision version complete contains languages exists nondeterministic turing machine majority nondeterministic computations accepts string string language pap 
map hardest 
map combines counting optimization np complete par 
np complete class languages recognized non deterministic turing machine polynomial time oracle query answered free 
special class bn topology polytrees pr mpe poly nomial pea 
map remains np complete polytrees par 
furthermore approximations mpe pr map np hard dl ah par 
particular mpe hard means means approximation scheme mpe np pap 
map polytrees hard par 
related works section review works directly related research applying machine learning approach solving algorithm selection problem 
algorithm selection problem algorithm selection problem originally formulated ric 
mainly applied selection problem solving method scientific computing hcr specifically performance evaluation numerical softwares 
ab model algorithm selection ric reproduced input instance problem space performance criteria 
input problem instance represented feature feature space feature extraction procedure 
task build selection mapping provides measured algorithm solve subject constrains performance optimized 
algorithm selection static dynamic 
static algorithm selection system selection commits selected algorithm dynamic algorithm selection system may change selection dynamically monitoring running algorithm 
special kind dynamic algorithm selection recursive algorithm selection ll ll decision algorithm selection needs time recursive call 
example sorting algorithm needs recursively sort smaller instances 
recursive call need decision sorting algorithm choose 
goal optimize model algorithm selection problem sequence algorithm selection decisions dynamically 
research study static algorithm selection 
methodology developed research extended solve dynamic algorithm selection 
algorithm selection various applications algorithm problem solving technique selection studied various fields 
data compression hsu hz studied automatic synthesis com pression techniques heterogenous files qualitative quantitative properties segment file 
machine learning brodley bro bro investigated selection inductive learning algorithms different learning tasks learning algorithm inductive bias selective superiority 
planning fink fin described selection technique statistical method select planning algorithm time bound problem solving 
constraint satis faction problem csp minton min developed inductive learning system configures constraints satisfaction programs 
works knowledge repre sentation meta reasoner set rules derived simple statistical models 
machine learning mature natural apply advanced models build better algorithm selection systems 
meta reasoning techniques researchers ai long realized great efficient gains achieved locating portion costly computational resources meta level deliberation best way solve problem 
particular breese horvitz bh studied intelligent reformulation restructuring belief network solving inference problem 
described metareasoning partition problem problem ideally resources meta analysis ac tual problem solving 
principles computing ideal partition resources uncertainty types user requirements 
field real time ai flexible computation hor anytime algorithms offer effi cient mechanism trade computation time quality result 
models computations interrupted time produce results guaranteed quality 
gomes selman gs studied problem algorithm portfolio design combine different algorithms portfolio order gain improvement terms performance 
crawford cfgs pro posed framework line adaptive control problem solving combines ideas control systems adaptive solving techniques 
particular field bayesian network inference researchers stud ied inference algorithm selection integration 
santos ssw developed dis tributed architecture unifying various probabilistic reasoning algo rithms anytime properties 
algorithms exploit intermediate results produced algorithms 
different algorithms anytime properties harnessed cooperative system exploits best characteristics algorithm 
framework bor williams wil studied algorithm selection problem bn inference algorithms 
performance profiles williams developed selection process simple analytical prin 
nicholson jn investigated various network characteristics empirically studied relationship instance features inference algorithm performance 
bayesian approach related field np hard problem solving researchers applied various machine learning techniques learn empirical hardness optimization problems tackle hard computational problems 
particular bayesian learning approach characterizing run time problem instances randomized backtrack style search algorithms developed solve hard class structured constraint satisfaction problems 
approach learn dynamic restart policies randomized search procedures take real time observations attributes instances solver behavior consideration 
earlier horvitz klein hk constructed bayesian models considering time expended far theorem proving 
summary chapter reviewed concepts various related fields 
fields aspects consists computational complexity theory computability theory algorithmic information theory 
fields related experimental aspects emphasize role algorithmic experiments approach 
major experimental issues discussed 
introduced basics concepts bayesian networks 
reviewed researches various areas closely related 
research aims unifying ideas different areas building gorithm selection systems experimental machine learning methods 
far aware attempt systematically combine algorithmic methods machine learning techniques solving algorithm selection problem 
researches algorithm selection simple knowledge representation statistic models simple rules 
rely analytical princi ples hard automated generalized 
approach relies experimental methods automation easier 
machine learning data mining techniques help gain insights knowledge algorithm run time behavior analytical methods provide 
believe proposed methodology extended dynamic algorithm selection 
perspective solving hard problems provides effective way integrating different algorithms build powerful solvers np hard optimization problems 
chapter theoretical results analytical algorithm selection chapter theoretical results algorithm selection prob lem 
show rice theorem general algorithm selection problem undecidable 
propose general theoretical framework instance hardness algorithm performance search algorithmic information ory kolmogorov complexity 
claim algorithm selection search undecidable kolmogorov complexity 
apply theoretical framework ga hardness show nonexistence predictive ga hardness measure description input instance con ga driven theoretical results turn feasible direction applying inductive approach analytical approach 
pro posed inductive approach relies significantly experimental methods machine learning techniques build algorithm selection systems 
undecidability general automatic algorithm selection problem introduced chapter algorithm rendered turing ma chine problem rendered language 
automatic algorithm selection problem asks design algorithm turing machine takes inputs descriptions candidate algorithms problem instance outputs evaluations algorithms performance criteria 
common criteria includes problem solving time quality solution returned 
assuming algorithms return correct solution instance consider time performs better selected 
language decision problem algorithm selection problem formulated follows definition language algorithm selection problem accepts steps returned solution corresponds algorithm 
input instance solved algorithm 
solution answer instance represents number steps operates 
recall rice theorem states non trivial property turing machine undecidable 
property corresponds set recursively enumerable languages possessing particular property 
property trivial empty satisfied languages satisfied languages 
property non trivial turing machine property property 
consequence rice theorem pragmatic computer science shown grading program example tay 
suppose instructor intro programming language class asked students write program computes partial recursive function 
suppose function defined dozens cases 
usually instructor needs review code examine listing output number inputs 
implementations vary tremendously student student instructor hope grading program run student source code determine fact computes correctly 
unfortunately rice theorem asserts grading program 
nontrivial set partial recursive functions nontrivial set recursive enumerable languages 
rice theorem membership set turing machine computes undecidable 
furthermore church turing thesis program equivalent turing machine 
membership set program computes undecidable 
conclude envisioned grading program exist church turing thesis true 
rice theorem eliminates hope algorithmically testing input output behavior arbitrary programs 
implies program verification techniques general tive 
general order establish program correctness resign ourself working techniques 
going back algorithm selection problem easy see non trivial language 
applying rice theorem result theorem undecidable 
proof 
recursively enumerable 
simply build tm simulate input see halts time returns solution second specifies property non trivial recursively enumerable languages represent tm accepts time returns solution exists language satisfies 
rice theorem non trivial property recursively enumerable languages undecidable undecidable 
implies general hope finding means automatic algorithm selection descriptions algorithms 
general result surprising halting problem basically says tell turing machine algorithm halt 
related result undecidability equivalence turing machines says turing machines language eq undecidable 
framework instance hardness algorithm performance search demonstrated undecidability automatic algorithm selection tur ing machine computational model 
section look restricted practical case automatic algorithm selection search 
intuitively search algorithm perform better compiles informa tion input instance search space 
need investigate information contained string algorithm string prob lem instance 
kolmogorov complexity lv measure absolute information content individual objects 
algorithmic information defined kolmogorov complexity measures mutual information individual objects 
natural concepts study instance hardness algorithm formance search 
general framework instance hardness algorithm performance search kolmogorov com plexity 
framework crude form needs refined 
predictive model 
apply show gen eral problem automatic algorithm selection search impossible mutual information search algorithm input instance 
set stage introducing black box optimization model wm wm cul 
black box optimization model function seek set optimal solutions maximizes minimizes finite set strings finite set real numbers 
people interchangeably called fitness function objective function cost function search space simply problem instance 
heuristic search algorithm usually starts initial point search space population initial points explores search space keeping population solutions associated values tries improve solutions 
search algorithm seen function mapping search history new point set new points search space heuristic 
simplicity assume revisits points searching history 
algorithms hill climbing simulated annealing tabu search gas working manner 
time solution quality common measures evaluating comparing search algorithm performance 
heuristic search time measured number search steps number times search space visited solution quality measured error global optimal best solution far 
general case algorithm performance measure defined function factors users requirements 
efficiency crucial precision important 
situation information extracted search histogram 
random instance random algorithm search problem search algorithm model represented strings string representation mapping table entries listing pairs forms strings 
defining amounts specifying 
observe practice problem instances search spaces regular 
regularity instance exploited design efficient algorithms 
algorithms compile useful problem domain information structures blindly random search solve problems efficient 
define randomness problem follows am definition randomness randomness problem instance defined bits kolmogorov complexity 
log 
ranges logarithm 
define random problem versus structured problem 
fix threshold particular choice crucial theory introduce definition am definition random structured problem problem random 
problem structured random 
search algorithm seen function coded string define concept random algorithm structured algorithm manner 
please note words random structured precise meaning regarding length shortest programs generate 
definition random probabilistic search general brute force enumeration search random algorithms terms kolmogorov complexity 
straightforward understand random probabilistic search random kolmogorov 
general brute force enumeration algorithm random just enumerates points search space treats points equally 
bias search points 
shortest program generate list mapping pairs 
fundamental result kolmogorov complexity nearly string random 
correspondingly say model problems random problems algorithms random algorithms 
fact suggests method generate random problem random algorithm probabilistic manner 
suppose fair coin ideal random probability source 
entry truth value table problem flip coin set value 
generate random problem high probability 
random problem instances random algorithm fundamental concepts subject 
random problems hardest problems contain internal structures algorithm exploit 
example needle haystack problem gre :10.1.1.49.1448
algorithm solve problem efficiently random search wm wm 
random kolmogorov search algorithms efficient algorithms comply information problem just visit search space randomly 
instance hardness algorithm performance randomness instance indicates hardness 
instance highly random contains internal structures algorithm exploit 
hard solve 
capture intuition define instance hardness follows definition instance hardness hardness problem instance defined randomness information instance algorithm indicates algo rithm performance solving instance 
algorithm contains lot infor mation instance solve instance efficiently 
example hill climbing algorithm assumes search space contains single peak algorithm solve instance easily 
define algorithm performance follows definition algorithm performance performance algorithm stance defined algorithmic information contained kolmogorov complexity provide natural definitions instance hardness algorithm performance kolmogorov complexity 
unfortunate fact kolmogorov complexity explicitly computed 
individual object way tell incompressible shown proof lv 
theorem kolmogorov complexity finite string computable 
proof 
binary strings length binary programs length computed program length complexity suppose 
computable 
lexicographical say xn complexity xn 
computability 
compute complexities binary strings length find lexicographical complexity definition xn 
information required compute xn number log bits plus fixed standard program obtain xn logn 
contradicts xn apart finite initial set values computable 
limits practical applications kolmogorov complexity 
result context means automatic algorithm selection search decidable 
deceptive problems deceptively solving algorithms consider question problem instance design algorithm performs worse random search 
problem happens random contains internal structured information design algorithm perform better worse random search 
structured problem information structure hard design algorithm worse random search know deceive 
case problem structured know information problem 
resulting algorithm called deceptively solving algorithm purpose seek perform worse random search 
structured algo rithm contains structural information problem 
algorithm uses structural information pathological way 
hand algorithms structured perform better random search 
algorithms called straightforwardly solving algorithms 
similarly nonrandom algorithm information search strategy dynamics impossible probability approaching zero come structured problem deceptive worse random problem 
knowing search strategy algorithm design problem fail 
call problem deceptive 
non deceptive problems called straightforward problems 
analysis see structured instance structured algorithms divided straightforwardly solving deceptively solving gorithms 
similarly structured algorithm space structured problem instances divided straightforward problems deceptive problems 
viewpoint information interpret structured prob lem straightforwardly solving algorithm contains positive information straightforward problem random algorithm contains zero information deceptively solving algorithm contains negative information 
similarly structured algorithm contains positive information straightforwardly solving problems contains zero information random problems contains negative information deceptive problems 
concept algorithm containing negative information problem instance captured kolmogorov complexity 
reasonable concept context instance hardness algorithm perfor mance 
turing machine invented turing order formalize notion algorithm effective 
tur turing sees operation turing machine different process human computing mechanic procedure 
implementation algorithm idea problem solving procedure existing algorithm designer brain 
designer comes idea information understanding input instance 
just idea designer brain right wrong agree true information input instance 
search searcher information search space important perfor mance 
accurate map search space great searcher wrong lead 
case say wrong map poor algorithm wrong heuristic contains negative information search space 
factors cause problem instance hard particular structured algorithm 
random problem structural information algorithm 
example needle haystack problem 

structured problem algorithm contains zero information 
case mismatched 

structured deceptive 
case say algorithm contains negative information problem performs worse random search 
turn call factors randomness mismatch deception 
ga hardness revisited section apply theoretical framework instance hardness algorithm performance study ga hardness 
discuss major tions previous ga hardness research propose promising directions research 
genetic algorithms gas gas defined generic search procedures mechanics natural selection genetics hol gol 
order apply ga search optimization problem encode artificial chromosomes strings parameter lists permutations codes meaningful computer codes 
second fitness function helps evaluate value search point 
having encoded problem chromosomes fixed fitness function discriminating chromosome bad start evolution process creating initial population encoded chromosomes 
gas genetic operators process population generation gener ation 
creates sequence populations hopefully contain better solutions evolution goes 
genetic operators include selection crossover recombination mutation 
operators genetic meaning 
selection allows better individuals offspring 
principle natural selection survival fittest 
prefers better solutions worse ones 
crossover recombination combines pieces parental chromosomes form new hopefully better offspring 
principle sexual reproduction reproduction strategy advanced species 
ensures mixing re combination genes offspring 
mutation simply modifies piece single parental chromosome create new individuals 
corresponds gene mutation genetics 
represents random walk neighborhood single individual 
process natural selection results mutation kept bad ones abandoned 
gas solve hard problems quickly reliably 
little problem specific information clear interface 
noise tolerant easily extensible 
gas successfully applied broad range applications search optimization machine learning 
gas fail times 
gas complex systems hard design analyze 
invention hol researchers put lot effort understanding gas function problem hard gas optimize 
quite open problems despite years research application 
research dynamics gas various models developed explain gas works definitive answer 
models aimed building ga theory mathematical rigor reliable predictive ability gained limited success simplest idealized settings 
include schema analysis hol gol whi difference equations gol gol markov chains gs dav nv vos epistasis analysis dav landscape analysis fm cl transform models bg line research interwoven ga dynamics ga hardness 
goal identify characteristics problem instances hard gas optimize 
characteristics measures proposed distinguish called ga hard ga easy problems 
characteristics achieved goal reliable predictive ga hardness measure 
clude deception gol gol whi isolation noise gol gol multimodality gdh hg rw landscape ruggedness fitness distance correlation jf epistasis variance dav nau epistasis correlation nau site wise optimization measure nau free lunch theorems wolpert macready free lunch nfl wm wm theorems theorems model 
nfl results respectively relating space possible problems space possible algorithms 
free lunch theorem averaging possible problems search algo rithm better algorithm 
free lunch theorem averaging possible search algorithms search problem harder problem 
proven ways including original proof wm wm adversary approach cul simple counting argument rw 
important lesson learn results nfl theorems account particular information search problem algorithm perform better average random search visits points search space uni random 
nfl theorems criticized general practically applicable 
illustrate misconceptions efforts ga hardness research 
misconceptions ga hardness research informally research studies ga hardness ask problem hard easy gas 
statement contains important terms needs elaborated problem gas hardness 
meaning terms precisely clarified agreed 
misunderstandings misconceptions 
section point major misconceptions previous ga hardness researches 
misconceptions problem space problems 
shown factors cause problem instance hard ga randomness mismatch deception 
misconception problem space blurring differences factors feeding gas problems hard meaningful 
random problem needle haystack hard algorithms 
meaningless test case 
example grefenstette deception considered harmful gre needle haystack problem counterexample dismiss utility deception theory building block bb hypothesis gol 
example prove deception factor ga hardness rule usefulness schema theory bb hypothesis explaining effectiveness gas structured problems gol 
just force bb hypothesis apply random problems 
gol goldberg points similar concept design envelope emphasize importance bounding problem difficulty 
bb searcher expected solve problems way unbounded 

second misconception specific problem instances support general result 
size test instances bits gol gre 
expect relative hardness problem instances vary roughly size 
small problem matter character easy algorithms solve 
fact minimal deceptive problem cause simple ga diverge support gas surprisingly powerful gol 
similarly fact small problem high deception easy gas optimize disprove deception role making problems larger size hard gas gre 
misconception problem space applying gas functions problems general realistic 
research theory treated gas generic search algorithm solve functions defined model random problems artificially functions fm fm hg expect meet real world applications 
problem space defined broad 
np problems small fraction problems 
real world optimization problems np complete hard gas deal 
sense working pathological man functions waste effort 
related viewpoint want mention nfl theorems wm wm caused great deal debate genetic evolutionary community published 
researchers directly applied nfl theorems ga claimed ga poor random search ga futile 
conjecture discouraging case 
nfl theorems proven space possible problems practice interested solving real world optimization problems 
purpose developing usable theory ga hardness care ga bad performance single random problem specific deceptive functions 
practice problem specific information generally willing able incorporate information gas operators speed ga performance 
goldberg points gol argument futility 
believe framework supports assertion 
nfl theorem eat ga lunch 
goldberg 
misconceptions algorithm space gas 
gas considered simulation natural adaptive system hol function optimizers bet general purpose search procedures gol assumed converge global optimum 
furthermore advanced operators invented gas changed simple messy gol gol unrecognized 
common ception algorithm space considering gas single algorithm seeking universal ga dynamics general separation ga hard ga easy problems 
efforts doomed fail problem change configuration parameters gas get totally different convergence results ga problem easy ga hard 
label ga easy ga hard 
easy see allowing possible ga operators gas model search algorithms defined model 
asking hardness problem gas averaging hardness problem possible algorithms 
nfl theorems wm wm told averaging space possible search algorithms problem harder problem 
experience algorithm design practice tells algorithms similar parameter values similar behavior problems 
consider classifying gas subclasses parameter values 
gas subclass similar parameters expected similar performance problem instances 
consequence research studies ga dynamics ga hardness done separately subclasses gas 
misconceptions performance measure space ga hardness 
complexity theory gj lee pap hardness complexity problem measured time complexity provably best algorithm possible function size input 
problem tractable find polynomial time algorithm intractable exponential time algorithm exists 
important complexity classes np class polynomial time solvable problems 
np class polynomial time verifiable problems 
np complete problems hardest np real world optimization problems np complete 
np hard approximate 
assuming np impossible find polynomial time algorithm including gas solve np complete problem 
subclasses np complete problem possible find ga solves polynomial time ga exploit structural characteristics problems 
goal ga hardness research identify characteristics instances gas converge polynomial time 
ga converges polynomial time instance class common feature say instances class easy ga feature 
hard ga consider different classes gas similar parameter setting gas 
misconception practice measuring ga performance instances considering scaling issue 
argument section applies 
serious misconception existence gen eral priori ga hardness measure predict ga performance instance 
efforts put searching mea sure dav gdh hg jf nau nk nk 
ideally people wish program takes inputs specification problem instance configuration ga returns estimation ga performance problem decision advance ga 
rice theorem hut told general impossible compute advance ga hardness measure running algorithm problem 
limits general analysis rule possibility inducing algorithm performance predictive model designed experimental results 
parameterized real world optimization problem list gas different configurations design controlled experiment run algorithms problems collect data inducing predictive model data 
approach differs traditional analytical method study performance algorithms 
relies empirical approaches investigate algorithmic performance depends problem characteristics 
proposed new definition ga hardness deter mined growth rate search space function size input instance 
minimum chromosome length mcl defined measure size smallest chromosome polynomial time computable representation evaluation function 
ga complexity class class problems take polynomial time solve defined problems linear mcl growth rate 
ga reduction defined mcl preserving translation problem representation 
problem ga hard problems particular class reduce 
theory important limitations 
attempts capture notion problem hard class problems class reduced 
analyzing ga hardness re ally concerned running performance ga problem 
concept hardness implies poor runtime performance reducibility problems 
second mcl hard compute 
third complexity classes defined mcl different complexity classes traditional complexity theory 
emphasizes representation mcl growth rate role making problem hard gas 
application mcl problems ear mcl growth rates hard gas 
np complete problems belong type solved ga easily converge polynomial time 
theory handle case 
general approach hard apply practice refinement needed 
related ga hardness research early goldberg gol suggested factors problems hard gas isolation noise multimodality crosstalk 
isolation noise considered randomness deception crosstalk mismatch 
multimodality independent factor 
gol goldberg built new model problem difficulty gas consists core difficulties noise scaling deception 
respectively correspond randomness mismatch deception model 
ga divided problem space model random problems straightforward problems deceptive problems mutual informa tion problem ga culberson cl cul studied model adversarial approach algorithm passes strings sary gets back evaluation 
indifferent adversary model corresponds random problems model 
similarly friendly adversary corresponds straightforward problems vicious adversary corresponds tive problems 
rice theorem show impossibility having predictive ga hardness measure description problem ga configurations 
reeves wright rw arrived concepts experimental design ed 
result verified failures previous efforts identifying indicator 
proposed properly classify gas parameter values study dynamics class separately 
de jong holds similar viewpoint 
dsg says 
uncomfortable notion ga hard problem independent details mean phase choice ga properties representations operators exist problem easy solve 

concluded necessary apply empirical methods study ga dynamics ga hardness 
coincides researchers point view 
reeves wright rw propose view gas form sequential experimental design 
goldberg gol committed engineer methodology wright decomposition method adopt design competent gas design ga airplane engineered object thing theoretical model 
alternative direction experimental approaches driven infeasibility analytical approaches turn propose inductive approach study problem hardness algorithm performance 
directions ga hardness research suggest research focus real world np complete optimization problems man functions 
suggest research study classification various gas considering 
furthermore research give seeking priori ga hardness measures descriptive information problem ga favor experimental algorithmic methods learn predictive model posterior results running various gas problem instances designed parameter settings 
experimental approach may contain steps 
pick real world optimization problem example tsp 

identify list problem characteristics may affect ga performance 

generate random test instances different parameter settings 

select list gas different configurations 

run gas designed instances collect perfor mance data 

apply proper data analysis tools machine learning methods build pre model experimental data 

new instance analyze characteristic learned model infer best ga optimize 
similarly solving automatic algorithm selection problem propose experimental procedure 

identify list feasible instance characteristics domain knowledge 

identify list candidate algorithms solving problem 

generate representative set test instances different characteristic value settings uniformly random 

run candidate algorithms designed instances col lect performance data 

apply bayesian network learning techniques induce predictive model bayesian network experimental data 

new instance analyze characteristic learned bayesian network infer appropriate algorithm solve 
summary chapter studies theoretical aspects automatic algorithm selection 
main include algorithm selection problem general algorithm selection search 
developed general framework instance hardness algorithm performance applied discuss misconceptions ga hardness research 
propose feasible inductive approach ga hardness research automatic gorithm selection systems 
inductive approach mainly relies experimental approaches machine learning techniques 
chapter multifractal properties joint probability distribution bayesian networks chapter report discovery multifractal properties joint probability distributions bayesian networks 
sufficient asymmetry individual prior conditional probability distributions jpd highly skewed shown druzdzel stochastically self similar clusters high probability instantiations scales 
discovered multifractal property developed tested phase hybrid random sampling search algorithm mpe problem 
experimental results showed multifractal property provides meta heuristic solving mpe problem 
mpe problem np complete multifractal meta heuristic solve hard optimization problems 
multifractal properties strengthen connections bayesian networks thermodynamics 
connections exploited popular bayesian network inference algorithms models statistical physics pa free energy minimization 
motivation bayesian networks bns pea provide compact representation jpd uncertain domain specifying jpd product local prior con ditional probability distributions 
jpd variables seen created multiplicative process combining prior conditional probabilities individual variables 
applying central limit theorem druzdzel demonstrated 
asymmetries individual distributions result exhibiting orders magnitude differences probabilities various states model 
particular usually small fraction states cover large portion total probability space 
druzdzel 
druzdzel result suggests considering small number probable states lead approximations belief updating 
questions interest find high probability instantiations space 
internal structure jpd facilitate search 
characterize 
attempts answer questions demonstrating jpd bn random multinomial multifractal created random multinomial multiplicative cascade 
applying multifractal analysis show existence multifractal structure jpd 
specifically jpd multifractal measure partitioned fractal subsets subset supports monofractal measure jpd consists clusters high probability tions scales 
multifractal properties designed tested new random sampling search algorithm finding mpe 
multifractal analysis fractals multifractals fractals extremely irregular self similar sets man em 
fractal charac fractal dimension 
example dimension irregular may greater indicating simply line cantor set space filling characteristics plane 
cantor set man oldest simplest man fractal 
shown constructed removing middle third unit interval remaining subintervals middle third removed 
continues infinitely 
formally cantor set defined follows kn kn kn kn 
dimension fractal set calculated counting number covers required cover set interest 
cantor set box length needed cover boxes needed number boxes length required cover consider mathematical constructs 
pointed dr rudolf riedi fractal mosaics long time 
minkowski fractal dimension defined log log log log log main difference fractal multifractal refers set refers measure 
measure assigns quantity member set measure support set defines distribution quantity support set 
multifractal analysis man em har related study distribution physical quantities geometric support set 
support may line plane fractal 
multifractal measures highly irregular self similar exactly stochastically 
instance distribution gold geographical map usa irregular 
high concentrations places lower concentrations places low concentrations 
description holds scales scale country state scale meters microscopic scale 
quantities exhibit behavior irregularity scales statistically man 
call kind self similar measure multifractal 
concept multifractal originally introduced mandelbrot discussions turbulence man applied contexts diffusion limited aggregation dla pattern bh earthquake distribution analysis har internet data traffic modelling rv 
multifractal generated elementary iterative scheme called multiplicative cascade 
multifractal spectrum characterize multifractal measure 
clearly need just fractal dimension 
simply counting boxes cantor set counting coins caring denomination 
find description assigns measure box weight em 
example cantor measure rv illustrate basic characterization multifractal 
consider cantor set 
extend allocating mass generating cantor measure probability subinterval division 
example allocate existing probability interval divided right hand subinterval left hand 
steps generating cantor measure shown 
step multifractal analysis define coarse lder exponent man em har logarithm measure box divided logarithm size box 
log box log multiplicative construction clear probability sequence intervals decay exponentially fast interval divided shrinks point 
thought local degree differentiability measure rate local probability change har strength singularity rie 
defined draw frequency distribution follows value count number boxes having coarse lder exponent equal define logarithm divided logarithm size box 
log log loosely interpreted approximation minkowski fractal dimen sion subsets boxes size having coarse lder exponent 
function lim called multifractal spectrum 
characterizes tal 
graph called curve man har shaped symbol usually leaning side 
usually bounds min max min max 
value peak called 
plots curve cantor measure 
preceding discussion see basic idea multifractal analysis classify singularities measure strength 
strength denoted singularity exponent coarse lder exponent 
points equal strength lie interwoven fractal subsets 
fractal subsets monofractal fractal dimension 
reasons term multifractal 
binomial multifractal cascade multifractal measures generated elementary iterative pro cedure called multiplicative cascade 
binomial measure simplest multiplicatively generated multifractal measure 
positive num bers adding 
stage cascade start construction uniform measure 
step measure uniformly spreads mass probability equal subinterval mass equal subinterval 
step split subintervals respectively receive fraction total mass 
applying procedure obtain curve cantor measure iteration procedure generates infinite sequence measures 
step assume measure defined defined follows consider arbitrary interval dyadic number form 
counting base 
uniformly spread fraction mass subinterval 
repetition scheme subintervals determines 
measure defined 
shows measure binomial multiplicative process 
construction binomial multifractal extended ways 
stage cascade intervals divided intervals equal size 
defines class multinomial multifractals 
second allocation mass subintervals step cascade randomized binomial measure random variable multiplier 
defines random multifractals 
multipliers need discrete shall discrete ones simplicity 
probabilistic roots multifractals multifractal measures generated mapped multiplicative cascade coarse lder exponent expressed sum random variables definition man 
behavior sums random variables central topic probability theory 
theorems dealing sums law large numbers lln central limit theorem clt large deviation theorem ldt 
lln says surely probability sample average converge expectation increases infinity 
lln guarantees existence role probable lder exponent 
lln holds limit dealing finite number multiplicative steps deviation expected value important finite relevant information yielded clt far important ldt 
clt concerned small fluctuations expected value 
context shows appearance quadratic maximum binomial measure coincidence 
consider random variable finite expectation ex ex 
large deviation theory concerned large fluctuations expected value behavior lim xh ex function lln tells lim xh ex quantity vanishes speed 
expect kh xh ex converge increases infinity 
question fast vanish 
ldt states converges exponentially fast 
section omit details generally speaking deduced large deviation theory provides probabilistic basis multifractals em kes 
furthermore large deviation theory continuous unbounded cases exists providing full justification called thermodynamic formalism multifractals 
refer reader details em man har kes 
thermodynamics formalism multifractals way get multifractal spectrum 
alter native method method moments define partition func tion analogous partition function thermodynamics statistic physics em man zq denote number boxes coarse lder exponents satisfied 
contribution subset boxes 
integrating obtain follows zq zq keeping dominant contribution equation introducing partition function scale zq easy see dq means computed vice versa 
relation called legendre transform man 
interesting consequence flexibly rich thermodynamic content hidden concept multifractals 
preceding discussion easily draw correspondence thermodynamic partition function temperature inverse energy entropy gibbs free energy information topic interested reader referred man 
bayesian networks random multinomial multifractals bayesian network pea directed acyclic graph dag nodes represent random variables arcs represent conditional dependence relationships variables 
node xi conditional probability table cpt contains probabilities variable value values parent nodes denoted xi 
bn represents exponentially sized joint probability distribution jpd compact manner 
entry instantiation nodes jpd computed information bn chain rule 
xn xi xi multifractal viewpoint jpd defined bn nodes measure belief distributed dimensional space random events 
topological ordering nodes map dimension space linear interval assigning event integer number address interval 
example linear interval node binary bn 
jpd bn considered generated multiplicative cascade number steps equals number nodes 
step cascade intervals divided subintervals number states current node multiplier allocating probability random variable defined cpt current node 
easy see general case bn corresponds multifractal generated random multinomial multiplicative cascade 
simplest multifractal binomial measure corresponds simplest bn binary bn links 
consider node binary bn node prior probability distribution 
cascade contains steps generates jpd instantiations shown 
simplest multifractal binomial measure 
consider process agent incremental understanding uncertain domain multiplicative cascade process 
agent identifies random variables 
knows causal rela tionships variables assume uniform distribution spreading belief evenly states 
agent belief redistributed learns domain connections nodes cpt values 
process belief redistribution typical multiplicative cascade process similar multiplicative cascade context multifractals 
example turbulence cas cade model describes nature energy dissipation turbulent fluid flow 
turbulence energy introduced system large scale storms stir jpd simplest bn nodes ring bowl water dissipated form heat small scales effect velocity friction particles important 
cascade models assume energy dissipated sequence eddies decreasing size reaches sufficiently small eddies energy dissipated heat 
case bayesian networks belief introduced domain high level uniform distribution 
learn cpts capture increas ingly refined causal structure domain 
substructures keep redistributing belief learn domain 
case study asia alarm section study multifractal properties joint probability distribu tions example networks asia alarm 
steps multiplicative cascade asia network self similarity jpd asia asia network nea demonstrate process agent incre mental understanding uncertain domain binomial multiplicative cascade process show jpd self similarity property 
asia small binary bn containing nodes jpd total instantiations 
shows changing jpd steps cascade process asia network cpts node smoking tuberculosis learned sequentially 
clearly see belief redistributed knowledge domain learned 
final jpd nodes cpts learned second half final jpd 
comparing figures see stochastic self similarity properties jpd 
jpd asia network second half jpd illustrating self similarity property number instantiations order probability sum instantiations order skewed jpd alarm analyze jpd alarm subset alarm network demonstrate multifractal characteristic clustering prop erty 
alarm network analyzed 
contains variables resulting non zero states 
probabilities states spread orders magnitude 
shows histograms number distributed order contribution total probability space 
axis negative order magnitude figures 
shows histogram number instantiations order appears normal distribution 
logarithmic scale axis shows curve alarm jpd actual distribution lognormal 
peak order contains instantiations contribution total probability space 
see high probability instantiations number dominate joint probability space 
instantiations probability probabilities total probability probabilities total probability probabilities total probability 
instantiations total covers total probability space 
highly skewed result ana druzdzel 
show multifractal structure jpd way instantiations different orders magnitudes fill space 
multifractal spectrum alarm applying multifractal analysis alarm jpd get multifractal spectrum curve shown 
axis coarse lder exponent axis fractal dimension subset instantiations 
curve confirms jpd bayesian network mul 
describes instantiations fill probability space point view fractal dimension 
see high probability instantiations corresponding small low dimension means fill probability space sparse way clusters high probability instantiations 
peak fractal dimension corresponding coarse lder exponent 
definition corresponds instantiations probability order instantiations peak 
means ations fill probability space dense way space 
instantiations low probabilities low dimensions 
means low probability instantiations rare events distribute sparsely clusters expected 
yields mathematical description inner structure jpd clusters high probability instantiations low probability instantiations instantiations middle distributed 
interestingly pattern coincides way people live real world high income people tend live community low income people middle class located 
quantifying clustering property show clustering property clearly draw distribution high probability instantiations distribution low probability instantiations 
contains instantiations probability higher axis address instantiations ranging axis actual probability value 
contains instantiations lower axis address instantiations axis just series number instantiation note different actual values small drawn neatly 
see clearly clusters clusters high probability instantiations clusters low probability instantiations graphs 
having shown clustering property thing want quantify property 
hamming distance bit string representations instantiations measure far located 
represented bit string 
bn number variables domain bi state index variable 
example hamming distance instantiation 
high probability instantiations small hamming distances clus ter clustering property 
draw averaging hamming distance graph instantiation 
axis negative order magnitudes axis instantiation instantiations order magnitude 
see instantiations lower probabilities larger hamming distance instantiation locate far away instantiation 
draw lowest instantiations 
put graphs instantiations orders provide global picture 
consists segments curves corresponding orders magnitudes 
curve consists points represents graph randomly picked instantiation order 
example segment segment 
see instantiations middle order magnitudes located distance orders places 
finding supports previous analysis expected distribution pattern 
multifractal search algorithm finding mpe jpd multifractal property meta heuristic develop new search algorithms finding mpe 
search space multifractal bad solutions clustered 
search divided phases identify communities localize search regions 
quality community evaluated current search point order direct search 
point better point neighbors better move look global optimal 
helps searcher escape local optimal simple hill climbing gets stuck 
consider scenario suppose asked find best house city 
drive identify areas look 
intensively search area 
neighborhoods get better chance hitting best house gets higher 
average hamming distance graphs input bn evidence set output complete assignment 
un 
step sampling algorithm generate set initial points step point start hill climbing search neighborhood quality evaluation function put local optima step point start normal hill climbing search return best solution far mpe 
hybrid random sampling search algorithm finding mpe developed phase sampling search algorithm solve mpe problem meta heuristics 
algorithm designed find probable explanation complete assignment observed evidence 
general mpe problem np complete shi hard approximate ah 
phase algorithm forward sampling feasible method quickly identify set communities 
second phase hill climbing search neighborhood quality evaluation function started community 
additional repair phase added set elite solutions set worst solutions collected search process 
refine final solutions flipping variable values agree majority elite solutions agree worst solutions 
rule satisfied returns best solution far mpe 
algorithm performance determined factors reliably sam pling algorithm brings searcher places close global optimal strength neighborhood quality evaluation function bring searcher near optimal place global optimal 
neighborhood quality search point defined sum likelihoods nearest neighbors 
approximated randomly drawing samples neighbors 
sampling radius set small positive value expected skewness cpts influence perfor mance algorithm 
experiments randomly generated groups networks different degrees cpt skewness test algorithm skewed normal 
skewness cpts computed follows jn 
vector column cpt table 
vm conditional probabilities skew mi vi skewness cpt node average skewness columns 
skewness network average skewness nodes 
skewness groups networks set respectively 
group consisted networks binary nodes 
number nodes number edges 
networks set sparse exact computed 
randomly generated evi dence values network size search space hugin compute exact 
group networks counted number times exact 
computed average relative error ratio absolute error exact mpe value recorded average ham ming distance returned mpe exact mpe 
table summarizes experimental results 
results see normally skewed networks easiest ones algorithm networks hardest 
normally skewed networks able find exact mpe missed close global optimal bits difference 
networks able find exact mpe 
average error average hamming distance returned mpe exact mpe largest ones 
results imply network distributions nearly uniform finding mpe hard search space table results randomly generated networks solved error exact skewed normal flat 
hand highly skewed bring trouble search algorithm attractiveness steep local 
summary demonstrated underlying bayesian networks created random multiplicative cascade processes 
jpd orders magnitude differences probabilities various instantiations highly skewed stochastically self similar exhibits clustering properties 
multifractal spectrum jpd describes instantiations different orders fill joint distribution space different fractal dimensions 
particular high low probability instantiations tend form clusters joint distribu tion space 
discussed model result hold self contained parts 
hypothesize holds dynamic models 
curve show long random multiplicative cascade process involved 
significance analysis provides important information characteristics joint probability distribution 
particularly clustering property useful meta heuristic searching mpe 
research bridges analytical gap multifractal bns sug interesting research directions 
seen multifractals deep probabilistic root rich thermodynamic content 
fact bn multifractal draws attention connection thermodynamics bns pa 
mpe problem np complete expect observe multifractal structure solution space hard combinational problems maxsat tsp 
applying multifractal meta heuristic solve problems interesting topic investigate 
jpd multifractal property provides potential instance hardness mea sure algorithm selection mpe problem 
possible direction study multifractal property bayesian network jpd various mpe algorithms relative performance 
multifractal property differentiate mpe algorithm space try developing efficiently computable measure characterize predictive measure corresponding mpe algorithm performance 
chapter machine learning techniques chapter review machine learning techniques construct algorithm selection systems sorting mpe problem 
cludes data preprocessing methods feature selection discretization ma chine learning algorithms decision tree learning naive bayes classifier bayesian network learning basic methods evaluate learned models cross validation confusion matrix meta learning schemes bag ging boosting stacking 
give overview process applying machine learning techniques solve algorithm selection problem 
machine learning data mining goal experimentally collect training data problem characteristics algorithm performance induce predictive algorithm selection model training data 
typical task machine learning mit data mining wf 
goal machine learning build computer programs improve auto matically experience 
dating mining automatically analyzing data discovering valuable implicit patterns regularities 
example data mining application process discovering customer loyalty pattern database customers choices customers profiles 
data mining involves machine learning practical way 
machine learning algorithms proven great practical value data mining 
precisely define machine learning follows mit definition machine learning computer program said learn experience respect class tasks performance perfor mance tasks measured improves experience machine learning supervised unsupervised 
supervised learning specified set classes example experience labelled appropriate class 
goal generalize examples identify class new example belong 
task called classification 
contrast supervised learning unsupervised learning 
goal decide examples grouped learner classes 
usually called clustering 
thesis concerned supervised learning 
example weather problem typical supervised machine learning problem experience data represented set training examples instances 
example described fixed number features attributes 
features typically take types values nominal numeric 
features special serves label denoting class example 
task classify examples 
performance simply classification accuracy 
usually set examples called testing examples 
training examples produce learned model testing examples evaluate classification accuracy 
testing class labels 
algorithm takes input test example returns output class label example 
look simple example weather problem 
dataset shown table 
contains examples describing weather days 
example table weather problem instance features class outlook temperature humidity play sunny false sunny true overcast false rainy false rainy false rainy true overcast true sunny false sunny false rainy false sunny true overcast true overcast false rainy true attributes outlook temperature humidity play 
play class attribute denoting day sports 
task learn model data model predict day day sports 
learning problem automatic algorithm selection algorithm selection problem task learn predict best algorithm arbitrary instance features 
target function want learn discrete valued function maps problem instance characteristic vector best algorithm 
experience data collected algorithmic experiments 
performance accuracy algo rithm selection 
state algorithm selection learning problem follows algorithm selection learning problem task selecting best algorithm performance 
measure percent correct algorithm selection predictions 
training experience results collected algorithmic experiments 
target function best algorithm discrete values kind task referred classification problem 
task problems classify examples category discrete set possible categories 
research set categories set candidate algorithms 
perspective machine learning viewing learning process search ing large space possible hypotheses order determine best fits available training data prior knowledge held learner mit 
perspective hypotheses space consists possible depen dency relationships problem characteristics algorithm performance 
hypothesis space defined representation linear func tions logical descriptions decision trees artificial neural networks 
hypothesis representations corresponding learning algorithms takes advantage different underlying structure search space 
thesis mainly consider representations corresponding learning algorithms decision tree qui mit naive bayes classifier mit jl bayesian networks pea nea rn 
machine learning algorithms decision tree learning decision tree learning popular inductive learning methods 
applied broad range tasks medical diagnosis credit risk assessment 
decision tree learning learned function represented decision tree 
decision trees essentially sets rules 
classify training examples sorting tree root leaf node id examples target attributes create root node examples target value give root label attributes empty label root common value calculate information gain attribute average entropy formula select attribute lowest average entropy highest information gain attribute tested root possible value attribute add new branch root corresponding examples examples examples empty new branch leaf node labelled common value examples new branch tree created id examples target attributes id algorithm decision tree learning provides classification example 
node tree represents test attribute training example branch corresponds possible values source node attribute 
language logic decision trees represent disjunction conjunctions constraints attribute values training examples 
path root node leaf corresponds conjunction attribute tests 
tree disjunction conjunctions 
decision tree learning algorithms id id successor important algorithms learning decision tree data 
id developed quinlan qui 
extended qui 
basic algorithm id learns decision tree constructing top 
node question asked attribute tested question answered statistical test determine attribute classifies training examples 
best attribute selected test current node tree 
descendant node created possible value attribute 
training examples sorted appropriate descendant node branch corresponding example value attribute 
entire process repeated training examples associated descendant node select best attribute test point tree 
attributes incorporated higher tree excluded 
attribute appear path tree 
process continues new leaf node attribute included path tree training examples associated leaf node target attribute values belong classification 
see id essentially top greedy algorithm searching space possible decision trees looks back reconsider previous choices 
important issue id algorithm select attribute test node tree 
intuitively want select attribute useful classifying examples point 
order measure statistical property need define information gain measures particular attribute classifies training examples target attribute values 
defining entropy order define information gain precisely 
definition entropy suppose collection training examples 
ex ample attributes called target attribute take different values 
entropy relative wise classification define entropy pi log pi information gain measure effectiveness attribute classifying training examples simply reduction entropy caused partitioning examples attribute 
definition information gain suppose collection examples attribute examples 
information gain gain attribute relative defined inductive bias id gain entropy sv entropy sv collection training examples typically decision trees consistent id simple complex greedy search strategy id chooses acceptable tree sees search process 
say id inductive biases favor shorter trees longer ones favor trees place attributes high information gain closest root 
id inductive bias partly justified famous occam razor states entities multiplied unnecessarily 
language machine learning means prefer simplest hypothesis fits data 
argument follows fewer short hypotheses long ones short hypothesis coincidentally fits data 
debates arguments remain unsolved today 
extensions variety extensions basic id algorithm developed result ing algorithm called qui 
extensions wf include incorporating numeric attributes handling missing values avoiding overfitting rule post pruning 
numeric attributes real world applications provide numeric data 
discretizes numeric attribute picking threshold produces greatest information gain 
procedure binary splits follows sort examples numeric attribute identify adjacent examples differ target classification generate set candidate threshold corresponding values shown fay value threshold maximizes information gain lie boundary 
candidate thresholds evaluated comparing information gain associated best discretize binary value 
extensions numeric attributes multiple intervals binary discussed fi 
missing values dataset may contain missing values certain attributes 
strategy deal issue assign common value current node 
complex procedure mit wf assign probability possible values probabilities estimated observed frequencies values current node 
example missing values converted fractional examples compute information gain 
pruning cases noise data number examples small representative sample true target function id produce trees overfit training examples 
overfitting defined follows definition overfit hypothesis said overfit training data exists hypothesis smaller error training data smaller error entire test data rule post pruning mit technique avoid overfitting data 
involves steps 
infer decision tree training data allowing overfitting 

convert learned tree set rules 
prune rule removing preconditions result improving estimated accuracy 

sort pruned rules estimated accuracy consider se quence classifying subsequent instances 
decision tree weather problem applying weather dataset applying weather dataset table get decision tree shown 
size tree number leaves 
represented disjunction conjunctions follows outlook sunny humidity outlook overcast outlook rain indy false learned decision tree classify example sunny alse initially involves examining feature root tree outlook 
value sunny left branch followed 
examine value humidity left branch followed value 
brings leaf node instance classified 
naive bayes classifier naive bayes classifier simple highly practical bayesian learning method mit rn 
shown times naive bayes classifier rivals outperforms sophisticated classification algorithms datasets 
bayes theorem provides way calculate posterior probability prior probability 
theorem bayes theorem suppose set training examples instance described conjunction attribute values 

target function take value finite set learning task determine target value new instance 

usually bayes method classifies new instance assigning probable target values vmap attribute values 

vmap argmax vj 
vj applying bayes theorem rewritten vmap argmax 
vj vj 
argmax 
vj vj vj want estimate terms equation training data easy calculate vj simply counting frequency target value occurs estimating 
vj infeasible huge set training examples 
probability small need large training dataset order obtain reliable estimates 
naive classifier computes term simplifying assumption attribute values ai independent target value vj 
probability observing conjunction 
just product probabilities individual attributes 
vj ai vj substituting equation equation naive bayes classifier follows argmax vj ai vj vj denotes target value output naive bayes classifier 
see naive bayes classifier greatly simplifies learning features independent class 
assumes hidden attributes influence learning 
advantages naive bayes classifier include simplicity clear semantics impressive performance practice 
main weakness comes strong assumption feature independence 
point needs mentioned learning algorithm explicitly search space hypotheses 
hypothesis computed simply counting frequency various data combinations training examples 
missing values numeric attributes naive bayes classifier learning missing values cause problem 
value missing simply included frequency counting 
numeric values handled jl wf assuming generated gaussian distribution ai vj 
numeric attribute ai class vj mean standard deviation calculated follows ai ai number examples class vj 
ai vj simply estimated gaussian distribution ai vj vj calculated way nominal attributes frequency counting 
normal distribution assumption just described restriction naive bayes method 
practice know distribution numeric value standard estimation procedures distribution gaussian distribution 
strategy deal numeric values simply discretize data learning 
applying naive bayes weather dataset applying naive bayes learning weather data learn simple bayes classifier shown 
root node play prior distribution 
outlook conditional distribution temperature gaussian distribution humidity gaussian distribution distribution 
outlook conditional distribution temperature gaussian distribution humidity gaussian distribution distribution 
naive bayes classifier classify new instance sunny false bayes rule applied compute map play computing lay sunny alse pick value larger proba bility class instance 
example returned posterior marginal probabilities play label instance 
naive bayes classifier weather problem bayesian network learning learning bayesian network data ch hec rn mit search network high posterior probability training data 
learning algorithm takes input training data domain knowledge outputs network structure cpts 
practice bayesian network learning problem varieties 
structure network known unknown variables network observed hidden hec rn 
research aims learn network contains dependency relationships tween instance features algorithm performance 
partial knowledge network structure nodes representing instances features parents nodes represent algorithms 
assume hidden variables 
assumption reasonable practice domain knowledge select algorithms corresponding problem features 
difficulty learning hidden variables unknown structure general algorithms known kind problem 
bayesian network learning algorithm search scoring algorithm bayesian score developed ch 
algorithm searches spaces possible structures looking structure best fits data scoring criteria 
search space usually huge application heuristic search justifiable 
bayesian score stated theorem 
theorem bayesian score suppose set random variables 
variable xi ri possible value assignments vi vi 

training dataset cases examples 
case contains value assignment variable bs denote bayesian network structure containing just variables variable xi bs set parents wij denotes jth unique instantiation relative qi unique instantiation nijk number cases variable xi value vik instantiated wij 
nij ri nijk 
bs bs qi ri 
ri nij ri 
nijk 
result find probable network structure training dataset 
space possible structures exponentially huge heuristics search efficient 
uses greedy heuristic search best parents set variable 
assumes uniform prior distribution structures 
assumes ordering available variables xi parent xj xi precedes xj 
algorithm described 
parameter values cpt probabilities calculated formula 
ijk bs nijk nij ri procedure true node red xi maximizes pnew pnew pnew false write node parents nodes algorithm algorithm takes input set nodes ordering nodes upper bound number parents node may database training examples 
variable assumes node parents adds parent set new node incrementally predecessors ordering added parent node increases probability resulting structure largest amount sense uses greedy search strategy 
stops adding parents improvement got parents 
outputs parents node 
discretize weather data run 
learned bayesian network shown 
order outlook temperature humidity play 
classification process bayesian networks naive bayes classifier computing posterior marginal probabilities class node value maximum probability 
bayesian network weather problem learned bayesian networks naive bayes classifier recall naive bayesian classifier independency assumption attributes 
conditionally independent target value assumption dramatically reduces complexity learning target function 
cases assumption overly restrictive 
contrast naive bayesian classifier bayesian networks constraining allowing conditional independence assumptions apply subsets variables variables 
provides intermediate approach restrictive comparing naive bayesian classifier tractable dealing exponentially sized joint probability space directly 
evaluation learning algorithms evaluating learning algorithm important part machine learning wf 
evaluation single classifier common criterion classification accuracy 
stratified fold cross validation standard method practice com pute classification accuracy 
common way confusion matrix kappa statistic describe disagreement actual classes predicted classes learned classifier 
classifier predict classifica tions probability weights root mean square error rmse applied 
comparing classifiers test data paired test compare average error rate cross validations 
classification accuracy error rate classification accuracy simply defined percentage test examples correctly classified learning algorithm 
error rate minus classification accu racy 
strictly speaking measure just sample error impossible get exact true error 
true error error rate entire unknown distribution examples 
sample error error rate sample data available 
fortunately sample error shown estimate true error mit 
specifically suppose want estimate true error hypothesis sample error errors measured sample examples 
suppose conditions samples drawn independently independent probability distribution commits errors examples errors true conditions statistic theory tells assertions information probable value true error sample error errors 
approximately probability true error lies interval errors errors errors data available just training set learn model different test set estimate classification accuracy supposing representative drawn distribution 
data scarce expensive practice 
amount data limited common performance evaluation method repeated cross validation 
idea partition data training test sets different ways 
learning algorithm trained tested partition classification accuracy averaged 
provides reliable estimate true classification accuracy 
stratified fold cross validation practice standard way evaluating classification accuracy called strat ified fold cross validation wf training data randomly divided mutually exclusive subsets approximately equal size 
subset class represented approximately proportions data set 
learning algorithm executed learned model tested times 
iteration subset held test set remaining subsets training 
estimates averaged yield classification accuracy 
times single stratified fold cross validation produce reliable estimate typically run cross validation times average results 
research fold cross validations executing learning algorithm times data sets tenth size original data 
confusion matrix kappa statistic possible method evaluating classification experiments count number correctly wrongly classified data 
gives rough impression classification order get better interpretation result useful know classes data misplaced 
confusion matrix matrix containing information actual pre classes kp convenient tool counting 
confusion matrix columns represent predicted classes piece data belongs column classified belonging class 
rows represent actual classes piece data represented particular row belongs corresponding class 
perfect classification results matrix diagonal 
cell diagonal high count signifies class row somewhat confused class column classification system 
confusion matrix fixed calculate kappa statistic kappa coefficient quantify agreement actual predicted classifications coh kra 
kappa statistic measures proportion agree ment chance agreements samples accidentally classified correctly removed considerations 
kappa statistic defined follows accuracy observed agreement estimate chance agreement 
computed equations ni ni mi total sum numbers matrix numbers diagonal mi marginal total row marginal total column increases agreement chance decreases negative chance agreement occurs 
agreement equals chance agreement 
evaluating predicted probabilities rmse classifiers predict classification provide probability weight class label 
type classifier calculates probability vector 
pk classes pi 
true classification represented vector 
ak ak 
case rmse frequently measure difference true classification estimated class probabilities cross validations 
ki pi ai rmse comparing classifiers paired test times want compare learning algorithms say naive bayes problem see better 
apply algorithm training data cross validation splits algorithm 
get pair results test data 
statistic method called paired test wf compare average error rate results 
paired sets xi yi result values paired test determines differ significant way 
xi ni yi xi xi yi yi 
defined ni xi yi statistic degrees freedom 
table student distribution confidence intervals determine significance level distributions differ 
table confidence limits table student distribution degrees freedom practice confidence level 
value evaluation scheme number experiments 
table confidence limits student distribution degrees freedom confidence level greater table conclude significant difference learning algorithms dataset 
easy see better comparing error rates 
evaluation learning weather problem learned decision tree weather data shown fig ure 
list evaluation result table 
table evaluation result weather data stratified fold cross validation evaluation summary total instances correctly classified instances classification accuracy error rate kappa statistic root mean squared error confusion matrix classified data preprocessing introduced models learning algorithms decision tree learning naive bayes classifier bayesian network learning 
applying machine learning algorithms practical data mining problems impor tant processes improve performance learning 
introduce important data preprocessing methods feature attribute selection discretization 
feature selection practical situations features attributes infinitely theory learning algorithms handle 
irrelevant redundant 
practice data preprocessed select relevant attributes learning 
called feature selection hal 
best way select relevant attributes manually understanding problem meaning attributes 
way combine domain expert knowl edge learning 
automatic methods useful especially human expert fails precise choice closely related attributes 
feature selection model compact speeds learning process outweighed computation feature selection 
important feature selection approaches filter method wrapper method kj 
evaluates worth feature subsets general characteristics data 
wraps machine learning algorithm ultimately feature selection process uses evaluate feature subsets 
methods involve searching space combinations features subset predict class best 
categories algorithms differentiated evaluation functions space feature subsets explored 
search space exponential number features 
heuristic searches 
example greedy search genetic algorithm common search strategies feature selection 
filters wrappers advantages disadvantages 
wrappers give better results terms final predictive accuracy learning algo rithm feature selection optimized particular learning algorithm 
wrappers expensive run intractable datasets features 
wrappers general tightly related learning algorithm 
wrappers rerun switching new learning algorithm 
filters faster wrappers independent learning algorithms 
thesis ga wrapper wf kj hsu perform feature selection learning 
compare model learned feature selection see feature selection improve learning 
tested weather dataset ga wrapper evaluator 
result shows feature selection able identify temperature irrelevant attribute reduces number features 
discretization numeric attributes discretization procedure takes data set converts continuous tributes nominal 
necessary preprocessing step learning algorithms require nominal attributes 
research shows common machine learning algorithms benefit treating features uniform fashion 
case discretization useful 
equal width equal frequency intervals simplest straightforward discretization method predetermined equal width intervals 
involves sorting values continuous feature ing range values equally sized bins 
done help domain knowledge time data collected 
runs risk table discretized weather data instance features class outlook temperature humidity play sunny inf false sunny true overcast inf false rainy inf false rainy false rainy inf true overcast inf inf true sunny inf false sunny false rainy false sunny true overcast true overcast false rainy true producing bad choices boundaries values evenly distributed 
improvement equal width equal frequency intervals 
sorts values feature divides ranges predetermined number bins assigns values bin 
equal width equal frequency binning unsupervised discretization methods class determining interval bound aries 
supervised methods advantages classes account discretization process produce better intervals 
entropy discretization mdl stopping rule practice best general techniques supervised discretization entropy method mdl stopping rules developed fayyad irani fi 
method uses class information entropy candidate partitions se lect bin boundaries 
idea similar process splitting numeric attribute learning decision tree 
suppose set instances continuous feature discretized partition boundary partitions subsets 
classes 
ck ci proportion examples class ci 
class entropy defined ci log ci 
class information entropy partition introduced 
feature boundary tmin minimizes entropy possible partitions selected binary discretization boundary 
procedure applied recursively subsets introduced tmin time creating multiple intervals feature minimal description length principle determine stopping cri 
recursive discretization process stops information gain smaller threshold gain log log ke number examples ki number classes si gain applying algorithm weather data get discretized data set shown table 
note temperature humidity discretized intervals 
meta learning combining multiple models data preprocessing methods engineer input data improve perfor mance learning 
contrast methods engineer output ma chine learning algorithms 
section introduce meta learning schemes bagging boosting stacking combine multiple learned models form stronger model 
bagging bagging bre wf stands bootstrap aggregating 
basic idea bagging generate multiple training datasets original dataset bootstrap sampling induce model dataset 
classifying new instance learned models vote final classification 
class receives votes returned predicted class 
bagging multiple models learned parallel manner 
algorithm described 
boosting bagging training examples int test instance learning sample instances di replacement apply learning algorithm di save learned model mi classification predict class instance mi return class predicted models 
algorithm bagging boosting fs wf adopts idea voting combine multiple models learns multiple models iteratively sequential manner 
learned models boosting complement predicting class particular subset examples 
difference boosting uses weighted vote assigns weight successful models 
boosting instance weight 
initially training instances assigned equal weights 
learning algorithm applied classifier generated 
instance output learned classifier 
weight correctly classified instances decreased misclassified ones increased 
iteration classifier built data 
consequently focuses classifying misclassified instances correctly 
iteration keeps going stopping criterion satisfied 
process generate series classifiers complementing 
weights update instances iteration determined error rate learned classifier iteration stops 
forming predictions outputs classifiers combined weighted vote 
weight classifier determined performs training dataset built 
calculated formula 
log weights classifiers vote particular class summed class greatest total weight returned predicted class 
algorithm described 
stacking bagging boosting multiple models combined type 
stacking wol wf able combine models built different learning algorithms 
example suppose learned decision tree naive bayes classifier bayesian network data 
stacking combine form new classifier 
stacking introducing meta learner learn classifier reliable decide best combine predictions classifiers 
levels boosting training examples test instance learning assign equal weight training example false apply learning algorithm di save learned model mi compute save error rate true training example di instance correctly classified multiply weight normalize weights classification assign weight zero classes learned model mi add log predicted class return class greatest weight 
algorithm boosting learning stacking 
level learners base learners trained stand original training data 
level learner meta learner needs different level training data reflects performance level learners 
done letting level learned model classify instance attaching predictions actual class value 
usually portion original training data reserved generating level training data order reduce biases 
done level level learner usually simple classifier linear model 
algorithm outline stacking described 
stacking training examples test instance learning hold subset training example level learning apply learning algorithms remaining data save learned models mi models classify generate level training data apply simple linear regression produce level classifier classification apply mi classify test instance result form level instance apply level model decide best class return best class 
algorithm stacking overview learning process algorithm selection working flow machine learning approach algorithm selection con sists procedures discussed far 
specifically instance generation data collection data preprocessing discretization feature selection learning multiple models combination bagging boosting stacking model evaluations 
gives overview process 
chapters apply process algorithm selection sorting mpe problem 
overview machine learning approach algorithm selection chapter algorithm selection sorting chapter apply machine learning approach selection sorting algorithm 
specifically examine relationships important presortedness measures inv run rem performance known sorting algorithms insertion sort shellsort heapsort mergesort quicksort 
sorting sorting process rearranging sequence comparable items descending order knu 
fundamental problems computing intensively studied years 
practical problem 
estimated percent world cpu running time spent sorting knu 
simplify matters assume loss generality orderable items unique integers entire sort done main memory 
type sorting called internal sorting 
consider external sorting sorts performed main memory done disk tape knu wei 
assume input permutation numbers assume abstraction simplify analysis implementations algorithms assume exploit information speed sort 
ascending order correct 
comparison swap model raw described follows 
problem array integers arbitrary order 
want ordered array integers 
environment integers different simplification 
derive order information comparing integers 
preserve order information swapping integers 
known sorting comparison swap model lower bound complexity log raw number items ordered 
sorting algorithms impossible investigate compare sorting algorithms variations 
experiments chosen sorting algorithms insertion sort shellsort heapsort mergesort quicksort chosen representatives various categories sorting algorithms 
briefly introduce algorithm 
interested readers consult knu raw details 
insertion sort insertion sort simplest sorting algorithms 
sorts list tally inserting unsorted items sorted sublist 
insertion sort consists passes 
pass ensures elements positions sorted order 
insertion sort fact insert unsorted element correct place relative sorted sublist 
insertion sort 
worst case occurs input reverse order 
average case complexity insertion sort 
important fact insertion sort linear nearly sorted input 
presortedness input list quantified various measures 
examine sections 
shellsort insertion sort slow inefficient moving data 
move element takes time proportional distance insertion sort exchanges adjacent elements 
shellsort boo named inventor shell simple extension insertion sort 
gains speed comparing exchanging elements distant 
shellsort uses increment sequence 
ht 
phase increment hk hk elements spaced hk apart sorted 
subsequences sorted resulting array nearly sorted final pass insertion sort called completely sort 
hk decreases algorithm runs shellsort referred diminishing increment sort 
increment sequences long better 
shell suggested natural choice increment sequences ht hk hk 
worst case running time shellsort increment sequences performance quite acceptable practice large permutations 
advantage shellsort requires small amount coding get working sorting algorithms significantly complicated little efficient 
heapsort heapsort wil uses time build binary heap elements 
performs deletemin operations elements leave heap sorted order 
recording copying elements back array sorted array 
deletemin operation takes log time total running time log 
heapsort finds smallest elements log time preferred cases incremental sorting needed 
mergesort mergesort recursively sorts array divide conquer strategy 
merge sort recursively breaks large sequences smaller subsequences sorts merges subsequences sorted list 
time merge sorted sub sequences linear 
worst case running time mergesort log number comparisons nearly optimal 
drawback uses extra memory 
quicksort quicksort hoa fastest known sorting algorithm practice 
mergesort quicksort divide conquer recursive algorithm 
uses idea parti recursive steps 
divide phase partitions array disjoint parts small elements left large ones right 
conquer phase sorts part separately 
divide phase need merge phase combine partial solutions 
quicksort average case running time log worst case 
worst case behavior observed practice exponentially little effort wei 
quicksort way picking pivot partition list crucial 
median partitioning 
uses pivot median left right center elements 
proven efficient practice reduces running time percent wei 
instance characteristics measures presortedness nearly sorted sequences common practice knu 
instances easy sense small amount needed sort 
sorting algorithms take advantage existing order input time taken sort function size input sequence disorder sequence 
types algorithms called adaptive man 
disorder sequence small algorithm better log 
example insertion sort linear nearly sorted sequences 
existing order sequence presortedness quantified measure 
lists different measures presortedness 
study number inversions number runs longest ascending subsequence 
chosen natural representative measures presortedness 
inversions inv 
permutation set 

inversion pair elements wrong order 
definition inversions inv ai aj inv important presortedness measure intensively studied knu 
sorted sequence inv sequence reverse order inv inv indicates exchanges adjacent elements needed sort sense accurate performance indicator algorithms adjacent element exchanges bubble sort 
generally drawbacks 
takes time compute exact inv 
second inputs type 

quadratic number inversions intuitively order 
sorted fast merging 
inversion table inversion table 
bn permutation 
obtained letting bj number elements left greater example permutation inversion table left left 
total inversions definition relations bijection inv 
bn bn 
important fact inversions inversion table uniquely determines corresponding permutation 
easy compute knu 
correspondence important translate problem stated terms permutations equivalent problem stated terms inversion tables may easier solve 
bijection design random generation algorithm generate permutations size specified inv uniformly random 
runs run run number ascending substrings runs permutation 
definition runs example permutation run ai ai runs 
run important measure represents number sorted subsequences input 
sorted sequence run sequence reverse order run run reflects intuition presortedness small number ascending runs indicates high degree presortedness 
easy compute time 
drawback measure capture local disorders 
example 
produces lot runs sorted quickly sorted exchanges adjacent elements 
want randomly generate permutations specified number runs algorithmic experiments 
need translate random generation problem simpler equivalent problem inv 
introduce new concepts permutations useful design random generation algorithm 
representations permutations permutation represented notations line notation cycle notation line notation sw 
line notation common representation 
example 
cycle notation fact permutation written product disjoint cycles 

cycle length corresponds fixed point example fixed point 
line notation lists bijection run cycle notation decomposition disjoint cycles gives bijection useful help designing random generation algorithms generate permutations size run uniformly random 
put smallest number cycle cycle put cycle order entries 
defines canonical cycle decomposition example canonical cycle decomposition 
permutation removing parentheses line notation 
mapping bijection recover follows cycle initial segment 
cycle ends smallest number appearing 
remaining cycles calculated manner 
bijection table 
entries 
entry cycle maps 

notice falls elements lie inside cycles example falls 
fall true aj am clearly reverse true gives fall 
permutation falls runs 
important theorem 
theorem number permutations runs equal number permutations line notation exactly positions 
take entry table example 
number general aiai fall ai ai table bijection permutations runs 
expect positions line notation element upper line larger corresponding element line theorem mapping table provides important bijection translate random generation problem run random generation permutations line notation positions satisfying mentioned property 
problem easier solve original problem 
longest ascending subsequence las rem las permutation length longest ascending subsequence definition las las max 


ait 
example longest ascending subsequence las 
clearly las sorted las las attains minimum value list reverse order 
related measure rem las indicates numbers removed sorted list 
large las small rem guarantees little local disorders inv run 
las rem computed log fre 
solve random generation problem las rem translate easier problem combinatorial properties permutations 
integer partitions young diagram integer partition tuple positive numbers 



number parts 
example partition parts 
describe giving number times part occurs called multiplicity useful picture partition array squares cells left justified decreasing order 
example diagrams called young diagrams 
young tableaux partition young tableaux shape young diagram cell filled positive number entries row increasing order left right entries column increasing top bottom 
example young tableaux shape 
simplicity just call tableaux 
bijection las correspondence correspondence bijection multiset permutations pairs tableaux shape 
consider special case permutations multiset permutations 
theorem knu sw 
theorem correspondence set tions 
set ordered pairs tableaux formed 
shape 
example bijection follows 
correspondence bijection build permutation recover 
algorithms described detail knu sw 
remarkable property correspondence length longest ascending subsequence hidden shape corresponding tableaux theorem 
theorem permutation 
pair tableaux determined correspondence 
number rows length longest ascending subsequence correspondence translate problem generating random permutations size specified las problem generating pairs tableaux rows recover permutation 
random generation permutations degree presortedness random generation test instances specified characteristics im portant empirical analysis algorithms 
form problem generate elements finite set combinatorial structures random uniform distribution 
elements selected chance 
exact random generation problem appears intractable important structures interest focused finding efficient randomized algorithms ap proximate 
brute force algorithm exhaustively listing possible elements choosing random viable extremely large 
section apply markov approach sin generate permutations degree presortedness uniformly random 
heart markov approach simple algorithmic paradigm simu markov chain states set combinatorial structures converges uniform distribution markov chain approach random generation problems countable set called state called state space 
call distribution 
random variable suppose set 
defines distribution random variable takes value probability say matrix pij stochastic row pij distribution 
called doubly stochastic column pij distribution 
definition markov chain say xn markov chain ini tial distribution transition matrix distribution distribu tion xt defining transition matrix pij previous values simplicity consider finite sets 
xt depends xt xt 
xt xt xt pit natural question markov chain determine probability state finite steps 
positive integer step transition matrix simply power ij xt xt independent markov chain called irreducible pair states ij state reached state finite number steps 
markov chain called aperiodic greatest denominator ii state non zero probability staying unchanged 
markov chain called ergodic exists distribution lim ij chain converges stationary distribution 
necessary sufficient conditions chain ergodic irreducible aperiodic finite markov chain irreducible aperiodic ergodic 
ergodic markov chain doubly stochastic transition matrix stationary distribution uniform 
theoretical results solve random generation prob lem constructing ergodic finite markov chain states correspond ele ments transition matrix doubly stochastic 
chain allowed evolve time distribution final state tends asymptotically uniform stationary distribution elements generated randomly probability simulating chain sufficient number steps outputting final state able generate elements distribution arbitrarily close uniform 
example card shuffling process random transpositions generates permutation cards uniformly random sin 
natural number sn denote set permutations set 

consider deck cards labelled elements set identify 
sn ordering deck ith card top define markov chain state space sn transitions picking cards deck random exchanging 
incorporate self loop probability state 
chain irreducible path exists permutations 
aperiodic non zero self loop probability 
transition matrix chain doubly stochastic 
stationary distribution uniform 
shown chain rapidly mixing close stationary visiting small fractional state space 
specifically number simulation steps required achieve tolerance log log 
random generation problem inv integer numbers problem generate permutation set 
uniformly random inv bijection permutation sion table translate problem generating corresponding inversion tables uniformly random 
recall inversion table 
bn permutation 
obtained letting bj number elements left greater problem generate inversion table uniformly random bi bi satisfies properties 
bn bn 
example permutations length inv corresponding inversion tables listed table 
state clearly think bi bin capacity task filling bins balls violating capacity constraints 
construct markov chain states possible inversion tables possible bin filling solutions 
transitions picking bins table permutations inversion tables permutations inversion tables random transferring ball bi bj possible 
bi bj full 
automatically non zero self loop probability 
start random walk state space steps return current state inversion table convert permutation 
algorithm listed 
prove algorithm generate permutation number inversions uniformly random 
theorem markov chain generated algorithm irreducible 
proof markov chain irreducible states chain path non zero probability state reaching state 
show constructing canonical path legal inversion table 
bn bi inversion table 

non zero probability picking pair bj 
decrease bj increase pick reach reverse process true 
exists path non zero probability states chain 
shows chain irreducible 
theorem markov chain generated algorithm aperiodic 
proof markov chain aperiodic possible chain stay state 
easy see step algorithm 
chain input length permutation number inversions number iterations output permutation 
inv step initialize legal inversion table 
bn bi step repeat loop times pick numbers uniformly random bi bj bi bi bj bj step convert current inversion table corresponding permutation return algorithm randomly generating permutation inv stay unchanged condition step satisfied self loop probability greater zero 
theorem transition matrix defined algorithm doubly stochastic 
proof constructed chain symmetric transition matrix 
paths states probability directions 
self loop probability step equals minus probability moves 
example transition matrix shown table 
rows columns transition matrix add 
theorem markov chain generated algorithm ergodic con uniform stationary distribution 
proof previous theorems chain irreducible aperiodic doubly stochastic ergodic stationary distribution uniform 
table transition matrix defined algorithm random generation problem run problem generate permutation set 
uniformly random run time bijection run shown table translate problem generating corresponding line notation permutations uniformly random 
recall correspondence permutation runs permutation line notation exactly positions upper line element larger lower line element consider example generate run generate line notation positions recover note upper line line notation 
construct markov chain states correspond possible permutations line notations exactly positions upper line number larger lower line number 
transitions randomly picking columns line notation permutation ing lower line numbers doing change property having positions upper line number larger 
start random walk input length permutation number runs number iterations output permutation 
run step initialize legal permutation 
bn lower line line notation permutation positions bi step repeat loop times pick numbers uniformly random exchanging bi bj destroys property exchange bi bj 
step convert current line notation permutation corresponding permutation return algorithm randomly generating permutation run state space chain steps return current state permutation line notation convert back permutation bijection 
algorithm listed 
proof ergodic property similar algorithm 
random generation problem las rem problem generate permutation set 
uniformly random las rem bijection random generation problem las correspon dence set permutations 
las set ordered pairs tableaux formed 
shape number rows problem little complicated need take care shape number input length permutation number runs number iterations output permutation 
las step randomly generate shape rows initialize pair young tableaux shape boolean false 
step repeat loop times randomly generate shape rows generate shape uniformly random randomly generate new pair shape set true re generated step convert current pair tableaux corresponding permutation return algorithm randomly generating permutation las rows tableaux 
state space chain contains tableaux rows 
note may different shapes 
shape represents subspace state space 
initialization step generate shape rows randomly partitioning parts 
select young tableaux shape uniformly random filling cells shape 
done inserting number corner position shape right probability inserting corner remaining shape random selection algorithm nw 
start random walk state space chain 
iteration random walk explore subspace defined current shape 
transitions randomly selecting filling new tableaux current shape 
change shape initial point subspace regenerated 
simulating process sufficient number steps algorithm selection meta reasoner sorting convert current state pair tableaux shape rows corresponding permutation defined correspondence return algorithm described 
proof ergodic property similar algorithm 
experiment setups environment goal experiments machine learning approach investigate instance features affect performance various sorting algo rithms 
algorithm space consists algorithms insertion sort shellsort heapsort mergesort quicksort 
feature space consists instance size measures presortedness inv run rem 
working procedure sorting algorithm selection meta reasoner illustrated 
experiments divided phases data preparation model tion model evaluation 
data preparation generate set training permutations different characteristic values 
run sorting algorithms permutations collect algorithm performance data get simple matrix table 
call training data 
algorithm running time measure performance 
algorithm consumes time sort ing instances labelled best 
learning algorithms require discrete data need discretize training data 
model induction run various machine learning algorithms training data induce predictive algorithm selection models 
consider basic models decision tree learning naive bayes classifier bayesian network learning 
classifiers catalog instances best algorithm solve 
evaluate learned classifiers performance algorithm selection system various test datasets 
learning experiments conducted weka open source ma chine learning software java 
weka wf collection machine learning algorithms solving real world data mining problems 
contains tools data preprocessing classification regression clustering association rules evaluation visualization 
decision tree learning naive bayes learning weka implementations 
bayesian network learning implemented managed plug weka weka evaluation modules 
hugin implementation clique tree propagation algo rithm build methods classifying new instances 
experiments ibm high resolution time stamp facility measure algorithm running time microseconds 
hardware platform includes quad xeon linux machines ghz amd athlon machine 
training instances generated linux machines learning evaluations conducted windows machine 
instance instance 
instance table basic experiment setup instances vs algorithms feature 
feature algo 

algo 
best experimental results evaluation induction predictive algorithm selection models sorting section report results series learning experiments sorting algorithm selection 
experiment designed verify known ob sorting algorithm performance specific datasets 
second determine measure disorder best feature sorting algorithm selection 
third applies various learning algorithms training dataset determine best model sorting algorithm selection 
fourth ex periment evaluates performance learned model meta level reasoner 
training datasets prepared training datasets learning experiments 
call dsort dsort dsort dsort 
datasets contain instances specific characteristics 
dsort contains possible permutations instances 
represents small size sorting instances 
dsort contains nearly sorted permutations size 
dsort contains randomly disordered permutations size 
fourth dataset dsort designed rep training dataset 
contains total permutations size varying presortedness measures varying 
specifically random generation algorithm algorithm random permutation dur 
composed smaller sub datasets 
sub dataset con tains permutations generated setting presortedness measures sizes 
second includes permutations reverse order size increasing 
third contains totally ordered permutations 
fourth con tains permutations form small numbers inversions large numbers runs 
fifth sub dataset contains permutations form 
small numbers runs large numbers inversions 
sixth contains randomly generated permutations size seventh contains nearly sorted permutations size 
permutation compute presortedness measures inv run rem record computational time measure 
run sorting algorithms permutation record running time algorithm winner takes time sort instance 
experiment verifying sorting algorithm performance specific datasets experiment apply decision tree learning algorithm dsort dsort dsort see learn datasets specific characteristics 
dsort instances small sizes dsort contains possible permutations totaling stances 
permutations size inv run rem 
loss generality presortedness measures normalized 
distributions measure different values illustrated 
measures correspond intuitive idea presortedness 
inv measures global presortedness run measures local presortedness rem combine elements 
quite independent 
theoretically comparing measures means easy job 
shows relationships inv run rem values permutations number permutations vs presortedness measures size table statistics inv run rem values dsort inv run rem mean stddev size 
statistics measure listed table 
preparing training data compute measures run sorting algorithms permutations 
record computational time measure running time algorithm 
average times shown 
see average insertion sort fastest algorithm inv takes longest time compute 
table training dataset dsort permutation size inv run rem winner shell shell insertion 
shell final training dataset fed learning algorithm consists attributes size inv run rem winner winner target attribute predicted attributes permutation question 
typical supervised learning classification problem 
applying dsort get result shown 
learned decision tree leaf insertion 
means learner thinks best algorithm selection strategy dsort just committing insertion sort algorithm 
classification accuracy 
confusion matrix shows numbers misclassified instances class 
experimental relationships inv run rem permutations size computational time measures running time sort algorithms dsort result verifies observation regarding sorting algorithm selection small permutations 
observation insertion sort best sorting small permutations 
dsort nearly ordered instances dsort contains nearly sorted permutations size 
instances generated swapping pairs randomly selected elements totally ordered permutation 
statistics dsort shown table 
applying dsort get decision tree leaf shown insertion 
classification accuracy dsort 
result verifies observation sorting algorithm selection nearly sorted permutations run information scheme weka classifiers relation sorting instances attributes size inv runs rem winner test mode fold cross validation classifier model full training set pruned tree insertion number leaves size tree time taken build model seconds stratified cross validation correctly classified instances incorrectly classified instances kappa statistic mean absolute error root mean squared error total number instances confusion matrix classified insertion shell heap merge quick result dsort table statistics inv run rem values dsort inv run rem minimum maximum mean stddev table statistics inv run rem values dsort inv run rem minimum maximum mean stddev observation insertion sort best sorting nearly sorted permutations 
dsort random instances dsort contains random permutations size 
generated random permutation generation algorithm dur 
statistics dsort shown table 
applying dsort get decision tree leaf shown 
time selected algorithm quick sort 
classification accuracy dsort 
result verifies observation sorting algorithm selection random disordered permutations observation quick sort best sorting random disordered permutations 
run information scheme weka classifiers relation sorting instances attributes size inv runs rem winner test mode fold cross validation classifier model full training set pruned tree insertion number leaves size tree time taken build model seconds stratified cross validation correctly classified instances incorrectly classified instances kappa statistic mean absolute error root mean squared error total number instances confusion matrix classified insertion shell heap merge quick result dsort run information scheme weka classifiers relation sorting instances attributes size inv runs rem winner test mode fold cross validation classifier model full training set pruned tree quick number leaves size tree time taken build model seconds stratified cross validation correctly classified instances incorrectly classified instances kappa statistic mean absolute error root mean squared error total number instances confusion matrix classified insertion shell heap merge quick result dsort table statistics inv run rem values dsort size inv run rem minimum maximum mean stddev experiment determining best feature sorting algorithm selection experiment representative training dataset dsort determine best feature sorting algorithm selection 
dsort contains permutations size varying presortedness measures varying 
distributions size presortedness measures dsort visualized 
statistics listed table 
wrapper feature selection apply ga wrapped feature selection classifier dsort see feature subset best 
wrapper uses evaluation classifier evaluate fitness feature subsets 
simple genetic algorithm search attribute space 
population size number generations 
crossover probability mutation probability 
shows configurations running information wrapper classifier 
ga converges generation outputs result subset 
features selected feature subset better complete feature set 
individual feature comparison feature algorithm selection takes short time compute higher classification accuracy 
order investigate feature best distribution size inv dsort distribution run rem dsort run information scheme weka classifiers weka classifiers weka weka classifiers weka relation sorting instances attributes size inv runs rem best test mode fold cross validation classifier model full training set attribute selection input data search method genetic search 
population size number generations probability crossover attribute subset evaluator supervised class nominal best wrapper subset evaluator learning scheme weka classifiers scheme options accuracy estimation classification error number folds accuracy estimation parameters attribute selection ga wrapper dsort run information ga wrapped initial population generation merit scaled subset merit scaled subset nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan selected attributes result attribute selection ga wrapper dsort sorting algorithm selection divide column training data dsort small training datasets dsort dsort dsort 
columns data 
dsort contains size inv winner 
dsort contains size run winner 
dsort contains size rem winner 
run datasets see gives highest classification accuracy 
result shows classification accuracy model induced dsort highest 
confusion matrices shown follows 
inv dsort run dsort rem dsort classified insertion shell heap merge quick classified insertion shell heap merge quick classified insertion shell heap merge quick compare average computational time measure see takes time 
results show average run takes time compute microseconds significantly better inv rem 
list complete results table 
computational time run significantly smaller measures classifi cation accuracy slightly worse inv conclude run best presortedness measure sorting algorithm selection 
run best feature experiments dsort table computation time classification accuracy inv run rem inv run rem average time microseconds accuracy training dataset 
saves time computing inv rem acceptable classification accuracy 
experiment determining best model sorting algorithm selection experiment induce predictive model sorting algorithm selection dsort 
introduced previous chapters representations learning schemes applied 
investigate basic representations decision tree naive bayes bayesian networks 
look meta learning schemes bagging boosting stacking 
bayesian network learning need discretize input data 
bagging bre base classifier bag 
bag size dsort 
number bagging iterations set 
boosting method freund schapire adaboost fs method 
basis classifier 
maximum number boost iterations set 
stacking wol base classifiers naive bayes classifier bayesian network learning 
meta classifier 
run learning schemes dsort compare results see learns best model highest classification accuracy needs reasoning time classifying new instance 
experimental results learning schemes listed table 
see bayesian network learning highest classification accuracy 
second third best models bagging 
table classification accuracy different learning schemes dsort naivebayes bagging boosting stacking accuracy stddev tice naivebayes worst performance inducers classification accuracies near 
confusion matrices best inducers bagging shown follows bagging classified insertion shell heap merge quick classified insertion shell heap merge quick classified insertion shell heap merge quick classification accuracy time needed classify instance reasoning time important criterion evaluate sorting algorithm selection model 
best models decision tree efficient classi fying new instance simply applies set rules 
bagging needs combine multiple models vote produce final classification takes table reasoning time microseconds classification accuracy best models bagging accuracy time longer time 
bayesian networks slower compared decision trees reasoning process involves discretizing data performing infer ence map propagation 
list average reasoning times classification accuracies models dsort table compare performance 
see classification accuracies decision tree better reasoning time 
draw models experimented decision tree best sorting algorithm selection 
experiment evaluating sorting algorithm selection system previous experimental results know run best feature decision tree learning best learning scheme sorting algorithm selection 
learned decision tree shown 
core sorting algorithm selection system 
new instance comes meta reasoner examines instance calculates features including size number runs 
uses decision tree model select best algorithm features 
calls selected algorithm sorts instance 
total time process consists parts obviously algorithm selection system worth having time spent examining reasoning compensated gain selecting best learned decision tree dsort table statistics test dataset dsort size inv run rem minimum maximum mean stddev algorithm 
section conduct experiment show datasets algorithm selection system provide best sorting perfor mance 
test data set dsort contains nearly sorted instances randomly disordered permutations size 
statistics dsort shown table 
classification accuracy learned model dsort 
incorrectly classified instances total 
confusion matrix follows 
classified insertion shell heap merge quick total time computing run values dsort microseconds 
time reasoning classification 
actual time spent sorting microseconds 
total time consumed system sorting dsort 
approximately algorithm selection system spends total time examining total time reasoning sorting 
com algorithm selection system th bar time sort algorithm bars applying solely dsort 
see algorithm selection system outperforms sorting algorithms dataset 
bar optimal sorting time collected running sorting algorithms time returns sorted list 
algorithm selection system perform save time examining reasoning classification error 
time spent algorithm dsort see working range algorithm selection system dataset exam ine describes change computational time consumed insertion sort quick sort algorithm selection system incrementally add randomly ordered instances dsort 
see nearly sorted instances insertion sort uses amount time quick sort uses amount time 
algorithm selection system lies comparing insertion sort algorithm selection system takes extra examining reasoning time 
start adding randomly ordered instances total time spent algorithm selection system 
working range algorithm selection system dsort time spent sort algorithm nearly sorted permutations time insertion sort jumps sharply worst algorithm soon th instance added 
mean time quick sort performance affected 
keep adding randomly ordered instances gorithm selection system lose advantage quick sort little little compared quick sort takes extra examining reasoning time 
th instance ia added quick sort outperforms algo rithm selection system best algorithm 
working region algorithm selection system ranges approximately algorithm selection system performs best data set ratio nearly ordered instances randomly ordered instances larger 
evaluated sorting algorithm selection system perfor time spent sort algorithm reversely ordered permutations time spent sort algorithm randomly ordered permutations mance test datasets containing permutations type 
est contains nearly sorted instances 
est contains permutations reverse order 
est contains totally random permutations 
results shown 
correspondent classification accu 
nearly sorted permutations insertion sort best selection system ranks second 
algorithm selection system outperformed insertion sort spend extra time examining reasoning 
classification accuracy high actual sorting time black portion bar optimal sorting time bar 
permutations reverse order shellsort best slightly outperforms quicksort 
algorithm selection system better worst algorithm insertion sort slightly worse 
mainly due low classification accuracy 
totally random permutations quicksort best 
algorithm selection system performs worse algorithms insertion sort 
classification accuracy misclassifies instances insertion sort quadratic ran dom permutations 
underlying reason tradeoff selecting run best predictive presortedness measure takes time compute distinct local disordered permutations totally random ones 
tests case algorithm selection system worst 
result suggests knowing underlying distribution input stances help meta level reasoner flexible decision making gain best performance 
example meta reasoner quickly sense distribution input instances changed nearly sorted instances totally random instances correspondingly modify algorithm selec tion model select best algorithm inputs examining reasoning input instances 
require reasoner acts meta meta level 
summary applied learning approach sorting algorithm selection 
experimental results show machine learning techniques build algorithm selection system gain efficient computation polynomial prob lems sorting 
algorithm selection polynomial computational problems time important factor 
classification accuracy com putational time instance feature reasoning time model crucial criteria determining feature model best algorithm se lection 
sorting experimentally number runs input permutation best feature decision tree best model 
algo rithm selection system built run decision tree provide best performance highly competitive classification accuracy 
system spends time meta level examining reasoning select best sort algorithm starts sorting 
doing able achieve better performance datasets containing nearly sorted highly random permutations 
chapter algorithm selection probable explanation problem chapter apply proposed machine learning approach algo rithm selection probable explanation mpe problem 
mpe problem instance consists components network topology cpts observed evidence 
correspondingly instance features include categories topo logical type connectedness network size skewness cpts pro portion distribution evidence nodes 
mpe algorithms consideration include exact clique tree propagation algorithm gibbs sampling forward sampling random restart hill climbing tabu search ant colony optimization 
mpe problem finding probable explanation important probabilistic inference problem bayesian networks 
recall bayesian network pair directed acyclic graph nodes set prior conditional probability tables node evidence set instantiated nodes 
explanation evidence complete assignment node values 
xn xn consistent evidence probable explanation explanation explanation higher probability 
notations definitions mpe problem defined follows 
mpe problem instance triple defines bayesian network nodes evidence set 
ek ek 
question computing mpe 
shown exact mpe computation np hard shi 
furthermore approximating mpe constant ratio bound np hard ah 
algorithms finding mpe mpe problem belongs important type probabilistic inference problem bayesian networks called belief revision 
type bayesian network inference problems belief updating computes posterior belief query nodes evidence belief updating known probabilistic inference 
practice algorithms belief updating belief revision possibly slight modifications vice versa 
bayesian network inference algorithms guo roughly classified ex act approximate 
exact algorithms include main categories conditioning clustering elimination 
approximation algorithms include model simplification methods stochastic sampling algorithms search algorithms loopy prop 
chapter study problem selecting best different mpe algorithms exact algorithm sampling algorithms search algorithms hybrid algorithm combining sampling search 
classification algorithms shown 
exact algorithm clique tree propagation clique tree propagation algorithm called junction tree join tree propagation popular exact algorithm bayesian network inference practice 
basic idea converting network topology probabilistic equivalent polytree clustering nodes 
transforms network tree cliques candidate mpe algorithms performs belief propagation tree pearl linear time polytree prop algorithm pea 
phase consists moralization triangulation clique tree identification 
second phase way message propagation 
applied belief updating second phase performs summation propagation 
finding mpe performs maximization propagation 
details algo rithm refer readers hd nea ls 
clique tree propagation time space complexities exponential size largest clique transformed undirected graph 
practice works sparse networks size network large nodes example 
network dense algorithm runs memory feel exponential growth time 
study hugin imple mentation clique tree propagation currently standard best implementation available 
stochastic sampling algorithms stochastic sampling algorithms cd divided importance sampling gorithms markov chain monte carlo mcmc methods 
differ way samples drawn 
importance sampling algorithms samples drawn independently importance function 
importance function different cpts may updated sampling process 
mcmc methods samples drawn depending previous samples 
sampling distribution variable computed previous sample states markov blanket nodes 
importance sampling algorithms include logic sampling hen forward sampling likelihood weighting fc sp back ward sampling ff self importance sampling heuristic importance sampling sp adaptive importance sampling cd mcmc methods divided gibbs sampling pea pea metropolis sampling hybrid monte carlo sam pling gg mac grs 
sampling algorithms easy implement applied large range network sizes 
network large evidence probable explanation 
probability algorithm hit sampling schemes low 
main drawback sampling algorithms 
gibbs sampling gibbs sampling pea pea called stochastic simulation markov chain mcmc method 
starts random initial assignment gen sample previous sample randomly flipping state non evidence node xi 
xi randomly selected sampling distribution xi computed previous sample states markov blanket nodes 
xi xi xi xi yj yj normalization constant yj jth child xi 
generating number samples best sample far returned approximate input bn evidence set output complete assignment 
un 
step randomly generate initial sample agrees evidence set best far initial sample 
step randomly select non evidence node xi compute xi values markov blanket nodes previous sample sample xi set xi sampled value 
step evaluate new sample update best far decrease 
step goto step return best far 
gibbs sampling finding mpe mpe 
research shown convergence gibbs sampling theoretically guaranteed convergence rate extremely slow 
forward sampling forward sampling cd importance sampling algorithm impor tance functions prior cpts change 
samples non evidence variable turn topological order 
forward sampling algorithm mpe problem listed 
search algorithms search algorithms studied extensively solving hard combinatorial opti mization problems 
mpe np hard optimization problem researchers applied various optimization algorithms solve best search sc input bn evidence set output complete assignment 
un 
step non evidence node xi draw random sample xi xi 
step evaluate new sample update best far decrease 
step goto step return best far 
forward sampling finding mpe linear programming san stochastic local search kd genetic algorithms men par tried convert mpe problem np complete prob lem max sat sat solver solve mpe problem indirectly 
search algorithms meta heuristic guide search order avoid getting stuck local optima 
popular meta heuristics include var ious hillclimbing algorithms hro simulated annealing kgv tabu search glw genetic algorithms gol random restart hill climbing hill climbing hro greedy local search method 
goes best neighbor possible 
east get stuck local optimal 
random restart hill climbing randomly restarts hill climbing local optimal reached 
stops criteria satisfied returns best solution far 
success algorithm depends landscape search space 
local find solution quickly 
tabu search input bn evidence set output complete assignment 
un 
step randomly generate start point agrees evidence step best neighbor better current point update best far decrease move step goto step return best far 
random restart hill climbing finding mpe tabu search glw hro heuristic local search 
local search gorithms hill climbing memory algorithms means step depends current feasible solution search history 
idea tabu search store information set feasible solutions generated called tabu list information generating solution 
notice searcher moves worse current point 
main difference local search algorithm 
usually update tabu list done inserting visited point tabu list 
tabu search uses tabu list forbid revisiting points visited steps 
avoid repetitions short cycles 
advanced advanced features added improve tabu search 
simple tabu search algorithm mpe problem described 
input bn evidence set output complete assignment 
un 
step randomly generate start point agrees evidence add tabu list 
step find best neighbor tabu list move update best far necessary decrease update tabu list step goto step return best far 
tabu search finding mpe hybrid algorithm ant colony optimization mpe problem ant colony optimization aco dg studies artificial systems take tion behavior real ant colonies solve hard combinatorial optimization problems 
aco meta heuristic introduced dorigo ph thesis dor defined dorigo di caro gambardella dcg 
ant algorithm optimization inspired observation real ant colonies foraging behavior particular ants find shortest paths food sources nest 
ants deposit ground chemical substance called pheromone walking nest food sources vice versa 
forms pheromone trails 
ants smell pheromone choose path favor trails strong pheromone concentrations 
ants pheromone trails find way food nest 
pheromone provides indirect way communications ant colony 
shown experimentally foraging behavior give rise emergence shortest paths employed colony ants 
colony ants able choose shortest path nest food source back exploiting pheromone trails left individual ants 
emergent behavior ant colony researchers developed arti ant systems solve hard discrete optimization problems 
ant system artificial ants created explore search space simulating real ants searching environment 
objective values correspond quality food length path food 
adaptive memory corresponds pheromone trails 
artificial ants local heuristic function help decision set feasible solutions 
addition usually pheromone mechanism included allow ant colony slowly forget past history direct search new directions explored past 
aco seen hybrid optimization algorithm combines advantages sampling search 
ant sample search decision making affected pheromone dropped previous ants 
search process seen cooperative learning process 
ant system travelling salesman problem tsp dg 
applied job shop scheduling problem graph coloring problem dh task applying aco optimization problem restate problem shortest longest path problem ant systems 
convert mpe longest path ant search problem conditional branches order constraint 
ant system artificial ants build solutions explanations mpe prob lem moving problem network node 
ants visit nodes topological order defined bayesian network parent nodes visited visiting child node 
evidence nodes ants allowed take branches agree evidence 
number cpt node represents conditional branch 
memory ant contains visited nodes selected branches tables node pheromone table pt heuristic function table ant decision table adt 
tables structure cpts 
cpts keep unchanged 
adt ai node obtained composition local pheromone trail values ijk local heuristic values ijk follows ijk ijk ijk ijk jth row kth column corresponding table ith node 
parameters control relative weight pheromone trails heuristic values 
probability ant chooses take conditional branch building tour pij aij aij column index adt value conditioned values parent nodes ith node 
ants built tour explanation ant deposits pheromone ijk corresponding pheromone trails conditioned branches node 
pheromone value dropped represents quality solution 
want find probable explanation likelihood tour pheromone value 
suppose generated tour 
xn pheromone value follows ijk 
xn xi xi 
xn computed chain rule 
xn xi xi updating pheromone tables done adding pheromone value corre sponding cells old pheromone tables 
ant drops pheromone cell pheromone table node jth row kth volume pt ith node 
dropping pheromone ant dies 
pheromone evaporation procedure happens just ants start deposit pheromone 
main role pheromone evaporation avoid stagnation ants selecting tour 
pheromone tables updated addition new pheromone ants pheromone evaporation follows ijk ijk ijk ijk ijk number ants iteration pheromone trail decay coefficient 
practice optional daemon activity added collect useful global information drop additional pheromone 
doing bias ant search process non local perspective 
example daemon allowed observe ants behavior give extra rewards best ants punish worst ants adding removing pheromone 
lists basic ant mpe algorithm 
initialization algorithm generates batch ants iterations 
iteration ants sample ant decision tables produce trail evaluate trails cpts save best sample iteration 
pheromone dropped pheromone tables updated 
pheromone evaporation optional daemon action triggered right 
iteration adts updated normalized 
procedure stops number iterations runs 
best solution far returned approximate mpe 
parameter values set initialization step 
initial amount pheromone set small positive constant pheromone tables 
number ants iteration set 
characteristics mpe problem instances mpe instance consists parts network cpts evidence 
instance characteristics study centered aspects 
input bn evidence set output complete assignment 
un 
initialization initialize ants set pts adts uniform cpts set best trail far null 
step generate ants ant trails sampling adts compute pheromone values decrease update best trail far step update pts dropping pheromone pheromone evaporation daemon actions 
step compute new adts cts normalize adts 
step goto step return best trail far 
ant mpe algorithm network characteristics network characteristics include network topological type network connectedness 
network topological type definition bayesian networks dags 
dags distinguish single connected graphs multiply connected graphs 
singly connected directed graphs directed path nodes 
multiply connected graphs path nodes loops graphs 
singly connected graphs distinguish trees polytrees level networks 
trees directed node parent 
polytrees allow node parents path nodes underlying undirected graphs underlying undirected graph tree 
level networks called noisy models 
describe called noisy relation generalization logical 
level noisy model nodes root level represent possible causes nodes leaf level represent effects 
general noisy models provide simpler model learning inference easier 
famous example noisy network qmr dt network 
trees polytrees singly connected vice versa 
level networks singly connected paths length 
polytrees underlying undirected graphs loops just multiply connected graphs 
complex topology expressive simple 
increases computational complexity 
inferences trees polytrees polynomial underlying undirected graphs tree structure 
graphs underlying undirected graphs contain cycles inference intractable general mpe problems level networks multiply connected networks np hard shi sd 
network connectedness number nodes network nodes number arcs arcs 
network connectedness conn calculated simply conn arcs simplest nodes topology polytree arcs complex topology fully connected dag conn lies 
usually graph said dense conn 
practice exact clique tree propagation algorithm runs fast sparse networks 
network dense induced width maximum clique size underlying undirected graph large 
time space complexity clique tree propagation algorithm exponential induced width underlying undirected graph intractable quickly assume underlying undirected graph connected graph 
network dense 
times get memory error feel exponential growth running time 
cpt characteristics cpt characteristics include cpt size cpt skewness 
cpt size node cpt size number cells cpt product state space parents times state space 
fix node state space binary just number parents node measure cpt size 
experiment consider maximum number parents node max parents 
performance exact algorithm influenced conn max parents factors affect size largest clique underlying undirected graph 
cpt skewness skewness cpts computed follows jn vector column cpt table 
vm conditional probabilities skew mi vi skewness cpt node average skewness columns skewness network average skewness nodes 
skewness influence performance sampling search algorithms 
evidence characteristics evidence characteristics includes proportion distribution evidence nodes 
number evidence nodes 
evidence proportion simply usually evidence nodes implies evidence 
nodes mpe quite probability hit sampling scheme high 
distribution evidence nodes affect hardness mpe prob lem 
evidence nodes cause nodes problem called predictive reasoning 
evidence nodes effect nodes problem called diagnostic reasoning 
proven singly connected networks predictive reasoning easier diagnostic reasoning sd 
specifically strictly predictive belief updating belief revision singly connected networks performed time linear size network diagnostic belief updating belief revision np hard 
strictly predictive defined follows sd definition bayesian network evidence set nodes inference problem called strictly predictive evidence nodes non evidence parents similarly define strictly diagnostic follows definition bayesian network evidence set nodes inference problem called strictly diagnostic evidence nodes non evidence children experiments consider types evidence distributions strictly predictive strictly diagnostic randomly distributed evidence 
random generation mpe instances random generation mpe instances controlled parameter values markov chain method introduced ic 
construct simulate markov chain walk randomly space possible networks satisfy constraints 
markov chain irreducible graph reached graphs 
chain non zero self loop probability guarantee chain stay unchanged 
random walk governed doubly stochastic transition matrix stationary distribution markov chain uniform 
control parameters include topological type num ber nodes maximum minimum number arcs maximum number parents node number states node skewness cpts 
experiment set nodes binary 
consider topology types polytree level singly connected networks multiply connected networks 
gener ating network cpts randomly generate set evidence nodes type predictive diagnostic random form complete mpe instance 
aspect instance features user requirement computational re sources user provide computational deadline mpe problem 
solutions return deadline 
experiment setups environment experiments apply proposed machine learning approach investigate algorithm selection finding mpe 
mpe algorithms include exact algorithm clique tree propagation algorithm stochastic sampling algorithms gibbs sampling forward sampling heuristic search algorithms random restart hill climbing tabu search hybrid algorithm ant colony optimization aco algorithm 
mpe instance features investi gate include number nodes network topological type network connectedness maximum number parents network cpt skewness proportion distribution evidence nodes 
general mpe problem np hard means expect time algorithm assuming np exact clique tree propagation algorithm solve polytrees sparse networks efficiently 
goal identify class mpe instances clique tree propagation algorithm applicable 
exact algorithm applicable probably due memory error need look various approximate algorithms 
second goal learn predictive model determine approximate algorithm algorithm selection meta reasoner mpe problem best input mpe instance characteristics 
far effective way fairly compare different heuristic algorithms allow algorithms consume amount computation resources distinctions quality solutions obtained ru 
experiments give algorithm numbers samples search points compare quality solutions algorithm returns 
algorithm returns highest mpe labelled winner 
record highest mpe generated algorithm 
algorithms return mpe spends time numbers samples search points labelled winner 
working procedure mpe algorithm selection meta reasoner illustrated 
experiments consists phases data preparation model duction model evaluation 
data preparation phase generate mpe instances different characteristic values 
run algorithms train ing instances collect performance data training dataset 
run various machine learning algorithms training data induce predic tive algorithm selection models 
sorting algorithm selection consider different kinds models decision tree learning naive bayes classifier bayesian network learning 
evaluate learned classifiers mpe algorithm selection system 
experimental results evaluation induction algorithm selection models finding mpe section conduct set experiments induce predictive algorithm selec tion models mpe problem 
extract characteristics real world bayesian networks 
generate training datasets real world characteristics 
training data apply machine learning algorithms induce algorithm selection models 
evaluate learned models 
experiment designed learn model decide ex act clique tree propagation algorithm applicable input mpe instance 
experiment determines exact algorithm applicable learns concept exactly computable 
second experiment designed select best feature subset approximate algorithm selection 
third experiment induce approximate algorithm selection model selects best set sampling search approximate mpe algorithms 
fourth experiment compares mpe algorithms performance different specific datasets 
fifth experiment evaluate learned model meta level reasoner mpe algorithm selection 
characteristics real world bayesian networks space possible mpe instances infinitely large time extreme characteristics rarely encountered real world applications reasonable necessary consider subset set real world table characteristics real world bayesian networks name nodes arcs conn roots skewness maxclique alarm barley cpcs cpcs diabetes insurance link munin munin munin pigs water problems rwp 
order simulate set real world bayesian networks extract real world distributions characteristic parameters collection real world samples generate bayesian networks mpe problem instances extracted distributions 
generate synthetic real world bayesian networks synthetic instances real world inference problems run candidate algorithms instances generate training data 
collected real world bayesian networks 
call dataset bn 
characteristics listed table 
analysis results see number nodes varies connect maximum number parents skewness varies 
maximum clique sizes available smaller 
training datasets analysis results real world bayesian network characteristics guide generation training datasets 
specifically set ranges instance features follows nodes conn topology polytree twolevel multiply maxp skewness predictive diagnostic random 
training dataset dmp induce model decides exact clique tree propagation algorithm selected 
time space complexity exact algorithm exponential maximum clique size underlying undirected graph 
practice exact clique tree propagation algorithm applicable sparse networks usually smaller cliques 
network dense exact clique tree propagation algorithms infeasible generate memory exceptions 
order determine exact inference algorithm generate dmp follows randomly generate networks connectedness varying maximum number parents varying 
number nodes 
run hugin randomly generated networks record perfor mance 
perform inference hugin compiles network clique tree 
record maximum clique size label network instance compilation successful 
throws memory error label instance 
dmp numeric attributes node topology connect 
target class takes boolean values representing exact algorithm applicable 
put real world networks dmp contains total instances 
second training dataset dmp generated induce model select best set approximate mpe algorithms 
polytrees easy exact algorithm dmp contains level multiply networks 
generate set networks different characteristic values run approximate algorithms networks different evidence settings 
run approximate inference algorithms number samples search points label stance best algorithm returns best mpe value samples search points 
total number samples algorithm 
dmp attributes node topology connectedness skew ness samples 
target class best algorithm instance 
dmp contains instances generated networks 
experiment determining exact mpe algorithm applicable experiment apply machine learning algorithms dmp induce model predict exact mpe algorithm 
run learning schemes previous chapter naive bayes bayes networks bagging boosting stacking compare results see learns best model highest classification accuracy 
statistic dmp shown table 
table statistics attribute values dmp nodes conn minimum maximum mean stddev topology label multiply twolevel polytree count experimental results listed table 
see boosting highest classification accuracy 
second third best models bagging 
notice naivebayes worst performance table classification accuracy different learning schemes dmp naivebayes bagging boosting stacking accuracy stddev inducers classification accuracies higher 
boosting classification accuracy simpler model best model exact mpe algorithm selection 
confusion matrix follows classified learned decision tree shown 
structure tree see basic rule exact algorithm selection exact clique tree propagation algorithm applicable network small sparse 
test learned boosting model real world networks 
classification accuracy correctly classify real world instances 
confusion matrix follows classified boosting networks correctly classified 
confusion matrix follows boosted classified tried cost sensitive classification 
gives result datasets 
cost matrix learned decision tree exact mpe algorithm selection true class prediction penalty true class prediction penalty experiment wrapper feature selection experiments look approximate mpe algorithm selection problem 
training dataset dmp 
format dmp shown table 
contains instances 
instance attributes 
predictive attributes target class attribute labels best approximate algorithm instance 
statistics dmp listed table 
see gibbs sampling winner ant colony optimization algorithm best nearly half instances 
table format training dataset dmp nodes topology conn skewness samples multiply predictive multi hc multiply random aco 
multiply diagnostic aco multiply random aco sorting apply ga wrapped feature selection classifier see feature subset best 
wrapper uses eval uation classifier evaluate fitness feature subsets 
simple genetic algorithm search attribute space 
population size number gener ations 
crossover probability mutation probability 
configuration output ga wrapper shown 
feature subset selected ga node skewness samples 
classification accuracy induced model 
con fusion matrix follows table statistics attribute values dmp nodes conn skewness samples minimum maximum mean stddev topology label multiply twolevel predictive diagnostic random count tabu aco count percentage classified gibbs sampling forward sampling multi hc tabu aco experiments selected feature subset learn predictive algorithm selection model 
experiment determining best model approximate mpe algorithm selection experiment apply machine learning algorithms selected feature sub set dmp induce model selection approximate mpe algorithms 
ran learning schemes selected subset dmp see learns best predictive model experimental results shown ta ble 
result see model induced highest classification accuracy 
naive bayes classifier worst performance 
run information attribute subset evaluator supervised class nominal best algorithm wrapper subset evaluator learning scheme weka classifiers scheme options accuracy estimation classification error number folds accuracy estimation selected attributes node skewness samples parameters output attribute selection ga wrapper dmp classification accuracy bayesian networks learning 
learned bayesian network shown 
learned network evidence nodes disconnected graph 
draw result shows number nodes skewness cpts number samples important features mpe algorithm selection 
second know domain knowledge statistics training data exists dependency relationships evidence characteristics sampling algorithm performance result implies dependencies weak capture due greedy search strategy 
classification accuracies reported test accuracies computed fold cross validation 
draw classification accuracies folds naive bayes 
table classification accuracy different learning schemes dmp naivebayes bagging boosting stacking accuracy stddev learned bn approximate mpe algorithm selection confusion matrices naive bayes classifier shown follows classified gibbs sampling forward sampling multi hc tabu naivebayes aco classified gibbs sampling forward sampling multi hc tabu aco classified gibbs sampling forward sampling multi hc tabu aco classification accuracies fold cross validation experiment mpe algorithm performance specific datasets know result feature selection node skewness samples ev relevant features approximate mpe algorithm selection 
experiment features partition dmp smaller subsets compare algorithm performance resulting spe cific sub datasets 
partitioning dmp number nodes shows partition dmp nodes 
see number nodes affects relative performance search algorithms forward sampling aco affected 
network size increases multi nodes number times tabu aco partitioning dmp number nodes start hill climbing best algorithm frequently chances tabu search best drops significantly 
phenomenon explained constant size tabu list 
network larger tabu list remain size tabu list relatively smaller 
may affect tabu search performance lose best algorithm position multi start hillclimbing 
partitioning dmp number samples time algorithm directly proportional number samples 
shows partition dmp samples 
relative perfor search algorithms affected forward sampling aco 
number samples increases tabu search best algorithm multi start hillclimbing loses top rank 
tabu search able utilize available number search points better multi start hillclimbing 
partitioning dmp skewness shows partition dmp cpt skewness 
see skewness significant influence relative performance algorithms 
skewness low search space flat search algorithms perform better sampling algorithms 
multi start hillclimbing wins best algorithm times tabu search 
skewness aco outperforms algorithms time 
skewness increases forward sampling aco winners 
notice forward sampling works better highly skewed networks aco works highly skewed networks medium skewed networks 
partitioning dmp evidence nodes evidence likelihood evidence directly affects sampling algorithm performance 
shows samples number times tabu aco partitioning dmp number samples skewness number times tabu aco partitioning dmp cpt skewness partition dmp 
general changing evidence percentage affect search algorithms relative performance 
affect forward sampling aco 
curves see aco performed forward sampling percentage evidence nodes increases 
note evidence percentage influence weaker skewness 
partitioning dmp shows partition dmp 
see relative performance multi start hillclimbing affected evidence distribution 
tabu search slightly affected 
diagnostic inference relatively hard forward sampling easy aco 
random distributed evidence relatively hard aco easy forward sampling 
running algorithms real world networks section show results running algorithms real world networks evidence alarm cpcs cpcs 
see alarm forward sampling aco find mpe samples forward sampling hits mpe earlier aco 
shows search history algorithm running cpcs 
top contains samples bottom contains total samples 
see samples aco leading 
samples forward sampling takes lead tabu search finds better solution samples 
final winner multi start hillclimbing samples 
example shows number samples important factor determining best algorithm networks 
shows search history algorithms cpcs 
trend similar 
alarm cpcs skewed cpts 
number times tabu aco partitioning dmp evidence percentage number times tabu aco predictive random diagnostic partitioning dmp evidence distribution search history algorithms alarm network verifying results decision tree previous analysis learned basic facts working regions approximate mpe algorithms 
general observed results skewness important feature determining best algorithm 
search algorithms sampling algorithms better networks 
second nodes samples obvious influence relative performance multi start hillclimbing tabu search 
little influence forward sampling aco 
third evidence percentage distribution affect forward sampling aco generally affect search algorithms 
knowledge verified decision tree learned 
tree shown 
see root node skewness 
left branch tree representing low skewness instances contains multi start hillclimbing tabu search 
right branch tree representing skewed instances con search history algorithms cpcs network search history algorithms cpcs network tains forward sampling aco 
left branch mainly nodes samples divide instance space implies influence search algorithms significant 
right branch right skewness divide 
region forward sampling aco highly competing 
experiment evaluating mpe algorithm selection system experiment evaluate learned mpe algorithm selection system test datasets 
algorithm selection test dataset est contains stances 
statistics dmp est listed table 
second test dataset real world dataset bn 
system contains classifiers ex act algorithm selection approximate algorithm selection 
call 
mpe instance determines exact clique tree propagation algorithm 
classification result system execute exact mpe gorithm 
classification result select best approximate algorithm 
selected algorithm executed final mpe value returned 
system evaluation synthetic networks apply est 
identifies instances cor rectly 
apply rest instances 
result shows correctly classified instances incorrectly clas instances 
classification accuracy 
confusion matrix follows learned decision tree approximate mpe algorithm selection table statistics attribute values dmp est nodes conn minimum maximum mean stddev topology label multiply twolevel polytree count classified gibbs sampling forward sampling multi hc tabu aco show algorithm selection system outperforms single algorithm partition instances groups skewness 
instances medium skewed instances highly skewed instances 
group instances plot total mpe returned algorithm compare total mpe returned algorithm selection system 
results shown 
medium skewed highly skewed instances algorithm selection system returns largest total mpe values 
instances system returns second largest mpe value largest total computed multi start hill climbing algorithm selection system result gets 
instances algorithm selection system returns largest total mpe value outperforms algorithms 
returned mpe values listed table 
total mpe algorithms instances total mpe algorithms medium skewed instances total mpe algorithms highly skewed instances search history algorithms munin network table total mpe returned algorithms algorithm selection system test instances skew gibbs fs mhc ts aco total system evaluation real world networks test system real world networks 
reported boosting classifier real world networks correctly classified ex 
networks 
exact mpe values exactly computable networks listed table predicted best algorithm best mpe returned approximate algorithms 
networks predicted best approximate algorithms agree actual best approximate algo rithms 
networks link munin 
selects aco best approximate algorithm networks 
link nodes huge joint probability space states 
cpt skewness 
numbers cpts zeros 
number samples algorithms returned mpe link 
due huge state space low skewness 
applying greedy sampling get mpe munin nodes 
state space states 
cpts skewed link skewness 
total numbers cpts zeros 
number samples aco returns best mpe forward sampling finds second best mpe returned algorithms return 
results summarized table 
search history approximate algorithms shown 
test results synthetic real world networks illustrate pro table mpe exactly computable bayesian networks network predicted predicted best exact approximate best algo 
appro 
algo 
mpe best mpe alarm exact fs barley exact aco cpcs exact aco cpcs exact aco diabetes exact aco exact aco insurance exact fs munin exact aco munin exact aco pigs exact aco water exact fs table mpe link munin network number predicted actual best samples best algo 
best algo 
mpe link aco munin aco aco posed machine learning approach solve algorithm selection problem mpe problem 
meta level reasoner learned models reasonable decision selecting exact best approximate mpe algorithms input mpe instance 
learned mpe algorithm selection system provides best performance solving mpe problem 
summary chapter studied machine learning approaches build algorithm selection system mpe problem 
system consists pre models classifiers 
decides exact mpe algorithm applicable 
test classification accuracy 
classifier classifies instance exactly computable second classifier determine approximate algorithm best input mpe instance 
second classifier classification accuracy 
different mpe instance characteristics different properties affect different algorithms performance 
experimental results show cpt skewness important feature approximate mpe algorithm selection 
shows search mpe algorithms better networks sampling algorithms better skewed networks 
features nodes samples affect gorithms relative performance degree strong skewness 
learned algorithm selection system uses polynomial time computable instance characteristics select best algorithm np hard mpe problem gains best performance terms returned mpe values 
scheme algorithm selection np hard problems 
general applicable solve algorithm selection computationally hard problems various fields 
chapter chapter summarize contributions identify main issues refined studied 
contributions thesis studied algorithm selection problem theoretically experimentally 
studied multifractal properties joint probability space bayesian networks apply solve mpe problem 
theoretically shown undecidability general automatic algo rithm selection problem applying rice theorem 
developed framework problem hardness algorithm performance kol complexity applied study ga hardness 
experimentally proposed implemented machine learning algorithm selection system 
experimental results sorting mpe prob lem proven approach useful algorithm selection np hard computational problems 
problems time important cri 
accordingly look instance features provide classification accuracy easy compute compared actual computation time 
np hard problems algorithm selection system consists classifiers 
encodes concept exactly computable 
second responsible selecting best approximate algorithm 
summary major contributions consist novel learning approach automatic algorithm selection sorting finding mpe 
theoretical framework problem hardness algorithm perfor mance kolmogorov complexity proof infeasibility purely analytical approach building automatic algorithm selection systems 
multifractal analysis joint probability distributions bayesian net works multifractal meta heuristic solving mpe prob lem 
contributions include study ga hardness problem hardness algorithm performance framework 
development phase sampling search algorithm finding mpe multifractal property jpd 
applying ant algorithms solve mpe problem 
development set random instance generation algorithms markov chain technique 
significance research lies aspects 
research systematically apply experimental algorithmic machine learning methods solve algorithm selection problem 
pro vides practical machine learning approach build algorithm selection systems tractable intractable problems 
artificial intelli gence machine learning community research identifies important application field empirical analysis algorithms 
techniques devel oped research helpful building real time intelligent systems require highly efficient solvers reasoning engines 
ex algorithmics community methodology applied research introduces set powerful tools analyze instance hardness algorithm performance machine learning uncertain reasoning approaches 
ge algorithm evolutionary computation community may benefit theoretical results experimental methodology approach applied help solve notorious ga hardness problem tally 

discovery multifractal properties bayesian networks original 
points structure jpd analyzed means theoretical machinery developed field fractals 
furthermore shown theoretical insight leads approximate algorithm finding mpe bayesian networks algorithm behavior pre theoretical grounds 
community uncertain ai bayesian networks may lead series results push boundaries sampling search algorithms 
provides promising direction general np hard problem solving mpe problem decision version np complete 
provides novel view multifractals multifractal community 
computational capability bayesian networks may provide powerful toolkit manipulate multifractal models 

research apply ant colony optimization algorithm solve mpe problem 
thoroughly investigates role skewness bayesian network inferences 
study cpt skewness influence sampling search inference algorithms performance provides new knowledge understanding working regions algorithms 

random permutation generation algorithms developed provide time uniform approach generate permutations degree differ ent presortedness measures uniformly random 
helpful wants experimentally study various sorting algorithms performances 
open questions contributions including methodology results exper theoretical investigations interesting 
give rise questions suggest number directions research 
briefly mention issues 
theoretical aspects information computation concept information play important role instance complex ity algorithm performance 
knowing problem instance specific information help design efficient algorithms 
information eas ily accessible 
instance information extract 
expensive measure information algorithm contains instances aims solve 
information affect relatively instance hardness algorithm performance 
concept information different shannon entropy kolmogorov algorithmic information defined program size complexity 
algorithm fore implementation just intuitive idea human mind information assumes wrong 
implies information computation negative 
pointed gregory chaitin cha new kind algorithmic information theory built lying plays role negative information 
exactly formalize concept negative information 
ga hardness study applied framework instance hardness algorithm perfor mance study ga hardness propose directions 
done verify hypothesis applying machine learning approach study ga hardness 
involve problem classify space possible gas performance practical controllable optimization problems 
experimental aspects random generation developed set random generation algorithms markov chain approach 
rate convergence chains theoretically analyzed 
techniques sin applied derive rapidly mixing bounds markov chains 
efficient approximate algorithms computing inv sorting number inversions best indicator sort algorithms performance expensive actual sorting process compute 
useful efficient algorithm approximates number inversions permutation 
applying machine learning methodology algorithm selection problems applied proposed learning approach sorting mpe problem 
algorithm selection technique problems 
example solve algorithm selection problem belief updating map 
belief updating problem map np pp complete par 
belief updating goal compute marginal probabilities 
map harder includes marginalization maximization 
cpt skewness plays plays important role algorithm selection mpe algorithm may true belief updating map 
interesting investigate feature important algorithm selection problems 
real time learning adaptive input instances changes real world applications distributions input instances may change time 
correspondingly meta reasoner sense distribution change input instances update reasoning model time time better accuracy achieved 
requires meta reasoner put weight input instances adaptive conducting sort real time learning 
interesting investigate related issues build autonomic algorithm selection system 
multifractal analysis study applied multifractal property joint probability space design algorithm mpe problem 
natural followup investigate design algorithms belief updating 
promising direction apply multifractal analysis solution space np hard problems max sat sp multifractal meta heuristic able apply np hard combinatorial optimization problem able solve np hard combinatorial optimization problem 
bibliography ah 
approximating maps belief networks np hard theorems 
artificial intelligence 
aks agrawal saxena 
primes 
am abu mostafa 
random problems 
journal complexity 
bet 
gas function optimizers 
phd thesis 
bg bridges goldberg 
nonuniform walsh schema trans form 
rawlins editor foundations genetic algorithms pages 
morgan kaufmann san mateo 
bh breese horvitz 
ideal reformulation belief networks 
uai pages 
bh havlin 
fractals disordered systems 
springer 
boo 
algorithm shellsort 
comm 
acm aug 
bor 
inference algorithm performance selection resources 
master thesis 
bre breiman 
bagging predictors 
machine learning 
bro brodley 
addressing selective superiority problem automatic algorithm model class selection 
proceedings tenth international conference machine learning pages 
bro brodley 
recursive automatic algorithm selection inductive learn ing 
phd thesis amherst 
cd cheng druzdzel 
ais bn adaptive importance sampling algorithm evidential reasoning large bayesian networks 
journal artificial intelligence research 
colorni dorigo maniezzo trubian 
ant system job shop scheduling 
journal operations research statistics computer science 
cfgs crawford fromherz shang 
frame line adaptive control problem solving 
aaai spring symposium intelligent distributed embedded systems stanford ca 
ch cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
cha chaitin 
length programs computing finite binary sequences 
journal association computing machinery 
cha chaitin 
algorithmic information theory 
cambridge university press cambridge 
cha charniak 
bayesian networks tears 
ai magazine 
cha chaitin 
personal communication september 
che cheeseman 
defense probability 
proceedings ninth ternational joint conference artificial intelligence pages 
morgan kaufmann 
cheeseman kanefsky taylor 
really hard problems 
proceedings twelfth international joint confer ence artificial intelligence ijcai sydney australia pages 
cl culberson 
searching ary hypercubes related graphs 
belew vose editors foundations genetic algorithms pages 
coh cohen 
coefficient agreement nominal scales 
educational psychological measurement 
coo cook 
complexity theorem proving procedures 
pro ceedings third acm symposium theory computing pages 
coo cooper 
computational complexity probabilistic inference bayesian belief networks 
artificial intelligence 
coo cook 
versus np problem 

ct cover thomas 
elements information theory 
john wiley sons 
cul culberson 
futility blind search algorithmic view free lunch 
evolutionary computation journal 
dav davidor 
epistasis variance viewpoint ga hardness 
rawlins editor foundations genetic algorithms 
morgan kaufmann 
dav davis 
extrapolation simulated annealing conver gence theory simple genetic algorithm 
phd thesis 
dcg dorigo di caro gambardella 
ant algorithms discrete optimization 
artificial life 
dg dorigo gambardella 
ant colonies traveling salesman problem 
biosystems 
dh costa hertz 
ants colour graphs 
journal oper research society 
dl dagum luby 
approximating probabilistic inference bayesian belief networks np hard 
artificial intelligence 
dor dorigo 
optimization learning natural algorithms 
phd thesis 
druzdzel 
properties joint probability distributions 
uai pages 
dsg dejong spears gordon 
markov chains analyze 
whitley vose editors foundations genetic algorithms san francisco ca 
morgan kaufmann 
dur 
algorithm random permutation 
communications association computing machinery 
castro wood 
survey adaptive sorting algorithms 
acm computing surveys 
em mandelbrot 
multifractal measures pages 
springer verlag 
fay fayyad 
induction decision trees multiple concept learning 
phd thesis ann arbor mi 
fc fung chang 
weighting integrating evidence tic simulation bayesian networks 
uncertainty artificial intelli gence pages 
ff fung 
backward simulation bayesian networks 
proceedings tenth annual conference uncertainty artificial intelligence pages san francisco ca 
morgan kaufmann publishers 
fi fayyad irani 
discretization continuous valued attributes classification learning 
proc 
th interna tional joint conference artificial intelligence ijcai 
fin fink 
solve automatically selection problem solving methods 
simmons veloso smith editors pro ceedings fourth international conference artificial intelligence planning systems pages 
fm forrest mitchell 
relative building block fitness building block hypothesis 
whitley editor foundations ge algorithms pages san mateo ca 
morgan mann 
fm forrest mitchell 
problem hard ga anomalous results explanation 
machine learning 
fre fredman 
computing length longest increasing subse quences 
discrete math 
fs freund schapire 
experiments new boosting algo rithm 
international conference machine learning pages 
gdh goldberg deb horn 
massive multimodality deception genetic algorithms 
manner manderick editors parallel problem solving nature pages 
north holland 
gg geman geman 
stochastic relaxation gibbs distribution bayesian restoration images 
ieee transactions pattern anal ysis machine intelligence 
gent macintyre prosser shaw smith walsh 

technical report report 
gj garey johnson 
computers intractability guide theory npcompleteness 
freeman 
glw glover laguna werra 
tabu search vol ume 

gol goldberg 
simple gas minimal deceptive problem pages 
pitman london 
gol goldberg 
genetic algorithms search optimization machine learning 
addison wesley 
gol goldberg 
making genetic algorithms fly lesson wright brothers 
february 
gol goldberg 
design innovation lessons com genetic algorithms 
kluwer academic publishers 
gre grefenstette 
deception considered harmful 
whitley editor foundations genetic algorithms pages san mateo ca 
morgan kaufmann 
grs gilks richardson spiegelhalter 
markov chain monte carlo practice 
chapman hall 
gs goldberg 
finite markov chain analysis genetic algorithms 
proceedings second international conference ge algorithms applications pages cambridge ma 
gs gomes selman 
algorithm portfolio design theory vs practice 
uncertainty artificial intelligence proceedings conference uai pages san francisco ca 
morgan kaufmann publishers 
guo guo 
survey algorithms real time bayesian network ference 
guo horvitz hsu santos editors aaai kdd uai joint workshop real time decision support diagnosis systems edmonton alberta canada 
hal hall 
correlation feature subset selection machine learn ing 
phd thesis 
har 
multifractals theory applications 
chapman hall crc 
hcr houstis rice verykios ramakrishnan houstis 
pythia ii knowledge database system man aging performance data recommending scientific software 
toms 
hd huang darwiche 
inference belief networks procedural guide 
intl 
approximate reasoning 
hec heckerman 
tutorial learning bayesian networks 
tech nical report microsoft research 
hen henrion 
propagating uncertainty bayesian networks tic logic sampling 
lemmer kanal editors uncertainty artificial intelligence pages 
hg horn goldberg 
genetic algorithm difficulty modality fitness landscapes 
whitley vose editors tions genetic algorithms pages san francisco ca 
morgan kaufmann 
hk horvitz klein 
reasoning metareasoning mathematical truth studies theorem proving limited resources 
proceed ings eleventh annual conference uncertainty artificial uai pages san francisco ca 
morgan kaufmann publishers 
hoa hoare 
algorithm quicksort 
comm 
acm june 
hol holland 
adaptation natural artificial systems 
university michigan press ann arbor mi usa 
hoo hooker 
needed empirical science algorithms 
operations research 
hor horvitz 
computation action bounded resources 
phd thesis 
horvitz ruan gomes kautz selman chick ering 
bayesian approach tackling hard computational problems 
proceedings seventeenth conference uncertainty artificial intelligence august 
hro 
algorithmics hard problems 
springer 
hs homer selman 
computability complexity theory 
springer verlag new york 
hsu hsu 
control inductive bias supervised learning evo computation wrapper approach 

hut 
rice theorem 
hz hsu 
automatic synthesis compression techniques heterogeneous files 
software practice experience 
ibm ibm 
high resolution time stamp facility 
ic ide cozman 
random generation bayesian networks 
brazilian symposium artificial intelligence brazil 
jf jones forrest 
fitness distance correlation measure prob lem difficulty genetic algorithms 
eshelman editor proceedings sixth international conference genetic algorithms pages san francisco ca 
morgan kaufmann 
jl john langley 
estimating continuous distributions bayesian classifiers 
uai pages 
jn nicholson 
belief network algorithms study performance domain characterization 
pricai workshops pages 
joh johnson 
catalog complexity classes 
leeuwen editor handbook theoretical computer science volume algorithms complexity pages 

joh johnson 
challenges theoretical computer science 
technical report 
joh johnson 
theoretician guide experimental analysis algo rithms 
goldwasser johnson mcgeoch ed data structures near neighbor searches methodology fifth sixth dimacs implementation challenges pages 

kd dechter 
stochastic local search bayesian networks 
international workshop artificial intelligence statistics 
kes 
large deviation weak gibbs measures tal spectra 
nonlinearity 
kgv kirkpatrick gelatt vecchi 
optimization simulated annealing 
science 
kautz horvitz ruan gomes bart selman 
dynamic restart policies 
proceedings aaai 
kj kohavi john 
wrappers feature subset selection 
artificial intelligence journal special issue relevance 
reeves 
properties fitness functions search landscapes 
rogers editors theoretical aspects evolutionary computing pages 
springer berlin 
knu knuth 
art computer programming sorting searching volume 
addison wesley 
kol kolmogorov 
approaches quantitative definition information 

kp kim pearl 
computational model combined causal diagnostic reasoning inference systems 
proceedings ijcai pages karlsruhe germany 
kp kohavi provost 
glossary terms 
machine learning 
kra kraemer 
kappa coefficient 
john wiley sons new york 
lee leeuwen editor 
handbook theoretical computer science volume algorithms complexity 
elsevier mit press 
lev levin 
lecture fundamentals computing 
ll lagoudakis littman 
algorithm selection reinforce ment learning 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
ll lagoudakis littman 
selecting right algorithm 
proceedings aaai fall symposium series uncertainty computation boston ma 
ls lauritzen spiegelhalter 
local computations prob abilities graphical structures application expert system 
royal statistic society 
lv li vitanyi 
kolmogorov complexity applications 
leeuwen editor handbook theoretical computer science volume algorithms complexity pages 

lv li vitanyi 
kolmogorov complexity applications 
springer verlag new york 
mac mackay 
monte carlo methods 
jordan editor learning graphical models 
mit press cambridge massachusetts 
man mandelbrot 
possible refinement lognormal hypothesis con distribution energy dissipation intermittent turbulence pages 
springer ny 
man mandelbrot 
fractal geometry nature 
freeman ny 
man mannila 
instance complexity sorting np complete problems 
phd thesis department computer science university 
man mandelbrot 
multifractal measures especially 

mcg mcgeoch 
analyzing algorithms simulation variance reduction techniques simulation speedups 
acm computing surveys 
mcg mcgeoch 
experimental analysis algorithms 
pardalos editors handbook global optimization volume heuristic approaches 
kluwer academic publishers 
men 
efficient bayesian network inference genetic algo rithms stochastic local search abstraction 
phd thesis 
mitchell forrest holland 
royal road gas fitness landscapes ga performance 
practice au systems proceedings european conference ar life 
mit press 
min minton 
automatically configuring constraint satisfaction programs case study 
constraints 
mit mitchell 
machine learning 
mcgraw hill 
mm 
random generation dags graph drawing 
technical report ins dutch research center mathe computer science 
mor moret 
discipline experimental algorithmics 
th dimacs challenge dimacs monograph series 
msf sanders fleischer cohen precup 
searching big oh data inferring asymptotic complexity experiments 
lecture notes computer science proceedings dagstuhl seminar experimental algorithmics 
springer verlag 
nau 
measuring ga hardness 
phd thesis antwerpen lands 
nea neapolitan 
probabilistic reasoning expert systems theory algorithms 
john wiley sons new york 
nk 
facts called ga hardness measures 
technical report centre de math matiques appliqu es palaiseau 
nk 
comparison predictive measures problem difficulty evolutionary algorithms 
ieee ec april 
nv nix vose 
modelling genetic algorithms markov chains 
annals mathematics artificial intelligence pages 
nw wilf 
combinatorial algorithms computers calculators 
academic press 
ko watanabe 
instance complexity 
journal acm 
pa anantharam 
belief propagation statistical physics 
conference information science systems 
pap papadimitriou 
computational complexity 
addison wesley 
par park 
map complexity results approximation methods 
proceed ings th annual conference uncertainty ai uai pages 
par park 
weighted max sat engines solve mpe 
proceedings th national conference artificial intelligence aaai pages 
pea pearl 
fusion propagation structuring belief networks 
artificial intelligence 
pea pearl 
evidential reasoning stochastic simulation causal mod els 
artificial intelligence 
pea pearl 
probabilistic reasoning intelligent systems networks sible inference 
morgan kaufmann san mateo ca 
qui quinlan 
induction decision trees 
machine learning 
qui quinlan 
programs machine learning 
morgan kaufmann 
raw rawlins 
compared 
freeman 
ric rice 
classes recursively enumerable sets decision problems 
transactions american mathematical society 
ric rice 
algorithm selection problem 
zelkowitz editor advances computers volume pages 

rie riedi 
multifractals 
technical report rice uni versity 
rn russell norvig 
artificial intelligence modern approach 
prentice hall englewood cliffs nj 
ru 
experimental evaluation heuristic opti mization algorithms tutorial 
journal heuristics 
rv riedi 
multifractal properties tcp traffic numer ical study 
technical report rice university 
rw rana whitley 
search binary representations counting optima 
davis de jong vose whitley editors evolutionary algorithms pages 
springer new york 
rw reeves wright 
genetic algorithms design experiments pages 
springer new york 

computational complexity genetic algorithm 
phd thesis 
san santos 
generation alternative explanations implications belief revision 
uai 
san santini 
random generation approximate counting structures 
phd thesis 
sc shimony charniak 
new algorithm finding map belief network 
uai 
sd shimony domshlak 
complexity probabilistic reasoning singly connected polytree 
bayes networks 
submitted publi cation 
sha shannon 
mathematical theory communication 
bell system technical journal 
shi shimony 
finding maps belief networks np hard 
artificial intelligence 
sin sinclair 
algorithms random generation counting markov chain approach 
birkhauser 
shwe middleton heckerman henrion horvitz lehmann 
probabilistic diagnosis reformulation internist qmr knowledge base probabilistic model ference algorithms 
methods information medicine 
sol solomonoff 
formal theory inductive inference 
information control 
sp shachter peot 
simulation approaches general prob inference belief networks 
uncertainty artificial intel volume pages new york 
elsevier science publishing 
ssw santos shimony williams 
distributed anytime ar chitecture probabilistic reasoning 
technical report en tr department electrical computer engineering air force technology 
sw stanton white 
constructive combinatorics 
springer verlag 
tay taylor 
models computation formal language 
oxford university press 
tur turing 
computable numbers application problem 
proc 
lond 
math 
soc volume pages 
vos vose 
modelling simple genetic algorithms 
foundations genetic algorithms 
morgan kaufmann 
wei weiss 
data structures algorithms analysis java 
addison wesley 
wf witten frank 
data mining practical machine learning tools techniques java implementations 
morgan kaufmann 
whi whitley 
fundamental principles deception genetic algorithms 
rawlins editor foundations genetic algorithms pages 
whi whitley 
executable model simple genetic algorithm 
foundations genetic algorithms pages 
morgan kaufmann 
wil williams 
algorithm heapsort 
comm 
acm june 
wil williams 
modelling intelligent control distributed cooperative inferencing 
phd thesis 
wm wolpert macready 
free lunch theorems search 
technical report sfi tr santa fe nm 
wm wolpert macready 
free lunch theorems opti mization 
ieee transactions evolutionary computation april 
wol wolpert 
stacked generalization 
neural networks 
yedidia freeman weiss 
bethe free energy kikuchi approx belief propagation algorithms 
technical report merl 
zilberstein 
operational rationality compilation anytime algorithms 
phd thesis 

