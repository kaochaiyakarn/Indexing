incremental singular value decomposition algorithms highly scalable recommender systems sarwar joseph konstan john riedl sarwar karypis konstan riedl cs umn edu grouplens research group army hpc research center department computer science engineering university minnesota minneapolis mn usa investigate dimensionality reduction improve performance new class data analysis software called recommender systems 
recommender systems apply knowledge discovery techniques problem making personalized product recommendations live customer interaction 
tremendous growth customers products years poses key challenges recommender systems 
producing high quality recommendations performing recommendations second millions customers products 
singular value decomposition svd recommendation algorithms quickly produce high quality recommendations undergo expensive matrix factorization steps 
propose experimentally validate technique potential incrementally build svd models promises recommender systems highly scalable 
recommender systems evolved extremely interactive environment web 
apply data analysis techniques problem helping customers find products purchase ecommerce sites 
systems especially collaborative filtering ones rapidly crucial tool commerce web 
nowadays stressed huge volume customer data existing corporate databases stressed increasing volume customer data available web 
tremendous growth customers products poses key challenges recommender systems 
challenge improve quality recommendations consumers 
consumers need recommendations trust help find products 
consumer trusts recommender system currently computer science department san jose state university san jose ca usa 
email sarwar cs edu phone purchases product finds product consumer recommender system 
challenge improve scalability collaborative filtering algorithms 
algorithms able search tens thousands potential neighbors real time demands modern commerce systems search tens millions potential neighbors 
ways challenges conflict time algorithm spends searching neighbors scalable worse quality 
reason important treat challenges simultaneously solutions discovered useful practical 
new technologies needed dramatically improve scalability recommender systems 
researchers suggest singular value decomposition svd may technology cases :10.1.1.117.3373
svd approach produced results better traditional collaborative filtering algorithm time applied movie data set 
svd recommender systems suffer serious limitation suitable large scale deployment commerce 
matrix factorization step associated systems computationally expensive major stumbling block achieving high scalability 
experiment incremental model building technique generating svd recommendations promise highly scalable producing predictive accuracy 
particular algorithm builds small pre computed svd model provides larger svd models inexpensive techniques 
experimental results suggest algorithm works twice fast producing similar prediction accuracy 
rest organized follows 
section gives brief overview prediction generation process discusses promises challenges 
section outlines incremental svd algorithm 
section presents experimental procedure results discussion 
final section provides concluding remarks research directions 
dimensionality reduction collaborative filtering section briefly discuss dimensionality reduction technique potentially prediction generation 
challenges algorithms propose incremental technique highly scalable 
promises dimensionality reduction goal cf recommendation algorithms suggest new products predict utility certain product particular customer customer previous liking opinions minded customers 
systems successful domains 
earlier papers mentioned limitations systems sparsity scalability synonymy 
weakness cf algorithms large sparse databases led explore alternative recommender system algorithms 
reviewing techniques decided try applying latent semantic indexing lsi reduce dimensionality customer product ratings matrix 
lsi dimensionality reduction technique widely information retrieval ir solve problems synonymy polysemy 
lsi uses singular value decomposition svd underlying dimensionality reduction algorithm maps nicely collaborative filtering recommender algorithm challenge 
singular value decomposition svd 
svd matrix factorization technique commonly producing low rank approximations 
matrix singular value decomposition sv defined sv dimensions respectively 
matrix diagonal matrix having nonzero entries effective dimensions matrices respectively 
orthogonal matrices diagonal matrix called singular matrix 
diagonal entries sr ofs property si ands sr columns represent orthogonal eigenvectors associated nonzero eigenvalues aa respectively 
words columns corresponding nonzero singular values span column space columns span row space matrix called left right singular vectors respectively 
svd important property particularly interesting application 
svd provides best low rank linear approximation original matrix possible retain singular values discarding entries 
term reduced matrix sk 
entries sorted sr reduction process performed singular values 
matrices reduced produce matrices uk vk respectively 
matrix uk produced removing columns matrix matrix vk produced removing multiply reduced matrices obtain matrix ak 
reconstructed matrix ak uk sk rank matrix closest approximation original matrix specifically ak minimizes frobenius norm ak rank matrices 
researchers pointed low rank approximation original space better original space due filtering small singular values introduce noise customer product :10.1.1.117.3373
dimensionality reduction approach svd useful collaborative filtering process 
svd produces set uncorrelated eigenvectors 
customer product represented corresponding eigenvector 
process dimensionality reduction may help customers rated similar products exactly products mapped space spanned eigenvectors 
outline prediction generation algorithm svd see details 
prediction generation svd 
ratings matrix decomposed reduced svd component matrices features uk sk vk prediction generated computing cosine similarities dot products pseudo customers uk 
sk pseudo products sk 
particular prediction score pi adding row average ri similarity 
formally pi 
sk ri uk 
sk svd decom position done prediction generation process involves dot product computation takes time constant 
challenges dimensionality reduction typical recommender system entire algorithm works separate steps 
step offline model building step second step line execution step 
user user similarity computation neighborhood formation thought line step cf system original user item matrix new ratings svd decomposition original matrix vk projection new items row space schematic diagram svd folding technique 
actual prediction generation online step 
usually line component time consuming computed relatively infrequently 
instance ecommerce site may compute user user similarity day week 
works ratings database static user behavior change significantly short period time 
researchers demonstrated svd algorithms neighborhood formation process cf systems highly scalable producing better results cases 
despite quality excellent line performance svd algorithms suffer serious drawback offline svd decomposition step computationally expensive 
user item matrix svd decomposition requires run time 
focus develop algorithms ensure highly scalable performance 
order achieve goal ensure online line algorithms scalable 
section devise algorithm line model building svd scalable achieving prediction quality comparable original svd scheme 
incremental svd algorithms svd property allows model incrementally computed 
method lsi researchers handle dynamic databases new terms documents may arrive model built 
shown projection additional terms documents potentially provide fairly approximation model 
extend idea build system compute suitably sized model projection method build incrementally 
resulting model perfect svd model space orthogonal quality expected potentially high performance gain 
algorithm 
projection technique known folding svd literature 
fold new users space reduced user item ma 
resulting xk dimensional user space trix ak compute coordinates vector basis uk 
size new user vector nu 
step folding compute projection projects nu space 
projection nu computed uk nu 
user set folded appending dimensional vector nu new column matrix sk shows schematic diagram folding approach 
folding technique allows devise modelbased approach svd prediction algorithms 
folding existing model uk sk vk new users items affect existing user items 
practice possible pre compute svd decomposition existing users 
user item ratings matrix matrix uk sk vk computed 
described previous section matrices prediction generation 
new set ratings added database necessary re compute low dimensional model scratch 
take advantage folding technique build incremental system potential highly scalable 
experimental evaluation section describes experimental verification incremental svd algorithm 
experimental platform data set evaluation metric computational environment 
experimental procedure followed results discussion 
experimental platform data set 
data movielens www movielens umn edu web research recommender system fall 
randomly selected users obtain ratings database considered mae determination suitable folding model size suitable model size folding model size determination threshold basis size folding svd system users rated movies 
data set converted user movie matrix rows users columns movies 
experiments divided data set training test portion 
varied training test data ratio parameter means data training algorithm test 
evaluation metric 
experiments widely popular statistical accuracy metric named mean absolute error mae deviation recommendations true userspecified values 
ratings prediction pair pi qi metric treats absolute error pi qi equally 
mae computed summing absolute errors corresponding ratings prediction pairs com puting average 
formally mae pi qi lower mae accurately recommendation engine predicts user ratings 
environment 
experiments done combination matlab running linux platform 
machine mhz intel pentium iii cpu mb ram kb cache memory 
experimental procedure experiment prediction generation algorithm svd described computing svd model decomposition matrix matrices users threshold size build initial model folding technique incrementally compute svd model additional users 
performing prediction experiments determine optimal values experimental parameters number dimensions threshold model size basis size 
perform folding step generate predictions incremental model 
investigate performance implications folding technique 
fold cross validation selecting random training test data experiments 
values experimental parameters optimal value determine optimal value number dimension performed experiment generated predictions different dimensions time 
plotted results obtained suitable value rest experiments 
optimal value basis size 
goal select basis size small produce fast model building large produce prediction quality 
start small basis size entire model computation fast due non orthogonal spaces prediction quality may 
hand large basis size defeat purpose incremental model building 
determined optimal basis size experiments fix basis size compute svd model projecting rest total basis users folding technique 
start model size go increment step 
apply mae evaluate prediction quality 
observe quality predictions improved increased basis size 
noticed basis size crossed improvement mae values relatively small 
observation select threshold basis size 
mae results discussion prediction quality experiments 
plots prediction quality results incremental model building folding technique 
report results different training test ratios 
experiment start model size equal threshold basis size folding 
projection method rest basis users svd space 
process gives approximate svd model users original svd model component matrices orthogonal 
component matrices generate prediction test set ratings 
start model size go increment 
plots show small 
value example full model size model size quality drop 
similar numbers values 
suggests inexpensive projection technique provides quality small basis size 
performance implications observation quality change dramatically varying model size suggests svd prediction generation system scalable folding method 
investigate scalability impacts record run time seconds run compute throughput performance metric 
throughput plot shows number predictions generated second different basis sizes 
plot corresponding basis size test case size 
means algorithm generates ratings seconds obtain throughput rate recommendations second 
accordingly model svd prediction folding different train test ratios folding model size prediction quality folding algorithms full model size throughput performance gain 
difference prominent lower values size larger 
folding technique shows potential useful addressing scalability challenge svd prediction generation systems 
demonstrated folding results slight quality loss due non orthogonality resultant space shows substantial performance gain 
svd recommendation generation technique leads fast online performance requiring just simple arithmetic operations recommendation computing svd expensive 
incremental svd algorithms folding significantly speed svd computation cost providing comparable prediction quality 
demonstrated incremental svd algorithms folding help recommender systems achieve high scalability providing predictive accuracy 
folding technique requires time storage space result loss quality due non orthogonality incremental svd space 
researchers pointed techniques incrementally update space retaining orthogonality 
method known svd update requires time memory folding technique 
zha points updating technique described fact inaccurate provide modified complex mathematical technique implement updating technique claim accurate 
implementation technique remains 
required understand exactly svd works recommender applications throughput predictions sec 

ways svd applied recommender systems problems including svd create lowdimensional visualizations ratings space svd identify significant products help bootstrapping recommender systems 
acknowledgments funding research provided part national science foundation iis iis iis additional funding net perceptions supported nsf ccr army research office contract da daag doe asci program army high performance computing research center contract number daah 
access computing facilities provided ah minnesota supercomputer institute 
anonymous reviewers valuable comments 
berry dumais brian 

linear algebra intelligent information retrieval 
siam review 
deerwester dumais furnas landauer harshman 

indexing latent semantic analysis 
journal american society information science 

goldberg nichols oki terry 

collaborative filtering weave information tapestry 
communications acm 
december 
gupta goldberg 

jester linear time collaborative filtering algorithm applied jokes 
proc 
acm sigir 
throughput vs folding basis size basis size throughput svd folding algorithm herlocker konstan borchers riedl 

algorithmic framework performing collaborative filtering 
proc 
acm sigir 
hill stead rosenstein furnas 

recommending evaluating choices virtual community 
proc 
resnick varian 

recommender systems 
communications acm 
pp 

resnick iacovou suchak bergstrom riedl 

grouplens open architecture collaborative filtering netnews 
proc chapel hill nc 
sarwar karypis konstan riedl 

application dimensionality reduction recommender system case study 
acm webkdd web mining commerce workshop 
sarwar karypis konstan riedl 

analysis recommendation algorithms commerce 
proc 
acm ec conference 
minneapolis mn pp 

shardanand maes 

social information filtering algorithms automating word mouth 
proc 
chi 
denver zha zhang 

matrices low rank plus shift structure partial svd latent semantic indexing 
siam journal matrix analysis applications vol 

