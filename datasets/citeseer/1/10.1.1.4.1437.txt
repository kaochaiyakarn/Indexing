self adjusting reinforcement learning ralf der michael herrmann universitat leipzig inst 
informatik postfach leipzig germany riken lab 
information representation japan variant learning algorithm automatic control exploration rate competition scheme 
theoretical approach accompanied systematic simulations chaos control task 
give interpretations algorithm context computational ecology neural networks 
reinforcement learning originally paradigm psychological learning theory established decade class powerful algorithms nonlinear control 
algorithms available try approximate value function set system states possible state transitions 
initial state function choose sequence state transitions approaches state maximal value 
order approximate value function state space explored random transitions externally increased bias transitions 
novel approach reinforcement learning state transitions compete reinforcement undergo controlled adaptation 
way algorithm robust parameter changes expected reliable complex control tasks 
section review learning scheme formal similarities algorithm suited comparison cf 
section 
approach competition state transitions locally governed fisher eigen equations described section iii 
global properties reinforcement learning discussed section iv 
results numerical simulations section interestingly novel learning algorithm allows challenging interpretations variety contexts cf 
section vi 
ii 
learning learning value function quality measure pair denotes system state control action 
adapted predict total discounted reinforcement performing action currently best possible strategy argmax 
discounting means reinforcement signal arriving time steps considered value decreased factor fl fl time horizon system 
addition state fact label knowledge learning algorithm receives system reinforcement signal 
state dependent assumes positive values goal state negative values failure state zero 
choices may useful accordance context 
updated ffl max probability maximal maximal common choice fi fi small exploration rate fi currently best action strongly favored 
write system moves deterministically state action applied state systems intrinsically stochastic apart random selection actions result operation state probability ij ij order avoid confusion ij probability choosing action state consider intrinsically deterministic systems 
learning proven find optimal strategies markovian systems state visited potentially infinitely adaptation rate ffl satisfies ffl ffl 
large state spaces actions finite computing time performance strongly depends time course ffl particularly exploration strategy terms accounts avoidance local minima 
learning requires fix time course variables ffl fi 
little known optimal cooling schemes fi 
iii 
self adjusting quasispecies algorithm population dynamics inspired approach described 
considering fixed state probabilities interpreted relative frequencies omitting index species fitness frequency species evolves hv hv average fitness individuals living site grows fitness exceeds average fitness decreases 
eq 
discrete version fisher eigen equations evolution properties 
probabilities conserved constant fisher eigen equations explicitly solvable va limit max lyapunov function satisfies dt relevance schemes clear fitness values fixed quantities determined value subsequent state reinforcement dynamics assumption constant fitness implicit fisher eigen equations hold case 
place evolution single population situation evolving subpopulation residing sites subpopulations interact backward transmission discounted reinforcement forward activation choosing control action current values 
iv 
self adjusting reinforcement learning returning reinforcement learning scheme eq 
hr multiplication eq 
hidden choice probabilities 
stochastic approximation coupled fisher eigen equations 
average rewritten def hr fl expected reinforcement state strategy stochastic action choice subsequent time steps currently best possible greedy strategy learning 
formulating eq 
stochastic approximation scheme order avoid performing explicit sum time step obtain update rule eq 
forms main equation line version algorithm 
solved simultaneously numerically convenient identical 
self adjusting reinforcement learning algorithm depends choice fixed times scales contrast cooling schemes learning 
order analyze complementary equations consider averaged versions 
changes values compared time scale recover situation discrete fisher eigen equation 
fitness constant short time scales convergence normalization properties preserved 
probabilities converge zero exploration impossible resulting strategy remains suboptimal 
hand ae look separately quasi stationary solutions 
introduce matrix ij intrinsically deterministic case analogously intrinsically stochastic case ij ij write stationary state eq 
vector notation probability matrix eigenvalues obey jj 
possible solve fl solution stable 
flm current value function solution eq 
adapting slower time scale 
values decay exponentially necessary changes due changes difficult maintain action close zero infrequently explored 
order avoid convergence local minima decay probabilities restricted small positive limit values 
way convergence rate improved effects small numbers removed 
theoretically restriction necessary probabilities recover exponentially fast small values 
comparable time scales eq 
ensures conservation probability continuous version 
case discretization stochastic approximation probabilities normalized explicitly 
simulation results applied proposed learning scheme pedagogical centering task analytical solution learning self adjusting reinforcement learning obtainable coincidence numerical results 
successful simulations cart pole problem control unstable periodic orbits mackey glass system 
systematic numerical study simple chaotic stabilization task stabilization unstable fixed point logistic map cf 

algorithm runs fixed number time steps inputs partition categories dimensional state space reinforcement signal assumes non zero values state passes near fixed point 
testing possible combinations fixed parameter turned critical 
values different orders magnitude change performance algorithm 
contrast large control task solved limited time 
smaller time scale hand convergence values quick poor solution reached sufficiently exploring state action space possible 
fig 
indicates regions average control time cent optimal stabilization time 
tau tau region parameter space allows average stabilization time random initial state cent minimal average time chaos control task 
comparison learning algorithm applied problem 
learning rate ffl exploration rate fi similar meaning self adjusting scheme 
fixed values ffl fi parameter space region allows solution control task small area large ffl relatively small fi 
fi decays linearly zero learning period ffl decays algebraically plot similar fig 
starting values ffl fi 
case successful region qualitatively similar smaller 
vi 
discussion learning algorithm allows challenging interpretations contexts computational ecology neural dynamics 
main field applications reinforcement learning algorithms control 
mainly language referred system states control actions 
computational ecology self adjusting reinforcement learning background computational ecology 
order illustrate relation give complete interpretation learning algorithm social model 
consider population traders living city different kinds traders specific trade relations certain cities 
city goods offered sale produced bought cities 
demand goods city depends value goods brought discounted factor fl cost transportation 
trade mission city turned successful usual trade relation frequently exploited update trade coordination department announce success update order increase demand 
system simple described produced goods arrive quickly possible consumers leaves evolving tree trade structure 
neural dynamics context neural systems action probabilities synaptic connections neurons mean firing rate neuron action potential send increase relate activated neurons activation connection strong 
dynamics neural implementation follows directly eqs 
stochastic counterparts 
learning rule synapses lead neurons contrast network structure real neural systems 
requiring minimal number action potentials arriving neuron activated introducing mechanisms keep total activity constant complex connectivity structures arise 
resulting neural arrangement detect coincidences arriving spikes functionally similar synfire chain architecture 
vii 
variant learning algorithm automatic control exploration rate competition scheme similar fisher known evolutionary dynamics 
self adjusting reinforcement learning algorithm different learning state action value function replaced evolution action probabilities state 
addition adjustable variables determined average possible actions time weighted adaptive transition probabilities currently optimal strategy 
algorithm simpler learning insofar parameter cooling schemes necessary 
particular self adjustment exploration rate superior fixed scheme reinforcement signals changing time 
implementation line version reinforcement learning neural network studied forthcoming 
authors rd gratefully acknowledges hospitality frontier research program riken tokyo 
der herrmann learning chaos controller 
proc 
orlando florida 
eigen mccaskill schuster molecular quasispecies 
adv 
chem 
phys 

sutton learning predict method temporal differences 
machine learning 
watkins learning delayed rewards 
phd thesis cambridge univ cambridge england 
watkins learning 
machine learning 
watkins dayan technical note learning 
machine learning 
mail der informatik uni leipzig de michael zoo riken go jp de new address mh mpi sf postfach germany 
