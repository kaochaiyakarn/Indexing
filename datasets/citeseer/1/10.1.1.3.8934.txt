journal machine learning research submitted published variable feature selection isabelle guyon isabelle com road berkeley ca usa andr elisseeff andre tuebingen mpg de empirical inference machine learning perception department max planck institute biological cybernetics bingen germany editor leslie pack kaelbling variable feature selection focus research areas application datasets tens hundreds thousands variables available 
areas include text processing internet documents gene expression array analysis combinatorial chemistry 
objective variable selection fold improving prediction performance predictors providing faster cost effective predictors providing better understanding underlying process generated data 
contributions special issue cover wide range aspects problems providing better definition objective function feature construction feature ranking multivariate feature selection efficient search methods feature validity assessment methods 
keywords variable selection feature selection space dimensionality reduction pattern discovery filters wrappers clustering information theory support vector machines model selection statistical testing bioinformatics computational biology gene expression microarray genomics proteomics text classification information retrieval 
special issue relevance including papers variable feature selection published blum langley kohavi john domains explored features 
situation changed considerably past years special issue papers explore domains hundreds tens thousands variables features new techniques proposed address challenging tasks involving irrelevant redundant variables comparably training examples 

call variable raw input variables features variables constructed input variables 
distinction terms variable feature impact selection algorithms features resulting pre processing input variables explicitly computed 
distinction necessary case kernel methods features explicitly computed see section 
isabelle guyon andr elisseeff 
guyon elisseeff examples typical new application domains serve illustration 
gene selection microarray data text categorization 
gene selection problem variables gene expression coefficients corresponding abundance mrna sample tissue number patients 
typical classification task separate healthy patients cancer patients gene expression profile 
usually fewer examples patients available altogether training testing 
number variables raw data ranges 
initial filtering usually brings number variables 
abundance mrna varies orders magnitude depending gene variables usually standardized 
text classification problem documents represented bag words vector dimension size vocabulary containing word frequency counts proper normalization variables apply 
vocabularies hundreds thousands words common initial pruning frequent words may reduce effective number words 
large document collections documents available research 
typical tasks include automatic sorting urls web directory detection unsolicited email spam 
list publicly available datasets issue see table 
potential benefits variable feature selection facilitating data visualization data understanding reducing measurement storage requirements reducing training utilization times curse dimensionality improve prediction performance 
methods put emphasis aspect point distinction special issue previous 
papers issue focus mainly constructing selecting subsets features useful build predictor 
contrasts problem finding ranking potentially relevant variables 
selecting relevant variables usually suboptimal building predictor particularly variables redundant 
conversely subset useful variables may exclude redundant relevant variables 
discussion relevance vs usefulness definitions various notions relevance see review articles kohavi john blum langley 
surveys papers special issue 
depth treatment various subjects reflects proportion papers covering problem supervised learning treated extensively unsupervised learning classification problems serve illustration regression problems vectorial input data considered 
complexity progressively introduced sections section starts describing filters select variables ranking correlation coefficients section 
limitations approaches illustrated set constructed examples section 
subset selection methods introduced section 
include wrapper methods assess subsets variables usefulness predictor 
show embedded methods implement idea proceed efficiently directly optimizing part objective function goodness fit term penalty large number variables 
turn problem feature construction goals include increasing predictor performance building compact feature subsets section 
previous steps benefit reliably assessing statistical significance relevance features 
variable feature selection briefly review model selection methods statistical tests effect section 
conclude discussion section go advanced issues section 
organization follow flow building machine learning application summarize steps may taken solve feature selection problem check list 
domain knowledge 
construct better set ad hoc features 

features commensurate 
consider normalizing 

suspect interdependence features 
expand feature set constructing conjunctive features products features computer resources allow see example section 

need prune input variables cost speed data understanding reasons 
construct disjunctive features weighted sums features clustering matrix factorization see section 

need assess features individually understand influence system number large need filtering 
variable ranking method section section anyway get baseline results 

need predictor 


suspect data dirty meaningless input patterns noisy outputs wrong class labels 
detect outlier examples top ranking variables obtained step representation check discard 

know try 
linear predictor 
forward selection method section probe method stopping criterion section norm embedded method section 
comparison ranking step construct sequence predictors nature increasing subsets features 
match improve performance smaller subset 
try non linear predictor subset 

new ideas time computational resources examples 
compare feature selection methods including new idea correlation coefficients backward selection embedded methods section 
linear non linear predictors 
select best approach model selection section 

want stable solution improve performance understanding 
sub sample data redo analysis bootstraps section 

caution reader check list heuristic 
recommendation surely valid try simplest things 

linear predictor mean linear parameters 
feature construction may render predictor non linear input variables 
variable ranking guyon elisseeff variable selection algorithms include variable ranking principal auxiliary selection mechanism simplicity scalability empirical success 
papers issue variable ranking baseline method see caruana de sa forman weston 
variable ranking necessarily build predictors 
common uses microarray analysis domain discover set drug leads see ranking criterion find genes discriminate healthy disease patients genes may code proteins proteins may drugs 
validating drug leads labor intensive problem biology outside scope machine learning focus building predictors 
consider section ranking criteria defined individual variables independently context 
correlation methods belong category 
limit supervised learning criteria 
refer reader section discussion techniques 
principle method notations consider set examples xk yk consisting input variables xk output variable yk 
variable ranking scoring function computed values xk yk convention assume high score indicative valuable variable sort variables decreasing order 
variable ranking build predictors nested subsets incorporating progressively variables decreasing relevance defined 
postpone section discussion selecting optimum subset size 
classification kohavi john variable ranking filter method preprocessing step independent choice predictor 
certain independence orthogonality assumptions may optimal respect predictor 
instance fisher criterion rank variables classification problem covariance matrix diagonal optimum fisher linear discriminant classifier duda 
variable ranking optimal may preferable variable subset selection methods computational statistical scalability computationally efficient requires computation scores sorting scores statistically robust overfitting introduces bias may considerably variance hastie 
introduce additional notation input vector interpreted realization random vector drawn underlying unknown distribution denote xi random variable corresponding th component similarly random variable outcome realization 
denote xi dimensional vector containing realizations th variable training examples dimensional vector containing target values 

ratio class variance class variance 

similarity variable ranking ordered fs algorithm ng indicates sample complexity may logarithmic number irrelevant features compared power law wrapper subset selection methods 
mean variable ranking tolerate number irrelevant variables exponential number training examples 
correlation criteria variable feature selection consider prediction continuous outcome pearson correlation coefficient defined cov xi var xi var cov designates covariance var variance 
estimate xk xi yk xk xi yk bar notation stands average index coefficient cosine vectors xi centered mean subtracted 
derived may assuming input values realizations random variable 
linear regression coefficient determination square represents fraction total variance mean value explained linear relation xi variable ranking criterion enforces ranking goodness linear fit individual variables 
extended case class classification class label mapped value 
shown closely related fisher criterion test criterion similar criteria see hastie 
developed section link test shows score may test statistic assess significance variable 
correlation criteria detect linear dependencies variable target 
simple way lifting restriction non linear fit target single variables rank goodness fit 
risk overfitting alternatively consider non linear preprocessing squaring square root log inverse simple correlation coefficient 
correlation criteria microarray data analysis illustrated issue weston 

single variable classifiers mentioned ranking criterion regression enforces ranking goodness linear fit individual variables 
extend classification case idea selecting variables individual predictive power criterion performance classifier built single variable 
example value variable negative account class polarity discriminant function 
classifier obtained setting threshold value variable mid point center gravity classes 

variant idea mean squared error variables comparable scales comparison mean squared errors meaningless 
variant rank variables positively correlated variables top ranked negatively correlated variables bottom ranked 
method choose subset variables proportion positively negatively correlated variables 
guyon elisseeff predictive power variable measured terms error rate 
various criteria defined involve false positive classification rate fpr false negative classification rate fnr 
tradeoff fpr fnr monitored simple example varying threshold 
roc curves plot hit rate fpr function false alarm rate fnr instrumental defining criteria break point hit rate threshold value corresponding fpr fnr area curve area roc curve 
case large number variables separate data perfectly ranking criteria classification success rate distinguish top ranking variables 
prefer correlation coefficient statistic margin distance examples opposite classes closest variable 
criteria described section extend case binary variables 
forman presents issue extensive study criteria binary variables applications text classification 
information theoretic ranking criteria approaches variable selection problem information theoretic criteria proposed reviewed issue dhillon forman torkkola 
rely empirical estimates mutual information variable target xi xi log xi dxdy xi xi probability densities xi xi joint density 
criterion measure dependency density variable xi density target difficulty densities xi xi unknown hard estimate data 
case discrete nominal variables probably easiest integral sum xi log xi xi xi 
probabilities estimated frequency counts 
example class problem variable takes values represents class prior probabilities frequency counts xi represents distribution input variable frequency counts xi probability joint observations frequency counts 
estimation obviously harder larger numbers classes variable values 
case continuous variables possibly continuous targets hardest 
consider discretizing variables approximating densities non parametric method parzen windows see torkkola 
normal distribution estimate densities bring back estimating covariance xi giving criterion similar correlation coefficient 
variable feature selection small revealing examples series small examples outline usefulness limitations variable ranking techniques situations variable dependencies ignored 
presumably redundant variables help 
common criticism variable ranking leads selection redundant subset 
performance possibly achieved smaller subset complementary variables 
may wonder adding presumably redundant variables result performance gain 
consider classification problem 
class drew random examples variables drawn independently normal distribution standard deviation 
class centers placed coordinates 
shows scatter plot dimensional space input variables 
show histograms projections examples axes 
facilitate reading scatter plot shown twice axis exchange 
shows scatter plots degree rotation 
representation axis projection provides better separation classes standard deviation classes distance centers projection 
equivalently rescale axis dividing obtain feature average input variables distance centers class standard deviation reduced factor 
surprising averaging random variables obtain reduction standard deviation factor noise reduction consequently better class separation may obtained adding variables presumably redundant 
variables independently identically distributed truly redundant 
correlation impact variable redundancy 
notion redundancy correlation 
previous example spite fact examples respect class conditional distributions variables correlated separation class center positions 
may wonder variable redundancy affected adding class variable correlation 
class centers positioned similarly previous example coordinates added variable variance 
consider cases direction class center line standard deviation class conditional distributions perpendicular direction small value 
construction goes zero input variables separation power case example standard deviation class distributions distance class centers 
feature constructed sum input variables better separation power standard deviation class center separation simple scaling change separation power 
limit perfect variable correlation zero variance direction perpendicular class center line single variables provide guyon elisseeff information gain presumably redundant variables 
class problem independently identically distributed variables 
class gaussian distribution covariance 
example degree rotation showing combination variables yields separation improvement factor 
variables truly redundant 
intra class covariance 
projection axes distributions variables previous example 
class conditional distributions high covariance direction line class centers 
significant gain separation variables just 
class conditional distributions high covariance direction perpendicular line class centers 
important separation gain obtained variables 
variable feature selection separation sum variables 
perfectly correlated variables truly redundant sense additional information gained adding 
contrast example principal direction covariance matrices class conditional densities perpendicular class center line 
case gained adding variables example 
notices spite great complementarity sense perfect separation achieved dimensional space spanned variables variables anti correlated 
anti correlation obtained making class centers closer increasing ratio variances class conditional distributions 
high variable correlation anti correlation mean absence variable complementarity 
examples variables distribution examples projection axis 
methods score variables individually independently loss determine combination variables give best performance 
variable useless useful 
concern multivariate methods prone overfitting 
problem aggravated number variables select large compared number examples 
tempting variable ranking method filter promising variables multivariate method 
may wonder potentially lose valuable variables filtering process 
constructed example example class conditional distributions identical covariance matrices principal directions oriented diagonally 
class centers separated axis 
variable useless 
dimensional separation better separation useful variable 
variable completely useless provide significant performance improvement taken 
question variables useless provide separation taken 
constructed example case inspired famous xor problem 
drew examples classes gaussians placed corners square coordinates 
class labels clumps attributed truth table logical xor function 
notice projections axes provide class separation 
dimensional space classes easily separated albeit linear decision function 
variables useless useful 

xor problem referred bit parity problem generalizable dimensions bit parity problem 
related problem chessboard problem classes pave space squares uniformly distributed examples alternating class labels 
problem generalizable multi dimensional case 
similar examples papers issue perkins :10.1.1.13.4021

incidentally variables uncorrelated 
guyon elisseeff variable useless useful 
variable completely overlapping class conditional densities 
jointly variable improves class separability compared variable 
xor chessboard problems 
classes consist disjoint clumps projection axes class conditional densities overlap perfectly 
individual variables separation power 
taken variables provide class separability variable subset selection previous section examples illustrate usefulness selecting subsets variables predictive power opposed ranking variables individual predictive power 
turn problem outline main directions taken tackle 
essentially divide wrappers filters embedded methods 
wrappers utilize learning machine interest black box score subsets variable predictive power 
filters select subsets variables pre processing step independently chosen predictor 
embedded methods perform variable selection process training usually specific learning machines 
wrappers embedded methods wrapper methodology popularized kohavi john offers simple powerful way address problem variable selection regardless chosen learning machine 
fact learning machine considered perfect black box method lends shelf machine learning software packages 
general formulation wrapper methodology consists prediction performance learning machine assess relative usefulness subsets variables 
practice needs define search space possible variable subsets ii variable feature selection assess prediction performance learning machine guide search halt iii predictor 
exhaustive search conceivably performed number variables large 
problem known np hard kann search quickly computationally intractable 
wide range search strategies including best branch bound simulated annealing genetic algorithms see kohavi john review 
performance assessments usually done validation set cross validation see section 
illustrated special issue popular predictors include decision trees na bayes square linear predictors support vector machines 
wrappers criticized brute force method requiring massive amounts computation necessarily 
efficient search strategies may devised 
strategies necessarily mean sacrificing prediction performance 
fact appears converse cases coarse search strategies may alleviate problem overfitting illustrated instance issue 
greedy search strategies particularly computationally advantageous robust overfitting 
come flavors forward selection backward elimination 
forward selection variables progressively incorporated larger larger subsets backward elimination starts set variables progressively eliminates promising ones 
methods yield nested subsets variables 
learning machine black box wrappers remarkably universal simple 
embedded methods incorporate variable selection part training process may efficient respects better available data needing split training data training validation set reach solution faster avoiding retraining predictor scratch variable subset investigated 
embedded methods new decision trees cart instance built mechanism perform variable selection breiman 
sections devoted families embedded methods illustrated algorithms published issue 
nested subset methods embedded methods guide search estimating changes objective function value incurred making moves variable subset space 
combined greedy search strategies backward elimination forward selection yield nested subsets variables 
call number variables selected algorithm step value objective function trained learning machine variable subset 
predicting change objective function obtained 
name greedy comes fact revisits decisions include exclude variables light new decisions 

algorithms section generally benefit variable normalization internal normalization mechanism gram schmidt orthogonalization procedure guyon elisseeff 
finite difference calculation difference computed variables candidates addition removal 

quadratic approximation cost function method originally proposed prune weights neural networks lecun 
backward elimination variables pruning input variable weights wi 
second order taylor expansion 
optimum order term neglected yielding variable variation change weight wi corresponds removing variable 
sensitivity objective function calculation absolute value square derivative respect xi respect wi 
training algorithms lend finite differences method exact differences computed efficiently retraining new models candidate variable 
case linear square model gram schmidt procedure permits performance forward variable selection adding step variable decreases mean squared error 
papers issue devoted technique rivals personnaz 
algorithms kernel methods approximations difference computed effi ciently 
kernel methods learning machines form kk xk kernel function measures similarity xk smola 
variation computed keeping values constant 
procedure originally proposed svms guyon issue baseline method weston 
optimum brain damage procedure method mentioned issue rivals personnaz 
case linear predictors particularly simple 
authors algorithm advocate magnitude weights wi pruning criterion 
linear predictors trained objective function quadratic wi criteria equivalent 
case instance linear square model xk yk linear svm optimum margin classifier minimizes constraints vapnik 
interestingly linear svms finite difference method method sensitivity method method boil selecting variable smallest wi elimination step 
sensitivity objective function changes wi method devise forward selection procedure issue perkins 
applications procedure linear model cross entropy objective function 
formulation proposed criterion absolute value wi wi xk 
case linear model criterion simple geometrical interpretation dot product gradient objective function respect margin values vector wi xk cross entropy loss function interesting variant sensitivity analysis method obtained replacing objective function leave cross validation error 
learning machines variable feature selection objective functions approximate exact analytical formulas leave oneout error known 
issue case linear square model rivals personnaz svms treated 
approximations nonlinear squares computed dreyfus 
proposal train non linear svms boser vapnik regular training procedure select features backward elimination rfe guyon :10.1.1.103.1189
variable ranking criterion computed sensitivity objective function leave bound 
direct objective optimization lot progress issue formalize objective function variable selection find algorithms optimize 
generally objective function consists terms compete goodness fit maximized number variables minimized 
approach bears similarity part objective functions consisting goodness fit term regularization term particularly effect regularization term shrink parameter space 
correspondence formally established weston 
particular case classification linear predictors svm framework boser vapnik :10.1.1.103.1189
shrinking regularizers type wp norm 
limit norm just number weights number variables 
weston proceed showing norm formulation svms solved approximately simple modification vanilla svm algorithm 
train regular linear svm norm norm regularization 

re scale input variables multiplying absolute values components weight vector obtained 

iterate steps convergence 
method reminiscent backward elimination procedures smallest wi 
variable normalization important method properly 
weston note algorithm approximately minimizes norm practice may generalize better algorithm really minimize norm provide sufficient regularization lot variance remains optimization problem multiple solutions 
need additional regularization stressed perkins 

authors part objective function includes goodness fit regularization term norm norm penalty large numbers variables norm 
authors propose computationally efficient forward selection method optimize objective 
issue bi 
uses norm svms iterative multiplicative updates 
authors find application norm minimization suffices drive weights zero 
approach taken context square regression authors tibshirani 
number variables reduced backward elimination 
guyon elisseeff knowledge algorithm proposed directly minimize number variables non linear predictors 
authors substituted problem variable selection variable scaling jebara jaakkola weston 
variable scaling factors hyper parameters adjusted model selection 
scaling factors obtained assess variable relevance 
variant method consists adjusting scaling factors gradient descent bound leave error weston 
method baseline method weston 
issue 
filters subset selection justifications filters subset selection put forward special issue 
argued compared wrappers filters faster 
proposed efficient embedded methods competitive respect 
argument filters mutual information criteria provide generic selection variables tuned learning machine 
compelling justification filtering preprocessing step reduce space dimensionality overcome overfitting 
respect reasonable wrapper embedded method linear predictor filter train complex non linear predictor resulting variables 
example approach bi 
linear norm svm variable selection non linear norm svm prediction 
complexity linear filters ramped adding selection process products input variables monomials polynomial retaining variables part selected monomial 
predictor neural network eventually substituted polynomial perform predictions selected variables rivals personnaz 
cases may contrary want reduce complexity linear filters overcome overfitting problems 
number examples small compared number variables case microarray data instance may need resort selecting variables correlation coefficients see section 
information theoretic filtering methods markov blanket algorithms koller sahami constitute broad family 
justification classification problems measure mutual information rely prediction process provides bound error rate prediction scheme distribution 
illustration methods issue problem variable subset selection 
refer interested reader koller sahami 
mutual information criteria individual variable ranking covered section application feature construction selection illustrated section 

markov blanket variable xi set variables including xi render xi unnecessary 
markov blanket xi safely eliminated 
furthermore backward elimination procedure remain unnecessary stages 
variable feature selection feature construction space dimensionality reduction applications reducing dimensionality data selecting subset original variables may advantageous reasons including expense making storing processing measurements 
considerations concern means space dimensionality reduction considered 
art machine learning starts design appropriate data representations 
better performance achieved features derived original input 
building feature representation opportunity incorporate domain knowledge data application specific 
number generic feature construction methods including clustering basic linear transforms input variables pca svd lda sophisticated linear transforms spectral transforms fourier hadamard wavelet transforms convolutions kernels applying simple functions subsets variables products create monomials 
distinct goals may pursued feature construction achieving best reconstruction data efficient making predictions 
problem unsupervised learning problem 
closely related data compression lot algorithms fields 
second problem supervised 
reasons select features unsupervised manner problem supervised 
possibly problems text processing applications come unlabelled data labelled data 
unsupervised feature selection prone overfitting 
issue papers address problem feature construction 
take information theoretic approach problem 
illustrate clustering construct features dhillon provides new matrix factorization algorithm tishby provides supervised means learning features variety models torkkola 
addition papers main focus directed variable selection address selection monomials polynomial model hidden units neural network rivals personnaz addresses implicit feature selection non linear kernel methods polynomial kernels weston 
clustering clustering long feature construction 
idea replace group similar variables cluster centroid feature 
popular algorithms include means hierarchical clustering 
review see textbook duda 

clustering usually associated idea unsupervised learning 
useful introduce supervision clustering procedure obtain discriminant features 
idea distributional clustering pereira developed papers issue 
distributional clustering rooted information bottleneck ib theory tishby 

call random variable representing constructed features ib method seeks minimize mutual information preserving mutual information 
global objective function built introducing lagrange multiplier 
method searches guyon elisseeff solution achieves largest possible compression retaining essential information target 
text processing applications usual targets techniques 
patterns full documents variables come bag words representation variable associated word proportional fraction documents word appears 
application feature construction clustering methods group words documents 
text categorization tasks supervision comes knowledge document categories 
introduced replacing variable vectors containing document frequency counts shorter variable vectors containing document category frequency counts words represented distributions document categories 
simplest implementation idea dhillon 
issue 
uses means clustering variables represented vector document category frequency counts 
non symmetric similarity measure kullback leibler divergence xj xi exp xk xk xk 
sum index runs document categories 
elaborate approach taken 
soft version means allowing words belong clusters progressively divide clusters varying lagrange multiplier monitoring tradeoff 
way documents represented distribution word centroids 
methods perform 
mention words belonging clusters hinting hard cluster assignment may sufficient 
matrix factorization widely method feature construction singular value decomposition svd 
goal svd form set features linear combinations original variables provide best possible reconstruction original data square sense duda 
unsupervised method feature construction 
issue tishby presents information theoretic unsupervised feature construction method sufficient dimensionality reduction sdr 
informative features extracted solving optimization problem monitors tradeoff data reconstruction data compression similar information bottleneck tishby 
features lagrange multipliers objective optimized 
non negative matrices dimension representing joint distribution random variables instance occurrence words documents considered 
features extracted information theoretic projections yielding reconstructed matrix special exponential form exp 
set features matrix th column ones matrix th column ones normalization coefficient 
similarly svd solution shows symmetry problem respect patterns variables 
supervised feature selection review approaches selecting features cases features distinguished variables appear simultaneously system nested subset methods 
number learning machines extract features part learning process 
include neural networks internal nodes feature extrac variable feature selection tors 
node pruning techniques lecun 
feature selection algorithms 
gram schmidt orthogonalization issue alternative 
filters 
torkkola proposes filter method constructing features mutual information criterion 
author maximizes dimensional feature vectors target vectors modelling feature density function parzen windows allows compute derivatives transform independent 
combining transform dependent derivatives gradient descent algorithm optimize parameters transform need linear wt wt wt 
direct objective optimization 
kernel methods possess implicit feature space revealed kernel expansion feature vector possibly infinite dimension 
selecting implicit features may improve generalization change running time help interpreting prediction function 
issue weston 
propose method selecting implicit kernel features case polynomial kernel framework minimization norm 
validation methods group section issues related sample performance prediction generalization prediction model selection 
involved various aspects variable feature selection determine number variables significant guide halt search variable subsets choose hyperparameters evaluate final performance system 
distinguish problem model selection evaluating final performance predictor 
purpose important set aside independent test set 
remaining data training performing model selection 
additional experimental sophistication added repeating entire experiment drawings test set 
perform model selection including variable feature selection hyperparameter optimization data testing may split fixed training validation sets various methods cross validation 
problem brought back estimating significance differences validation errors 
fixed validation set statistical tests validity doubtful crossvalidation independence assumptions violated 
discussion issues see instance dietterich nadeau bengio 
sufficiently examples may necessary split training data comparisons training errors statistical tests see rivals personnaz issue 
cross validation extended time series data assumptions 
fact author uses quadratic measure divergence usual mutual information 

limit test set example leave carried outer loop outside feature variable selection process estimate final performance predictor 
computationally expensive procedure cases data extremely scarce 
guyon elisseeff hold anymore possible estimate generalization error confidence intervals see bengio issue 
choosing fraction data training validation open problem 
authors resort leave cross validation procedure known high variance estimator generalization error vapnik give overly optimistic results particularly data properly independently identically sampled true distribution 
leave procedure consists removing example training set constructing predictor basis remaining training data testing removed example 
fashion tests examples training data averages results 
previously mentioned exist exact approximate formulas leave error number learning machines dreyfus rivals personnaz 
leave formulas viewed corrected values training error 
types penalization training error proposed literature see vapnik hastie 
new family methods called metric methods proposed schuurmans :10.1.1.51.9998
bengio issue illustrates application variable selection 
authors unlabelled data readily available application considered time series prediction horizon 
consider models fa fb trained nested subsets variables call fa fb discrepancy models 
criterion involves ratio du fa fb dt fa fb du fa fb computed unlabelled data dt fa fb computed training data 
ratio significantly larger sheds doubt usefulness variables subset variable ranking nested subset ranking methods sections statistical approach taken 
idea introduce probe data random variable 
roughly speaking variables relevance smaller equal probe discarded 
bi 
consider simple implementation idea introduce data additional fake variables drawn randomly gaussian distribution submit variable selection process true variables 
subsequently discard variables relevant fake variables weight magnitude criterion 

propose sophisticated method gram schmidt forward selection method 
gaussian distributed probe provide analytical formula compute rank probe associated risk accepting irrelevant variable 
non parametric variant probe method consists creating fake variables randomly shuffling real variable vectors 
forward selection process fake variables disturb selection fake variables discarded encountered 
step forward selection process call ft fraction true variables selected far true variables ff fraction fake variables encountered fake variables 
halting criterion place threshold ratio ff ft upper bound fraction falsely relevant variables subset selected far 
method variable ranking 
parametric version gaussian distributions statistic ranking criterion test 
variable feature selection advanced topics open problems variance variable subset selection methods variable subset selection sensitive small perturbations experimental conditions 
data redundant variables different subsets variables identical predictive power may obtained initial conditions algorithm removal addition variables training examples addition noise 
applications want purposely generate alternative subsets subsequent stage processing 
find variance undesirable variance symptom bad model generalize ii results reproducible iii subset fails capture picture 
method stabilize variable selection explored issue bootstraps bi 
variable selection process repeated sub samples training data 
union subsets variables selected various bootstraps taken final stable subset 
joint subset may predictive best bootstrap subset 
analyzing behavior variables various bootstraps provides insight described 
particular index relevance individual variables created considering frequently appear bootstraps 
related ideas described context bayesian variable selection jebara jaakkola ng jordan vehtari lampinen 
distribution population models various variable subsets estimated 
variables ranked marginal distribution reflecting appear important subsets associated probable models 
variable ranking context section limited presenting variable ranking methods criterion computed single variables ignoring context 
section introduced nested subset methods provide useful ranking subsets individual variables variables may low rank redundant highly relevant 
bootstrap bayesian methods section may instrumental producing variable ranking incorporating context 
relief algorithm uses approach nearest neighbor algorithm kira rendell 
example closest example class nearest hit closest example different class nearest selected 
score th variable computed average examples magnitude difference distance nearest hit distance nearest projection th variable 
unsupervised variable selection target provided want select set significant variables respect defined criterion 
obviously criteria problems stated 
number variable ranking criteria useful applications including saliency entropy smoothness density reliability 
variable guyon elisseeff forward backward selection 
variables example third separates classes best bottom right histogram 
best candidate forward selection process 
variables better taken subset including 
backward selection method may perform better case 
salient high variance large range compared 
variable high entropy distribution examples uniform 
time series variable smooth average local curvature moderate 
variable high density region highly correlated variables 
variable reliable measurement error bars computed repeating measurements small compared variability variable values quantified anova statistic 
authors attempted perform variable feature selection clustering applications see xing karp ben hur guyon 
forward vs backward selection argued forward selection computationally efficient backward elimination generate nested subsets variables 
defenders backward elimination argue weaker subsets forward selection importance variables assessed context variables included 
illustrate argument example 
example variable separates variable feature selection classes better ones taken selected forward selection 
step complemented variables resulting class separation dimensions obtained jointly variables discarded step 
backward selection method may forward selection eliminating step variable provides best separation retain variables perform best 
reason need get single variable backward elimination gotten rid variable works best 
multi class problem variable selection methods treat multi class case directly decomposing class problems methods mutual information criteria extend naturally multi class case see issue dhillon torkkola 
multi class variable ranking criteria include fisher criterion ratio class variance class variance 
closely related statistic anova test way implementing probe method section multi class case 
wrappers embedded methods depend capability classifier handle multi class case 
examples classifiers include linear discriminant analysis lda multi class version fisher linear discriminant duda multi class svms see weston 
may wonder advantageous multi class methods variable selection 
hand contrary generally admitted classification multiclass setting sense easier variable selection class case 
larger number classes random set features provide separation 
illustrate point consider simple example features drawn independently distribution target assume features correspond rolling die faces times number samples 
probability fixed feature exactly finding feature corresponds target embedded sea noisy features easier large 
hand forman points issue case uneven distributions classes multi class methods may represent abundant easily separable classes 
possible alternative mix ranked lists class problems 
weston 
propose mixing strategy 
selection examples dual problems feature selection construction pattern selection construction 
symmetry problems explicit tishby issue 
likewise 
weston 
point algorithm applies selection examples kernel methods 
pointed similarity complementarity problems blum langley 
particular mislabeled examples may induce choice wrong variables 
conversely labeling highly reliable selecting wrong variables associated guyon elisseeff confounding factor may avoided focusing informative patterns close decision boundary guyon 
inverse problems special issue concentrates problem finding small subset variables useful build predictor 
applications particularly bioinformatics necessarily goal variable selection 
diagnosis problems instance important identify factors triggered particular disease unravel chain events causes symptoms 
reverse engineering system produced data challenging task building predictor 
readers interested issues consult literature gene networks conference proceedings pacific symposium biocomputing psb intelligent systems molecular biology conference ismb causality inference literature see pearl 
heart problem distinction correlation causality 
observational data data available machine learning researchers allow observe correlations 
example observations correlations expression profiles genes profiles symptoms leap faith deciding gene activated turn triggered symptom 
issue caruana de sa presents interesting ideas variables discarded variable selection additional outputs neural network 
show improved performance synthetic real data 
analysis supports idea variables efficiently outputs inputs 
step distinguishing causes consequences 
developments variable feature selection addressed problem pragmatic point view improving performance predictors 
met challenge operating input spaces variables 
sophisticated wrapper embedded methods improve predictor performance compared simpler variable ranking methods correlation methods improvements significant domains large numbers input variables suffer curse dimensionality multivariate methods may overfit data 
domains applying method automatic feature construction yields improved performance compact set features 
methods proposed special issue tested wide variety data sets see table limits possibility making comparisons papers 
includes organization benchmark 
approaches diverse motivated various theoretical arguments unifying theoretical framework lacking 
shortcomings important starting new problem baseline performance values 
recommend linear predictor choice linear svm select variables alternate ways variable ranking method correlation coefficient mutual information nested subset selection method performing forward backward selection multiplicative updates 
road connections need variable feature selection data set description patterns variables classes linear artificial linear reg multi cluster artificial non linear ps chemistry reg bt uci ml repository lvq pak phoneme data bench 
uci delve statlog ra microarray cancer classif 
microarray gene classification aston univ pipeline transport nips unlabeled data reg ri newsgroup news postings text filtering trec ir datasets med cran cisi reuters newswire docs 
open dir 
proj 
web directory table publicly available data sets special issue 
approximate numbers ranges patterns variables classes effectively provided 
classes column indicates reg regression problems number queries information retrieval ir problems 
artificial data sets fraction variables relevant ranges 
initial author provided bk bn bengio bt bennett caruana dhillon forman perkins re ra ri rivals torkkola weston 
please check jmlr web site additions preprocessed data 
www tuebingen mpg de bs people weston www com isabelle projects nips artificial zip nis www lanl gov data jmlr www rpi edu bij html www ics uci edu mlearn mlrepository html www cis hut fi research software shtml ida gmd de data benchmarks htm www aston ac uk gtm html cis ca nips kdd ics uci edu databases newsgroups newsgroups html trec nist gov data html filtering track collection www cs utk edu lsi www com resources reuters dmoz org www cs utexas edu users dmoz txt www cs technion ac il thesis html guyon elisseeff problems variable feature selection experimental design active learning effort move away observational data experimental data address problems causality inference 
kann 
approximation minimizing non zero variables unsatisfied relations linear systems 
theoretical computer science 
el yaniv tishby winter 
distributional word clusters vs words text categorization 
jmlr issue 
ben hur guyon 
detecting stable clusters principal component analysis 
editors methods molecular biology pages 
press 
bengio 
extensions metric model selection 
jmlr issue 
bi bennett embrechts song 
dimensionality reduction sparse support vector machines 
jmlr issue 
blum langley 
selection relevant features examples machine learning 
artificial intelligence december 
boser guyon vapnik 
training algorithm optimal margin classifiers 
fifth annual workshop computational learning theory pages pittsburgh 
acm 
breiman friedman olshen stone 
classification regression trees 
wadsworth brooks 
caruana de sa 
variables variable selection discards 
jmlr issue 
dhillon kumar 
divisive information theoretic feature clustering algorithm text classification 
jmlr issue 
dietterich 
approximate statistical test comparing supervised classification learning algorithms 
neural computation 
duda hart stork 
pattern classification 
john wiley amp sons usa nd edition 
golub molecular classification cancer class discovery class prediction gene expression monitoring 
science 
forman 
extensive empirical study feature selection metrics text classification 
jmlr issue 
variable feature selection cristianini duffy schummer haussler 
support vector machine classification validation cancer tissue samples microarray expression data 
bioinformatics 
tishby 
sufficient dimensionality reduction 
jmlr issue 

adaptive scaling feature selection svms 
nips 
guyon weston vapnik 
gene selection cancer classification support vector machines 
machine learning 
hastie tibshirani friedman 
elements statistical learning 
springer series statistics 
springer new york 
jebara jaakkola 
feature selection dualities maximum entropy discrimination 
th annual conference uncertainty artificial intelligence 
kira rendell 
practical approach feature selection 
sleeman edwards editors international conference machine learning pages aberdeen july 
morgan kaufmann 
kohavi john 
wrappers feature selection 
artificial intelligence december 
koller sahami 
optimal feature selection 
th international conference machine learning pages july 
lecun denker solla howard jackel 
optimal brain damage 
touretzky editor advances neural information processing systems ii san mateo ca 
morgan kaufmann 
dreyfus 
withdrawing example training set analytic estimation effect nonlinear parameterized model 
neurocomputing letters 
nadeau bengio 
inference generalization error 
machine learning appear 
ng 
feature selection learning exponentially irrelevant features training examples 
th international conference machine learning pages 
morgan kaufmann san francisco ca 
ng jordan 
convergence rates voting gibbs classifier application bayesian feature selection 
th international conference machine learning 
pearl 
causality 
cambridge university press 
guyon elisseeff pereira tishby lee 
distributional clustering english words 
proc 
meeting association computational linguistics pages 
perkins theiler 
grafting fast incremental feature selection gradient descent function space 
jmlr issue 

variable selection svm criteria 
jmlr issue 

overfitting making comparisons variable selection methods 
jmlr issue 
rivals personnaz 
mlps mono layer polynomials multi layer perceptrons non linear modeling 
jmlr issue 
smola 
learning kernels 
mit press cambridge ma 
schuurmans 
new metric approach model selection 
th innovative applications artificial intelligence conference pages 
dreyfus dubois 
ranking random feature variable feature selection 
jmlr issue 
tibshirani 
regression selection shrinkage lasso 
technical report stanford university palo alto ca june 
tishby pereira bialek 
information bottleneck method 
proc 
th annual allerton conference communication control computing pages 
torkkola 
feature extraction non parametric mutual information maximization 
jmlr issue 
tibshirani chu 
significance analysis microarrays applied radiation response 
pnas april 
vapnik 
estimation dependencies empirical data 
springer series statistics 
springer 
vapnik 
statistical learning theory 
john wiley amp sons 
vehtari lampinen 
bayesian input variable selection posterior probabilities expected utilities 
report 
weston tipping 
zero norm linear models kernel methods 
jmlr issue 
weston mukherjee chapelle pontil poggio vapnik 
feature selection svms 
nips 
xing karp 
cliff clustering high dimensional microarray data iterative feature filtering normalized cuts 
th international conference intelligence systems molecular biology 

