learning means greg charles elkan cs ucsd edu department computer science engineering university california san diego la jolla california clustering dataset right number clusters obvious choosing automatically hard algorithmic problem 
improved algorithm learning clustering 
means algorithm statistical test hypothesis subset data follows gaussian distribution 
means runs means increasing hierarchical fashion test accepts hypothesis data assigned means center gaussian 
key advantages hypothesis test limit covariance data compute full covariance matrix 
additionally means requires intuitive parameter standard statistical significance level results experiments showing algorithm works better method bic penalty model complexity 
experiments show bic ineffective scoring function penalize strongly model complexity 
related clustering algorithms useful tools data mining compression probability density estimation important tasks 
clustering algorithms require user specify number clusters called clear best value shows examples improperly chosen 
choosing ad hoc decision prior knowledge assumptions practical experience 
choosing difficult data dimensions clusters separated 
center clustering algorithms particular means gaussian expectationmaximization usually assume cluster adheres unimodal distribution gaussian 
methods center model subset data follows unimodal distribution 
multiple centers describe data drawn mode centers needlessly complex description data fact multiple centers capture truth subset center 
simple algorithm called means discovers appropriate statistical test deciding split means center centers 
describe examples experimental results show new algorithm clusterings improperly chosen 
dark crosses means centers 
left centers 
right centers center sufficient representing data 
general center represent gaussian cluster 
successful 
technique useful applicable clustering algorithms means consider means algorithm simplicity 
algorithms proposed previously determine automatically 
method previous methods wrappers means clustering algorithm fixed wrapper methods splitting merging rules centers increase decrease algorithm proceeds 
pelleg moore proposed regularization framework learning call means :10.1.1.19.3377
algorithm searches values scores clustering model called bayesian information criterion bic cjx jc log jc log likelihood dataset model number parameters model dimensionality cluster centers number points dataset 
means chooses model best bic score data 
aside bic scoring functions available 
bischof minimum description length mdl framework description length measure data fit model 
algorithm starts large value removes centers reduces choice reduces description length 
steps reducing means algorithm optimize model fit data 
hierarchical clustering algorithms methods may employed determine best number clusters 
build merging tree dendrogram data cluster distance metric search areas tree stable respect inter intra cluster distances section 
method estimating best applied domain specific knowledge human intuition 
gaussian means means algorithm means algorithm starts small number means centers grows number centers 
iteration algorithm splits centers data appear come gaussian distribution 
round splitting run means entire dataset centers refine current solution 
initialize just choose larger value prior knowledge range means repeatedly decisions statistical test data assigned center 
data currently assigned means center appear gaussian want represent data center 
data appear algorithm means initial set centers usually xg 
kmeans 
fx jclass jg set datapoints assigned center statistical test detect fx jclass jg follow gaussian distribution confidence level 
data look gaussian keep replace centers 
repeat step centers added 
gaussian want multiple centers model data properly 
algorithm run means multiple times times finding centers time complexity times means 
means algorithm implicitly assumes datapoints cluster spherically distributed center 
gaussian expectation maximization algorithm assumes datapoints cluster multidimensional gaussian distribution covariance matrix may may fixed shared 
gaussian distribution test valid covariance matrix assumption 
test accounts number datapoints tested incorporating calculation critical value test see equation 
prevents means algorithm making bad decisions clusters datapoints 
testing clusters gaussian fit specify means algorithm fully need test detect data assigned center sampled gaussian 
alternative hypotheses data center sampled gaussian 
data center sampled gaussian 
accept null hypothesis believe center sufficient model data split cluster sub clusters 
reject accept want split cluster 
test anderson statistic 
dimensional test shown empirically powerful normality test empirical cumulative distribution function 
list values converted mean variance ith ordered value 
cumulative distribution function 
statistic log log stephens showed case estimated data clustering correct statistic subset data dimensions belongs center hypothesis test proceeds follows 
choose significance level test 

initialize centers called children see text ways 

run means centers run completion early stopping point desired 
child centers chosen means 

dimensional vector connects centers 
direction means believes important clustering 
project hx vi dimensional representation data projected transform mean variance 

range non critical values confidence level accept keep original center discard fc reject keep fc place original center 
primary contribution simplifying test gaussian fit projecting data dimension test simple apply 
authors approach online dimensionality reduction clustering 
dimensional representation data allows consider data direction means important separating data 
related problem projection pursuit means searches direction data appears non gaussian 
choose significance level test desired probability making type error incorrectly rejecting 
appropriate bonferroni adjustment reduce chance making type errors multiple tests 
example want chance making type error tests apply bonferroni adjustment test 
find final centers means algorithm statistical tests bonferroni correction need extreme 
tests 
consider ways initialize child centers 
approaches initialize center chosen 
method chooses random dimensional vector small compared distortion data 
second method finds main principal component data having eigenvalue chooses 
deterministic method places centers expected locations principal component calculations require nd time space want main principal component fast methods power method takes time linear ratio largest eigenvalues 
principal component splitting 
example shows run means algorithm synthetic dataset true clusters points 
critical value anderson test confidence level 
starting center iteration means centers statistic 
larger critical value reject accept split 
iteration split new center repeat statistical test 
values splits critical value 
accept tests discard splits 
means gives final answer 
statistical power shows power anderson test compared bic 
lower better plots 
run tests data point plotted plots 
left example running means iterations dimensional dataset true clusters points 
starting center left plot means splits centers middle 
test normality significant means rejects keeps split 
splitting center right test values significant means accepts tests accept splits 
middle plot means answer 
see text details 
number datapoints means means type ii error number datapoints means means comparison power anderson test versus bic 
ad test fix significance level bic significance level depends left plot shows probability incorrectly splitting type error true cluster elliptical 
right plot shows probability incorrectly splitting true clusters separated type ii error 
plots functions plots show bic overfits splits clusters small 
plot test generate datapoints single true gaussian distribution plot frequency bic means choose commit type error 
bic tends overfit choosing centers data strictly spherical means 
consistent tests real world data section 
means commits type ii errors small prevents overfitting data 
bic considered likelihood ratio test significance level fixed 
significance level varies depending 
change number model parameters models 

decrease significance level increases bic weaker statistical test 
shows effect varying authors show penalty methods require tuning don generalize methods cross validation 
experiments table shows results running means means large synthetic 
synthetic datasets spherically distributed clusters means means equally table results synthetic datasets 
report distortion relative optimum distortion correct clustering closer better time reported relative means run correct bic larger values better clear finding correct clustering coincide finding larger bic 
items star means chose largest number centers allowed 
dataset method distortion optimal bic time means synthetic means means synthetic means means synthetic means means synthetic means means synthetic means means synthetic means means synthetic means means synthetic means means synthetic means means synthetic dataset true clusters 
left means correctly chooses centers deals non spherical data 
right bic causes means overfit data choosing unevenly distributed clusters 
finding correct maximizing bic statistic don show results 
real world data spherical 
synthetic datasets datapoints dimensions 
true ks 
synthetic dataset type generate datasets true center means chosen uniformly randomly unit hypercube choosing clusters closer apart 
cluster transformation non spherical multiplying data randomly chosen scaling rotation matrix 
run means starting center 
allow means search centers true number clusters 
means algorithm clearly better finding correct non spherical data 
results closer true distortions correct ks 
bic statistic means uses formulated maximize likelihood spherically distributed data 
overestimates number true clusters non spherical data 
especially evident number points cluster small datasets true clusters 
cluster cluster nist pendigits datasets correspondence digit row cluster column means 
means labels meaningful clusters corresponding labels 
overestimation means hits limit centers 
shows example overfitting dataset true clusters 
means chooses means finds true cluster centers 
note means distribute centers evenly clusters clusters receive center receive 
means runs faster means dimensions expect kd tree structures means fast low dimensions take time exponential making slow dimensions 
code written matlab means written discovering true clusters labeled data tested algorithms real world datasets handwritten digit recognition nist dataset pendigits dataset 
goal cluster data knowledge labels measure clustering captures true labels 
datasets true classes digits 
nist training examples dimensions pixels 
randomly chosen examples reduce dimension random projection 
pendigits dataset examples dimensions change data way 
cluster dataset means means measure performance comparing cluster labels true labels define partition quality pq pq kc true number classes number clusters algorithm 
metric maximized induces partition data words points cluster true label estimated true term frequency probability datapoint labeled quality normalized sum true probabilities squared 
statistic related rand statistic comparing partitions 
nist dataset means finds clusters seconds pq score 
means finds clusters seconds clusters contain point indicating overestimation problem bic 
means receives pq score 
pendigits dataset means finds clusters seconds pq score means finds clusters seconds pq score 
shows hinton diagrams means clusterings datasets showing means succeeds identifying true clusters concisely aid labels 
confusions different digits nist dataset seen diagonal elements common researchers sophisticated techniques see 
discussion introduced new means algorithm learning statistical test determining datapoints random sample gaussian distribution arbitrary dimension covariance matrix 
splitting uses dimension reduction powerful test gaussian fitness 
means uses statistical test wrapper means discover number clusters automatically 
parameter supplied algorithm significance level statistical test easily set standard way 
means algorithm takes linear time space plus cost splitting heuristic test number datapoints dimension means linear time space 
empirically means algorithm works finding correct number clusters locations genuine cluster centers shown works moderately high dimensions 
clustering high dimensions open problem years 
research shown may preferable dimensionality reduction techniques clustering low dimensional clustering algorithm means clustering high dimension directly 
author shows simple inexpensive linear projection preserves properties data cluster distances making easier find clusters 
need quality fast clustering algorithms low dimensional data 
step direction 
additionally image segmentation algorithms normalized cut eigenvector computations distance matrices :10.1.1.160.2324:10.1.1.19.8100
spectral clustering algorithms means post processing step find actual segmentation require specified 
expect means useful combination spectral clustering 
horst bischof leonardis alexander 
mdl principle robust vector quantisation 
pattern analysis applications 
blake merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
sanjoy dasgupta 
experiments random projection 
uncertainty artificial intelligence proceedings sixteenth conference uai pages san francisco ca 
morgan kaufmann publishers 
del corso 
estimating eigenvector power method random start 
siam journal matrix analysis applications 
chris ding zha horst simon 
adaptive dimension reduction clustering high dimensional data 
proceedings nd ieee international conference data mining 
fredrik james lewis charles elkan 
scalability clustering algorithms revisited 
sigkdd explorations 
peter huber 
projection pursuit 
annals statistics june 
hubert arabie 
comparing partitions 
journal classification 
jain murty flynn 
data clustering review 
acm computing surveys 
robert kass larry wasserman 
bayesian test nested hypotheses relationship schwarz criterion 
journal american statistical association 
michael kearns mansour andrew ng dana ron 
experimental theoretical comparison model selection methods 
computational theory colt pages 
yann lecun leon bottou bengio patrick haffner 
gradient learning applied document recognition 
proceedings ieee 
andrew ng michael jordan yair weiss 
spectral clustering analysis algorithm 
neural information processing systems 
dan pelleg andrew moore :10.1.1.19.3377
means extending means efficient estimation number clusters 
proceedings th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
peter sand andrew moore 
repairing faulty mixture models density estimation 
proceedings th international conf 
machine learning 
morgan kaufmann san francisco ca 
jianbo shi jitendra malik 
normalized cuts image segmentation 
ieee transactions pattern analysis machine intelligence 
stephens 
edf statistics goodness fit comparisons 
american statistical association september 
