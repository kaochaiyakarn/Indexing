journal machine learning research submitted revised published principled design large scale recursive neural network architectures dag rnns protein structure prediction problem pierre baldi ics uci edu pollastri ics uci edu school information computer science institute genomics bioinformatics university california irvine irvine ca usa editor michael jordan describe general methodology design large scale recursive neural network architectures dag rnns comprises fundamental steps representation domain suitable directed acyclic graphs dags connect visible hidden node variables parameterization relationship variable parent variables feedforward neural networks application weight sharing appropriate subsets dag connections capture stationarity control model complexity 
principles derive specific classes dag rnn architectures lattices trees structured graphs 
architectures process wide range data structures variable sizes dimensions 
resulting models remain probabilistic internal deterministic dynamics allows efficient propagation information training gradient descent order tackle large scale problems 
methods derive state art predictors protein structural features secondary structure fine coarse grained contact maps 
extensions relationships graphical models implications design neural architectures briefly discussed 
protein prediction servers available web www uci edu tools htm 
keywords recursive neural networks recurrent neural networks directed acyclic graphs graphical models lateral processing protein structure contact maps bayesian networks 
recurrent recursive artificial neural networks rnns rich expressive power deterministic internal dynamics provide computationally attractive alternative graphical models probabilistic belief propagation 
exceptions systematic design training application recursive neural architectures real life problems remained somewhat elusive 
describes classes rnn architectures large scale applications derived dag rnn approach 
dag rnn approach comprises basic steps representation domain suitable directed acyclic graphs dags connect visi 
subtle non fundamental distinction recurrent recursive context 
recurrent temporal connotation recursive general spatial connotation obvious examples 
pierre baldi pollastri 
baldi pollastri ble hidden node variables parameterization relationship variable parent variables feedforward neural networks matter class parameterized functions application weight sharing appropriate subsets dag connections capture stationarity control model complexity 
absence cycles ensures neural networks unfolded space back propagation training 
approach applied systematically large scale problems new traced obscure form number publications including baldi chauvin bengio frasconi sperduti lecun 
frasconi 

new derivation number specific classes architectures process wide range data structures variable sizes dimensions systematic application protein structure prediction problems 
results describe expand improve previously reported pollastri 
associated contact map predictor obtained results similar predictor 
casp critical assessment techniques protein structure prediction experiment llnl gov casp casp html 
background protein structure prediction predicting structure protein chains primary sequence amino acids fundamental open problem computational molecular biology 
approach problem deal basic fact protein structures invariant translations rotations 
address issue proposed machine learning pipeline protein structure prediction decomposes problem steps baldi pollastri including intermediary step computes topological representation protein invariant translations rotations 
precisely step starts primary sequence possibly conjunction multiple alignments leverage evolutionary information predicts structural features classification amino acids secondary structure classes alpha helices beta strands coils relative exposure classes surface buried 
second step uses primary sequence structural features predict topological representation terms contact distance maps 
contact map representation neighborhood relationships consisting adjacency matrix distance cutoff typically range aat amino acid level 
distance map replaces binary values pairwise euclidean distances 
fine grained contact distance maps derived amino acid finer atomic level 
coarse contact distance maps derived looking secondary structure elements instance centers gravity 
alternative topological representations obtained local angle coordinates 
third step pipeline predicts actual coordinates atoms protein constraints provided previous steps primarily contact maps possibly constraints provided physical statistical potentials 
predictors step recurrent neural networks described baldi brunak pollastri 

methods address third step developed nmr literature typically ideas distance geometry molecular dynamics stochastic optimization recover coordinates contacts 
methods discussed see weiss application graphical models problem placing side chains 
dag rnns protein structure problem primary focus second step prediction contact maps 
various algorithms prediction contacts valencia pollastri distances lund contact maps developed particular neural networks 
instance feed forward neural network amino acid contact map predictor reported average precision diagonal elements derived approximately recall performance see lesk 

result encouraging chance level factor greater currently provide level accuracy sufficient reliable structure prediction 
section illustrate dag rnn approach design large scale recursive neural network architectures 
step prediction pipeline review onedimensional version architectures second step introduce key generalizations prediction dimensional fine grained coarse grained contact maps derive lattice dag rnn architectures dimension sections describe data sets implementation details important actual results essential grasp basic ideas 
performance results section 
briefly addresses relationship graphical models implications protein structure prediction design neural architectures 
architectural remarks generalizations appendix 

dag rnn architectures example dimensional dag rnn architecture prediction protein structural features 
dimensional case bidirectional rnns suitable dag case described associated set input variables forward backward chain hidden variables set output variables 
essentially connectivity pattern input output hmm bengio frasconi augmented backward chain hidden states 
backward chain course optional capture spatial temporal properties biological sequences 
relationship variables modeled types feed forward neural networks compute output forward backward variables respectively 
fairly general form weight sharing assume stationarity output forward backward networks leads dag rnn architectures previously named bidirectional rnn architecture brnn implemented neural networks form depicted 
form output depends local input position forward upstream hidden context ir backward downstream hidden context ir usually boundary conditions set length sequence processed 
alternatively boundaries treated learnable parameter 
intuitively think terms wheels baldi pollastri pipeline strategy machine learning protein structures 
example scj complex protein 
stage predicts structural features including secondary structure contacts relative solvent accessibility 
second stage predicts topology protein primary sequence structural features 
coarse topology represented cartoon providing relative proximity secondary structure elements alpha helices circles beta strands triangles 
high resolution topology represented contact map residues protein 
final stage prediction actual coordinates residues atoms structure 
rolled sequence 
prediction position roll wheels opposite directions starting ends position combine wheel outputs position input compute output prediction protein secondary structure prediction instance output computed normalized exponential units correspond membership probability residue position classes alpha beta coil 
simple case input ir represents single amino acid orthogonal encoding 
larger input windows ex dag rnns protein structure problem dag associated input variables output variables forward backward chains hidden variables 
brnn architecture left forward right backward context associated recurrent networks wheels 
tending amino acids possible 
proteins orientation symmetric problems weight sharing forward backward networks possible 
usual regression tasks performance model typically assessed mean square error classifications tasks secondary structure prediction performance typically assessed relative entropy estimated target distributions 
weights brnn architecture including weights recurrent wheels trained supervised fashion generalized form gradient descent derived unfolding baldi pollastri wheels space 
brnn architectural variations obtained changing size input windows size window hidden states directly influences output number size hidden layers network forth 
brnn architectures stage prediction pipeline giving rise state art predictors secondary structure solvent accessibility coordination number pollastri available www uci edu tools htm 
dimensional case rnn architectures general purpose machine learning tool general subclass architectures process objects tackle problem predicting contact maps 
turns canonical generalization described figures 
basic version corresponding dag organized horizontal layers input plane hidden planes output plane 
plane contains nodes arranged vertices square lattice 
hidden plane edges oriented cardinal corners 
instance ne plane edges oriented north east 
vertical column input variable hidden variables ne nw sw se associated cardinal corners output variable 

easy check proven directed graph directed cycles 
precise nature inputs contact map prediction problem described section 
generalization case assume stationarity translation invariance output hidden variables 
results dag rnn architecture controlled neural networks form nw ne sw se ne ne ne ne nw nw nw nw sw sw sw sw se se se se ne plane instance boundary conditions set ne 
activity vector associated hidden unit ne depends local input activity vectors units ne ne activity ne plane propagated row row west east south north vice versa 
update schemes easy code continuous ones require jump row column 
continuous update scheme zig zag scheme runs successive diagonal lines se nw orientation 
scheme faster smoother particular software hardware implementations 
alternatively possible update units parallel random fashion equilibrium reached 
case learning proceed gradient descent recurrent networks 
practice getting gradient descent learning procedures recurrent architectures requires degree experimentation learning parameters variety reasons ranging vanishing gradients poor local minima 
additional issues problem specific include competition collaboration tradeoffs hidden dags play symmetric role 
contact map prediction problem instance sw ne planes play dag rnns protein structure problem output plane input plane hidden planes ne nw sw se general layout dag processing dimensional objects contact maps nodes regularly arranged input plane output plane hidden planes 
plane nodes arranged square lattice 
hidden planes contain directed edges associated square lattices 
edges square lattice hidden plane oriented direction possible cardinal corners ne nw sw se 
additional directed edges run vertically column input plane hidden plane hidden plane output plane 
special role associated main diagonal corresponding self alignment proximity 
dimensional case clear build family rnns process inputs outputs dimension version instance input cube associated input variables hidden cubes associated hidden variables 
output cube associated output variables ranging hidden cubic lattice edges oriented corners 
generally version dimensions hidden lattices 
lattice connections directed corresponding hypercube 
class dag rnn architectures number hidden hyperplanes exponential dimension acceptable small values 
covering spatiotemporal applications tractable high dimensional spaces 
full complement hyperplanes architecture allows directed path exist input vertex output vertex 
requirement may necessary applications subset hidden hyperplanes may sufficient depending application 
instance contact map prediction problem planes advantageous planes associated opposite cardinal corners main diagonal sw ne combination sw nw diagonal nw se main diagonal correspond contacts self alignment hidden plane 
baldi pollastri se nw se sw ne nw se ne ne nw sw sw details connections column 
input variable connected hidden variables hidden plane 
input variable hidden variables connected output variable 
vector inputs position 
corresponding output 
connections hidden unit lattice neighbors plane shown 
architectural generalizations possible deferred appendix keep focus contact map prediction 
proceeding data simulations worth proving graphs obtained far acyclic 
see note directed cycle contain input node resp 
output node nodes source resp 
sink nodes outgoing resp 
incoming edges 
directed cycle confined hidden layer layer consists disjoint components contained entirely components 
components specifically designed dags directed cycle 

data described curated data sets train test fine coarse grained contact map predictors 
dag rnns protein structure problem fine grained contact maps experiments fine grained contact maps reported curated data sets small large derived established protocols 
small training testing data sets small sets extracted protein data bank pdb solved structures pdb select list february containing proteins 
list structures additional information obtained ftp site ftp ftp embl heidelberg de pub databases 
avoid biases set redundancy reduced identity threshold distance derived corresponds sequence identity roughly long alignments higher shorter ones 
set reduced excluding chains shorter amino acids defined structures resolution worse result nmr ray experiments contain non standard amino acids contain contiguous xs interrupted backbones 
extract coordinates secondary structure solvent accessibility run program sander pdb files pdb select list excluding crashes due instance missing entries format errors 
final set consists proteins 
speed training comparable systems developed tested short proteins extract subsets proteins length containing proteins proteins length containing proteins small 
small subset contains pairs amino acids table 
experiments reported small set networks trained half data sequences tested remaining half sequences 
large training testing data sets large sets extracted pdb similar protocol slightly different redundancy reduction procedure 
avoid biases set redundancy reduced ordering sequences increasing resolution running alignment threshold leading set containing proteins 
set reduced selecting proteins length 
final set proteins contains pairs roughly times small set 
contact maps strongly depend selection distance cutoff sets different thresholds yielding different classification tasks 
number pairs amino acid class contact cutoff tables 
table composition small dataset number pairs amino acids separated close far distance thresholds 
non contact contact baldi pollastri table composition large dataset number pairs amino acids separated contact non contact distance thresholds 
non contact contact coarse grained contact maps coarse maps compact roughly orders magnitude smaller corresponding fine grained maps easier case exploit long proteins 
selection redundancy reduction procedure large set derive data sets table 
proteins length selected identically large 
proteins length selected comparison pollastri 
leading set proteins selected training remaining split validation test 
proteins kept set length cutoffs 
sets secondary structure assigned program 
segments defined contact centers average position atoms segment closer different definition contact segments contact closer proved highly correlated leading practically identical classification results shown 
table composition datasets coarse grained contact map prediction number pairs secondary structure segments separated contact non contact contact 
implementation details section describe inputs contact map predictors parameters architectures settings learning rates 
dag rnns protein structure problem fine grained contact maps inputs fine grained contact map prediction obvious input location pair corresponding amino acids yielding sparse binary vectors dimension orthogonal encoding 
second type input consideration profiles correlated mutation gobel 
profiles essentially form alignment homologous proteins implicitly contain evolutionary structural information related proteins 
information relatively easy collect known alignment algorithms ignore structure applied large data sets proteins including proteins unknown structure 
profiles known improve prediction secondary structure percentage points presumably secondary structure conserved primary amino acid sequence 
case secondary structure prediction input enriched profile vectors positions yielding dimensional probability vectors 
profiles derived psi blast program described pollastri 

distant pair positions multiple alignment considered horizontal correlations sequences may exist correlated mutations completely lost profiles independent columns 
correlations result important structural constraints 
expanded input retains information consists matrix corresponding probability distribution pairs amino acids observed corresponding columns alignment 
typical alignment contains dozen sequences general matrix sparse 
unobserved entries set zero regularized small non zero values standard dirichlet priors baldi brunak 
attempted larger inputs considered correlations extracted positions respect immediate neighborhoods including instance 
compensate small alignment errors rapidly lead intractably large inputs size size neighborhood considered 
compression techniques weight sharing coarse amino acid classes higher order neural networks conjunction expanded inputs 
potentially relevant inputs simulations include coordination numbers bridges 
specific structural features added input secondary structure classification relative solvent accessibility percentage indicator residue surface buried hydrophobic core globular protein pollastri 
features increase number inputs pair amino acids inputs position buried exposed 
value indicators close exact derived pdb structures noisier estimated secondary structure accessibility predictor 
previous studies somewhat comparable inputs failed assess contribution feature performance 
specifically address issue inputs size just amino acids profiles size correlated profiles size correlated profiles plus secondary structure relative solvent accessibility amino acid pair 
baldi pollastri architectures dag rnn approach hidden planes associated similar independent neural networks 
neural network single hidden layer 
resp 
noh denote number hidden units hidden resp 
output layer neural networks associated hidden planes 
input size total number inputs hidden layer hidden networks noh square lattice hidden plane 
including total number parameters hidden networks associated hidden plane noh noh basic version architectures output network input size noh similar notation 
assuming output network single hidden layer units total number parameters noh including thresholds single output unit predict contacts distances 
number parameters architectures simulations reported table 
table model architectures fine grained map prediction 
number hidden units output network 
number hidden units hidden networks 
noh number output units hidden networks 
total number parameters computed input size 
model noh parameters learning initialization training implemented line adjusting weights complete presentation protein 
shown plain gradient descent error function relative entropy contacts mean square distances unable escape large initial flat plateau associated difficult problem 
effect remains multiple random starting points 
experimentation variants modified form gradient descent update dw weight piecewise linear different ranges 
case instance positive backpropagated gradient dw dw dw dw dw learning rate similarly negative gradients proper sign changes 
shows heuristic approach effective solution large plateau problem 
learning rate set divided number protein examples 
prior learning dag rnns protein structure problem epochs example learning curves 
blue corresponds gradient descent 
green corresponds piecewise linear learning algorithm 
large initial plateau problematic plain gradient descent 
weights unit various neural networks randomly initialized 
standard deviations controlled flexible way avoid bias ensure expected total input unit roughly range 
coarse grained contact maps coarse grained contact map prediction natural inputs considered position secondary structure classification corresponding segments length position segments protein 
length amino acid length segment 
location represents center segment linear amino acid sequence 
corresponds inputs location 
fine grained information segment including amino acid sequence profile relative solvent accessibility may relevant need compressed 
solution problem compress fine grained information coarsegrained representation example averaging solvent accessibility representing amino acid secondary structure composition profile segment 
disadvantage solution discards potentially important information contained relative amino acid positions 
case compressing fine grained information coarse grained level representation directly learn adaptive encoding sequence data 
specifically brnn model entire protein terminal output units segment encode segment properties 
brnn takes input profile obtained psi blast multiple alignments described pollastri 
plus secondary structure solvent accessibility residue 
outputs brnn taken terminus segment input dag rnn 
output encoded vector baldi pollastri dimension dag rnn coarse map prediction inputs location 
training gradient compound architecture underlying brnn computed backpropagation structure 
number parameters dag rnn architecture computed formula count number parameters brnn encoder 
resp 
denotes number hidden units hidden resp 
output layer neural network associated hidden chains brnn resp 
denotes number hidden units hidden resp 
output layer output network brnn total number parameters networks hidden chains output network 
denotes size input brnn encoder 
example case noh amino acid frequencies secondary structures solvent accessibility total number parameters brnn 
total number parameters models simulations reported table 
learning rates algorithms similar case fine grained maps 
table model architectures coarse grained map prediction 
number hidden units output network rnn 
number hidden units hidden networks rnn 
noh number output units hidden networks rnn 
number output hidden units output network encoding brnn 
number hidden output units network associated chains encoding brnn 
model noh parameters 
results section provides sample simulation results 
assessing accuracy predicted contact maps trivial variety reasons 
imbalance number contacts non contacts fine grained maps small cutoffs number contacts grows roughly linearly length protein number distant contacts roughly half length protein total number contact times length protein number non contacts grows quadratically 
second contacts far away dag rnns protein structure problem diagonal significant difficult predict 
third prediction slightly shifted may reflect prediction topology may receive poor score 
better metrics assessing contact map prediction remain developed variety measures address problems allow comparisons results published literature especially 
pollastri baldi pollastri 

particular percentage correctly predicted contacts non contacts threshold cutoff 
percentage correctly predicted contacts non contacts threshold cutoff band corresponding amino acids fine grained maps 

coarse grained map difference diagonal diagonal terms significant 
precision specificity defined tp tp fp 
recall sensitivity defined tp tp fn 
measure defined harmonic mean precision recall rp 
roc receiver operating characteristic curves describing recall sensitivity true positive rate varies sensitivity precision specificity false positive rate 
roc curves particular previously systematic way assess contact map predictions 
fine grained contact maps small sets results contact map predictions distance cutoffs model noh provided table 
experiment system trained half set proteins length tested half 
results obtained plain sequence inputs amino acid pairs information profiles correlated mutations structural features 
cutoff instance system able recover contacts 
table percentages correct predictions different contact cutoffs small validation set 
model noh trained tested proteins length 
inputs correspond simple pairs amino acids sequence profiles correlated profiles structural features 
non contact contact baldi pollastri course essential able predict contact maps longer proteins 
attempted training rnn architectures larger data sets containing long proteins 
noted systems developed accommodate inputs arbitrary lengths system trained short proteins produce predictions longer proteins 
fact overwhelming majority contacts proteins linear distances shorter amino acids reasonable expect decent performance system 
observe table 
cutoff percentage correctly predicted contacts proteins length 
table percentages correct predictions different contact cutoffs validation set proteins length aa 
model trained proteins length small training set 
model noh 
inputs correspond simple pairs amino acids sequence 
non contact contact tables results network output symmetric respect diagonal symmetry constraints enforced learning 
symmetric output derived averaging output values positions 
application averaging procedure yields small improvement prediction performance seen table 
possible alternative enforce symmetry training phase 
results additional experiments conducted assess performance effects larger inputs displayed tables 
inputs size corresponding correlated profiles performance increases marginally roughly contacts instance table 
secondary structure relative solvent accessibility threshold added input performance shows remarkable improvement range contacts 
example contacts predicted accuracy 
row table provides standard deviations accuracy protein basis 
standard deviations reasonably small proteins predicted levels close average 
results support view secondary structure relative solvent accessibility important prediction contact maps useful table table symmetric prediction constraints 
non contact contact dag rnns protein structure problem table percentages correct predictions different contact cutoffs small validation set 
model noh 
inputs size correspond correlated profiles multiple alignments derived psi blast program 
non contact contact table percentages correct predictions different contact cutoffs small validation set 
table inputs include secondary structure relative solvent accessibility threshold derived program 
row represents standard deviations protein basis 
non contact contact std profiles correlated profiles 
cutoff model predicts contacts correctly achieving state art performance previously reported results 
table percentages correct predictions different contact cutoffs validation set proteins length aa 
model trained proteins length small training set 
inputs include primary sequence correlated profiles secondary structure relative solvent accessibility 
non contact contact small improvement derived combining multiple predictors ensemble 
results combination models noh reported table 
improvements single model range aand terms diagonal prediction sensitivity amino acids satisfying contrasted reported 

baldi pollastri table percentages correct predictions different contact cutoffs small validation set obtained ensemble models percentages correct predictions different contact cutoffs small validation set obtained ensemble predictors noh noh noh distance cutoff 
inputs include correlated profiles secondary structure relative solvent accessibility 
non contact contact large set large set split training set proteins test set non homologous proteins 
training set case roughly times larger small training set contains approximately pairs amino acids 
results ensembles models cutoffs reported table 
performance significantly better system trained small data set table 
testing directly large dataset ensemble trained small set leads results weaker reported table 
supports view increase training set size current levels lead consistent improvements prediction 
roc curves ensembles reported 
terms diagonal prediction sensitivity amino acids satisfying case strong improvement reported 

table percentages correct predictions different contact cutoffs large validation set obtained ensembles predictors percentages correct predictions different contact cutoffs large validation set obtained ensembles predictors 
models trained tested large data sets 
inputs include correlated profiles secondary structure relative solvent accessibility 
non contact contact scope article experience see 
shows useful reconstruction 
typical example contact prediction reported 
dag rnns protein structure problem example exact bottom left predicted fine contact map protein ig 
grey scale white non contact black contact 
baldi pollastri precision fine map prediction roc curves ensembles prediction fine maps thresholds increments 
coarse contact maps prediction results ensembles models coarse contact map problem sets shown tables roc curves 
ensembles trained underlying brnn encoder finegrained information check contribution 
absence results substantially similar pollastri 

introducing amino acid level information causes noticeable improvement roughly terms sets 
example typical prediction coarse contact map shown 
expected prediction coarse contact maps accurate probably linear scale long ranged interactions reduced order magnitude 
table percentages correct predictions coarse contact maps sets underlying brnn encoder 
full model brnn difference tot set contact non tot contact non tot dag rnns protein structure problem table percentages correct predictions coarse contact maps sets underlying brnn encoder 
precision recall measures 
full model brnn difference precision recall precision recall precision coarse map prediction set set set roc curves coarse map ensembles different sets thresholds increments 

rnns computationally powerful far difficult design train rnn architectures address complex problems systematic way 
dag rnn approach partly answers challenge providing principled way design complex recurrent systems trained address real world problems efficiently 
feedforward neural networks process input vectors fixed size ability process data structures graphical support vary format sequences trees lattices dimensionality size 
case contact maps instance input length varies sequence 
furthermore necessary length dimensions objects different sizes shapes baldi pollastri example exact bottom left predicted coarse contact map protein 
grey scale white non contact black contact 
dag rnns protein structure problem combined processed 
likewise necessary dimensions input hidden output layers maps spaces different dimensions considered 
believe architectures suitable processing data biological sequences applications diverse machine vision games computational chemistry see micheli 

underlying dag dag rnns related graphical models bayesian networks particular 
precise formal relationship dag rnns bayesian networks discussed clear internal deterministic semantics dag rnns trade expressive power increased speed learning propagation 
experience baldi protein structure prediction problems tradeoff worthwhile current computational environment 
currently combining contact map predictors reconstruction algorithm produce complete predictor protein tertiary structures complementary approaches baker simons large scale structural projects 
computational time machine learning approach absorbed training phase 
trained ab initio approaches system produce predictions scale faster proteins fold 
respect predicted coarse contact maps may prove particularly useful ability capture long ranged contact information remained far elusive methods 
layered dag rnn architectures process contact maps may shed broader light neural style computations multi layered systems including distant biological relatives 
preferential directions propagation hidden layer integrate context multiple cardinal directions 
second computation visible output requires computation hidden outputs corresponding column 
final output converges correct value center output sheet progressively propagates boundaries 
third weight sharing exact physical implementation effect fluctuations ought investigated 
particular additional locally limited degrees freedom may provide increased flexibility substantially increasing risk overfitting 
dag rnn architectures lateral propagation massive 
stands sharp contrast conventional connectionist architectures primary focus remained feedforward feedback pathways lateral propagation mere lateral inhibition winner take operations 
acknowledgments pb gp part supported laurel faculty innovation award nih sun microsystems award pb uci 
gp part supported california institute telecommunications information technology cal fellowship university california systemwide biotechnology research education program 
dag rnn architectures pb bioinformatics school san italy september 
baldi pollastri appendix architectural remarks generalizations architectures derived section extended directions incrementally enriching connectivity drastically completely different dags 
example proof underlying graphs acyclic shows immediately connections added connected dag components hidden layer long introduce cycles 
trivially satisfied instance connections run hidden dag second second third forth hidden dag 
brnn architectures example sparse implementation idea consists adding connection connections break symmetry architecture hidden dag plays role 
similar vein feedback connections output layer hidden layer introduced selectively 
brnn architectures feedback edges added introducing cycles simultaneously 
case translation invariant example obtained connecting case feedback connections introduced nodes located instance ne plane ne ne nodes ne kl direction enriching dag rnn architectures consider diagonal edges associated triangular hexagonal hidden planes similarly higher dimensions 
doing length diagonal path cut case dimensions moderate increase number model parameters 
additional long range connections added case higher order markov models sequences complexity caveats 
connectivity constraints varied 
instance weight sharing approach hidden dags extended different hidden dags 
dimensional isotropic brnn case instance single neural network shared forward backward chains 
practice weight sharing occur hidden dags depends complexity symmetries problem 
dag rnn architectures combined modular hierarchical ways instance cover multiple levels resolution image processing problems 
general classes architectures obtained considering completely different dags 
particular necessary nodes arranged dimensional square lattice 
example tree architecture process tree structured data depicted 
generally arbitrary dags hidden layer additional connections running input hidden input output hidden output layers 
fact input feeding hidden layers need identical input feeding output layer 
dag connections input output layer possible dag rnn architectures outputs inputs hmms inputs outputs markov chains 
particular regular structure hidden dags models lattice trees interesting 
dag rnn said homogeneous hidden dags underlying graphical structure play symmetric role 
complete directed path input output 
generally lattice rnns full complement hidden dags tree dag rnn edges oriented away root complete 
dag rnns protein structure problem dimensional lattice architectures dags vary size input keep topology 
possible consider situations topology dag varies input pollastri 
coarse contacts represented directly edges nodes representing secondary structure elements 
case nn weight sharing extended hidden dags vary input example long indegrees hidden graphs remain bounded 
output tree input tree hidden trees tree dag rnn 
formalize point derive general boundary conditions dag rnn architectures consider connected dags hidden layer node inputs 
nodes strictly inputs called boundary nodes 
particular dag source node outgoing edges source node boundary node 
boundary node inputs add distinct input nodes called frontier nodes 
source nodes frontier nodes added 
pre processing step hidden dag baldi pollastri regular sense nodes exactly inputs exception frontier nodes source nodes 
rnn defined having neural network shared nodes 
network single output vector corresponding activity node input vectors 
dimension input vector vary long neural network inputs defined unambiguously total ordering set edges 
vectors associated frontier nodes set zero matching dimensions properly 
propagation activity proceeds forward direction frontier nodes sink nodes incoming edges 
note multiple sink sources see instance case tree dag rnns 
fact graph dag boundary conditions ensures consistent order updating nodes 
forward order may unique case dag rnns tree dag rnns breadth versus depth 

aligned sequences share fold 
mol 
biol 
taylor 
global fold determination small number distance restraints 
mol 
biol 
baker 
protein structure prediction structural genomics 
science 
baldi brunak 
bioinformatics machine learning approach 
mit press cambridge ma 
second edition 
baldi brunak frasconi pollastri soda 
exploiting past protein secondary structure prediction 
bioinformatics 
baldi chauvin 
hybrid modeling hmm nn architectures protein applications 
neural computation 
baldi pollastri 
machine learning strategy protein analysis 
ieee intelligent systems 
special issue intelligent systems biology 
bengio frasconi 
input output hmm sequence processing 
ieee trans 
neural networks 

neural network predictor residue contacts proteins 
protein engineering 

prediction number residue contacts proteins 
proceedings conference intelligent systems molecular biology ismb la jolla ca pages 
aaai press menlo park ca 
valencia 
prediction contact maps neural networks correlated mutations 
protein engineering 
frasconi gori sperduti 
general framework adaptive processing data structures 
ieee trans 
neural networks 
dag rnns protein structure problem gobel sander schneider valencia 
correlated mutations residue contacts proteins 
proteins structure function genetics 

connectionist approach learning search control heuristics automated deduction systems 
ph thesis tech 
univ munich computer science 
lund andersen brunak 
sequence motifs enhanced neural network prediction protein distance constraints 
proceedings seventh international conference intelligent systems molecular biology ismb la jolla ca pages 
aaai press menlo park ca 
schneider sander 
selection representative data sets 
prot 
sci 
sander 
dictionary protein secondary structure pattern recognition hydrogen bonded geometrical features 
biopolymers 
lecun bottou bengio haffner 
gradient learning applied document recognition 
proceedings ieee 
lesk lo conte hubbard 
assessment novel fold targets casp predictions dimensional structures secondary structures contacts 
proteins 
lund bohr bohr hansen brunak 
protein distance constraints predicted neural networks probability density functions 
prot 
eng 
micheli sperduti 
analysis internal representations developed neural networks structures applied quantitative structure activity relationship studies 
chem 
inf 
comput 
sci 
clore 
determination dimensional structures proteins distance data dynamical simulated annealing random array atoms 
lett 
clore 
determination dimensional structures proteins distance data hybrid distance geometry dynamical simulated annealing calculations 
lett 
rost valencia 
effective sequence correlation conservation fold recognition 
mol 
biol 
valencia 
improving contact predictions combination correlated mutations sources sequence information 
fold 
des 
ausiello valencia 
correlated mutations contain information protein protein interactions 
mol 
biol 
baldi pollastri pollastri baldi 
contact maps recurrent neural networks lateral propagation cardinal corners 
bioinformatics supplement 
proceedings ismb conference 
pollastri baldi 
prediction coordination number relative solvent accessibility proteins 
proteins 
pollastri baldi frasconi 
prediction protein topologies generalized iohmms rnns 
thrun becker obermayer editors advances neural information processing systems pages 
mit press cambridge ma 
pollastri rost baldi 
improving prediction protein secondary classes recurrent neural networks profiles 
proteins 
sander 
dimensional contacts proteins predicted analysis correlated mutations 
protein engineering 
simons strauss baker 
prospects ab initio protein structural genomics 
mol 
biol 
sperduti 
supervised neural networks classification structures 
ieee transactions neural networks 
domany 
recovery protein structure contact maps 
folding design 
weiss 
approximate inference protein folding 
becker thrun obermayer editors advances neural information processing systems nips conference volume 
mit press cambridge ma 
press 

