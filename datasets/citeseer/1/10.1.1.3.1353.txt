di usion kernels statistical manifolds john la erty guy lebanon january cmu cs school computer science carnegie mellon university pittsburgh pa family kernels statistical learning introduced exploits geometric structure statistical models 
kernels heat equation riemannian manifold de ned fisher information metric associated statistical family generalize gaussian kernel euclidean space 
important special case kernels geometry multinomial families derived leading kernel learning algorithms apply naturally discrete data 
bounds covering numbers rademacher averages kernels proved bounds eigenvalues laplacian riemannian manifolds 
experimental results document classi cation multinomial geometry natural motivated improvements obtained standard gaussian linear kernels standard text classi cation 
research partially supported advanced research development activity information technology arda contract number mda national science foundation nsf ccr iis 
views contained document authors interpreted representing ocial policies expressed implied arda nsf government 
keywords kernels heat equation di usion information geometry text classi cation 
mercer kernels transforming linear classi cation regression schemes nonlinear methods fundamental idea recognized early development statistical learning algorithms perceptron splines support vector machines aizerman kimeldorf wahba boser 
resurgence activity kernel methods machine learning community led development important technique demonstrating kernels key components tools tackling nonlinear data analysis problems integrating data multiple sources 
kernel methods typically viewed terms implicit representation high dimensional feature space terms regularization theory smoothing poggio girosi 
case standard mercer kernels gaussian radial basis function kernel require data points represented vectors euclidean space 
initial processing data real valued feature vectors carried ad hoc manner called dirty laundry machine learning dietterich initial euclidean feature representation crucial little theoretical guidance obtained 
example text classi cation standard procedure preparing document collection application learning algorithms support vector machines represent document vector scores dimension corresponding term possibly scaling inverse document frequency weighting takes account distribution terms collection joachims 
representation proven ective statistical justi cation transform categorical data euclidean space unclear 
kondor la erty directly motivated need kernel methods applied discrete categorical data particular data lies graph 
kondor la erty propose discrete di usion kernels tools spectral graph theory data represented graphs 
propose related construction kernels heat equation 
key idea approach statistical family natural data analyzed represent data points statistical manifold associated fisher information metric family 
exploit geometry statistical family speci cally consider heat equation respect riemannian structure fisher metric leading mercer kernel de ned appropriate function spaces 
result family kernels generalizes familiar gaussian kernel euclidean space includes new kernels discrete data statistical families multinomial 
kernels intimately geometry fisher information metric heat di usion equation associated riemannian manifold refer information di usion kernels 
apparent limitation discrete di usion kernels kondor la erty diculty analyzing associated learning algorithms discrete setting 
stems fact general bounds spectra nite nite graphs dicult obtain research concentrated bounds rst eigenvalues special families graphs 
contrast kernels investigate continuous parameter spaces case underlying data discrete leading amenable spectral analysis 
draw considerable body research di erential geometry studies eigenvalues geometric laplacian apply machinery developed analyzing generalization performance kernel machines setting 
framework proposed fairly general focus application ideas text classi cation natural statistical family multinomial 
simplest case words document modeled independent draws xed multinomial non independent draws corresponding grams complicated mixture models possible 
gram models maximum likelihood multinomial model obtained simply normalized counts smoothed estimates remove zeros 
mapping embedding document statistical family geometric framework applies 
perspective associating multinomial models individual documents explored information retrieval promising results ponte croft zhai la erty :10.1.1.54.6410
statistical manifold dimensional multinomial family comes embedding multinomial simplex dimensional sphere isometric fisher information metric 
multinomial family viewed manifold constant positive curvature 
discussed mathematical technicalities due corners edges boundary multinomial simplex intuitively multinomial family viewed way riemannian manifold boundary address technicalities rounding procedure simplex 
heat kernel manifold closed form approximate kernel closed form leading term expansion small time asymptotic expansion heat kernel great di erential geometry 
results kernel readily applied text documents motivated mathematically statistically 
detailed experiments text classi cation webkb reuters data sets standard test collections 
experimental results indicate multinomial information di usion kernel performs empirically 
improvement part attributed role fisher information metric results points near boundary simplex relatively importance euclidean metric 
viewed di erently ects similar obtained heuristically designed term weighting schemes inverse document frequency seen arise automatically geometry statistical manifold 
remaining sections organized follows 
section review relevant concepts required riemannian geometry de ne heat kernel general riemannian manifold expansion 
section de ne fisher metric associated statistical manifold distributions examine detail special cases multinomial spherical normal families proposed heat kernel approximation statistical manifold main contribution 
section derives bounds covering numbers rademacher averages various learning algorithms new kernels borrowing results di erential geometry bounds geometric laplacian 
section describes results applying multinomial di usion kernels text classi cation conclude discussion results section 
riemannian geometry heat kernel brie reviewing elementary concepts riemannian geometry construction information di usion kernels concepts widely machine learning 
refer details background milnor elegant concise overview introductory texts di erential geometry include material 
basic properties heat kernel riemannian manifold section 
excellent introductory account topic rosenberg authoritative spectral methods riemannian geometry yau 
readers di erential geometry repair may wish proceed directly section section 
basic de nitions dimensional di erentiable manifold set points locally equivalent smooth transformations supporting operations di erentiation 
formally di erentiable manifold set collection local charts bijection 
pair local charts required open ij di 
tangent space thought directional derivatives operating set real valued di erentiable functions equivalently tangent space viewed terms equivalence class curves passing curves equivalent case tangent local chart charts sense derivatives exist equal 
cases interest manifold submanifold larger manifold example open dimensional simplex de ned submanifold case tangent space submanifold subspace may represent tangent vectors terms standard basis tangent space open simplex di erential manifold single global chart 
manifold boundary de ned similarly local charts satisfy mapping patch half space fx 
general open sets topology induced di induces di intf fx 
sense de ne interior int boundary open dimensional manifold boundary dimensional manifold boundary 
di manifold manifold induces push mapping associated tangent spaces 
vector eld tm mapped push forward tn satisfying 
intuitively push forward mapping transforms velocity vectors curves velocity vectors corresponding curves new manifold 
mapping transforming metrics described 
geometric laplacian construction kernels geometric laplacian order de ne generalization familiar laplacian 
manifolds needs notion geometry particular way measuring lengths tangent vectors 
riemannian manifold di erentiable manifold family smoothly varying positive de nite inner products riemannian manifolds isometric case di occasionally hard computations manifold transformed easier computations isometric manifold 
manifold riemannian metric 
example manifold embedded whitney embedding theorem euclidean metric induces metric manifold embedding 
fact riemannian metric obtained way nash embedding theorem 
local coordinates represented ij ij non singular symmetric positive de nite matrix depending smoothly tangent vectors represented local coordinates example consider open dimensional simplex de ned 
metric expressed symmetric positive de nite matrix ij described nelson laplace operator various manifestations beautiful central object mathematics 
probability theory mathematical physics fourier analysis partial di erential equations theory lie groups di erential geometry revolve sun light penetrates obscure regions number theory algebraic geometry 
induces metric ij metric enables de nition lengths vectors curves distance points manifold 
length tangent vector kvk hv vi length curve kdt velocity vector path time de nition lengths curves de ne distance points length shortest piecewise di erentiable curve connecting geodesic distance turns riemannian manifold metric space satisfying usual properties positivity symmetry triangle inequality 
riemannian manifolds support convex neighborhoods 
particular open set containing points connected unique minimal geodesic manifold said complete case geodesic curve extended de ned shown milnor equivalent complete complete metric closed bounded subsets compact 
particular compact manifolds complete 
hopf theorem milnor asserts complete points joined minimal geodesic 
minimal geodesic necessarily unique seen considering antipodal points sphere 
exponential map exp maps neighborhood di neighborhood de nition exp point geodesic starting initial velocity dt geodesic satis es rv rs 
mapping de nes local coordinate system called normal coordinates computations especially convenient 
function gradient grad vector eld de ned xi local coordinates gradient grad ij ij inverse ij 
divergence operator de ned adjoint gradient allowing integration parts manifolds special structure 
orientation manifold smooth choice orientation tangent spaces meaning local charts di erential orientation preserving sign determinant constant 
riemannian manifold orientable possible de ne volume form positively oriented volume form turn enables de nition divergence vector eld manifold 
local coordinates divergence div det det laplace beltrami operator functions de ned 
div grad local coordinates 
det ij det de nitions preserve familiar intuitive interpretation usual operators euclidean geometry particular gradient points direction steepest ascent divergence measures ow minus ow liquid heat 
heat kernel laplacian model heat di geometric manifold ow governed second order di erential equation initial conditions 
value describes heat location time initial distribution heat time zero 
heat di usion kernel solution heat equation initial condition dirac delta function consequence linearity heat equation heat kernel generate solution heat equation arbitrary initial conditions dy simple special case consider heat ow circle dimensional sphere parameterizing manifold angle letting cos discrete cosine transform solution heat equation initial conditions seen heat equation leads equation dt cos easily solved obtain cos 
time parameter gets large solution converges average value heat di uses manifold uniform temperature 
express solution terms integral kernel note fourier inversion formula hf ij ij ij ij expressing solution heat kernel cos simple example shows properties general solution heat equation compact riemannian manifold particular note eigenvalues kernel scale dimension case 
heat kernel familiar gaussian kernel solution heat equation expressed dy seen heat di uses nity 
compact laplacian discrete eigenvalues 
corresponding eigenfunctions satisfying 
manifold boundary appropriate boundary conditions imposed order 
self adjoint 
dirichlet boundary conditions set neumann boundary conditions require outer normal direction 
theorem summarizes basic properties kernel heat equation refer yau proof 
theorem complete riemannian manifold 
exists function called heat kernel satis es properties 



lim 

dz addition compact expressed terms eigenvalues eigenfunctions laplacian 
properties imply solves heat equation starting point heat source follows 
dy solves heat equation initial conditions dy 
dy dy 
lim lim dy 
property implies 


physically intuitive interpretation heat di usion time composition heat di usion time heat di usion additional time 
positive operator dx dy 
dx hf 
fi positive de nite 
compact case positive de niteness follows directly expansion shows eigenvalues integral operator properties show de nes mercer kernel 
heat kernel natural candidate measuring similarity points respecting geometry encoded metric furthermore geodesic distance mercer kernel fact enables statistical kernel machines 
kernel classi cation text classi cation experiments section discriminant function interpreted solution heat equation initial temperature labeled data points initial temperature 
expansion geometries closed form solution heat kernel 
short time behavior solutions studied asymptotic expansion called expansion 
fact existence heat kernel asserted theorem directly proven rst showing existence expansion 
local expansion contains wealth geometric information modern di erential geometry notably index theory expansion generalizations 
section employ rst order expansion text classi cation 
recall heat kernel dimensional euclidean space euclid exp kx yk kx yk jx squared euclidean distance expansion approximates heat kernel locally correction euclidean heat kernel 
de nition exp 
currently unspeci ed functions denotes square geodesic distance manifold 
idea obtain recursively solving heat equation approximately order small di usion time denote length radial geodesic normal coordinates de ned exponential map 
functions shown 
dr log det dr df dr 
fh 

df dr dh dr starting basic relations calculus shows 
exp de ned recursively det 
ds recursive de nition functions expansion de ned locally extended smoothing cut function speci cation constant 
order de ned suggested equation approximate solution heat equation satis es suciently close particular unique 
details refer yau rosenberg 
general positive de nite de ne mercer kernel positive de nite suciently small 
particular de ne min spec min spec denotes smallest eigenvalue 
continuous function time interval positive de nite case 
fact employ approximation heat kernel statistical learning 
di usion kernels statistical manifolds proceed main contribution application heat kernel constructions reviewed previous section geometry statistical families order obtain kernels statistical learning 
mild regularity conditions general parametric statistical families come equipped canonical geometry fisher information metric 
geometry long recognized rao rich line research statistics threads machine learning sought exploit geometry statistical analysis see kass survey discussion monographs kass vos amari extensive treatments 
spite fundamental nature geometric perspective statistics researchers concluded occasionally provides interesting alternative interpretation contributed new results methods obtained conventional analysis 
kernel methods propose arguably motivated derived geometry statistical manifolds 
geometry statistical families fp 
dimensional regular statistical family set assume open nite measure 
density respect 
identify manifold assuming mapping 
discuss cases closed leading manifold boundary 
denote log 
fisher information metric de ned terms matrix ij log log score mean zero ij seen variance positive de nite 
assumption smoothly varying de nes riemannian metric statistical manifold mean simply manifold densities metric induced fisher information matrix general notion riemannian manifold possibly non metric connection de ned lauritzen 
equivalent suggestive form fisher information matrix seen case multinomial ij equivalent form ij 
see note log log log ij possible choices metric di erentiable manifold important consider motivating properties fisher information metric 
intuitively fisher information may thought amount information single data point supplies respect problem estimating parameter 
interpretation justi ed ways notably eciency estimators 
particular asymptotic variance maximum likelihood estimator obtained sample size ng mle asymptotically unbiased inverse fisher information represents asymptotic uctuations mle true value 
cram er rao lower bound variance unbiased estimator bounded ng additional motivation fisher information metric provided results characterize metric multiplication constant invariant respect certain probabilistically meaningful transformations called congruent embeddings 
connection familiar similarity measure worth noting 
densities respect kullback leibler divergence de ned log kullback leibler divergence behaves nearby points square information distance 
precisely shown lim convergence uniform 
comment relationship may approximating information di usion kernels complex models 
basic examples illustrate geometry fisher information metric associated di usion kernel induces statistical manifold 
spherical normal family corresponds manifold constant negative curvature multinomial corresponds manifold constant positive curvature 
multinomial important example develop report extensive experiments resulting kernels section 
di usion kernels gaussian geometry consider statistical family fp 

gaussian having mean variance 
compute fisher information metric family convenient general expression equation 

simple calculations yield ij dx ij ni dx dx nn log dx dx letting new coordinates de ned see fisher information matrix ij ij fisher information metric gives structure upper half plane hyperbolic space 
distance minimizing geodesic curves hyperbolic space straight lines circles orthogonal mean subspace 
particular univariate normal density hyperbolic geometry 
generalization dimensional case location scale family densities seen hyperbolic geometry kass vos 
families densities form example decision boundaries kernel classi er information di usion kernels spherical normal geometry right constant negative curvature compared standard gaussian kernel euclidean space left 
data points simply contrast underlying geometries 
curved decision boundary di usion kernel interpreted statistically noting variance decreases mean known increasing certainty 
heat kernel hyperbolic space explicit form yan 
odd sinh exp sinh exp cosh cosh ds geodesic distance points mean unspeci ed associated kernel standard gaussian rbf kernel 
possible kernel statistical learning data points naturally represented sets 
suppose data point form fx xm data represented mapping sends group points corresponding gaussian mle 
di usion kernel hyperbolic space compared euclidean space gaussian kernel 
curved decision boundary di usion kernel intuitive sense variance decreases mean known increasing certainty 
note fact consider manifold boundary allowing non negative strictly positive 
case densities boundary singular point masses mean boundary simply manifold boundary required 
di usion kernels multinomial geometry consider statistical family multinomial outcomes fp 

parameter space open simplex de ned equation submanifold compute metric denote draw multinomial 
log likelihood derivatives log log log log ij dimensional submanifold express dimensional vectors note due constraint sum components tangent vector zero 
basis de nition fisher information metric equation compute hu vi log geodesic distances dicult compute general case multinomial information geometry easily compute geodesics observing standard euclidean metric surface positive sphere pull back fisher information metric simplex 
relationship suggested form fisher information equation 
concrete transformation di simplex positive portion sphere radius denote portion sphere tangent vectors pull back fisher information metric equal distance contours upper right edge left column center center column lower right corner right column 
distances computed fisher information metric top row euclidean metric bottom row 
ki li transformation 
isometry geodesic distance may computed shortest curve connecting 
shortest curves portions great circles intersection dimensional plane length arccos section noted connection kullback leibler divergence information distance 
case multinomial family close relationship hellinger distance 
particular easily shown hellinger distance dh related dh sin dh agrees second order dh fisher information metric places greater emphasis points near boundary expected important text problems typically sparse statistics 
shows equal distance contours fisher information euclidean metrics 
spherical geometry derived nite multinomial geometry non parametrically arbitrary subset probability measures leading spherical geometry hilbert space dawid 
multinomial di usion kernel explicit expression gaussian geometry discussed explicit form heat kernel sphere positive orthant sphere 
resort expansion derive approximate heat kernel multinomial 
recall section obtained local expansion equation extending smoothly zero outside neighborhood diagonal de ned exponential map 
just derived results multinomial family exp arccos 

rst order expansion obtained example decision boundaries support vector machines information di usion kernels trinomial geometry simplex top right compared standard gaussian kernel left 
sphere shown function leading order correction gaussian kernel fisher information metric det sin berger 
leading order multinomial di usion kernel exp sin experiments approximate kernel exp arccos appealing asymptotic expansion note sin blows large kernel compared standard euclidean space gaussian kernel case trinomial model svm classi er 
rounding simplex case multinomial geometry poses technical complications analysis di usion kernels due fact open simplex complete rounding simplex 
closed simplex manifold boundary carry rounding procedure remove edges corners 
rounded simplex closure union balls lying open simplex 
closure di erentiable manifold boundary 
technically possible apply results di erential geometry bounds spectrum laplacian adopted section 
brie describe technical patch allows derive needed analytical results sacri cing practice methodology derived far 

denote closure open simplex 
usual probability simplex allows zero probability items 
form compact manifold boundary boundary edges corners 
words local charts de ned di erentiable 
adjust idea round edges 
obtain subset forms compact manifold boundary closely approximates original simplex 
fy kx yk denote open euclidean ball radius centered denote ball centers points simplex balls lie completely simplex fx denote interior de ne union balls contained pn rounded simplex 
de ned closure 
rounding procedure yields 
suggested 
note general rounded simplex 
contain points single component having zero probability 
set 
forms compact manifold boundary image isometry 
compact submanifold boundary sphere 
appealing results compact manifolds boundary tacitly assumed rounding procedure carried case multinomial 
spectral bounds covering numbers rademacher averages turn establishing bounds generalization performance kernel machines information di usion kernels 
adopting approach guo 
estimating covering numbers making bounds spectrum laplacian riemannian manifold vc dimension techniques bounds turn yield bounds expected risk learning algorithms 
calculations give indication underlying geometry uences entropy numbers inverse covering numbers 
show bounds rademacher averages may obtained plugging spectral bounds di erential geometry 
primary drawn analyses point view generalization error bounds di usion kernels behave essentially standard gaussian kernel 
covering numbers recalling main result guo 
modifying notation slightly conform 
compact subset dimensional euclidean space suppose mercer kernel 
denote 
eigenvalues mapping 

dy 
denote corresponding eigenfunctions 
assume ck def sup 
points kernel hypothesis class fx weight vector bounded de ned collection functions fr ff hw kwk rg 
mapping feature space de ned mercer kernel 

denote corresponding hilbert space inner product norm 
interest obtain uniform bounds covering numbers fr de ned size smallest cover fr metric induced norm kfk max jf main result guo 

theorem integer denote smallest integer 
de ne ckr tj 
sup fx fr apply result obtain bounds indices spectral theory riemannian geometry 
bounds eigenvalues laplacian due li yau 
theorem compact riemannian manifold dimension non negative ricci curvature 
denote eigenvalues laplacian dirichlet boundary conditions 
volume constants depending dimension 
note manifold multinomial model satis es conditions theorem 
results establish bounds covering numbers information di usion kernels 
assume dirichlet boundary conditions similar result proven neumann boundary conditions 
include constant vol di usion coecient order indicate bounds depend geometry 
theorem compact riemannian manifold volume satisfying conditions theorem 
covering numbers dirichlet heat kernel satisfy log fr log proof lower bound theorem dirichlet eigenvalues heat kernel satisfy log tc log 
tc log tc log second inequality comes dx upper bound theorem inequality hold tc log tc log equivalently tc log inequality hold case log tc log may assume log new constant 
plugging bound expression theorem algebra log log inverting expression log gives equation 
note theorem guo 
show bound fact depend xed covering numbers scale log log xed scale log di usion time rademacher averages describe di erent family generalization error bounds derived machinery rademacher averages bartlett mendelson bartlett 
bounds fall directly mendelson computing local averages kernel function classes plugging eigenvalue bounds theorem 
seen covering number bounds related complexity term form tj 
case rademacher complexities risk bounds controlled similar simpler expression form smallest integer mendelson acting parameter bounding error family functions 
place context quote results bartlett 
mendelson apply family loss functions includes quadratic loss refer bartlett 
details technical conditions 
independent sample unknown distribution loss function family measurable functions objective minimize expected loss 
inf member inf denotes empirical expectation 
rademacher average family functions fg rg de ned expectation er sup independent rademacher random variables theorem convex class functions de ne er constants depend loss function 
probability additional constants 
suppose mercer kernel ff hk unit ball reproducing kernel hilbert space associated minfr bx bound excess risk kernel machines framework suces bound term minfr involving spectrum 
bounds eigenvalues typically easy 
theorem compact riemannian manifold satisfying conditions theorem 
rademacher term dirichlet heat kernel satis es log constant depending geometry proof minfr tc ce tc constant rst inequality follows lower bound theorem 
case log theorem log log equivalently log follows log new constant bound shown high probability log behavior expected gaussian kernel euclidean space 
covering numbers rademacher averages resulting bounds essentially obtained gaussian kernel dimensional torus standard way euclidean space get laplacian having discrete spectrum results guo 
formulated case corresponding circle bounds di usion kernels derived case positive curvature apply special case multinomial similar bounds general manifolds curvature bounded negative constant attainable 
multinomial di usion kernels text classi cation section application multinomial di usion kernels problem text classi cation 
text processing subject dirty laundry referred documents cast euclidean space vectors special weighting schemes empirically applications information retrieval inspired rst principles 
text multinomial geometry natural motivated experimental results er insight useful geometry may classi cation 
representing documents assuming vocabulary size document may represented sequence words alphabet classi cation tasks unreasonable discard word order humans typically easily understand high level topic document inspecting contents mixed bag words 
denote number times term appears document 
fx sample space multinomial distribution document modeled independent draws xed model may change document document 
natural embed documents multinomial simplex embedding function consider embeddings correspond known feature representations text classi cation joachims 
term frequency tf representation uses normalized counts corresponding embedding maximum likelihood estimator multinomial distribution tf common representation term frequency inverse document frequency df 
representation uses distribution terms documents discount common terms document frequency df term de ned number documents term appears 
variants proposed simplest commonly embeddings df log df log df log df log df number documents corpus 
note text classi cation applications tf df representations typically normalized unit length norm norm joachims 
example tf representation normalization 
similarly df 
support vector machines linear gaussian kernels normalized tf df achieve higher accuracies normalized counterparts 
di usion kernels normalization necessary obtain embedding simplex 
di erent embeddings feature representations compared experimental results reported 
clear list kernels compare 
linear kernel lin gaussian kernel gauss exp squared euclidean distance 
multinomial di usion kernel mult exp arccos derived section 
experimental results experiments multinomial di usion kernel tf embedding compared linear gaussian rbf kernel tf df embeddings support vector machine classi er webkb reuters collections standard data sets text classi cation 
shows test set error rate webkb data representative instance versus classi cation task designated class course 
results choices positive class qualitatively similar results summarized table 
similarly shows test set error rates versus experiments reuters data designated classes chosen acq 
results reuters versus tasks shown table 
webkb dataset contains web pages sites universities craven 
pages classi ed student faculty course project sta pages categories contain instances respectively 
student faculty course project classes contain documents restricted attention classes 
reuters dataset collection newswire articles classi ed news topic lewis ringuette 
topics topics fewer documents reason restricted attention frequent classes earn acq grain crude sizes documents respectively 
webkb reuters collections created types binary classi cation tasks 
rst task designate speci class label document class positive example label document topics negative example 
second task designate class positive class choose experimental results webkb corpus svms linear dotted gaussian dash dotted kernels compared di usion kernel multinomial solid 
classi cation error task labeling course vs faculty project student shown plots function training set size 
left plot uses tf representation right plot uses df representation 
curves shown error rates averaged fold cross validation error bars representing standard deviation 
results vs labeling tasks qualitatively similar shown 
results webkb corpus svms linear dotted gaussian dash dotted kernels compared di usion kernel solid 
course pages labeled positive student pages labeled negative results label pairs qualitatively similar 
left plot uses tf representation right plot uses df representation 
experimental results reuters corpus svms linear dotted gaussian dash dotted kernels compared di usion kernel solid 
classes acq top bottom shown classes qualitatively similar 
left column uses tf representation right column uses df 
curves shown error rates averaged fold cross validation error bars representing standard deviation 
experimental results reuters corpus svms linear dotted gaussian dash dotted kernels compared di usion solid 
classes top grain bottom labeled positive class earn labeled negative 
left column uses tf representation right column uses df representation 
tf representation df representation task linear gaussian di usion linear gaussian di usion course vs faculty vs project vs student vs table experimental results webkb corpus svms linear gaussian multinomial di usion kernels 
left columns tf representation right columns df representation 
error rates shown averages obtained fold cross validation 
best performance training set size shown boldface 
di erences statistically signi cant paired test level 
negative class frequent remaining class student webkb earn reuters 
cases size training set varied keeping proportion positive negative documents constant training test set 
show representative results second type classi cation task goal discriminate speci classes 
case webkb data results shown course vs student 
case reuters data results shown vs earn grain vs earn 
results classes qualitatively similar numerical results summarized tables 
tf representation df representation task linear gaussian di usion linear gaussian di usion course vs student faculty vs student project vs student table experimental results webkb corpus svms linear gaussian multinomial di usion kernels 
left columns tf representation right columns df representation 
error rates shown averages obtained fold cross validation 
best performance training set size shown boldface 
di erences statistically signi cant paired test level 
gures leftmost plots show performance tf features rightmost plots show performance df features 
mentioned case di usion kernel normalization give valid embedding probability simplex linear gaussian kernels normalization works better empirically kernels 
curves show test set error rates averaged iterations cross validation function training set size 
error bars represent standard deviation 
gaussian di usion kernels test scale parameters gaussian kernel di usion kernel set 
results reported best parameter value range 
performed experiments popular mod apte train test split top categories reuters collection 
split training set documents highly biased negative documents 
report table test set accuracies tf representation 
df representation di erence di erent kernels statistically signi cant amount training test data 
provided train set achieve outstanding performance tf representation df representation task linear gaussian di usion linear gaussian di usion earn vs acq vs vs grain vs crude vs table experimental results reuters corpus svms linear gaussian multinomial di usion kernels 
left columns tf representation right columns df representation 
error rates shown averages obtained fold cross validation 
best performance training set size shown boldface 
asterisk indicates di erence statistically signi cant paired test level 
kernels absence cross validation data results noisy interpretation 
results consistent previous experiments text classi cation svms observed linear gaussian kernels result similar performance joachims 
multinomial di usion kernel signi cantly outperforms linear gaussian kernels tf representation achieving signi cantly lower error rate kernels 
df representation di usion kernel consistently tf representation df representation task linear gaussian di usion linear gaussian di usion acq vs earn vs earn grain vs earn crude vs earn table experimental results reuters corpus svms linear gaussian multinomial di usion kernels 
left columns tf representation right columns df representation 
error rates shown averages obtained fold cross validation 
best performance training set size shown boldface 
asterisk indicates di erence statistically signi cant paired test level 
outperforms kernels webkb data usually outperforms linear gaussian kernels reuters data 
reuters data larger collection webkb document frequency statistics basis inverse document frequency weighting df representation evidently ective collection 
notable multinomial information di usion kernel achieves high accuracy heuristic term weighting scheme 
results er evidence multinomial geometry theoretically motivated practically ective document classi cation 
category linear rbf di usion earn acq money fx grain crude trade interest ship wheat corn table test set error rates reuters top classes tf features 
train test sets created mod apte split 
discussion introduced family kernels intimately geometry riemannian manifold associated statistical family fisher information metric 
metric canonical sense uniquely determined requirements invariance choice heat kernel natural ectively encodes great deal geometric information manifold 
geometric perspective statistics led reformulations results viewed traditionally kernel methods developed clearly depend crucially geometry statistical families 
main application ideas develop multinomial di usion kernel 
related spherical geometry multinomial developed 
experimental results indicate resulting di usion kernel ective text classi cation support vector machine classi ers lead signi cant improvements accuracy compared linear gaussian kernels standard application 
results section notable accuracies better comparable obtained heuristic weighting schemes df achieved directly geometric approach 
part attributed role fisher information metric square root embedding sphere terms infrequent document ectively weighted terms typically rare document collection 
primary degree freedom information di usion kernels lies speci cation mapping data model parameters 
multinomial maximum likelihood mapping 
model families mappings remains interesting direction explore 
kernel methods generally model free distributional assumptions data learning algorithm applied statistical models er advantages attractive explore methods combine data models purely discriminative methods 
approach combines parametric statistical modeling non parametric discriminative learning guided geometric considerations 
aspects related methods proposed jaakkola haussler 
kernels proposed current di er signi cantly fisher kernel jaakkola haussler 
particular score log single point parameter space 
case exponential family model covariance covariance heuristically exponentiated 
contrast information di usion kernels full geometry statistical family invariant reparameterization family 
conceptually related belkin niyogi suggest measuring distances data graph approximate underlying manifold structure data 
case underlying geometry inherited embedding euclidean space fisher geometry 
information di usion kernels general dicult compute cases explicit formulas equations hyperbolic space rare 
approximate information di usion kernel may attractive geodesic distance points done multinomial 
cases distance dicult compute exactly compromise may approximate distance nearby points terms kullback leibler divergence relation fisher information noted section 
ect approximation incorporated kernels proposed moreno 
multimedia applications form exp exp viewed terms leading order approximation heat kernel 
results moreno 
suggestive di usion kernels may attractive multinomial geometry complex statistical families 
acknowledgments rob kass leonid jian zhang helpful discussions 
research supported part nsf ccr iis arda contract mda 
mark aizerman emmanuel braverman lev er 
theoretical foundations potential function method pattern recognition learning 
automation remote control 
shun ichi amari hiroshi 
methods information geometry volume translations mathematical monographs 
american mathematical society 
peter bartlett olivier bousquet shahar mendelson 
local rademacher complexities 
manuscript 
peter bartlett shahar mendelson 
rademacher gaussian complexities risk bounds structural results 
journal machine learning research 
mikhail belkin niyogi 
manifold structure partially labeled classi cation 
advances neural information processing systems 
marcel berger paul edmond 
le une 
lecture notes mathematics vol 
springer verlag 
bernhard boser isabelle guyon vladimir vapnik 
training algorithm optimal margin classi ers 
computational theory pages 

statistical decision rules optimal inference volume translation mathematical monographs 
american mathematical society 
mark craven dan dipasquo dayne freitag andrew mccallum tom mitchell kamal nigam se slattery 
learning construct knowledge bases world wide web 
arti cial intelligence 
philip dawid 
comments comments bradley efron 
annals statistics 
tom dietterich 
ai seminar 
carnegie mellon 
alan 
exponential spherical subfamily models 
phd thesis stanford university 
alexander yan 
heat kernel hyperbolic space 
bulletin london mathematical society 
ying guo peter bartlett john shawe taylor robert williamson 
covering numbers support vector machines 
ieee trans 
information theory january 
tommi jaakkola david haussler 
exploiting generative models discriminative classi ers 
advances neural information processing systems volume 
thorsten joachims 
maximum margin approach learning text classi ers methods theory algorithms 
phd thesis dortmund university 
thorsten joachims nello cristianini john shawe taylor 
composite kernels hypertext categorisation 
proceedings international conference machine learning icml 
robert kass 
geometry asymptotic inference 
statistical science 
robert kass paul vos 
geometrical foundations asymptotic inference 
wiley series probability statistics 
john wiley sons 
george kimeldorf grace wahba 
results spline functions 
math 
anal 
applic 
kondor john la erty 
di usion kernels graphs discrete input spaces 
sammut ho mann editors proceedings international conference machine learning icml 
morgan kaufmann 
stefan lauritzen 
statistical manifolds 
amari nielsen kass lauritzen rao editors di erential geometry statistical inference pages 
institute mathematical statistics hayward ca 
david lewis marc ringuette 
comparison learning algorithms text categorization 
symposium document analysis information retrieval pages las vegas nv april 
univ nevada las vegas 
peter li shing tung yau 
estimates eigenvalues compact riemannian manifold 
geometry laplace operator volume proceedings symposia pure mathematics pages 
shahar mendelson 
performance kernel classes 
journal machine learning research 
john milnor 
morse theory 
princeton university press 
pedro moreno ho nuno vasconcelos 
kullback leibler divergence kernel svm classi cation multimedia applications 
advances neural information processing systems 
mit press cambridge ma 
edward nelson 
tensor analysis 
princeton university press 
tomaso poggio girosi 
regularization algorithms learning equivalent multilayer networks 
science 
jay ponte bruce croft 
language modeling approach information retrieval 
proceedings acm sigir pages 
rao 
information accuracy attainable estimation statistical parameters 
bull 
calcutta math 
soc 
steven rosenberg 
laplacian riemannian manifold 
cambridge university press 
richard shing tung yau 
lectures di erential geometry volume conference proceedings lecture notes geometry topology 
international press 
michael 
di erential geometry volume 
publish perish 
zhai john la erty 
study smoothing methods language models applied ad hoc information retrieval 
proceedings sigir pages sept 

