merging splitting eigenspace models new deterministic methods eigenspace models representing set dimensional observations merge models yield representation union sets split model represent difference sets done accurately keep track mean 
methods efficient computing new eigenspace models directly observations dimensionally small compared total number observations 
methods important provide basis novel techniques machine learning dynamic split paradigm optimally cluster observations 
theoretical derivation methods empirical results relating efficiency accuracy techniques general applications including line construction gaussian mixture models 
keywords eigenspace models principal component analysis model merging model splitting merge split 
contributions method merging eigenspace models method splitting eigenspace models explicitly accurately keep track mean observations 
methods merging updating splitting eigenspace models exist generally fail handle change mean adequately 
methods provide update mean properly crucial importance classification problems problems mean represents centre cluster observations class 
classification problems original reason investigating eigenspace model updating 
eigenspace models wide variety applications example classification recognition systems characterising normal modes vibration dynamic models heart motion sequence analysis temporal tracking signals 
motivation arose context building models blood vessels ray interpretation building eigenspace models images 
example image database employees may require frequent changes records methods permit addition deletion new images need recompute eigenspace model ab initio 
incrementally build gaussian mixture models separate gaussian distributions describe data falling clusters classes 
updating means prerequisite case mean represents centre distribution class classification mahalanobis distance measures distance mean units standard deviation 
building models dynamically significant application methods 
currently em algorithm building models 
primarily concerned deriving new theoretical framework merging splitting eigenspaces empirical evaluation new techniques particular application area 
demonstrate methods ways building database images security application dynamic construction gaussian mixture models 
authors department computer science university wales cardiff po box cardiff cf xf wales uk peter cs cf ac uk peter hall david marshall ralph martin eigenspace model statistical description set observations dimensional space model may regarded multi dimensional gaussian distribution 
geometric point view eigenspace model thought characterises set observations centre mean observations axes point directions spread observations maximised subject orthogonal surface contour lies standard deviation mean 
flat certain directions modelled having lower dimension space embedded 
eigenspace models computed eigenvalue decomposition evd called principal component analysis decomposition svd 
wish distinguish batch incremental computation 
batch computation observations simultaneously compute eigenspace model 
incremental computation existing eigenspace model updated new observations 
previous research incremental computation eigenspace models considered adding exactly new observation time eigenspace model 
common theme methods require original observations retained 
description sufficient information incremental computation new eigenspace model 
previous approaches allows change dimensionality single additional axis added necessary 
previous allows shift centre methods keep fixed origin 
proves crucial eigenspace model classification demonstrated set observations mean far origin clearly modelled centred origin 
incremental methods previous observations need kept reducing storage requirements making large problems computationally feasible 
incremental methods observations available simultaneously 
example computer may lack memory resources required store observations 
true low dimensional methods compute eigenspace 
mention low dimensional methods section ii give advantage number observations dimensionality space true observations images 
observations available usually faster compute new eigenspace model incrementally updating existing batch computation 
incremental methods typically compute eigenvectors min 
disadvantage incremental methods accuracy compared batch methods 
incremental updates inaccuracy small probably acceptable great majority applications 
thousands updates eigenspace models incremented single observation time inaccuracies build methods exist circumvent problem 
contrast methods allow new set observations added single step reducing total number updates existing model 
section ii defines eigenspace models detail standard methods computing representing classifying observations 
section iii discusses merging eigenspace models section iv addresses splitting 
section presents empirical results section vi presents applications 
section vii gives 
ii 
eigenspace models section describe mean eigenspace models briefly discuss standard methods batch computation observations represented 
firstly establish notation rest 
vectors columns denoted single underline 
matrices denoted double underline 
size vector matrix important wish emphasise size denoted subscripts 
particular column vectors matrix denoted superscript superscript vector denotes particular observation set observations treat observations column vectors matrix 
example ith column vector matrix 
denote matrices formed concatenation square brackets 
mn matrix vector appended mn column 
theoretical background consider observations column vector compute eigenspace model follows mean observations covariance nn mn note nn real symmetric 
axes spread observations axis eigenvectors eigenvalues eigenproblem nn nn nn nn equivalently eigenvalue decomposition nn nn nn nn nn columns nn eigenvectors nn nal matrix eigenvalues 
eigenvectors orthonormal identity matrix 
nn nn nn ith eigenvector ith eigenvalue ii associated nn eigenvalue length eigenvector ith axis 
typically min eigenvectors significant eigenvalues eigenvectors need retained 
observations correlated covariance matrix approximation small eigenvalues presumed negligible 
eigenspace model spans dimensional subspace ndimensional space embedded 
different criteria discarding eigenvectors eigenvalues exist suit different applications different methods computation 
common methods stipulate fixed integer keep largest eigenvectors keep eigenvectors size larger absolute threshold keep eigenvectors specified fraction energy computed sum eigenvalues retained 
having chosen discard certain eigenvectors eigenvalues recast equation block form matrices vectors 
loss generality permute eigenvectors eigenvalues eigenvectors kept np pp eigenvalues 
nd dd discarded 
may rewrite equation nn nn nn nn pp np nd dp pd dd np pp np nd dd nd nn np pp np np nd error nd dd nd small dd dd define eigenspace model mean reduced set eigenvectors eigenvalues number observations np pp low dimensional computation eigenspace models low dimensional batch methods compute eigenspace models especially important dimensionality observations large compared number 
may compute eigenspace models infeasible 
incremental methods low dimensional approach 
principle computing eigenspace model requires construct matrix dimension observation 
practice model computed matrix number observations 
advantage applications image processing typically show done considering relationship eigenvalue decomposition singular value decomposition 
leads simple derivation low dimensional batch method computing eigenspace model 
results obtained greater length see 
nn set observations shifted mean svd nn nn nn nn nn nn left singular vectors identical eigenvectors previously nn matrix singular values leading diagonal nn nn nn nn right singular vectors 
nn nn orthonormal matrices 
compute eigenspace models lowdimensional way follows nn nn nn nn nn nn nn nn nn eigenproblem 
nn nn presence extra trailing zeros main diagonal nn discard small singular values singular vectors remaining eigenvectors vectors np nn np 
pp result formed basis incremental technique developed murakami kumar allow change origin approach readily generalise merging splitting 
chandrasekaran observe solution matrix product nn lead inaccurate re nn sults conditioning problems develop method incrementally updating svd solutions single observation 
svd methods proven accurate see generalised block updating change mean provided right singular vectors maintained 
seen published derivation long include 
perform svd evd reduced set left singular vectors computed 
allow shift mean block 
derivation long provide brief explanation difficulty 
svd data sets ap erf problem compute svd ap requires terms left hand side separated difficult 
approach follows bunch authors know address problem multiply right transpose matrices inner product converting problem evd follows ap ap erf erf leading ap cq er conclude general dynamic change svd possible directly case evd 
svd methods proposed quite early development incremental eigenproblem analysis 
early included proposal delete single observations extend merging splitting 
svd formed basis proposal incrementally update eigenspace observations step 
contrary method possible change dimension solution eigenspace considered 
furthermore methods considered change origin 
incremental method matrix product nn specifically approximation equation 
generalisation earlier appears naturally special case adding single observation 
nn nn representing classifying observations high dimensional observations may approximated lowdimensional vector eigenspace model 
eigenspace models may classification 
briefly discuss ideas prior results section 
dimensional observation represented eigenspace model np pp dimensional vector np xn shifts observation mean represents components eigenvector 
called karhunen lo transform 
dimensional residue vector defined np xn np np xn orthogonal vector np residue error representation respect 
likelihood associated observation exp exp nn det nn nn nn nn det nn clearly definition directly cases nn rank degenerate 
cases alternative definition due moghaddam pentland scope 
iii 
merging eigenspace models turn attention main contributions merging eigenspace models 
derive solution problem 
nn nm sets observations 
eigenspace models np pp nq qq respectively 
problem compute eigenspace model nr rr nn nm 
clearly total number new observations combined mean nn combined covariance matrix zz nc nn xt md nn zz nn nn nm nn nn covariance matrices nn nm respectively 
wish compute eigenvectors eigenvalues satisfy nn ns ss ns eigenvalues subsequently discarded give nonnegligible eigenvectors eigenvalues 
problem solved size necessarily bounded max explain surprising additional upper limit section iii briefly needed allow vector difference means method solution problem may solved steps construct orthonormal basis set ns spans eigenspace models basis differs required eigenvectors ns rotation ss ns ns ss ns derive intermediate eigenproblem 
solution problem provides eigenvalues ss needed merged eigenmodel 
eigenvectors ss comprise linear transform rotates basis set ns compute eigenvectors ns discard eigenvectors eigenvalues chosen criteria discussed yield nr rr give details step 
construct orthonormal basis set construct orthonormal basis combined choose set orthonormal vectors span subspaces subspace spanned eigenvectors np subspace spanned tors nq subspace spanned 
single vector 
necessary vector joining centre eigenspace models need belong eigenspace 
accounts additional upper limit bounds equation 
see consider pair dimensional eigenspaces embedded dimensional space 
eigenvectors eigenspace define parallel planes separated vector perpendicular 
clearly merged model ellipse vector origins models contain component perpendicular eigenspaces 
sufficient spanning set ns np nt nt orthonormal basis set component eigenspace orthogonal eigenspace addition accounts component orthogonal eigenspaces construct nt start computing residues eigenvectors nq respect eigenspace pq np nq nq nq np pq nq orthogonal np sense general nq zero vectors vectors represent intersection eigenspaces 
zero vectors removed leave compute residue nq respect eigenspace equation 
computed finding orthonormal basis nt nq sufficient ensure orthonormal 
ns schmidt may nt nq forming intermediate eigenproblem form new eigenproblem substituting equation equation result equation equation obtain nn nn nm np nt ss ss ss np nt multiplying sides left np nt right np nt fact np nt left inverse np nt obtain np nt nn nn nm np nt ss ss ss new eigenproblem solution eigenvectors constitute ss seek eigenvalues provide eigenvalues combined eigenspace model 
know covariance matrices nn nn eliminated follows term equation proportional np nt nn np np nn np nt nn np np nn nt nt nn nt equation np nn np pp np nt con pt struction equation conclude np nt nn np nt pp tp pt tt second term equation proportional np nt nn np nt np nn np nt nn np np nn nt nt nn nt 
nn nq qq substitution gives right nq hand side np nq qq nq np nt nq qq nq np nq qq nq nt nt nq qq nq nt equation pq np nq set tq nt nq obtain np nt np nt qq pq tq qq pq consider final term equation np np nt np pq qq tq tq qq tq np nt np nt np nt nt nt setting np nt new eigenproblem solved may approximated nm qq pq tq qq pq pp tp pq qq tq tq qq tq pt tt ss ss ss matrix size min 
eliminated need original covariance matrices 
note reduces size central matrix left hand side 
crucial computational importance eigenproblem tractable cases dimension datum large case image data 
computing eigenvectors matrix ss eigenvalue matrix set compute 
eigenvectors ss comprise rotation ns equation compute eigenvectors ss eigenvectors eigenvalues need kept may discarded criterion previously discussed section ii 
discarding eigenvectors eigenvalues usually carried time pair eigenspace models merged 
notice sources error 
rounding error introduced finite machine precision 
second source error introduced truncation eigenmodel discarding eigenvectors eigenvalues 
dominant source precise behaviour deserves investigation theoretically empirically 
experience backs intuition discarding eigenvectors eigenvalues worsens approximation 
furthermore note require lower bound number eigenvectors contrast svd methods right singular vectors kept block update shifting mean 
discussion form solution briefly justify solution obtained correct form considering special cases 
suppose eigenspace models null specified 
system clearly degenerate null eigenvectors zero eigenvalues computed 
exactly eigenspace model null non null eigenspace model computed returned process 
see suppose null 
second third matrices left hand side equation disappear 
matrix reduces pp exactly eigenvalues remain unchanged 
case rotation ss identity matrix eigenvectors unchanged 
null model second matrix remain 
nt nq related rotation identical 
solution eigenproblem computes inverse rotation eigenspace model remain unchanged 
suppose exactly observation specified 
middle term left equation disappears nt unit vector direction scalar eigenproblem pp gp exactly form obtained observation explicitly added proven 
special case interesting properties new observation lies subspace spanned np change tors eigenvalues explained rotation scaling caused furthermore event right matrix disappears altogether case eigenvalues scaled eigenvectors unchanged 
indicating stable model limit 
exactly observation specified 
matrix left equation disappears 
pq zero matrix component nt nq orthogonal eigenspace 
eigenproblem tq qq tq case tq nq nq tq qq tq form row column zeros appended 
qq nq 
substitution terms shows case solution reduces special case adding single new observation equation form equation readily shown 
models identical case third term left equation disappears 
furthermore tq zero matrix pq np identity matrix np second matrices left equation identical reduce matrices eigenvalues 
adding identical yields third identical respect change number observations 
notice fixed solution tends model 
conversely fixed solution tends model 
tend simultaneously final term loses significance 
algorithm completeness express mathematical results obtained merging models form algorithm direct computer implementation see 
function merge returns ug column vector discard column small magnitude 
endfor ug size size number basis vectors construct lhs equation eigenvalues eigenvectors discard small appropriate fig 

algorithm merging eigenspace models 
complexity computing eigenspace model size single batch may incur computational cost 
experimental results bear faster methods available svd 
examination merging algorithm shows requires intermediate eigenvalue problem solved steps giving cubic computational requirements 
suppose observations represented eigenvectors observations represented eigenvectors 
typically respectively 
compute model batch method requires operations 
assuming models merged known merging method requires operations problem solved smaller greater amount overlap eigenspaces 
fact number operations required see section iii models merged unknown initially incur extra cost reduces advantage 
typical scenario expect known existing large database observations relatively small batch observations added 
case extra penalty little significance compared 
exact analysis complicated data dependent expect efficiency gains time memory resources practice 
furthermore computer memory limited subdivision initial set may unavoidable order reduce eigenspace model computation tractable problem 
iv 
splitting eigenspace models show split eigenspace models 
eigenspace model nr rr remove nq qq give third model np pp 
rr ss available general 
ask reader carefully note splitting means removing subset observations method inverse merging sense 
impossible regenerate information discarded model created batch methods 
split eigenspace model larger eigenvectors form subspace larger 
derivation algorithm splitting follow straightforward way analogy merging 
state results splitting proof 
clearly new mean case merging new eigenvalues eigenvectors computed intermediate eigenproblem 
case rr rp rp gt rr rr rp nr nq nr 
eigenvalues seek non zero elements diagonal rr permute rr rr write loss generality rr rrr rr rp rt pp rp pp rp tp pt tt rp rt need identify eigenvectors rr non zero eigenvalues compute np np nr rp terms complexity splitting involve solution eigenproblem size algorithm splitting may readily written similar approach merging 
results section describes various experiments carried compare computational efficiency batch method new methods merging splitting compare eigenspace models produced 
compared models terms euclidean distance means mean angular deviation corresponding eigenvectors mean relative absolute difference corresponding eigenvalues 
doing took care models number dimensions 
simple measures performance measures may relevant eigenspace models particular applications tests performed 
eigenspace models may approximating high dimensional observations low dimensional vector error size residue vector 
sizes residue vectors readily compared batch incremental methods 
eigenspace models may classifying observations giving likelihood observation belongs cluster 
different eigenspace models may compared relative differences likelihoods 
average differences corresponding observations 
database face images pixels available line tests reported similar results obtained tests randomly generated data 
gray levels images scaled range division preprocessing done 
implemented functions commercially available software matlab computer standard configuration sun sparc ultra hz mb ram 
results images physical resources computer meant heavy paging started occur limit batch method paging affect incremental method 
tests experimental procedure compute eigenspace models batch method compare models produced merging splitting models produced batch method 
case largest data sets contained images 
partitioned data sets containing multiple images 
included degenerate cases model contained zero images 
note tested smaller models merged larger ones vice versa 
number eigenvectors retained model including merged model set maximum ease comparing results 
initial tests strategies indicate resulting eigenspace model little effected 
timing measuring cpu time ran code times chose smallest value minimise effect concurrently running process 
initially measured time taken compute model batch methods data sets different sizes 
results show cubic complexity predicted 
merging measured time taken merge previously constructed models 
results shown 
shows time complexity approximately symmetric point half number input images 
result may surprising algorithm merging symmetric respect inputs despite fact mathematical solution independent order 
approximate symmetry explained assuming independent eigenspaces fixed upper bound number eigenvectors suppose numbers eigenvectors models complexities main steps approximately follows computing new spanning set solving eigenproblem rotating new eigenvectors 
time complexity stated conditions approximately symmetric 
olivetti database faces www cam orl uk 
cpu time seconds observed data cubic fit number input images fig 

time compute eigenspace model batch method versus number images time approximated cubic cpu time seconds incremental time cubic joint time approximation number images model number images second model number number images model fig 

time merge eigenspace models images merge versus number images 
number images total number different images compute constant 
times taken compute eigenspace model images total batch method merging method compared 
incremental time time needed compute eigenspace model merged merge precomputed existing 
joint time time compute smaller merge 
expected incremental time falls additional number images required falls 
joint time approximately constant similar total batch time 
incremental method offers time saving cases memory 
clearly seen model computed images paging effects set batch method time taken rose seconds 
time produce equivalent model merging sub models size took half 
splitting time complexity splitting eigenspaces depend principally size large eigenspace smaller space removed size smaller eigenspace little effect 
size cpu time seconds incremental time joint time batch time number images model fig 

time complete eigenspace model database images 
incremental time addition time construct eigenspace added 
joint time time compute eigenspace models merge 
eigenproblem solved depends size larger space dominates complexity 
expectations borne experimentally 
computed large eigenmodel images 
removed smaller models sizes images inclusive steps images 
eigenvectors kept model average time taken approximately constant ranged seconds mean time seconds 
figures smaller observed merging large eigenspace contains eigenvectors 
matrices involved computation size merging size computations involved computing orthonormal basis 
similarity performance measures assessing similarity performance batch incremental methods described 
merging compared means models produced method euclidean distance 
distance greatest models merged number input images case fall smoothly zero models merged empty 
value maximum typically small measured units gray level 
compares favourably working precision matlab compared directions eigenvectors produced method 
error eigenvector direction measured mean angular deviation shown 
ignoring degenerate cases models empty see angular deviation single minimum eigenspace models built number images 
may small model added large model information tends swamped 
results show angular deviation small average 
sizes eigenvalues methods compared 
general observed smaller eigenvalues larger errors expected contain relatively little information susceptible noise 
give mean absolute difference eigenvalue 
rises single peak number input images models 
maximal value small units gray level 
largest eigenvalue typically 
eigenvector mean angular deviation degrees number input images fig 

angular deviation eigenvectors produced batch incremental methods versus number images eigenspace model 
eigenvalue mean absolute deviation number input images fig 

difference eigenvalues produced batch incremental methods versus number images eigenspace model 
turn performance measures 
merged eigenspaces represent image data little loss accuracy measured mean difference residue error 
performance measure typically small units gray level pixel clearly noticeable effect 
compared differences likelihood values equation produced methods 
difference small typically order shows compared mean likelihood observations order differences classifications models small 
splitting similar measures splitting computed exactly conditions described testing timing splitting exactly characteristics described merging 
case model subtracted computed batch method removed model splitting procedure 
batch model purposes comparison residual data set 
follows phrase size removed eigenspace means number images construct eigenspace removed difference mean residue number input images fig 

difference reconstruction errors pixel produced batch incremental methods versus number images eigenspace model 
mean class difference number input images fig 

difference likelihoods produced batch incremental methods versus number images eigenspace model 
eigenspace built images 
euclidean distance means models produced method grows monotonically size removed eigenspace falls exceeds gray level units 
splitting slightly accurate respect merging 
mean angular deviation corresponding eigenvector directions rises similar fashion degrees size removed eigenspace removed eigenmodel size 
represented maximum deviation error error degree obtained removed model size 
angular deviations somewhat larger merging 
mean difference eigenvalues shows general trend 
maximum units gray level size removed eigenspace 
larger error case merging relatively small compared maximum eigenvalue 
case merging deviation eigenvalue grows larger size importance eigenvalue falls 
difference reconstruction error rises size removed eigenspace falls 
size order units gray level pixel negligible 
difference likelihoods significant relative difference cases factors 
conducting experiments relative difference sensitive errors introduced eigenvectors eigenvalues discarded 
surprise likelihood differences magnified exponentially 
changing criteria discarding eigenvectors reduced relative difference likelihood order achieved cases 
conclude application require splitting require classification eigenvectors eigenvalues discarded care 
suggest keeping eigenvectors corresponding eigenvalues exceed threshold 
trend clear accuracy performance grew worse measure size eigenmodel removed falls 
vi 
applications turn applications methods 
experimented building point distribution models dimensional blood vessels texture classification similar methods updating image motion parameters selecting salient views building large image databases 
feel appropriate discuss applications general nature intention furnish reader practically useful appreciation characteristics methods avoid particular specific application 
chose building large databases images security application dynamic construction gaussian mixture models 
building large database obvious application methods build eigenspace images store memory 
arise case large databases previously suggested 
intuition suggests images database better represented model construction evd svd fits hyperplane data squares sense 
test built images subset images batch incremental methods small test set allow comparisons ideal case 
expected experiments batch incremental eigenspaces turned similar comparing models built subset images models built 
cases models built subset images represented images construction low residue error 
images construction badly represented having high residue error 
images eigenmodel fit improved images previous subset slightly represented subset better represented 
result confirms evd svd models generalise 
classification results follow similar trend image better classified eigenspace uses images 
conclude constructed data possible cases incremental methods provide option 
turn attention applications substantive nature 
security application aim show methods useful classification applications 
update mean statistical computations commonly classification mahalanobis distance require accurate mean 
consider security application identification 
scenario wishing efficiently store photographs thousands employees security reasons admitting entry building laboratory 
chose store data eigenmodel images projected eigenmodel stored tens thousands numbers 
conventional batch methods eigenmodel images fit memory 
additionally database requires changing year employees come go 
methods allow eigenspace constructed maintained 
initial eigenmodel constructed building eigenspaces large possible merging data large 
eigenmodel maintained simply merging splitting required 
note splitting means removal images database 
illustrate data base faces previously 
constructed eigenmodel selection people photographs person 
recognise individual new photograph weight evidence database database 
compute weight maximum mahalanobis distance moghaddam pentland method photographs construct eigenmodel 
new photograph judged mahalanobis distance maximum 
person photographs associated compute weight person fraction photographs classified 
recognise crude measure merits fold provides economic alternative extensive image processing aligning faces segmenting shape texture second measure sufficient demonstrate update image databases classification measure aim 
initialised eigenmodel people images 
change adding second person removing arbitrary convenient choices 
show weight evidence measured change 
upper plot shows measure images batch model 
lower plot shows measure images 
notice models produce false positives sense people classified weight larger zero 
notice incrementally computed eigenspace gives rise false positives eigenmodel computed batch methods line earlier observations subtraction 
weight evidence factor case matter eigenmodel computed fact sophisticated test pre processing eliminate false positives point develop fully operational robust security application demonstrate potential methods classification 
conclude additive incremental eigenanalysis safe classification metrics subtractive incremental eigenanalysis needs greater degree caution 
dynamic gaussian mixture models interested methods construct dynamic gaussian mixture models gmm 
models increasingly common vision literature method dynamic construction useful 
methods possible 
note block updating maintainance mean prerequisites dynamic gmms 
focus merging existing gmms show construct dynamic gmm library photographs 
aim discuss issues surrounding dynamic gmms full unduly extend seek demonstrate dynamic gmms feasible methods 
partition data sets set construct gmm follows data set build eigenmodel incremental methods necessary 
second project datum set eigenmodel 
thirdly construct gmm projected weight evidence weight evidence weight person person database person index weight person person database person index fig 

weight evidence measures change batch top incremental 
data em algorithm 
represent gaussian mixture eigenmodel 
gmm hierarchy eigenspace models 
regard similar hierarchy models proposed improve specificity 
gaussians need dimension 
merge gmms merged base eigenspaces 
second transformed dependent previous model new basis eigenspace 
merged new dependent eigenspaces sufficiently close 
simple volume measure adequate cases 
volume semi axes element square root eigenvalue dimension characteristic radius square root mahalanobis distance permanently merged pair gmm sum individual volumes greater volume merged 
works dimensionality problems notwithstanding 
example photographs distinct toys photographed degree angles turntable 
photographs 
examples photographs seen 
photographs input groups photographs 
group eigenmodel projected photographs eigenmodel projections construct gmm eighteen clusters 
gaussians making mixture represented eigenmodel 
gmms wanted merge large gmm 
fig 

example images toy 
merge gmms added added eigenspaces complete eigenspace 
transformed gmm clusters space bringing ensemble clusters common space 
gaussian cluster mixture model new space represented eigenmodel 
merged cluster pairwise volume criterion 
able reduce number gaussians mixture 
clusters tend model different parts cylindrical trajectories original data projected large eigenspace 
examples cluster centres shown toys clearly seen different positions 
clusters may identify toy pose example 
murase nayar recognise pose 
addition clusters occupying space toys example seen 
artifact clustering appears derive high dimensionality space clusters side effect method 
notice clusters removed picture matches 
conclude experiments dynamic gmms feasible proposition methods 
note ability merge complete spaces updating mean prerequisites dynamic gmms 
dynamic gmms important application deserve attention 
vii 
shown merging splitting eigenspace models possible allowing sets new observations processed 
theoretical results novel experimental results show methods wholly practical computation times feasible advantageous compared batch methods 
batch incremental eigenspaces similar performance characteristics residue error differ little 
methods useful applications illustrated general nature 
concluded merging eigenspaces stable reliable advise caution splitting 
splitting principle weakness methods interesting ask process reliable 
point omissions providing avenue 
performed analytic error analysis relying experiment 
errors arise discarding eigenvectors eigenvalues 
best knowledge unique compared method 
previous considered inclusion fig 

dynamic gaussian mixture models 
top line shows examples original photographs 
examples cluster centres 
arranged show clusters toy space 
single new datum able comparisons 
svd tends accurate updating mean crucial classification applications 
note demonstrated adding datum time accurate adding complete spaces 
omitted derivation block update svd change mean want space 
indicated svd require evd step 
general applications particular application allows highlight important generic applications 
expect methods find wider applicability mentioned updating image motion parameters selecting salient views building large image databases applications exist 
methods routinely construct impossible means allowed experiment image compression methods 
experimented image segmentation building models dimensional blood vessels texture classification 
believe dynamic gaussian mixture models provide interesting path 
bunch nielsen sorenson 
rank modification symmetric eigenproblem 
numerische mathematik 
bunch nielsen 
updating singular value decomposition 
numerische mathematik 
chandrasekaran manjunath wang winkler zhang 
eigenspace update algorithm image analysis 
graphical models image processing september 
roberts 
efficient numerically rank updating 
ieee trans 
acoustics speech signal processing february 
murakami kumar 
efficient calculation primary images set images 
ieee trans 
pattern analysis machine intelligence september 
moghaddam pentland 
probabilistic visual learning object representation 
ieee trans 
pattern analysis machine intelligence july 
nastar ayache 
frequency nonrigid motion analysis application dimensional medical images 
ieee trans 
pattern analysis machine intelligence november 
chaudhuri sharma chatterjee 
recursive motion parameters 
computer vision image understanding november 
peter hall milton ngan peter 
reconstruction vascular networks dimensional models 
ieee transactions medical imaging december 
hall 
drawing example 
th eurographics uk conference pages leeds 
heap hogg 
improving specificity pdms approach 

british machine conference pages 
hinton dayan revow 
modeling manifolds images handwritten digits 
ieee trans 
neural networks january 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
included 
sirovich kirby 
low dimensional procedure characterization human faces 
journal optical society america march 
chien fu 
generalised karhunen loeve expansion 
ieee trans 
information theory 
golub van loan 
matrix computations 
johns hopkins 
gu 
stable fast algorithm updating singular value decomposition 
technical report yale dcs rr department computer science yale university 
cootes taylor cooper graham 
training models shape sets examples 
proc 
british machine vision conference pages 
hi 
murase nayar 
visual learning recognition objects appearance 
international journal computer vision 
pinz 
active object recognition parametric eigenspace 
british vision conference pages 

