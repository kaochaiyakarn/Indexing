merl mitsubishi electric research laboratory www merl com structure learning conditional probability models entropic prior parameter extinction matthew brand tr august introduce entropic prior multinomial parameter estimation problems solve maximum posteriori map estimator 
prior bias maximally structured minimally ambiguous models 
conditional probability models hidden state iterative map estimation drives weakly supported parameters extinction effectively turning 
structure discovery folded parameter estimation 
establish criteria simplifying probabilistic model graphical structure trimming parameters states guarantee deletion increase posterior probability model 
trimming accelerates learning model 
operations monotonically maximally increase posterior probability yielding structure learning algorithms slightly slower parameter estimation expectation maximization em orders magnitude faster search structure induction 
applied hidden markov model hmm training resulting models show superior generalization held test data 
cases resulting models sparse concise interpretable hidden states strongly correlate meaningful categories 
may copied reproduced part commercial purpose 
permission copy part payment fee granted nonprofit educational research purposes provided partial copies include notice copying permission mitsubishi electric information technology center america acknowledgment authors individual contributions applicable portions copyright notice 
copying reproduction republishing purpose shall require license payment fee mitsubishi electric information technology center america 
rights reserved 
copyright mitsubishi electric information technology center america broadway cambridge massachusetts publication history 
limited circulation oct 

submitted neural computation dec 

accepted aug 

modified released aug 
structure learning conditional probability models entropic prior parameter extinction matthew brand oct revised aug introduce entropic prior multinomial parameter estimation problems solve maximum posteriori map estimator 
prior bias maximally structured minimally ambiguous models 
conditional probability models hidden state iterative map estimation drives weakly supported parameters extinction effectively turning 
structure discovery folded parameter estimation 
establish criteria simplifying probabilistic model graphical structure trimming parameters states guarantee deletion increase posterior probability model 
trimming accelerates learning model 
operations monotonically maximally increase posterior probability yielding algorithms slightly slower parameter estimation expectationmaximization em orders magnitude faster search structure induction 
applied hidden markov model hmm training resulting models show superior generalization held test data 
cases resulting models sparse concise interpretable hidden states strongly correlate meaningful categories 
probabilistic models widely model classify signals 
efficient algorithms fitting models data user obliged specify structure model hidden variables hidden observed variables interact independent 
particularly important data incomplete hidden structure case model structure hypothesis causal factors observed 
typically user guesses may introduce unintended assumptions model 
testing guess computationally intensive methods comparing results debated dietterich 
process tedious necessary structure primary determinant model selectivity speed computation 
shares structure learning view science seeks discover relations hidden processes observable effects structure part model sheds light phenomenon modeled 
show fold structure learning highly efficient parameter estimation processes expectation maximization em 
introduce entropic prior apply multinomials building blocks conditional probability models 
prior bias sparsity structure determinism probabilistic models 
iterative maximum posteriori map estimation prior tends drive weakly supported parameters extinction sculpting lower dimensional model structure comes reflect data 
accelerate process establish weakly supported parameters trimmed model 
transform removes model local probability maximum simplifies opens training 
operations monotonically increase posterior probability training proceeds directly locally optimal structure parameterization 
attractive properties em retained polynomial time re estimation monotonic convergence non zero initialization maximal gains step 
develop entropic prior map estimator trimming criterion models containing multinomial parameters 
demonstrate utility prior learning structure mixture models hidden markov models hmms 
resulting models topologically simpler show superior generalization average generalization measured prediction classification held data 
interesting property prior leads models interpretable discover interesting deep structure dataset just looking learned structure entropically trained model 
deriving main results inx 
inx mixture models visually illustrate difference entropic conventional estimation 
develop train trim algorithm transition matrix hmms experimentally compare entropically conventionally estimated hmms 
inx extend algorithm output parameters discrete output hmms explore ability find meaningful structure datasets music text 
inx draw connections literatures hmm model induction maximum entropy methods 
inx discuss open questions potential weaknesses approach 
show entropic map estimator solves classic problem graph theory raise interesting mathematical questions arise connection prior 
maximum structure entropic prior claims prior beliefs compelling reasons specify prior probability density function 
likelihood function interpreted bauer pointed possible larger gains initializations near solution cost losing convergence guarantees initializations 
structure learning density specifying measure parameter space provided prior 
modeler simply wants data speak prior non informative invariant particular way likelihood function parameterized 
common follow laplace specify uniform prior pu parameter values knows parameter values best fit unobserved evidence laplace 
main appeal non informative prior problem reduces maximum likelihood ml equations conveniently tractable 
uniform prior invariant problem exp probably underestimates prior knowledge prior beliefs specific problem prior beliefs learning model 
entropic estimation assert parameters reduce uncertainty improbable 
example multinomial distribution mutually exclusive kinds events parameter chance adds information model wasted degree freedom 
hand parameter near zero removes degree freedom making model selective resistant overfitting 
view learning process increasing specificity model equivalently minimizing entropy 
capture intuition simple expression takes particularly elegant form case multinomials pe exp log pe non informative degree favor parameter set provided specify equally uncertain models 
invariant entropy function model distribution parameterization 
alternatively define entropy terms distribution canonical parameterization exists 
inx discuss prior derived mathematically 
concentrate behavior 
convex curve shows prior averse chance values favors parameters near extremes 
combining prior multinomial yields entropic posterior pe pe ny ny ny non negative evidence event type shows ample evidence distribution sharply peaked maximum likelihood estimate scant evidence flattens skews stronger odds 
note opposite behavior obtains dirichlet prior dir learning bayes net parameters data heckerman 
dirichlet map estimate skews weaker odds 
lower case probabilities capital probability distribution functions subscripted pe having entropy term 
structure learning posterior probability map values entropic distributions coin flips ratios total coin flips entropic prior ml estimate binomial parameter entropic binomial parameter entropic map estimates evidence ratios total evidence map values data size entropic posterior distributions binomial models tg weighted coin sample statistics tg indicate heads twice tails 
mass data varied curves 
convex curve pe exp shows extremal values preferred absence evidence 
dotted show map estimates 
map estimates function mass data 
map estimates converge ml estimates 
structure learning prior pe initially formulated push parameters far possible non informative initializations 
subsequently discovered interesting connection maximum entropy methods 
methods typically seek weakest model explain data 
seek strongest sparsest structured closest deterministic model compatible data 
brand resolve apparent opposition showing minimum entropy prior constructed directly maximum entropy considerations 
map estimator maximum posteriori estimator yields parameter values maximize probability model data 
analytic form available leads learning algorithms considerably faster precise gradient methods 
obtain map estimates entropic posterior set derivative log posterior zero lagrange multiplier ensure nx log ny log nx nx log yields system simultaneous transcendental equations 
widely known non algebraic systems mixed polynomial logarithmic terms eqn 
solved 
solve lambert function inverse mapping satisfying log log setting working backwards eqn 
log ex log ex log log ex log ex log setting log eqn 
simplifies eqn 
log 
log log log implies log log structure learning eqns 
define fix point turn yields fast iterative procedure entropic map estimator calculate normalize calculate repeat 
may understood measure dynamic range increases convergence fast initial guess hlog 
iff typically takes iterations converge machine precision 
calculations involve adding values logarithms care taken avoid loss precision near branch points dynamic ranges greater ulp case machine precision exhausted intermediate values polish result newton raphson 
appendix recurrences computing interpretation entropic map estimator strikes balance favors fair ml parameter values data extensive biases low entropy parameters data scarce 
patterns large samples significant small datasets patterns may plausibly discounted accidental properties sample noise sampling artifacts 
entropic map estimator may understood select strongest hypothesis compatible data best unbiased model 
say better start strong opinions moderated experience correct predictions garner credibility incorrect predictions provide diagnostic information learning 
note balance determined mass evidence may artificially adjusted scaling 
formally manipulation posterior eqn 
allows understand map estimate terms entropies max log pe min log min min nx nx nx ny log log log log log nx min log log log min 
minimizing sum entropies map estimator reduces uncertainty respects 
term sum useful interpretation entropy measures ambiguity model 
cross entropy measures divergence nx structure learning parameters data descriptive statistics lower bound expected number bits needed code aspects dataset captured model noise 
problems hidden variables expected sufficient statistics computed relative structure model 
lower bound expected number bits needed specify variations allowed model instantiated data 
declines model increasingly structured near deterministic 

declines model comes agree underlying structure data 
declines residual aspects data captured model structured approaching pure normally distributed noise 
alternatively understand eqn 
show map estimator minimizes lower bound expected coding lengths model data relative 
light entropic em search highly efficient form structure learning minimum description length constraint 
training entropic posterior defines distribution possible model structures parameterizations class small accurate models having minimal ambiguity joint distribution probable 
find models simply replace step em entropic map estimator effect step distributes probability mass unevenly model model perfect accordance intrinsic structure training data 
estimator dynamic range multinomials improbable parts model 
drives weakly supported parameters zero concentrates evidence surviving parameters causing estimates approach ml estimate 
structurally irrelevant parts model gradually expire leaving skeletal model surviving parameters increasingly supported accurate 
trimming map estimator increases structure model driving irrelevant parameters asymptotically zero 
explore conditions reify behavior altering graphical structure model removing dependencies variables 
entropic prior licenses simple tests identify opportunities trim parameters increase posterior probability model 
may trim parameter loss likelihood balanced gain prior pe ijx jx jn pe pe pe pe jn log pe log pe log log jn structure learning log log jn small positive substitute differentials log sum parameter trimmed varying increases entropy faster log likelihood 
combination left right terms eqns 
yield trimming criterion 
example may substitute entropic prior multinomials left hand eqn 
set right hand eqn 
yielding log ilog dividing exponentiating obtain log exp conveniently gradient log likelihood log calculated re estimation learning algorithms 
trimming accelerates training removing parameters decay asymptotically zero 
mathematics recommendation trim matter practice wait model near convergence 
trimming bumps model local probability maximum parameter subspace simpler geometry enabling training 
trimming near convergence give confidence training nearly extinct parameter 
note model life long learning periodic gradual retraining samples slowly evolving non stationary process trimming advised nearly extinct parameters may revived model new structures arise process evolves 
mixture models semi parametric distributions mixture cluster models usually require iterative estimation single multinomial mixing parameters step em calculate expected sufficient statistic usual nx probability mixture component ci th data point 
dividing yields maximum likelihood estimate conventional step 
structure learning initial conventional entropic mixture models estimated entropically right conventionally center identical initial conditions left 
dots data points sampled annular region ellipses iso probability contours gaussian mixture components 
entropic estimation apply entropic map estimator obtain trimming criterion derives directly eqn 
log exp exp nx pm known annulus problem bishop affords opportunity visually illustrate qualitative difference entropically conventionally estimated models random points sampled annular region gaussian components form mixture model 
shows entropic estimation effective procedure discovering essential structure data 
components cause fitting removed surviving components provide coverage data 
maximum likelihood model captive accidental structure data irregularities sampling 
case examples entropic estimation took roughly half iterations conventional em 
conventional em method theory cause excess gaussian components collapse individual data points leading infinite likelihoods 
problem ameliorated entropic framework components typically trimmed collapse 
continuous output hmms hidden markov model dynamically evolving mixture model mixing probabilities time step conditioned previous time step matrix transition probabilities 
hmms mixture components known states 
transition matrix stack multinomials probability state structure learning initial conventional entropic initial baum welch entropically re estimated transition matrices 
row depicts transition probabilities single state white zero 
matrices fully upper diagonal rightmost sparse 
state ith element row entropic estimation hmm transition probabilities conventional step obtain probability mass transition ijj xt ijj transition probability state xt probability state outputting data point xt step statistics obtained forward backward analysis rabiner 
map step calculate new applying entropic map estimator igi 
conventional baum welch re estimation uniform prior simply sets ijj compared entropically conventionally estimated continuous output hmms sign language gesture data provided computer vision lab starner pentland 
experimental conditions subsequent tests detailed appendix entropic estimation consistently yielded hmms simpler transition matrices having parameters near zero lower entropy dynamical models 
tested held sequences source entropically trained hmms fit yielded higher log likelihoods held test data conventionally trained hmms 
analysis variance indicates result significant equivalently probability observed superiority entropic algorithm due chance factors 
translated improved classification entropically estimated hmms yielded superior generalization binary gesture classification task measuring statistical significance mean difference correct classifications 
interestingly dynamic range surviving transition parameters far greater obtained conventional training 
remedies common complaint continuous output hmms model selectivity determined mainly model structure secondly output distributions lastly transition prob structure learning abilities smallest dynamic range bengio 
historically users structure selective parameter values ignored sakoe chiba 
transition trimming obtain trimming criterion hmm transition parameters substitute step statistics eqn 
yielding log ijj exp exp ijj xt pn test licenses deletion transition relatively improbable source state seldom visited 
note ijj quite small gradient log likelihood quite large 
fortunately map estimator brings parameter values trimming range 
eqn 
conservative take account effect renormalizing multinomial parameters 
may consider gain obtained redistributing trimmed probability mass surviving parameters particular parameter kjj maximizes pe jx kjj 
leads aggressive trimming test ijj ijj kjj log ijj log kjj ijj log ijj log kjj log ijj log kjj log xj kjj ijj log xj ijj gesture data experiments repeated deletion trimming criterion eqn 

batch deleted exit transition state re estimations 
small statistically significant improvement generalization expected deletions free model local maxima 
resulting models simpler faster removing transitions average state models state models state models thought ideal state count gesture data set 
complexity linear number transitions produce considerable speed 
continuous output hmms entropic training produce kinds states data modeling having output distributions tuned subsets data gating having near zero durations iji having highly non selective output probabilities 
gating states appear serve branch points transition graph bracketing alternative sub paths 
main virtue compress structure learning initial conventional entropic entropic training reserves states purely transition logic 
graphical view right gating state forks sub paths gating state collects branches forwards state 
initial conventional entropic near optimal number states entropic training occasionally pinch state deleting incoming transitions 
problem state removed 
graphical views shown right 
transition graph summarizing common transition patterns 
benefit trimming hmms successfully encode long term dependencies 
dense transition matrices cause diffusion credit learning long term dependency gets exponentially harder time bengio frasconi 
sparsity dramatically reduce diffusion 
bengio frasconi suggested hand crafted sparse transition matrices discrete optimization space sparse matrices remedies 
entropic training trimming essentially incorporates discrete optimization em 
state trimming interesting properties entropic training tends reduce occupancy rate states little direct flow probability mass vice broad output distributions non selective exit transitions 
result incoming transitions attenuated states virtually transition graph 
transitions may detect state si balancing prior probability incoming exit transitions probability mass flows eqn 

iji iji ny jji jji ijj ijj structure learning deleted gated log likelihood error perplexity table average state deletions conversions function initial state counts 
log likelihood mean advantage conventionally trained models recognizing held data nats data point statistical significance mean 
error measuring mean difference errors binary classification shows entropically estimated models consistently better 
computed modified forward analysis set output probabilities state zero tp 
speculative computation wish avoid 
propose non speculative heuristic equally effective bias transition trimming zero self transitions 
continued entropic training drives affected state output probabilities extremely small values dropping state occupancy low lead 
experiments measuring number states removed resulting classification accuracy statistically significance difference methods 
ran gesture data experiments addition state trimming 
average number states deleted quite small algorithm appears prefer keep superfluous states gating states table 
clearly algorithm converge ideal state count discounting gating states 
data records continuous motion trajectories clear ideal 
note models various initial sizes appear converge constant perplexity conventional hmms perplexity typically proportional state count 
strongly suggests entropic training finding dynamically simplest model data statically simplest model 
ambient video brand full algorithm learn concise probabilistic automaton hmm modeling human activity office setting motion time series extracted hour video see appendix details 
compared generalization discrimination entropically trained hmms conventionally trained hmms entropically trained hmms transition parameters subsequently flattened chance 
data sets employed train test test reversed altered behavior video subject large amounts coffee 
shows entropically trained hmm best discriminating class sequences 
conventional hmm shows fitting training set little ability distinguish dynamics test datasets 
flattened case shows structure learning log likelihood relative test set train reversed coffee entropic conventional flat log likelihoods different classes video normalized sequence length compared test class 
classes differ substantially static distribution points dynamics 
discrete output hmms discrete output hmms composed entirely cascaded multinomials experiments entropically estimate transition output probabilities 
cases simply replace step map estimator 
state criterion considering gain prior obtained discarding state output parameters 
bach chorales chorales widely dataset containing melodic lines bach surviving chorales 
modeling dataset hmms seek find underlying dynamics accounts melodic structure genre 
expected dataset especially challenging entropic estimation predicated noisy data chorales noiseless 
addition chorales sampled non stationary process bach highly open influences composing style evolved considerably early years leipzig 
compared entropically conventionally estimated hmms prediction classification tasks variety different initial state counts 
illustrates resulting differences 
despite substantial loss parameters sparsification entropically estimated hmms average better predictors notes test set conventionally estimated hmms 
better discriminating test chorales temporally reversed test chorales challenging bach structure learning parameters zeroed entropic versus ml hmm models bach chorales notes correctly predicted states initialization entropic modeling bach chorales 
lines indicate mean performance trials error bars standard deviations 
employed melodic reversal compositional device 
hand entropically estimated hmms showed greater divergence note likelihoods training test sequences 
raises possibility estimator pay price assuming noise 
possibility entropically estimated models capturing dynamical structure training melodies able deeper distinctions melodies different styles 
accords observation chorales particular low likelihoods rotated test set 
interesting difference conventionally estimated hmms wholly uninterpretable discern entropically estimated hmms basic musical structures including self transitioning states output tonic dominant triads lower upper register diatonic tones unfortunately dataset unlabeled relate observation bach chorales 
sequences correctly classified structure learning ced high probability states subgraphs interest state hmm 
tones output state listed order probability 
extraneous arcs removed clarity thr td ei 
nearly deterministic subgraph text modeling hmm 
nodes show state number symbols output state order probability 

dynamically states lead tonic leading tone chordal state sequences 
generally patterns easier discern larger sparser hmms 
explore theme briefly modeling text 
text human signals music language enormous amounts hidden state 
interesting patterns discovered entropic training hmms having modest numbers states 
example entropically conventionally trained state symbol discrete output hmms original version article 
entropic training states trimmed transition parameters output parameters leaving states output average symbols 
states hmm formed near deterministic chains shows subgraph output word fragments rate rotation tradition predict character random text fragments taken body entropic hmm scored correct conventional hmm scored 
subgraph probably accounts entropic hmm correct prediction word fragment 
structure learning probability prediction symbols order probability nonzero entries transition output matrices state text model 
prediction entropic conventional hmms letter whitespace 
shows entropic model correctly predicts range plausible continuations 
conventionally trained model specific predictions errs favor typical order effects follows 
predicting entropic model context going back symbols expectation demonstrate motivated automaton patterns occurred training sequence 
entropic estimation models seeks hidden states optimally compress context expect see interesting categories finished model 
data state initialization obtained model shown 
hidden states hmm highly correlated phonotactic categories regularities spoken language give rise indirectly patterns written language 
consonants words consonant clusters str structure learning oea sed graphical model state hmm trained text 

vowels semi vowels 

whitespace punctuation inter word symbols 

common word endings plural 
consonants contexts 
identified categories forward backward analysis assign probable states character text cross entropy statistics stress interpretation correlative true genesis states probably follows having discovered statistically salient categories vowels vs consonants vs inter word symbols entropic estimation went identify phenomena reliably happen boundaries categories word endings consonant cluster beginnings 
related extensive literature searches suggest entropic prior map estimator trimming criteria novel 
prior antecedents maximum entropy literature turn 
maximum entropy geometric priors maximum entropy refers set methods constructing probability distributions prior knowledge introducing unintended assumptions jaynes 
structure learning ignorance preserving distributions maximum entropy regard unknowns avoid modeling patterns inadequate support 
classic deals assertions expectations random variable samples 
probabilistic modelers typically deal samples 
uses bayesian ignorance preserving considerations lead construction prior 
unique prior community phrase entropic prior come form skilling rodriguez rodriguez pme dj jj cross entropy current parameter set model positive constant indicating confidence andj fisher information matrix model parameterized exponentiated term applicable setting typically model 
second term jj jeffreys non informative prior 
typically motivated differential geometry uniform prior space distributions invariant changes parameterization 
interesting relation minimum entropy prior distribution specified jeffreys prior divides posterior volume parameterizations yield equivalent distributions infinite data balasubramanian prior divides posterior volume distribution typical set small typical sets equivalent parameterizations 
priors measure specificity jeffreys prior stronger bias 
cases jeffreys felt jj problematic recommended non informative priors gaussian variance estimation jeffreys derived general form entropic prior brand show prior derived directly classical maximum entropy treatment assertion expected unpredictability process modeled finite hmm induction literature hmm structure induction wholly generate searches space discrete output hmms state splitting merging perturb model followed parameter estimation test improvement 
example proposed heuristic scheme set randomly pruned hmms compared looking model combines small loss likelihood large number prunings 
stolcke omohundro disjunction possible samples maximally fit model iteratively merged states dirichlet prior bayesian posterior probability criterion test success failure completion stolcke omohundro 
took opposite approach single state heuristically splitting states adding transitions 
ikeda similar scheme objective function built structure learning information criterion limit fitting ikeda 
speech recognition literature contains numerous variants strategy including maximum likelihood criteria splitting ostendorf singer search genetic algorithms splitting describe exceptions fujiwara valtchev 
nearly algorithms beam search generate multiple heads compensate dead ends declines posterior probability computation 
reported run times typically hours days discrete output hmms computational compared continuous output hmms 
contrast hill climbing algorithm applies kind state space markov model takes slightly longer classic em examples required seconds cpu time 
proposals include stage methods data statically clustered yield state space transition topology falaschi 
second stage conventional training 
mdl methods applied prevent fitting stage 
fairly easy construct problems thwart stage methods uniformly distributed samples structure virtue patterns time 
entropic estimation similar spirit neural network pruning schemes particularly weight elimination heuristic regularization term objective function causes small weights decay zero hanson pratt lang hinton 
fact weights decay near zero bishop necessary add pruning step cost increase error lecun damage minimized small adjustments surviving weights hassibi stork 
schemes require hand chosen regularization parameters 
brand propose entropic training trimming rules nonlinear dynamical systems including recurrent neural networks 
outside probabilistic modeling small growing combinatorial optimization literature embeds discrete problems continuous functions having hidden indicator variables 
gradient descent combined entropy error function forces system broadly explore search space settle syntactically valid near optimal state 
stolorz gave traveling salesman problems treatment brand show tsp reformulated map problem 
limitations open questions believe entropic estimation best fit model fit model 
fit structurally correct model reason believe entropically estimated parameters biased superior maximum likelihood parameters correction noise 
advisable polish entropically estimated model cycles maximum likelihood re estimation 
caution framework presently agnostic vulnerable ambiguities regard choice entropy measure 
example sequence data structure learning may choose entropy entropy rate entropy symbol 
case continuous distributions complicated fact differential entropy log dx pathologies lead infinitely negative entropy 
kinds complex models analytically tractable form entropy 
cases decompose model simpler distributions entropies known 
subadditivity principle sum entropies upper bound true entropy map estimator reduce entropies 
scheme sum entropies eqn 
clear interpretation description length 
inx upper bounded entropy rate hmm manner 
alternatively conditional entropies prior case markov models conditional entropy entropy rate asymptotically equal exp pj ijj log ijj context hmms pj stationary probability state estimated data 
map estimate easily obtained scaling jj 
jj 
jj pj eqns 

remains done mathematical characterization entropic prior including closed form solution lagrangian term normalization terms multivariate priors zp kx pk ijj ijj pj normalized posterior necessary comparing different models data 
turn questions prior open connections related fields 
graph theory readers combinatorics recognize eqn 
tree function enumeration trees graphs sets labeled vertices wright janson computing distribution cycles random mappings flajolet 
connections dynamical stability function sparse graph enumeration function intriguing may lead arguments entropic prior optimal learning concise sparse models 
offer tantalizing clue reworking solving partial result mid century connectivity neural tissue solomonoff rapoport random graphs erd nyi people vertices average acquaintances edges apiece individual starts rumor probability randomly selected individual heard rumor connected ap structure learning landau 
solving obtain ae note bracketed term essentially map estimator eqn 
lagrange multiplier set 
may understand map estimator striking compromise extreme graph sparsity minimally adequate graph reachability compromise governed statistics training set 
essentially perplexity population see formal connection entropic map estimate connectedness transition graph perplexity data 
optimization entropic estimation pushes model corners infinite dimensional hypercube containing parameter space 
typically local optima different corners identical modulo permutations parameter matrix hidden variables 
map estimator posterior mean meaningless point center hypercube 
seek region posterior having greatest probability mass posterior multimodal spiky 
hallmark discrete optimization problems 
unfortunately initial conditions determine particular optimum em finite resource optimizer 
approach improve quality local optimum single trial em particular simple generalization entropic map estimator automatically folds deterministic annealing em brand 
open question relevance em generalized yield successively better optima additional compute time 
general prior specific form virtually probability density function remains solve map estimators trimming criteria 
forthcoming companion extend entropic structure discovery framework similar results covariances weights variety parameter types demonstrate applications models interest including generalized recurrent neural networks radial basis functions brand 
mathematical framework simultaneously estimating parameters simplifying model structure probabilistic models containing hidden variables multinomial parameters hidden markov models 
key entropic prior prefers low entropy estimates fair estimates evidence limited premise small datasets representative generating process profoundly contaminated noise sampling artifacts 
main result solution map estimator drives weakly supported parameters extinction effectively turning excess parameters 
augment extinction process structure learning explicit tests transforms parameter deletion model accelerate learning rescue em local probability maxima 
hmms entropic estimation gradually zeroes superfluous transitions non selective states model 
sparsity provides protection fitting experimentally translates superior generalization prediction classification tasks 
addition entropic estimation converts data modeling states gating states effectively output distributions serve compress transition graph 
interesting structure discovered entropic estimation shed light hidden process generated data 
entropic estimation monotonically maximally hill climbs posterior probability wasted computation backtracking beam search 
consequently able train trim hmms related models times comparable conventional em produce simpler faster better models 
acknowledgments robert provided useful series calculating function excellent review applications fields 
thad starner provided computer vision sign language data starner pentland 
local anonymous reviewers pointing numerous avenues improvement 
appendix computing multi valued having infinite number complex branches partly real branches 
real contains solution eqn 

branches function computed quickly method third order generalization newton method finding roots 
recurrence equations wj wj wj wj wj wj see details selecting initial value leads desired branch 
necessary compute outside range values digital floating point representations 
cases observe ex log ex licenses swiftly converging recurrence wj structure learning appendix experimental details gesture data trials database sign language gestures obtained computer vision 
sequences particular gesture taken randomly training trial 
remaining testing 
identical initial conditions provided entropic conventional training algorithms 
transition matrices initialized ijj imposes forward topology skip ahead probabilities decline exponentially size jump 
topology mainly ease analysis results generally pronounced full transition matrices 
sure results artifact data set checked similar outcomes duplicate set experiments synthetic data yt sin random corrupted gaussian noise 
office activity data roughly half hour video taken frames second randomly course days 
adaptive statistical models background pixel variation foreground motion identify foreground frame 
largest set connected pixels foreground modeled single gaussian 
iso probability contour gaussian ellipse recorded parameters necessary describe ellipse frame plus derivatives time series video episode 
roughly time series training 
training initialized fully dense matrices random parameters 
bach chorales dataset obtained uci machine learning repository merz murphy 
sequence tracks pitch duration key time signature melody 
combined pitch key information obtain symbol time series representing pitch relative tonic 
compared entropically conventionally estimated hmms training chorales testing remaining 
trials chorales rotated test set 
prior experimentation full dataset randomly reordered minimize non stationarity due changes bach composing style 
hmms estimated entropically conventionally identical initial conditions fully dense random transition output matrices 
note prediction task test sequence truncated random length hmms predict missing note 
text readable characters article originally submitted training 
original character set condensed symbols letters whitespace symbol classes punctuation 
hmms estimated entropically conventionally identical initial conditions fully dense random transition output matrices 
training prior probabilities hidden states set average occupancy probabilities hmms tested text sequence start symbol training set 
prediction task test sequences symbols taken randomly body text 
structure learning acknowledgment anonymous reviewers helpful comments 
balasubramanian balasubramanian 

statistical inference occam razor statistical mechanics space probability distributions 
neural computation 
bauer bauer koller singer 

update rules parameter estimation bayesian networks 
proc 
uncertainty artificial intelligence 
bengio bengio 

markovian models sequential data 
technical report university montreal 
submitted statistical science 
bengio frasconi bengio frasconi 

diffusion credit markovian models 
tesauro touretzky leen editors advances neural information processing systems volume pages 
mit press 
bishop bishop 

neural networks pattern recognition 
oxford university press 
brand brand 

learning concise models human activity ambient video 
technical report mitsubishi electric research labs 
brand brand 

entropic estimation blends continuous discrete optimization 
technical report mitsubishi electric research labs 
preparation 
brand brand 

pattern discovery entropy minimization 
appear proceedings uncertainty 
society artificial intelligence statistics 
morgan kaufmann 


bach instruments music performance practices 
stauffer may editors great eighteen chorales bach process genesis pages 
indiana press bloomington indiana 
gonnet hare jeffrey knuth 

lambert function 
advances computational mathematics 
dietterich dietterich 

approximate statistical tests comparing supervised classification learning algorithms 
neural computation 
press 
structure learning erd nyi erd nyi 

evolution random graphs 
mta mat 

int 
zl 
see collected papers nyi 
falaschi falaschi 

automatic derivation hmm alternative pronunciation network topologies 
proc nd european conference speech communication technology volume pages 
flajolet flajolet 

gaussian limiting distributions number components structures 
journal combinatorial theory series 
fujiwara fujiwara 

motif extraction improved iterative duplication method hmm topology learning 
pacific symposium biocomputing pages 
hanson pratt hanson pratt 

comparing biases minimal network construction back propagation 
touretzky editor advances neural information processing systems volume pages 
morgan kauffman 
hassibi stork hassibi stork 

second order derivatives network pruning optimal brain surgeon 
hanson cowan giles editors advances neural information processing systems volume pages 
mit press 
heckerman heckerman 

tutorial learning bayesian networks 
technical report msr tr microsoft research 
available www ftp ftp research microsoft com pub tr tr html 
ikeda ikeda 

construction phoneme models model search hidden markov models 
international workshop intelligent signal processing communication systems sendai 
janson janson knuth 

birth giant component 
random structures algorithms 
jaynes jaynes 

probability theory logic science 
fragmentary edition march available www 
jeffreys jeffreys 

theory probability 
oxford university press 
landau landau 

problems random nets 
bulletin mathematical biophysics 
lang hinton lang hinton 

dimensionality reduction prior knowledge set recognition 
touretzky pages 
laplace laplace 

theorie des probabilities 
paris 
structure learning lecun lecun denker solla 

optimal brain damage 
touretzky pages 
merz murphy merz murphy 

uci repository machine learning databases 
ostendorf singer ostendorf singer 

hmm topology design maximum likelihood successive state splitting 
computer speech language 
rabiner rabiner 

tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
rodriguez rodriguez 

entropic priors 
technical report suny albany department mathematics statistics 
rodriguez rodriguez 

bayesian robustness new look geometry 
editor maximum entropy bayesian methods 
kluwer academic publishers 
sakoe chiba sakoe chiba 

dynamic programming algorithm optimization spoken word recognition 
ieee transactions acoustics speech signal processing volume assp pages 
skilling skilling 

classical maxent data analysis 
skilling editor maximum entropy bayesian methods 
kluwer academic publishers 
solomonoff rapoport solomonoff rapoport 

connectivity random nets 
bulletin mathematical biophysics 
starner pentland starner pentland 

american sign language recognizer 
international symposium wearable computing volume 
ieee press 
stolcke omohundro stolcke omohundro 

best model merging hidden markov model induction 
technical report tr international computer science institute center st berkeley ca usa 
stolorz stolorz 

recasting deterministic annealing constrained optimization 
technical report santa fe institute 


automatic generation hidden markov networks successive state splitting algorithm 
systems computers japan 
structure learning 


automatic generation hidden markov model successive state splitting contextual domain temporal domain 
technical report sp ieice 


isolated word recognition hmm structure selected genetic algorithm 
ieee international conference acoustics speech signal processing volume pages 
touretzky touretzky editor 
advances neural information processing systems volume 
morgan kauffman 
valtchev valtchev odell woodland young 

mmie training large vocabulary recognition systems 
speech communication 
jr el boston 

algorithm determine hidden markov model topology 
ieee international conference acoustics speech signal processing conference volume pages 


structured markov models speech recognition 
international conference acoustics speech signal processing volume pages 
wright wright 

number sparsely connected edged graphs 
journal graph theory 
ishikawa tanaka asai 

signal pattern extraction dna sequences hidden markov model genetic algorithm 
transactions information processing society japan 
