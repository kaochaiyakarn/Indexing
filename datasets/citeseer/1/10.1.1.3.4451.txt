ieee transactions neural networks vol 
september input space versus feature space kernel methods bernhard sch lkopf sebastian mika chris burges philipp klaus robert ller gunnar tsch alexander smola collects ideas targeted advancing understanding feature spaces associated support vector sv kernel functions 
discuss geometry feature space 
particular review known shape image input space feature space map influences capacity sv methods 
describe metric governing intrinsic geometry mapped surface computed terms kernel example class inhomogeneous polynomial kernels sv pattern recognition 
discuss connection feature space input space dealing question vector feature space find preimage exact approximate input space 
describe algorithms tackle issue show utility applications kernel methods 
reduce computational complexity sv decision functions second combine kernel pca algorithm constructing nonlinear statistical denoising technique shown perform real world data 
index terms denoising kernel methods pca reduced set method sparse representation support vector machines 
reproducing kernels functions forall pattern sets give rise positive matrices 
compact set data lives typically necessarily subset support vector sv community reproducing kernels referred mercer kernels section ii show 
provide elegant way dealing nonlinear algorithms reducing linear ones feature space nonlinearly related input space dot product corresponds mapping data possibly highdimensional dot product space usually nonlinear map manuscript received january revised may 
part done bell labs sch lkopf smola department engineering australian national university canberra 
supported arc dfg ja 
sch lkopf gmd berlin germany 
research cambridge cb mika 
ller tsch smola gmd berlin germany 
burges bell laboratories holmdel nj usa 
max planck institut kybernetik bingen germany 
publisher item identifier 
ieee dot product virtue property shall call feature map associated linear algorithm carried terms dot products nonlinear substituting priori chosen kernel 
examples algorithms include potential function method sv machines kernel pca 
price pay elegance solutions obtained expansions terms input patterns mapped feature space 
instance normal vector sv hyperplane expanded terms sv just kernel pca feature extractors expressed terms training examples evaluating sv decision function kernel pca feature extractor normally problem due multiplying mapped test point transforms kernel expansion evaluated lives infinite dimensional space 
cases reasons comprehensive understanding exactly connection patterns input space elements feature space expansions 
field far understood current attempts gather ideas elucidating problem simultaneously proposes algorithms situations connection important 
problem denoising kernel pca problem speeding sv decision functions 
remainder organized follows 
section ii discusses different ways understanding mapping input space feature space 
briefly review feature space algorithms sv machines kernel pca section iii 
focus interest way back feature space input space described section iv general form constructing sparse approximations feature space expansions section algorithms proposed sections experimentally evaluated section vi discussed section vii 
ii 
input space feature space section show feature spaces question defined choice suitable kernel function 
insight sch lkopf input space versus feature space structure feature space gained considering relation reproducing kernel hilbert spaces approximated empirical map extrinsic geometry leads useful new capacity results intrinsic geometry computed solely terms kernel 
mercer kernel map start stating version mercer theorem 
assume finite measure space 
mean sets measure zero 
theorem mercer suppose symmetric real valued kernel integral operator positive normalized eigenfunctions associated eigenvalues sorted nonincreasing order 
holds case series converges absolutely uniformly statement follows corresponds dot product fact uniform convergence series implies exists range infinite dimensional approximated accuracy dot product images reproducing kernel map think feature space reproducing kernel hilbert space rkhs 
see recall rkhs hilbert space functions set evaluation functionals maps continuous 
case riesz readers chiefly interested applications algorithms want consider skipping section 
finite measure space set algebra defined measure defined satisfying scaling factor probability measure 
algebra family subsets closed elementary set theoretic operations countable unions intersections complements contains member 
measure function additive measure set disjoint unions sets equals sum measures 
function variables gives rise integral operator 
representation theorem exists unique function call function obtained fixing second argument dot product rkhs 
contrast denote canonical euclidean dot product 
view property called reproducing kernel 
note implies identically zero 
set functions spans rkhs 
dot product rkhs needs defined extended rkhs linearity continuity 
follows particular implies symmetric 
note means reproducing kernel corresponds dot product space 
consider mercer kernel satisfies condition theorem construct dot product reproducing kernel hilbert space containing functions linearity mercer kernel chosen orthogonal respect dot product straightforward construct dot product kronecker symbol case reduces reproducing kernel property 
provides feature map associated proposition mercer kernel exists rkhs empirical kernel map giving interesting alternative theoretical viewpoint map appear useful sight 
practice pointless map inputs functions infinite dimensional objects 
dataset possible approximate evaluating points cf 
ieee transactions neural networks vol 
september definition patterns call empirical kernel map regard example consider case mercer kernel evaluate training patterns 
carry linear algorithm feature space take place linear span mapped training patterns 
represent losing information 
dot product representation simply canonical dot product usually form orthonormal system 
turn feature map associated need endow dot product ansatz positive matrix 
enforcing training patterns yields self consistency condition cf denote kernel gram matrix condition satisfied instance pseudoinverse equivalently incorporated rescaling operation corresponds kernel pca whitening directly map modifying simply amounts dividing eigenvector basis vectors eigenvalues parallels rescaling eigenfunctions integral operator belonging kernel 
data sets number examples smaller dimensionality computationally attractive carry explicitly kernels subsequent algorithm svm kernel pca say wants 
aside note case kernel pca described section iii need worry whitening step canonical dot product simply lead diagonalizing yields eigenvectors squared eigenvalues 
pointed 
section notes illustrate need restricted special case just discussed 
general kernels 
nonsymmetric kernels canonical dot product note dot product written form 
require definiteness null space projected leading lower dimensional feature space 
understood singular pseudoinverse effectively matrix general note positive semidefinite matrix written wanted carry whitening step cf footnote concerning potential singularities 
different evaluation sets 
mika performed experiments speed kernel pca choosing proper subset described kernel map detail including variations shall look properties 
specifically study effect capacity kernel methods section ii induced geometry feature space section ii 
capacity kernel map vapnik gives bound capacity measured vc dimension optimal margin classifiers 
takes form upper bound constraining length weight vector hyperplane canonical form radius smallest sphere containing data space hyperplane constructed 
smaller sphere smaller capacity beneficial effects generalization error bounds 
data distributed reasonably isotropic way case input space fairly precise 
distribution data fill sphere wasteful 
argument remainder section summarized shows kernel typically entails data fact lies box rapidly decaying smaller sphere 
statement theorem exists constant depending kernel essentially contained axis parallel parallelepiped side lengths cf 
see effectively restricts class functions note done terms dot products 
compensate invertible linear transformation data corresponding inverse adjoint transformation set admissible weight vectors invertible operator may construct diagonal scaling operator inflates sides parallelepiped possible ensuring lives sphere original radius fig 

change factor right hand side buys terms entropy numbers due tightness rate famous theorem 
sch lkopf input space versus feature space fig 

done terms dot products scaling data compensated scaling weight vectors choosing data contained ball radius effectively reduce function class parameterized weight vector leads better generalization bounds depend kernel inducing map regarding second factor show function class essentially behaves finite dimensional cut determined rate decay eigenvalues 
reasoning leads improved bounds somewhat intricate presently explained full detail 
nutshell idea compute capacity measured terms covering numbers sv function class evaluated sample entropy numbers suitable linear operator 
consider operator purpose entropy numbers applied sphere radius crucial 
computed entropy numbers operator factorization properties entropy numbers upper bounded account scaling operator precise way 
faster eigenvalues integral operator associated kernel decay smaller entropy numbers capacity corresponding feature space algorithm function class stronger generalization error bounds prove 
example consider entropy numbers depend asymptotically eigenvalues proposition exponential polynomial decay suppose mercer kernel example kernel gaussian proposition allows formulation priori generalization error bounds depending eigenvalues kernel 
similar entropy number methods possible give precise data dependent bounds terms eigenvalues kernel gram matrix 
consider normed spaces px th entropy number set defined exists cover containing fewer points recall covering number essentially functional inverse measures balls radius needs cover entropy numbers operator defined entropy numbers image unit ball note intuitively higher entropy number allow finer characterization complexity image 
note entropy numbers nice properties covering numbers lacking 
instance scaling subset normed vector space factor simply scales entropy numbers factor 
entropy numbers promising tool studying capacity feature space methods 
due fact linear case interested feature space algorithms studied powerful methods functional analysis 
metric kernel map way gain insight structure feature space consider intrinsic shape manifold data mapped 
important distinguish feature space surface space points input space map call general dimensional submanifold embedded simplicity assume sufficiently smooth structures riemannian metric defined 
follow analysis reader referred details application class inhomogeneous polynomial kernels new 
note intrinsic geometrical properties derived know riemannian metric induced embedding riemannian metric defined symmetric metric tensor interestingly need know explicit mapping construct written solely terms kernel 
see consider line element indexes correspond vector space input space letting represent small finite displacement read components metric tensor class kernels functions dot products points covariant contravariant ieee transactions neural networks vol 
september components metric take simple form prime denotes derivative respect argument illustrate compute intrinsic geometrical properties surface corresponding class inhomogeneous polynomial mercer kernels constant 
properties relating intrinsic curvature surface completely captured riemann curvature tensor symbols second kind defined find class kernels riemann curvature arbitrary input space dimension polynomial order interesting compare result homogeneous case analysis shows adding constant kernel results striking differences geometries 
curvatures vanish expected 
vanishes powers vanish furthermore surfaces homogeneous kernels nonvanishing curvature singularity corresponding surfaces inhomogeneous kernels provided providing insight geometrical structure surfaces generated choice kernel geometrical analysis gives concrete results 
example note kernel mercer kernel analysis apply 
expect density surface ill behaved data norm small homogeneous polynomial kernels inhomogeneous case 
similar problems may arise pattern recognition case data lies near singularity 
considerations extended compute intrinsic volume element may compute density give simple necessary tests satisfied kernel mercer kernel reader referred details 
iii 
feature space algorithms describe algorithms sv machines kernel pca 
sv algorithm shall described detail briefly fix notation 
sv classifiers construct maximum margin hyperplane input space corresponds nonlinear decision boundary form training examples 
called sv applications solving quadratic program turn zero 
excellent classification accuracies ocr object recognition obtained sv machines 
generalization case regression estimation leading similar function expansion exists 
kernel principal component analysis carries linear pca feature space extracted features take nonlinear form normalization components th eigenvector matrix understood follows 
wish find eigenvectors eigenvalues covariance matrix feature space case high dimensional impossible compute directly 
able solve problem uses mercer kernels 
need derive formulation uses dot products 
replace occurrence way avoid dealing mapped data explicitly may intractable terms memory computational cost 
find formulation pca uses dot products substitute covariance matrix note solutions assume mapped data centered 
general true computations easily reformulated perform explicit centering 
sch lkopf input space versus feature space equation lie span images training data 
may consider equivalent system expand solution substituting defining matrix arrive problem cast terms dot products solve details step see 
normalizing solution translates extract features compute projection image test point th eigenvector feature space usually cheaper dot product feature space explicitly 
conclude brief summary kernel pca state characterization involves regularizer length weight vector sv machines 
proposition th kernel pca feature extractor scaled optimal feature extractors form sense shortest weight vector subject conditions orthogonal kernel pca feature extractors feature space applied training set leads unit variance set outputs 
sv machines kernel pca utilize mercer kernels generalize linear algorithm nonlinear setting regularizer albeit different domains learning supervised versus unsupervised 
feature extraction experiments handwritten digit images kernel pca shown linear hyperplane classifier trained extracted features perform nonlinear sv machine trained directly inputs 
fig 

preimage problem 
point span mapped input data necessarily image input pattern 
point written expansion terms mapped input patterns kernel pca eigenvector svm hyperplane normal vector necessarily expressed image single input pattern 
iv 
feature space input space section ii described get input space feature space study way back 
fair amount aspects problem context developing called reduced set methods :10.1.1.54.1171:10.1.1.54.9934
pedagogical reasons shall postpone reduced set methods section focus problem complex start 
preimage problem stated feature space algorithms express solutions expansions terms mapped input points 
map feature space nonlinear generally assert expansion preimage point fig 

preimage existed easy compute shown result 
proposition consider feature space expansion exists invertible function compute orthonormal basis input space 
proof expand ieee transactions neural networks vol 
september remarks proposition 
examples kernels invertible functions polynomial kernels sigmoid kernels odd similar result holds rbf kernels polarization identity need kernel allows reconstruction evaluated input points allowed choose details cf 
crucial assumption clearly existence preimage 
unfortunately situations preimages 
illustrate consider feature map 
clearly points feature space written preimage map 
characterize set points specific example consider gaussian kernels case maps input gaussian sitting point 
known gaussian written linear combination gaussians centered points 
gaussian case expansions excluding trivial cases term exact preimage 
problem initially set solve turned general case 
try ask 
trying find exact preimages consider approximate ones 
call approximate preimage small 
vectors approximate preimages exist 
described section iii kernel pca pca provides projections optimal approximation property 
assume sorted nonincreasing eigenvalues smallest nonzero eigenvalue 
proposition dimensional projection minimizing expected approximate preimage trivially approximate just small needs order form satisfactory approximation depend problem hand 
refrained giving formal definition 
preimage 
shall see experiments better preimages interesting applications possible denoising noisy map discard higher components obtain compute preimage hope main structure data set captured directions remaining components mainly pick noise sense thought denoised version compression eigenvectors small number features cf compute preimage approximate reconstruction useful smaller dimensionality input data 
interpretation visualize nonlinear feature extractor computing preimage 
focus point 
section shall develop method minimizing experimental section apply case algorithm approximate preimages section gives analysis case gaussian kernel proven perform applications proposes iteration procedure computing preimages kernel expansions 
start considering problem slightly general preimage problem seeking approximate observe minimizing minimize distance orthogonal projection span fig 
maximize expressed terms kernel 
maximization preferable comprises lower dimensional problem different scaling behavior 
maximum extended minimum setting cf 
function minimized standard techniques particular choices kernels fixed point iteration methods shown presently :10.1.1.54.9934
kernels satisfy gaussian kernels reduces sch lkopf input space versus feature space fig 

vector try approximate multiple vector image input space nonlinear map finding projection distance aa minimized 
extremum evaluate gradient terms substitute get sufficient condition gaussians obtain leading gaussian kernel arrive devise iteration denominator equals nonzero neighborhood extremum extremum zero 
occurs projection linear span zero case pointless try approximate numerical instabilities related small approached restarting iteration different starting values 
interestingly interpreted context clustering 
determines center single gaussian cluster trying capture positive possible simultaneously avoids negative sv classifiers sign equals label pattern sign distinguishes plain clustering parametric density estimation 
occurrence negative signs related fact trying estimate parametric density difference densities modulo normalization constants 
see define sets shorthands target reads trying find point difference unnormalized probabilities classes maximal estimate approximation gaussian centered note rewrite reduced set rs methods problem move slightly general problem studied longer looking single preimages expansions :10.1.1.54.9934
turn method developed section design algorithm general case 
assume vector expanded images input patterns looking single preimage try approximate reduced set expansion minimize crucial point explicitly computed minimized terms kernel :10.1.1.54.9934
nist benchmark handwritten digits sv machines accurate single classifier inferior neural nets run time classification speed :10.1.1.54.1171
applications ieee transactions neural networks vol 
september issue desirable come methods speed things making sv expansion sparse replacing 
finding coefficients evidently rs problem consists parts 
determine rs vectors compute expansion coefficients start partly easier partly common different rs methods 
proposition optimal coefficients approximating linearly independent norm note linearly independent want approximation full rank 
pseudoinverse select solution largest number zero components 
proof see 
evaluate derivative distance set zero 
substituting obtain rs algorithm norm optimality criterion circumvent result 
instance suppose algorithm computes simultaneously comes solution 
proposition recompute optimal coefficients get solution original 
different algorithms differ way determine vectors place 
dealt section simply selects subsets section uses vectors different original reduced set selection selection kernel pca idea algorithm arises observation null space gram matrix precisely tells vectors removed expansion committing zero approximation error assuming correctly adjust coefficients sparse sv expansion say changing 
gram matrix computed examples nonzero larger set want expansion 
interestingly turn problem closely related kernel pca 
start simplest case 
assume exists eigenvector eigenvalue reads means linearly dependent comes nonzero expressed terms 
may eigenvectors eigenvalue zero eliminate certain terms expansion happens nonzero eigenvalues case gaussian kernels 
intuitively believe longer precisely true give approximation 
crucial difference order get best possible approximation need take account coefficients expansion commit error removing say error depend 
select optimal clearly find coefficients minimizing error commit replacing establish connection kernel pca change variables 
define equals normalizing obtain leads problem minimizing note invariant rescaling straightforward calculation shows recover approximation coefficients values add leaving minimizing nonlinear function devise computationally attractive approximate solution 
motivated observation minimized eigenvector minimal eigenvalue consistent special case discussed cf 
case idea approximating support vector expansion optimally removing individual support vectors adjusting coefficients remain minimize resulting error arrived independently olivier chapelle derived expression minimized private communication 
sch lkopf input space versus feature space generally normalized eigenvector eigenvalue minimized operations performing kernel pca scanning matrix complexity reduced considering smallest eigenvalues chosen priori 
eliminate chosen principled efficient way 
setting computational considerations aside optimal greedy solution selection problem equivalent obtained proposition compute optimal solution possible patterns leave subsets size evaluate case 
applies subsets size 
resources exhaustively scan subsets size proposition provides optimal way selecting best expansion size 
better expansions obtained drop restriction approximation written terms original patterns done section 
matter choosing approximate scheme iterated expansion sufficiently sparse 
wants avoid having find smallest eigenvalue step anew approximate schemes heuristics conceived 
experiments reported compute eigenvectors step gram matrix computed sv selected 
selection penalization consider method enforcing sparseness inspired shrinkage cf 
expansion approximate minimizing constant determining tradeoff sparseness quality approximation 
constants set mean say 
case hoping sparser decomposition emphasis put shrinking terms small 
reflects intuition promising try shrink large terms 
ideally count number nonzero coefficients sum moduli lead efficiently solvable optimization problem 
dispose modulus rewrite terms new variables quadratic programming problem subject problem solved standard quadratic programming tools 
solution directly expansion coefficients 
optimal precision merely select patterns expansion nonzero coefficients recompute optimal coefficients proposition 
multiclass case applications face problem simultaneously approximating set feature space expansions 
instance digit classification common approach train binary recognizers digit 
quadratic programming formulation section modified subject indexes understood range sv expansion vectors nonzero coefficient recognizers classes respectively 
rationale mean term constraint ensures penalize largest coefficients pertaining individual sv 
soon coefficient expansions cost 
precisely want speed purposes compute certain dot product anyway just reuse expansions 
problem arrive subject ranges sv 
ieee transactions neural networks vol 
september note utility reduced set selection expect procedures useful 
sv machines positive value regularization constant reason believe sv expansion sparser removing constraint note care constraint 
second number eigenvalues zero number patterns removed loss small depends problem hand kernel 
instance gaussian kernel gram matrices zero eigenvalues patterns duplicates 
approximations possible eigenvalues gaussian kernels decay rapidly 
reduced set construction far dealt problem select reduced set expansion vectors original 
return originally posed problem includes construction new vectors reach high reduction rates 
suppose want approximate vector expansion type iterate procedure section iv denotes obtained iterating 
apply needs utilized representation terms mapped input images need set coefficient computed vectors orthogonal best approximation span obtained computing orthogonal projections direction 
need compute optimal coefficients anew step proposition discrepancy reached zero invertible 
iteration stopped steps specified advance monitoring falls specified threshold 
solution vector takes form 
conclude section noting cases multiclass sv machines multiple kernel pca feature extractors may want approximate vectors simultaneously 
leads complex equations 
imagine case certain pattern appears twice training set sv expansion utilize copies upper bound constraint limits coefficient gx fig 

kernel pca toy example see text lines constant feature value nonlinear principal components extracted principal components top middle right separate clusters 
components split clusters 
components split orthogonal splits 
vi 
experiments see proposed methods practice ran toy real world experiments 
section vi give denoising results approach finding approximate preimages section iv 
section vi experiments reduced set methods described sections 
kernel pca denoising toy examples experiments reported carried gaussian kernels minimizing iteration scheme 
similar results obtained polynomial kernels 
matlab code computing kernel pca available web svm gmd de 
generated artificial data set point sources points gaussian noise performed kernel pca fig 

resulting eigenvectors extracted nonlinear principal components set test points generated model reconstructed points varying numbers principal components 
fig 
shows discarding higher order components leads removal noise points move respective sources 
second experiment table generated data set gaussians zero mean variance component selecting source points training set points test set centers gaussians randomly chosen applied kernel pca training set computed projections points test set 
carried denoising yielding approximate preimage test point 
procedure repeated different numbers components reconstruction different values kernel 
compared results provided algorithm linear pca mean squared distance denoised test points corresponding center 
table shows ratio values ratios larger indicate kernel pca performed better linear pca 
choice sch lkopf input space versus feature space fig 

kernel pca denoising reconstruction projections eigenvectors fig 

generated new points gaussian represented feature space nonlinear principal components computed approximate preimages shown upper pictures top left original data top middle ai top right ap 
note discarding higher order principal components small remove noise inherent nonzero variance gaussians 
lower pictures show original points move denoising 
corresponding case linear pca obtain lines see fig 
kernel pca clusters shrink points 
fig 

reconstructions point movements linear pca principal component 
kernel pca better 
note components linear pca just basis transformation denoise 
extreme superiority kernel pca small due fact test points case located close spots input space linear pca cover directions 
kernel pca moves point correct source small number components 
get intuitive understanding low dimensional case fig 
depicts results denoising half circle square plane kernel pca nonlinear autoencoder principal curves linear pca 
principal curves algorithm iteratively estimates curve capturing structure data 
data projected closest point curve algorithm tries construct point average data points projecting 
shown table denoising gaussians ih see text 
performance ratios larger indicate better kernel pca compared linear pca different choices gaussians standard deviation different numbers components reconstruction fig 

denoising see text 
depicted data set small points denoised version big points joining solid lines 
linear pca component reconstruction components reconstruction perfect denoise 
note algorithms approach problems capturing circular structure bottom example taken 
straight lines satisfying principal components principal curves generalization 
algorithm uses smoothing parameter annealed iteration 
nonlinear autoencoder algorithm bottleneck layer network trained reproduce input values outputs autoassociative mode 
hidden unit activations third layer form representation data closely related pca see instance 
training done conjugate gradient descent 
algorithms parameter values selected best possible denoising result obtained 
shows closed square problem kernel pca subjectively best followed principal curves nonlinear autoencoder linear pca fails completely 
note algorithms kernel pca provide explicit dimensional parameterization data kernel pca provides means mapping points denoised versions case kernel pca features obtain dimensional parameterization 
handwritten digit denoising test approach real world data applied algorithm usps database handwritten digits training patterns test patterns size 
digits randomly chose examples training set examples test set 
method section iv width equals twice average data variance ieee transactions neural networks vol 
september fig 

visualization eigenvectors see text 
depicted th eigenvector left right 
row linear pca second third row different visualizations kernel pca 
dimension 
fig 
give possible depictions eigenvectors kernel pca compared linear pca usps set 
second row shows approximate preimages eigenvectors iterative algorithm 
third row image computed follows pixel projection image th canonical basis vector input space corresponding eigenvector feature space upper left lower right 
linear case methods simply yield eigenvectors linear pca depicted row sense may considered generalized eigenvectors input space 
see eigenvectors identical arbitrary signs 
see eigenvectors linear pca start focus high frequency structures smaller eigenvalue size 
understand note linear pca maximum number eigenvectors contrary kernel pca gives number training examples possible eigenvectors 
explains results working usps set figs 

experiments linear kernel pca trained original data 
test set added additive gaussian noise zero mean standard deviation speckle noise pixel flipped black white probability noisy test sets computed projections linear nonlinear components carried reconstruction case 
results compared mean squared distance reconstructed digit noisy test set original counterpart 
optimal number components linear kernel pca approach better factor gaussian noise speckle noise optimal number components linear pca kernel pca respectively 
identical numbers components algorithms kernel pca times better linear pca 
note kernel pca comes higher computational complexity 
speeding support vector decision rules section vi usps handwritten digit database 
approximated sv expansions binary classifiers trained separate digit fig 

denoising usps data see text 
left half shows top occurrence digit test set second row upper digit additive gaussian noise rows reconstruction linear pca ai components rows results approach number components 
right half show speckle noise probability rest 
gaussian kernel approximation techniques described sections 
original sv system average sv classifier 
tables ii iii show classification error results approximation reduced set selection techniques described section table iv gives results reduced set construction method described section varying numbers rs vectors 
shown line number misclassified digits single classifier error combined class machine 
rs systems optimal sv threshold recomputed training set 
method section rss means binary classifier removed support vectors average left method section section approximated decision function vectors steps described iteration procedure 
large numbers small reductions decision functions complexity accuracy original system ap sch lkopf input space versus feature space fig 

mean squared error denoised images versus number features kernel pca linear pca 
kernel pca exploits nonlinearities potential utilize features code structure noise 
outperforms linear pca denoising sufficiently large number features 
table ii numbers test errors binary recognizer test error rates class classification rs method section section 
top numbers svs original sv rbf system 
bottom row original sv system sv average rows systems varying average numbers rs vectors 
inthe system rss equal fractions sv removed recognizer average rs vectors left closely techniques 
tables ii iii see removal support vectors leaves error practically unchanged 
reducing numbers support vectors lead large performance losses 
table iii numbers test errors binary recognizer test error rates class classification rs method section ax row original sv system sv average rows systems varying average numbers rs vectors 
inthe system adjusted average number rs vectors left constant parentheses chosen numbers comparable table ii 
results improved method section ax instance expansion vectors number get union sv ur system led improved error rate reduced set construction method computationally conceptually complex performs better situation able utilize vectors different original support patterns expansion 
get speedup factor system rs vectors rsc 
method table iv classification accuracy drops moderately ieee transactions neural networks vol 
september fig 

complete display reduced set vectors constructed iterative approach section ph coefficients top recognizer digit bottom digit 
note positive coefficients roughly correspond positive examples classification problem 
table iv number test errors binary recognizer test error rates class classification rs construction method section 
row original sv system sv see tables ii iii rows systems varying numbers rs vectors rsc stands vectors constructed binary recognizer computed iterating term approximations separately recognizer 
rows subsequent global gradient descent results improved see test competitive convolutional neural networks data base 
improve result adding second phase traditional rs algorithm global gradient descent performed space computationally expensive phase orders magnitude led error rate :10.1.1.54.1171:10.1.1.54.9934
considered kernel identical traditional rs method yielded polynomial kernels method led speedup :10.1.1.54.9934
note traditional rs method restarts second phase times sure global minimum cost function plausible final results similar 
fig 
shows rs vectors binary classifiers 
aside note approach algorithm produces images look meaningful digit :10.1.1.54.9934
vii 
discussion algorithms utilizing mercer kernels construct solutions expansions terms mapped input patterns 
map unknown complex provide intuition solution motivated efforts reduce complexity expansion summarized 
extreme case studied approximate single find approximate preimage proposed fixed point iteration algorithm perform task 
situations approximate preimage exists reduce complexity expressing sparser expansion proposed methods computing optimal coefficients coming suitable patterns selecting algorithm 
iterating preimage sch lkopf input space versus feature space types approximations theoretical interest feature space methods lead practical applications 
considered applications problem statistical denoising kernel pca reconstruction problem speeding sv decision rules 
address turn 
kernel pca denoising denoising experiments realworld data obtained results significantly better linear pca 
interpretation finding follows 
linear pca extract components dimensionality data 
basis transform components fully describe data 
data noisy implies certain fraction components devoted extraction noise 
kernel pca hand allows extraction features number training examples 
accordingly kernel pca provide larger number features carrying information structure data experiments 
addition structure extracted nonlinear linear pca necessarily fail illustrated toy examples 
open questions problems include choice suitable kernel noise reduction problem possibly conjunction regularization properties kernel application approach compression comparison connection alternative nonlinear denoising methods cf 
speeding sv machines shown experimentally approximation algorithms speed sv machines significantly 
note gaussian rbf case approximation original kernel matrix full rank 
rs construction results obtained objective function decrease zero rs construction experiments reduced factor phase depending rs vectors computed global gradient descent yielded factor :10.1.1.54.1171
conjecture due classification interested underlying probability distribution patterns cf 
consistent fact performance rs sv classifier improved recomputing optimal threshold previous rs construction method sv kernel new limited fast led interpretable rs images interesting connection clustering approximation feature spaces :10.1.1.54.1171:10.1.1.54.9934
appears intriguing pursue question connection exploited form general types approximations sv kernel pca expansions making gaussians variable widths 
rs selection methods hand applicable sv kernel 
experiments led worse reduction rates rs construction simpler computationally faster 
rs selection methods described section slightly superior higher reductions section computationally cheaper remove sv time need iterated 
improved simultaneously approximating vectors corresponding binary recognizers digit recognition task 
proposed methods applicable feature space algorithm mercer kernels 
instance speed sv regression machines kernel pca feature extractors 
expect possibilities open mercer kernel methods applied increasing number learning signal processing problems 
acknowledgment authors elisseeff helpful discussions 
aizerman braverman theoretical foundations potential function method pattern recognition learning automat 
remote contr vol 
pp 

theory reproducing kernels trans 
amer 
math 
soc vol 
pp 

blackmore williamson decision region approximation polynomials neural networks ieee trans 
inform 
theory vol 
pp 

boser guyon vapnik training algorithm optimal margin classifiers proc 
th annu 
acm wkshp 
comput 
learning theory haussler ed 
pittsburgh pa acm july pp 

buhmann data clustering learning handbook brain theory neural networks arbib ed 
cambridge ma mit press pp 

burges simplified support vector decision rules proc :10.1.1.54.9934
th int 
conf 
machine learning saitta ed 
san mateo ca morgan kaufmann pp 

tutorial support vector machines pattern recognition data mining knowledge discovery vol 
pp 

geometry invariance kernel methods advances kernel methods support vector learning sch lkopf burges smola eds 
cambridge ma mit press pp 

burges sch lkopf improving accuracy speed support vector learning machines advances neural information processing systems mozer jordan petsche eds :10.1.1.54.1171
cambridge ma mit press pp 

carl entropy compactness approximation operators 
cambridge cambridge univ press 
kung principal component neural networks 
new york wiley 
harrison linear programming support vector machines pattern classification regression estimation sr algorithm improving speed tightness vc bounds sv algorithms university sheffield department automatic control systems engineering res 
rep 
girosi equivalence sparse approximation support vector machines neural comput vol 
pp 

hastie stuetzle principal curves jasa vol 
pp 

hastie tibshirani generalized additive models vol 
monographs statistics applied probability 
london chapman hall 
lecun boser denker henderson howard hubbard jackel backpropagation applied handwritten zip code recognition neural comput vol 
pp 

mallat zhang matching pursuit time frequency dictionary ieee trans 
signal processing vol 
pp 

personal communication 
micchelli interpolation scattered data distance matrices conditionally positive definite functions constructive approximation vol 
pp 

mika feature ieee transactions neural networks vol 
september degree thesis technische universit berlin germany german 
mika sch lkopf smola 
ller scholz tsch kernel pca denoising feature spaces advances neural inform 
processing syst 

osuna girosi reducing run time complexity support vector machines advances kernel methods support vector learning sch lkopf burges smola eds 
cambridge ma mit press pp 

saitoh theory reproducing kernels applications 
harlow longman 
sch lkopf support vector learning oldenbourg verlag nchen degree thesis tu berlin german 
sch lkopf smola burges fast approximation support vector kernel expansions interpretation clustering approximation feature spaces 
dagm symp levi 
may eds informatik berlin germany springer verlag pp 

sch lkopf mika smola tsch 
ller kernel pca pattern reconstruction approximate preimages proc 
th int 
conf 
artificial neural networks perspectives neural computing niklasson bod ziemke eds 
berlin germany springer verlag pp 

sch lkopf shawe taylor smola williamson kernel dependent support vector error bounds proc 
icann published 
sch lkopf simard smola vapnik prior knowledge support vector kernels advances neural information processing systems jordan kearns solla eds 
cambridge ma mit press pp 

sch lkopf smola 
ller nonlinear component analysis kernel eigenvalue problem neural comput vol 
pp 

sch lkopf sung burges girosi niyogi poggio vapnik comparing support vector machines gaussian kernels radial basis function classifiers ieee trans 
signal processing vol 
pp 

smola sch lkopf regularization operators support vector kernels advances neural information processing systems jordan kearns solla eds 
cambridge ma mit press pp 

tutorial support vector regression neurocolt rep 
available svm gmd de tsuda support vector classifier asymmetric kernel function proc 
esann verleysen ed 
brussels facto pp 

vapnik nature statistical learning theory 
new york springer verlag 
statistical learning theory 
new york wiley 
wahba spline models observational data vol 
cbms nsf regional conference series applied mathematics 
philadelphia pa siam 
williamson smola sch lkopf entropy numbers operators support vector kernels advances kernel methods support vector learning sch lkopf burges smola eds 
cambridge ma mit press pp 

generalization performance regularization networks support vector machines entropy numbers compact operators neurocolt tech 
rep 
available www neurocolt com bernhard sch lkopf received sc 
degree mathematics university london degree physics universit bingen germany 
received ph degree computer science technical university berlin germany 
worked max planck institute biological bell labs australian national university 
taught humboldt university berlin 
scientific interests include machine learning perception 
dr sch lkopf dissertation support vector learning won prize german association computer science gi 
won lionel cooper memorial prize university london 
sebastian mika received degree computer science technical university berlin 
currently doctoral student gmd berlin 
scientific interests fields machine learning kernel methods 
chris burges distinguished member technical staff bell laboratories lucent technologies 
educated physicist joined worked network performance routing 
moved applied neural network research worked handwriting machine print recognition speaker identification 
years concentrated theory application svm philipp computer science student university bingen germany 
worked various companies area years including limited bell labs hewlett packard torr gmbh ag max planck institute 
worked areas computer science computer graphics support vector learning machines network programming system administration code optimization databases compression technology wavelet analysis 
current interest fields highperformance clustered network servers real time presentation visual stimuli low linux workstations optimizations improvements reduced set vector methods support vector machines 
klaus robert ller received degree mathematical physics ph degree theoretical computer science university karlsruhe germany 
postdoctoral fellow research institute computer architecture software technology german national research center information technology gmd berlin 
european community stp research fellow university tokyo prof amari lab 
tenure position department head intelligent data analysis ida group gmd berlin 
ida group twice won price best scientific technical project gmd 
currently humboldt university technical university berlin germany 
worked statistical physics statistical learning theory neural networks time series analysis 
interests expanded support vector learning machines boosting nonstationary blind separation techniques medical data analysis meg eeg 
sch lkopf input space versus feature space gunnar tsch received degree computer science university potsdam germany 
doctoral student gmd berlin 
scientific interests fields boosting kernel methods 
tsch received prize best student faculty natural sciences university potsdam 
alexander smola received degree physics technical university munich germany 
years received ph degree computer science technical university berlin germany thesis learning kernels 
postdoctoral fellow gmd berlin 
studies supported spent year bell labs pavia italy 
addition spent spells australian national university 
scientific goal generalization error bounds statistical learning theory applicable practice 
