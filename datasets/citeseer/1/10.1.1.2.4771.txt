unlabeled data improve text classification kamal paul nigam may cmu cs school computer science carnegie mellon university pittsburgh pa submitted partial fulfillment requirements degree doctor philosophy 
thesis committee tom mitchell chair andrew mccallum john lafferty tommi jaakkola massachusetts institute technology copyright fl kamal paul nigam research sponsored national science foundation nsf 
sbr defense advanced research projects agency darpa contract 
digital equipment dec 
views contained document author interpreted representing official policies expressed implied nsf darpa government entity 
keywords text classification text categorization unlabeled data expectationmaximization unsupervised learning key difficulty text classification learning algorithms require hand labeled examples learn accurately 
dissertation demonstrates supervised learning algorithms small number labeled examples inexpensive unlabeled examples create high accuracy text classifiers 
assuming documents created parametric generative model expectation maximization em finds local maximum posteriori models classifiers data labeled unlabeled 
generative models capture intricacies text domains technique substantially improves classification accuracy especially labeled data sparse 
problems arise basic approach 
unlabeled data hurt performance domains generative modeling assumptions strongly violated 
case assumptions representative ways modeling sub topic class structure modeling super topic hierarchical class relationships 
doing model probability classification accuracy come correspondence allowing unlabeled data improve classification performance 
second problem representative model improvements unlabeled data sufficiently compensate paucity labeled data 
limited labeled data provide em initializations lead low probability models 
performance significantly improved active learning select high quality initializations alternatives em avoid low probability local maxima 
acknowledgments different people provided help support input brought thesis 
looking example advisor especially recommend mine tom mitchell 
tom enthusiastic advisor providing encouragement insight valuable big picture perspective 
catalyst 
extend andrew mccallum 
years generous collaborator sincere thoughtful mentor close friend 
memories graduate school traditional walks box completing submission 
john lafferty tremendously helpful providing mathematical insight rigor 
addition provided great moral support 
left meetings john feeling renewed sense optimism confidence 
tommi jaakkola providing fresh external viewpoint research 
tommi asking questions hoped hear 
raising difficult issues focused attention critical areas 
fortunate privileged different researchers field 
provided invaluable knowledge shown means perform careful responsible research 
collaborators authors doug baker mark craven dan dipasquo dayne freitag ghani rosie jones john lafferty andrew mccallum tom mitchell mladenic sasha popescul quek jason rennie ellen riloff seymore sean slattery sebastian thrun lyle ungar 
years provided willing ear occasions alex gray bryan loyall dimitris michael mateas eric andrew tomkins 
thesis possible love support parents sister 
teachers inspired love learning natural curiosity 
importantly wife 
provides faith confidence endure enjoy 
vi contents unlabeled data 
crazy 
questions asked 
road map 
incorporating unlabeled data generative models 
generative model text 
model probability optimization criteria 
naive bayes text classification 
training naive bayes classifier labeled data 
classifying new documents naive bayes 
learning naive bayes model labeled unlabeled data expectation maximization 
discussion 
experimental results 
datasets protocol 
em unlabeled data increases accuracy 
em unlabeled data hurt accuracy 
discussion 
vii enhancing generative model 
modeling sub topic structure 
em multiple mixture components class 
dataset protocol 
experimental results 
discussion 
modeling super topic structure 
estimating parameters hierarchical model 
dataset protocol 
experimental results 
discussion 
finding probable models 
influence labeled examples 
random starting points 
choosing random starting points 
dataset protocol 
experimental results 
discussion 
actively finding em initializations 
query committee active learning 
qbc text classification 
dataset protocol 
experimental results 
discussion 
avoiding local maxima deterministic annealing 
viii deterministic annealing 
experimental results 
discussion 
related text classification 
learning labeled unlabeled data 
likelihood maximization approaches 
discriminative approaches 
theoretical value unlabeled data 
active learning 
uses unlabeled data supervised learning 
findings 
directions 
complete hierarchy keywords bibliography ix chapter suppose web site maintains public listing job openings different companies 
user web site find new career opportunities browsing openings specific job category 
job postings web come category label 
reading job post manually determine label helpful system automatically examines text decision 
automatic process called text classification 
general text classification systems categorize documents set pre defined topics interest 
text classification great practical importance today massive volume online text available 
years explosion electronic text world wide web electronic mail corporate databases chat rooms digital libraries 
way organizing overwhelming amount data classify descriptive topical taxonomies 
example yahoo maintains large topic hierarchy web pages 
automatically populating maintaining taxonomies aid people search knowledge information 
automatic text classifiers created 
early attempts manual construction rule sets 
approach person compose detailed set rules automatically specifying class document 
example rule read job posting contains phrase expertise java job category computer programmer 
highly accurate text classifiers built approach significant cost 
constructing complete rule set requires lot domain knowledge substantial amount human time tune rules correctly 
exceptions impractical approach text classification 
efficient approach supervised learning construct classifier 
provide algorithm example set documents class allow find representation decision rule classifying documents 
approach gives high accuracy classifiers significantly expensive manual construction algorithm automatically constructs decision rule 
supervised text classification algorithms successfully wide variety practical domains 
examples cataloging news articles lewis gale joachims web pages craven shavlik learning reading interests users pazzani lang sorting electronic mail lewis knowles sahami 
supervised learning approach effortless hope 
key difficulty algorithms require large prohibitive number labeled training examples learn accurately 
labeling typically done person painfully time consuming process 
take example task learning newsgroup articles interest particular person reading usenet news 
lang person read articles learned classifier achieved precision making predictions top documents confident 
users practical system patience label articles especially obtain level precision 
obviously prefer algorithms provide accurate classifications dozen articles thousands 
need large quantities expensive labeled examples raises important question sources information reduce need labeled data 
goal thesis demonstrate supervised learning algorithms small number labeled examples large number unlabeled examples create high accuracy text classifiers 
general unlabeled examples expensive easier come labeled examples 
particularly true text classification tasks involving online data sources web pages email news stories huge amounts unlabeled text readily available 
collecting text frequently done automatically feasible quickly gather large set unlabeled examples 
unlabeled data integrated supervised learning building text classification systems significantly faster expensive 
unlabeled data 
crazy 
glance gained unlabeled data 
unlabeled document doesn contain important piece information class 
intuitive example unlabeled data useful 
suppose interested recognizing web pages academic courses 
just known course non course web pages large number web pages unlabeled 
looking just labeled data determine pages containing word homework tend academic courses 
fact estimate classification unlabeled web pages find word lecture occurs frequently unlabeled examples believed belong positive class 
occurrence words homework lecture large set unlabeled training data provide useful information construct accurate classifier considers homework lecture indicators positive examples 
thesis explain unlabeled data increase classification accuracy especially labeled data scarce 
formally information unlabeled data provide 
give knowledge distribution examples feature space 
general case distributional knowledge provide helpful information supervised learning 
consider classifying uniformly distributed instances conjunctions literals 
relationship uniform instance distribution space possible classification tasks clearly unlabeled data help 
need introduce appropriate bias learner assuming dependence instance distribution classification task 
standard supervised learning labeled data introduce bias order learn 
known proven result watanabe theorem ugly duckling shows bias pairs training examples look equally similar generalization classes impossible 
result long ago william ockham philosophy radical david hume 
somewhat zhang oles formalized proved supervised learning unlabeled examples assume dependence instance distribution classification task 
thesis assume dependence captured parametric generative model text documents class labels statistical process creates words class document 
example generative model encodes words common class 
creates document class randomly selecting words class word frequencies 
know process realistic statistical models capture subtleties authoring process 
assumptions encode relationship document distribution classification task allow unlabeled data incorporated learning 
specific classification task select model parameter values evidence contained labeled unlabeled data 
parameterization model judge probable model generated data 
learn text classifier searching parameter setting probable generated labeled unlabeled data 
statistical technique expectation maximization em dempster 
typically find probable parameter values find local maxima parameter space 
em gives parameter setting generative model text classification 
test document class label evaluate class label generated words document 
hope highly probable generative models unlabeled data correspond high accuracy text classifiers models perfect 
questions asked thesis asks answers central questions generative model approach unlabeled data improve text classification types generative models propose text classification simple comparison complexity human authoring 
maintain sense syntax context word order 
certainly obvious maximizing probability imperfect model increase classification accuracy 
mod els representative purposes text classification 
reason initially pessimistic 
similar generative model approaches related text tasks part speech tagging merialdo information extraction seymore shown incorporating unlabeled data supervised learning decreases performance systems 
text classification dependent tasks model correctness local level 
demonstrate generative models helpful valid approach unlabeled data text classification 
text classification domains generative model 
different text classification domains considerably properties 
short documents quite disparate lengths 
tasks easy understood complex ill defined 
classes narrow multi faceted 
wide variety text appropriate wonder tasks successfully unlabeled data generative model class 
demonstrate domains unlabeled data easily straightforward generative model suggested 
tasks incorporating unlabeled data basic model lowers classification accuracy 
adapt basic generative model unlabeled data challenging text classification domains 
unlabeled data useful generative model approach classification accuracy model likelihood correlated 
perfect model sufficient unlabeled data simple models text guarantees 
modeling assumptions approximate true data distribution unlabeled data improve classification 
cases may possible change relax assumptions closely match data 
adapt models different directions modeling sub topic structure class super topic hierarchical class relationships 
adapted models allow unlabeled data useful learning text classifiers difficult domains 
improve accuracy labeled data sparse 
generative model needs adaptation unlabeled data improve accuracy significantly compensate extreme lack labeled data 
cases em maximization process unable find highly probable parameters 
viewing em iterative hill climbing algorithm initialization sparse labeled data source trouble 
confront weakness ways selecting documents labeling provide higher quality initializations likelihood maximization techniques insensitive poor initialization 
road map outline thesis follows 
technical content contained chapters 
chapter presents baseline generative model text documents classes 
derives learning algorithm incorporates unlabeled data supervised learning likelihood maximization em 
show promising experimental results showing significant benefit unlabeled data datasets 
datasets basic approach inadequate unlabeled data hurt classification accuracy 
chapter explores change modeling assumptions response poor performance unlabeled data baseline model 
domains multifaceted complex classes sub topic structure class accurately modeled relaxing assumption correspondence mixture components classes 
domains hierarchy indicating class similarities relationships super topic structure accurately modeled relaxing assumptions independence classes 
enhancements allow unlabeled data improve text classification accuracy domains unresponsive basic approach 
chapter improves performance classifiers built unlabeled data sparse labeled data 
analysis effects limited labeled data demonstrate em initialization hinders performance 
improve accuracy query committee active learning approach select high quality labeled documents initialization 
show deterministic annealing likelihood maximization technique similar em sensitive poor initialization finds accurate text classifiers 
chapter surveys current state text classification 
relates different approaches combining labeled unlabeled data contrasts approach taken thesis 
chapter discusses dissertation answers questions posed 
proposes avenues research suggested findings 
chapter incorporating unlabeled data generative models way incorporate unlabeled data supervised learning assume statistical process generates documents 
precisely specifying generative model statistical technique expectation maximization find high probability parameters model combination labeled unlabeled data 
model turned text classification generative model embedded notion class 
experimental evidence shows unlabeled data expectationmaximization increase classification accuracy 
additional evidence shows straightforward approach perform 
hypotheses explain assumptions generative process result unrepresentative model model probability classification accuracy correlated expectation maximization search suffers getting stuck local maxima low probability models 
remainder thesis shows overcome problems 
dissertation appeal framework statistical modeling 
text written people strong complicated set rules governing composition grammar specify simple statistical generative model production text documents 
implicit model assumptions documents generated class specific probability distributions words occur documents independently class 
model straightforward find parameters set labeled data 
despite generative process practitioners successfully naive bayes approach text classification years 
approach combination labeled unlabeled data 
statistical model naive bayes add evidence unlabeled data finding high probability generative parameters assumed model 
introduce algorithm learning labeled unlabeled documents combination expectation maximization em naive bayes classifier 
em iterative technique maximum likelihood maximum posteriori parameter estimation problems incomplete data dempster 
case unlabeled data considered incomplete come class labels 
algorithm trains classifier available labeled documents assigns probabilistically weighted class labels unlabeled document classifier calculate expectation 
trains new classifier documents originally labeled unlabeled iterates 
em performs hill climbing model probability space finding classifier parameters locally maximize probability model data labeled unlabeled 
combine em naive bayes classifier mixture multinomials commonly text classification 
experimental results obtained text different real world tasks show unlabeled data reduces classification error 
fixed classification error unlabeled data dramatically reduce number labeled examples needed 
example identify source newsgroup usenet article classification accuracy traditional learner requires labeled examples alternatively algorithm takes advantage unlabeled examples requires labeled examples achieve accuracy 
task technique reduces need labeled training examples factor 
labeled documents class accuracy improved adding unlabeled data 
findings illustrate power unlabeled data text classification problems demonstrate strength algorithms proposed 
different dataset show basic em suffer misfit modeling assumptions unlabeled data 
results raise number interesting questions motivate remainder dissertation 
outline chapter follows 
section specifies naive bayes generative model baseline thesis 
section discusses model probability optimization criteria choosing parameter settings generative model 
section explains optimize model likelihood set labeled data model text classification purposes 
section presents locally optimize model likelihood combination labeled unlabeled data 
section presents experimental results showing combination labeled unlabeled data outperform labeled data 
section discusses findings poses challenges questions addressed rest dissertation 
generative model text section presents probabilistic framework characterizing nature documents classifiers 
framework defines probabilistic generative model data embodies assumptions generative process data produced mixture model correspondence mixture components classes mixture components multinomial distributions individual words words document produced independently class 
assumptions derive naive bayes classifier simple commonly tool text categorization finding probable parameters model 
documents generated mixture multinomials model mixture component corresponds class 
jcj classes vocabulary size jv document jdj words 
create document model 
roll biased jcj sided die determine class document 
pick biased jv sided die corresponds chosen class 
roll die jdj times write indicated words 
words form generated document 
formally document generated probability distribution defined parameters mixture model denoted 
probability distri bution consists mixture components fc jcj component parameterized disjoint subset 
document created selecting mixture component mixture weights class probabilities having selected mixture component generate document parameters distribution jc 
characterize likelihood document sum total probability mixture components jcj jc document class label 
assume correspondence mixture model components classes indicate jth mixture component jth class 
class label particular document written document generated mixture component say class label may may known document 
document considered ordered list word events hw write word position document word vocabulary hw jv example paragraph document word document 
document generated particular mixture component document length jd chosen independently component 
note assumes document length independent class 
selected mixture component generates word sequence specified length 
assume generates word independently length 
expand second term equation express probability document mixture component terms constituent features document length words document 
note general setting probability word event conditioned words precede 
jc hw jd jd jd jc standard notational shorthand random variables jy written jy random variables values previous naive bayes formalizations include document length effect 
general case document length modeled parameterized class class basis 
standard naive bayes assumption words document generated independently context independently words document class label 
assume probability word independent position document example probability seeing word homework position document seeing position 
express assumptions jc jc combining equations gives naive bayes expression probability document class jc jd jd jc parameters individual mixture component define multinomial distribution words collection word probabilities written jc jc jc jv jg jc 
assume classes document length identically distributed need parameterized classification 
parameters model mixture weights class probabilities written indicate probabilities selecting different mixture components 
complete collection model parameters defines set multinomials class probabilities jc cg 
model probability optimization criteria ultimate goal building text classifier find high accuracy previously unseen examples 
don examples hand labels find alternative criteria selecting classifier parameters 
astute statistician note multinomial coefficients missing truly multinomial distribution 
put word order generative model 
real mixture multinomials uses order gives exactly classifiers coefficients cancel mccallum nigam 
criteria described literature reasonable amount success large set labeled examples 
potential criteria maximize classification accuracy labeled examples hand 
done various gradient descent approaches lewis 
problem approach quite easy find parameters maximize classification accuracy labeled set getting generalization examples 
dimensionality text classification large easy overfit training data 
additionally possible solutions correctly classify labeled data clear choose 
hard imagine unlabeled data optimization criteria labels examples play role estimating classification accuracy 
second criteria maximize margin different classes 
approach support vector machines example find linear separator classes furthest away data 
approach tends find separators lie valleys low document density 
approach adapted labeled unlabeled data joachims shown promising results 
intuitively classification boundary separates data far away data provides method achieving low classification error 
presence assumed generative model scenario gives third way build classifier 
approach take maximize probability model training data 
setting strong probabilistic interpretation natural talk probability model parameterization set data 
generative model correct infinite amount data probable solution give solution maximizes classification accuracy 
don necessarily believe generative model interesting question consider maximizing model probability reasonable optimization criteria 
naive bayes text classification section presents naive bayes text classifier 
traditionally trained collection labeled documents 
naive bayes finds probable parameters statistical model introduced section set labeled data 
naive bayes foundation build integrating unlabeled data supervised learning 
training naive bayes classifier labeled data learning naive bayes text classifier consists estimating parameters generative model set labeled training data fd jdj estimate parameters written 
naive bayes uses maximum posteriori map estimate finding arg max jd 
value probable evidence training data prior 
perform map estimation specify prior distribution model space 
prior distribution formed product dirichlet distributions class multinomial class probabilities 
dirichlet commonly conjugate prior distribution multinomials 
form dirichlet jc jv jc ff gamma ff constants greater zero 
set ff corresponds prior favors uniform distribution 
dirichlet distributions stolcke omohundro 
parameter estimation formulae result maximization data prior familiar ratios empirical counts 
estimated probability word class jc simply number times word occurs training data class divided total number word occurrences training data class counts numerator denominator augmented pseudo counts word come dirichlet prior jc type prior referred laplace smoothing 
smoothing necessary prevent zero probabilities infrequently occurring words 
word probability estimates jc jc jc jdj ij jv jv jdj ij count number times word occurs document ij class label 
class probabilities estimated manner involve ratio counts smoothing jdj ij jcj jdj derivation ratios counts formulae comes directly maximum posteriori parameter estimation 
finding maximizes jd accomplished breaking expression terms bayes rule jd dj 
term calculated product document likelihoods equation 
second term prior distribution parameters product 
expression maximized solving system partial derivatives log jd lagrange multipliers enforce constraint word probabilities class sum 
maximization yields ratio counts seen 
classifying new documents naive bayes estimates parameters calculated training documents equations possible turn generative model backwards calculate probability particular mixture component generated document 
derive application bayes rule substitutions equations jd jc jd jc jcj jd jc task classify test document single class class highest posterior probability arg max jd selected 
note assumptions generation text documents mixture model correspondence mixture components classes word independence document length distribution violated real world text data 
documents mixtures multiple topics 
words document independent grammar topicality 
despite violations empirically naive bayes classifier job classifying text documents lewis ringuette craven yang pedersen joachims mccallum 
observation explained part fact classification estimation function sign binary classification function estimation domingos pazzani friedman 
word independence assumption causes naive bayes give extreme class probability estimates 
estimates poor classification accuracy remains high 
learning naive bayes model labeled unlabeled data previous section showed find maximum posteriori parameter estimates set labeled data 
labeled unlabeled data similarly find map parameters 
labels unlabeled data closed form equations previous section applicable 
expectation maximization em technique statistics find locally map parameter estimates generative model 
em technique applied case labeled unlabeled data naive bayes yields straightforward appealing algorithm 
schematic algorithm shown 
naive bayes classifier built standard supervised fashion limited amount labeled training data 
perform classification unlabeled data naive bayes model 
note just class probabilities associated class 
rebuild new naive bayes classifier data labeled unlabeled estimated class probabilities true class labels 
means unlabeled documents treated fractional documents class probabilities 
iterate process classifying unlabeled data rebuilding naive bayes model converges stable classifier set labels data 
statistical foundation algorithm described section 
schematic building text classifiers labeled unlabeled data em 
expectation maximization set training documents task build classifier form previous section 
previously subset documents come class labels rest documents subset come class labels 
disjoint partitioning section learning classifier approached calculating maximum posteriori estimate arg max dj 
consider second term maximization probability training data probability data simply product probabilities document documents independent model 
unlabeled data probability individual document sum total probability classes equation 
labeled data generating component labels need refer mixture components just corresponding class 
probability data ffl inputs collections labeled documents unlabeled documents 
ffl build initial naive bayes classifier labeled documents 
maximum posteriori parameter estimation find arg max dj see equations 
ffl loop classifier parameters improve measured change jd complete log probability labeled unlabeled data prior see equation ffl step current classifier estimate component membership unlabeled document probability mixture component class generated document jd see equation 
ffl step re estimate classifier estimated component membership document 
maximum posteriori parameter estimation find arg max dj see equations 
ffl output classifier takes unlabeled document predicts class label 
table basic em algorithm described section 
dj jcj jc theta jy trying maximize jd directly log jd maximum maximum 
equation write log jd log log jcj jc log jy call incomplete log probability unlabeled data complete labels 
dropped constant term log convenience 
notice equation contains log sums unlabeled data maximization partial derivatives computationally intractable 
access class labels documents set containing binary indicators ij express complete log probability parameters log jd log sums term inside sum non zero 
log jd log jcj ij log jc replace ij unlabeled documents expected value current model jd equation bounds incomplete log probability equation 
shown application jensen inequality log log 
result find locally maximum hill climbing procedure 
formalized expectation maximization em technique proven dempster 

iterative hill climbing procedure alternately recomputes expected value maximum posteriori parameters expected value note labeled documents ij known 
estimated unlabeled documents 
denote estimates iteration algorithm finds local maximum jd iterating steps ffl step set zjd 
ffl step set arg max jd 
practice step corresponds calculating probabilistic labels jd unlabeled documents current estimate parameters equation 
step maximizing complete likelihood equation corresponds calculating new maximum posteriori estimate parameters current estimates jd equations 
initialization parameters lead local maxima em 
instantiations em choosing starting model parameterization randomly 
case selective starting point unlabeled data labeled data 
iteration process initialized priming step labeled documents estimate classifier parameters equations 
cycle begins step uses classifier probabilistically label unlabeled documents time 
algorithm iterates converges point change iteration 
algorithmically determine convergence occurred observing threshold change log probability parameters equation height surface em hill climbing 
table gives outline basic em algorithm section 
discussion summary em finds locally maximizes posterior probability parameters data labeled unlabeled 
provides method unlabeled data augment limited labeled data contribute parameter estimation 
interesting empirical question probable parameter estimates improve classification accuracy 
section discussed fact naive bayes usually performs classification despite violations assumptions 
hold true unlabeled data 
note justifications approach depend assumptions stated section data produced mixture model correspondence mixture components classes 
generative modeling assumptions correct maximizing model probability criteria 
bayes optimal classifier corresponds map parameter estimates model 
assumptions hold certainly case real world textual data benefits unlabeled data clear 
section show empirically method dramatically improve accuracy document classifier especially labeled documents 
expectation maximization known family algorithms long history applications 
application classification new statistics literature 
idea em procedure improve classifier un labeled data predates formulation em titterington 
survey related section 
application em mixture multinomials application approach text classification 
experimental results section provide empirical evidence combining labeled unlabeled training documents em outperforms traditional naive bayes trains labeled documents 
experimental results different text corpora usenet newsgroups newsgroups web pages webkb newswire articles reuters 
results show improvements accuracy due unlabeled data dramatic especially number labeled training documents low 
example newsgroups data set classification error reduced trained labeled unlabeled documents 
datasets protocol newsgroups data set mitchell joachims mccallum collected ken lang consists articles divided evenly different usenet discussion groups 
task classify article newsgroup posted 
categories fall confusable clusters example comp discussion groups discuss religion 
words stoplist common short words removed unique words occur feature selection 
tokenizing data skip usenet headers discarding subject line tokens formed contiguous alphabetic characters left unstemmed 
word counts document scaled document constant length potentially fractional word counts 
preliminary experiments newsgroups indicated naive bayes classification accurate word count normalization 
newsgroups data set collected usenet postings period months 
naturally data time dependencies articles data sets available internet www cs cmu edu www research att com lewis 
nearby time thread occasional quotations may contain words 
practical classifier data set asked classify articles trained articles past 
preserve scenario create test set documents selecting posting date articles newsgroup 
unlabeled set formed randomly selecting documents remaining 
labeled training sets formed partitioning remaining documents non overlapping sets 
sets created equal numbers documents class 
experiments different labeled set sizes create sets size obviously fewer sets possible experiments labeled sets containing documents 
non overlapping training set comprises new trial experiment 
webkb data set craven contains web pages gathered university computer science departments 
collection includes entirety departments additionally assortment pages universities 
pages divided categories student faculty staff course project department 
thesis populous non categories student faculty course project containing pages 
task classify web page appropriate categories 
consistency previous studies data set craven tokenizing webkb data numbers converted time phone number token appropriate sequence length token 
stemming stoplist stoplist hurt performance 
example stopword excellent indicator student homepage fourth ranked word mutual information 
limit vocabulary informative words measured average mutual information class variable 
feature selection method commonly text yang pedersen koller sahami joachims 
selected vocabulary size running leave cross validation training data optimize classification accuracy 
webkb data set collected part effort create crawler explores previously unseen computer science departments classifies web pages knowledge base ontology 
mimic crawler intended avoid reporting performance idiosyncrasies particular single department test leave university approach 
create test sets containing pages complete computer science departments 
test set unlabeled set pages formed randomly selecting remaining web pages 
non overlapping training sets formed method newsgroups 
reuters distribution data set consists articles topic categories reuters newswire 
studies joachims liere tadepalli build binary classifiers populous classes identify news topic 
documents data set multiple class labels category traditionally evaluated binary classifier 
words inside text 
tags including title remove reuter tags occur top bottom document 
stoplist stem 
reuters best vocabulary size differs depending category interest 
variance optimal vocabulary size unsurprising 
previously noted joachims categories wheat corn known strong correspondence small set words title words categories categories acq known complex characteristics 
categories narrow definitions attain best classification small vocabularies broader definition require large vocabulary 
vocabulary size reuters trial selected optimizing accuracy measured leave cross validation labeled training set 
newsgroups data set time dependencies reuters 
standard modapte train test split divides articles time documents form test set earlier available training 
experiments documents training set randomly selected form unlabeled set 
remaining training documents randomly select non overlapping training sets just positively labeled documents negatively labeled documents previously described data sets 
non uniform number labelings classes negative class frequent positive class binary reuters classification tasks 
results reuters reported classification accuracy precision recall breakeven points standard information retrieval measure binary classification 
accuracy performance metric high accuracy achieved predicting negative class 
task data set classification filtering find positive examples large sea negative examples 
recall precision capture inherent duality task defined recall correct positive predictions positive examples precision correct positive predictions positive predictions classifier achieve trade precision recall adjusting decision boundary positive negative class away previous default jd 
precision recall breakeven point defined precision recall value equal joachims 
algorithm experiments em described table 
classification results reported classification accuracy averages trials number labeled unlabeled documents appropriate 
posterior model probability reported shown graphs additive multiplicative constants dropped relative values maintained 
computational complexity em prohibitive 
iteration requires classifying training documents step building new classifier step 
experiments em usually converges iterations 
wall clock time read document word matrix disk build em model iterating convergence classify test documents minute webkb data set minutes newsgroups 
newsgroups data set takes longer documents words vocabulary 
em unlabeled data increases accuracy consider basic em incorporate information unlabeled documents 
shows effect basic em unlabeled data newsgroups data set 
vertical axis indicates average classifier accuracy test sets horizontal axis indicates amount labeled training data log scale 
vary amount labeled training data compare classification accuracy number labeled documents unlabeled documents unlabeled documents classification accuracy newsgroups data set unlabeled documents 
small amounts training data em yields accurate classifiers 
large amounts labeled training data accurate parameter estimates obtained unlabeled data methods converge 
accuracy traditional naive bayes unlabeled documents em learner access unlabeled documents 
em performs significantly better traditional naive bayes 
example labeled documents documents class naive bayes reaches accuracy em achieves 
represents reduction classification error 
note em performs small number labeled documents documents single labeled document class naive bayes obtains em 
expected lot labeled data naive bayes learning curve close plateau having unlabeled data help nearly labeled data accurately estimate classifier parameters 
labeled documents class classification accuracy increases 
results statistically significant 
way view results consider unlabeled data reduce need labeled training examples 
example reach classification accuracy naive bayes requires labeled examples em requires statistical results chapter number labeled examples small multiple trials paired tests 
number labeled examples large single trial report results mcnemar test 
tests discussed dietterich 
log probability model scatterplot showing correlation posterior model probability accuracy model trained labeled unlabeled data 
strong correlation implies model probability optimization criteria newsgroups dataset 
labeled unlabeled examples achieve accuracy 
results indicate incorporating unlabeled data supervised learning generative models significantly improve accuracy text classification especially labeled data sparse 
em find accurate classifiers 
optimizing posterior model probability classification accuracy directly 
generative model perfect expect model probability accuracy correlated em helpful 
know simple generative model capture properties contained text 
newsgroups results show need perfect model em help text classification 
generative models representative purposes text classification model probability accuracy correlated allowing em indirectly optimize accuracy 
illustrate definitively look newsgroups experiments empirically measure correlation 
demonstrates correlation point scatterplot labeled unlabeled splits 
labeled data setting em initialization iterations 
plot classification performance accuracy section shows labeled data just setting starting point gives essentially performance em iterations 
exclude labeled data accuracy number unlabeled documents labeled documents labeled documents labeled documents labeled documents labeled documents classification accuracy varying number unlabeled documents 
effect shown newsgroups data set different amounts labeled documents varying amount unlabeled data horizontal axis 
having unlabeled data helps 
note dip accuracy small amount unlabeled data added small amount labeled data 
hypothesize caused extreme estimates component membership jd unlabeled documents caused naive bayes word independence assumption 
test data show posterior model probability 
dataset classification accuracy model probability correspondence 
correlation coefficient accuracy model probability strong correlation 
take post hoc verification dataset amenable unlabeled data generative model approach 
optimization criteria model probability applicable tandem accuracy 
shown amount labeled data increases accuracy increases 
consider effect varying amount unlabeled data 
different quantities labeled documents hold number labeled documents constant vary number unlabeled documents horizontal axis 
naturally having unlabeled data helps helps labeled data 
notice adding small amount unlabeled data small amount labeled data hurts performance 
hypothesize occurs word iterations allow model probability numbers comparable trials 
iteration iteration iteration intelligence dd dd dd artificial lecture lecture understanding cc cc dd dd dist dd dd due identical handout rus due homework arrange problem assignment games set handout dartmouth tay set natural hw cognitive exam logic homework problem proving kfoury prolog sec postscript knowledge postscript solution human exam quiz representation solution chapter field ascii table lists words predictive course class webkb data set change iterations em specific trial 
second iteration em common course related words appear 
symbol indicates arbitrary digit 
independence assumption naive bayes leads overly confident jd estimates step cause unlabeled document heavily weighted class strong evidence 
bias naive bayes step spread unlabeled data evenly classes 
number unlabeled documents large problem disappears unlabeled set provides large sample smooth sharp discreteness naive bayes overly confident classification 
provide intuition em works detailed trace evolution classifier course em iterations 
table shows changing definition course class webkb dataset 
column shows ordered list words model indicates predictive course accuracy number labeled documents unlabeled documents unlabeled documents classification accuracy webkb data set unlabeled documents 
small numbers labeled documents em improves accuracy 
labeled documents em degrades performance slightly indicating misfit data assumed generative model 
class 
words judged predictive weighted log likelihood ratio 
symbol indicates arbitrary digit 
iteration parameters estimated randomly chosen single labeled document class 
notice course document specific artificial intelligence course dartmouth 
em iterations unlabeled documents see em unlabeled data find words generally indicative courses 
classifier corresponding column achieves accuracy em converges classifier achieves accuracy 
weighted log likelihood ratio rank words jc log jc understood information theoretic terms word contribution average inefficiency encoding words class code optimal distribution words sum quantity words kullback leibler divergence distribution words distribution words cover thomas 
precision recall breakeven classification accuracy category nb em nb vs em nb em nb vs em acq corn crude earn grain interest money fx ship trade wheat table precision recall breakeven accuracy showing performance binary classifiers reuters naive bayes nb basic em em labeled unlabeled data 
em unlabeled data hurt accuracy datasets newsgroups em increases accuracy 
webkb data set see incorporation unlabeled data decrease classification accuracy 
graph shows performance basic em unlabeled documents webkb 
em improves accuracy significantly amount labeled data small 
labeled documents class traditional naive bayes attains accuracy em reaches 
lot labeled data em hurts performance slightly 
labeled documents naive bayes obtains accuracy em worse 
differences performance statistically significant university test sets respectively 
em hurts performance data fit assumptions generative model mixture components best explain unlabeled data precise correspondence class labels 
webkb em hurts accuracy relatively large amount labeled data 
em hurt performance labeled data sparse 
table shows reuters dataset 
incorporating unlabeled data parameter estimation precision recall breakeven classification accuracy decrease 
indicates generative model accurate dataset 
reuters categories em finds significantly probable model evidence labeled unlabeled data 
frequently probable model corresponds lower accuracy classifier hope 
discussion experimental results see domains labeled data sparse unlabeled data helps classification considerably 
significant finding demonstrates text classification tasks addressed significantly human labeling effort 
clear maximizing probability simple generative model reasonable optimization criteria unlabeled data 
text classification tasks likelihood accuracy correspond generative model approach works cases 
domains see unlabeled data hurting sparse labeled data plentiful labeled data 
reasonable hypotheses explain em 
hypothesis explain mixed performance em goes back assumed generative model 
discussed text documents blatantly generated mixture multinomials process 
em maximizes probability subject assumption generative model correct guarantees em produce reasonable classifier simple models text 
em give high probability parameters unlabeled data may give accuracy 
take example webkb dataset 
labeled data sufficient saturate learner order magnitude unlabeled data 
case great majority data determining parameter estimates comes unlabeled set 
think em performing unsupervised clustering mixtures multinomials world model positioning mixture components maximize likelihood unlabeled documents 
mixture model assumptions just little bit natural clustering unlabeled data may produce mixture components correspondence class labels detrimental classification accuracy 
text tasks posterior probability maximization detrimental amount labeled data reasonable 
em increased likelihood parameters accuracy part speech taggers merialdo information extractors mccallum went 
tasks example defined just small amount local context 
correctness model important correlated features 
text classification documents longer sensitive local correctness generative model 
understandable performance reuters decreased addition unlabeled data 
consider validity generative model newswire domain 
multinomial component models documents single topic trade 
documents topics modeled second multinomial component 
overly optimistic try model documents trade single multinomial entire newswire sub clusters documents 
consider relation desired classification task probable clustering data multinomials 
natural clustering multinomials correspond unusual separation data 
better generative model take account classification task natural distribution unlabeled documents 
second hypothesis explain em poorly unlabeled data em gets caught poor local maxima model probability space hillclimbing process 
trouble arise model probability accuracy correspondence 
em guaranteed converge local maxima guarantee best global solution 
fact highdimensional parameter space mixture multinomials certainty local maxima 
example case mixture gaussians data point introduces singularity likelihood surface day 
practice statisticians concerned poor local maxima em 
standard approach problem run em times randomly chosen starting points select local maximum highest probability 
adopted approach presence labeled data indicate starting point 
approaches find probable maxima addressed chapter 
models higher probability may correspond classifiers higher accuracy 
remainder thesis explores hypotheses detail 
chapter addresses concerns hypothesis changing generative model different ways 
changes bring model probability accuracy correlation complex datasets 
chapter addresses second hypothesis shows ways reduce cost incurred local maxima 
approaches increase benefit unlabeled data model representative 
chapter enhancing generative model generative modeling assumptions vary reality addition unlabeled data hurts classification accuracy 
happens models high probability unlabeled data correspond high accuracy classifiers test data 
negative effect reversed accurate statistical model domain question 
may structure classes represented generative model 
example may improve model represent sub topic structure modeling class mixture subtopics just single topic 
may capture super topic structure integrating model hierarchical relationships classes 
experimental evidence shows sophisticated model classes higher probability models give accurate text classifiers 
basic model hurts classification performance improved models allow supervised learning benefit inclusion unlabeled data 
previous chapter assumed simple statistical model generated text documents dataset 
model posterior maximization approach able estimate parameters model data labeled unlabeled 
addition unlabeled data enabled find probable parameters em labeled data especially labeled data sparse 
saw domains probability example learning labeled unlabeled data generative model representative 
presence unlabeled data hurts classification accuracy 
model parameters strongly correlated classification accuracy unlabeled data select parameterizations gave accurate classifiers 
domains model probability accuracy correlated generative model representative datasets unlabeled data hurt classification performance 
chapter confront problem changing generative assumptions different ways models representative unlabeled data help text classification challenging domains 
unrepresentative models decouple relationship accuracy model probability 
consider binary classification task non text dataset shown 
assume generative model mixture gaussians class fixed uniform covariance matrices estimate parameters data shown 
note wrong generative model clusters data best represented quite different covariance matrices 
maximum posteriori methods build classifier indicated linear separator labeled datapoints shown performs reasonably accurately 
consider happens add unlabeled data indicated dots 
model maximization performed large mass negative examples captured small positive gaussian draw positive gaussian iterations em 
result scenario shown right hand side classification accuracy poor 
went wrong 
generative model representative data 
em probable parameterization put mixture covering side negative class leaving positive class unattended 
model representative fixed covariance matrices unrealistically em able model data better give accurate classifier 
text classification domains strong assumptions generative model text documents 
documents generated mixture model correspondence mixture components classes mixture component multinomial distribution words 
authors people compose documents fashion 
domains posterior model probability accuracy correlated generative model allowing unlabeled data helpful classification 
cases correlation hold toy example suggests modify assumptions representative true data distribution 
chapter relax turn assumptions text documents 
section consider assumption class modeled single mixture component 
classification task demand single class cover complicated multi faceted topic 
case unrealistic model class mixture component 
representative model multiple mixture components allow sub topic class expressed separately 
previous chapter reuters dataset characteristic original generative model unlabeled data hurt classification performance 
multiple mixture components class including unlabeled data parameter estimation text classifiers accurate 
section consider assumption data produced mixture model 
means documents class unrelated 
case natural hierarchical organization classes classes similar 
hierarchy model class relationships allow parameters shared classes estimated accurately 
cora dataset collection computer science research papers hierarchy sub fields cs 
experimental results show leveraging hierarchy improve classification accuracy unlabeled data example data violates assumption correspondence mixture components gaussians 
labeled unlabeled data indicated maximizing probability incorrect model gives poor classifier 
basic model classification accuracy decreases 
modeling sub topic structure second assumption generative model states correspondence classes components mixture model 
assumption strongly violated incorporating unlabeled data model estimation hurt classification performance 
example return gaussian mixture model previous section 
saw assumption mixture components covariance matrix unrealistic harmful 
shows example labeled unlabeled data violates assumption class exactly mixture component 
positive class modeled gaussian negative class 
data distributed sea examples positive class small island 
representative model 
modeling sea negative examples single mixture component better model components 
way negative component maximization capture clump sea examples 
shows data modeled mixture components negative class 
see realistic data time modeled mixture components negative class 
assumed generative model representative 
modeling data 
section takes exactly approach suggested example text data relaxes assumption correspondence mixture components classes 
replace restrictive assumption correspondence mixture components classes 
allows model sub topic structure class 
text classification analogous scenario unusual 
consider task text filtering want identify small defined class documents large pool stream documents 
example system watches network administrator incoming emails identify rare emergency situation require paging vacation 
modeling non emergency emails negative class multinomial distribution result unrepresentative model 
negative class contains emails variety subtopics personal emails non emergency requests 
multiple mixture components accurately model negative class 
allow model probability maximization labeled unlabeled data give classifier provides significantly higher accuracy 
section show relax assumptions em new generative model estimate highly probable parameters data 
section return problematic reuters dataset incorporating unlabeled data original model hurt performance 
section shows representative model improving model probability em improves classification accuracy 
em multiple mixture components class correspondence mixture components classes original generative model previous chapter longer applicable 
new generative model account sub topic structure 
old model pick class biased die roll 
class sub topics pick sub topics biased die roll 
sub topic determined document words generated 
picking length independently sub topic class draw words sub topic multinomial distribution 
previously missing values unlabeled document class sub topic 
labeled data missing values class known sub topic 
access missing class sub topic labels technique em estimate local map generative parameters 
section em instantiated iterative algorithm alternates estimating values missing class sub topic labels calculating map parameters estimated labels 
em converges high probability parameter estimates generative model text classification turning bayes rule 
table gives overview modified em algorithm 
derive details algorithm starting point notation introduced chapter 
new generative model specifies separation mixture components classes 
denote denotes jth mixture component 
write ath class topic component belongs class aj 
represents pre determined deterministic mapping mixture components classes 
indicate class label sub topic label document respectively 
document generated mixture component say document belongs class say class sub topic labels known dataset finding map ffl inputs collections labeled documents unlabeled documents 
ffl set number mixture components class cross validation see sections 
ffl initialize mixture component randomly assigning non zero jd labeled document document class label 
ffl build initial classifier labeled documents 
maximum posteriori parameter estimation find arg max dj see equations 
ffl loop classifier parameters improve measured deltal jd change complete log probability generative model ffl step current classifier estimate class sub topic memberships document see equations 
restrict membership probability estimates labeled documents zero components associated classes renormalize 
ffl step re estimate classifier estimated component membership document 
maximum posteriori parameter estimation find arg max dj see equations 
ffl output classifier takes unlabeled document predicts class label equation 
table modified algorithm integrating unlabeled data em multiple mixture components class 
estimates generative parameters straightforward application closedform equations similar naive bayes seen section 
formula word probability parameters identical equation naive bayes jc jc jdj jd jv jv jdj jd class probabilities analogous equation new notation classes components jdj jd jt jdj sub topic probabilities similar estimated documents component class jt jdj jd jcj aj jdj jd classification time estimate class membership probabilities unlabeled document 
done calculating sub topic membership summing sub topics get class probabilities 
sub topic membership calculated analogously mixture component membership naive bayes small adjustment account presence priors class sub topic just jd jt aj jt jd jc jcj jt br jt jd jc class membership calculated sum probability class sub topics jd jcj aj jd equations supervised learning applicable training documents class sub topic labels 
em 
derivation em process find parameters model labeled unlabeled data runs analogously section 
step basic em builds maximum posteriori parameter estimates multinomials priors 
done equations probabilistic class subtopic memberships estimated previous step 
step unlabeled documents calculate probabilistically weighted sub topic class memberships equations 
labeled documents estimate sub topic membership 
know class label sub topic memberships zero sub topics belong classes 
calculate sub topic memberships unlabeled data setting appropriate ones zero normalizing non zero ones topics belong class 
original generative model initialized em labeled data 
approach multiple mixture components initial subtopic labels provided 
randomly spread labeled training example mixture components belong class 
components initialized performing randomized step jd sampled uniform distribution mixture components belonging document class 
set class labeled data set unlabeled data apply em specification number sub topics class 
information typically available 
result resort techniques model selection 
commonly approaches model selection cross validation aic bic 
availability limited number labeled documents cross validation select number sub topics classification performance 
tension model selection process complexity model data sparsity 
sub topics documents perfectly model training data sub topic covers training document 
large number sub topics accurately model existing data generalization performance poor 
multinomial parameters estimated documents suffer sparse data 
sub topics opposite problem arise 
accurately estimate multinomials model overly restrictive representative true document distribution 
cross validation help selecting compromise tensions specific regard classification performance 
note multiple mixture components class allows capture dependencies words class level 
example consider sports class consisting documents hockey baseball 
documents words ice puck occur words bat base occur 
dependencies captured single multinomial distribution words sports class 
multiple mixture components class multinomial cover hockey sub topic baseball sub topic 
hockey sub topic word probability ice puck significantly higher class 
occurrence hockey documents single multinomial assumption 
dataset protocol test domain new generative model return reuters collection newswire articles previous chapter 
documents dataset multiple class labels category traditionally evaluated binary classifier 
original generative model negative class covers distinct categories expect task strongly violate assumption data negative class generated single mixture component 
reason model positive class single mixture component negative class mixture components unlabeled data 
pre process data follow experimental protocol described section 
positive negative documents labeled unlabeled 
classification results reported precision recall breakeven points classification accuracy 
algorithm experiments em described table 
leave cross validation performed conjunction em simplification computational efficiency 
run em convergence training data subtract word counts labeled document turn testing document 
performing cross validation specific combination parameter settings run em required run em labeled example 
note residual effects held document 
experimental results table contains summary precision recall breakeven results reuters 
nb em columns reproduce results section original generative model mixture component class 
remember incorporating unlabeled data em hurts performance 
hypothesize negative class truly multi modal fitting single naive bayes class em data accurately capture negative class word distribution 
nb column shows results modeling negative class multiple mixture components just labeled data 
nb column number components selected optimize best precision recall breakeven point 
category nb nb em em em vs nb em vs nb acq corn crude earn grain interest money fx ship trade wheat table precision recall breakeven points showing performance binary classifiers reuters traditional naive bayes nb multiple mixture components just labeled data nb basic em em labeled unlabeled data multiple mixture components em labeled unlabeled data em 
nb em number components selected optimally trial median number components trials negative class shown parentheses 
note multi component model natural reuters negative class consists topics 
unlabeled data multiple mixture components class increases performance naive bayes 
median number components selected trials indicated parentheses breakeven point 
note unlabeled data complex representation improves performance traditional naive bayes 
em automatically finds high probability division negative class sub topics help improve classification 
column labeled em shows results em multiple mixture components labeled unlabeled data selecting best number components 
performance better nb traditional naive bayes nb naive bayes multiple mixture components class component class em worse 
increase unlabeled data measured trials reuters statistically significant 
indicates multiple mixture components increases performance traditional naive bayes combination unlabeled data multiple mixture components increases performance 
generative model representative multi modal data increasing model likelihood em increases classification category nb nb em em em vs nb em vs nb acq corn crude earn grain interest money fx ship trade wheat table classification accuracy reuters traditional naive bayes nb multiple mixture components just labeled data nb basic em em labeled unlabeled data multiple mixture components em labeled unlabeled data em table 
accuracy 
table shows results table classification accuracy precision recall breakeven 
general trends accuracy precision recall 
accuracy optimal number mixture components negative class greater precision recall 
nature precision recall focuses modeling positive class accuracy focuses modeling negative class frequent 
allowing mixture components negative class accurate model achieved 
interesting note average em uses mixture components nb 
suggests addition unlabeled data supports complex model 
unlabeled examples data sparsity problems severe best tradeoff model representation parameter generally lies fewer mixture components class 
unlabeled data plentiful complex models representative accurate 
provide scatterplots model probability versus accuracy demonstrate correlated multiple mixture components 
acq classification task compare mixture components median number picked acq 
provide fair comparisons number labeled documents em negative components em negative component naive bayes comparison models unlabeled data reuters acq task 
accuracy best mixture components negative class incorporating unlabeled data 
mixture component em finds high likelihood models correlate high accuracy classifiers 
different training set splits labeled data set em initialization fix vocabulary top words training data median vocabulary size selected 
vary number labeled data random training set splits amount 
shows mixture components consistently gives best classification accuracy compared em single mixture component naive bayes 
note em just component quite poorly significantly worse naive bayes 
scatterplots tell single component poor representation 
shows correlation model probability classification accuracy mixture component left mixture components right 
note component correlation strong gamma wrong direction 
models higher probability significantly lower classification accuracy 
examining solutions em find mixture components positive negative probable clustering data component majority negative documents second positive documents significantly negative documents 
section shows labeled data just setting starting point gives essentially performance em iterations 
log probability model log probability model scatterplots showing relationship model probability classification accuracy reuters acq task 
left mixture component negative class probability accuracy inversely proportional exactly want 
right mixture components negative moderate positive correlation model probability classification accuracy 
classes separate high probability models 
mixture components story quite different 
right shows moderate correlation model probability classification accuracy right direction 
solutions component covers nearly positive documents negatives 
components distributed remaining negative documents 
model representative data classification task classification accuracy model probability correlated 
allows beneficial unlabeled data generative model approach 
tables show complete results experiments multiple mixture components unlabeled data respectively 
note general mixture components hurts performance 
components assumptions overly restrictive model representative 
components parameters estimate amount data suffer unlabeled data sparsity 
substantially larger amounts unlabeled data hypothesize able support mixture components point model longer representative 
obvious question automatically select best number mixture components having access test set labels 
leave cross category em em em em em em acq corn crude earn grain interest money fx ship trade wheat table performance em different numbers mixture components negative class unlabeled documents 
precision recall breakeven points shown experiments mixture components 
note mixture components results poor performance 
validation computational short cut entails running em described section 
results technique em cv compared naive bayes nb best em em shown table 
note cross validation perfectly select number components perform best test set 
discussion experiments see advantageous explicitly model sub topic structure text classification single multinomial restrictive model class 
allowing multiple multinomial mixture components class fit parameters em 
new model unlabeled data improves classification accuracy naive bayes labeled data reuters classification tasks 
choosing complex model method cross validation consistently selects mixture components 
cross validation computational short cut bias model held document hypothesize favors fewer components 
computationally expensive category nb nb nb nb nb nb acq corn crude earn grain interest money fx ship trade wheat table performance em different numbers mixture components negative class unlabeled data 
precision recall breakeven points shown experiments mixture components 
category nb em em cv em cv vs nb acq corn crude earn grain interest money fx ship trade wheat table performance multiple mixture components number components selected cross validation em cv compared optimal selection em straight naive bayes nb 
note cross validation usually selects components 
complete cross validation perform better 
model selection methods may perform better maintaining computational efficiency 
include robust methods cross validation ng minimum description length rissanen metric approach uses unlabeled data 
research improved methods model selection algorithm area 
believe learning generative model approach situations labeled unlabeled data require closer match data model labeled data 
intended target concept model differ actual distribution data strongly unlabeled data hurt help performance 
just labeled data things somewhat resilient 
example assumed equivalent covariance matrices classes labeled data derived classifier accurate 
similarly naive bayes text non text give classification class probability estimates poor linear boundary important classification domingos pazzani friedman 
unlabeled data dependence generative model stronger model give unlabeled data estimated class labels 
reuters example see improvements labeled data case expanded generative models 
labeled unlabeled data differences larger 
modeling super topic structure previous section modeled sub topic structure relaxing assumption relationships class mixture components 
model super topic structure changing assumption relationship classes 
mixture models require parametric form class independent classes 
section model dependencies classes hierarchical relationships 
text domains classes interest arranged hierarchy 
example hierarchical organizations web yahoo canonical example 
dewey decimal system assigns books hierarchical fashion 
patents classified large hierarchy different fields innovation 
research arranged hierarchically computing research repository arxiv org uses acm computer science hierarchy categorize submitted research 
complex datasets classes large amount unlabeled data italian food gardening mexican food simple hierarchy demonstrating relationships classes 
may sufficient model document distribution accurately 
happens likelihood maximization find parameters model unlabeled data reflect true distribution data 
way algorithms overfit unlabeled data 
significantly unlabeled data problem alleviated domains finite amount unlabeled data may exist 
scenario unrepresentative generative models exacerbate problems overfitting 
example consider way classification task 
frequency generic cooking words grill similar italian food mexican food classes 
basic generative model classes assumed independent word frequencies estimated class available data 
estimate different matching peculiarities documents class 
model noted relationship training data classes pooled form accurate estimates generic cooking word frequencies 
representative model encodes hierarchical class relationships help ensure models high likelihood unlabeled data models true data distribution 
relationship holds helps parameter estimation find high accuracy classifiers 
section show learn high likelihood parameters hierarchical model em 
section cora dataset collection computer science research papers comes class hierarchy describes sub fields ffl inputs collection unlabeled training documents class hierarchy keywords class 
ffl generate initial labels unlabeled documents possible term matching keywords rule list fashion 
ffl split data disjoint subsets ffl initialize shrinkage weights jc uniform path leaf node root hierarchy 
ffl estimate word probability class prior parameters initially labeled data equations 
ffl loop parameter convergence ffl step current classifier estimate class labels document equation 
accumulate ancestor word generation counts documents equation ffl step re estimate shrinkage weights normalizing ancestor word counts equation 
re estimate class priors word probability parameters documents equations 
ffl output text classifier takes unlabeled document predicts class label equation 
table modified outline likelihood maximization unlabeled data hierarchical model em 
cs 
section demonstrates representative model learn parameters unlabeled data increase classification accuracy basic model accuracy decreases 
section discusses implications results 
estimating parameters hierarchical model leverage hierarchy technique known shrinkage carlin louis 
hierarchical shrinkage calculates word probability estimates leaf class calculating weighted average estimates path leaf root 
technique balances trade specificity reliability 
estimates leaf specific unreliable hierarchy estimates reliable unspecific 
mixture weights set em time word probability parameters set em labeled unlabeled data 
joint em process described 
think hierarchical shrinkage generative model uses hierarchy 
leaf hierarchy class 
pick class biased die roll class priors 
pick document length independently class 
generating words document generic words class specific 
choose hierarchy ancestor class node path root leaf including possibly shrinkage weights 
choose word multinomial selected ancestor 
repeat process choosing ancestor picking word position document 
way document created mixture specific general words drawn hierarchical relationships provided 
introduce notation describe hierarchies 
originally class class leaf node hierarchy 
node hierarchy denoted note distinction class node 
leaf hierarchy node class 
ancestors class include nodes path leaf root inclusive 
conversely set leaf contains class leaf nodes possibly including node hierarchy 
hierarchical shrinkage weights denote jc 
note shrinkage weights dependent class implying class set weights nodes tree 
weight node ancestor class zero 
ran em set word probabilities shrinkage weights set data shrinkage weights concentrate leaves 
happen specific model best fit training data 
result exactly overfitting trying prevent 
avoid problem split training data disjoint sets 
documents subset estimate word probability class prior parameters 
shrinkage weights subset concurrently perform em set shrinkage weights em set multinomials 
step estimate class label unlabeled document 
cal class probabilities previous sections jd jd jc jcj jd jc step shrinkage weights accumulate counts words class generated ancestor node 
accumulate counts just documents subset jd jd jd jc ja jd jd am jd am jc ja step take estimates class labels document counts word sources calculate new parameter estimates higher likelihood 
maximum posteriori parameter estimation 
motivation prevent word probabilities zero infrequently occurring words 
accomplish different way 
augment hierarchy placing new root node top old 
permanently fix word probabilities root node uniform vocabulary 
uniform multinomial ensures word probabilities mixed path class root non zero 
extra hierarchical twist maximum likelihood estimation maximum posteriori estimation 
benefit approach class determines best amount smoothing 
laplace smoothing fixed dirichlet prior data determines smoothing needed 
effect true shrinkage weights levels hierarchy 
classes lot data word distributions different ancestors expect shrinkage weights favor leaf node 
classes sparse training data expect significantly higher dependence shared ancestors including uniform root node 
calculating new shrinkage weights quite easy 
simply take accumulated counts normalize probabilities jc am am jc estimating class probabilities done just summing fractional memberships documents exactly original generative model jd jd jcj jdj calculate new word probabilities steps 
documents calculate ancestor node word probabilities pooling counts leaf classes calculating maximum likelihood estimates ja jd leaf jd jvj jd leaf jd calculate new word probability parameters class take weighted average ancestors shrinkage weights jc jc jc completes explanation em iterations 
dataset labeled data available parameter estimation em 
em initialization process quite different 
labeled data domain question provided small number key words phrases class 
keywords assign initial labels unlabeled documents term matching rule list fashion document step keywords place document category keyword matches 
document matches keywords initialization 
treat initial labels class membership estimates 
initialize shrinkage weights uniform path leaf root 
initial labels shrinkage weights calculate word probability parameters 
gives initialization em 
priming step initial labels discarded replaced step estimates iteration 
way limited domain knowledge keywords provide reasonable initialization em 
artificial intelligence systems operating information retrieval robotics robot robots robotics learning machine human computer interaction reinforcement learning neural networks 
retrieval filtering libraries 
computer science hardware architecture 
document filtering text classification document document categorization classification digital library planning planning nlp language nlp natural processing temporal reasoning reasoning time information retrieval subset cora computer science hierarchy complete keyword list categories 
keywords find unlabeled documents initialize em 
complete hierarchy keywords detailed appendix dataset protocol test domain hierarchical generative model cora dataset collection computer science research papers classified sub fields cs 
dataset build text classifiers automatically place research papers web postscript format appropriate sub field 
taxonomy search engine papers automatically constructed citation graph publicly available resource researches practitioners mccallum 
support taxonomy hierarchy computer science topics part shown 
hierarchy created examining conference proceedings computer science sites web 
small test set created expert hand labeling random sample research papers papers cora computer science archive time experiments 
third fit category discarded resulting labeled documents 
discarded papers outside area computer science astrophysics papers papers complete hierarchy considered computer science papers 
class frequencies data skewed drastically test set populous class accounted documents 
research represented words title author institution 
segments automatically extracted trained hidden markov model 
extraction performed independently classification task described detail seymore 

words occurring fewer documents words standard stoplist discarded 
stemming 
documents labels documents test set measure classification accuracy 
leave documents available labeled training data 
replacement labeled data human provided key words phrases class set starting point em 
keywords quicker generate small number labeled documents 
keywords generate initial labels unlabeled documents initial labels set em starting point 
shows examples number type keywords selected experimental domain 
complete hierarchy keywords detailed appendix provide keywords class classification keyword matching inaccurate incomplete 
keywords tend provide high precision low recall brittleness leaves documents unlabeled 
documents match keywords wrong class 
experimental domain example unlabeled documents contain keywords 
documents containing keywords precision keyword matching test set 
complete algorithm hierarchical experiments table 
experiments randomly selected documents estimate shrinkage weights remaining estimate word probabilities 
fewer documents needed accurately estimate relatively small set shrinkage parameters 
experimental results section provide empirical evidence em unlabeled data hierarchy produces high accuracy text classifier 
table shows classification results different classification techniques 
interesting baselines provide sense difficulty classification task 
labeled documents test set leave fashion naive bayes reaches accuracy 
test set relabeled second human expert agreement original labeling showing classification task question challenging 
method labeled initially labeled unlabeled accuracy naive bayes human agreement keyword matching naive bayes em shrinkage em hierarchy em hierarchy table classification results different techniques keyword matching naive bayes em hierarchical generative model 
classification accuracy number labeled keyword matched initially labeled unlabeled documents variant shown 
best algorithmic performance achieved unlabeled data hierarchical generative model 
keywords initially label documents em starting point 
keywords applied test set give accuracy 
applied unlabeled data keywords generate initial labels documents 
initially labeled documents build naive bayes classifier treating labels correct get accuracy 
fix labels run em shrinkage ignoring remaining documents accuracy increase 
original generative model hierarchy em finds parameters higher likelihood unlabeled data 
classification accuracy decreases 
likelihood maximization naive bayes model complex classification task sparse data 
switch hierarchical model see likelihood maximization increases classification accuracy 
hierarchical em unlabeled data improves accuracy 
shows representative generative model able overcome negative effects overfitting 
discussion interesting experimental result difference performance em hierarchy 
hierarchy likelihood maximization documents test set containing keywords assigned class rule list classifier assigned populous class default 
em lowers classification accuracy 
representative model increasing likelihood em increases classification accuracy 
representative model sparse data models higher likelihood unlabeled data correspond higher accuracy classifiers due overfitting 
interesting notice hierarchical shrinkage improve performance data 
labeled data classification accuracy increase better model 
provides evidence learning labeled unlabeled sensitive match data model regular supervised learning unlabeled data 
hierarchical models text literature unsupervised clustering purely supervised clustering labeled data 
mccallum 
hierarchical shrinkage form reliable parameter estimates sparse labeled training data text classification 
show improved classification accuracy datasets hierarchy 
hofmann puzicha related model create hierarchy unsupervised fashion collection scientific abstracts 
chapter apply models learning combination labeled unlabeled data 
chapter changed generative assumptions 
studies examined relaxing word independence assumption supervised learning labeled data 
context multi variate bernoulli generative model sahami allows limited word dependencies class 
specifically allows word depend exactly essence creating dependency tree 
finds classification performance model higher strict word independence model 
multinomial distributions equivalent unigram language models 
mladenic grobelnik explore sophisticated bigram language model classification 
model word occurrence probability depends word previous document 
findings show bigram modeling gives better classification performance 
li yamanishi relaxes correspondence differently 
allow correspondence clusters classes relationship 
number mixture components classes introduce probabilistic relationship components classes 
models chapter extended natural ways 
multiple mixture components class hierarchical generative models combined 
approach suggested hierarchy provided class hierarchy complex multi faceted 
hierarchical model naturally extended model directed acyclic graphs restricting class relationships expressed tree structures 
summary chapter demonstrated generative model approach necessary representative model 
model probability accuracy correlated unlabeled data hurt classification help 
needed generative model improved 
shown different ways making models representative demonstrated unlabeled data successfully learning text classifiers 
chapter finding probable models demonstrated assuming generative model incorporate unlabeled data supervised learning improve accuracy text classification 
chapter provides ways improve performance unlabeled data generative model representative labeled data sparse 
conditions number labeled examples bottleneck significant improvements 
bottleneck caused sparse labeled data provide poor em initialization results low probability local maximum 
traditionally mitigated running em times different random starting points 
text domains best classifier random initializations better initialized deterministically labeled data 
parameter space large randomly explore initializations 
alternative technique limited interaction human labeler specific documents selected labeling learning algorithm 
allows learner influence em initialization 
query committee approach active learning requests labels documents prototypical high classification variance 
experimentally data labeled fashion provide higher accuracy initialization em accurate classifier 
way improve weakness em consider maximization techniques 
deterministic annealing technique avoids local maxima maximizing smooth probability surface gradually making bumpy tracking maximum surface gets complex 
experimentally deterministic annealing finds probable models higher accuracy classifiers labeled training data sparse 
accuracy number labeled documents unlabeled documents unlabeled documents classification accuracy newsgroups data set unlabeled documents 
unlabeled data helps labeled data sparse significant room improvement 
previous chapter saw datasets unlabeled data basic algorithm chapter 
generative model assumptions strongly violated true data distribution parameters high probability correspond classifiers high accuracy 
modifying assumed generative model representative data classification accuracy model probability came correspondence 
integrating evidence unlabeled data improved model allowed find classifiers higher accuracy 
chapter explore improve learning performance classification accuracy model probability correspondence 
example section experimental evidence newsgroups dataset showing classification accuracy model probability strong correspondence integrating unlabeled documents posterior model maximization improves text classification accuracy 
labeled data sparse improvements largest performance substantially achieved plentiful labeled data 
findings originally shown shown 
infinite amount unlabeled data true parameters generative model recovered 
just labeled examples class assigned cluster resulting bayes optimal classifier 
having sparse labeled data matter 
example consider case estimating univariate normal distributions classification 
infinite amount unlabeled data recover global maximum posteriori parameters mixture component indicated 
just labeled examples correctly match classes components bayes optimal classifier castelli cover 
things different reality large infinite amount unlabeled data highdimensional feature spaces 
local maxima abound probability surface easily find global maximum 
reason weak performance sparse labeled data juice unlabeled data 
modifying algorithms may able effective unlabeled data hand 
chapter explores ways improve performance generative model representative labeled data limited 
cases performance suffers getting stuck local maxima em search model probability space 
section shows primarily caused poor em initializations labeled data 
section shows standard approach multiple em runs random initialization helpful text classification 
section active learning algorithm conjunction labeler provides improved initializations lead classification accuracy increases 
section demonstrates 















table randomly selected confusion matrix test set labeled documents class newsgroups 
true classes rows predicted clusters columns 
preponderance documents lying diagonal indicates class cluster correspondence significant factor classification accuracy 
deterministic annealing maximization algorithm similar em achieves higher model probability classification accuracy local maxima avoidance 
influence labeled examples understanding role labeled data play algorithm help see performance better labeled data sparse 
learning labeled unlabeled data labeled data influence result algorithm ways correlate cluster class influence parameter estimation em iterations initialize starting point em 
section examine relative strengths influences conclude initialization important role played labeled data 
number labeled documents unlabeled documents class reassignment unlabeled documents unlabeled documents classification accuracy newsgroups data set labeled unlabeled documents class reassignment 
benefits reassignment minimal indicating faulty class correlation significant factor classification 
em finds high probability model classification poor simply swap class identities clusters 
example classifier perfectly distinguishes sports arts suddenly get accuracy swap definitions 
experiments clusters correlated classes assigning cluster class documents initialized 
implicit assumptions initialization places cluster right neighborhood class cluster successfully tracks class em iterations 
contrast approach correlation process done iterations complete seeing cluster class labeled documents fall 
clustering perfect method requires small amount labeled data correctly match cluster class castelli cover 
correspondence happening poorly labeled data sparse 
table shows typical confusion matrix newsgroups labeled documents class 
case cluster assigned class plurality documents 
anecdotally diagonal confusion matrices dominant way 
measure loss accuracy due incorrect class correspondence greedily reassigning classes maximize accuracy accuracy number labeled documents em labeled start iterations em labeled start unlabeled data comparison em performances differing labeled data 
top line shows normal em labeled unlabeled data 
second line overlapping shows performance em labeled data set initial parameters em 
strong similarity indicates power labeled data comes iterations setting initial parameters 
performance test data 
shows results indicate minimal loss poor reassignment 
example labeled documents class total class correspondence problems lower accuracy 
deduce classification accuracy small amount labeled data suffer significantly due class correspondence problems 
consider effect labeled data iterations em 
way think application em performs semi supervised clustering 
labeled data provide supervision iterations em remaining fixed correct class 
orders magnitude unlabeled documents labeled ones great majority data comes unlabeled set 
em iterations expect class mixture components positioned maximize likelihood unlabeled documents 
explicitly measure effect labeled data iterations optimally reassign classes feasible class problem involve considering permutations 
smaller classification problems factor 
problem huge number classes significant factor number possible errors grows 
simple experiment 
test effect performing regular algorithm withholding labeled data em iterations 
compares effects labeled data just initialization initialization iterations 
results experiment newsgroups shown 
accuracy classifiers built labeled data iterations essentially typical algorithm especially labeled data sparse 
example labeled documents class accuracy cases 
curves start marginally diverge large amount labeled data 
indicates influence labeled data em iterations quite minimal 
simple experiments eliminated possible effects labeled data class component correspondence influence em iterations 
deduce limited labeled data hinder unlabeled data approach primarily giving poor em initializations 
initialization effect 
possible explanation probable initialization poor local maxima completely bypassed 
model probability increase em initialization successfully avoids local maxima lower probability 
second explanation regularity probabilities local maxima 
initialization may direct em regions parameter space tend higher maxima 
optimization algorithms domains focus explicitly effect model regularities local maxima select initializations boyan 
different ways address problem limited labeled data give poor em initializations 
choose initializations different way strong dependence labeled data 
second continue labeled data initialize em ensure labeled data highquality 
third idea dispense em different algorithm maximizing model posterior sensitive labeled data 
directions addressed sections turn 
section explores random initialization labeled data deterministically 
section uses active learning select high quality labeled data 
section uses deterministic annealing em avoid poor local maxima 
random starting points previous section demonstrated learning text classifiers labeled data primary influence form initial parameter estimates em 
labeled data plentiful initial parameters high probability accuracy em incorporate evidence unlabeled data output high accuracy classifier 
hand labeled data sparse initial parameter estimates lower probability 
result em gets caught local maximum gives improved accuracy significantly obtained better initialization 
algorithm initialized parameter estimates map estimates derived labeled data 
initialization places classifier approximate neighborhood parameter space corresponds document class distributions 
applications em framework analog labeled data set starting point 
cases em typically initialized random parameter estimates 
single random parameter initialization frequently results poor local maximum parameter space local maxima small minority high probability 
overcome standard practice run em times different random initializations 
set em results highest model probability selected best parameterization 
multiple random runs allow exploration local maxima provide robustness poor parameter initialization 
apply approach text classification task 
unlabeled data setting starting point deterministically labeled examples help classification 
room improvement 
hope randomly selecting starting points find better better initialization deterministic produce higher accuracy classifier 
may hard practice find better initialization labeled data provide significant information 
choosing random starting points ways randomness parameter initializations 
example possibility set word probability parameters small perturbation uniform distribution 
known domain reasonable approach 
situation poor implementation choice setting represent basic knowledge text 
example mixture component significantly higher average probabilities common words round em unlabeled documents classified belonging cluster 
running em initialization give terrible classification model 
preliminary experiments random initializations showed consistently got low probability models 
text domains guesses word probabilities looking limited labeled data vast unlabeled data 
words large differences probabilities classes estimate fair approximation 
uniform distribution baseline mixture word frequencies labeled unlabeled data baselines 
specifically experiments priming step assign uniform classification posteriors unlabeled document 
set baseline initial component map parameter estimate limited labeled documents spread unlabeled ones 
baseline accurately represents knowledge labeled data significant influence unlabeled 
introduce randomness baseline need perturb estimates appropriately 

generative model approach 
labeled uniformly spread unlabeled data baseline class map estimate arg max jd 
words training data specifies probability distribution parameters mixture components probable parameterization distribution 
introduce randomness selecting parameters probability distribution labeled unlabeled data just choosing probable 
notation introduced chapter data define dirichlet distribution multinomial parameters class jd jv jc ff gamma ff smoothed number word occurrences seen training data class ff jd dirichlet distribution commonly conjugate prior distribution multinomials 
intuitive dirichlet distributions stolcke omohundro 
sampling dirichlet distribution relatively straightforward 
performed drawing weights tj word class gamma distribution tj gamma ff 
set parameters jc normalized weights jc tj sj sampling gamma distribution hard method detailed press 

initialize parameters randomly generative model assumptions 
enables choose parameters fall reasonable range text space provide generous amount variation 
random initializations em find different local maxima hopefully give high probability high accuracy model 
dataset protocol remainder chapter subset newsgroups dataset 
subset news confusable comp classes comp graphics comp os 
misc comp sys ibm pc hardware comp sys mac hardware comp windows 
dataset contains documents nearly evenly divided classes 
data pre processed newsgroups described section 
briefly stoplist stem ignore usenet headers subject 
order provide comparable model probability numbers fix single vocabulary experiments 
top words mutual information number labeled documents em regular starting point em random restarts perfect class assignment em random restarts empirical class assignment naive bayes unlabeled data comparison different techniques choosing starting point 
best performance achieved single deterministic initialization labeled data 
probable model random initializations accuracy worse labeled training data sparse 
classes clusters matched perfectly results deterministic initialization 
measured entire labeled dataset 
chapter discusses issue feature selection learning labeled unlabeled data 
experiments random documents class total treated unlabeled 
fixed number labeled examples class randomly selected 
remaining documents test set 
experiment fixed number labeled examples perform random test train unlabeled splits 
splits paired conditions 
exact splittings compare algorithm 
performing multiple random restarts run em times split select run highest model probability 
random starting point chosen described section 
model highest probability selected chosen model measure classification accuracy test data 
experimental results shows accuracies achieved multiple random restarts 
moderate amount labeled data random initialization performs essentially accuracy deterministic initialization 
just training examples class accuracy random initialization significantly worse deterministic initialization 
example labeled documents class total random initialization finds model accuracy single deterministic initialization get model accuracy 
problem choosing starting points randomly randomness overcome signal labeled data remember labeled data choosing random initialization 
problem worst small amount labeled data baselines mixture component quite close 
random variation easily throw class component correspondence 
measuring strength class correspondence effect easy 
test data reassign classes components maximize classification accuracy test set 
way identify cases example data cluster comp graphics misc class represented mixture component initialized comp windows results analysis shown demonstrating upper bound performance random initialization 
accuracy random initializations optimal class assignment essentially deterministic initialization labeled set sizes 
example labeled documents class improve compared single initialization 
indicates random initializations dramatically improve performance labeled data sparse perfectly correlate clusters classes 
think results suggest belief model probability accuracy correlated 
multiple random initialization approach finding probable models deterministic approach 
chances find different local maxima 
wonder model probability accuracy strongly correlated 
analyzing results find case random initializations find probable models 
shows scatterplot indicating relationship model probability accuracy random deterministic initializations 
show log probability model em regular starting point em random restarts perfect class assignment scatterplot showing relationship model probability accuracy random initialization deterministic initialization labeled data 
cases distributed similarly reinforcing evidence model probability accuracy strongly correlated 
points test train unlabeled set indicating result deterministic starting point showing best random initializations 
findings interest 
see strong correlation accuracy model probability seen newsgroups dataset 
surprising news subset newsgroups shares properties 
importantly note random initializations approximately distribution deterministic initializations 
correlations accuracy model probability 
indicates relationship accuracy model probability holds outside strict subspace defined deterministic starting points labeled data 
discussion experiments shown fair bit random exploration find em initializations better just labeled data 
course infinite amount random exploration maxima discovered best identified 
lot local maxima difficult find ones labeled data pretty job getting regions reasonable local maxima 
labeled data sparse things get difficult 
previous section argued labeled data sparse significant room improvement improvement come finding better initialization parameters em 
section experimented finding initializations random exploration 
allocating orders magnitude time random exploration gains quality starting points 
suggests look improvements 
actively finding em initializations creating labeled data inherently involves human effort 
real world domains small amount labeled data expected 
typically documents selected randomly labeling available documents 
certainly reasonable learning algorithms perform better able select documents get labeled 
machine learning active learning setting exactly limited amount interaction human labeler 
setting learning algorithm selects examples labeler hand classification 
task learner select informative examples labeling learn classifier examples 
typically active learning algorithms learn labeled data 
active learning framework nature access unlabeled documents selection documents choose 
natural think applying active learning learning labeled unlabeled data examples selected labeling provide valuable information incorporated algorithms explicitly 
key idea section active learning algorithm carefully select unlabeled documents labeling 
informative labeled documents provide high quality initial values model parameters 
em find high accuracy parameters labeled documents remaining unlabeled documents 
expect approach better traditional method randomly selecting documents labeling 
query committee active learning active learning aims select informative example settings defined class label known maximally reduce error classifier trained extra example 
assumes learner unbiased reducing classification error equivalent reducing classification variance data distribution 
follows decomposition error bias variance geman 
cases expected variance reduction estimated empirically data iteratively selected labeling approach cohn 
frequently calculating expected variance reduction closed form prohibitively complex impractical best 
cases active learning proceed appealing query committee qbc framework freund 
selecting document maximally reduces classification variance qbc selects document labeling high classification variance 
consistent error free learning framework getting label document high classification variance eliminates hypotheses agree label 
cases qbc provides exponential speed ups learning rate random selection documents label freund 
learning task theoretically clean intuition qbc documents high classification variance lie regions learning algorithm needs help 
getting true label region significant uncertainty eliminated 
approach successfully applied real world text tasks part speech tagging hmm representation argamon engelson dagan text classification winnow perceptron learners liere 
query committee gets name measures classification variance example 
creating committee classifier variants suggested data labeled far 
qbc classifies unlabeled documents committee member measures disagreement classifications approximating classification variance 
qbc asks class label document committee disagrees strongly 
newly labeled document included training data new committee sampled making set requests 
probabilistic framework classification labeled training data specify posterior distribution classifiers 
selecting committee members ffl inputs collections labeled documents unlabeled documents 
ffl calculate density unlabeled document eq 

ffl loop person willing label loop times committee member create initial committee member sampling dirichlet distribution defined labeled training data equation 
initialization em combine labeled unlabeled data find table 
probabilistically label unlabeled documents eq 

calculate disagreement unlabeled document eq 
multiply density request class label highest score 
ffl run em combine labeled remaining unlabeled data creating classifier table 
ffl output classifier takes unlabeled document predicts class label 
table active learning algorithm finding em initializations 
step italics optional 
sampling posterior distribution classifiers 
approach successfully argamon engelson dagan 
probabilistic generative model approach applicable 
framework exists ad hoc approaches taken different random initializations liere 
qbc text classification section details apply qbc active learning finding initializations em learning labeled unlabeled data 
need specify things form committee members measure committee disagreement select document disagreement metric 
resulting algorithm summarized table 
form committee size approximate distribution classifiers indi cated labeled data different ways 
individual committee members denoted discussed section distribution classifiers induced labeled data set dirichlet distributions class mixture component 
committee member draw parameterization set 
approach committees initializations parameters committee member 
committee approximates distribution em initializations direct goal select document improve initialization 
second approach committees maxima draws initialization em incorporate unlabeled data change parameters 
em converges classifier committee member 
approximate parameter distribution corresponding local maxima try select document improve initialization sense puts initialization region high local maxima 
note techniques set labeled data unlabeled data 
previously section unlabeled data set wanted initializations capture information class unconditional word frequencies 
take approach want focus explicit weaknesses labeled data cover 
measuring committee disagreement dagan engelson suggest vote entropy entropy class label distribution resulting having committee member vote probability mass winning class 
disadvantage vote entropy consider confidence committee members classifications indicated class probabilities pm jd member 
improvement measure committee disagreement document jensen shannon divergence lin 
vote entropy compares committee members top ranked class jensen shannon divergence measures strength certainty disagreement calculating differences committee members class distributions pm cjd 
committee member produces posterior class distribution pm cjd random variable classes 
jensen shannon divergence average kullback leibler divergence naive bayes accurate probability estimator domingos pazzani naive bayes classification scores somewhat correlated confidence fact naive bayes scores successfully accuracy coverage trade offs craven slattery testament 
distribution mean distributions pm cjd jjp avg cjd avg cjd class distribution mean committee members avg cjd pm cjd kullback leibler divergence delta information theoretic measure difference distributions capturing number extra bits information required send messages sampled distribution code optimal second 
kl divergence distributions jjp jcj log disagreement calculated metric document selected class label request 
consider ways selecting documents stream pool density weighted pool 
previous applications qbc dagan engelson liere tadepalli simulated stream unlabeled documents 
time document considered measuring classification disagreement committee members deciding disagreement select document labeling 
dagan engelson heuristically dividing vote entropy maximum possible entropy create probability selecting document 
disadvantages stream sampling sparsely samples full distribution possible document labeling requests decision label document individually irrespective alternatives 
alternative aims address problems pool sampling 
selects unlabeled documents largest disagreement 
loses benefit stream sampling implicit modeling data distribution may select documents high disagreement unimportant sparsely populated regions 
retain distributional information selecting documents classification disagreement density region document 
density weighted pool sampling method prefers documents high classification variance similar documents 
stream approach approximates implicitly accomplish accurately especially labeling small number documents modeling density explicitly 
approximate density region particular document measuring average distance document documents 
distance individual documents measured exponentiated kl divergence gammafi jd jj jd gamma random variable words vocabulary jd maximum likelihood estimate words sampled document jd jd marginal distribution words parameter determines smoothing encoding distribution ensure zeroes prevent infinite distances fi parameter determines sharpness distance metric 
essence average kl divergence document documents measures degree overlap documents exponentiation converts information theoretic number bits information scalar distance 
calculating average distance documents computationally efficient calculate geometric mean arithmetic mean distance documents share words words calculated advance need corrections words appear geometric mean define density document jdj ln combine density metric disagreement selecting labeling document largest product density equation disagreement equation 
density weighted pool sampling selects document representative documents confident committee disagreement 
algorithmic choices qbc dimensions 
form committees initializations committees maxima 
select documents simulating stream picking straight unlabeled pool density weighting picking pool 
sections experimentally evaluate choices empirically show active learning finds initializations em give higher accuracy classifiers 
dataset protocol news data described section 
experimental trial documents randomly selected placement test set 
initially randomly selected document class labeled remaining documents unlabeled 
learning proceeds described table 
experiments run active learning iterations round selecting document labeling 
smoothing parameter sharpness parameter fi 
little effort tune fi tune qbc committee size initial experiments showed committee size little effect 
em runs perform em iterations classification accuracy improve seventh iteration 
results averages runs condition 
experimental results evaluating selection strategies qbc 
compare stream sampling pool sampling density weighted pool sampling 
calculate baseline random selection documents 
techniques draw committee members directly dirichlet run em get associated local maxima 
shows quality em initialization cases measured classification accuracy 
best selection technique density weighted pool sampling achieves accuracy acquiring labeled documents 
reach accuracy unweighted pool sampling needs labeled documents 
switch sampling need labelings accuracy 
random selection baseline requires labeled documents 
density weighted pool sampling statistically significantly better methods pairing 
interesting note documents selected approach usually faqs various newsgroups 
clearly effect density weighting content faq representative class documents 
incorporating density weighting biases selection longer documents number training documents density weighted pool sampling pool sampling stream sampling random sampling comparison selection strategies qbc shows density weighted pool sampling gives higher accuracy em initializations strategies 
note order legend matches order curves resolution vertical axes range 
documents word distributions representative corpus 
generally better label long short documents labeling effort long document provides information words 
compare committee creation techniques 
pool sampling provided best initializations 
shows accuracy initializations accuracy final classifier em compared random selection baseline 
starting labeling mark committees initializations reaches accuracy em incorporates unlabeled documents 
committees local maxima lag slightly requiring labeled documents accuracy 
random selection needs labeled documents 
committee selection techniques statistically significantly different statistically significantly better random selection threshold 
interestingly committee selection methods perform roughly equally em incorporates unlabeled documents committees initializations provide accurate starting points em committees local maxima 
understand remembering committees focus attentions differently 
committees initializations consider specifically accuracy em accuracy number training documents post em committees initializations post em committees local maxima post em random selection pre em committees initializations pre em committees local maxima pre em random selection performance active learning select em initialization 
committee formation techniques perform 
better random selection 
note order legend matches order curves resolution vertical axes range 
starting point 
indirectly improves final classification quality em starting point correlated accuracy final classifier 
committees local maxima focus final accuracy initializations 
surprising technique initialization accuracy lower 
interesting technique recovers loss em 
discussion studies qbc probabilistic classifiers similar way studies active learning improve text classification 
survey active learning section 
previous active learning combination algorithms unlabeled data 
comparison previous active learning studies text classification domains lewis gale liere tadepalli magnitude classification accuracy increase relatively modest 
previous studies consider binary classifiers skewed distributions positive class rare 
infrequent positive class random selection perform extremely poorly nearly documents selected labeling negative class 
tasks class distributions random selection perform better making improvement active learning dramatic 
separate mccallum nigam experiment selection technique reuters domain skewed prior distributions show larger improvements 
conclude accuracy improvements class priors random selection provides relatively strong performance baseline 
results section indicate actively selecting labeled examples increase accuracy em initial parameterization increase accuracy final classifier 
active learning especially important small number examples labeled performance random selection weakest 
avoiding local maxima deterministic annealing previous sections chapter taken approach finding better em initializations 
modest success improving unlabeled data finding accurate classifiers 
local maxima significant problem em especially labeled training data sparse 
time seek alternatives em finding highly probable models 
section different maximization technique robust local maxima deterministic annealing 
typically variants alternatives em created purpose speeding rate convergence mclachlan krishnan chapter 
domain text classification seen convergence fast 
easily consider alternatives em improve local maxima situation expense slower convergence 
deterministic annealing exactly tradeoff 
intuition deterministic annealing begins maximizing smooth convex surface remotely related true probability surface interest 
initially find global maximum simple surface 
slowly change surface bumpy close true probability surface 
follow original maximum surface ffl inputs collections labeled documents unlabeled documents 
ffl initialize fi small number near zero 
ffl build initial naive bayes classifier unlabeled documents spread evenly classes labeled documents maximum posteriori parameter estimation find arg max dj see equations 
ffl loop fi ffl loop convergence ffl step current classifier calculate expected value cluster membership unlabeled document ij see equation 
ffl step re estimate clustering parameters expected cluster membership document 
maximum posteriori parameter estimation find arg max dj see equations 
ffl increase fi 
ffl output classifier takes unlabeled document predicts class label 
table deterministic annealing algorithm described section 
gets complex original surface ll highly probable maximum 
way avoids local maxima em get caught 
deterministic annealing section sketch derivation deterministic annealing specifically applies learning labeled unlabeled data text classification 
general treatment deterministic annealing rose 

mentioned notation introduced chapter 
deterministic annealing algorithm summarized table 
approach problem learning classifier labeled unlabeled data semi supervised clustering 
cluster assignments labeled data unknown unlabeled data 
cluster parameterized multinomial distribution 
semi supervised clustering complete multinomial distribution correspond class naive bayes classifier 
represent clustering assignments matrix binary indicator variables hz ij iff ij non zero entry gives cluster membership datapoint 
treat semi supervised clustering optimization problem 
loss function optimization function model parameters class assignments zjd gamma ij log jc loss function negative complete data log probability section mixture multinomials model deterministic class labels 
seen thesis minimizing loss function target building classifier 
loss function motivated model probability explicitly assume data generated target parameterization 
think parameters classifier model data generation 
finding model fixed loss way finding model class assignments minimize loss solve related task 
task find select clustering specific loss value 
subsection give algorithm minimizing loss uses step sub component 
fixed value loss function set pairs models cluster assignments loss 
target loss asked select know pairs choose 
answer question apply principle maximum entropy jaynes csisz ar 
principle says absence knowledge estimated probability distribution uniform possible maximal entropy 
principle commonly successfully applied learning tasks 
find maxent distribution model parameters data select model parameters solution 
target loss goal find parameters maxent distribution parameters assignments give target loss 
target distribution model parameters cluster assignments maximizes entropy gamma log subject constraints gamma ij log jc constraint enforces requirement target distribution valid probability distribution 
second constraint enforces fixed value loss model parameters cluster labels 
solving constrained maximization problem lagrange multipliers find solution maxent problem gibbs distribution zjd exp fi ij log jc exp fi ij log jc fi lagrange multiplier determined value desired loss 
fi range desired loss large desired loss small 
form joint likelihood models cluster assignments select single classification model 
remember cluster assignments unlabeled data incidental purposes finding classifier 
express probability classifier parameterization looking ahead want incorporate priors parameters avoid word probabilities zero 
easily done replacing maximum entropy criteria minimum relative entropy prior distribution 
generalization derivation minimum relative entropy straightforward notationally complex 
simplicity maxent derivation give minimum relative entropy algorithm 
independent labelings unlabeled data marginalizing equation jd zjd substituting simplifying logarithm get expression log likelihood classification model jd log jc fi log jy fi note exception fi equation equation incomplete model probability generative assumptions identical 
fi identical 
computationally intractable find model log sums unlabeled data equation 
analogously section apply em framework find local maximum likelihood solution iterating steps ffl step calculate expected value class assignments ij ij jc fi cr jc fi ffl step find model expected class assignments arg max jd step identical section step includes loss constraint fi 
note pedagogical distinction step section em process 
calculating expectation class labels respect assumed generative distribution documents 
calculate expectation class labels respect maximum entropy distribution class labelings classifiers target loss 
finding low loss model find local maximum likelihood posteriori model fixed loss find model minimum loss 
consider log probability models equation affected different target losses 
target loss large fi close zero probability model nearly prior probability influence data negligible 
limit fi goes zero probability surface convex single global maximum 
somewhat smaller loss target fi small negligible 
probability data stronger influence 
longer single global maximum 
fi familiar probability surface previous chapters local maxima 
observations suggest annealing process finding low loss model 
temperature algorithm inverse fi 
initialize temperature high easily find global maximum posteriori solution em surface convex 
lower temperature probability surface get slightly bumpy complex data likelihood larger impact probability model 
complex new maximum close old maximum lowered temperature slightly 
searching maximum em initialize old maximum converge maximum new probability surface 
way gradually lower temperature system tracking highly probable solution 
eventually temperature fi local maximum probability model maximum entropy minimum relative entropy fixed loss assumptions 
conveniently local maximum generative model assumptions probability surfaces identical point 
high probability local maximum labeled unlabeled data classification 
note computational cost deterministic annealing significantly higher em 
iteration takes computation iterations deterministic annealing temperature reduced slowly 
example experiments performed iterations deterministic annealing em 
extra computation afforded benefit accurate classifiers 
number labeled documents deterministic annealing perfect class assignment deterministic annealing empirical class re assignment deterministic annealing default class assignment em unlabeled data naive bayes unlabeled data performance deterministic annealing compared em 
class component assignment done perfectly deterministic annealing considerably accurate em labeled data sparse 
default correspondence poor corrected small amount domain knowledge 
experimental results section see empirically deterministic annealing finds probable parameters accurate classifiers em labeled training data sparse 
experimental results news dataset 
setup protocol described section 
running deterministic annealing initialize fi iteration increase fi multiplicative factor fi 
little effort tune parameters 
deterministic annealing proceeds table 
time increase fi probability surface changes slightly run iteration em temperature setting 
compares classification accuracy achieved deterministic annealing achieved regular em 
initial results indicate methods perform essentially labeled data plentiful deterministic annealing performs worse labeled data sparse 
example labeled examples class total em gives accuracy deterministic annealing gives 
close investigation confusion matrices shows significant detrimental effect incorrect class component correspondence deterministic annealing labeled data sparse 
accuracy log probability model em regular starting point deterministic annealing scatterplot comparing model probabilities accuracies em deterministic annealing 
results show deterministic annealing succeeds finds models significantly higher probability 
sense 
temperature high global maximum multinomial mixture component close prior 
priors mixture component essentially identical 
temperature lowers mixture components distinct component easily track cluster associated wrong class 
attempt remedy alter class cluster correspondence classification labeled example deterministic annealing complete 
word counts example subtracted final classifier effect example cluster 
shows accuracy obtained empirically selected correspondence optimal accuracy achieved perfect correspondence 
see empirically setting correspondence deterministic annealing improves marginally 
got changing correspondence increase better em 
perform perfect class correspondence accuracy deterministic annealing considerably higher em 
verify higher accuracy deterministic annealing comes finding probable models shows scatterplot model probability versus accuracy deterministic annealing optimal class assignment em 
results note stand 
deterministic annealing finds comp graphics jpeg image graphics images gif format pub ray tiff siggraph comp os ms windows misc windows ei win um dos ms ini microsoft nt el comp sys ibm pc hardware scsi ide drive controller bus dx bios drives mb card comp sys mac hardware apple mac lc fpu comp windows window widget motif xterm server lib entry usr sun table top words class news dataset 
words sorted weighted log likelihood ratio 
note just top words person domain knowledge correctly correspond clusters classes 
probable models small amount labeled data 
accounts added accuracy deterministic annealing 
second note interest models deterministic annealing lie probability accuracy correlation line 
provides evidence model probability accuracy strongly correlated dataset correlation just artifact em 
discussion experimental results show deterministic annealing help classification considerably class component correspondence solved problem 
deterministic annealing successfully avoids getting trapped poor local max ima finds probable models 
high probability models correlated high accuracy classifiers deterministic annealing unlabeled data text classification 
class correspondence problem severe limited labeled data 
fewer labeled examples small perturbations lead correspondence astray 
just little bit human knowledge class correspondence problem typically solved trivially 
largest confusing classification tasks straightforward identify class indicative words measured metric weighted log likelihood ratio equation 
example top words class dataset metric shown table 
just words person slightest bit domain knowledge problem perfectly assigning classes components 
unreasonable require just small amount human effort correct class correspondence deterministic annealing finished 
labeled training data sparsest deterministic annealing successfully find probable accurate models traditional em 
limited domain knowledge available possible class correspondence automatically 
perform em deterministic annealing data 
em solutions generally correct class correspondence model fix correspondence deterministic annealing model 
measure distance em class multinomial deterministic annealing class multinomial kl divergence example 
matrix distances assign class labels em multinomials closest match multinomial deterministic annealing model 
possible way explicit deterministic annealing insensitivity labeled data 
deterministic annealing insensitive initialization influence labeled data minimal perform deterministic annealing unlabeled data 
way residual effects labeled data performing cluster correspondence 
exactly recommendation castelli cover 
deterministic annealing introduced rose 
way construct hierarchy unsupervised clustering 
motivated strong analogy statistical physics 
related deterministic annealing applications estimate parameters mixture gaussians unlabeled data ueda nakano constructing text hierarchy unlabeled data hofmann puzicha 
chapter addressed techniques improving unlabeled data labeled data sparse 
conditions provide best opportunity benefit unlabeled data pose significant challenges 
specifically process integrating unlabeled data generative models suffers poor initialization em optimization 
improved unlabeled data different ways 
learning algorithm interact human limited labeling effort carefully select documents get labeled 
helps finding higher quality initializations em lead accurate classifiers 
tempered variation em 
deterministic annealing avoids local maxima finds probable accurate classifiers 
different techniques better unlabeled data labeled data sparse 
chapter related field text classification rich existing ongoing scientific research 
related theoretical empirical approaches incorporating unlabeled data supervised learning provide strong foundation thesis 
chapter surveys current state fields intersection 
text classification text classification different forms time 
early application text classification author identification 
seminal mosteller wallace examined authorship different federalist papers bayesian analysis features word sentence length frequency function words vocabulary diversity 
text classification applied wide variety practical applications cataloging news articles lewis gale joachims classifying web pages symbolic ontology craven finding person homepage shavlik rad automatically learning reading interests users pazzani lang automatically threading filtering email content lewis knowles sahami book recommendation mooney roy 
early popular machine learning technique text classification naive bayes lewis mitchell 
straightforward probabilistic nature amenable variety extensions 
limited word dependencies modeled tan trees sahami 
leverage class hierarchy provided statistical shrinkage mccallum ad hoc techniques koller sahami 
class component correspondence relaxed li yamanishi 
thesis extended naive bayes inclusion unlabeled data 
different generative models naive bayes 
thesis multinomial language modeling terms unigram model classifier mixture multinomials tracks number times word appears document mccallum nigam 
formulation numerous practitioners naive bayes text classification lewis gale joachims li yamanishi mitchell mccallum lewis 
second formulation naive bayes text classification uses generative model word vocabulary binary feature modeled mixture multi variate robertson sparck jones lewis larkey croft koller sahami 
empirical comparisons show multinomial formulation yields classifiers consistently higher accuracy mccallum nigam 
variety machine learning techniques naive bayes applied text classification 
support vector machines shown promise joachims dumais 
approaches maximum entropy nigam neural nets wiener shavlik rad rule learning algorithms apte cohen singer moulinier craven 
memory methods nearest neighbor yang chute cohen hirsch variety boosting approaches schapire singer apte sebastiani 
date single technique emerged clearly better evidence suggests knn svms perform algorithms lot labeled data class interest yang 
studies text classification simple document representation bags words tracking number times word occurs document just occurred 
consistently efforts include substantial linguistic semantic information provided modest improvements classification accuracy 
furnkranz 
uses shallow syntactic phrase patterns finds improvements naive bayes rule learning algorithms 
mladenic selects variable length phrases text classification web pages yahoo hierarchy 
studies incorporated text classification information wordnet semantic network english language rodriguez scott matwin 
learning labeled unlabeled data turn survey combining labeled unlabeled data 
initial area started statistics joined machine learning community 
likelihood maximization approaches idea learning classifiers combination labeled unlabeled data old statistics community 
early suggested labeled unlabeled combined building classifiers likelihood maximization testing possible class assignments hartley rao 
seminal day presents iterative em approach parameters mixture normals known covariances unlabeled data 
similar iterative algorithms building maximum likelihood classifiers labeled unlabeled data followed primarily mixtures normal distributions mclachlan titterington 
dempster 
theory em framework bringing formalizing commonalities previously suggested iterative techniques likelihood maximization missing data 
applicability estimating maximum likelihood maximum posteriori parameters mixture models labeled unlabeled data murray titterington classification little recognized immediately 
approach continues studied mclachlan shahshahani landgrebe 
excellent surveys history em application mixture modeling books mclachlan basford mclachlan krishnan mclachlan peel 
likelihood maximization mixture models combining labeled unlabeled data classification way machine learning community miller uyar nigam baluja 
approach earlier similar purposes 
mixture models generative model unsupervised clustering parameters fit em cheeseman cheeseman stutz hofmann puzicha 
ghahramani jordan em fill missing feature values examples learning incomplete data assuming mixture model 
hierarchical mixtures experts similar mixture models parameters typically set em jordan jacobs 
dissertation explore combining labeled unlabeled data text classification 
discriminative approaches transductive support vector machine vapnik discriminatively finds parameters linear separator labeled data data tested 
general approach equally applicable scenarios labeled unlabeled data 
high level finding linear separator labeled examples class maximizes margin labeled unlabeled examples 
joachims demonstrates efficacy approach text classification tasks 
bennett demiriz find small improvements uci datasets computationally easier variant transduction 
intuition transductive svms assume decision boundaries lie classes low density regions instance space unlabeled examples help find areas 
zhang oles argue theoretically experimentally transductive svms helpful classification general 
maximum entropy discrimination framework jaakkola margin classification approach unlabeled data 
finds maximum entropy minimum relative entropy distribution classifier parameters subject soft constraints labeled example fixed margin decision boundary 
classification performed calculating integration expected value example class induced parameter distribution 
unlabeled data adding distribution unknown class labels jointly estimated maximum entropy 
approach margin constraints added unlabeled data encourage estimation commit labels unlabeled examples 
distribu tion estimation practical iterative relaxation algorithm proposed converges local minima relative entropy 
encouraging experimental results combining labeled unlabeled data context predicting dna splice points 
szummer jaakkola uses unlabeled data classification kernel expansion 
essence features labeled datapoint include kernel densities examples labeled unlabeled 
features maximum entropy discrimination framework maximum likelihood derives linear separator classification task 
intuition unlabeled data weight relative importance labeled data instance density 
weighting informs discriminative training examples important 
approach essential carefully select form width kernel accurately model underlying instance distribution 
best done open challenging question 
theoretical value unlabeled data effort quantify relative value labeled unlabeled examples learning mixture distribution classification 
unsurprisingly results literature concern mixtures gaussians 
mclachlan calculate order approximation asymptotic relative efficiency labeled unlabeled examples classification univariate normals known equal variances 
neill goes step quantifies asymptotic relative efficiency multivariate normals equivalent known covariance matrices 
venkatesh perform similar analysis pac framework 
general cases mixtures gaussians known 
class mixture distributions including normals chen bounds rate convergence parameter estimates unlabeled examples number mixture components bounded known 
results cover parameter estimation classification error 
mentioned results assume global maximum likelihood parameterization problems local maxima data generated model 
general challenging cases applicable thesis known results 
cited result castelli cover show labeled examples exponentially valuable unlabeled examples 
results apply estimating class probability parameters underlying component distributions known correct 
related result show labeled data reduce error exponentially fast infinite amount unlabeled data component distributions known class component correspondence castelli cover 
study zhang oles examine value unlabeled data discriminative trainers transductive svms active learning 
mentioned question generality helpfulness transductive svms 
active learning explicitly stated selective sampling way integrating unlabeled data supervised learning 
selective sampling cohn form active learning select existing example labeling 
approach selecting examples try maximally reduce variance component classification error cohn 
approach taken thesis query committee seung freund committee classifier variants select example high classification variance 
theoretical analysis shows consistent error free learning domain qbc exponentially reduce number labeled examples needed learning freund 
qbc shown powerful paradigm practice 
argamon engelson dagan query committee approach learn part speech tagger 
similarly section statistical models classifiers create committee 
contrast stream sampling vote entropy measure committee disagreement 
text classification shown pool sampling jensen shannon divergence metric perform better 
liere uses committees winnow perceptron learners qbc active learning text classification 
document selected labeling randomly selected committee members disagree class label 
committee members formed multiple random initializations classifiers 
studies investigated active learning selective sampling specifically domain text categorization 
lewis gale examine uncertainty sampling relevance sampling pool setting lewis gale lewis :10.1.1.16.3103
techniques select documents single classifier committee approximate classification variance 
uncertainty sampling selects labeling example classified uncertainty uniform class posteriors relevance sampling chooses example certainty peaked class posteriors 
schohn cohn approach similar uncertainty sampling support vector machines 
select labeling example closest linear decision boundary classifier 
tong koller perform selective sampling support vector machines approach motivated trying maximally reducing size version space hypotheses 
studies shown significant benefit active learning applied text classification domains 
uses unlabeled data supervised learning training setting allows unlabeled data new ways 
specifies example described disjoint views data 
example web classification task instance words occurring web page words hyperlinks pointing web page 
blum mitchell show certain theoretical assumptions weak learner arbitrarily improved sufficient unlabeled examples 
training algorithm iteratively selects unlabeled example gives label 
nigam ghani argue training algorithm variants succeed part robust assumptions underlying classifier representations 
collins singer boosting algorithm learning training setting tries minimize disagreement unlabeled data classifiers different views data 
goldman zhou show training approaches succeed datasets disjoint views carefully selected underlying classifiers 
developments distantly labeled data proven useful 
distantly labeled data data labeled related task direct map task hand 
autoslog ts system riloff takes documents labeled domain interest automatically extracts indicative case frames match items extracted 
seymore 
distantly labeled data estimate output parameters hmm information extraction research domain 
similar vein hirsh unlabeled data background knowledge augment nearest neighbor classifier 
matching test example directly closest labeled example match test example labeled example measuring similarity common set unlabeled examples 
bootstrapping techniques allow learning algorithms nearly labeled data iteratively develop concept interest 
riloff jones start just small dictionary known locations set unlabeled data bootstrap larger dictionary locations case frame patterns indicative locations 
yarowsky bootstraps word sense disambiguation algorithm starting small set seed collocations words senses 
unlabeled data reduce prevent overfitting 
example strong evidence overfitting disagreement unlabeled data candidate classifiers regressors larger sum errors labeled data 
observation selecting best complexity polynomial regression schuurmans pruning decision trees schuurmans 
ismail unlabeled test data reduce overfitting linear regression augmenting minimization criteria mean squared error terms unlabeled data 
chapter dissertation addressed problem integrating unlabeled data supervised learning text classification 
labeled data expensive collect human take time effort label 
frequently case labeled training data sparse 
contrast unlabeled data inexpensive plentiful 
especially true text classification tasks type text readily available electronic form 
findings significant findings dissertation unlabeled data useful text classification 
taken approach specifying simple statistical generative model class documents choosing parameters model highly probable evidence labeled unlabeled documents 
basic model document distribution mixture model mixture component generates documents class multinomial distribution words 
labeled data familiar naive bayes text classifier maximum posteriori parameters closed form equations 
access unlabeled data find local maximum parameters data expectation maximization em technique 
parameters model turned bayes rule classification 
domains strong correspondence model probability classification accuracy finding probable parameters yields accurate classifiers 
experiments detailed section labeled unlabeled data build text classifiers newsgroups collection usenet articles 
dataset show classification accuracy model probability strongly correlated unlabeled data reduces classification error labeled training data sparse 
conclude integrating unlabeled data supervised learning reduce error text classifiers 
initially may surprising generative model approach em increase text classification accuracy unlabeled data 
intricacies text documents captured mixture multinomials model 
interesting finding model approximation maximizing probability model provides increased accuracy classifiers 
model captures information documents purposes classification 
modeling sub topic structure models representative 
datasets adequately modeled basic generative process 
cases unlabeled data parameter estimation increase classification error finding parameters poorly match true data distribution 
class complex sub topics single multinomial mixture component insufficient representative model required 
section multiple mixture components class reuters dataset model single class containing news stories topics 
unlabeled data basic model increases classification error 
generative model multiple mixture components class representative dataset bring model probability classification accuracy correlation classification error decreases topics compared labeled data 
sufficiently representative model maximizing model posterior probability finds accurate classifiers labeled unlabeled data 
labeled data researchers shown generative models allow class conditional word dependencies yield accurate classifiers 
unlabeled data principles model representation arise strongly 
assume model correctness labeling unlabeled data parameter estimation assumption strongly violated generated class labels reflect biases model provide useful accurate model parameters 
cases necessary adjust bias assumed parametric form model closely matches true document distribution 
modeling super topic hierarchical class relationships reduces overfitting 
classification domains hierarchical relationships classes 
basic generative process ignores relationships models class independent mixture component 
parameters different classes closely related re estimated class separately 
increases opportunity overfitting unlabeled data 
explicitly modeling hierarchical class relationships efficient unlabeled data estimating shared parameters data 
approach reduces overfitting making model probability unlabeled data closely match model probability true document distribution 
cora dataset computer science hierarchy leveraged just way 
section show hierarchy unlabeled data reduce classification error 
representative hierarchical model efficient data error reduced naive bayes baseline relative human performance 
conclude modeling known hierarchical relations generative process efficiently unlabeled data produce accurate text classifiers 
active learning creates improved em initializations 
algorithm incorporating unlabeled data supervised learning labeled unlabeled data play different roles 
primary effect labeled data initialize maximization process unlabeled data significant effect em iterations 
quality initialization typically order determinant accuracy resulting classifier 
provides opportunity unlabeled data improve text classification accuracy 
limited interaction human labeler query committee active learning select unlabeled documents get labeled provide high quality initializations em 
section apply approach news dataset 
initializing em actively selected documents reduce number labelings needed reach accuracy threshold classifier 
conclude actively selecting unlabeled documents labeling reduces bottleneck imposed labeled data 
deterministic annealing finds accurate text classifiers em 
reason em initialization strongly influences classification accuracy em gets trapped local maxima probability space 
em approach performing maximization 
alternative technique deterministic annealing 
robust local maxima maximizes smooth surface gradually probability surface bumpy maximizing way maximum original probability surface 
cooling process deterministic annealing able track high probability maximum settle high accuracy classifier 
section apply technique news dataset 
deterministic annealing consistently finds probable solutions em especially labeled data sparse initializations poor 
deterministic annealing prone errors class component correspondence easily corrected minimum human effort 
probable parameters deterministic annealing gives classifiers reduce classification error em 
conclude deterministic annealing finds probable models accurate text classifiers em effectively avoiding local maxima 
summary basic approach unlabeled data classification maximizing probability statistical generative models 
approach works model representative classification accuracy correlated model probability 
basic model representative accurate model successfully 
representative model unlabeled data help labeled data bottleneck better performance 
issue addressed actively selecting labeled data maximization techniques sensitive labeled data 
directions thesis suggests directions research deserving attention 
outline 
generative model approach approaches incorporating unlabeled data supervised learning take generative model approach joachims szummer jaakkola jaakkola haussler 
take discriminative approach focus directly estimating decision boundary classes 
interesting important area understand types domains amenable approach 
extreme example data really generated statistical model posterior model maximization approach better bias algorithm exactly correct allow efficient data 
know real world text dataset completely captured statistical model 
sufficiently representative model data discriminative approach appropriate 
discriminative approaches assumptions subject violations 
example transductive svms joachims assume low density region linear separator passes 
domains need case generative approach suited 
exciting understand assumptions approach violated 
ideally test unlabeled limited labeled data predict approach beneficial 
example number statistical goodness fit tests multinomial mixture multinomials distributions chen cressie read 
probability parameterization proved poor fit mixture multinomials distribution may suggest discriminative approach better suited domain 
hypothesize cases fit model model representative beneficial generative approaches 
existing tests may sufficient 
better approach directly compare relative accuracies document distributions measured mixture multinomials non parametric kernel density estimator 
non parametric estimator better predictor previously unseen data kernel discriminative approaches szummer jaakkola may perform best 
possibility explicitly examine correlation probability generative model classification accuracy 
modest extremely small set labeled examples accuracy measured different parameter settings totally unsupervised clustering 
strength correlation may predictor appropriateness generative model 
example section performed runs em random initializations 
data displayed correlation model probability accuracy high indicating appropriateness approach dataset 
feature selection multiple classification tasks taken extreme generative model approach suggests single classification task domain 
example just global maximum parameterization mixture multinomials news data set parameterization goal 
documents typically authored intent fitting arbitrary categories 
additionally imagine different classification tasks domain 
example addition task newsgroup source interested classifying data nationality author relevance computer science researchers relevance system administrators 
reconcile contradiction generative models multiple classification tasks data 
way reconcile feature selection 
different sets features domain generative model dramatically different posteriori parameterizations 
feature selection essential step building text classifier labeled data yang pedersen 
limited labeled data difficult select appropriate set features considering words occurred labeled data 
hypothesize new interesting set feature selection algorithms appropriate application combinations labeled unlabeled data 
promising direction iterative feature selectors style improved iterative scaling della pietra boosting schapire singer 
model maximization feature selection interleaved allowing influence unlabeled data maintaining direction labeled data 
may provide robustness overfitting local maxima way deterministic annealing 
appendix complete hierarchy keywords appendix presents entire hierarchy key words phrases cora experiments section 
ffl information retrieval ffl retrieval information retrieval ffl extraction information extraction wrapper induction wrapper www ffl filtering text classification document classification document categorization document filtering ffl digital library digital library ffl encryption compression ffl encryption encryption cryptographic cryptology cryptanalysis cryptography mutual distrust decentralized authority secure secret ffl security computer security security hole security holes security attack ssl network security firewall kerberos ffl compression compression entropy code video audio audio coding video coding mpeg ffl human computer interaction ffl multimedia multimedia ffl interface design gui interface design ffl graphics virtual reality virtual reality telepresence computer graphics siggraph ffl cooperative cscw ffl wearable computers wearable computers wearable computer ffl artificial intelligence ffl expert systems expert systems ffl knowledge representation knowledge representation ffl machine learning ffl reinforcement learning reinforcement learning temporal difference learning ffl genetic algorithms genetic algorithms genetic algorithm genetic programming natural selection evolutionary evolutionary computation ffl probabilistic methods bayes rule density estimation bayesian network bayes network bayesian networks bayes networks belief revision uncertain inference probabilistic reasoning uai ffl neural networks neural network neural networks gradient descent projection pursuit self organizing map ijcnn ffl theory pac colt warmuth freund schapire ffl rule learning rule learning ilp ffl case case instance memory ffl robotics robotics robot robots ffl vision pattern recognition pattern recognition pami computer vision ffl speech speech recognition ffl planning planning temporal reasoning reasoning time ffl nlp nlp natural language processing ffl theorem proving theorem proving ffl agents agent agents multiagent artificial life ffl games search game search combinatorial optimization stochastic optimization ffl data mining data mining database mining ffl databases ffl temporal database temporal databases temporal ffl deductive deductive database deductive databases ffl concurrency concurrency database ffl query evaluation query evaluation database database queries ffl object oriented oodbms object oriented database ffl relational relational database ffl performance database real time databases real time database parallel databases parallel database scalable ffl programming ffl garbage collection garbage collection ffl semantics programming language semantics denotational semantics programming language ffl compiler design compiler design compiler algorithm parallelizing compiler compiler language optimized code compiling programs ffl debugging debugging ffl java java ffl object oriented object oriented programming smalltalk ffl functional functional programming lisp ffl logic logic programming logic programming prolog prolog ffl software development software engineering design tools software metrics programming environments computer aided engineering software reuse software portability configuration management ffl operating systems ffl memory management memory management shared memory ffl distributed distributed system distributed computing distributed computing distributed os distributed system distributed systems distributed network environment distributed operating systems distributed operating system distributed file system distributed file systems network file system mobile computing networks workstations cluster workstations distributed storage system ffl realtime realtime real time rtss ffl fault tolerance fault tolerance tolerate faults ffl networking ffl protocols network protocols communication protocol communication protocols multicast mbone atm tcp ip udp ffl routing routing qos switched networks switched network routing networks ffl internet internet internet architecture web caching ffl wireless wireless network wireless networking mobile network ffl hardware architecture ffl high performance computing high performance computing ffl vlsi vlsi ffl memory structures memory structures tlb numa hardware ffl distributed architectures distributed architectures distributed architecture distributed hardware ffl microprogramming microprogramming microprocessor controller ffl logic design circuit circuits cmos logic design gate level multi level circuit analog circuits analog systems analog converters gate circuit gate circuits circuit transistor circuit fan ffl input output storage disk drive scsi ffl data structures algorithms theory ffl randomized randomized algorithms ffl parallel parallel algorithms ffl formal languages finite state machine method context free languages formal model automata theory automata ffl computational complexity space complete complexity deciding complexity class bounds complexity ffl sorting sorting ffl hashing hashing ffl computational geometry computational geometry ffl logic finite model theory program verification format analysis verification modal logic ffl quantum computing quantum bibliography apte damerau weiss 

automated learning decision rules text categorization 
acm transactions information systems 
apte damerau weiss 

text mining decision trees decision rules 
workshop learning text web conference automated learning discovery 
argamon engelson dagan 

committee sample selection probabilistic classifiers 
journal artificial intelligence research 
baluja 

probabilistic modeling face orientation discrimination learning labeled unlabeled examples 
advances neural information processing systems pp 

bennett demiriz 

semi supervised support vector machines 
advances neural information processing systems pp 

blum mitchell 

combining labeled unlabeled data cotraining 
proceedings th annual conference computational learning theory pp 

boyan 

learning evaluation functions global optimization 
doctoral dissertation computer science department carnegie mellon university 
carlin louis 

bayes empirical bayes methods data analysis 
chapman hall 
castelli cover 

exponential value labeled samples 
pattern recognition letters 
castelli cover 

relative value labeled unlabeled samples pattern recognition unknown mixing parameter 
ieee transactions information theory 
ismail 

incorporating test inputs learning 
advances neural information processing systems pp 

cheeseman kelley self stutz taylor freeman 

autoclass bayesian classification system 
machine learning proceedings fifth international conference pp 

cheeseman stutz 

bayesian classification autoclass theory results 
fayyad piatetsky shapiro smyth uthurusamy eds advances knowledge discovery data mining 
mit press 
chen 

optimal rate convergence finite mixture models 
annals statistics 
chen 

penalized likelihood ratio test finite mixture models multinomial observations 
canadian journal statistics 
cohen hirsch 

joins generalize text categorization whirl 
proceedings fourth international conference knowledge discovery data mining pp 

cohen singer 

context sensitive learning methods text categorization 
sigir proceedings nineteenth annual international acm sigir conference research development information retrieval pp 

cohn atlas ladner 

improving generalization active learning 
machine learning 
cohn ghahramani jordan 

active learning statistical models 
journal artificial intelligence research 
collins singer 

unsupervised models named entity classification 
proceedings joint sigdat conference empirical methods natural language processing large corpora 
cover thomas 

elements information theory 
new york john wiley sons 
craven dipasquo freitag mccallum mitchell nigam slattery 

learning construct knowledge bases world wide web 
artificial intelligence 
craven slattery 

relational learning statistical predicate invention better models hypertext 
machine learning 
craven slattery nigam 

order learning web mining 
proceedings th european conference machine learning pp 

cressie read 

multinomial goodness fit tests 
journal royal statistical society series 
csisz ar 

maxent mathematics information theory 
hanson silver eds maximum entropy bayesian methods 
kluwer academic publishers 
dagan engelson 

committee sampling training probabilistic classifiers 
machine learning proceedings twelfth international conference pp 

day 

estimating components mixture normal distributions 
biometrika 
della pietra della pietra lafferty 

inducing features random fields 
ieee transactions pattern analysis machine intelligence 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
dietterich 

approximate statistical tests comparing supervised classification learning algorithms 
neural computation 
domingos pazzani 

optimality simple bayesian classifier zero loss 
machine learning 
dumais platt heckerman sahami 

inductive learning algorithms representations text categorization 
proceedings seventh international conference information knowledge management pp 



baum welch re estimation help taggers 
proceedings fourth acl conference applied natural language processing pp 

freund seung shamir tishby 

selective sampling query committee algorithm 
machine learning 
friedman 

bias variance loss curse dimensionality 
data mining knowledge discovery 
furnkranz mitchell riloff 

case study linguistic phrases text categorization www 
learning text categorization papers aaai workshop pp 

tech 
rep ws aaai press 


classification mixture approaches clustering maximum likelihood 
applied statistics 
mclachlan 

efficiency linear discriminant function unclassified initial samples 
biometrika 
geman bienenstock doursat 

neural networks bias variance dilemma 
neural computation 
ghahramani jordan 

supervised learning incomplete data em approach 
advances neural information processing systems pp 

goldman zhou 

enhancing supervised learning unlabeled data 
proceedings seventeenth international conference machine learning icml 


maximum entropy hypothesis formulation especially multidimensional contingency tables 
annals mathematical statistics 
hartley rao 

classification estimation analysis variance problems 
review international statistical institute 
hofmann puzicha 

statistical models occurrence data 
tech 
rep ai memo 
artificial intelligence laboratory mit 
jaakkola haussler 

exploiting generative models discriminative classifiers 
advances neural information processing systems pp 

jaakkola meila jebara 

maximum entropy discrimination 
advances neural information processing systems pp 

jaynes 

information theory statistical mechanics 
physical review 
joachims 

probabilistic analysis rocchio algorithm tfidf text categorization 
machine learning proceedings fourteenth international conference pp 

joachims 

text categorization support vector machines learning relevant features 
machine learning ecml tenth european conference machine learning pp 

joachims 

transductive inference text classification support vector machines 
machine learning proceedings sixteenth international conference 
jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation 
koller sahami 

hierarchically classifying documents words 
machine learning proceedings fourteenth international conference pp 

lang 

newsweeder learning filter netnews 
machine learning proceedings twelfth international conference pp 

larkey croft 

combining classifiers text categorization 
sigir proceedings nineteenth annual international acm sigir conference research development information retrieval pp 

lewis 

evaluation phrasal clustered representations text categorization task 
sigir proceedings fifteenth annual international acm sigir conference research development information retrieval pp 

lewis 

sequential algorithm training text classifiers corrigendum additional data 
sigir forum 
lewis 

naive bayes independence assumption information retrieval 
machine learning ecml tenth european conference machine learning pp 

lewis gale 

sequential algorithm training text classifiers 
sigir proceedings seventeenth annual international acm sigir conference research development information retrieval pp 

lewis knowles 

threading electronic mail preliminary study 
information processing management 
lewis ringuette 

comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pp 

lewis schapire callan papka 

training algorithms linear text classifiers 
sigir proceedings nineteenth annual international acm sigir conference research development information retrieval pp 

li yamanishi 

document classification finite mixture model 
proceedings th annual meeting association computational linguistics pp 

liere 

active learning committees approach efficient learning text categorization linear threshold algorithms 
doctoral dissertation department computer science oregon state university 
liere tadepalli 

active learning committees text categorization 
proceedings fourteenth national conference artificial intelligence pp 

lin 

divergence measures shannon entropy 
ieee transactions information theory 
little 

discussion professor dempster professor laird dr rubin 
journal royal statistical society series 
mccallum nigam 

comparison event models naive bayes text classification 
learning text categorization papers aaai workshop pp 

tech 
rep ws aaai press 
mccallum nigam 

employing em pool active learning text classification 
machine learning proceedings fifteenth international conference pp 

mccallum nigam rennie seymore 

automating construction internet portals machine learning 
information retrieval 
mccallum rosenfeld mitchell ng 

improving text classification shrinkage hierarchy classes 
machine learning proceedings fifteenth international conference pp 

mclachlan basford 

mixture models 
new york marcel dekker 
mclachlan peel 

finite mixture models 
new york john wiley sons 
mclachlan 

iterative reclassification procedure constructing asymptotically optimal rule allocation discriminant analysis 
journal american statistical association 
mclachlan 

updating discriminant function basis unclassified data 
communications statistics simulation computation 
mclachlan krishnan 

em algorithm extensions 
new york john wiley sons 
merialdo 

tagging english text probabilistic model 
computational linguistics 
miller uyar 

generalized gaussian mixture classifier learning labelled unlabelled data 
proceedings conference information science systems 
mitchell 

machine learning 
new york mcgraw hill 
mladenic 

machine learning non homogeneous distributed text data 
doctoral dissertation faculty computer information science university ljubljana slovenia 
mladenic grobelnik 

feature selection unbalanced class distribution naive bayes 
machine learning proceedings sixteenth international conference pp 

mooney roy 

content book recommending learning text categorization 
proceedings fifth acm conference digital libraries pp 

mosteller wallace 

inference disputed authorship federalist 
reading massachusetts addison wesley 
moulinier ganascia 

text categorization symbolic approach 
fifth annual symposium document analysis information retrieval pp 

murray titterington 

estimation problems data mixture 
applied statistics 
ng 

preventing overfitting cross validation data 
machine learning proceedings fourteenth international conference pp 

nigam ghani 

analyzing effectiveness applicability training 
ninth international conference information knowledge management pp 

nigam lafferty mccallum 

maximum entropy text classification 
ijcai workshop machine learning information filtering pp 

nigam mccallum thrun mitchell 

learning classify text labeled unlabeled documents 
proceedings fifteenth national conference artificial intelligence pp 

neill 

normal discrimination unclassified observations 
journal american statistical association 
pazzani muramatsu billsus 

syskill webert identifying interesting web sites 
proceedings thirteenth national conference artificial intelligence pp 

press teukolsky vetterling flannery 

numerical recipes cambridge university press 
venkatesh 

learning mixture labeled unlabeled examples parametric side information 
proceedings eighth annual conference computational learning theory pp 

riloff 

automatically generating extraction patterns untagged text 
proceedings thirteenth national conference artificial intelligence pp 

riloff jones 

learning dictionaries information extraction multi level boot strapping 
proceedings sixteenth national conference artificial intelligence pp 

rissanen 

universal prior integers estimation minimum description length 
annals statistics 
robertson sparck jones 

relevance weighting search terms 
journal american society information science 
rodriguez gomez 

wordnet complement training information text categorization 
proceedings international conference advances natural language processing pp 

rose gurewitz fox 

statistical mechanics phase transitions clustering 
physical review letters 
rose gurewitz fox 

vector quantization deterministic annealing 
ieee transactions information theory 
sahami 

learning limited dependence bayesian classifiers 
kdd proceedings second international conference knowledge discovery data mining pp 

sahami dumais heckerman horvitz 

bayesian approach filtering junk mail 
learning text categorization papers aaai workshop pp 

tech 
rep ws aaai press 
schapire singer 

boostexter boosting system text categorization 
machine learning 
schohn cohn 

active learning support vector machines 
proceedings seventeenth international conference machine learning 
schuurmans 

new metric approach model selection 
proceedings fourteenth national conference artificial intelligence pp 

schuurmans 

adaptive regularization criterion supervised learning 
proceedings seventeenth international conference machine learning 
scott matwin 

text classification wordnet hypernyms 
usage wordnet natural language processing systems proceedings workshop pp 

sebastiani sperduti 

improved boosting algorithm application text categorization 
proceedings ninth international conference information knowledge management pp 

seung opper sompolinsky 

query committee 
machine learning proceedings fifth international conference pp 

seymore mccallum rosenfeld 

learning hidden markov model structure information extraction 
machine learning information extraction papers aaai workshop 
tech 
rep ws aaai press 
shahshahani landgrebe 

effect unlabeled samples reducing small sample size problem mitigating hughes phenomenon 
ieee transactions geoscience remote sensing 
shavlik rad 

intelligent agents web tasks advice approach 
learning text categorization papers aaai workshop pp 

tech 
rep ws aaai press 
stolcke omohundro 

best model merging hidden markov model induction 
tech 
rep tr 
icsi university california berkeley 
szummer jaakkola 

kernel expansions unlabeled data 
advances neural information processing systems 
appear 
titterington 

updating diagnostic system unconfirmed cases 
applied statistics 
tong koller 

support vector machine active learning applications text classification 
proceedings seventeenth international conference machine learning 
ueda nakano 

deterministic annealing variant em algorithm 
advances neural information processing systems pp 

vapnik 

statistical learning theory 
new york john wiley sons 
watanabe 

knowing guessing quantitative study inference information 
new york john wiley sons 
wiener pedersen weigend 

neural network approach topic spotting 
proceedings fourth annual symposium document analysis information retrieval pp 

yang 

evaluation statistical approaches text categorization 
information retrieval 
yang chute 

example mapping method text classification retrieval 
acm transactions information systems 
yang pedersen 

feature selection statistical learning text categorization 
machine learning proceedings fourteenth international conference pp 

yarowsky 

unsupervised word sense disambiguation rivaling supervised methods 
proceedings rd annual meeting association computational linguistics pp 

hirsh 

improving short text classification unlabeled background knowledge assess document similarity 
proceedings seventeenth international conference machine learning 


goodness fit tests large sparse multinomial distributions 
journal american statistical association 
zhang oles 

probability analysis value unlabeled data classification problems 
proceedings seventeenth international conference machine learning pp 


