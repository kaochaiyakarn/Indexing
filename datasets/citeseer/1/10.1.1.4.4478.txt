mitsubishi electric research laboratories www merl com face recognition subspaces gregory shakhnarovich moghaddam tr may images faces represented high dimensional pixel arrays belong manifold intrinsically low dimension 
face recognition computer vision research general witnessed growing interest techniques capitalize observation apply algebraic statistical tools extraction analysis underlying manifold 
chapter describe roughly chronological order techniques identify parameterize analyze linear nonlinear subspaces original eigenfaces technique introduced bayesian method probabilistic similarity analysis discuss comparative experimental evaluation techniques 
discuss practical issues related application subspace methods varying pose illumination expression 
published handbook face recognition eds 
stan li anil jain springer verlag may copied reproduced part commercial purpose 
permission copy part payment fee granted nonprofit educational research purposes provided partial copies include notice copying permission mitsubishi electric research laboratories acknowledgment authors individual contributions applicable portions copyright notice 
copying reproduction republishing purpose shall require license payment fee mitsubishi electric research laboratories rights reserved 
copyright mitsubishi electric research laboratories broadway cambridge massachusetts publication history 
printing tr may face recognition subspaces gregory shakhnarovich moghaddam massachusetts institute technology cambridge ma usa 
gregory ai mit edu mitsubishi electric research laboratories cambridge ma usa 
merl com images faces represented high dimensional pixel arrays belong manifold intrinsically low dimension 
face recognition computer vision research general witnessed growing interest techniques capitalize observation apply algebraic statistical tools extraction analysis underlying manifold 
chapter describe roughly chronological order techniques identify parameterize analyze linear nonlinear subspaces original eigenfaces technique introduced bayesian method probabilistic similarity analysis discuss comparative experimental evaluation techniques 
discuss practical issues related application subspace methods varying pose illumination expression 
face space dimensionality computer analysis face images deals visual signal light reflected surface face registered digital sensor array pixel values 
pixels may encode color intensity chapter assume case gray level imagery 
proper normalization resizing fixed size pixel array represented point vector mn dimensional image space simply writing pixel values fixed typically raster order 
critical issue analysis multi dimensional data dimensionality number coordinates necessary specify data point 
discuss factors affecting number case face images 
image space vs face space order specify arbitrary image image space needs specify pixel value 
nominal dimensionality space dictated pixel representation mn high number images modest chapter shakhnarovich moghaddam size 
recognition methods operate representation suffer number potential disadvantages rooted called curse dimensionality handling high dimensional examples especially context similarity matching recognition computationally expensive 
parametric methods number parameters needs estimate typically grows exponentially dimensionality 
number higher number images available training making estimation task image space ill posed 
similarly non parametric methods sample complexity number examples needed efficiently represent underlying distribution data prohibitively high 
surface face smooth regular texture 
pixel sampling fact unnecessarily dense value pixel typically highly correlated values surrounding pixels 
appearance faces highly constrained example frontal view face roughly symmetrical eyes sides nose middle vast proportion points image space represent physically possible faces 
natural constraints dictate face images fact confined subspace referred face space 
principal manifold basis functions common model face space possibly disconnected principal manifold embedded high dimensional image space 
intrinsic dimensionality determined number degrees freedom face space goal subspace analysis determine number extract principal modes manifold 
principal modes computed functions pixel values referred basis functions principal manifold 
concepts concrete consider straight line passing origin parallel vector point line described coordinates subspace consists points line single degree freedom principal mode corresponding translation direction consequently representing points subspace requires single basis function 
analogy line face space image space 
note theory described model face image fall face space 
practice due sensor noise signal usually non zero component outside face space 
introduces uncertainty model requires algebraic statistical techniques handbook face recognition verlag capable extracting basis functions principal manifold presence noise 
section briefly describe principal component analysis plays important role techniques 
detailed discussion see 
principal component analysis principal component analysis pca dimensionality reduction technique extracting desired number principal components multi dimensional data 
principal component linear combination original dimensions maximum variance th principal component linear combination highest variance subject orthogonal principal components 
idea pca illustrated axis labeled corresponds direction maximum variance chosen principal component 
case second principal component determined uniquely orthogonality constraints higher dimensional space selection process continue guided variances projections 
pca closely related karhunen lo transform klt derived signal processing context orthogonal transform basis 
minimizes average reconstruction error data points 
show assumption data zero mean formulations pca klt identical 
loss generality assume data zero mean mean face subtracted data 
basis vectors klt calculated way 
data matrix columns 
xm observations signal embedded context face recognition number available face images mn number pixels image 
klt basis obtained solving eigenvalue problem covariance matrix data xix 
eigenvector matrix diagonal matrix eigenvalues 
main diagonal eigenvector corresponding th largest eigenvalue 
shown eigenvalue variance data projected chapter shakhnarovich moghaddam pca basis pca reduction fig 

concept pca klt 
solid lines original basis dashed lines klt basis 
dots selected regularly spaced locations straight line rotated perturbed isotropic gaussian noise 
projection reconstruction data principal component 
perform pca extract principal components data project data columns klt basis correspond highest eigenvalues 
seen linear projection retains maximum energy variance signal 
important property pca data covariance matrix diagonal 
main properties pca summarized ky approximate reconstruction orthonormality basis decorrelated principal components yi respectively 
properties illustrated pca successful finding principal manifold successful due clear nonlinearity principal manifold 
pca may implemented singular value decomposition svd svd matrix matrix matrix orthonormal columns matrix singular values main diagonal zero 
shown svd allows efficient robust computation pca need estimate data covariance matrix singular value matrix square root eigenvalue xx handbook face recognition verlag 
number examples smaller dimension crucial advantage 
dimensionality important largely unsolved problem dimensionality reduction choice intrinsic dimensionality principal manifold 
analytical derivation number complex natural visual signal available date 
simplify problem common assume noisy embedding signal interest case point sampled face space high dimensional space signal noise ratio high 
statistically means variance data principal modes manifold high compared variance complementary space 
assumption relates set eigenvalues data covariance matrix 
recall th eigenvalue equal variance th principal component reasonable algorithm detecting search location decreasing value drops significantly 
typical face recognition problem natural choice spectrum shown 
practice choice guided computational constraints related cost matching extracted principal manifold number available face images please see sections discussion issue 
linear subspaces simplest case principal manifold analysis arises assumption principal manifold linear 
origin translated mean face average image database subtracting image means face space linear subspace image space 
section describe methods operate assumption generalization multi linear manifold 
eigenfaces related techniques ground breaking kirby sirovich proposed pca face analysis representation 
followed eigenfaces technique turk pentland application pca face recognition 
basis vectors constructed pca dimension input face images named eigenfaces 
shows example mean face top eigenfaces 
chapter shakhnarovich moghaddam fig 

eigenfaces average face left followed top eigenfaces 
face image projected subtracting mean face principal subspace coefficients pca expansion averaged subject resulting single dimensional representation subject 
test image projected subspace euclidean distances coefficient vector representing subject computed 
depending distance subject distance minimized pca reconstruction error image classified belonging familiar subjects new face non face 
demonstrates dual subspace techniques detection appearance object class faces modeled subspace distance subspace serve classify object member non member class 
probabilistic eigenspaces role pca original eigenfaces largely confined dimensionality reduction 
similarity images measured terms euclidean norm difference projected subspace essentially ignoring variation modes subspace outside 
improved extension eigenfaces proposed moghaddam pentland uses probabilistic similarity measure parametric estimate probability density 
major difficulty estimation normally nearly data estimate parameters density high dimensional space 
moghaddam pentland overcome problem pca divide vector space subspaces shown principal subspace obtained columns orthogonal complement spanned remaining columns 
operating assumption data intrinsic dimensionality reside exception additive white gaussian noise image decomposed orthogonal components projection spaces 
shows decomposition distance face space difs distance face space dffs 
probability density decomposed orthogonal components pf 
simplest case gaussian density 
derived complete likelihood estimate case written product handbook face recognition verlag difs dffs fig 

decomposition principal subspace orthogonal complement gaussian density typical eigenvalue spectrum division orthogonal subspaces 
independent marginal gaussian densities pf exp pf true marginal density pf estimated marginal density yi principal components pca reconstruction error 
information theoretic optimal value noise density parameter derived minimizing kullback leibler kl divergence shown simply average smallest eigenvalues 
special case general factor analysis model called probabilistic pca ppca proposed tipping bishop 
formulation expression maximum likelihood solution latent variable model opposed minimal divergence solution derived 
practice majority eigenvalues computed due insufficient data estimated example fitting chapter shakhnarovich moghaddam nonlinear function available portion eigenvalue spectrum estimating average eigenvalues principal subspace 
fractal power law spectra form thought typical natural phenomenon fit decaying nature illustrated 
probabilistic framework recognition test image carried terms computing database example xi difference xi decomposition components ranking examples value 
linear discriminants fisherfaces substantial changes illumination expression variation data due changes 
pca techniques essentially select subspace retains variation consequently similarity face space necessarily determined identity 
belhumeur propose solve problem fisherfaces application fisher linear discriminant fld :10.1.1.10.3247
fld selects linear subspace maximizes ratio sb sw sb ni xi xi class scatter matrix sw xi xi xi class scatter matrix number subjects classes database 
intuitively fld finds projection data classes linearly separable 
shown dimension 
practice sw usually singular fisherfaces algorithm reduces dimensionality data pca computed applies fld reduce dimensionality 
recognition accomplished nn classifier final subspace 
experiments reported performed data sets containing frontal face images people drastic lighting variations set faces people varying expressions drastic illumination changes 
reported experiments fisherfaces achieve lower error rate eigenfaces 
comparison note objective pca bee seen maximizing total scatter images database 
bayesian methods handbook face recognition verlag consider feature space vectors differences images ij ik 
define classes facial image variations intrapersonal variations corresponding example different facial expressions illuminations individual variations corresponding variations different individuals 
similarity measure expressed terms intrapersonal posteriori probability belonging bayes rule note particular bayesian formulation proposed moghaddam casts standard face recognition task essentially ary classification problem individuals binary pattern classification problem densities classes modeled high dimensional gaussians efficient pca method described section densities zero mean ij ii exists ii ij 
pca gaussians known occupy subspace image space face space top eigenvectors gaussian densities relevant modeling 
densities evaluate similarity 
computing similarity involves subtracting candidate image database example ij 
resulting image projected eigenvectors gaussian eigenvectors intrapersonal gaussian 
exponentials computed normalized combined 
operation iterated examples database example achieves maximum score considered match 
large databases evaluations expensive desirable simplify line transformations 
compute likelihoods database images ij pre processed whitening transformations 
image converted stored set whitened subspace coefficients intrapersonal space space vx matrices largest eigenvalues eigenvectors respectively substituting symbol 
chapter shakhnarovich moghaddam fig 

signal flow diagrams computing similarity images original eigenfaces 
bayesian similarity 
difference image projected sets intra extra eigenfaces order obtain likelihoods 
pre processing evaluating gaussians reduced simple euclidean distances 
denominators course precomputed 
likelihoods evaluated compute maximum posteriori map similarity 
euclidean distances computed ki dimensional vectors ke dimensional vectors 
roughly ke ki arithmetic operations required similarity computation avoiding repeated image differencing projections ij yj ki ij yj ke maximum likelihood ml similarity matching simpler intra personal class evaluated leading modified form similarity measure handbook face recognition verlag yj ki 
approach described requires projections difference vector likelihoods estimated bayesian similarity measure 
computation flow illustrated 
projection steps linear posterior computation nonlinear 
double pca projections required approach called dual eigenspace technique 
note projection difference vector dual eigenfaces computation posterior 
instructive compare contrast lda fisherfaces dual subspace bayesian technique noting similar roles played class class intrapersonal subspaces 
key differences techniques lda fact viewed special case dual subspace bayesian approach 
analysis pca lda bayesian matching unified parameter subspace approach compared terms performance 
likewise experimental studies years shown intra extra bayesian matching technique performs lda 
bear mind ultimately optimal probabilistic justification lda case distributions equal covariance lda tends perform condition strictly true 
contrast dual subspace bayesian formulation completely general probabilistic definition appeals gaussianity geometry underlying data meta classes intra extra 
intra extra probability distributions take form 
arbitrary mixture models just single gaussians case allow easy visualization diagonalizing dual covariances sets eigenfaces 
ica source separation pca minimizes sample covariance second order dependency data independent component analysis ica minimizes higherorder dependencies components ica designed non gaussian 
pca ica yields linear projection different properties ay yi approximate reconstruction non orthogonality basis near factorization joint distribution marginal distributions non gaussian ics 
chapter shakhnarovich moghaddam fig 

ica vs pca decomposition data set 
bases pca orthogonal ica non orthogonal 
left projection data top principal components pca 
right projection top independent components ica 

example ica basis shown computed set points 
subspace recovered ica appears reflect distribution data better subspace obtained pca 
example ica basis shown see unordered non orthogonal ic vectors roughly aligned principal component vector direction maximum variance 
note actual non gaussianity statistical independence achieved toy example minimal best success ica recovering principal modes data 
ica intimately related blind source separation problem decomposition input signal image linear combination mixture independent source signals 
formally assumption unknown mixing matrix 
ica algorithms try find separating matrix wx data consist observations variables input ica arranged matrix bartlett investigated ica framework face recognition fundamentally different architectures architecture rows independent basis images combined yield input images learning allows estimate basis images rows practice reasons computational tractability pca performed input data find top eigenfaces arranged columns matrix ica performed images variables pixel values number algorithms exist notably jade infomax fastica 
eigenfaces linear combination original images assumptions ica affect resulting decomposition 
handbook face recognition verlag fig 

basis images ica architecture top ii bottom 

observations 
pca coefficient matrix ce independent ica basis images top estimated rows coefficients data computed ew architecture ii architecture algorithm assumes sources independent coefficients columns mixing matrix basis images variables source separation problem pixels 
similar architecture ica preceded pca case input ica coefficient matrix resulting ica basis consists columns ea bottom coefficients rows wc coefficients give factorial representation data 
generally bases obtained architecture reflect local properties faces bases architecture ii global properties resemble faces see 
multi linear svd linear analysis methods discussed shown suitable pose illumination expression fixed face database 
parameters allowed vary linear subspace representation capture variation see section 
section discuss recognition nonlinear subspaces 
alternative multi linear approach called proposed terzopoulos 
tensor multidimensional generalization matrix order tensor object indices elements denoted ai note ways flatten tensor rearrange elements matrix th row obtained concatenating elements form ai 
generalization matrix multiplication tensors mode product tensor matrix th dimension chapter shakhnarovich moghaddam fig 


data tensor dimensions visualized identity illumination pose pixel vector 
fifth dimension corresponds expression sub tensor neutral expression shown 
decomposition 

il il ai il il 
definition terzopoulos propose algorithm call mode svd decomposes dimensional tensor un 
role core tensor decomposition similar role singular value matrix svd governs interactions mode matrices 
un contain orthonormal bases spaces spanned corresponding dimensions data tensor 
mode matrices obtained flattening tensor corresponding dimension performing pca columns resulting matrix core tensor computed notion tensor applied face image ensemble way consider set pixel images np people faces photographed nv viewpoints ni illuminations ne expressions 
entire set may arranged np nv ni ne tensor order handbook face recognition verlag 
illustrates concept dimensions shown visualize fifth expression imagine dimensional tensors different expressions stacked 
context face image tensor decomposed uv ui ue 
mode matrix represents parameter object appearance 
example columns ne ne matrix ue span space expression parameters 
columns span image space exactly eigenfaces obtained direct pca entire data set 
person database represented single np vector contains coefficients respect bases comprising tensor uv ui ue 
viewpoint illumination expression np matrix bv obtained indexing flattening resulting np sub tensor identity people mode 
training image xp person conditions written xp cp cj th row vector 
input image candidate coefficient vector cv computed combinations viewpoint expression illumination solving equation 
recognition carried finding value yields minimum euclidean distance vectors cj illuminations expressions viewpoints 
authors report experiments involving data tensor consisting images np subjects photographed ni illumination conditions nv viewpoints ne different expressions images resized cropped contain pixels 
performance reported significantly better standard eigenfaces described section 
nonlinear subspaces section describe number modeling techniques principal manifolds strictly nonlinear 
emphasize mathematics methods readily applicable types data practice distinguish intrinsic nonlinearity data technique estimate parameters illumination associated variability input images 
chapter shakhnarovich moghaddam fig 

pca basis linear ordered orthogonal ica basis linear unordered non orthogonal principal curve parameterized nonlinear manifold 
circle shows data mean 
nonlinearity arises due improper choice parameterization 
example object translation linear visual representation spatially sampled image example highly nonlinear 
judicious choice coordinate frame centered linearize data manifold obviating need computationally difficult intractable nonlinear modeling techniques 
possible seek right parameterization problem 
principal curves nonlinear pca defining property nonlinear principal manifolds inverse image manifold original space nonlinear curved surface passes middle data minimizing sum total distance data points projections surface 
referred principal curves formulation essentially nonlinear regression data 
example principal curve shown 
simplest methods computing nonlinear principal manifolds nonlinear pca nlpca auto encoder multi layer neural network shown 
called bottleneck layer forms lower dimensional manifold representation means nonlinear projection function implemented weighted sum sigmoids 
resulting principal components inverse mapping similar nonlinear reconstruction function reproduces input data accurately possible 
nlpca computed multi layer sigmoidal neural network equivalent certain exceptions principal surface general definition class functions attainable neural network restricts projection function smooth differentiable suboptimal cases 
handbook face recognition verlag fig 

auto associative bottleneck neural network computing principal manifolds input space 
summarize main properties nlpca 
corresponding nonlinear projection approximate reconstruction typically prior knowledge regarding joint distribution components respectively see zemel example devising suitable priors cases 
principal curve generated layer neural network type shown 
note principal curve yields compact relatively accurate representation data contrast linear models pca ica 
kernel pca kernel fisher methods nonlinear principal component analysis revived kernel eigenvalue method sch lkopf 
basic methodology kpca apply nonlinear mapping input solve linear pca resulting feature space larger possibly infinite 
increase dimensionality mapping implicit economical kernel functions satisfying mercer theorem xi xj xi xj kernel evaluations xi xj input space correspond higher dimensional feature space 
computing covariance dot products performing pca feature space formulated kernels input space explicit possibly chapter shakhnarovich moghaddam prohibitively expensive direct computation 
specifically assuming projection data feature space zero mean centered covariance xi xi resulting eigenvector equation kv 
eigenvectors columns lie span training data xi true training point xi xi kv 
exist coefficients wi wi xi 
definition substituting equation defining resulting matrix kij xi xj leads equivalent eigenvalue problem formulated terms kernels input space kw wt vector expansion coefficients eigenvector defined 
kernel matrix kij xi xj diagonalized standard pca orthonormality eigenvectors leads equivalent normalization respective expansion coefficients 
subsequently kpca principal components input vector efficiently computed simple kernel evaluations dataset 
th principal component yn yn vn xi vn th eigenvector feature space defined 
pca eigenvectors vn ranked decreasing order eigenvalues dimensional manifold projection yd individual components defined 
significant advantage kpca neural network principal curves kpca require nonlinear optimization subject overfitting require prior knowledge network architecture number dimensions 
furthermore traditional pca computing requires centering data computing mean xi 
explicit computation xi covariance matrix centered details see 
handbook face recognition verlag eigenvector projections input dimensionality data kpca matrix number eigenvectors features available 
hand selection optimal kernel associated parameters remains engineering problem typical kernels include gaussians exp xi xj polynomials xi xj sigmoids tanh xi xj satisfy mercer theorem 
similar derivation kpca may extend fisherfaces method see section applying fld feature space 
yang derives kernel fisherfaces algorithm maximizes scatter scatter ratio feature space kernel matrix experiments data sets contained images subjects respectively varying pose scale illumination algorithm showed performance clearly superior ica pca kpca somewhat better standard fisherfaces 
empirical comparison subspace methods moghaddam reports extensive evaluation subspace methods described large subset feret dataset see chapter :10.1.1.140.8914
experimental data consisted training gallery individual feret faces probe images containing views person gallery 
images aligned normalized described 
multiple probe images reflected different expressions lighting glasses study compared bayesian approach described section number techniques tested limits recognition algorithms respect image resolution equivalently amount visible facial detail bayesian algorithm independently evaluated darpa feret face recognition competition medium resolution images pixels achieving accuracy individuals decided lower resolution number pixels factor :10.1.1.140.8914
aligned faces dataset downsampled pixels yielding input vectors space 
examples shown figures 
reported results obtained fold cross validation cv analysis 
total dataset faces unique individuals collective probes randomly partitioned subsets unique non overlapping individuals associated probes 
subset contained gallery probe images unique individuals 
subsets recognition task correctly matching multiple probes gallery faces subsets training data 
note entire dataset training nearly times training samples data dimensionality chapter shakhnarovich moghaddam fig 

experiments feret data 
faces gallery 
multiple probes individual different facial expressions eye glasses variable ambient lighting image contrast 
eigenfaces 
ica basis images 
parameter estimations pca ica kpca bayesian method properly constrained 
resulting experimental trials pooled compute mean standard deviation recognition rates method 
fact training testing sets overlap terms individual identities led evaluation algorithms generalization performance ability recognize new individuals part manifold computation density modeling training set 
baseline recognition experiments default manifold dimensionality 
choice reasons led reasonable pca reconstruction error mse pixel normalized intensity range baseline pca recognition rate different partition dataset leaving sizeable margin improvement 
note recognition experiments essentially way classification task chance performance approximately 
pca recognition baseline algorithm face recognition experiments standard pca eigenface matching 
principal eigenvectors computed single partition shown 
projection test set probes dimensional linear manifold computed pca training set followed nearest neighbor matching gallery images euclidean metric yielded mean recognition rate highest rate achieved shown table 
full nearest neighbor template matching yielded recognition rate see dashed line 
clearly performance degraded dimensionality reduction expected 
ica recognition handbook face recognition verlag ica recognition architecture ii see section different algorithms th order cumulants tried jade algorithm cardoso fixed point algorithm hyv rinen oja 
algorithms pca whitening step sphering preceded core ica decomposition 
corresponding non orthogonal jade derived ica basis shown 
similar basis faces obtained hyv rinen method 
basis faces columns matrix linear combination specified ics reconstructs training data 
ica manifold projection test set obtained nearestneighbor matching ica euclidean norm resulted mean recognition rate highest rate shown table 
little difference ica algorithms noted ica resulted largest performance variation trials std 
dev 
mean recognition rates unclear ica provides systematic advantage pca non gaussian independent components result better manifold recognition purposes dataset 
note experimental results bartlett feret faces favor ica pca 
disagreement reconciled considers differences experimental setup choice similarity measure 
advantage ica seen primarily difficult time separated images 
addition compared faces experiment cropped tighter leaving information regarding hair face shape lower resolution factors combined recognition task harder 
second factor choice distance function measure similarity subspace 
matter investigated draper 
best results ica obtained cosine distance eigenfaces metric appears optimal metric experiments performance ica architecture ii similar eigenfaces 
kpca recognition kpca parameters gaussian polynomial sigmoidal kernels fine tuned best performance different partition validation set gaussian kernels best dataset 
trial kernel matrix computed corresponding training data 
test set gallery probes projected kernel eigenvector basis order obtain nonlinear principal components nearest neighbor matching test set probes test set gallery images 
mean recognition rate chapter shakhnarovich moghaddam table 
recognition accuracies subspace projections fold cross validation 
partition pca ica kpca bayes mean std 
dev 
table 
comparison various techniques multiple attributes 
pca ica kpca bayes accuracy complexity uniqueness projections linear linear nonlinear linear highest rate shown table 
standard deviation kpca trials slightly higher pca indicates kpca fact better pca ica justifying nonlinear feature extraction 
map recognition bayesian similarity matching appropriate training classes dual pca density estimates modeled single gaussians subspace dimensions ki ke respectively 
total subspace dimensionality divided evenly densities setting ki ke modeling 
gaussian subspace dimensions ki ke respectively 
note ki ke matching total number projections principal manifold techniques 
maximum posteriori map similarity bayesian matching technique yielded mean recognition rate highest rate achieved shown table 
standard practice ki ke yields results 
fact ke obtains maximum likelihood similarity ki dataset percent accurate map 
handbook face recognition verlag pca ica kpca bayes fig 

recognition performance pca ica kpca manifolds vs bayesian map similarity matching dimensional subspace dashed line performance nearest neighbor matching full dimensional image vectors 
deviation partitions algorithm lowest see 
compactness manifolds performance different methods different size manifolds compared plotting recognition rates function principal components 
manifold matching techniques simply means subspace dimension components pca ica kpca bayesian matching technique means subspace gaussian dimensions satisfy ki ke methods number subspace projections 
test premise key points investigated number subspace projections techniques better data modeling subsequent recognition 
presumption achieving highest recognition rate smallest dimension preferred 
particular dimensionality test total dataset images partitioned split half training set gallery images randomly selected corresponding probes testing set containing remaining gallery images corresponding probes 
training test sets overlap terms individuals identities 
previous experiments test set probes matched test set gallery images projections densities computed chapter shakhnarovich moghaddam bayes kpca pca fig 

recognition accuracy pca kpca bayesian similarity increasing dimensionality principal subspace ica results shown similar pca 
training set 
results experiment shown plots recognition rates function dimensionality subspace revealing comparison relative performance different methods compactness manifolds defined lowest acceptable value important consideration regards generalization error fitting computational requirements 
performance manifolds relative performance principal manifold techniques bayesian matching summarized table 
advantage probabilistic matching metric matching linear nonlinear manifolds quite evident increase pca kpca 
note dimensionality test results indicate kpca performs pca margin principal components similar effect reported sch lkopf kpca performs pca low dimensional manifolds 
bayesian matching achieves projections dominates pca kpca entire range subspace dimensions 
comparison subspace techniques respect multiple criteria shown table 
note pca kpca dual subspace density estimation uniquely defined training set making experimental comparisons repeatable ica unique due variety handbook face recognition verlag different techniques compute basis iterative stochastic optimizations involved 
considering relative computation training kpca required floating point operations compared pca operations 
average ica computation order magnitude larger pca 
bayesian similarity method learning stage involves separate pcas computation merely twice pca order magnitude 
considering significant performance advantage low subspace dimensionality relative simplicity dual eigenface bayesian matching method highly effective subspace modeling technique face recognition 
independent feret tests conducted army laboratory bayesian similarity technique performed pca subspace techniques fisher linear discriminant margin 
experimental results described show similar recognition accuracy achieved mere thumbnails times fewer pixels images feret test 
results demonstrate bayesian matching technique robustness respect image resolution revealing surprisingly small amount facial detail required high accuracy performance learning technique 
methodology usage section discuss issues require special care practitioner particular approaches designed handle database varying imaging conditions 
number extensions modifications subspace methods 
multi view approach pose problem face recognition general viewing conditions change pose approached eigenspace formulation 
essentially ways approaching problem eigenspace framework 
individuals different views recognition pose estimation universal eigenspace computed combination mc images 
way single parametric eigenspace encode identity pose 
approach example murase nayar general object recognition 
alternatively individuals different views build view set distinct eigenspaces capturing variation individuals common view 
view eigenspace essentially extension eigenface technique multiple sets eigenvectors combination scale orientation 
view architecture set parallel observers trying explain image data set eigenvectors 
view multiple observer approach step chapter shakhnarovich moghaddam fig 

parametric vs view eigenspace methods 
reconstructions input image left parametric middle view right eigenspaces 
top training image bottom novel test image 
schematic illustration difference way approaches span manifold 
determine location orientation target object selecting eigenspace best describes input image 
accomplished calculating likelihood estimate eigenvectors selecting maximum 
key difference view parametric representations understood considering geometry face space schematically illustrated 
high dimensional vector space input image multiple orientation training images represented set distinct regions defined scatter individuals 
multiple views face form non convex connected regions image space 
resulting ensemble highly complex nonseparable manifold 
parametric eigenspace attempts describe ensemble projection single low dimensional linear subspace corresponding eigenvectors mc training images 
contrast view approach corresponds independent subspaces describing particular region face space corresponding particular view face principal manifold vc region extracted separately 
relevant analogy modeling complex distribution single cluster model union component clusters 
naturally view representation yield accurate representation underlying geometry 
difference representation evident considering quality reconstructed images different methods 
fig 
compares reconstructions obtained methods trained images handbook face recognition verlag fig 

example multi view face image data experiments described section 

faces multiple orientations 
top row fig 
see image training set followed reconstructions image parametric eigenspace view eigenspace 
note parametric reconstruction pose identity individual adequately captured 
view reconstruction hand provides better characterization object 
similarly bottom row fig 
see novel view respect training set 
reconstructions correspond nearest view training set view reconstruction seen representative individual identity 
quality reconstruction direct indicator recognition power information theoretic point view multiple eigenspace representation accurate representation signal content 
view approach evaluated data similar shown fig 
consisted images views people 
viewpoints evenly spaced horizontal plane 
series experiments interpolation performance tested training subset available views testing intermediate views percent average recognition rate obtained 
second series experiments tested extrapolation performance training range views testing novel views outside training range 
testing views separated training range average recognition rates percent 
testing views average recognition rates percent 
modular recognition eigenface recognition method easily extended facial features shown 
leads improvement recognition performance incorporating additional layer description terms facial chapter shakhnarovich moghaddam fig 

modular eigenspaces 
rectangular patches appearance modeled eigenfeatures 
performance eigenfaces eigenfeatures layered combination function subspace dimension 
features 
viewed modular layered representation face coarse low resolution description head augmented additional higher resolution details terms salient facial features 
pentland called component eigenfeatures 
utility layered representation eigenface plus eigenfeatures tested small subset large face database representative sample individuals views person corresponding different facial expressions neutral vs smiling 
set images partitioned training set neutral testing set smiling 
difference particular facial expressions primarily articulated mouth feature discarded recognition purposes 
fig 
shows recognition rates function number eigenvectors eigenface combined representation 
surprising small dataset eigenfeatures sufficient achieving asymptotic recognition rate percent equal eigenfaces 
surprising observation lower dimensions eigenspace eigenfeatures outperformed eigenface recognition 
combined representation gains slight improvement asymptotic recognition rate percent 
similar effect reported brunelli poggio cumulative normalized correlation scores handbook face recognition verlag templates face eyes nose mouth showed improved performance face templates 
potential advantage layer ability overcome shortcomings standard eigenface method 
pure eigenface recognition system fooled gross variations input image hats 
feature representation may find correct match focusing characteristic non occluded features eyes nose 
recognition sets interesting recognition paradigm involves scenario input consists single image set images unknown person 
set may consist contiguous sequence frames video non contiguous unordered set photographs extracted video obtained individual snapshots 
case discussed chapter recognition video 
case consider temporal information available 
possible approach fact taken apply standard recognition methods image input set combine results typically means voting 
large set images contains information individual image provides clue possible appearance face typical patterns variation 
technically just set images known contain individual face allows represent individual estimated intrinsic subspace unlabeled input set leads subspace estimate represents unknown subject 
recognition task formulated terms matching subspaces 
approaches task mutual subspace method msm extracts principal linear subspace fixed dimension pca measures distance subspaces means principal angles minimal angle vectors subspaces 
msm desirable feature builds compact model distribution observations 
ignores important statistical characteristics data eigenvalues corresponding principal components means samples disregarded comparison 
decisions may statistically sub optimal 
probabilistic approach measuring subspace similarity proposed 
underlying statistical model assumes images th person face probability density pj density unknown subject face denoted 
task recognition system find class label satisfying argmax pr pj chapter shakhnarovich moghaddam set images distributed solving amounts optimally choosing hypotheses form statistics referred sample hypothesis sets examples come distribution 
principled way solving task choose hypothesis kullback leibler divergence pj minimized 
reality distributions pj unknown need estimated data 
shakhnarovich model distributions gaussians subject estimated method described section kl divergence computed closed form 
experiments reported method significantly outperforms msm 
modeling distributions single gaussian somewhat limiting wolf shashua extend approach propose non parametric discriminative method kernel principal angles 
devise positive definite kernel operates pairs data matrices projecting data columns feature space arbitrary dimension principal angles calculated computing inner products examples application kernel 
note approach corresponds nonlinear subspace analysis original space instance polynomial kernels arbitrary degree 
experiments included face recognition task set subjects method significantly outperformed msm gaussian kl divergence model 
subspace methods shown highly successful face recognition vision tasks 
exposition chapter roughly follows chronological order methods evolved 
notable directions evolution discerned transition linear general possibly non linear disconnected manifolds probabilistic specifically bayesian methods dealing uncertainty similarity 
methods share core assumption ostensibly complex visual phenomena images human faces represented high dimensional measurement space intrinsically low dimensional 
exploiting low dimensionality allows face recognition system simplify computations focus attention features data relevant identity person 
bartlett figures papers helpful comments 
acknowledge contributed research described chapter 
handbook face recognition verlag 
bartlett lades sejnowski 
independent component representations face recognition 
proceedings spie conference human vision electronic imaging iii volume pages 

belhumeur hespanha kriegman 
eigenfaces vs fisherfaces recognition class specific linear projection 
ieee transactions pattern analysis machine intelligence july 

bichsel pentland 
human face recognition face image set topology 
cvgip image understanding 

brunelli poggio 
face recognition features vs templates 
ieee transactions pattern analysis machine intelligence 


cardoso 
high order contrasts independent component analysis 
neural computation 

comon 
independent component analysis new concept 
signal processing 

courant hilbert 
methods mathematical physics volume 
interscience new york 

cover thomas 
elements information theory 
john wiley sons new york 

demers cottrell 
nonlinear dimensionality reduction 
advances neural information processing systems pages 
morgan kaufmann 

draper bartlett beveridge 
recognizing faces pca ica 
computer vision image understanding july aug 

fukunaga 
statistical pattern recognition 
academic press second edition 


relationships svd klt pca 
pattern recognition 

hastie 
principal curves surfaces 
phd thesis stanford university 

hastie stuetzle 
principal curves 
journal american statistical association 

hyv rinen oja 
family fixed point algorithms independent component analysis 
technical report helsinki university technology 

hyv rinen oja 
independent component analysis algorithms applications 
neural networks 

jolliffe 
principal component analysis 
springer verlag new york 

jutten herault 
blind separation sources part adaptive algorithm neuromimetic architecture 
signal processing 

kirby sirovich 
application karhunen loeve procedure characterization human faces 
ieee transactions pattern analysis machine intelligence jan 

kramer 
nonlinear principal components analysis autoassociative neural networks 
aiche journal 

lo 
probability theory 
van nostrand princeton 
chapter shakhnarovich moghaddam 

theoretical results nonlinear principal component analysis 
technical report northwestern university 

moghaddam 
principal manifolds bayesian subspaces visual recognition 
ieee transactions pattern analysis machine intelligence june 

moghaddam jebara pentland 
efficient map ml similarity matching face recognition 
proceedings international conference pattern recognition pages brisbane australia aug 

moghaddam jebara pentland 
bayesian face recognition 
pattern recognition nov 

moghaddam pentland 
probabilistic visual learning object detection 
proceedings ieee international conference computer vision pages cambridge usa june 

moghaddam pentland 
probabilistic visual learning object representation 
ieee transactions pattern analysis machine intelligence july 

murase nayar 
visual learning recognition objects appearance 
international journal computer vision jan 

sirovich 
global dimensionality face space 
proc 
ieee internation conf 
face gesture recognition pages grenoble france 

pentland moghaddam starner 
view modular eigenspaces face recognition 
proceedings ieee computer vision pattern recognition pages seattle wa june 
ieee computer society press 

phillips moon rauss rizvi 
feret evaluation methodology face recognition algorithms 
proceedings ieee computer vision pattern recognition pages june 

sch lkopf smola 
muller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 

shakhnarovich fisher darrell 
face recognition longterm observations 
proceedings european conference computer vision pages copenhagen denmark may 

tipping bishop 
probabilistic principal component analysis 
technical report ncrg aston university sept 

turk pentland 
eigenfaces recognition 
journal cognitive neuroscience 

turk pentland 
face recognition eigenfaces 
proceedings ieee computer vision pattern recognition pages maui hawaii dec 

terzopoulos 
multilinear subspace analysis image ensembles 
proceedings ieee computer vision pattern recognition pages madison wi june 

terzopoulos 
multilinear analysis image ensembles 
proceedings european conference computer vision pages copenhagen denmark may 

wang tang 
unified subspace analysis face recognition 
proceedings ieee international conference computer vision nice france june 
handbook face recognition verlag 
wolf shashua 
learning sets kernel principal angles 
journal machine learning research oct 

yamaguchi 
maeda 
face recognition temporal image sequence 
proc 
ieee internation conf 
face gesture recognition pages nara japan apr 


yang 
kernel eigenfaces vs kernel fisherfaces face recognition kernel methods 
proc 
ieee internation conf 
face gesture recognition pages washington dc may 

zemel hinton 
developing population codes minimizing description length 
cowan tesauro alspector editors advances neural information processing systems volume pages 
morgan kaufmann publishers 
