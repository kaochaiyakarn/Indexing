stork making data placement class citizen grid miron livny computer sciences department university wisconsin madison west dayton street madison wi miron cs wisc edu todays scientific applications huge data requirements continue increase drastically year 
data generally accessed users globe 
implies major necessity move huge amounts data wide area networks complete computation cycle brings problem efficient reliable data placement 
current approach solve problem data placement doing manually employing simple scripts automation fault tolerance capabilities 
goal data placement activities class citizens grid just computational jobs 
queued scheduled monitored managed check pointed 
importantly sure complete successfully human interaction 
believe data placement jobs treated differently computational jobs may different semantics different characteristics 
purpose developed stork scheduler data placement activities grid 

grid evolves data requirements scientific applications increase drastically 
just couple years ago data requirements average scientific application measured terabytes today petabytes measure 
data requirements continue increase rapidly year 
example compact muon cms project high energy physics project participating grid physics network griphyn 
particle physics data grid ppdg deliverables cms data volume cms currently couple terabytes year expected subsequently increase rapidly accumulated data volume reach terabytes 
data volume required application data intensive applications projects similar data requirements ranging genomics biomedical cosmology 
problem huge needs data intensive applications number users access datasets 
projects number people accessing datasets range 
furthermore users located single site distributed country globe 
predominant necessity move huge amounts data wide area networks complete computation cycle brings problem efficient reliable data placement 
data need located moved staged replicated cached storage allocated de allocated data necessary cleaned user done data 
just compute resources network resources need carefully scheduled managed scheduling data placement activities grid crucial access data potential main bottleneck data intensive applications 
especially case data stored tape storage systems slows access data due mechanical nature systems 
currently data placement activities grid performed manually simple scripts 
say data placement activities regarded second class citizens computation dominated grid world 
goal data placement activities class citizens grid just computational jobs 
need queued scheduled monitored managed checkpointed 
believe data placement jobs treated differently computational jobs may different semantics different characteristics 
existing computational job schedulers understand semantics data transfers 
example transfer large file fails may want simply restart job re transfer file 
may prefer transfer remaining part file 
similarly transfer protocol fails may want try protocols supported source destination hosts perform transfer 
traditional computational job scheduler may able handle cases 
purpose data placement jobs computational jobs differentiated 
data placement jobs submitted scheduler capable scheduling managing data placement jobs computational jobs submitted scheduler capable scheduling managing computational jobs 
help separating computational data placement jobs give users ability perform asynchronously 
purpose developed stork scheduler data placement activities grid 

grid data placement challenges grid provides researchers resources brings challenges 
order utilize grid resources efficiently researchers overcome challenges 
data placement related challenges trying solve 
heterogeneous resources 
grid heterogeneous environment different storage systems different data transfer middleware protocols coexist 
fundamental problem data required application stored heterogeneous repositories 
easy task interact possible different storage systems access data 
negotiating system access different kinds storage systems different underlying middleware file transfer protocols 
hiding failures applications 
grid brings failed network connections performance variations transfers crashed clients servers storage systems 
generally applications prepared kind problems 
applications assume perfect computational environments failure free network storage devices unlimited storage availability data computation starts low latency 
expect application consider possible failures performance variations system prepared 
able hide application mediating system 
different job requirements 
job may different policies different priorities 
scheduling done needs individual job 
global scheduling decisions able tailored individual requirements job 
global policies may affective efficient 
job description language strong flexible support job level policies 
job scheduler able support enforce policies 
overloading limited resources 
network storage resources application access limited efficiently 
common problem distributed computing environments jobs submitted remote sites start execution time start pulling data home storage systems stage concurrently 
overload network resources local disks remote execution sites 
may bring load home storage systems data pulled 
approach pre allocate network storage resources 
approach works fine long pre allocation supported resources user knows long resources application 
general solution control total number transfers happening anytime sites 
job schedulers control total number jobs submitted executed time solution sufficient best solution cases 
reason overlapping cpu causes cpu wait performed 
problem gets complex jobs complete try move output data back home storage systems stage 
case stage ins stage outs different jobs may interfere especially overloading network resources 
intelligent scheduling mechanism developed control number stage specific storage systems anytime cause waste cpu time 

related visualization scientists los alamos national laboratory lanl solution data placement dumping data tapes sending sandia national laboratory federal express faster electronically transmitting tcp mbps oc wan backbone 
reliable file transfer service allows byte streams transferred reliable manner 
handle wide variety problems dropped connections machine reboots temporary network outages automatically retrying 
built top gridftp secure reliable data transfer protocol especially developed high bandwidth wide area networks 
lightweight data replicator ldr replicate data sets member sites virtual organization datagrid 
primarily developed replicating ligo data globus tools transfer data 
goal minimum collection components necessary fast secure replication data 
single data transport protocol gridftp 
ongoing effort provide unified interface different storage systems building storage resource managers srms top 
currently couple data storage systems hpss support srms top 
srms manage distributed caches pinning files 
sdsc storage resource broker srb aims provide uniform interface connecting heterogeneous data resources accessing replicated data sets 
srb uses metadata catalog provide way access data sets resources attributes names physical locations 
thain propose ethernet approach grid computing introduce simple scripting language handle failures manner similar exceptions languages 
ethernet approach aware semantics jobs running duty retrying job number times fault tolerant manner 
kangaroo tries achieve high throughput making opportunistic disk network resources 

stork solutions grid data placement problems stork provides solutions data placement problems encountered grid environment 

interaction higher level planners applications grid require moving input data job remote site execution site executing job moving output data execution site remote site 
application want take risk getting disk space execution site may want allocate space transferring input data release space moves output data 
regard computational data placement steps real jobs represent nodes directed acyclic graph dag 
dependencies represented directed arcs shown 
stork interact higher level planners directed acyclic graph manager dagman 
allows users able schedule cpu resources 
step plan 
computation remote site input output data requirements achieved step plan represented node dag 
storage resources 
enhancements dagman differentiate computational jobs data placement jobs 
submit computational jobs computational job scheduler condor condor data placement jobs stork 
shows sample dag specification file enhancement data placement nodes dag handled dagman 
way sure input file required computation arrives storage device close execution site computation starts executing site 
similarly output files removed remote storage system soon computation completed 
storage device cpu occupied needed jobs wait idle input data available 

interaction heterogeneous resources stork completely modular extended easily 
straightforward add support stork favorite storage system data transport protocol middleware 
crucial feature system designed heterogeneous grid environment 
users applications may expect storage systems support interfaces talk 
expect applications talking different kinds storage systems protocols middleware 
needs negotiating system interact systems easily translate different protocols 
stork developed capable 

interaction higher level planners 
prototype model stork interacts higher level planner dagman 
dag specification file consisting computational data placement jobs submitted dagman 
dagman submits computational jobs condor condor data placement jobs stork 
modularity stork allows users insert plug support favorite storage system protocol middleware easily 
stork support different storage systems data transport protocols middleware 
users immediately extra 
stork interact currently data transfer protocols ftp gridftp data storage systems srb unitree nest data management middleware srm 
stork maintains library pluggable data placement modules 
modules get executed data placement job requests coming stork 
perform translations memory buffer transfers available 
order transfer data systems direct inter protocol translation supported consecutive stork jobs 
stork job performs transfer source storage system local disk cache stork second stork job performs transfer local disk cache stork destination storage system 

flexible job representation multilevel policy support stork uses classad job description language represent data placement jobs 
classad language provides flexible extensible data model 
job representation stork 
sample data placement dap requests shown allocate space second transfer file reserved space third de allocate reserved space 
represent arbitrary services constraints 
shows sample data placement dap requests 
request allocate mb disk space hours nest server 
second request transfer file srb server reserved space nest server 
third request de allocate previously reserved space 
addition reserve transfer release data placement job types locate find data located stage move data tertiary storage secondary storage order decrease data access time actual transfers 
stork enables users specify job level policies global ones 
global policies apply jobs scheduled stork server 
users overwrite specifying job level policies job description classads 
example shows overwrite global policies job level 
dap type transfer 
max retry restart hours particular example user specifies particular job retried times case failure transfer get completed hours killed restarted 

run time adaptation stork decide data transfer protocol corresponding transfer dynamically automatically run time 
performing transfer stork quick check identify protocols available source destination hosts involved transfer 
stork checks host protocol library see hosts involved transfer library 
stork tries connect particular hosts different data transfer protocols determine availability specific protocol particular host 
stork creates list protocols available host stores lists library host name quest ncsa uiuc edu supported protocols gridftp ftp host name cs wisc edu supported protocols gridftp ftp protocols specified source destination urls request fail perform transfer stork start trying protocols host protocol library carry transfer 
users option specify particular protocols request letting stork decide protocol run time dap type transfer src url slic sdsc edu tmp foo dat dest url quest ncsa uiuc edu tmp foo dat example stork select available protocols source destination hosts perform transfer 
users need care hosts support protocols 
just send request stork transfer file host stork take care deciding protocol 
users provide preferred list alternative protocols transfer 
case protocols list protocols library stork dap type transfer src url slic sdsc edu tmp foo dat dest url quest ncsa uiuc edu tmp foo dat alt protocols nest nest example user asks stork perform transfer slic sdsc edu quest ncsa uiuc edu protocol primarily 
user instructs stork nest gridftp protocols case protocol 
stork try perform transfer protocol 
case failure switch alternative protocols try complete transfer successfully 
primary protocol available stork switch 
whichever protocol available successfully complete user request 

failure recovery efficient resource utilization stork hides kind network storage system middleware software failures user applications 
retry mechanism retry failing data placement job number times returning failure 
kill restart mechanism allows users specify maximum allowable run time data placement jobs 
job execution time exceeds specified time killed stork automatically restarted 
feature overcomes bugs systems cause transfers hang forever return 
repeated number times specified user 
stork control number concurrent requests coming storage system access sure storage system network link storage system get overloaded 
perform space allocation sure required storage space available corresponding storage system 
space reservations supported stork long corresponding storage systems support 

case studies show applicability contributions stork case studies 
case study shows stork create data pipeline heterogeneous storage systems 
case stork transfer data mass storage systems common interface 
done fully automatically failures course transfers recovered human interaction 
second case study shows stork run time adaptation data transfers 
data transfer particular protocol fails stork uses protocols available successfully complete transfer 

building data pipelines ncsa scientists wanted transfer digital sky survey image data residing srb mass storage system sdsc california unitree mass storage system ncsa illinois 
total data size tb files gb 

transfer steps 
nodes representing steps single transfer combined giant dag perform transfers srb unitree data pipeline 
concurrency level 
direct interface srb unitree time experiment way perform data transfer storage systems build data pipeline 
purpose designed stork 
pipeline set cache nodes source destination storage systems 
cache node slic sdsc edu sdsc site close srb server second cache node quest ncsa uiuc edu ncsa site near unitree server 
pipeline configuration allowed transfer data srb server sdsc cache node underlying protocol srb sdsc cache node ncsa cache node third party transfers ncsa cache node unitree server underlying protocol unitree 
ncsa cache node gb local disk space store image files space 
implied done file cache node remove create space transfer file 
including removal step file transfer file consisted basic steps considered real jobs submitted condor stork scheduling systems 
steps represented nodes dag arcs representing dependencies steps 
node dags joined form giant dag shown 
process managed dagman 
srb server unitree server sdsc cache node gigabit ethernet mb interface cards installed 
ncsa cache node fast ethernet mb interface card installed 
bottleneck link fast ethernet interface card 
automated failure recovery case network cache node software problems 
transfers recovered automatically despite possible failures occurring uw cs network goes sdsc cache node goes stops responding 
ncsa cache node 
got transfer rate mb srb server unitree server 
study shown successfully build data pipeline heterogeneous systems srb unitree 
fully automated operation pipeline successfully transferred terabytes data srb server unitree server human interaction 
transfers srb unitree wide variety failures 
times source destination mass storage systems stopped accepting new transfers due software failures scheduled maintenance activity 
wide area network outages software upgrades 
third party transfer hang 
failures recovered automatically transfers completed successfully human interaction 
shows multiple failures occurring 
dynamic protocol selection 
server running sdsc machine gets killed twice points gets restarted points 
cases stork employed available protocol gridftp case complete transfers 
course transfers 
sdsc cache machine rebooted uw cs network outage disconnected management site execution sites couple hours 
pipeline automatically recovered failures 
server stopped responding couple hours 
problem partially caused network reconfiguration hosting server 
automatic failure recovery worked fine 

run time adaptation data transfers submitted data transfer requests stork server running university wisconsin 
cs wisc edu 
request consisted transfer gb image file total gb sdsc slic sdsc edu ncsa quest ncsa uiuc edu protocol 
server installed sl net responsible routing transfers 
gridftp servers running sdsc ncsa sites enabled third party gridftp transfers necessary 
experiment gridftp services available 
stork started transferring files sdsc ncsa protocol directed user 
killed server running intentionally 
stork immediately switched protocols continued transfers gridftp interruption 
switching gridftp caused decrease performance transfers shown 
reasons decrease performance fact gridftp perform auto tuning 
experiment set number parallel streams gridftp transfers perform tuning disk block size tcp buffer size 
performs auto tuning network parameters including number order fully utilize available bandwidth 
sophisticated routing achieve better performance 
letting stork alternative protocol case gridftp perform transfers restarted server sdsc site 
time stork switched back transfers preferred protocol user 
switching back faster protocol resulted increase performance 
repeated couple times observed system behaved way time 
experiment shows alternate protocol capability grid data placement jobs new high performance protocols switch robust lower performance protocol high performance fails 

planning enhance interaction stork higher level planners computational schedulers 
result scheduling computational data resources allow users resources efficiently 
currently scheduling data placement activities stork performed file level 
users move complete files 
planning add support data level block level scheduling 
way users able schedule movements partial files specific blocks file 
planning add intelligence adaptation transfers 
different data transfer protocols may different optimum concurrency levels source destination nodes 
stork able decide concurrency level transfers performing consideration source destination nodes transfer link importantly protocol performing transfers 
case availability multiple protocols transfer data different nodes stork able choose best performance reliable user preferences 
stork able decide path ideally optimum transfer data enhanced integration tool 
able select nodes deployed start nodes transfer data optimizing path network utilization 
enhancement done adding checkpointing support data placement jobs 
transfer fails started scratch remaining parts file transfered 

introduced specialized scheduler data placement activities grid 
data placement efforts done manually simple scripts regarded class citizens just computational jobs 
queued scheduled monitored managed fault tolerant manner 
showed current data placement efforts grid stork provide solutions 
introduced framework computational data placement jobs treated scheduled differently corresponding schedulers management type jobs performed higher level planners 
case studies shown applicability contributions stork 
stork transfer data heterogeneous systems fully automatically 
recover storage system network software failures human interaction 
dynamically adapt data placement jobs environment execution time 
shown generates better performance results dynamically switching alternative protocols case failure 
allcock chervenak foster kesselman ke 
secure efficient data transport replica management high pe data intensive computing 
ieee mass storage conference san diego ca april 
baru moore rajasekar wan 
sdsc storage resource broker 
proceedings cascon toronto canada 
bird hess kowalski 
building mass storage system jefferson lab 
proceedings th ieee symposium mass storage systems san diego california april 
butler pennington 
mass storage ncsa sgi hp unitree 
proceedings th cray user group conference 
cms 
compact muon project 
cern ch 
condor 
directed acyclic graph manager 
www cs wisc edu condor dagman 
gal de carvalho brunner longo 
digital sky survey 
wide field surveys cosmology 
feng 
high performance transport protocols 
los alamos national laboratory 

mass storage system 
www gov docs products 
foster kesselman tuecke 
anatomy grid enabling scalable virtual organizations 
international journal supercomputing applications 
foster 
globus toolkit grid architecture 
grid blueprints new computing infrastructure pages morgan kaufmann 
frey tannenbaum foster tuecke 
condor computation management agent multi institutional grids 
proceedings tenth ieee symposium high performance distributed computing san francisco california august 
livny 
flexible infrastructure high performance large scale data transfers 
technical report cs tr university wisconsin 
moe 
lightweight data replicator 
www lsc group phys edu ldr overview html 
ligo 
laser interferometer gravitational wave observatory 
www ligo caltech edu 
litzkow livny mutka 
condor hunter idle workstations 
proceedings th international conference distributed computing systems pages 
allcock 
reliable file transfer service 
www unix mcs anl gov main html 
postel 
ftp file transfer protocol specification 
rfc 
ppdg 
ppdg deliverables cms 
www ppdg net archives ppdg doc doc 
raman livny solomon 
matchmaking distributed resource management high throughput computing 
proceedings seventh ieee international symposium high performance distributed computing hpdc chicago illinois july 
sdsc 
high performance storage system hpss 
www sdsc edu hpss 
sim gu 
storage resource managers middleware components grid storage 
nineteenth ieee symposium mass storage systems 
thain livny 
ethernet approach grid computing 
proceedings twelfth ieee symposium high performance distributed computing seattle washington june 
thain son livny 
kangaroo approach data movement grid 
proceedings tenth ieee symposium high performance distributed computing san francisco california august 

