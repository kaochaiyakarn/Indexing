discriminative learning label sequences boosting altun thomas hofmann mark johnson department computer science department cognitive linguistics sciences brown university providence ri cs brown edu mark johnson brown edu investigates boosting approach discriminative learning label sequences sequence rank loss function 
proposed method combines advantages boosting schemes eciency dynamic programming methods attractive conceptually computationally 
addition discuss alternative approaches hamming loss label sequences 
sequence boosting algorithm ers interesting alternative methods hmms proposed conditional random fields 
applications areas technique range natural language processing information extraction computational biology 
include experiments named entity recognition part speech tagging demonstrate validity competitiveness approach 
problem annotating segmenting observation sequences arises applications variety scienti disciplines prominently natural language processing speech recognition computational biology 
known applications include part speech pos tagging named entity classi cation information extraction text segmentation phoneme classi cation text speech processing problems protein homology detection secondary structure prediction gene classi cation computational biology 
predominant formalism modeling predicting label sequences hidden markov models hmms variations thereof 
despite success generative probabilistic models hmms special case major shortcomings rst point 
generative probabilistic models typically trained maximum likelihood estimation mle joint sampling model observation label sequences 
emphasized frequently mle joint probability model inherently non discriminative may lead suboptimal prediction accuracy 
secondly ecient inference learning setting requires questionable conditional independence assumptions 
precisely case hmms assumed markov blanket hidden label variable time step consists previous labels th observation 
implies dependencies past observations mediated neighboring labels 
investigate discriminative learning methods learning label sequences 
line research continues previous approaches learning conditional models conditional random fields crfs discriminative re ranking 
crfs main advantages compared hmms trained discriminatively maximizing conditional pseudo likelihood criterion exible modeling additional dependencies direct dependencies th label past observations 
strongly believe lines research worth pursuing may er additional bene ts improvements 
main emphasis exponential loss function boosting algorithms may preferable logarithmic loss function crfs 
particular boosting algorithm additional advantage performing implicit feature selection typically resulting sparse models 
important model regularization reasons eciency high dimensional feature spaces 
secondly discuss loss functions explicitly minimize zero loss labels hamming loss alternative loss functions ranking predicting entire label sequences 
additive models exponential families formally learning label sequences generalization standard supervised classi cation problem 
goal learn discriminant function sequences mapping observation sequences label sequences 
availability training set labeled sequences ng learn mapping data assumed 
focus discriminant functions written additive models 
models consideration take general form denotes discrete feature language maximum entropy modeling weak learner language boosting 
context label sequences typically form 
rst type features model dependencies observation sequence th label sequence second type model inter label dependencies neighboring label variables 
ease presentation assume features binary learner corresponds indicator function 
typical way de ning set weak learners follows denotes kronecker binary feature function extracts feature observation pattern refer label values weak learner active 
natural way associate conditional probability distribution label sequences additive model de ning exponential family xed observation sequence yjx exp exp distribution exponential normal form parameters called natural canonical parameters 
performing sum sequence index see corresponding sucient statistics 
sucient statistics simply count number times feature active labeled sequence 
logarithmic loss conditional random fields crfs log loss model parameters set sequences de ned negative sum conditional probabilities training label sequence observation sequence log log jx log proposed modi cation improved iterative scaling parameter estimation crfs gradient methods conjugate gradient descent ecient minimizing convex loss function eq 
cf 

gradient readily computed log jx expectations taken yjx 
stationary equations simply state uniformly averaged training data observed sucient statistics match conditional expectations 
computationally evaluation straightforward counting summing sequences compute jx performed dynamic programming dependency structure labels simple chain 
ranking loss functions label sequences alternative logarithmic loss functions propose minimize upper bound ranking loss adapted label sequences 
ranking loss discriminant function set training sequences de ned rnk simply sum number label sequences ranked higher equal true label sequence training sequences 
straightforward see term term comparison upper bound rank loss exponential loss function exp exp jx interestingly simply leads loss function uses inverse conditional probability true label sequence de ne probability exponential form eq 

notice compared include sequences just top list generated external mechanism 
show shortly explicit summation possible availability dynamic programming formulation compute sums sequences eciently 
order derive gradient equations exponential loss simply elementary facts log log easy see exp jx jx di erence eq 
eq 
non uniform weighting di erent sequences inverse probability putting emphasis training label sequences receive small conditional probability 
boosting algorithm label sequences alternative simple gradient method turn derivation boosting algorithm boosting formulation 
introduce relative weight distribution label sequence training instance exp exp yjx jx jx jx addition de ne 
eq 
shows split relative weight training instance relative weight sequence re normalized conditional probability yjx 
notice approach perfect prediction case jx 
de ne boosting algorithm round aims minimizing partition function weight normalization constant weak learner corresponding optimal parameter increment yjx jx exp bjx exp bjx yjx jx fy bg 
minimization problem tractable number features small dynamic programming run accumulators feature required order compute probabilities bjx probability th feature active exactly times conditioned observation sequence cases intractable assume case applications minimize upper bound general idea exploit convexity exponential function bound exp exp min max max min exp max min max min valid min max 
introduce shorthand notation ik max ik max ik max max max ik min ik min ik min min min ik yjx jx allows rewrite exp ik max ik ik max ik min ik ku min ik ik min ik max ik min ik ku max ik ik ku min ik ik ku max ik ik max ik ik max ik min ik second derivative easy verify convex function minimized simple line search 
willing accept looser bound interval min max union intervals min ik max ik training sequence obtain upper bound ku min ku max max ik max min solved analytically max min log min max general lead conservative step sizes 
nal boosting procedure picks round feature upper bound minimal performs update course elaborate techniques nd optimal selected upper bound approximation may underestimate optimal step sizes 
important see quantities involved ik respectively simple expectations sucient statistics computed features simultaneously single dynamic programming run sequence 
hamming loss label sequences applications primarily interested label label loss hamming loss 
investigate train models minimizing upper bound hamming loss 
logarithmic loss aims maximizing log probability individual label log log jx log yjx focusing gradient descent methods gradient log jx jx seen expected sucient statistics compared empirical values expected values conditioned label value entire sequence 
order evaluate expectations perform dynamic programming algorithm described independently focused hamming loss functions context crfs 
algorithm complexity forward backward algorithm scaled constant 
similar log loss case de ne exponential loss function corresponds margin quantity single label 
propose minimizing loss function exp exp log exp exp exp jx motivation point case sequences length reduce standard multi class exponential loss 
ectively model prediction label mimic probabilistic marginalization arg max log exp similar log loss case gradient exp jx jx jx see di erences log loss exponential loss time individual labels 
labels marginal probability jx small accentuated exponential loss 
computational complexity computing exp log practically 
able derive boosting formulation loss function mainly written sum exponential terms 
resorted conjugate gradient descent methods minimizing exp experiments 
experimental results named entity recognition named entity recognition ner subtask information extraction task nding phrases contain person location organization names times quantities 
word tagged type name position name phrase rst item phrase order represent boundary information 
spanish corpus provided special session conll ner 
data collection news wire articles tagged person names organizations locations miscellaneous names 
simple binary features ask questions word tagged previous tag hmm features 
example feature current word clinton tag person 
features ask detailed questions spelling features current word current word capitalized tag location intermediate 
neighboring words 
questions asked principled way generative hmm model 
ran experiments comparing di erent loss functions optimized conjugate gradient method boosting algorithm 
designed sets features hmm features detailed features current word detailed features neighboring words 
feature objective set log exp boost table test error spanish corpus named entity recognition 
results summarized table demonstrate competitiveness proposed loss functions respect log observe di erent sets features ordering performance loss functions changes 
boosting performs worse conjugate gradient hmm features information features identity word labeled 
consequently boosting algorithm needs include weak learners ensemble exploit feature sparseness 
detailed features boosting algorithm competitive conjugate gradient method advantage generating sparser models 
conjugate gradient method uses available features boosting uses features 
part speech tagging feature objective set log exp boost table test error penn treebank corpus pos penn treebank corpus part speech tagging experiments 
features similar feature sets described context ner 
table summarizes experimental results obtained task 
seen test errors obtained di erent loss functions lie relatively small range 
qualitatively behavior di erent optimization methods comparable ner experiments 
general comments tighter bound boosting formulation features selected times conservative estimate step size parameter updates 
expect speed convergence boosting algorithm sophisticated line search mechanism compute optimal step length conjecture addressed 
real valued features experiments observed including real valued features conjugate gradient formulation challenge natural features boosting algorithm 
noticed experiments de ning distribution training instances inverse conditional probability creates problems boosting formulation data sets highly unbalanced terms length training sequences 
overcome problem divided sentences pieces variation length sentences small 
conjugate gradient optimization hand appear su er problem 
contributions problem learning label sequences 
ecient algorithm discriminative learning label sequences combines boosting dynamic programming 
algorithm compares favorably best previous approach conditional random fields ers additional bene ts model sparseness 
secondly discussed methods optimize label label loss shown methods bear promise improving classi cation accuracy 
investigate performance accuracy computational expenses di erent loss functions di erent conditions noise level size feature set 
acknowledgments sponsored nsf itr award number iis 
collins 
discriminative reranking natural language parsing 
proceedings th international conference machine learning pages 
morgan kaufmann san francisco ca 
collins 
ranking algorithms named entity extraction boosting voted perceptron 
proceedings th annual meeting association computational linguistics acl pages 
durbin eddy krogh mitchison 
biological sequence analysis probabilistic models proteins nucleic acids 
cambridge university press 
friedman hastie tibshirani 
additive logistic regression statistical view boosting 
annals statistics 
teh roweis 
alternative objective function markovian elds 
proceedings th international conference machine learning 
la erty mccallum pereira 
conditional random elds probabilistic models segmenting labeling sequence data 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
manning sch 
foundations statistical natural language processing 
mit press 
minka 
algorithms maximum likelihood logistic regression 
technical report cmu department statistics tr 
schapire singer 
improved boosting algorithms con dence rated predictions 
machine learning 
