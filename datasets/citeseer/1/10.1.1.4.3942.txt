automatic bayes carpentry unlabeled data semi supervised classification hui zou department statistics stanford university stanford ca stat stanford edu ji zhu department statistics university michigan ann arbor mi umich edu trevor hastie department statistics stanford university stanford ca hastie stat stanford edu semi supervised classification important question enormous amount unlabeled data improve classifier built labeled data 
point certain situations labeled data collected biased sampling random sampling supervised learning algorithms longer bayes consistent due inherent bias 
show unlabeled data adaptively infer sampling scheme estimate bias 
inferred result propose universal carpentry technique ensures bayes consistency popular supervised classifiers including boosting kernel machines 
direct bias correction formula estimation calibrated classifiers 
demonstrate efficacy proposed carpentry methods simulation 
standard class classification problem denote class label xj 
features 
classifier measurable function feature space classification rule assign label sign observation features primary goal classification achieve minimum misclassification error unseen data 
loss smallest error bayes rule usually unknown 
supervised learning builds classifier training data set 
order approximate bayes rule supervised classification usually requires fairly large set training data expensive applications 
example text documents classification problem enormous amount unlabeled documents cheaply obtained unlabeled documents chosen labeled human experts training data supervised classifier 
labeling document time consuming costly size labeled documents set limited 
kind scenario natural ask huge amount unlabeled documents help classification 
optimistic view machine learning literature 
example nigam mccallum combined expectation maximization em algorithm naive bayes classifier learning labeled unlabeled documents text classifications 
claimed unlabeled data reduced classification error 
popular algorithm transductive support vector machines 
justification combining labeled unlabeled data unclear 
cozman showed semi supervised learning mixture models unlabeled data lead increase classification error model assumptions exactly satisfied 
zhang oles pointed transductive svm unreliable may maximize wrong margin declared success reported literature due specific experimental setups general advantage transductive svm versus supervised svm 
considering opposite opinions feel safe say combine unlabeled data labeled data open problem semi supervised learning 
study unlabeled data different point view 
consider scenario labeled data collected biased sampling random sampling 
biased sampling occurs supervised learning algorithms produce inconsistent classifiers due inherent bias caused biased sampling 
propose unlabeled data algorithms adaptively eliminate bias guaranteeing bayes consistency classifiers 
unlabeled data helpful 
call property automatic bayes carpentry 
section expose role random sampling assumption consistency modern supervised classifiers show inconsistency caused biased sampling 
section describes universal carpentry technique popular supervised classifiers including boosting kernel machines 
direct bias correction formula available estimation calibrated classifiers 
section illustrate proposed carpentry methods simulation 
random sampling vs biased sampling delving details automatic bayes carpentry techniques briefly review consistency result supervised classifiers issue biased sampling 
probability measure log suppose distribution generating data classification problem sign bayes rule 
classification error classifier assuming loss rf sign dp 
classification error sign denoted 
consistency random sampling learning algorithm produces classifier fn training data yi xi 
fn said bayes consistent fn probability 
typically training data yi xi assumed samples referred random sampling literature 
bayes consistency fn studied random sampling assumption 
assumed true care random sampling assumption crucial consistency fn 
illustrate point clearly examine consistency popular classifiers practice kernel machines boosting 
classifiers derived loss paradigm fn arg min yi xi nj 
usually convex surrogate loss ensure computationally tractable algorithm 
regularization term hk hk rkhs reproducing kernel wahba kernel machine 
example support vector machine svm uses hinge loss reproducing kernel 
boosting minimizes em risk different regularization method 
ht 
dictionary basis functions tht shown idealized version boosting 
exact form regularization crucial arguments 
definition consider infinity sample risk expectation taken respect probability measure 
arg min 
sign sign said classification calibrated 
classification calibrated loss notion adopted bartlett named fisher consistency lin 
shown random sampling assumption classification calibrated condition sufficient consistency fn 
arguments understood follows 
empirical risk converges expectation taken respect distribution training data 
random sampling assumption implies infinity sample risk 
target function fn tries estimate 
technical conditions vanishes right rate hk rich ensure convergence fn rf note sign bayes rule tar get distribution classification calibrated condition 
fn bayes consistent 
problem biased sampling shown previous subsection random sampling decides target function fn classification calibrated condition ensures target function bayes rule 
turns popular convex loss practice classification calibrated 
random sampling assumption necessarily true situations 
sample sizes classes respectively 
set experimenter random variables faithfully representing 
demonstrate argument considering retrospective sampling frequently medical studies 
suppose class indicates disease class means healthy 
features medical measurements person 
disease rare event random sampling inefficient gathering data study relation disease efficient retrospective sampling conducted patients included similar sized sample healthy people 
described sampling scheme called biased sampling generally number samples randomly drawn class weight class corresponds chosen prior recall labeled data semi supervised learning labeled human true prior random sampling assumption violated 
pc distribution collected training samples 
label features follow true conditional distribution 
pc pc pc 
define log pc log pc 
suppose algorithm subsection build fn 
note pc distribution training data limit empirical risk dpc 
classification calibrated sign minimizer equal sign 
fn approximates sign 
true distribution bayes rule sign implies fn approaches wrong target function 
see log log 
bias due biased sampling 
shows bias independent 
bayes rule unique 
arrive biased sampling fn asymptotically sub optimal 
important note bias inherent reasons 
bias exists algorithm consistent random sampling secondly bias removed labeled data learner extra knowledge 
automatic bayes carpentry section show bias eliminated uses unlabeled data appropriately 
sense unlabeled data helpful 
inferring correct prior unlabeled data construct unlabeled data probability 
consider nonparametric estimate moment equations 
evaluate expectation function ep dx 
bayes theorem ep 
identity ep ep 
ep ep empirical distribution sample moments unlabeled data give consistent estimate ep consistently estimated labeled data 
simple consistent estimate 
moment estimate fully non parametric requiring assumption 
worth pointing subset avoiding curse dimensionality estimating 
quality estimate may depend choice possible take average estimates different functions get stable estimate 
universal carpentry weighted loss consistent obtained bayes consistency ensured weighted loss technique long loss classification calibrated 
theorem suppose estimate 
denote number labeled data class respectively 
classification calibrated loss define arg min yi fi nj assuming technical conditions stated subsection bayes consistent 
proof rewrite weighted loss yi fi fi yi yi fi yi yi fi fi ec ec ec means expectation pc 
pc yi fi target distribution 
right target function 
rest consistency proof random sampling case 
weighted loss technique root statistical theory 
note weights estimates likelihood ratio yi xi pc yi xi works importance sampling 
straightforward show weighted loss technique works case biased sampling sampling scheme long consistently estimate likelihood ratio 
vardi considered non parametric maximum likelihood estimate cdf basis samples weighted version known weight function 
vardi method replaces usual non parametric maximum likelihood criterion weighted version 
nearly finishing noted lin proposed similar technique svms nonstandard situations 
weights assumed known 
additional significant contributions works classification calibrated loss functions limited hinge loss svm weights adaptively provided unlabeled data 
estimation calibrated loss practice loss functions sign agrees bayes rule related deterministic function 
definition loss function said estimation calibrated continuous nondecreasing function 
shown technical conditions subsection fn converges probability 
hand know pc pc facts suggest bias correction method 
proof basic probability arguments omitted 
lemma suppose estimation calibrated sign fn bayes consistent assuming technical conditions subsection easy see estimation calibrated loss function reverse true 
example hinge loss population minimizer exactly bayes rule providing information 
regarded major drawback svms 
lemma describes family estimation calibrated loss functions including popular smooth loss functions practice gives explicit expression 
lemma applicable result 
lemma margin loss function yf 
suppose convex continuous estimation calibrated 
sketch proof necessary condition note estimation calibrated classification calibrated convexity requires theorem bartlett 
sufficient condition 
inf empty set 
implies 
fix minimizes 
differentiating 
show increasing 

note define follows 
desired function 
experiment section simulation demonstrate efficacy bayes carpentry techniques 
setup chap designed mixture gaussians bayes decision boundary nonlinear 
set simulated unlabeled data 
generated labeled data points class 
svm kernel logistic regression klr classifiers fitted labeled data 
unlabeled data help estimate constructed modified svm klr classifiers carpentry methods 
classifier test error rate calculated true model 
process repeated times 
seen table naive uses svm klr ignoring biased psfrag replacements klr radial kernel table error rates svm klr carpentry 
bayes svm klr table error rates svm klr carpentry 
klr ec exp sampling undesirable classification errors bayes error 
table shows results carpentry 
weighted svm weighted klr better job misclassification error 
klr estimation calibrated bias correction klr ec achieves low error rate close bayes error 
tried different functions carpentry methods 
results similar 
displays decision boundaries svm klr classifiers carpentry 
clearly visualizes effect carpentry 
svm radial kernel bayes svm psfrag replacements svm radial kernel klr radial kernel klr ec bayes boundaries carpentry compared bayes boundary 
effect carpentry transparent 
small triangles circles indicate class respectively 
summary emphasized importance sampling scheme learning considered nuisance learning algorithms 
proved inconsistency popular classifiers caused sampling bias 
semi supervised learning advantage unlabeled data adaptively non parametrically infer sampling klr bias 
general carpentry techniques proposed guarantee bayes consistency classifier 
demonstrated efficacy carpentry methods simulation 
cautious sampling distribution target distribution learning importance sampling idea help correct sampling bias 
weights importance sampling unknown may consider consistent estimates 
semi supervising learning enormous amount unlabeled data useful estimating weights 
suggest infinity unlabeled data framework studying value unlabeled data 
suppose learning algorithm labeled data 
assume known question construct new algorithm qp labeled data qp lower asymptotic error rate 
obtained tentative interesting results 
bartlett jordan mcauliffe 
convexity classification risk bounds 
technical report division computer science department statistics berkeley 
cozman cohen 
semi supervised learning mixture models 
proceedings twentieth international conference machine learning 
hastie tibshirani friedman 
elements statistical learning 
springer verlag new york 
joachims 
transductive inference text classification support vector machines 
proceedings sixteenth international conference machine learning 
san francisco morgan kaufmann 
lin 
note margin loss functions classification 
technical report department statistics university wisconsin madison 
lin lee wahba 
support vector machines classification nonstandard situations 
machine learning lugosi 
bayes risk consistency regularized boosting methods 
annals statistics 
mccullagh nelder 
generalized linear models nd edition 
chapman hall 
nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning 
rosset zhu hastie 

boosting regularized path maximum margin classifier 
journal machine learning research appear 


consistency support vector machines regularized kernel classifiers 
technical report los alamos national laboratory los alamos nm vardi 
empirical distributions selection bias models 
annals statistics 
wahba 
support vector machine reproducing kernel hilbert spaces randomized 
advances kernel methods support vector learning mit press 
zhang oles 
probability analysis value unlabeled data classification problems 
int 
joint conf 
machine learning 
zhang 
statistical behavior consistency classification methods convex risk minimization 
annals statistics 
zhu hastie kernel logistic regression import vector machine 
advances neural information processing systems 
mit press cambridge ma 
