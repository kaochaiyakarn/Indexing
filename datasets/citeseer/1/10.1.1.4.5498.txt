effects dimensionality data analysis neural networks verleysen fran ois simon universit catholique de louvain dice place du louvain la neuve belgium av 
louvain la neuve belgium verleysen simon dice ucl ac francois auto ucl ac 
modern data analysis faces high dimensional data 
neural network data analysis tools adapted highdimensional spaces conventional concepts euclidean distance scale poorly dimension 
shows limitations concepts suggests research directions alternative distance definitions non linear dimension reduction 

years data analysis specific discipline far mathematical statistical origin understanding problems limitations coming data valuable developing complex algorithms methods 
specificity modern data mining huge amounts data considered 
new fields data mining crucial medical research financial analysis furthermore collecting huge amount data easier cheaper 
main concern direction dimensionality data 
think measurement data observation observation composed set variables 
different analyze observations variables analyzing observations variables 
way get feeling difficulty imagine observation point space dimension number variables 
observations dimensional space probably form structured shape clouds possible extract relevant information principal directions variances clouds contrary sight observations dimensional space represent specific number observations low 
modern data unpleasant characteristic highdimensional 
despite difficulties ways analyze data mv senior research associate belgian 
gs funded belgian 
df vw supported attraction pole initiated belgian federal state ministry sciences technologies culture 
scientific responsibility rests authors 
mira ed lncs pp 

springer verlag berlin heidelberg verleysen extract information observations 
current data analysis methodologies adapted high dimensional sparse data duty develop adapted methods admitted concepts questioned 
particular artificial neural network methods widely successfully data analysis faced high dimensional data modified necessary 
presenting generic solutions problem current state art far 
illustrate surprising facts section high dimensional data general neural networks context section 
particular show standard notions euclidean distance nearest neighbor generally similarity search adapted high dimensional spaces 
need alternative solutions gives paths developments section new research activity influence considerably field neural networks data mining years 

weird facts high dimensional space high dimensional spaces fact escape mental representations 
take granted dimension quite easily hold higher dimensions 
highlight weird facts 

empty space phenomenon scott thompson noticed counter intuitive facts related high dimensional euclidean spaces described called empty space phenomenon 
fact 
volume hyper sphere unit radius goes zero dimension 
volume sphere radius dimensions 
shows volume see volume rapidly decreases higher dimension unit sphere nearly empty 
fact 
ratio volumes sphere cube radius tend zero increasing dimension illustrated 
dimension volumes equal dimensions ratio approximately higher dimension say volume hyper cube concentrate corners 
fact 
ratio volume sphere radius tend zero obvious fact value equal power small original radius contains volume outer sphere volume concentrates outer shell 
holds hyper cubes hyper ellipsoids 
fig 

left volume unit sphere right ratio volumes unit sphere unit cube respect dimension space 
observations imply high dimensional spaces empty 
show local neighborhoods points empty case uniform distributions data concentrated borders volume interest 

concentration measure phenomenon deeper look behavior widely euclidean distance norm difference applied high dimensional vectors 
fact 
standard deviation norm random vectors converges constant dimension increases expectation norm square root dimension 
precisely proven assumption xi var constants depending xi 
note law applies euclidean distance points happens random vector 
fact 
difference distances randomly chosen point furthest nearest neighbor decreases dimensionality increases 
illustrated asymptotic behavior relative contrast effects dimensionality data analysis neural networks lim var dmax dmin dmin dmin dmax distance respectively nearest furthest neighbors particular point :10.1.1.23.7409
note hypothesis theorem induced equation 
general proof theorem 
verleysen draw observations high dimensional spaces points tend equally distant respect euclidean distance 
dimension increase observed distance points tends constant 
illustrated computing histograms distances random points increasing dimensionality 
appears mean histogram variance shrinks 

curse dimensionality look weird ignored fact richard bellman named curse dimensionality 
refers huge amount points necessary high dimensions cover input space example regular grid spanning certain portion space 
filling hypercube dimensions spaced grid needs points 

consequences neural network learning considerations developed previous section important consequences ann artificial neural networks learning 
subsections give examples consequences specific contexts 

supervised learning modeling process producing output basis observed values particular inputs fit chosen model dataset 
extensive dataset accurate model 
ideally dataset span input space interest order ensure predicted value output model result interpolation process hazardous extrapolation occurs 
face curse dimensionality 
silverman addressed problem finding necessary number training points samples approximate gaussian distribution fixed gaussian kernels 
results show required number samples grows exponentially dimension 
fukunaga obtained similar results nn classifier showing observations sufficient dimensions necessary dimension 

local models local artificial neural networks argued sensitive dimensionality global ones 
local models mean approximators classifiers density estimators combination local functions example gaussian kernels 
gaussian functions unexpected effects dimensionality data analysis neural networks behavior extended high dimensional spaces 
examples approximators rbfn radial basis function networks kernel methods 
normal distribution standard deviation assumed probability density function find point distance center distribution maximum dimension maximum center distribution expected dimension diverges center see nearly empty gaussian distribution maximal shows gaussian kernels local higher dimensions models seen sums local kernels behave high dimensions 
fig 

probability density point normal distribution distance center space dimensions 
fig 

example distance histogram clusters distributions 
limitation local models particular gaussian kernels severe 
emphasized fact global models example mlp multi layer perceptrons probably equally suffer 
cases sums sigmoids mlp result functions significant values limited region spaces 
mathematically different models mlp rbf behave similarly practice 
enforces conviction local global models equally suffer curse dimensionality related effects probably harder prove global models 

similarity search euclidean distances neural network models clustering techniques rely computation distances vectors 
rbfn distance data kernel center 
mlp scalar product data verleysen weight input layer 
distance measures may related similarity search clustering techniques vector quantization lvq kohonen maps similarity search consists finding dataset closest element point 
context clustering example efficient clustering achieved data cluster similar close respect distance function data different clusters far away 
data contain clusters distance histograms ideally show peaks intra cluster distances extra cluster distances 
distance histogram contains peak peaks close clustering difficult 
unfortunately fact high dimensions distance histogram tends concentrated peak making clustering task uneasy 
direct consequence concentration measure phenomenon 

solutions effects curse dimensionality related limitations neural network learning unavoidable high dimensional spaces 
paths explore remedy situation 

alternative distance measures euclidean distance data conventional rarely discussed 
obvious definition distance appropriate circumstances particular high dimensional spaces 
practice distance measure vectors components xi yi form considered yi 
practice applicable positive value asymptotical behavior distance definition distances subject concentration phenomenon 
convergence rate differs different values intuition tells high values mitigate effects loss locality gaussian kernels 
shown lower values keep relative contrast high particular dimension :10.1.1.23.7409
unfortunately known reason numerical computation related arguments find lower bound optimal sense remains find proper set lower bound optimal probably dimension dependant compromise 

non linear projection preprocessing way limit effects high dimensionality reduce dimension working space 
data real problems lie near submanifolds input space redundancy variables 
redundancy consequence lack information type input variable helpful case large amount noise unavoidable data coming example measures physical phenomena 
convinced positive just imagine physical quantity measured sensors adding independent gaussian noise measurement averaging measures strongly decrease influence noise measure 
concept applies sensors measure quantities 
projection data submanifolds may help 
way project data standard pca principal component analysis 
pca linear cases submanifolds linear think example horseshoe distribution pca efficient 
alternative nonlinear methods exist project data nonlinear way 
examples kohonen self organizing maps usually project data twodimensional spaces methods distance preservation 
include multi dimensional scaling sammon mapping curvilinear component analysis extensions 
methods principle data points dimensional space try place points projection space keeping mutual distances pair points unchanged input space corresponding pair projection space 
course having condition strictly fulfilled impossible generic case conditions satisfy nm degrees freedom methods weight conditions shorter distances satisfied strictly large distances 
weighting aims conserving local topology locally sets input points resemble sets output points 
example successful application approach context financial prediction 

effects dimensionality data analysis neural networks theoretical considerations show classical concepts data analysis neural networks process high dimensional data may appropriate 
reason underlying hypotheses obvious lower dimension verified higher dimensions 
practice observes severe performance loss data processing algorithms data high dimensional 
need adapt models high dimensionality 
way think consider new similarity measures data ancestral euclidean distance 
way reduce dimension projection verleysen 
non linear submanifolds 
cases deep investigation required order successfully adapt data processing tools high dimensional data 

scott thompson probability density estimation higher dimensions 
douglas 
ed computer science statistics 
proceedings fifteenth symposium interface north holland elsevier amsterdam new york oxford 
analyse de donn es par de neurones auto ph dissertation french institut national polytechnique de grenoble france 
aggarwal hinneburg keim surprising behavior distance metrics high dimensional spaces 
van den bussche vianu 
eds proceedings database theory icdt th international conference lecture notes computer science vol 
springer london uk 
beyer goldstein ramakrishnan shaft nearest neighbor meaningful 
beeri buneman 
eds database theory icdt th international conference 
lecture notes computer sciences vol springer jerusalem israel 
adaptive control processes guided tour 
princeton univ press 
silverman density estimation statistics data analysis 
chapman hall 
fukunaga statistical pattern recognition 
academic press boston ma 
gu rin searching embedded manifolds highdimensional data problems unsolved questions 
proceedings esann european symposium artificial neural networks side public bruges belgium 
steinbach kumar challenges clustering high dimensional data 
new statistical physics applications physics bioinformatics pattern recognition springer verlag verleysen learning high dimensional data 
acc 
public 
gori 
eds limitations trends neural computation ios press 
shepard analysis proximities multidimensional scaling unknown distance function parts ii psychometrika shepard carroll parametric representation nonlinear data structures 
krishnaiah ed international symposium multivariate analysis academic press sammon nonlinear mapping algorithm data structure analysis ieee trans 
computers curvilinear component analysis self organizing neural network nonlinear mapping data sets ieee neural networks 
lee lendasse verleysen curvilinear distance analysis versus isomap 
proceedings esann th european symposium artificial neural networks public bruges belgium 
lendasse lee de verleysen dimension reduction technical indicators prediction financial time series application bel market index 
european journal economic social systems pp 

