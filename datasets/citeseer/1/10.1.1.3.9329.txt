submission osdi 
bridging gap programming sensor networks application specific virtual machines philip levis david gay david culler pal culler cs berkeley edu intel research net eecs department intel research berkeley university california berkeley avenue berkeley ca berkeley ca propose application specific virtual machines method safely efficiently program sensor networks 
sensor networks encompass wide range application domains network supports single 
vm tailored particular deployment provide flexibility application class keeping programs efficient 
mat architecture customizing vms wide range sensor network applications 
customizing instruction set triggering events allows language flexibility provides high code density enables wide range applications 
evaluate mat comparing custom built vms existing proposals user level sensor network programming regions tree aggregation tinydb 
show vm implemented architecture provide equivalent functionality current implementations proposals improving efficiency 
additionally decomposing application domains set reusable fine grained software components implementing new user level programming abstractions greatly simplified 

operating systems research long history designing flexible abstractions 
sensor networks provide new set challenges opportunities area deployment features specific hardware features processing capabilities deployed networks reprogrammable response observed data evolving needs 
deployment targets specific application domain habitat monitoring tracking system environment tailored domain needs 
situ reprogramming efficient safe 
energy limiting resource sensor network stalling new programs executing energy efficient 
sensor networks typically dense fields small nodes embedded environment recovering program induced crash failure rebooting example difficult impossible 
programming system able safely recover buggy badly conceived programs 
need higher level sensor network programming led proposed programming models 
example regions showed data parallel operators concisely represent applications require inter node data aggregation tinydb demonstrated effectiveness declarative sql queries data collection 
proposals steps right direction address limited set application domains address requirements 
tinydb sql general programming model query interpretation program execution inefficient 
regions compiles programs full tinyos image installs entire executable tens kilobytes program installation inefficient safety 
application specific virtual machines provide way capturing application building blocks allowing flexible dynamic composition operating system level 
propose application specific virtual machines intermediate layer application domain mote operating system tinyos 
goal built mat general architecture application specific vms 
user tailors mat vm specific application domain selecting programming language users program set domain specific extensions support planar feature detection 
mat framework compiles programs generated vm instruction set bytecodes vm runtime automatically propagates code network 
code analysis allows vm provide race free layered decomposition situ reprogramming 
deadlock free execution parallelism 
separation layers user program network representation execution engine shown schematically 
bytecodes transmission layer provides safety tuning level abstraction selecting appropriate language set extensions particular application requirements leads efficient execution overhead minimized compiling application specific bytecodes programs small tens bytes keeping propagation efficient 
mat general support existing proposals sensor network programming models 
built vm extensions region operations typical programs order bytes section reduction size originally proposed regions implementation 
built vm performing similar sensor network queries tinydb section energy savings 
implemented languages top mat tinyscript simple basic language motlle language believe engine general support programming languages querying vm language motlle 
increases efficiency come separating layers 
separating transmission layer execution layer reduces cost installing regions program separating transmission layer programming layer reduces cost query execution 
earlier proposed particular virtual machine mechanism reprogram sensor networks imposed limitations ultimately unusable 
extends general framework supports multiple languages programming models applied wide range applications 
section covers design mat architecture 
section presents evaluates vms comparison regions pursuer evader application tinydb various data collection queries 
con module main uses interface stdcontrol module provides interface stdcontrol control uses interface sendmsg configuration provides interface sendmsg uint id configuration components main engine components comm main engine control engine comm sendmsg am error 
nesc wiring examples 
wires main generic comm 
provides sendmsg parameterized interface parameter active message id wiring engine generic comm sendmsg requires specifying message type 
clude discussion implications results section survey related section areas section 

background tinyos popular sensor network operating system runs range limited resource devices motes 
motes need able operate unattended small batteries better part year minimizing energy costs greatly influences design 
correspondingly hardware resources limited 
typical tinyos motes mhz microcontroller kb data ram kb program flash memory radio application level data transmission rates kb energy limitations force sensor networks operate low utilization 
mote limited resources application domains significant limitation 
example great duck island deployments motes minutes sensors second transmitted single data packet readings 
second cpu essentially idle 
exception trend low utilization ram 
ram limitations result market current commercial microcontrollers fundamental technical issues 
larger amounts megabytes significant energy costs motes feature significantly data memory 
nesc language implement tinyos applications provides basic abstractions component programming model low overhead event driven concurrency model 
components units program composition 
component set interfaces requires set interfaces pro mat architecture 
vides 
programmer builds application wiring interface providers see 
nesc supports kinds components modules configurations 
module represents actual program logic implementation configuration wiring subcomponents 
configurations allow subsystem encapsulation 
example tinyos component encapsulates entire networking stack components provides just interfaces power management packet reception packet transmission 
addition basic interfaces nesc parameterized interfaces 
essentially component provide copies interface single copies distinguished parameter value name 
parameterized interfaces support runtime dispatch set components 
tinyos event driven concurrency model allow blocking 
calls long lasting operations sending packet typically split phase call operation returns immediately called component signals event caller operation completes 

design mat principal goal define flexible architecture building application specific sensor network scripting environments 
environment parts programming language sensor network users virtual machine bytecode interpreter motes execute user programs 
virtual machines jvm goal provide fixed abstraction boundary goal allow sensor network developers define boundary level suitable particular application domain 
shows functional decomposition mat vm architecture 
mat vms major abstractions contexts operations capsules 
contexts units concurrent execution operations units execution functionality capsules units code propagation 
vm components fall classes components vm includes basic template components define particular mat instance tailored particular language application domain 
basic vm template includes scheduler concurrency manager capsule store 
scheduler executes runnable contexts fifo round robin fashion 
concurrency manager submits contexts scheduler ready run safely access shared resources require 
tinyos scheduler concurrency manager support blocking operations 
capsule store manages capsule storage loading propagates capsules network notifies higher level components receives new code 
basic template include data storage components 
provided selected language 
mat defines set standard types currently integer sensor reading stack pass values functions see 
possible add additional types mat currently hard language independent fashion 
expect add improved support type extension 
specific vm instance wires set contexts operations basic template 
set contexts defines events trigger vm execution 
set operations defines vm instruction set 
vm bytecode maps operation component implements corresponding operation interface shown 
choosing language vm selects set operations known primitives provide basic features needed language accessing variable arithmetic 
selecting set appropriate contexts functions tailors vm application domain 
functions operations take arguments return result mat stack generally language independent 
examples include functions control timers obtain sensor readings 
functions invocable bytecode language specific mechanisms 
context languageindependent component triggers execution handler response event packet reception 
handler sequence bytecodes stored capsule 
actual mapping handlers capsule code sequences language specific 
rest section presents core components scheduler concurrency manager capsule store interaction application specific languages may include functions 
interface command result execute uint opcode context module uses interface code uint code result execute context fetch opcode 
execute parameterized interface call bytecode execute op op context configuration components vm components vm code op add vm code op send vm code op halt vm code op vm code op get vm code op get 
vm code op get mat scheduler interfaces contexts operations greater depth 
particular show concurrency manager provides deadlock free execution contexts access shared resources 
conclude example building simple vm region programming 
scheduler execution model core mat architecture simple fifo scheduler 
scheduler maintains queue runnable contexts interleaves execution fine granularity operations 
scheduler executes context fetching bytecode capsule store dispatches corresponding operation component typical vm may components 
contains nesc code snippets showing structure allows core vm independent particular instruction set implements 
component implementing context starts response event submitting concurrency manager 
operations wish halt pause requests concurrency manager see 
requests add remove context run queue come concurrency manager 
ability pause context allows functions encapsulate split phase tinyos call synchronous interface mat programmer 
function executes pauses current context concurrency manager 
tinyos completion event fires function component resumes context concurrency manager submits scheduler decides run race free 
concurrency manager parallelism traditionally default behavior concurrent pro gramming environments threads device drivers allow race conditions provide synchronization primitives users protect shared variables 
places onus programmer protect shared resources 
return skilled systems programmer fine tune primitives maximum cpu performance 
mat takes opposite approach embedded systems event driven difficult debug demand greater robustness 
vm installs new capsule runs conservative program analysis determine set shared resources capsule handlers resources corresponding contexts need 
mat concurrency model statically named resources shared variables 
operations specify shared resources analysis determines handlers complete resource usage language specific 
conservative form analysis assumes handlers share resources motlle language takes approach 
precludes possible parallelism 
section describes tinyscript resource model analysis 
simplest instantiation model context acquires resources begins releases ends 
concurrency manager receives request run context checks resources context require available 
submits context scheduler 
places context wait queue 
context releases resources concurrency manager checks wait queue submits runnable contexts scheduler 
sensor network applications typically low utilization starvation issue 
handlers scheduling points improve parallelism 
certain vm operations yield function functions pause contexts scheduling points 
scheduling point set resources rs releases set resources acquires subset rs 
default rs 
handler temporarily relinquish resource duration scheduling point adding rs 
permanently relinquish resource adding rs 
currently tinyscript motlle support scheduling points written assembly programs functionality 
inequality rs sufficient building deadlock data race free scheduler assuming user temporarily relinquish resource scheduling point needs atomic access scheduling point 
race free behavior simple context access resources hold handlers may access resource run concurrently 
include proof deadlock freeness appendix 
dynamic code updates complicate race free execution 
state diagram mat capsule propagation 
terminating context new new code arrived leave data inconsistent state 

waiting complete may option old version infinite loop 
new capsule arrives vm reboots clearing existing state 
applications require persistent state include functions atomically store load state 
capsule store propagation initial mat vm forwarded programs imperative forw instruction broadcast code fragments 
single copy self forwarding program autonomously reprogram network 
imperative forwarding major limitations 
inefficient 
node continued transmit code network reprogrammed 
second easily saturate network 
sort feedback density estimation dense cluster nodes consume lot bandwidth endlessly broadcasting copies code 
tied propagation execution 
handlers ran rarely quickly forward rely 
current mat architecture solves problems trickle algorithm 
trickle uses quickly propagate new data minimize overhead nodes share data 
just explicit forwarding user installs single copy program network mat installs mote 
pushing propagation vm runtime basic service means users responsible fine tuning performance particular vm include functions manipulate propagation policies application required 
code propagation uses epidemic approach node newer code broadcast local neighbors 
trickle algorithm efficiently broadcast entities version packets contain bit version numbers installed capsules capsule status packets describes fragments capsule mote needs essentially capsule fragments short segments capsule 
contains state diagram motes code propa gation 
mote states maintain exchanging version packets request sending capsule status packets respond sending fragments 
nodes start maintain state 
enter request state hear indicates newer capsule version capsule status fragment packet 
requesting node returns maintain state receives entire capsule 
node enters respond state maintain state hears older capsule version packet needs part current capsule capsule status packet 
state transitions mean nodes prefer requesting responding node defer forwarding capsules thinks completely date 
trickle suppression operates type packet version capsule status capsule fragment individually 
capsule fragment transmission suppress fragment transmissions suppress version packets 
allows meta data exchanges propagation sending fragment cause suppress message saying fragments needs 
fragments means code propagates slow controlled fashion quickly possible 
significantly disrupt existing traffic prevents network overload 
show section mat programs small tens bytes code propagate rapidly large multi hop networks tens seconds 
propagation security self replicating code poses network security risks 
specifically adversary introduce single copy malicious program take control entire network 
mat version numbers finite installing highest possible version number prevent reprogramming 
mat vm provide additional levels security assume trusted pc users write scripts 
motes physically secure 
private key cryptography compute packet checksums authentication codes tinysec similar protocols 
private key installed part mat vm 
motes physically compromised pc computes digital signatures variant biba algorithm provides signatures computationally intensive produce inexpensive verify 
motes maintain way hash chains stored eep rom validating signatures allow verify pc generated program 
scheme attacked isolating nodes network observing hash chains rest network 
nodes reject vm name dir apps regionsvm language name function name send function name mag function name cast function name id function name sleep function name function name function name function name function name function name function name context name boot minimal description file regions vm shown 
malicious program expired hash values 
incorporated level security mat proof concept part distribution degree security pressing requirement current deployments 
building mat vm build vm scripting environment user specifies things language set functions set contexts 
specification mat generates tinyos component implementing vm java classes assembler 
shows description file minimalist regions vm discuss section 
mat supported language supplies set primitives needs mat 
example motlle includes primitives build closures read local variables tinyscript primitives read named shared variables 
generating assembler separates language compilation framework 
example tinyscript compiler invokes mat assembler produce vm specific opcodes 
different vms provide language may different instruction opcode mappings 
customizing vm application domain current mat framework comes collection contexts timers functions sensor access 
fairly small range size lines nesc 
context function accompanied xml specification providing additional information mat number arguments functions user documentation 
user building vm add new contexts functions writing additional nesc components implementing appropriate interfaces bytecode functions interface concurrency manager contexts 
buffer packet call packet packet call light call send packet tinyscript light pushc send mat bytecodes tinyscript function invocation simple sense send loop 
operand stack passes parameters functions 
example scripting environment mapped variable packet buffer 
compiled program bytes long 

evaluation briefly tinyscript motlle languages mat currently supports section measure basic overheads interpretation concurrency section verify effectiveness code propagation section 
evaluate effectiveness mat vms versus previous implementation high level sensor network programming models regions section tinydb section 
languages mat currently supports languages tinyscript motlle 
tinyscript basic imperative language dynamic typing simple data buffer abstraction 
statically named data function pointers 
resource analysis concurrency straightforward resources accessed handler simply union resources accessed operations 
static naming allows easy incremental code updates tinyscript mapping handlers capsules 
tinyscript represents bare bones language provides minimalist data abstractions control structures compiled concise code 
figures contain tinyscript samples 
full tinyscript primitive set contains operations leaves space opcode set functions 
simplest tinyscript vm single context functions uses kb ram kb code includes tinyos networking stack propagate code 
motlle mote language little extensions dynamically typed scheme inspired language syntax 
examples code shown figures heavily commented introduce motlle features example 
main prac monolithic decomposed overhead operations sec instruction issue rates mat 
race condition safety decomposed vms imposes execution overhead 
operation cycles time lock unlock check run resume analysis synchronization overhead 
times assume mhz clock 
tical difference tinyscript richer data model motlle supports vectors lists strings firstclass functions 
allows significantly complicated algorithms expressed vm price accurate data analysis longer feasible mote 
preserve safety motlle serializes execution event handlers reporting concurrency manager contexts access shared resource 
motlle appropriate applications bursty processing requirements require rapid response events 
motlle code transmitted single capsule contains handlers support incremental changes running programs 
basic motlle vm including functions manipulate lists strings vectors takes kb code kb ram kb available user programs 
mote network layout soda hall 
motes programmed time seconds reprogramming time distribution byte capsule soda hall network 
measured computation overhead mat synchronization operations 
measured cycle counter mote platforms 
results cpu overhead summarized 
values averaged samples 
measurements vm shared resources byte long programs 
check cost checking locks context requires seeing measured bytecode interpretation overhead mat held obtained 
running cost imposes writing tight loop counting obtaining context locks posting times ran seconds 
shows results 
tinyos task execution 
resuming cost loop accessed shared variables involve lock incurred context triggered run event checks concurrency manager 
mat involves checking task runnable sue just instructions second 
obtain locks running 
slower previous published non configurable experiment context run monolithic version 
parallelism effectively sum check nature mat operations especially functions runnable running additional overhead 
overhead fairly minor 
example func installing cost installing new tinyscript sends packet imposes approximately clock mote necessitates full program anal cycles cpu overhead operation consumes ysis 
clock cycles 
believe vms higher level functions reducing overhead fur propagation ther 
example regionsvm described measure mat code propagation rate de creating region transmits packets simple vm mote testbed fourth receives single opcode 
clearly implement floor soda hall uc berkeley 
shows ing complex mathematical codes mat inefficient physical topology network topology approxi application domain needs significant processing mately hops hops av include functions 
erage node distance 
standard mat pa rameters trickle 
status version packets range second minutes redundancy constant 
fragments trickle suppression operate fixed window size second 
request timeout seconds 
injected byte fragment program single node wired link measured time reception nodes network 
repeated experiment times letting settle tests 
shows aggregate results 
percent nodes received capsule seconds percent minute 
long tail took minutes distribution characteristic trickle transmits packets keep average case inexpensive small number nodes left 
vm continues trickle code indefinitely nodes eventually reprogram minutes average percent nodes required program 
parameters described stable node sends packets hour 
regions evaluate mat regions tracking application 
compare application implemented native regions code vm extended region primitives regionsvm vm customized pursuer evader tracking 
pursuer evader tracking pursuer evader game demo peg dense field motes poll times second detect evader robot 
nodes smooth readings exponentially weighted moving average filter transient noise 
mote detects evader move filtered reading goes threshold broadcasts reading 
field dense nearby nodes detect evader broadcast readings 
broadcasts implicit leader election algorithm leader mote highest reading 
broadcasting mote waits short time deciding leader hear reading higher 
leader aggregates readings heard single packet sends waypoint forwarding moving pursuer robot 
nesc version peg demo took half dozen students staff order month working full time implement deploy 
regionsvm regions programming proposal simplify sensor network programming 
model nodes operate region shared tuple spaces regions geographic proximity network connectivity node properties 
proposed programming model tinyos provides single synchronous execution context fiber supports blocking operations 
users compile full tinyos image regions program install binary image network 
building regionsvm provide support regions programming fairly simple requires writing components regions abstractions vm functions 
doing approximately lines nesc code region type 
moving regions mat framework user longer constrained single execution context case proposed regions fiber model regionsvm respond execution event mat context exists 
additionally installing new tinyos image user installs single image injects small vm programs 
shows regions pseudocode peg application publication welsh corresponding tinyscript code alongside 
major distinctions stem tinyscript data model support structures object oriented style functions 
additionally parameter creation function compile time constant regions compiler 
mat implementation bytes long 
shows relative sizes tinyos images 
mat doubles size executable adds bytes data storage 
breakdown data storage costs shown 
buffers variables responsible approximately third storage 
costs scale number contexts capsule store needs space store programs contexts need allocate context variables context set private variables 
mat doubles size tinyos image time cost wide range regions programs 
sending tens kilobytes data network user send order bytes reduction 
course user free install new vm network new level abstraction needed 
additionally mat programs run sandboxed vm environment buggy code crash network 
mat vm functions sens compile applications modified standard allocation constants default regions settings precluded installing mote designed 
specifically set req queue len tuple space max key route table size queue size send queue size 
location get location get nearest neighbors region nearest region create true reading get sensor reading store local data shared variables region reading key reading region reg key reading location region reg key reading location reading threshold id node max value max id region reduce op reading key am leader node 
max id id sum region reduce op sum reading key sum region reduce op sum reg key sum region reduce op sum reg key centroid sum sum centroid sum sum send basestation centroid sleep periodic delay regions pseudocode 
create nearest neighbor region call reading call cast call mag 
store local data shared variables call reading call reading call call reading call reading threshold 
id node max value max id call 
am leader node max id id sum call sum call sum call buffer sum sum buffer sum sum call send buffer call sleep periodic delay tinyscript code regions pseudocode corresponding tinyscript 
pseudocode programming sensor networks regions tinyscript program right bytes long 
buffer shared reading private curr 
read magnetometer sensor curr call cast call mag 
filter reading ewma reading reading reading reading curr 
detected vehicle reading 
broadcast id reading call id reading call bcast 
fire timer ms call timer bytes buffer sendbuf shared reading shared high 
timer call 
leader 
reading high 
buffer full head call sendbuf call id sendbuf reading 
just append sendbuf call id sendbuf reading call send sendbuf 
clear state aggregation call sendbuf high timer bytes buffer recvbuf buffer sendbuf shared high 
get received data recvbuf call received 
higher current high 
recvbuf high high recvbuf 
add reading buffer call sendbuf sendbuf recvbuf sendbuf recvbuf receive bcast bytes example peg implementation mat 
timer fires periodically samples magnetometer sensor exponentially weighted moving average smooth transient noise 
detects spike reading marks sensed broadcasts message local neighbors schedules aggregation timer timer fire ms 
node hears broadcast puts heard value send buffer keeps track highest reading heard 
timer stops shot timer checks sensed highest reading neighborhood leader 
routes aggregate buffer pursuer send function 
static mat program flash application regions network stack timers data ram application regions network stack timers component data buffers capsule store variables contexts scheduler locks total resource utilization static mat tinyos regions images bytes 
buffer shared reading private curr curr call cast call mag avg avg avg avg curr avg call id avg call bcast call timer bytes buffer shared reading reading call call id reading call bcast call bytes moving peg event boundary 
timer polls sensor runs underlying tinyos detection implementation fires event 
ing local multi hop communication designed support pursuer evader tracking 
shows corresponding peg implementation 
specialized averaging detecting magnetometer pulses event shown 
factoring sampling logic code simpler shorter 
comparison evaluate accurately regionsvm implementations estimate position pursuer set environment configuration similar peg deployment 
arranged nodes grid spacing feet packet loss rates drawn empirical model packet level simulation models media access collisions maximum communication rate packets second approximately current motes capable 
real peg deployment sensing threshold range just grid spacing modeled pursuer point sensor data source quadratic strength real peg deployment spacing yards feet 
chose feet network density identical simulation results welsh reported regions implementation 
packets event estimation error regionsvm bandwidth large regionsvm bandwidth mat peg implementation dropoff distance threshold just feet 
placed pursuer random points grid compared center mass estimation aggregated readings actual position 
shows results 
implementation estimated point source tenths foot quarter grid space average cost half transmissions detection event 
initial attempts run peg regionsvm failed 
reduction operation requires approximately transmissions 
node sensing pursuer performs reduction leader performs 
sampling rate hz approximately times available network bandwidth 
behavior precluded evaluating regionsvm empirically 
increasing available bandwidth fold observed large errors roughly feet 
due formulation program reduction separate losses independent skew results 
example request large reading lost sum reduction coordinate reductions divisor low computing centroid formulation program ignores data dependency 
multi value reduction remove possibility error current regions library support operation 
increased bandwidth fold packets second 
regionsvm able compute centroid accurately feet 
increased accuracy comes directly regions implementation increased communication 
implementation actual deployment aggregates values threshold 
contrast regionsvm aggregates values nodes threshold perform aggregation 
push pull tuple space implementation probably significantly reduce bandwidth required 
tinydb tinydb complex mote application allows mote sensor network treated streaming database 
database queried sql language main extension sql specification sample period query repeated 
instance select nodeid light sam ple period collect identity light sensor reading nodes sensor network minute 
supports simple data collection equivalent tinydb query select nodeid parent light sample period fire timer epoch set update update multihop route min define timer handler assigning function global variable timer handler timer handler fn send sends message string multihop network encode encodes contents vector string epoch advances epoch snoop epoch advance call send encode vector epoch id parent light intercept snoop handlers run multihop message passes intercept handler overhead snoop handler mote 
message epoch advance epoch 
snoop handler fn heard snoop msg intercept handler fn heard intercept msg heard fn msg decode decodes string argument vector case bytes string decoded integer 
vector decode msg vector snoop epoch advances epoch network converge consistent epoch 
snoop epoch simple simple data collection query motlle previous example aggregate queries select avg temperature sample period measure average temperature network 
computation aggregate queries performed network data collected routed 
built custom vm queryvm motlle perform similar data collection tasks 
designed extensions queryvm idea motlle code responsible data collect message layouts 
vm extensions responsible communication common patterns necessary perform tinydb operations 
led extensions areas list primitives handlers extensions detail comment examples multi hop communication provide primitives handlers access tree multihop communication layer tinydb 
epoch handling tinydb query results notion time epoch 
result query happens epoch second epoch epoch numbers included query results help support aggregation 
ensure consistent epoch numbers maximized network snooping query results 
add primitives vm avoid replicating epoch handling logic program 
aggregation add primitives perform logic tinydb select nodeid temp light group nodeid sample period fire timer epoch set update update multihop route min builds function invocation evaluates returns exponentially decaying average constant fn function attr int bits int running maintains running total return function samples decays attr fn running running running bits attr bits temp timer handler fn epoch light vector epoch nodeid decode messages heard update epoch necessary snoop handler fn heard snoop msg intercept handler fn heard intercept msg heard fn msg snoop epoch decode message msg decode message fn msg decode msg vector conditional conditional query motlle necessary spatial aggregation 
chose implement time aggregates exponential decay directly motlle see 
shows motlle code tinydb se lect nodeid parent light sample period query 
core code single send 
advances epoch collects data sends multi hop network 
queryvm application takes kb code kb ram kb user programs tinydb uses kb code kb ram 
evaluate queryvm comparing performance tinydb queries simple simple data collection query conditional conditional time averaged query query 
main metric average power consumption controls lifetime sensor network 
report encoded query tinydb vs program queryvm size 
ran queries network mica motes spread intel berkeley lab standard mica sensor board 
tinydb queryvm multi hop routing layer low power listening radio stack 
measured power consumption non routing mote physically close root multi hop network 
power reflects mote overhears traffic count message send tinydb query bytes 
tinydb queryvm query size mw sizes mw simple conditional table query size power consumption tinydb queryvm sends relatively messages 
table presents results experiments 
results conditional tinydb incorrect nodes reporting result epoch report identical value include result 
queryvm queries energy efficient 
simple data collection estimate difference mw due larger data packets tinydb 
spatial averaging communication cost simple data collection mw energy difference due higher computational cost tinydb 
clear examples tinydb query concise easier write corresponding queryvm code 
queryvm expressive possible measure exponentially decaying average queryvm possible tinydb provided tinydb compiled 
similarly debugged spatial averaging writing code motlle expense larger program vs bytes power consumption vs mw 
ultimately choice appropriate programming abstraction depend needs application 
results show functionality tinydb provided mat comparable cost 
power consumption mw pair aa batteries mah approximately rds usable mote nearly months 
lowering sample rate optimizations believe lifetimes months readily achievable 
shows mat realistic option long term low duty cycle sensor net deployments 

discussion mat model user programs distinct representations shown 
user representation programs high level scripts 
mat meets needs level programming ease abstraction supporting multiple languages 
compiler transforms user representation bytecodes transmission representation 
bytecodes designed conciseness safety allowing networks freely propagate install 
currently mat merely interprets network representation reason just time compilation techniques generate native code 
tinydb select avg temp sample period fire timer epoch set update update multihop route min returns object methods performing spatial averaging methods spatial sample measure locally spatial agg include results child spatial get get completed results subtree string length length ready transmission object vector functions elements 
fn function attr attr epoch change fn epoch update vector fn data intercept data fn sample fn get temp timer handler fn summary root id aggregation id epoch spatial sample summary spatial get summary encode vector epoch summary decode messages heard update epoch necessary add child summaries summary snoop handler fn snoop epoch decode message snoop msg intercept handler fn vector fields decode message intercept msg snoop epoch fields spatial agg fields decode message fn msg decode msg vector string length spatially averaged query motlle section showed mat architecture generate vms provide proposed query programming models vms efficient monolithic implementations emulate 
improved efficiency comes mat decomposes situ programming layers 
certainly optimized fully integrated implementation models efficient implemented mat results suggest implementations benefit similar layering 
additionally decomposing environment set reusable robust building blocks architecture greatly simplifies process developing programming models new applications 
sensor networks tinyos networks particular notoriously difficult program 
level individual mote software mix embedded system kernel code resulting complexities 
current mote hardware support traditional protection boundaries open question platforms variety design considerations stack memory usage 
mat provides equiv user land programming environment intermediate representation 
light building vm similar building customized os kernel primitives functions system calls 
additionally tailoring representation particular application domain mat encode programs concisely native code making propagation efficient conserving ram 

related extensible abstraction boundaries long history operating systems 
proposals scheduler activations net show having richer boundaries allow application os cooperation greatly improve application performance 
operating systems exokernel spin take aggressive approach software structure language safety allow flexible os service composition 
providing fixed interface allow users write interface improve performance increased control 
mat uses similar techniques adding extensions easy execution performance driving goal architecture extensibility 
enabling users easily compose vms tailored specific application results simple programs concise code conciseness minimizes overhead 
similar os kit defining system boundaries compositions simple easy 
additionally making system building blocks self contained components software engineering practice localize faults bugs easier track microkernel efforts mach similar benefits different goals 
virtual machines ucsd system java virtual machine microsoft clr provide common abstractions wide range platforms supporting languages 
contrasts strongly mat goal providing virtual machine particular deployment 
systems share advantage smaller native code mat believe mat vms take advantage application specific nature provide compact code examples section specialize existing vm show 
vmgen tool building interpreters different focus mat 
main goals simplify specification interpreter instructions addition pushing constants stack increase interpreter execution efficiency 
contrast mat aims simplify extending core functionality provided language application specific extensions provide core system services concurrency management code propagation 
tech niques proposed improve performance mat vms 
number techniques proposed reduce code size interpreters regular processors 
optimizations applied reduce size mat programs 

argue application specific virtual machines effective solution safe efficient mote network programming 
vm takes role traditional os kernel 
system calls provides set primitives 
vm schedules concurrent vm level threads manages shared resources enforces protection boundaries 
vms application specific provide set primitives particular user needs making programs short simple high level application logic encoded bytes 
bytecodes sit high level operations interpretation overhead small 
additionally virtualization gives traditional benefits handling platform heterogeneity 
time research challenges remain 
framework provide better mechanisms type extensions benefit resource analysis framework capable supporting languages dynamic memory allocation 
power efficiency important part sensor network applications mat addresses cost propagation execution manage hardware resources sensors combination static analysis concurrency dynamic resource tracking automatically enable disable hardware devices needed 
placing clear divisions program layers allows individually optimized separate goals 
variety proposals exist high level programming models sensor network operating systems highly optimized embedded systems 
providing architecture bridge gap mat allows users efficiently safely reprogram sensor networks 
supported part defense department advanced research projects agency national science foundation nsf iis california micro program intel 
research infrastructure provided national science foundation eia 

tc tg 
common language infrastructure cli 
technical report ecma 
anderson bershad lazowska levy 
scheduler activations effective kernel support user level management parallelism 
acm transactions computer systems february 
bershad savage pardyak sirer becker fiuczynski chambers eggers 
extensibility safety performance spin operating system 
proceedings th acm symposium operating systems principles sosp 
ernst evans fraser lucco proebsting 
code compression 
sigplan conference programming language design implementation pages 
ertl gregg krall 
vmgen generator efficient virtual machine interpreters 
software practice experience 
evans fraser 
bytecode compression profiled grammar rewriting 
sigplan conference programming language design implementation pages 
ford back benson lepreau lin shivers 
flux oskit substrate kernel language research 
symposium operating systems principles pages 
gay levis von behren welsh brewer culler 
nesc language holistic approach networked embedded systems 
sigplan conference programming language design implementation pldi june 
cheriton 
application controlled physical memory external page cache management october 
hill culler 
mica wireless platform deeply embedded networks 
ieee micro november december 
kaashoek engler ganger brice hunt mazi res grimm jannotti mackenzie 
application performance flexibility exokernel systems 
proceedings th acm symposium operating systems principles sosp october 
karlof sastry wagner 
tinysec security tinyos 
presentation nest group meeting 
feeley 
generation fast interpreters huffman compressed bytecode 
proceedings acm sigplan workshop interpreters virtual machines emulators 
levis culler 
mat tiny virtual machine sensor networks 
international conference architectural support programming languages operating systems san jose ca usa oct 
levis lee welsh culler 
simulating large wireless sensor networks tinyos motes 
proceedings acm conference embedded networked sensor systems sensys 
levis patel culler shenker 
trickle self regulating algorithm code maintenance propagation wireless sensor networks 
usenix acm symposium network systems design implementation nsdi 
lindholm yellin 
java virtual machine specification 
addison wesley second edition 
madden franklin hellerstein hong 
design query processor sensor networks 
proceedings acm sigmod international conference management data pages 
acm press 
madden franklin hellerstein hong 
tag tiny aggregation service ad hoc sensor networks 
proceedings acm symposium operating system design implementation osdi dec 
mainwaring polastre szewczyk culler anderson 
wireless sensor networks habitat monitoring 
proceedings acm international workshop wireless sensor networks applications sept 
pemberton daniels 
pascal implementation compiler 
ellis horwood 
perrig 
biba time signature broadcast authentication protocol 
acm conference computer communications security pages 
rashid baron forin david golub orr 
mach foundation open systems 
proceedings second workshop workstation operating systems 
von eicken basu vogels 
net user level network interface parallel distributed computing 
proceedings th annual acm symposium operating systems principles december 
welsh 
programming sensor networks regions 
usenix acm symposium network systems design implementation nsdi 
woo tong culler 
taming underlying challenges reliable multihop routing sensor networks 
proceedings international conference embedded networked sensor systems pages 
acm press 
appendix deadlock free proof follow proof shows mat concurrency model deadlock free 
proof contradiction assume deadlock exists 
represent invocations traditional lock dependency graph context vertex 
vertices ca suspended waiting state record time ta ca suspended newly created invocations ta invocation creation time 
directed edge ca cb ca waiting resource cb holds 
edge represents situations 
ca may newly created invocation waiting execution 
second cb may holding resources ca released suspending time ta wants reacquire 
resources sets atomically acquired scheduling point follows cb resumed execution ca suspended 
cb suspends time tb conclude tb ta 
contexts acquire resource sets atomically context waiting start execution hold resources 
deadlock cycle 
cn graph 
context cycle waiting state 
context ck cycle start vertex incoming edge invocation cj ck cycle tk tj 
induction follows contradiction 
