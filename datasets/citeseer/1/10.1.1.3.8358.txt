similarity clustering sequences hidden markov models rio figueiredo dipartimento di informatica universit di ca le italy sci instituto de es instituto superior cnico lisboa portugal mtf lx pt 
hidden markov models constitute widely employed tool sequential data modelling clustering context poorly investigated 
novel scheme sequential data clustering proposed inspired similaritybased paradigm introduced supervised learning context 
approach new representation space built object described vector similarities respect set objects 
similarities determined hidden markov models 
clustering performed space 
way difficult problem clustering sequences transposed manageable format clustering points vectors features 
experimental evaluation synthetic real data shows proposed approach largely outperforms standard hmm clustering schemes 
unsupervised classification clustering data undoubtedly interesting challenging research area defined organization collection patterns groups similarity 
known data clustering inherently difficult task compared supervised classification classes identified system adequately trained 
intrinsic difficulty worsens sequential data considered structure underlying process difficult infer typically different length sequences dealt 
clustering sequences assumed increasing importance years due wide applicability emergent contexts data mining dna genome modelling analysis 
sequential data clustering methods generally classified categories proximity methods feature methods model methods 
proximity approaches main effort clustering process devising similarity distance measures sequences 
measures standard distance method agglomerative clustering applied 
feature methods extract set features individual data sequence captures temporal information 
problem sequence clustering reduced addressable point vector features clustering 
model approaches assume analytical model cluster aim clustering find set models best fit data 
examples models employed include time series models spectral models finite state automata hidden markov models hmm :10.1.1.131.2084:10.1.1.131.2084
hmms widely tool sequence modelling importance rapidly grown decade 
context sequence clustering hmms extensively papers literature corresponding state art section 
proposed approaches mainly fall proximity third model categories 
alternative hmm clustering scheme proposed classifiable belonging feature class extends similarity paradigm 
paradigm introduced supervised classification purposes differs typical pattern recognition approaches objects represented sets vectors features 
similarity paradigm objects described pairwise dis similarities distances objects data set 
state art similarity paradigm reviewed section 
propose extend paradigm problem clustering sequences new feature space sequence characterized similarity sequences 
problem find suitable metric measuring dis similarities sequences shown hmms suitable tool purpose :10.1.1.8.6627:10.1.1.44.3648
space clustering performed standard techniques difficult task sequence clustering transposed manageable format clustering points vectors features 
experimental evaluation synthetic real data shows approach largely outperforms standard hmm clustering schemes 
rest organized follows section summarizes state art hmm clustering sequences reviews similaritybased paradigm 
section reviews fundamentals hidden markov models section details proposed strategy 
experimental results reported section 
section devoted presenting directions 
state art hmm sequence clustering hmms extensively employed clustering sequences papers exploring direction 
specifically early approaches related speech recognition 
methods belong proximity clustering class 
hmms employed compute similarities sequences different approaches see example standard pairwise distance matrix approaches agglomerative hierarchical obtain clustering 
strategy considered standard method hmm clustering sequences better detailed section 
approach directly linked speech smyth see general 
approach consists steps pairwise distance observed sequences computing symmetrized similarity 
similarity obtained training hmm sequence log likelihood ll model sequence computed 
information build ll matrix cluster sequences groups hierarchical algorithm 
second step hmm trained cluster resulting models merged composite global hmm hmm design disjoint part composite model 
initial estimate refined standard baum welch procedure 
result global hmm modelling data obtained 
number clusters selected cross validation method 
respect mentioned taxonomy approach classified belonging proximity class pairwise distance derived initialize model model class model clustering data obtained 
example hmm method sequence clustering proposed hmms cluster prototypes 
clustering obtained employing rival penalized competitive learning algorithm method originally developed point clustering state merging strategy aimed finding smaller hmms 
relevant contribution model hmm clustering methodology li biswas 
basically approach clustering problem addressed focusing model selection issue search hmm topology best representing data clustering structure issue finding number clusters 
issue addressed bayesian information criterion extending continuous case bayesian model merging approach 
regarding issue sequence hmm likelihood measure enforce group similarity criterion 
optimal number clusters determined maximizing partition mutual information pmi measure inter cluster distances 
problems addressed terms bayesian model selection bic stutz cs approximation 
comprehensive version appeared method tested real world ecological data 
clustering methodologies applied specific domains physiology ecology social science dynamic model structure readily available 
obtained results published 
similarity classification literature similarity classification vast 
jain zongker obtained dissimilarity measure handwritten digit recognition problem deformable templates multidimensional scaling approach project dissimilarity space low dimensional space nearest neighbor nn classifier employed classify new objects 
graepel investigate problem learning classifier data represented terms pairwise proximities approach vapnik structural risk minimization 
jacobs weinshall studied distance classification non metric distance functions verify triangle inequality 
duin active authors area having produced papers 
motivation basic features similarity methods described shown experiments real applications bayesian classifier regularized linear normal density classifier dissimilarity space outperforms nearest neighbor rule 
aspects thoroughly investigated classifiers dissimilarity space studied digit recognition bioinformatics problems 
generalized kernel approach introduced dealing classification aspects dissimilarity kernels 
hidden markov models discrete time hidden markov model viewed markov model states directly observed state characterized probability distribution function modelling observations corresponding state 
formally hmm defined entities sn finite set possible hidden states transition matrix aij representing probability moving state si state sj aij qt sj qt si aij aij qt denotes state occupied model time emission matrix sj indicating probability emission symbol system state sj discrete alphabet continuous set ir case sj probability density function :10.1.1.131.2084:10.1.1.131.2084
initial state probability distribution 
si see www ph tn tudelft nl research neural index html convenience represent hmm triplet 
learning hmm parameters set observed sequences oi usually performed known baum welch algorithm able determine parameters maximizing likelihood oi :10.1.1.131.2084:10.1.1.131.2084
steps baum welch algorithm evaluation step required compute model sequence computed forward backward procedure :10.1.1.131.2084:10.1.1.131.2084
standard hmm clustering sequences standard proximity method clustering sequences hmms algorithm 
consider set sequences clustered algorithm performs steps 
train hmm sequence oi 

compute distance matrix oi oj representing similarity measure sequences models typically obtained forward probability oj devising measure distances models 
past authors proposed approaches computing distances early approaches euclidean distance discrete observation probability entropy emission probability models bayes probability error see 

pairwise distance matrix method agglomerative method perform clustering 
proposed strategy idea basis proposed approach conceptually simple build new representation space similarity values sequences obtained hmms perform clustering space 
similarity values allow discrimination quantity high similar objects sequences belonging group low objects different clusters 
interpret similarity measure oi sequence sequence oi feature sequence fact suggests construction feature vector similarities set sequences ok characterized pattern set features ok ok 
formally set sequences clustered proposed approach briefly described follows pr set representative objects objects may belong set sequences may defined 
basic case train hmm sequence pr represent sequence oi data set set similarities dr oi elements representative set computed hmms oi log oi oi log oi dr oi ti 
oi pr log oi ti length sequence oi 
perform clustering ir denotes cardinality general technique necessarily hierarchical appropriate clustering points euclidean space 
simplest case representative set data set resulting similarity space dimensionality computationally heavy large data sets interesting analyze discriminative power space 
experimental results section proposed technique compared standard hmm clustering scheme section 
likelihood similarity matrix obtained clustering step performed algorithms variants agglomerative hierarchical clustering techniques complete link scheme ward scheme 
non parametric pairwise distance clustering technique called clustering friends technique produces partition data similarity matrix 
partition obtained iteratively applying step transformation proximity matrix 
step transformation represents point relation data points second step re estimates pairwise distances proximity measure representations 
transformations algorithm partitions data clusters 
partition data clusters method applied times recursively 
regarding proposed approach obtaining similarity representation sequences representatives clustering algorithms hierarchical agglomerative complete link ward methods distance euclidean metrics similarity space performed compare representations algorithms standard means algorithm 
clustering accuracies measured synthetic real data 
regarding synthetic case consider class problem sequences generated hmms defined fig 

data set composed sequences fig 

generative hmms synthetic data testing transition matrix initial state probability contains parameters emission density gaussians indicated means variances 
length classes dimensionality similarity vectors 
notice clustering task easy hmms similar differing slightly variances emission densities 
accuracy clustering quantitatively assessed computing number errors clustering error occurs sequence assigned cluster majority sequences class 
results table averaged repetitions 
table 
clustering results synthetic experiments 
standard classification ml classification standard clustering 
complete link 
ward clus 
friends clustering similarity space st 
complete link 
ward means table possible notice proposed methodology largely outperforms standard clustering approaches best performing algorithm partitional means similarity space produces perfect clustering 
order better insight discriminative power proposed feature space computed supervised classification results synthetic example 
decisions taken standard maximum likelihood ml approach unknown sequence assigned class model shows highest likelihood 
note classification scheme similarity space introduced represents supervised counterpart standard clustering approach proposed section 
classification error computed standard leave loo scheme 
important note clustering results similarity space better classification results confirming high discrimination ability similarity space 
real data experiment regards shape recognition shapes modelled proposed briefly object contours described curvature curvature sequences modelled hmms gaussian mixtures emission probabilities 
object database sebastian shown fig 

case number clusters known 
clustering algorithms try group shapes different clusters similarity 
results averaged repetitions table 
tables evident proposed representation permits greater discrimination resulting increasing clustering accuracies 
case ml classification accuracy computed loo scheme 
table possible note clustering results better classification performances confirming high discriminative potentiality proposed similarity space 
scheme sequence clustering hidden markov modelling similarity paradigm proposed 
approach builds fig 

objects set testing 
table 
clustering results real experiments 
standard classification ml classification standard clustering 
complete link 
ward clus 
friends clustering similarity space st 
complete link 
ward means features sequence represented vector similarities predefined set sequences 
standard point clustering method performed representations 
consequence difficult process clustering sequences cast simpler problem clustering points established techniques proposed 
experimental evaluation synthetic real problems shown proposed approach largely outperforms standard hmm clustering approaches 
main drawback approach high dimensionality resulting feature space equal cardinality data set 
obviously problem represents central topic investigation 
previously addressed issue context similarity supervised learning 
unsupervised context idea linear reduction techniques order reduce dimensionality space 
idea directly address problem adequately choosing representatives problem casted context feature selection unsupervised prototypes chosen features selected 

jain dubes algorithms clustering data 
prentice hall 
rabiner tutorial hidden markov models selected applications speech recognition 
proc 
ieee 
jain zongker representation recognition handwritten digits deformable templates 
ieee trans 
pattern analysis machine intelligence 
graepel herbrich obermayer classification pairwise proximity data 
kearns solla ed advances neural information processing 
volume mit press 
jacobs weinshall classification nonmetric distances image retrieval class representation 
ieee trans 
pattern analysis machine intelligence 
duin automatic pattern recognition similarity representations 
electronics letters 
duin generalized kernel approach classification 
journal machine learning research 
duin dissimilarity representations allow building classifiers 
pattern recognition letters 
smyth clustering sequences hidden markov models 
mozer jordan petsche eds advances neural information processing 
volume mit press 
hidden markov model approach sequential data clustering 
caelli amin duin kamel de ridder eds structural syntactic statistical pattern recognition 
lncs springer 
rabiner lee juang hmm clustering connected word recognition 
proc 
ieee icassp 

lee context dependent phonetic hidden markov models continuous speech recognition 
ieee transactions acoustics speech signal processing 
speaker independent phone modeling speaker dependent hmm composition clustering 
int 
proc 
acoustics speech signal processing 
volume 

burkhardt measuring hmm similarity bayes probability error application online handwriting recognition 
proc 
int 
conf 
document analysis recognition 

cadez smyth general probabilistic framework clustering individuals 
proc 
acm sigkdd 

law kwok rival penalized competitive learning model sequence 
proc 
int 
conf 
pattern recognition 
volume 

xu oja rival penalized competitive learning clustering analysis rbf nets curve detection 
ieee trans 
neural networks 
li bayesian approach temporal data clustering hidden markov model methodology 
phd thesis vanderbilt university 
li biswas clustering sequence data hidden markov model representation 
proc 
spie conf 
data mining knowledge discovery theory tools technology 

li biswas bayesian approach temporal data clustering hidden markov models 
proc 
int 
conf 
machine learning 

li biswas applying hidden markov model methodology unsupervised learning temporal data 
int 
journal knowledge intelligent engineering systems 
li biswas dale dale hmm temporal data clustering methodology modeling system dynamics 
intelligent data analysis journal press 
schwarz estimating dimension model 
annals statistics 
stolcke omohundro hidden markov model induction bayesian model merging 
hanson cowan giles eds advances neural information processing systems 
volume morgan kaufmann san mateo ca 
cheeseman stutz bayesian classification autoclass theory results 
advances knowledge discovery data mining 

vapnik statistical learning theory 
john wiley new york 
el yaniv gdalyahu tishby new nonparametric pairwise clustering algorithm iterative estimation distance profiles 
machine learning 
theodoridis pattern recognition 
academic press 
investigating hidden markov models capabilities shape classification 
submitted publication 
sebastian klein kimia recognition shapes editing shock graphs 
proc 
int conf 
computer vision 

figueiredo similarity classification sequences hidden markov models submitted publication 

law jain figueiredo feature selection mixture clustering 
neural information processing systems nips vancouver 
