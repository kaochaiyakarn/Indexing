bayesian kernel methods alexander smola bernhard sch lkopf australian national university canberra act australia max planck institut kybernetik bingen germany :10.1.1.25.1089
bayesian methods allow simple intuitive representation function spaces kernel methods :10.1.1.25.1089
chapter describes basic principles gaussian processes implementation connection kernel bayesian estimation methods relevance vector machine :10.1.1.25.1089
bayesian approach allows intuitive incorporation prior knowledge process estimation :10.1.1.25.1089
possible bayesian framework obtain estimates confidence reliability estimation process :10.1.1.25.1089
estimates typically relatively easy compute :10.1.1.25.1089
surprisingly shall see bayesian approach leads algorithms akin developed framework risk minimization :10.1.1.25.1089
allows provide new insight kernel algorithms sv classification regression :10.1.1.25.1089
addition similarities help design bayesian counterparts risk minimization algorithms laplacian processes section vice versa section :10.1.1.25.1089
words tap knowledge worlds combine create better algorithms :10.1.1.25.1089
section overview basic assumptions underlying bayesian estimation :10.1.1.25.1089
explain notion prior distributions encode prior belief concerning likelihood obtaining certain estimate concept posterior probability quantifies plausible functions appear observe data :10.1.1.25.1089
furthermore show inference performed certain numerical problems arise alleviated various types maximum posteriori map estimation :10.1.1.25.1089
basic tools introduced analyze specific properties bayesian estimators different types prior probabilities gaussian processes section describes theory section implementation rely assumption adjacent coefficients correlated laplacian processes section assume estimates expanded sparse linear combination kernel functions favor hypotheses relevance vector machines section assume contribution kernel function governed normal distribution variance :10.1.1.25.1089
article :10.1.1.25.1089
mendelson smola eds advanced lectures machine learning lnai pp :10.1.1.25.1089

springer verlag berlin heidelberg smola sch lkopf readers interested quick overview principles underlying bayesian statistics find sufficient :10.1.1.25.1089
recommend reader focus sections :10.1.1.25.1089
subsequent sections ordered increasing technical difficulty decreasing bearing core issues bayesian estimation kernels :10.1.1.25.1089
central characteristic bayesian estimation assume certain prior knowledge beliefs data generating process dependencies encounter :10.1.1.25.1089
natural extension maximum likelihood framework loosely speaking prior knowledge behaves exactly way additional data observed prior actual estimation problem :10.1.1.25.1089
stated observe sample :10.1.1.25.1089
xm 
ym carry inference typically set observations :10.1.1.25.1089

notational convenience :10.1.1.25.1089
xm ym overview fundamental ideas see details :10.1.1.31.4284:10.1.1.11.2062:10.1.1.25.1089
maximum likelihood bayes rule assume set observations drawn probability distribution :10.1.1.25.1089
follows value assess observe conversely observations wecan try find value particularly observation :10.1.1.25.1089
instance know composed observations scalar random variable drawn normal distribution mean variance try infer vice versa assess came normal distribution :10.1.1.25.1089
process determining data maximizing referred maximum likelihood estimation :10.1.1.25.1089
obtain ml argmax :10.1.1.25.1089
note deliberately write conditional probability distribution implies random variable certain probability distribution additional assumption may want :10.1.1.25.1089
note viewed function referred likelihood :10.1.1.25.1089
sake simplicity gloss details algebras :10.1.1.25.1089
abuse notation correspond density probability measure depending context :10.1.1.25.1089
bayesian kernel methods exists definition conditional distributions obtain bayes rule :10.1.1.25.1089
means soon assume random variable infer observe certain value way transformed parameter density random variable allows statements value presence data :10.1.1.25.1089
typically referred posterior distribution known prior called evidence prior prior knowledge evidence observations come posterior assessment probability having observed certain value :10.1.1.25.1089
mode maximum posteriori map estimation find estimate map argmax :10.1.1.25.1089
note map typically preferred ml avoid unreasonable values making suitable prior assumptions range :10.1.1.25.1089
examples maximum likelihood estimation introducing various models useful regression classification problems stage chapter :10.1.1.25.1089
simplifying assumption obtained iid identically independently distributed yi :10.1.1.25.1089
furthermore distribution zero mean :10.1.1.25.1089
words studying estimation location parameter problem :10.1.1.25.1089
depending properties obtain various solutions pml :10.1.1.25.1089
practical purpose finding ml assumption advantageous rewrite ml argmin log yi :10.1.1.25.1089
smola sch lkopf reduces problem maximizing joint function yi minimizing average terms dependent instances yi :10.1.1.25.1089
term log considered function referred negative log likelihood :10.1.1.25.1089
normal distribution assume normal distribution fixed variance zero mean exp consequently log constant independent :10.1.1.25.1089
means mp satisfies mp argmin derivatives simple algebra shows case mp yi :10.1.1.25.1089
yi 
words mp normal distribution leads mean random variables estimate location parameter :10.1.1.25.1089
laplacian distribution distribution zero mean longer tails normal distribution :10.1.1.25.1089
satisfies exp consequently log constant independent :10.1.1.25.1089
means mp satisfies mp argmin yi :10.1.1.25.1089
derivatives see minimizer mp terms satisfy yi terms yi :10.1.1.25.1089
consequently median solution mp :10.1.1.25.1089
clearly median robust way estimating expected value distribution corrupt additional data taken true distribution obtain estimate regardless possibly large numerical value corrupted additional observations :10.1.1.25.1089
case excluded extreme values ends :10.1.1.25.1089
assume distributions dealing symmetric zero mean mean median coincide :10.1.1.25.1089
bayesian kernel methods robust estimation median estimation appears drastic especially reason believe data corrupted :10.1.1.25.1089
huber formalized notion trimmed mean discards certain fraction extreme values takes mean rest :10.1.1.25.1089
purpose density introduced exp exp show ml mean fraction observations lie interval ml equal amounts observations yi exceed ml :10.1.1.25.1089
insensitive density computational convenience vapnik introduced variant density model insensitive loss function :10.1.1.25.1089
essentially laplacian distribution neighborhood size mean data equally probable :10.1.1.25.1089
formalize follows exp max :10.1.1.25.1089
estimators ml similar properties ones previous paragraph difference takes mean extreme values neighborhood expectation mean set :10.1.1.25.1089
advantage somewhat peculiar estimator optimization problems arising lower number active constraints :10.1.1.25.1089
exploited support vector regression :10.1.1.25.1089
estimating mean real valued random variable may deal discrete valued ones :10.1.1.25.1089
simplicity consider binary :10.1.1.25.1089
case useful estimate probability :10.1.1.25.1089
directly pays consider indirect strategies finding estimate :10.1.1.25.1089
reason stage indirect parameterization estimate function additional parameter case call label pattern process greatly simplified indirect parameterization :10.1.1.25.1089
typically study probabilities parameterized may take various functional forms :10.1.1.25.1089
classification location dependent values referred margin location logistic transfer function exp :10.1.1.25.1089
exp real valued case develop reasoning introduce conditioning part section :10.1.1.25.1089
smola sch lkopf normal distribution huber robust laplace distribution vapnik insensitive log fig :10.1.1.25.1089

densities corresponding negative log density :10.1.1.25.1089
upper left gaussian upper right laplacian lower left huber robust lower right insensitive :10.1.1.25.1089
words note logistic regression equivalent obtaining ln :10.1.1.25.1089
quite logarithm ratio class probabilities referred log odds ratio :10.1.1.25.1089
probit assume sign corrupted gaussian noise see instance sgn :10.1.1.25.1089
case sgn exp 
logistic regression bayesian kernel methods cumulative normal distribution log fig :10.1.1.25.1089

conditional probabilities corresponding negative log probability :10.1.1.25.1089
left logistic regression right probit :10.1.1.25.1089
distribution function normal distribution :10.1.1.25.1089
note correspondence asymptotic behaviors logistic probit model hand linear quadratic soft margin loss functions :10.1.1.25.1089
explains linear soft margin loss proxy logistic regression quadratic soft margin probit model :10.1.1.25.1089
furthermore indicates linear soft margin cheap proxy optimization purposes logistic adequate model fit densities subsequently quadratic soft margin probit model preferred :10.1.1.25.1089
inference problem finding suitable estimates understand way observations generated underlying distribution may simply want infer new values observations purpose need compute :10.1.1.25.1089
factorization conditional probabilities leads :10.1.1.25.1089
means soon compute joint probability distribution function able predict normalization term needed instance mode independent scale :10.1.1.25.1089
example inference normal distributions :10.1.1.25.1089
assume jointly yy yy normal covariance matrix mean smola sch lkopf drawn normal distribution covariance cond yy yy mean cond yy :10.1.1.25.1089
seen follows jointly normal conditional distribution normal distribution obtained collecting terms depend know det exp yy yy writing inverse see collecting terms yields result :10.1.1.25.1089
section example perform gaussian process prediction :10.1.1.25.1089
moment just note know easy estimate :10.1.1.25.1089
quite unfortunately disposition immediately :10.1.1.25.1089
may :10.1.1.25.1089
instance settings described section assumed know distribution observations expected value :10.1.1.25.1089
means order obtain need integrate latent variable :10.1.1.25.1089
achieved follows 
eq 
may may computable closed form :10.1.1.25.1089
exist various strategies deal problem obtaining :10.1.1.25.1089
list 
exact solution solve explicitly proceed estimation procedure solving integral :10.1.1.25.1089
important special case furthermore rm rm situation may occur composed random variables normally distributed mean covariance :10.1.1.25.1089
typically assumes role additive noise multiple unit matrix :10.1.1.25.1089
need case deal colored noise :10.1.1.25.1089
construction normally distributed simply add means variances constituents obtain :10.1.1.25.1089
effort computed integral remarking normal distributions mean variance add :10.1.1.25.1089
note inference situation proceeds identically discussion example :10.1.1.25.1089
chapter setting name gaussian process regression normal noise :10.1.1.25.1089
bayesian kernel methods sampling methods approximate integration randomly drawing performing inference distribution various methods carry samplings exist :10.1.1.25.1089
typically uses markov chain monte carlo mcmc methods :10.1.1.25.1089
see details :10.1.1.25.1089
advantage methods sufficient computational resources able obtain estimate distribution furthermore quality estimate keeps increasing wait samples :10.1.1.25.1089
obvious downside obtain closed form analytic expression density :10.1.1.25.1089
furthermore sampling methods computationally quite expensive especially wish predict large amount data :10.1.1.25.1089
variational methods reason resort approximating integrand lend closed form solution integral :10.1.1.25.1089
may modified version able perform integral :10.1.1.25.1089
option normal distribution mean coincides mode respect second derivative ln variance :10.1.1.25.1089
referred laplace approximation :10.1.1.25.1089
set see instance ln :10.1.1.25.1089
advantage procedure integrals remain tractable :10.1.1.25.1089
reasons normal distributions enjoy high degree popularity bayesian methods :10.1.1.25.1089
normal distribution informative distribution largest entropy distributions bounded variance :10.1.1.25.1089
indicates single gaussian may sufficient capture important properties :10.1.1.25.1089
elaborate parametric model ofp mixture gaussian densities improve approximation :10.1.1.25.1089
common strategy resort variational methods :10.1.1.25.1089
details technical go scope section :10.1.1.25.1089
interested reader referred overview application relevance vector machine section :10.1.1.25.1089
theorem describes basic idea :10.1.1.25.1089
theorem variational approximation densities :10.1.1.25.1089
denote random variables corresponding densities :10.1.1.25.1089
density bound holds ln ln ln df ln :10.1.1.25.1089
smola sch lkopf mean mode mean fig 

left mode mean distribution coincide map approximation satisfied :10.1.1.25.1089
right multi modal distributions map approximation arbitrarily bad :10.1.1.25.1089
proof :10.1.1.25.1089
equality :10.1.1.25.1089
may decompose additionally lnp lnp 
ln kl kullback leibler divergence :10.1.1.25.1089
nonnegative quantity proves second part :10.1.1.25.1089
true posterior distribution usually approximation :10.1.1.25.1089
practical advantage ln computed easily simple :10.1.1.25.1089
furthermore maximizing suitable choice maximize lower bound ln :10.1.1.25.1089
expectation maximization method approximating suggested maximizes integrand jointly unknown variable latent variable :10.1.1.25.1089
clearly equivalent solving integral cases may hope maximum integrand differ location mean :10.1.1.25.1089
furthermore gain interpretation possibly plausible value conjunction noise level estimation process certain parameters function class degree smoothness options disposal comes maximizing respect bayesian kernel methods firstly function maximization procedure directly joint distribution :10.1.1.25.1089
propose called expectation maximization algorithm leads local maximum intuitive fashion :10.1.1.25.1089
ignoring proof details proceed follows :10.1.1.25.1089
define ey log log expectation taken previously chosen value :10.1.1.25.1089
computing commonly referred expectation step :10.1.1.25.1089
find maximum replace argmax :10.1.1.25.1089
commonly known maximization step 
exist special cases calculations carried closed form notably distributions derived exponential family :10.1.1.25.1089
show iteration procedure converge local maximum :10.1.1.25.1089
clear local maximum :10.1.1.25.1089
depends initial guess :10.1.1.25.1089
alternative modify definition conditional expectation argument expectation words merge variable :10.1.1.25.1089
way guaranteed find joint local maximum maximizing function reduced number parameters beneficial general function optimizer :10.1.1.25.1089
come back observation section :10.1.1.25.1089
likelihood priors recall :10.1.1.25.1089
means weigh values tell fits model parameterized byp prior knowledge possible value :10.1.1.25.1089
instance describes jointly normal distribution iid random variables mean variance models prior knowledge occurrence particular pair :10.1.1.25.1089
instance know variance exceeds certain value mean positive :10.1.1.25.1089
quite may sure specific form case assume distribution possible priors :10.1.1.25.1089
called hyperprior describing uncertainty :10.1.1.25.1089
summary dependency model smola sch lkopf order obtain need solve integral previous section may resort various methods approximating :10.1.1.25.1089
far popular method maximize integrand obtain commonly referred map approximation map argmax :10.1.1.25.1089
possible integrates solving inner integrals obtain map argmax argmax :10.1.1.25.1089
words assumes role new prior integrated :10.1.1.25.1089
approach section obtain numerically attractive optimization methods estimation :10.1.1.25.1089
gaussian processes gaussian processes prior assumption adjacent observations convey information :10.1.1.25.1089
particular assumed observed variables normal coupling takes place means covariance matrix normal distribution :10.1.1.25.1089
turns convenient way extending bayesian modeling linear estimators nonlinear situations cf :10.1.1.25.1089

furthermore represents counterpart kernel trick methods minimizing regularized risk :10.1.1.25.1089
basic ideas relegate details efficient implementation optimization procedure required inference section :10.1.1.25.1089
far assumed observations density possibly independently data :10.1.1.25.1089
wish perform regression classification observations typically depend patterns words data comes xi yi pairs novel patterns wish estimate locations :10.1.1.25.1089
introduce conditioning reasoning :10.1.1.25.1089
note affect derivations :10.1.1.25.1089
correlated observations observing yi locations xi natural assume values correlated depending location xi :10.1.1.25.1089
case able perform inference definition independent random variables yi depend observations yj :10.1.1.25.1089
bayesian kernel methods fact strong assumption regarding distribution yi form normal distribution mean covariance matrix course assume arbitrary distribution settings result inference problems expensive compute :10.1.1.25.1089
furthermore theorem show exists large class assumptions distribution yi normal distribution limit :10.1.1.25.1089
observations assume zero mean covariance shows corresponding density random variables :10.1.1.25.1089
assume observe :10.1.1.25.1089
gives information allows state conditional density :10.1.1.25.1089
conditional density known mean need longer variance decreased :10.1.1.25.1089
example performed inference observation obtain possible values :10.1.1.25.1089
similar fashion may infer distribution yi variables provided know corresponding mean covariance matrix means determines closely prediction relates previous observations yi :10.1.1.25.1089
section formalize concepts show matrices generated efficiently :10.1.1.25.1089
definitions basic notions assume distribution observations yi locations :10.1.1.25.1089
xm 
directly specifying observations yi generated underlying functional dependency simply assume generated gaussian process :10.1.1.25.1089
loosely speaking gaussian processes allow extend notion set random variables random functions :10.1.1.25.1089
formally definition definition gaussian process :10.1.1.25.1089
denote stochastic process parameterized arbitrary index set :10.1.1.25.1089
gaussian process :10.1.1.25.1089
xm random variables :10.1.1.25.1089
xm normally distributed :10.1.1.25.1089
note denote covariance matrix :10.1.1.25.1089
done consistency literature reproducing kernel hilbert spaces support vector machines denotes kernel matrix :10.1.1.25.1089
see plays role gaussian processes kernel matrix plays settings :10.1.1.25.1089
convenient trick obtain normal distributions consider function keeping fixed observed value :10.1.1.25.1089
linear quadratic terms completely determine normal distribution :10.1.1.25.1089
smola sch lkopf fig 

normal distribution variables :10.1.1.25.1089
top left normal density zero mean covariance top right contour plot bottom left conditional density bottom left conditional density :10.1.1.25.1089
note plots normally distributed nonzero mean :10.1.1.25.1089
denote function generating covariance matrix cov :10.1.1.25.1089
xm mean distribution :10.1.1.25.1089
write kij xi xj :10.1.1.25.1089
leads 
xm :10.1.1.25.1089
gaussian processes positive definite matrices :10.1.1.25.1089
function defined symmetric matrix positive definite eigenvalues negative :10.1.1.25.1089
proof :10.1.1.25.1089
show defined :10.1.1.25.1089
definition cov 
xm ij cov xi xj 
bayesian kernel methods consequently kij function arguments xi xj shows defined :10.1.1.25.1089
follows directly definition covariance symmetric :10.1.1.25.1089
show positive definite prove rm inequality holds :10.1.1.25.1089
follows var iy xi cov xi xj :10.1.1.25.1089
positive definite function admissible kernel :10.1.1.25.1089
note happens smooth function turns reasonable assumption actual realizations drawn gaussian process need smooth :10.1.1.25.1089
fact may pointwise discontinuous 
closer look prior distribution resulting assumptions :10.1.1.25.1089
standard setting implies prior knowledge particular value estimate assume small values preferred :10.1.1.25.1089
set :10.1.1.25.1089
xm prior density function det exp :10.1.1.25.1089
cases try avoid inverting simple substitution consequently det exp :10.1.1.25.1089
logs see term identical penalty term arising regularized risk framework cf :10.1.1.25.1089
chapter support vectors :10.1.1.11.2062:10.1.1.25.1089
result connects gaussian process priors estimators reproducing kernel hilbert space framework kernels favoring smooth functions translate immediately covariance kernels similar properties bayesian context :10.1.1.25.1089
simple hypotheses analyze detail functions considered simple gaussian process prior :10.1.1.25.1089
know hypotheses low complexity correspond vectors small :10.1.1.25.1089
particular case normalized eigenvectors vi large eigenvalues ivi yields vi :10.1.1.25.1089
smola sch lkopf fig 

hypotheses corresponding eigenvectors gaussian kernel width uniform distribution interval :10.1.1.25.1089
top bottom left right functions corresponding eigenvectors lower right eigenvalues note information contained eigenvalues :10.1.1.25.1089
plots obtained computing equidistant grid points :10.1.1.25.1089
computed eigenvectors plotted corresponding function values possible :10.1.1.25.1089
words estimator biased solutions small means spectrum eigensystem represent practical means viewing effect certain prior degree smoothness estimates :10.1.1.25.1089
consider practical example gaussian covariance kernel exp assumption uniform distribution obtain functions depicted simple base hypotheses estimator :10.1.1.25.1089
note similarity fourier decomposition means kernel strong preference slowly oscillating functions :10.1.1.25.1089
gaussian process setting allows simple connection parametric models uncertainty see :10.1.1.25.1089
instance assume observations derived patterns functional dependency bayesian kernel methods ifi :10.1.1.25.1089
clearly random variables jointly normal stem underlying gaussian process :10.1.1.25.1089
calculate covariance function follows :10.1.1.25.1089

fn cov 
words starting parametric model want estimate coefficients arrived gaussian process covariance function :10.1.1.25.1089
special case interest set fi fi encodes th coordinate :10.1.1.25.1089
simplest gaussian process kernel possible :10.1.1.25.1089
kernels include exp laplacian kernel polynomial kernel gaussian rbf kernel :10.1.1.25.1089
details choice kernels see :10.1.1.11.2062:10.1.1.25.1089
regression put previous discussion practical :10.1.1.25.1089
sake simplicity regression study classification setting section :10.1.1.25.1089
natural assumption observations generated gaussian process covariance matrix mean :10.1.1.25.1089
reasoning example infer novel xi ky ky yy yy cond ky yy means variance reduced ky degree controlled correlation ky inherent degree variability yy :10.1.1.25.1089
certain certainty carries likewise default estimate mean corrected ky yy deviation default estimate weighted correlation random variables furthermore note purpose inferring mean need store yy :10.1.1.25.1089
similar establish connection regularized risk functional prior probabilities :10.1.1.25.1089
get back case linear covariance kernel :10.1.1.25.1089
rewrite xx xx px smola sch lkopf px projection space spanned means larger grows reliably able infer case spans entire space may suffice px identity consequently :10.1.1.25.1089
words perform inference certainty :10.1.1.25.1089
reader notice prediction certainty pose problem model quite exact data fraught measurement errors noise :10.1.1.25.1089
instance observe lie dimensional subspace spanned conclude occur :10.1.1.25.1089
words modeling problem :10.1.1.25.1089
way address issue replace apparently unsuitable kernel guarantees full rank :10.1.1.25.1089
cumbersome question nondegenerate depends data covariance function :10.1.1.25.1089
elegant way ensure statistical model better suited task introduce extended dependency model latent variables follows means random variables conditionally independent :10.1.1.25.1089
wish infer means need integrate latent variable fashion discussed section :10.1.1.25.1089
regression popular setting assume additive noise drawn distribution gaussian laplacian huber robust ones :10.1.1.25.1089
discussed section integrals arising elimination may easily solvable may need resort approximations :10.1.1.25.1089
additive normal noise simple case :10.1.1.25.1089
situation sum independent normal random variables :10.1.1.25.1089
consequently re difference covariance function :10.1.1.25.1089
gives example regression additive normal noise :10.1.1.25.1089
specializing estimation new location assuming obtain 
xm ky cond ky ky :10.1.1.25.1089
note mean linear combination kernel functions xi :10.1.1.25.1089
estimate element xi obtain xi :10.1.1.25.1089
bayesian kernel methods fig :10.1.1.25.1089

gaussian process regression additive normal noise :10.1.1.25.1089
stars denote observations dotted lines correspond confidence intervals prediction solid line crossing boundaries sine function additive normal noise generate observations :10.1.1.25.1089
convenience plot width confidence interval :10.1.1.25.1089
obtain yi shrinking yi similarly shrinkage estimator james stein :10.1.1.25.1089
additive noise may general able integrate noise models regression typically allow perform simplifications comes estimation :10.1.1.25.1089
convenience split latent variable corresponding assume additive noise zero mean :10.1.1.25.1089
recall section approximation marginalization maximize joint density respect :10.1.1.25.1089
fact random variables yi drawn iid conditionally independent arrive nonzero mean case simply add offset effectively reduces additive noise zero mean :10.1.1.25.1089
smola sch lkopf 
note appears :10.1.1.25.1089
yi mode yi know maximizes need care remainder maximize respect simplification occurs constant typically case additive noise observations :10.1.1.25.1089
maximization respect carried maximizing :10.1.1.25.1089
note gaussian maximum obtained mean cond corresponding normal distribution :10.1.1.25.1089
cond constant consider cond function :10.1.1.25.1089
means reduced problem maximizing :10.1.1.25.1089
depends training data approach computationally attractive :10.1.1.25.1089
summary steps needed reducing :10.1.1.25.1089
conditional distribution yi peaked yi :10.1.1.25.1089
yi constant considered function :10.1.1.25.1089
predict cond known :10.1.1.25.1089

maximizer maximizing posterior probability :10.1.1.25.1089
shall see section assumptions lead optimality may able obtain reasonably estimates :10.1.1.25.1089
situations special cases discussed applies need resort reasoning similar gaussian process classification :10.1.1.25.1089
situations rare discuss detail :10.1.1.25.1089
classification main difference regression classification observations yi part discrete set denote case binary classification :10.1.1.25.1089
multiclass problems :10.1.1.25.1089
clear situation need conditional probability yi transform problem involving gaussian processes :10.1.1.25.1089
lead various models logistic transfer function probit multiclass extensions :10.1.1.25.1089
bayesian kernel methods binary case plays role parameter responsible calibration conditional probabilities yi yi yi :10.1.1.25.1089
discrete random variable gives probability observing classification slightly easier regression provided able solve immediately know confidence random variables arising :10.1.1.25.1089
yi tells estimator classifies xi probability obtaining labels :10.1.1.25.1089
calculations regarding variation quite important regression :10.1.1.25.1089
multiclass classification theory design yi allows multiple discrete values yi assigns corresponding probabilities far clear goal best achieved :10.1.1.25.1089
typically uses vector valued coordinate related probability class occurring :10.1.1.25.1089
derive model observation logistic regression exp yi exp exp exp yi exp exp :10.1.1.25.1089
words probability class occurring proportional exp exp subject normalization constraint defined coordinate extension classes straightforward assume probability class proportional exp normalize yi sum exp yi exp :10.1.1.25.1089
default assumption drawn gaussian process :10.1.1.25.1089
knowledge relation various classes typically assumes coordinates drawn independently :10.1.1.25.1089
extension uncertain labels yi straightforward :10.1.1.25.1089
assume know specific class merely probability pij assessing pattern xi belongs class :10.1.1.25.1089
case need integrate obtain smola sch lkopf replacement yi conditional probability :10.1.1.25.1089
closely related uncertain multiclass settings maximum margin theory :10.1.1.25.1089
see details :10.1.1.25.1089
solve inference problem arising classification case need resort approximations :10.1.1.25.1089
solve integral explicitly main goal approximately maximize joint density :10.1.1.25.1089
convenience study binary case :10.1.1.25.1089
recall appears joint density :10.1.1.25.1089
knowing maximized respect set sgn turn problem jointly maximizing continuous random variables unfortunately resulting optimization problem ill behaved :10.1.1.25.1089
resort em algorithm related method described section :10.1.1.25.1089
unknown labels latent variables obtained jointly expectation write see follows ey log log log log log log denotes value obtained previous estimate :10.1.1.25.1089
proceeds follows step maximizes subsequently updates convergence :10.1.1.25.1089
show algorithm converges local maximum joint density :10.1.1.25.1089
unfortunately practice local maximum achieved process :10.1.1.25.1089
small modification leads expression empirically tends lead better estimates lead local maximum joint density :10.1.1.25.1089
idea remove replace directly :10.1.1.25.1089
case log log log yi log log pj log pj entropy function maximized jointly standard nonlinear optimization methods :10.1.1.25.1089
note maxima fixed point coincide 
know local maxima joint density know yield maxima joint density :10.1.1.25.1089
minimum description length point view minimizing viewed minimizing number bits base convenience bayesian kernel methods needed encode firstly encoding requires bits :10.1.1.25.1089
secondly prior probability remaining terms describe number bits needed encode random variables respect code log log bits :10.1.1.25.1089
shannon optimal code length random variables distributed assumed prior distribution :10.1.1.25.1089
note quite commonly ignores terms dependent comes finding estimate effectively resorts optimization setting described section justification additive noise regression case :10.1.1.25.1089
tends yield acceptable results important bear mind better estimates obtained joint maximization variables carried :10.1.1.25.1089
gives example classification account :10.1.1.25.1089
fig 

gaussian process classification left right knowledge test data :10.1.1.25.1089
circles stars correspond respective classes crosses unlabeled observations :10.1.1.25.1089
note errors introduced ignoring test data left :10.1.1.25.1089
adjusting hyperparameters gaussian processes know exact amount additive noise specific form covariance kernel parameters :10.1.1.25.1089
address problem hyperparameter formalism section needed :10.1.1.25.1089
previous section em approach may costly expectations set hyperparameters set latent variables costly carried :10.1.1.25.1089
consequently practical method available coordinate descent optimize em algorithm fixed maximize respect fixed repeat convergence occurs :10.1.1.25.1089
avoid technicalities discuss special somewhat simpler case regression additive gaussian noise latent variables smola sch lkopf integrated :10.1.1.25.1089
refer reader integration methods markov chain monte carlo approximations see overview :10.1.1.18.3953:10.1.1.17.729:10.1.1.25.1089
specifically assume additive normal noise functions exp det :10.1.1.25.1089
words tells observe :10.1.1.25.1089
maximize integrand respect require information gradient respect :10.1.1.25.1089
explicit expression logarithm monotonic equivalently minimize negative log posterior ln :10.1.1.25.1089
shorthand obtain ln ln det ln tr ln :10.1.1.25.1089
follows standard matrix algebra :10.1.1.25.1089
likewise compute hessian ln respect second order optimization method :10.1.1.25.1089
assume flat hyperprior const optimization simply gradient descent ln words term depending vanishes :10.1.1.25.1089
computing expensive numerically involves inversion matrix :10.1.1.25.1089
option parameterize assume covariance kernel drawn gaussian process case need restrict cone mercer kernels :10.1.1.25.1089
setting optimized called expansion :10.1.1.25.1089
see details :10.1.1.25.1089
exist numerous techniques sparse greedy approximation methods alleviate problem :10.1.1.25.1089
selection techniques section :10.1.1.25.1089
additional detail topic hyperparameter optimization section hyperparameters play crucial role determining sparsity estimate :10.1.1.25.1089
implementation gaussian processes section discuss various methods perform inference case gaussian process classification regression :10.1.1.25.1089
general purpose technical reader encouraged consult literature detail :10.1.1.25.1089
note clearly improper hyperprior may lead overfitting :10.1.1.25.1089
bayesian kernel methods technique laplace approximation essentially application newton method problem minimizing negative log posterior density :10.1.1.25.1089
second order method applicable long log densities second order derivatives :10.1.1.25.1089
readers interested basic ideas gaussian process estimation may skip section :10.1.1.25.1089
classification logistic transfer function variational method section due jaakkola jordan gibbs mackay linear system equations optimization purposes :10.1.1.25.1089
special case regression presence normal noise admits efficient optimization algorithms approximate minimization quadratic forms section :10.1.1.25.1089
subsequently discuss scaling behavior approximation bounds algorithms :10.1.1.25.1089
convenience study problem maximizing ignoring considerations test data put forward section :10.1.1.25.1089
extension methods cases straightforward expectation maximization setting cases essentially impossible direct mdl approach :10.1.1.25.1089
skip 
maximizing equivalent minimizing log log log log log log constant commonly referred negative log posterior remainder section devoted efficient methods minimizing :10.1.1.25.1089
laplace approximation note gaussian process regression additive normal noise yi :10.1.1.25.1089
minimization achieved solving quadratic optimization problem minimum ky identical estimate obtained :10.1.1.25.1089
means normal distributions seeking mode density performing quadratic approximation mode exact :10.1.1.25.1089
general negative log posterior quadratic minimum analytically typically able smola sch lkopf study variation estimate explicitly :10.1.1.25.1089
possible solution successive quadratic approximations negative log posterior minimize iteratively :10.1.1.25.1089
strategy referred laplace approximation newton raphson method numerical analysis see fisher scoring method statistics :10.1.1.25.1089
necessary condition minimum differentiable function derivative :10.1.1.25.1089
convex functions requirement sufficient :10.1.1.25.1089
approximate linearly xg :10.1.1.25.1089
substituting ln definitions ln 
ln ym diag ln 
ln ym obtain update rule new kc kc old :10.1.1.25.1089
usually efficient way finding maximizer log posterior far clear update rule convergent prove need show initial guess lies radius attraction :10.1.1.25.1089
approximation turns practice implementation update rule relatively simple :10.1.1.25.1089
major stumbling block want apply large problems update rule requires inversion matrix :10.1.1.25.1089
costly effectively precludes efficient exact solutions problems size significantly larger due memory computational requirements :10.1.1.25.1089
able provide low rank approximation may compute efficiently :10.1.1.25.1089
instance follows immediately sherman woodbury morrison formula obtain update rule new sub uc old :10.1.1.25.1089
strictly speaking laplace approximation refers fact approximate mode posterior gaussian distribution :10.1.1.25.1089
gaussian approximation second order method order maximize posterior :10.1.1.25.1089
practical purposes approximations just represent different points view subject :10.1.1.25.1089
bayesian kernel methods particular number operations required solve mn :10.1.1.25.1089
numerically stable efficient easily implementable methods sherman woodbury morrison method exist discussion somewhat technical :10.1.1.25.1089
see details :10.1.1.25.1089
ways obtain approximation :10.1.1.25.1089
way project xi random subset dimensions express missing terms linear combination resulting sub matrix nystr method proposed seeger williams :10.1.1.25.1089
construct randomized sparse greedy algorithm select dimensions see details resort positive diagonal pivoting strategy :10.1.1.25.1089
approximation leading principal components done machine learning usually undesirable computation eigensystem costly time required prediction rise number observations expect leading eigenvectors contain significant number zero coefficients :10.1.1.25.1089
variational methods case logistic regression jaakkola jordan compute upper lower bounds logistic exploiting log concavity eq gp logistic model convex function bounded tangent point quadratic sufficiently large curvature provided maximum curvature original function bounded :10.1.1.25.1089
bounds exp exp furthermore binary entropy function ln ln :10.1.1.25.1089
likewise bounds follow :10.1.1.25.1089
equations calculated quite easily linear quadratic functions means fixed parameters optimize upper lower bound log posterior techniques gaussian process regression section :10.1.1.25.1089
approximations tight chosen suitably :10.1.1.25.1089
adapt parameters iteration exact solution instance gradient descent minimizing upper bound maximizing lower bound correspondingly :10.1.1.25.1089
see details :10.1.1.25.1089
factorizations rank degenerate matrices previous section efficient implementation :10.1.1.25.1089
smola sch lkopf logistic lower bound upper bound fig :10.1.1.25.1089

variational approximation :10.1.1.25.1089
note quality approximation varies widely depending value :10.1.1.25.1089
approximate solutions gaussian process regression approximations section indicate efficient ways implementing gaussian process estimation large amounts data find low rank approximation matrix approximation needed practice show exact solutions gaussian processes hard come :10.1.1.25.1089
computed see table scaling behavior prediction mean new location requires operations :10.1.1.25.1089
particular memory requirements store cpu time matrix inversions typically required second order methods scales :10.1.1.25.1089
limit approximation map solution :10.1.1.25.1089
criteria impose posterior probability approximate solution close maximum posterior probability :10.1.1.25.1089
note requirement different requirement closeness approximation represented instance expansion coefficients requirement tresp devised efficient way estimating test set known time training :10.1.1.25.1089
proceeds projecting estimators subspace spanned functions xi xi training data :10.1.1.25.1089
likewise csat opper design iterative algorithm performs gradient descent partial posterior distributions simultaneously projects estimates subspace :10.1.1.25.1089
bayesian kernel methods gibbs mackay :10.1.1.25.1089
proximity coefficients want take account importance individual variables :10.1.1.25.1089
instance invariant transformations scale parameters :10.1.1.25.1089
remainder current section consider additive normal noise :10.1.1.25.1089
log posterior takes quadratic form :10.1.1.25.1089
theorem uses idea gives bound approximation quality minima quadratic forms applicable :10.1.1.25.1089
convenience rewrite terms :10.1.1.25.1089
theorem approximation bounds quadratic forms :10.1.1.25.1089
denote symmetric positive definite matrix define quadratic forms :10.1.1.25.1089
suppose minima lmin min rm lmin min equalities lmin min minimizing addition bound closeness optimum vice versa :10.1.1.25.1089
proof :10.1.1.25.1089
minimum obtained opt minimizes lmin min :10.1.1.25.1089
allows combine lmin min lmin min definition lmin likewise min may solve lmin min obtain lower bounds quantities :10.1.1.25.1089
proves :10.1.1.25.1089
equation useful computing approximation map solution objective function identical ignoring constant terms independent obtain error bars estimate :10.1.1.25.1089
see full rank attains minimum value opt :10.1.1.25.1089
additional minimize :10.1.1.25.1089
smola sch lkopf note calculating variance expensive quantity compute :10.1.1.25.1089
min rm :10.1.1.25.1089
close look reveals expression inside parentheses see :10.1.1.25.1089
consequently approximate minimizer gives upper bound error bars lower bounds obtained :10.1.1.25.1089
practice relative discrepancy upper lower bounds gap determine approximation proceed :10.1.1.25.1089
solutions subspaces central idea algorithm improvements speed achieved reduction number free variables :10.1.1.25.1089
denote extension matrix words projection :10.1.1.25.1089
ansatz find solutions minimized :10.1.1.25.1089
solution opt 
rank minimizer :10.1.1.25.1089
cases approximation :10.1.1.25.1089
rm analyze computational cost involved computing :10.1.1.25.1089
need nm operations evaluate ky operations kp kp operations inversion matrix :10.1.1.25.1089
brings total cost :10.1.1.25.1089
predictions require entails operations :10.1.1.25.1089
likewise may minimize needed upper bound log posterior :10.1.1.25.1089
costs :10.1.1.25.1089
compute posterior variance approximately minimize done cost compute pkp cost likewise upper bounds :10.1.1.25.1089
addition minimize kp costs inverse matrices computed may compute error bars different locations limiting cost :10.1.1.25.1089
accurate lower bounds error bars especially crucial bad estimate leads worst overly conservative confidence intervals negative effect :10.1.1.25.1089
note need compute store kp sub matrix :10.1.1.25.1089
table summarizes scaling behavior optimization algorithms :10.1.1.25.1089
bayesian kernel methods table :10.1.1.25.1089
computational cost various optimization methods :10.1.1.25.1089
note different values conjugate gradient sparse decomposition sparse greedy approximation methods ncg nsd nsga search spaces progressively restricted :10.1.1.25.1089
near optimal results obtained :10.1.1.25.1089
exact conjugate sparse sparse greedy solution gradient decomposition approximation memory nm nm initialization nm training prediction mean error bars nm leads question choose optimum efficiency :10.1.1.25.1089
possibilities include principal components performing conjugate gradient descent minimize performing symmetric diagonal pivoting applying sparse greedy approximation directly :10.1.1.25.1089
methods disadvantage take specific form account lead expansions cost prediction require computation storage full matrix :10.1.1.25.1089
contrast methods data adaptive version sparse greedy approximation algorithm :10.1.1.25.1089
may consider matrices collection unit vectors ei ei ij select number rows equal rank see template sparse greedy algorithm :10.1.1.11.2062:10.1.1.25.1089
choose ei minimal :10.1.1.25.1089
case permit consider possible indices find best trying :10.1.1.25.1089
assume solution contains columns :10.1.1.25.1089
order improve solution expand projection operator matrix pnew ei rm seek best ei pnew minimizes min pnew :10.1.1.25.1089
note method similar matching pursuit iterative reduced set support vector algorithms difference target approximated full solution implicitly :10.1.1.25.1089
zhang proved lower bounds rate sparse approximation schemes :10.1.1.25.1089
particular shows subspace projection algorithms enjoy rate convergence :10.1.1.25.1089
implementation issues performing full search possible indices excessively costly :10.1.1.25.1089
full search remaining indices select basis function prohibitively expensive :10.1.1.25.1089
simple result concerning tails rank smola sch lkopf statistics see comes aid states high probability small subset size chosen random guarantees near optimal performance :10.1.1.25.1089
satisfied finding relatively index best index may resort selecting random subset :10.1.1.25.1089
algorithm sparse greedy quadratic minimization :10.1.1.25.1089
require training data :10.1.1.25.1089
xm targets noise precision corresponding quadratic forms initialize index sets :10.1.1.25.1089

repeat choose find argmin ei opt move set ei ei :10.1.1.25.1089
arg mini ei opt opt opt opt opt output set indices opt kp crucial obtain values opt cheaply ei assuming previously :10.1.1.25.1089
see need rank update inverse :10.1.1.25.1089
show obtained mn operations provided inverse smaller subsystem known :10.1.1.25.1089
expressing relevant terms ki obtain ei old ki ki kii computation terms costs nm know :10.1.1.25.1089
furthermore write inverse strictly positive definite matrix inversion costs :10.1.1.25.1089
find matrix size takes time :10.1.1.25.1089
error bars kp generally starting value minimization typical cost mn mn :10.1.1.25.1089
additional numerical stability required want replace rank update rule cholesky decompositions corresponding positive definite matrix :10.1.1.25.1089
furthermore may want add kernel function chosen positive diagonal pivoting selected subset order ensure sub matrix remains invertible :10.1.1.25.1089
see numerical mathematics textbooks detail update rules :10.1.1.25.1089
hardness approximation results bayesian kernel methods worthwhile study theoretical guarantees performance algorithm described algorithm :10.1.1.25.1089
turns technique closely resembles sparse linear approximation problem studied natarajan find minimal number nonzero entries ax :10.1.1.25.1089
define may write ky constant independent :10.1.1.25.1089
problem sparse approximate minimization special case natarajan problem matrix square strictly positive definite :10.1.1.25.1089
addition algorithm considered involves sequentially choosing columns maximally decrease ax :10.1.1.25.1089
equivalent algorithm described may apply result sparse greedy gaussian process algorithm :10.1.1.25.1089
theorem natarajan :10.1.1.25.1089
sparse greedy algorithm approximately solve problem needs minimize ax ln non zero components minimum number nonzero components vectors ax matrix obtained normalizing columns unit length :10.1.1.25.1089
corollary approximation rate gaussian processes :10.1.1.25.1089
algorithm satisfies opt ln ky non zero components minimum number nonzero components vectors opt smallest magnitude singular values matrix obtained normalizing columns show np hardness sparse approximation gaussian process regression :10.1.1.25.1089
theorem holds smola sch lkopf theorem np hardness approximate gp regression :10.1.1.25.1089
exist kernels labels problem finding minimal set indices minimize corresponding quadratic function precision nphard :10.1.1.25.1089
proof :10.1.1.25.1089
hardness result theorem natarajan quadratic approximation problem terms specifically proceed opposite direction show exist equivalent optimization problem :10.1.1.25.1089
ax value enters means find :10.1.1.25.1089
check possible find suitable positive definite identical eigensystems subsequently solving equations ai respective eigenvalues ai furthermore satisfy ba :10.1.1.25.1089
see recall ba linear combination nonzero eigenvectors rank image vector ba represented exists equivalent proves np hardness reduction :10.1.1.25.1089
shows sparse greedy algorithm efficient approximate solution np hard problem :10.1.1.25.1089
experimental evidence conclude section brief experimental demonstration efficiency sparse greedy approximation methods abalone dataset :10.1.1.25.1089
specifically gaussian covariance kernels split data training test examples assess training speed assess generalization performance training test set split :10.1.1.25.1089
optimal parameters chosen average test error sparse greedy approximation trained gap indistinguishable corresponding error obtained exact solution full system :10.1.1.25.1089
applies log posterior :10.1.1.25.1089
see table details :10.1.1.25.1089
consequently practical purposes full inversion covariance matrix sparse greedy approximation comparable generalization performance :10.1.1.25.1089
important quantity practice number basis functions needed minimize log posterior sufficiently high precision :10.1.1.25.1089
table shows number precision gap variation function kernel width dependency observed number gap bayesian kernel methods number iterations fig :10.1.1.25.1089

speed convergence :10.1.1.25.1089
plot size gap upper lower bound log posterior gap samples abalone dataset :10.1.1.25.1089
top bottom subsets size :10.1.1.25.1089
results averaged runs :10.1.1.25.1089
relative variance gap size :10.1.1.25.1089
see subsets size ensure rapid convergence :10.1.1.25.1089
kernels determines time memory needed prediction training :10.1.1.25.1089
cases kernel functions suffice find minimizer log posterior sufficient compute error bars :10.1.1.25.1089
significant improvement direct minimization approach :10.1.1.25.1089
similar result obtained larger datasets :10.1.1.25.1089
illustrate generated synthetic data set size adding normal noise variance function consisting randomly chosen gaussians width normally distributed expansion coefficients centers :10.1.1.25.1089
avoid trivial sparse expansions deliberately inadequate gaussian process prior correct noise level consisting gaussians width :10.1.1.25.1089
iterations basis functions size gap :10.1.1.25.1089
demonstrates feasibility sparse greedy approach larger datasets :10.1.1.25.1089
laplacian processes far dependency latent variables factorized easily terms xi :10.1.1.25.1089
xi 
due fact smola sch lkopf table 
performance sparse greedy approximation vs explicit solution full learning problem :10.1.1.25.1089
experiments abalone dataset split training test samples :10.1.1.25.1089
obtain reliable estimates algorithm run random splits dataset :10.1.1.25.1089
generalization error log posterior optimal solution sparse greedy approximation table :10.1.1.25.1089
number basis functions needed minimize log posterior abalone dataset training samples various kernel widths :10.1.1.25.1089
number basis functions required approximate needed compute error bars :10.1.1.25.1089
results averaged test samples :10.1.1.25.1089
kernel width kernels log posterior kernels error bars want infer knowing value amodel couple various yi fail regard :10.1.1.25.1089
simple trick achieve factorization retain inference properties estimator introduce layer dependence modelling assume xi yi ti :10.1.1.25.1089
moved mixing various yi design matrix note requirement positive semidefinite :10.1.1.25.1089
fact arbitrary matrix :10.1.1.25.1089
practical purposes typically choose function kij xi xj :10.1.1.25.1089
go technical details give motivation complexity estimate depend locations data occurs effectively updating prior assumptions observing data placement :10.1.1.25.1089
note modify prior assumptions targets yi result distribution patterns xi different input distribution densities instance correspond different assumptions regarding smoothness function class estimated :10.1.1.25.1089
example advisable favor smooth functions areas data scarce allow complicated functions observations abound :10.1.1.25.1089
care smoothness regions little chance patterns occurring problem handwritten digit recognition bayesian kernel methods care behavior estimator inputs looking faces :10.1.1.25.1089
specific benefit strategy provides correspondence linear programming regularization bayesian priors function spaces analogy regularization reproducing kernel hilbert spaces gaussian processes :10.1.1.135.1907:10.1.1.25.1089
examples factorizing priors study priors factorizing coefficient space :10.1.1.25.1089
construction exp ti ik xi :10.1.1.25.1089
chosen exp integrable corresponding normalization term xi examples priors depend locations xi include feature selection prior weight decay prior laplacian prior :10.1.1.25.1089
prior introduced log concave :10.1.1.25.1089
characteristic unfavorable general corresponding optimization problem exhibits local minima negative log posterior strictly concave choose laplacian noise equivalent loss regression :10.1.1.25.1089
basic result convex analysis means optimum occurs extreme points optimization feasible :10.1.1.25.1089
eq 
describes popular weight decay prior bayesian neural networks :10.1.1.31.4284:10.1.1.25.1089
assumes coefficients independently normally distributed :10.1.1.25.1089
relax assumption common normal distribution section introduce individual hyper parameters si :10.1.1.25.1089
resulting prior si exp si leads construction relevance vector machine sparse function expansions :10.1.1.25.1089:10.1.1.25.1089
assumption underlying laplacian prior basis functions nonzero :10.1.1.25.1089
specific form prior call estimators laplacian processes :10.1.1.25.1089
prior significant advantages leads convex optimization problems integral carl magnus rasmussen discussions suggestions :10.1.1.25.1089
smola sch lkopf finite allows normalization case call improper prior :10.1.1.25.1089
laplacian prior corresponds regularization functional employed sparse coding approaches wavelet dictionaries coding natural images independent component analysis linear programming regression :10.1.1.25.1089
focus 
straightforward see map estimate obtained minimizing negative log posterior constant terms ln yi xi xi :10.1.1.25.1089
depending ln yi ti may formulate minimization linear quadratic program :10.1.1.25.1089
samples prior order illustrate reasoning show priors correspond useful distributions generate samples prior distribution :10.1.1.25.1089
gaussian processes smooth kernels correspond smooth priors :10.1.1.25.1089
surprising show section theorem exists corresponding gaussian process kernel distribution :10.1.1.25.1089
obvious advantage need worry mercer condition take arbitrary function generate laplacian process :10.1.1.25.1089
draw samples kernels gaussian rbf kernel laplacian rbf kernel tanh neural networks kernel :10.1.1.25.1089
valid kernels gaussian process estimation satisfy mercer condition gives sample realizations corresponding process :10.1.1.25.1089
impossible gp priors diagonalize matrix explicitly render positive definite replacing :10.1.1.25.1089
costly procedure see involves computing eigensystem estimation aims laplacian prior coefficients achieve sparsity expansion appear sensible bayesian averaging covariance matrix positive definite times :10.1.1.25.1089
analogous application theory conditionally positive definite kernels possible :10.1.1.25.1089
simply assumes gaussian process prior linear subspace yi :10.1.1.25.1089
bayesian kernel methods fig :10.1.1.25.1089

left column grayscale plots realizations laplacian processes :10.1.1.25.1089
black dots represent data points :10.1.1.25.1089
right column plots samples process :10.1.1.25.1089
data points sampled random uniform distribution :10.1.1.25.1089
top bottom gaussian kernel laplacian kernel neural networks kernel :10.1.1.25.1089
note laplacian kernel significantly smooth gaussian kernel gaussian process laplacian kernels :10.1.1.25.1089
observe neural networks kernel corresponds non stationary process covariance properties translation invariant :10.1.1.25.1089
smola sch lkopf scheme compute mean posterior distribution scheme leads nonzero coefficients :10.1.1.25.1089
seek obtain mode distribution map estimate :10.1.1.25.1089
pointed previous section finding mode need give exact solution mode mean coincide laplacian regularization recall :10.1.1.25.1089
map estimate computationally attractive ln convex optimization problem unique minimum :10.1.1.25.1089
assumption write joint density follows yi ti yi ti exp exploited fact prior factorizes data generated iid :10.1.1.25.1089
maximization equivalent minimizing negative log posterior leads minimize ln yi ti subject log leads linear program solution readily map estimate laplacian processes similar reasoning holds soft margin loss functions :10.1.1.25.1089
likewise gaussian noise obtain quadratic program simple objective function dense set constraints analogy basis pursuit :10.1.1.25.1089
confidence intervals gaussian noise key advantages bayesian modeling obtain explicit confidence intervals predictions provided assumptions regarding priors distribution satisfied :10.1.1.25.1089
gaussian noise explicit meaningful expansion map estimate map possible non differentiable quadratic approximation :10.1.1.25.1089
slight modification permits computationally efficient approximation error bounds :10.1.1.25.1089
modification consists dropping variables map expansion renders distribution flatter overestimates error replacing remaining variables linear approximations replace sgn map :10.1.1.25.1089
words assume variables zero coefficients influence expansion signs remaining variables change :10.1.1.25.1089
bayesian kernel methods sensible approximation large sample sizes laplacian processes designed address posterior strongly peaked mode :10.1.1.25.1089
contribution log map considered approximately linear :10.1.1.25.1089
denote vector nonzero variables obtained deleting entries map vector elements map km matrix generated removing columns corresponding map :10.1.1.25.1089
posterior written terms convenience approximated exp yi km collecting linear quadratic terms see exp :10.1.1.25.1089
map km map km :10.1.1.25.1089
equation map follows conditions optimal solution quadratic programming problem directly maximizing fixed :10.1.1.25.1089
predictions new point approximately normally distributed km km km km :10.1.1.25.1089
xm xi nonzero map considered :10.1.1.25.1089
additional stems fact additive gaussian noise variance addition laplacian process :10.1.1.25.1089
equation expensive compute cheaper invert dense square matrix map may sparse :10.1.1.25.1089
addition greedy approximation methods described instance section column generation techniques render computation numerically efficient :10.1.1.25.1089
equivalent gaussian process conclude section proof large sample size limit exists gaussian process kernel expansion prior coefficients purpose proof slightly modify normalization condition assume yi ik xi exp :10.1.1.25.1089
large sample size theorem holds :10.1.1.25.1089
smola sch lkopf theorem convergence gaussian process :10.1.1.25.1089
denote independent random variables require identical distributions unit variance zero mean :10.1.1.25.1089
furthermore assume exists distribution sample :10.1.1.25.1089
xm drawn bounded random variable converges gaussian process zero mean covariance function :10.1.1.25.1089
means laplacian process prior factorizing prior expansion coefficients limit obtain equivalent stochastic process :10.1.1.25.1089
proof :10.1.1.25.1089
prove part need check linear combination xj arbitrary converge normal distribution :10.1.1.25.1089
application theorem cram sufficient prove distributed gaussian process :10.1.1.25.1089
random variable sum independent random variables bounded variance bounded :10.1.1.25.1089
limit virtue central limit theorem arbitrary linear combinations gaussian distributions iyi jk xi xj allows application central limit theorem sum inner sum jk xi xj bounded xi :10.1.1.25.1089
implies jyj proves distributed gaussian process :10.1.1.25.1089
show note zero mean :10.1.1.25.1089
covariance function finite expectation respect jk xi xj xi xj independent zero mean :10.1.1.25.1089
expression converges riemann integral density asm :10.1.1.25.1089
completes proof :10.1.1.25.1089
bayesian kernel methods relevance vector machines deconvolution problems probabilistic model introduced may lead somewhat intractable optimization problems particular negative log posterior convex :10.1.1.25.1089
functions may correspond useful statistical assumptions distribution coefficients function expansions :10.1.1.25.1089
turning priors tipping proposed method circumvent numerical problems inherent certain set deconvolution :10.1.1.25.1089:10.1.1.25.1089
presenting specific choices sake sparse function expansions general principle extended priors coefficients likelihood terms alike ideas :10.1.1.25.1089
method works follows assume prior si si exp si si plays role hyperprior :10.1.1.25.1089
si studied methods inference gaussian prior perform required estimation steps :10.1.1.25.1089
quite able perform exact inference hyperparameters si :10.1.1.25.1089
key suitable choice si clearly si si si dsi exp si si dsi :10.1.1.25.1089
means may able find si holds 
hyperprior si large weight si desirable leads distribution :10.1.1.25.1089
parameter transformation integral yields exp :10.1.1.25.1089
transform corre si inverse laplace transform suitable variable changes :10.1.1.25.1089
fact allows match priors corresponding si fairly automatic fashion :10.1.1.25.1089
particular assume normal distribution adjustable variance :10.1.1.25.1089
determined hyperparameter value prior expressed analytically normal hyperprior normal distribution si si exp :10.1.1.25.1089
smola sch lkopf performing laplace transform leads modified bessel function second kind :10.1.1.25.1089
see properties function :10.1.1.25.1089
density log fig 

normal hyperprior :10.1.1.25.1089
left right log 
note sharp peak linear increase :10.1.1.25.1089
hyperprior tipping gamma hyperprior design relevance vector machine :10.1.1.25.1089:10.1.1.25.1089
si si sa ba exp sib si :10.1.1.25.1089
non informative flat logspace priors typically chooses leads polynomial prior exp ln :10.1.1.25.1089
note heavily peaked si :10.1.1.25.1089
regression similar assumption concerning amount additive gaussian noise typically note priors imposed inverse :10.1.1.25.1089
depicts scaling behavior non informative priors :10.1.1.25.1089
choices possible obtained consulting tables legendre transformations :10.1.1.25.1089
density bayesian kernel methods log fig :10.1.1.25.1089

gamma hyperprior :10.1.1.25.1089
left right log 
note distribution peaked observe sublinear increase negative log density :10.1.1.25.1089
expansions similar approach transform arbitrary laplacian distributions suitable hyperprior :10.1.1.25.1089
connection laplace transform time si directly inverse argument done exp :10.1.1.25.1089
means laplace transform si si yields effective prior vice versa :10.1.1.25.1089
laplacian distribution may quite desirable normal distribution typically lead solution linear program simple matrix inversion favorable direct attempt computing mode distribution :10.1.1.25.1089
worth noting employ methods obtain normal distribution transform yi ti normal distributions combined hyperprior yi ti exp yi ti :10.1.1.25.1089
ensuing considerations completely analogous ones previous chapter :10.1.1.25.1089
come back dealing general factorizing estimation problems :10.1.1.25.1089
regression hyperparameters simple case regression additive gaussian noise :10.1.1.25.1089
know smola sch lkopf diag :10.1.1.25.1089
sm 
satisfies :10.1.1.25.1089
consequently obtain exp :10.1.1.25.1089
wish determine need estimate assuming matrix inversion formula plus application sherman morrison woodbury formula yields :10.1.1.25.1089
kk 
follows directly example :10.1.1.25.1089
estimation training set equations reduce :10.1.1.25.1089
elimination integration impossible resort times approximation maximizing joint density si :10.1.1.25.1089
assuming gamma prior si negative logarithm written ln ln ks ks ln si ln const :10.1.1.25.1089
course set flat prior terms vanish left maximizing parameters improper prior :10.1.1.25.1089
note similarity logarithmic barrier methods constrained optimization constrained minimization problems transformed unconstrained problems adding logarithms constraints initial objective function :10.1.1.25.1089
words gamma distribution viewed positivity constraint hyperparameters si differentiating setting corresponding terms leads update rules si si ii bayesian kernel methods kk quantity si ii measure degree corresponding parameter determined data :10.1.1.25.1089
likewise obtain 
si ii turns parameters si tend infinity optimization process :10.1.1.25.1089
means corresponding distribution strongly peaked may drop variables optimization process :10.1.1.25.1089
speeds process minimization progresses :10.1.1.25.1089
wasteful consider full set possible functions xi weed functions needed prediction :10.1.1.25.1089
greedy method building predictors similar greedy strategy employed gaussian processes section :10.1.1.25.1089
approach proposes algorithm :10.1.1.25.1089
initializing predictor single basis function bias example test new basis function yields improvement :10.1.1.25.1089
achieved guessing large initial value si performing update step :10.1.1.25.1089
leads increase si reject corresponding basis function retain optimization process :10.1.1.25.1089
classification classification follow scheme similar section :10.1.1.25.1089
order keep matters simple consider binary classification case :10.1.1.25.1089
specifically carry logistic regression model distribution labels yi :10.1.1.25.1089
regression kernel expansion time latent variables :10.1.1.25.1089
negative log density conditioned ln ln yi ti ln si const 
:10.1.1.25.1089
regression minimize explicitly resort approximate methods laplace approximation see section :10.1.1.25.1089
computing second derivatives definitions yields ln kc ln ck :10.1.1.25.1089
allows obtain map estimate iterative application obtain update rule manner analogous new old ck kc old ck ck old :10.1.1.25.1089
smola sch lkopf iteration scheme converges converge minimum negative log posterior :10.1.1.25.1089
provide iterative method updating hyperparameters note need :10.1.1.25.1089
integrate explicitly resort iterative method obtain mode distribution best gaussian approximation obtained :10.1.1.25.1089
gives approximation value posterior distribution allows apply update rules developed regression classification :10.1.1.25.1089
setting map ck optimize si 
see detail motivation :10.1.1.25.1089
summary overview common techniques bayesian estimation gaussian processes relevance vector machine novel method laplacian processes :10.1.1.25.1089
due wealth existing concepts algorithms developed bayesian statistics impossible give comprehensive treatment single :10.1.1.25.1089
goal merit writing book right :10.1.1.25.1089
refer reader detail :10.1.1.25.1089
topics left discuss markov chain monte carlo methods application bayesian estimation alternate way performing bayesian inference :10.1.1.25.1089
sampling posterior distribution computing approximation mode :10.1.1.25.1089
model side maximum entropy discrimination paradigm worthy concept right powerful spawn family new inference algorithms kernels :10.1.1.25.1089
main idea seek informative estimate prediction purposes :10.1.1.25.1089
addition requiring specific function satisfy certain constraints require distribution satisfy constraints average :10.1.1.25.1089
methods bayes point machine kernel billiard estimation purposes :10.1.1.25.1089
idea methods play billiard version space average existing trajectories :10.1.1.25.1089
version space set separating hyperplanes empirical risk vanishes bounded previously chosen constant :10.1.1.25.1089
proponents strategy claim rapid convergence due mixing properties dynamical system :10.1.1.25.1089
left field graphical models see instance completely untouched :10.1.1.25.1089
algorithms model dependency structure different random variables explicit fashion efficient approximate inference techniques solve optimization problems :10.1.1.25.1089
clear combine graphical models kernels :10.1.1.25.1089
key issues topics covered include deterministic approximate methods bayesian inference emphasis maximum posteriori bayesian kernel methods map estimate treatment hyperparameters :10.1.1.25.1089
side effect observe minimization regularized risk closely related approximate bayesian estimation :10.1.1.25.1089
consequences link connection gaussian processes support vector machines :10.1.1.25.1089
defined terms correlations random variables derived smoothness assumptions regarding estimate feature space considerations :10.1.1.25.1089
connection allows exchange uniform convergence statements bayesian error bounds types reasoning :10.1.1.25.1089
side effect connection gives rise new class prior corresponding regularization linear programming machines :10.1.1.25.1089
coefficients follow laplacian distribution name corresponding stochastic process laplacian process :10.1.1.25.1089
new point view allows derivation error bars estimates way easily possible statistical learning theory framework :10.1.1.25.1089
turns leads data dependent prior function space :10.1.1.25.1089
relevance vector machine introduces individual hyperparameters distributions coefficients certain optimization problems tractable matrix inversion remained infeasible map estimate student distribution prior :10.1.1.25.1089
expect technique representing complex distributions normal distribution cum hyperprior promising approach estimation problems :10.1.1.25.1089
view expect convergence different estimation algorithms inference principles derived risk minimization bayesian estimation minimum description length concepts :10.1.1.25.1089
laplacian processes relevance vector machine examples convergence :10.1.1.25.1089
hope methods follow years :10.1.1.25.1089
:10.1.1.25.1089
bennett demiriz shawe taylor :10.1.1.25.1089
column generation algorithm boosting :10.1.1.25.1089
langley editor proceedings international conference machine learning san francisco :10.1.1.25.1089
morgan kaufmann publishers :10.1.1.25.1089

bennett mangasarian :10.1.1.25.1089
robust linear programming discrimination linearly inseparable sets :10.1.1.25.1089
optimization methods software :10.1.1.25.1089

bishop 
neural networks pattern recognition :10.1.1.25.1089
clarendon press oxford :10.1.1.25.1089

bishop tipping 
variational relevance vector machines :10.1.1.25.1089
proceedings th conference uncertainty artificial intelligence uai pages :10.1.1.25.1089

bradley mangasarian :10.1.1.25.1089
feature selection concave minimization support vector machines :10.1.1.25.1089
shavlik editor proceedings international conference machine learning pages san francisco california :10.1.1.25.1089
morgan kaufmann publishers :10.1.1.25.1089
ftp ftp cs wisc edu math prog tech reports ps :10.1.1.25.1089
chen donoho saunders :10.1.1.25.1089
atomic decomposition basis pursuit :10.1.1.25.1089
siam journal scientific computing :10.1.1.25.1089
smola sch lkopf 
cover thomas :10.1.1.25.1089
elements information theory :10.1.1.25.1089
john wiley sons new york :10.1.1.25.1089

cram mathematical methods statistics :10.1.1.25.1089
princeton university press :10.1.1.25.1089

csat opper :10.1.1.25.1089
sparse representation gaussian process models :10.1.1.25.1089
leen dietterich tresp editors advances neural information processing systems pages :10.1.1.25.1089
mit press :10.1.1.25.1089

dempster laird rubin :10.1.1.25.1089
maximum likelihood incomplete data em algorithm :10.1.1.25.1089
journal royal statistical society :10.1.1.25.1089

duane kennedy 
hybrid monte carlo :10.1.1.25.1089
physics letters :10.1.1.25.1089

fine :10.1.1.25.1089
efficient svm training low rank kernel representation :10.1.1.25.1089
technical report ibm watson research center new york :10.1.1.25.1089

fletcher :10.1.1.25.1089
practical methods optimization :10.1.1.25.1089
john wiley sons new york :10.1.1.25.1089

fung mangasarian :10.1.1.25.1089
data selection support vector machine classifiers :10.1.1.25.1089
proceedings kdd :10.1.1.25.1089
data mining institute technical report university wisconsin madison :10.1.1.25.1089

gelman carlin stern rubin :10.1.1.25.1089
bayesian data analysis 
chapman hall london 

gibbs mackay 
variational gaussian process classifiers :10.1.1.25.1089
technical report cavendish laboratory cambridge uk :10.1.1.25.1089

gibbs 
bayesian gaussian methods regression classification :10.1.1.25.1089
phd thesis university cambridge :10.1.1.25.1089

mark gibbs david mackay :10.1.1.25.1089
efficient implementation gaussian processes :10.1.1.25.1089
technical report cavendish laboratory cambridge uk :10.1.1.25.1089
available wol ra phy cam ac uk mng gp :10.1.1.25.1089

gill murray wright :10.1.1.25.1089
practical optimization :10.1.1.25.1089
academic press :10.1.1.25.1089

girosi :10.1.1.25.1089
models noise robust estimates :10.1.1.25.1089
memo artificial intelligence laboratory massachusetts institute technology :10.1.1.25.1089

goldfarb :10.1.1.25.1089
product form cholesky factorization method handling dense columns interior point methods linear programming :10.1.1.25.1089
technical report ibm watson research center yorktown heights :10.1.1.25.1089

golub van loan 
matrix computations :10.1.1.25.1089
john hopkins university press baltimore md rd edition :10.1.1.25.1089

:10.1.1.25.1089
table integrals series products :10.1.1.25.1089
academic press new york :10.1.1.25.1089

graepel herbrich obermayer :10.1.1.25.1089
classification pairwise proximity data :10.1.1.25.1089
kearns solla cohn editors advances neural information processing systems pages cambridge ma :10.1.1.25.1089
mit press :10.1.1.25.1089

haussler :10.1.1.25.1089
convolutional kernels discrete structures :10.1.1.25.1089
technical report ucsc crl computer science department uc santa cruz :10.1.1.25.1089

herbrich :10.1.1.25.1089
learning kernel classifiers theory algorithms :10.1.1.25.1089
mit press :10.1.1.25.1089

ralf herbrich graepel colin campbell :10.1.1.25.1089
bayes point machines estimating bayes point kernel space :10.1.1.25.1089
proceedings ijcai workshop support vector machines pages :10.1.1.25.1089

horn johnson :10.1.1.25.1089
matrix analysis :10.1.1.25.1089
cambridge university press cambridge :10.1.1.25.1089
bayesian kernel methods :10.1.1.25.1089
huber :10.1.1.25.1089
robust statistics review :10.1.1.25.1089
annals statistics 

jaakkola meila jebara :10.1.1.25.1089
maximum entropy discrimination :10.1.1.25.1089
technical report artificial intelligence laboratory massachusetts institute technology :10.1.1.25.1089

jaakkola haussler :10.1.1.25.1089
exploiting generative models discriminative classifiers :10.1.1.25.1089
kearns solla cohn editors advances neural information processing systems pages cambridge ma :10.1.1.25.1089
mit press :10.1.1.25.1089

jaakkola jordan :10.1.1.25.1089
computing upper lower bounds likelihoods networks :10.1.1.25.1089
proceedings th conference uncertainty ai :10.1.1.25.1089
morgan kaufmann publishers :10.1.1.25.1089

james stein 
estimation quadratic loss :10.1.1.25.1089
proceedings fourth berkeley symposium mathematics statistics probability volume pages berkeley :10.1.1.25.1089
university california press :10.1.1.25.1089

jebara jaakkola :10.1.1.25.1089
feature selection dualities maximum entropy discrimination :10.1.1.25.1089
uncertainty artificial intelligence :10.1.1.25.1089

jordan bishop :10.1.1.25.1089
probabilistic graphical models :10.1.1.25.1089
mit press :10.1.1.25.1089

jordan jaakkola saul :10.1.1.25.1089
variational methods graphical models :10.1.1.25.1089
learning graphical models volume jordan pages :10.1.1.25.1089
kluwer academic :10.1.1.25.1089

lewicki sejnowski 
learning nonlinear overcomplete representations efficient coding :10.1.1.25.1089
jordan kearns solla editors advances neural information processing systems pages cambridge ma :10.1.1.25.1089
mit press :10.1.1.25.1089

luenberger :10.1.1.25.1089
linear nonlinear programming :10.1.1.25.1089
addison wesley reading ma 


handbook matrices :10.1.1.25.1089
john wiley sons chichester :10.1.1.25.1089

mackay 
bayesian methods adaptive models :10.1.1.25.1089
phd thesis computation neural systems california institute technology pasadena ca :10.1.1.25.1089

mackay 
evidence framework applied classification networks :10.1.1.25.1089
neural computation :10.1.1.25.1089

mallat zhang 
matching pursuit time frequency dictionary :10.1.1.25.1089
ieee transactions signal processing :10.1.1.25.1089

mangasarian :10.1.1.25.1089
linear nonlinear separation patterns linear programming :10.1.1.25.1089
operations research :10.1.1.25.1089

natarajan :10.1.1.25.1089
sparse approximate solutions linear systems :10.1.1.25.1089
siam journal computing :10.1.1.25.1089

neal 
priors infinite networks :10.1.1.25.1089
technical report crg tr dept computer science university toronto :10.1.1.25.1089

neal 
bayesian learning neural networks :10.1.1.25.1089
springer :10.1.1.25.1089

radford neal :10.1.1.25.1089
probabilistic inference markov chain monte carlo methods :10.1.1.25.1089
technical report dept computer science university toronto :10.1.1.25.1089
crg tr 

olshausen field 
emergence simple cell receptive field properties learning sparse code natural images :10.1.1.25.1089
nature :10.1.1.25.1089

opper winther :10.1.1.25.1089
mean field methods classification gaussian processes :10.1.1.25.1089
kearns solla cohn editors advances neural information processing systems pages cambridge ma :10.1.1.25.1089
mit press :10.1.1.25.1089
smola sch lkopf 
opper winther :10.1.1.25.1089
gaussian processes svm mean field :10.1.1.25.1089
smola bartlett sch lkopf schuurmans editors advances large margin classifiers pages cambridge ma :10.1.1.25.1089
mit press :10.1.1.25.1089

platt 
probabilities sv machines :10.1.1.25.1089
smola bartlett sch lkopf schuurmans editors advances large margin classifiers pages cambridge ma :10.1.1.25.1089
mit press :10.1.1.25.1089

poggio 
optimal nonlinear associative recall :10.1.1.25.1089
biological cybernetics :10.1.1.25.1089

press teukolsky vetterling flannery :10.1.1.25.1089
numerical recipes art scientific computing nd ed :10.1.1.25.1089
cambridge university press cambridge :10.1.1.25.1089
isbn 

rasmussen 
evaluation gaussian processes methods non linear regression :10.1.1.25.1089
phd thesis department computer science university toronto :10.1.1.25.1089
ftp ftp cs toronto edu pub carl thesis ps gz :10.1.1.25.1089

tsch mika smola 
adapting codes und embeddings :10.1.1.25.1089
neural information processing systems volume :10.1.1.25.1089
mit press :10.1.1.25.1089
appear :10.1.1.25.1089

ripley 
pattern recognition neural networks :10.1.1.25.1089
cambridge university press cambridge :10.1.1.25.1089

rockafellar :10.1.1.25.1089
convex analysis volume princeton mathematics series :10.1.1.25.1089
princeton university press :10.1.1.25.1089

marchand :10.1.1.25.1089
computing bayes kernel classifier :10.1.1.25.1089
smola bartlett sch lkopf schuurmans editors advances large margin classifiers pages cambridge ma :10.1.1.25.1089
mit press :10.1.1.25.1089

playing version space :10.1.1.25.1089
neural computation :10.1.1.25.1089

sch lkopf mika burges :10.1.1.25.1089
ller tsch smola :10.1.1.25.1089
input space vs feature space kernel methods :10.1.1.25.1089
ieee transactions neural networks :10.1.1.25.1089

sch lkopf smola 
ller :10.1.1.25.1089
kernel principal component analysis :10.1.1.25.1089
sch lkopf burges smola editors advances kernel methods support vector learning pages :10.1.1.25.1089
mit press cambridge ma :10.1.1.25.1089

sch lkopf smola 
learning kernels :10.1.1.25.1089
mit press cambridge ma :10.1.1.25.1089

seeger :10.1.1.25.1089
bayesian methods support vector machines gaussian processes :10.1.1.25.1089
master thesis university edinburgh division informatics :10.1.1.25.1089

skilling 
maximum entropy bayesian methods :10.1.1.25.1089
cambridge university press :10.1.1.25.1089

smola sch lkopf tsch 
linear programs automatic accuracy control regression :10.1.1.25.1089
ninth international conference artificial neural networks conference publications pages london :10.1.1.25.1089
iee 

smola 
learning kernels :10.1.1.25.1089
phd thesis technische universit berlin :10.1.1.25.1089
gmd research series :10.1.1.25.1089

smola bartlett :10.1.1.25.1089
sparse greedy gaussian process regression :10.1.1.25.1089
leen dietterich tresp editors advances neural information processing systems pages :10.1.1.25.1089
mit press :10.1.1.25.1089

smola sch lkopf 
sparse greedy matrix approximation machine learning :10.1.1.25.1089
langley editor proceedings international conference machine learning pages san francisco :10.1.1.25.1089
morgan kaufmann publishers :10.1.1.25.1089
bayesian kernel methods :10.1.1.25.1089
smola vishwanathan 
cholesky factorization rank modifications diagonal matrices :10.1.1.25.1089
siam journal matrix analysis :10.1.1.25.1089
submitted 

soon ong smola williamson 
:10.1.1.25.1089
neural information processing systems volume :10.1.1.25.1089
mit press :10.1.1.25.1089
appear :10.1.1.25.1089

spiegelhalter lauritzen :10.1.1.25.1089
sequential updating conditional probabilities directed graphical structures :10.1.1.25.1089
networks :10.1.1.25.1089

:10.1.1.25.1089
numerical analysis :10.1.1.25.1089
springer new york second edition :10.1.1.25.1089

tipping 
sparse bayesian learning relevance vector machine :10.1.1.25.1089
journal machine learning research :10.1.1.25.1089

tresp :10.1.1.25.1089
bayesian committee machine 
neural computation :10.1.1.25.1089

vapnik 
nature statistical learning theory :10.1.1.25.1089
springer new york :10.1.1.25.1089

vapnik smola 
support vector method function approximation regression estimation signal processing :10.1.1.25.1089
mozer jordan petsche editors advances neural information processing systems pages cambridge ma :10.1.1.25.1089
mit press :10.1.1.25.1089

wahba 
spline models observational data volume cbms nsf regional conference series applied mathematics :10.1.1.25.1089
siam philadelphia 

watkins 
dynamic alignment kernels :10.1.1.25.1089
csd tr royal holloway university london egham surrey uk :10.1.1.25.1089

watson 
treatise theory bessel functions :10.1.1.25.1089
cambridge university press cambridge uk edition :10.1.1.25.1089

williams 
prediction gaussian processes linear regression linear prediction :10.1.1.25.1089
jordan editor learning inference graphical models :10.1.1.25.1089
kluwer academic :10.1.1.25.1089

williams 
prediction gaussian processes linear regression linear prediction :10.1.1.25.1089
jordan editor learning inference graphical models pages :10.1.1.25.1089
mit press :10.1.1.25.1089

williams rasmussen 
gaussian processes regression :10.1.1.25.1089
touretzky mozer hasselmo editors advances neural information processing systems pages cambridge ma :10.1.1.25.1089
mit press :10.1.1.25.1089

williams matthias seeger :10.1.1.25.1089
nystrom method speed kernel machines :10.1.1.25.1089
leen dietterich tresp editors advances neural information processing systems pages cambridge ma :10.1.1.25.1089
mit press :10.1.1.25.1089

christopher williams david barber :10.1.1.25.1089
bayesian classification gaussian processes :10.1.1.25.1089
ieee transactions pattern analysis machine intelligence pami :10.1.1.25.1089

zhang 
sparse approximation bounds regression problems :10.1.1.25.1089
proc :10.1.1.25.1089
th international conf :10.1.1.25.1089
machine learning pages :10.1.1.25.1089
morgan kaufmann san francisco ca :10.1.1.25.1089
