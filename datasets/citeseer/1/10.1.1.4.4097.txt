gaussian processes machine learning matthias seeger department eecs university california berkeley soda hall berkeley ca usa cs berkeley edu february gaussian processes gps natural generalisations multivariate gaussian random variables nite countably continuous index sets 
gps applied large number elds diverse range ends deep theoretical analyses various properties available 
gives gaussian processes fairly elementary level special emphasis characteristics relevant machine learning 
draws explicit connections branches spline smoothing models support vector machines similar ideas investigated 
gaussian process models routinely solve hard machine learning problems 
attractive exible non parametric nature computational simplicity 
treated bayesian framework powerful statistical methods implemented er valid estimates uncertainties predictions generic model selection procedures cast nonlinear optimization problems 
main drawback heavy computational scaling alleviated generic sparse approximations :10.1.1.25.1089:10.1.1.12.8842
mathematical literature gps large uses deep concepts required fully understand machine learning applications 
tutorial aim characteristics gps relevant machine learning show precise connections kernel machines popular community 
focus simple presentation detailed sources provided 
overview gaussian processes nutshell section introduce basic reasoning non parametric random eld gaussian process models 
readers exposed concepts may jump section overview remaining sections 
machine learning problems aim generalise nite set observed data sense ability predict uncertain aspects problem improves making previously institute adaptive neural computation university edinburgh uk 
observations 
possible postulate priori relationship variables observe ones wish predict 
relationship uncertain making generalisation non trivial problem 
example spatial statistics observe values function certain locations want predict ones 
temporal statistics want predict values time series past 
situations interested postulated relationship represented ensemble distribution functions 
helpful imagine observed data generated picking function ensemble gives rise sample typically observations imperfect noisy 
important stress generative view crude abstraction mechanism really hold capable simulating phenomenon long probabilistic inversion leads satisfying predictions 
inversion obtained conditioning generative ensemble observed data leads new adapted ensemble pinned observation points variable 
parametric statistics agree function class indexed nite number parameters 
distribution parameters induces ensemble functions 
learning observations means modify distribution adapt ensemble data 
priori postulate informed function class motivated physical theory phenomenon parametric approach method choice aspects phenomenon unknown hard describe explicitly nonparametric modelling versatile powerful 
important stress aim solely obtain accurate predictions valid estimates uncertainty explain inner workings true generative process 
case nonparametric modelling applicable 
non parametric statistics regularities relationship postulated requiring ensemble concentrated easily describable class 
example may assume ensemble stationary isotropic see section allows infer properties generative ensemble observations come single realisation thereof 
postulate smoothness nearby points space time similar values high probability periodicity boundary conditions contrast parametric case clear represent generative ensemble explicitly 
random eld mapping input space real valued random variables natural generalisation joint distribution nite index set 
joint distribution try describe eld low order cumulants mean covariance function bivariate form satisfying positive semide niteness property akin covariance matrix joint distribution 
cumulants second order vanish random eld gaussian gaussian process 
importantly properties stationarity isotropy smoothness periodicity enforced choice covariance function 
furthermore nite dimensional marginal distributions eld jointly gaussian inference prediction require little numerical linear algebra 
brief hope motivated reader browse detailed sections follow 
section de nes gaussian processes introduces important subclasses stationary isotropic gps develops di erent views gps extension complex valued random elds straightforward 
machine learning applications require real valued elds concentrate case simplicity 
prominent machine learning 
elementary gp models introduced section 
approximate inference techniques models discussed section generic framework 
theoretical aspects gps understood associating reproducing kernel hilbert spaces rkhs shown section 
traditionally gp models context penalised maximum likelihood spline smoothing motivated section 
variant spline smoothing support vector machine gained large popularity machine learning community relationship bayesian gp techniques section 
gp models extensively spatial statistics estimation procedure called kriging described section 
nal section deals choice covariance function central importance gp modelling 
describe classes standard kernels properties show kernels constructed elementary parts discuss methods learning aspects kernel nally illustrate classes covariance functions discrete index sets 
readers interested practical machine learning aspects may want skip sections contain theoretical material required understand gp applications 
notational conventions familiar probability theorists introduced section careful motivate formalism applied sections 
gaussian processes process weight space view gaussian process gp models constructed classical statistical models replacing latent functions parametric form linear functions truncated fourier wavelet expansions multi layer perceptrons random processes gaussian prior 
section introduce gps highlight aspects relevant machine learning 
develop simple views gps pointing similarities key di erences distributions induced parametric models 
follow chap 

concepts required study gp prediction chap 

concepts vocabulary general probability theory refer 
non empty index set 
main parts arbitrary assume group assume 
nutshell random process collection random variables common probability space 
measure theoretic de nition awkward basically single variable 
viewed function probability space reals 
functions obtained xed atomic event called sample paths random process seen corresponding distribution sample paths 
nite obtain random variable jxj evaluating process points distribution called nite dimensional distribution 
assume random process exists consider system clear symmetric consistent permutation components result distribution equally permuted random vector marginal distributions intersection starting ones identical 
formally borel sets addition origin negation permutation ng 



importantly kolmogorov proved symmetry consistency sucient conditions speci cation guarantee existence random process concrete measure theoretic sense 
question uniqueness random processes tricky processes equivalent surely equivalent processes called versions di er signi cantly sure properties sample paths 
example construct version smooth process sample paths di erentiable nite number points surely 
context machine learning applications interested sample path properties di erentiability lesser importance focus properties introduced shortly characterised directly invariant change version 
words general identify process equivalence class versions particularly nice member class simple nature applications interested guarantees practice 
see global sample path properties process sense smoothness average variability directly related corresponding properties 
see adler methods studying sample path properties 
fx sequence real valued random variables recall quadratic mean mean square jx xj 
convergence weaker sure convergence turns useful mode discussing gp aspects require 
general equivalent jx 
nutshell property traditionally de ned terms limits continuity di erentiability typically de ne corresponding property scalar random variables substituting normal convergence 
suppose random process 
rst second order statistics mean function covariance function obviously depend process 
covariance function central studying characteristics process mean square sense 
positive semide nite function sense clear jxj 
positive means nite set symmetric matrix jxj jxj obtained evaluating positive semide nite 
note implies called positive de nite holds 
example wiener process see section version continuous sample paths 
term uniquely literature replaced non negative de nite positive de nite di erent meaning 
positive semide niteness leads important spectral decomposition discussed section 
positive semide nite referred kernel pointing role kernel linear integral operator see section 
stationary processes situations behaviour process depend location observer restriction rich theory developed linking local properties process behaviour close origin 
process called strictly homogeneous strictly stationary invariant simultaneous translation variables 
implies constant function write case 
process ful lling conditions called weakly homogeneous weakly stationary 
stationary process choice origin re ected statistics second order 
called correlation function 
stationary process spectral representation stochastic fourier integral chap 
chap 
theorem asserts positive semide nite furthermore uniformly continuous characteristic function variable df 
probability distribution function 

density 
lebesgue measure called spectral density 
theorem allows prove positive semide niteness computing fourier transform checking non negative 
proportional spectral density 
note function spectral distribution symmetric 
exists 
process determine mean square properties true general sure properties continuity di erentiability sample paths 
stronger zero mean process properties usually determined entirely covariance function 
stationary processes merely behaviour origin counts derivative exists exists 
smoothness process sense grows degree di erentiability 
example process rbf gaussian covariance function analytic analytic di erentiable order 
isotropic processes stationary process called isotropic covariance function depends kxk 
case spectral distribution invariant isotropic isomorphisms rotations 
loosely speaking second order characteristics isotropic process dx denotes di erential functional 
position direction observed 
simpler characterise isotropic correlation functions stationary ones general 
kxk 
spectral decomposition simpli es df 

fk df 
distribution function bessel function rst kind see sect 

recall dimensionality input space right hand side hankel transform order see sect 

alternatively spectral density 
exists 

df 

easily convert spectral representation terms 
denote set corresponding isotropic correlation functions note characterises theorem 
clear isotropic correlation function restricted dimensional subspace beware 

depend dimension induce correlation function see 
show exp df 
result due schoenberg note assumption isotropy puts strong constraints correlation function especially large example inf large negative correlations ruled 
non negative 
furthermore large smooth may jump additive white noise 
nonsingular correlation function called anisotropic 
examples isotropic covariance functions section 
views gaussian processes gaussian process gp process gaussian 
gaussian determined rst second order cumulants involve pairwise interactions completely determined mean covariance function 
means gps strong weak stationarity concept 
gps far accessible understood processes uncountable index sets 
clear positive semide nite function exists zero mean gp covariance function kolmogorov theorem gps modelling tool exible 
importantly choosing properly encode properties function distribution implicitly desired section 
ag surface area unit sphere conjunction latent variable modelling techniques wide variety non parametric models constructed see section 
fact gaussian covariance matrices induced obtain approximations bayesian inference fairly straightforwardly see section approximations turn accurate parametric models equal exibility multilayer perceptrons 
interesting note derivatives gp gps exist derivative observations incorporated model way function value observations applications see 
characteristics di erentiability order controlled covariance function see section example section 
thoroughly studied gps wiener process brownian motion continuous random walk covariance function multivariate generalisations brownian sheets see chap 

characterised ju having orthogonal increments note stationary stationary version orthogonal increments ornstein uhlenbeck process see section 
wiener process example di usion process 
large number applications mathematics physics mathematical nance 
property orthogonal increments allows de ne stochastic integrals chap 
wiener process random measure 
continuous di erentiable point 
fact version wiener process constructed continuous sample paths version sample paths di erentiable probability 
wiener process explicitly construct gps means stochastic integrals procedure sketched section 
develop elementary views gaussian processes process weight space view 
usually simpler allows relate gp models parametric linear models directly 
follow 
process view zero mean gp covariance function spirit gp de nition 
de ned implicitly nite subset induces vector process values points fx kolmogorov theorem guarantees existence gp family 
practice modelling problems involving unknown functional relationship formulated nite number linear characteristics evaluations derivatives linked observations predictive queries cases process view boils dealing projection gp multivariate gaussian distribution simple linear algebra quadratic forms 
orthogonality implies independence process gaussian 
term process view function space view employed 
relationship gps associated spaces smooth functions bit subtle introduced section 
continuous version exists continuous sample paths require 
practice knowledge numerical mathematics required avoid numerically instable proce gps seen weight space viewpoint relating linear model 
bayesian context view rst suggested hagan localised regression model weight space nite dimensional generalisation arbitrary gp priors developed uses process view 
rst address gp regression rigorous bayesian context equivalence spline smoothing bayesian estimation processes noticed earlier kimeldorf wahba see section 
recall linear model feature map covariate independent gaussian noise 
gp covariance function satis es weak constraints written albeit possibly nite dimensional weight space 
develop view facts discussed detail section 
mild conditions covariance function construct sequence converges quadratic mean 
variables 
orthonormal eigenfunctions operator induced corresponding eigenvalues 
sense precise section 
quadratic mean 
weight space view gps allows view non parametric regression model direct nite dimensional generalisation linear model spherical gaussian prior say maps feature space typically countably nite dimensional 
important note construction feature map individual components scaling sense norm hilbert space drawn operates 
comparable di erent rkhs norm scales roughness function 
intuitively graph increasingly complicated see section details 
inference purposes concerned derivatives linear functionals process weight space view equivalent lead identical results 
feel process view simpler avoiding spurious relying familiar gaussian manipulations 
hand weight space view frequently machine learning literature peculiarities may reason perception gp models dicult interpret 
danger false intuitions 
matrices dealt positive semide nite hard 
reliable techniques mentioned section 
need pointwise convergence stronger statements possible mild assumptions sect 

cancel magically weight space viewpoint occur process view rst place 
developed interpolating geometrical arguments low dimensional euclidean space feature space 
note weight space representation gp terms feature map course unique 
route eigenfunctions covariance operator way establish 
invariant 
gaussian processes limit priors parametric models conclude section mentioning prime reasons focusing current machine learning interest gp models highly original di erent way establishing weight space view proposed 
consider model multi layer perceptron mlp hidden layer functions weights output layer weights suppose independent identical priors resulting bounded surely compact region interest 
independently 
converges quadratic mean zero mean gp covariance function eu 
stronger conditions assure sure convergence uniformly compact region 
bottom line take conventional parametric model linearly combines outputs large number feature detectors scale outputs isolation negligible contribution response just corresponding gaussian process model 
neal shows non zero number non gaussian feature outputs signi cant impact response non zero probability limit process typically gaussian 
conclude weight space view relate non parametric gp models parametric linear models fairly directly 
important di erences general 
neal showed gps obtained limit distributions large linear combinations features feature contribution negligible output distributions architectures strong feature detectors typically gaussian 
predictions gp model smoothed versions data sense concrete section interpolate minimising general smoothness constraints encoded gp prior opposed parametric models predict focusing functions family consistent data 
hagan discusses di erences optimal design 
gives example 
universal covariance function kernels discussed property nite disjoint subsets separated hyperplane feature space distances points plane lie interval arbitrarily small size 
concludes nite dimensional interpretation geometric situation feature space universal kernel fail 
strongly agree 
example section discuss role reproducing kernel sense 

hilbert space inner product 
de ne map 

hilbert space weight space 
gaussian process models simplest gaussian process model useful regression estimation priori zero mean gaussian process covariance function independent noise 
inference model simple analytically tractable observation process zero mean gaussian covariance data ng test point distinct training points jx jx du jk see model posterior predictive process gaussian mean function covariance function note mean function prediction linear targets xed furthermore posterior covariance function depend targets 
practice posterior mean predictions required prediction vector computed linear conjugate gradients solver runs eigenvalue spectrum shows fast decay 
predictive variances test points required cholesky decomposition ll computed variance computation requires single back substitution 
pointwise predictive variance larger corresponding prior variance shrinkage decreases increasing noise level result derived weight space view applying standard derivation bayesian linear regression 
note just parametric linear regression smoothed prediction linear function observations mean function predictive process js see section 
note kx gets big predictive mean variance points far data tend prior mean prior variance 
second level inference problems selecting values hyperparameters parameters integrating analytically context model interesting note stationary continuous sum continuous stationary covariance white noise covariance furthermore sch conjectured isotropic bounded covariance function continuous possibly 
symmetric matrix positive de nite unique cholesky decomposition ll lower triangular positive diagonal elements 
tractable approximations applied 
approximate model selection discussed section 
generalise model allowing arbitrary noise distribution retaining gp prior 
generative view sample process 
prior ju independent 
likelihood function factors product univariate terms yjx 
ju likelihood depends 
nite set predictive posterior process written dp 
js dp 
js nite prior measure shifted multiplication depending process values training points 
predictive process gaussian general mean covariance function obtained knowledge posterior mean covariance matrix discussed section 
test point jx ju expectation predictive distribution 
general model rst level inference analytically tractable 
section general approximate inference framework discussed 
markov chain monte carlo mcmc methods applied fairly straightforwardly example gibbs sampling latent variables 
methods attractive marginalisation hyperparameters dealt framework 
naive realisations may prohibitive running time due large number correlated latent variables advanced techniques dicult handle practice 
mcmc advanced widely class approximate inference techniques discussed detail see review 
generalised linear models 
binary classi cation large class models kind obtained starting generalised linear models replacing parametric linear function process gp prior 
seen direct nite dimensional generalisation employing weight space view see section 
spline smoothing context framework detail 
employs noise distributions exp exponential family natural parameter sucient statistics log partition function 
scale hyperparameter 
linear model generalised easily allow bounded linear functionals latent process 
evaluation functional discussed section 
special case technically attractive feature framework log strictly concave leading strictly log concave unimodal posterior 
binary classi cation glm binomial noise distribution logistic regression logit noise log cosh 
frequently binary classi cation noise model probit noise fy seen noisy heaviside step exponential family 
noise models strictly log concave 
models latent processes allow xed number latent variables case processes 
likelihood factors ju zero mean gaussian priori covariance function theoretically possible cross covariance functions prior covariances di erent may hard come suitable class functions 
furthermore assumption processes independent priori leads large computational savings joint covariance matrix data assumes block diagonal structure 
note structure separate di erent block diagonal structures coming factorised likelihood separate cases important example latent processes class classi cation 
likelihood comes multinomial glm multiple logistic regression 
convenient binary encoding class labels class cg 
noise multinomial softmax exp 
exp 
called softmax mapping 
note mapping invertible add changing 
words parameterisation multinomial overcomplete due linear constraint corresponding glm log partition function log exp strictly convex 
usual remedy constrain example xing uc 
ne context tting parameters maximum likelihood may problematic hyperparameters may shared prior processes making marginally dependent 
vector notation associated single case 
confused vector notation group variables cases 
bayesian inference 
mentioned typically priors uc induced prior exchangeable distribution component permutations di erent distributions singled technical reasons 
think preferable bayesian context retain symmetry accept 

dealing non identi ability inference approximations hard softmax invertible plane orthogonal strictly convex 
anyway detail di erent blocking structures mentioned renders implementations approximate inference class model somewhat involved binary case see example 
examples process models ordinal regression ranking models see likelihood suggestions multivariate regression 
robust regression gp regression gaussian noise lead poor results data prone outliers due light tails noise distribution 
robust gp regression model obtained heavy tailed noise distribution laplace student distribution 
interesting idea fact obtained starting integrate precision gamma distribution 
robust model written drawn gamma distribution parameters hyperparameters 
posterior conditioned precision values gaussian computed way case 
sampled mcmc may chosen maximise posterior js 
marginal likelihood yj gaussian computed easily 
note case number hyperparameters grows invalidate usual justi cation marginal likelihood maximisation see section 
approximate inference learning seen previous section posterior process likelihood general form written shifted version prior 
processes context dealt feasibly gaussian ones general way obtaining gp approximation posterior process approximate gaussian leading process dq 
dp 
gaussian recall section concise way writing nite 
optimal way choosing minimise relative entropy de nition 
js 
conditioning 
omitted notational simplicity 
equality intuitively clear 

js 
conditional formally follows fact dp 
js dq 
dp 
js dq 
recall notation section 
minimum point unique 
js mean covariance function 
equivalent moment matching requires nd mean covariance matrix 
unfortunately intractable general large datasets non gaussian noise 
gaussian approximation leads gp posterior approximation 
intractable valuable guideline 
primarily interested approximate inference methods gp models employ gp approximations posterior processes depend data covariance function kernel matrix hyperparameters 
class contains variety methods proposed literature 
virtually reduced parameterisation restricted form 
di 
diagonal positive entries ng jij methods mentioned section 
sparse gp approximations case ni simplicity replacing :10.1.1.25.1089:10.1.1.12.8842
approximate predictive posterior distribution test point determined easily jx generally gp posterior approximation mean function covariance function predictive distribution jx obtained averaging ju 
expectation analytically tractable done gaussian quadrature sect 
ju smooth grow faster polynomial 
simple numerically stable way determine predictive variances compute cholesky decomposition ll variance requires back substitution important stress inference approximation gp models boils simple linear algebra crucial practice choose representations procedures numerically stable 
presence positive de nite matrices techniques cholesky factorisation known stable 
furthermore representation conditioned eigenvalues 
refer prediction vector 
generally mentioned section derivative information bounded linear functionals latent process likelihood variables predicted fact corresponding nite set scalar variables multivariate gaussian prior covariance matrix derived covariance function discussed detail section 
generalisation multi process models section straightforward principle 
dimension restricted form merely block diagonal blocks diagonal 
processes priori independent consist blocks diagonal 
general formulae prediction modi ed eciency 
details involved may depend concrete approximation method process models discussed detail 
examples simple ecient way obtaining gaussian approximation laplace method called saddle point approximation proposed binary classi cation logit noise 
nd posterior mode done variant newton raphson fisher scoring see 
iteration consists weighted regression problem requires solution positive de nite linear system 
done approximately conjugate gradients solver 
mode diag diag logistic function diag diagonal elements positive 
recall laplace approximation replaces log posterior quadratic tted local curvature mode logit noise log posterior strictly concave dominated gaussian prior far general gaussian approximation fairly accurate 
hand true posterior signi cantly skewed meaning mode quite distant mean optimal covariance approximation local curvature mode poor 
expectation propagation ep algorithm gp models signi cantly outperform laplace gp approximation terms prediction accuracy costly 
somewhat harder ensure numerical stability 
hand ep general example deal discontinuous non di erentiable log likelihoods 
fact special case ep gaussian elds earlier opper winther name ep seen iterative generalization older bayesian online learning techniques 
matrix inversion recommended gp machine learning literature 
known numerical mathematics inversion avoided possible reasons stability context gp framework cholesky decomposition ecient 
partly due complex iterative structure elementary steps smaller laplace technique eciently 
range di erent variational approximations suggested :10.1.1.28.8322:10.1.1.28.8322
note variational method chosen minimise 
easy see best gaussian variational distribution covariance matrix form sect :10.1.1.19.8785

sparse approximations gp inference developed 
original application online learning understood easier cations ep 
approximations mentioned far training time scaling sparse inference approximations reduce scaling adjustable problems sparse approximations attain sucient accuracy essentially linear time allows application data rich settings 
idea concentrate subset ng jij training data call active set approximate true likelihood model likelihood approximation function components 
replacement inference linear seen formulae section allow active set 
challenge choose form way best approximate moments true posterior staying resource limitations time memory 
ju gaussian sparse technique embedded inference approximation kind discussed section 
details sparse schemes generic schemes ep algorithm information theoretic selection heuristics described :10.1.1.25.1089:10.1.1.25.1089:10.1.1.12.8842:10.1.1.12.8842:10.1.1.135.8784:10.1.1.135.8784:10.1.1.19.8785
free matlab software released csat model selection far concerned rst level inference conditioned xed hyperparameters 
useful general method provide means select values parameters marginalise see section 
correct way proceed strict bayesian sense approximated mcmc techniques model selection computationally attractive 
frequently general empirical bayesian method nuisance hyperparameters marginal likelihood maximisation maximum likelihood ii called evidence maximisation 
technique applied generic gp approximation described section leading powerful generic way adjusting hyperparameters nonlinear optimization scales linearly number parameters 
important point automatic model selection techniques strong advantage bayesian gp methods kernel machines svms see section know selection strategies similar power generality 
denote hyperparameters marginal likelihood sj yj latent primary parameters integrated 
suciently large small xed dimension js frequently highly concentrated mode js marginalise replace posterior simply plug example maximum pos choosing completely random possible performs poorly situations classi cation uence patterns posterior di erent 
see www tuebingen mpg de bs people ogp index html 
map approximation 
finding basically amounts maximising marginal likelihood hyperprior simple form 
conditions suciently peaked hard come general usually realistic models 
marginal likelihood maximisation solve model selection problem general shown empirical studies featuring di erent models description plug approximation bayesian marginalisation may lead successful extensions cases simple method fails 
readers worry point propose select maximising likelihood yj maximum likelihood techniques prone tting 
key di erence marginal likelihood primary parameter 
integrated 
choosing primary parameters maximise likelihood leads ts generalise badly true general marginal likelihood maximisation 
simple argument proof value leading complicated 
needs assign mass 
functions value leading simple 
linear low order polynomial likelihood higher complicated 
process marginalisation complicated functions stronger integral simpler functions integral 
occam razor ect analysed mackay :10.1.1.31.4284
obviously possible create situations marginal likelihood maximisation leads tting 
general rule thumb dimensionality hyperparameters scale occam razor argument just intuitively apply situation know de nite test separating non cases general 
focus marginal likelihood maximisation general model selection technique 
log marginal likelihood log yj dicult compute posterior approximated general 
easy see variational lower bound log yj eq log ju log uj eq log ju uj holds distribution recall relative di erential entropy section 
slack bound relative entropy 
note posterior approximation depends feasible general obtain exact gradient variational em important special case lower bound maximisation algorithm iterative turn freezing maximising lower bound chosen family variational distributions 
alternatively multimodality arise non identi ability model symmetries exist di erent datasets interest 
case just pick dominant modes arrive predictions chosen peak train featuring equivalent modes 
integrate variable dimension training sample independent conditional process 
general nite dimensional variable central limit theorem directly assert gaussianity gets large 
example maliciously set 
special situations technique may applicable see section 
analytically tractable gaussian likelihood example case gp regression gaussian noise discussed log 
chosen di erent way approximation posterior example ep algorithm sparse approximations 
deviation variational choice maximises lower bound family candidates criticised ground choices lead decreases lower bound algorithm increase criterion strictly monotonically 
hand chosen di erent way may lie outside families lower bound maximised eciently may result larger value variational family 
furthermore lower bound criterion motivated fact gradient log uj ignoring dependence approximates true gradient log yj log uj point close mentioning interesting point lower bound maximisation gp models deviate usual practice parametric architectures 
customary maximise lower bound keeping completely xed gradient ignored 
sense long independent prior distribution model context approximate gp inference methods dependence gp prior quite explicit example covariance depends strongly kernel matrix merely diagonal matrix 
argue keeping xed maximisation merely ignore dependence essential parameters typically leads involved gradient computation potentially closer true gradient 
alternatively computation resource limits indirect dependencies may ignored 
optimisation problem slightly non standard due lack strict monotonicity modi ed take account 
details sect :10.1.1.135.8784:10.1.1.135.8784:10.1.1.19.8785

reproducing kernel hilbert spaces theory reproducing kernel hilbert spaces rkhs characterise space random variables obtained bounded linear functionals gp method prediction nite information 
apart rkhs provide uni cation ideas wide area mathematics mentioned 
interested reader may consult 
exposition taken 
section skipped readers interested primarily practical applications 
reproducing kernel hilbert space rkhs hilbert space functions evaluation functionals bounded 
implies exists kernel example bound gaussians covariance matrix form nding prohibitively costly practice proposed variational schemes restricted subfamilies :10.1.1.28.8322:10.1.1.28.8322
simple analytic formula dependence better ignoring 



inner product speci hilbert space vector space inner product complete sense cauchy sequence converges element space 
example hilbert space generated inner product space functions adjoining limits cauchy sequences note operation adjoined objects need functions usual sense 
example obtained completing vector space functions shown contain functions de ned pointwise 
rkhs anomalies occur functionals bounded jf kfk riesz representation theorem exists unique representer holds 
easy see kernel positive semide nite 
called reproducing kernel rk note 

important note rkhs norm convergence implies pointwise convergence pointwise de ned function jf kf fk hand positive semide nite exists unique rkhs rk set nite linear combinations 


inner product space extended hilbert space adjoining limits cauchy sequences 
norm convergence implies pointwise convergence inner product space adjoined limits pointwise de ned functions rkhs rk conclude rkhs properties nicer general hilbert space 
functions pointwise de ned representer evaluation functional explicitly 

existence functions means expressions interpreted care 
element de ned set equivalent cauchy sequences de ne cauchy sequences equivalent sequence obtained interleaving cauchy 
expression understood limit limn fn gn fn gn existence limit established independently 
sequel convention 
bounded functionals called continuous 
rkhs mercer eigendecomposition 
karhunen loeve expansion mentioned rkhs general kernels contains unique rkhs subspace 
recall contains functions holds 
standard inner product taken indicator function compact set unit hypercube 
positive semide nite regarded kernel representer positive semide nite linear operator sense kf 
eigenfunction eigenvalue 
eigenvalues real non negative 
furthermore suppose continuous mercer hilbert schmidt theorems exists countable orthonormal sequence continuous eigenfunctions eigenvalues 
expanded terms 
seen generalisation eigendecomposition positive semide nite hermitian matrix 
reproducing property positive semide nite kernels recognised moore develop notion general positive hermitian matrices 
case characterise rkhs embedded explicitly 
de ne fourier coecients consider subspace hk 
hk hilbert space inner product fourier series converges pointwise fourier coecients 
equation 
particular de ned pointwise 
rk hk important distinguish clearly inner products 

hk see details relationship inner products 

measures expected squared distance 
measure roughness function 
example eigenfunctions increasingly rough 
spectral decomposition leads important representation zero mean gp covariance function karhunen loeve expansion 
sequence independent variables converges quadratic mean stronger statement additional conditions 
de ned quadratic mean 
expansion section introduce weight space view 
note variances decay gp approximated nite partial sums expansion see 
duality rkhs gaussian process zero mean gp covariance function exact relationship rkhs rk 
think seen distribution hk wrong pointed sect 

fact version sample functions process hk probability 
seen noting partial sums ku roughly speaking hk contains smooth non erratic functions characteristics expect sample paths random process 
better intuition hk turn contain expected values conditioned nite amount information posterior mean functions interested 
duality hk hilbert space noticed important context theoretical analyses 
construct hilbert space hgp way starting positive semide nite replace 
inner product gp ab gp sense high frequency components usual fourier transform 
hgp space random variables functions isometrically isomorphic hk mapping 

gp 

purposes regard hgp rkhs rk space hgp important context inference gp models interested contains exactly random variables condition predict situations nite amount information available observations linear functionals process 
bounded linear functional hk representer hk lk isometry maps random variable hgp formally denote lu 
note lu 
lk generally functionals 
hk 


clear lu 
general di erent process obtained applying sample paths 
fact surely hk apply general 
correct interpretation quadratic mean isometry hgp hk example suppose di erential functional evaluated retrieve observations section derivatives gp 
penalised likelihood 
spline smoothing gp models interested origin spline smoothing techniques penalised likelihood estimation low dimensional input spaces spline kernels widely due favourable approximation properties splines computational advantages 
comprehensive account spline smoothing relations bayesian estimation gp models exposition mainly 
spline smoothing special case penalised likelihood methods giving view reproducing kernel green function regularisation operator introduced 
section skipped readers interested primarily practical applications 
section discussed duality gaussian process rkhs covariance function 
apart bayesian viewpoint gp models di erent direct approach estimation non parametric models penalised likelihood approach oldest widely incarnations spline smoothing methods 
introduce basic ideas dimensional model leads general notion regularisation operators penalty functionals connections rkhs 
omit details important computational issues multidimensional generalisations see details 
elementary account 
sketch ideas rigorous details see 
interpolation smoothing splines originates sch :10.1.1.19.8785
natural spline order de ned knots 

denotes set polynomials order 
natural cubic splines obtained 
de ne roughness penalty jm dx jm penalises large derivatives order large value example large functions large curvature 
xed function values interpolant minimising jm de ned spline order precisely wm called sobolev space absolutely continuous 
consider related smoothing problem minimising penalised empirical risk jm wm clear natural spline order wm replaced spline values knots change risk term increase jm 
taylor theorem gm dt gm 
ui fu 
gm 
gm green function boundary value problem functions form hilbert space inner product dt rkhs rk gm dt interesting note zero mean gp covariance function obtained fold integrated wiener process introduced section 
wiener process covariance function 
possible de ne stochastic integral process independent increments 
process de ned stochastic integral gm dw see sect 
easy derivation 
important note stochastic integral random variable arising integrating sample paths process integrals exist cases stochastic integral constructed 
zero mean gp covariance function chosen sample paths continuous wm dgm dx gm fx tg equivalent 
note written dw dx xm 
boundary values satis ed direct sum space trivially rkhs inner product choice choose orthonormal basis de ne kernel sum outer products basis functions 
kernel direct sum sum nite dimensional kernel 
note 
full space kpk sketch general case see details 
duality rkhs regularisation erential operator 
hilbert space pf 
consider operator null space example restrict orthogonal complement 
operator positive de nite inverse green function kernel rk inner product pf pg penalty functional simply squared rkhs norm 
exists 

pf rk 

hand start rkhs rk derive corresponding regularisation operator give additional insight meaning covariance function see 
fact stationary continuous theorem 

spectral density take 
spectrum dimensional example readily generalised splines unit sphere thin plate splines details get quite involved see chap 

kimeldorf wahba generalised setup general variational problem rkhs allowing general bounded linear functionals 
determined coecients dimension null space di erential operator associated spline case 
computed direct formulae sect 

general penalised likelihood approach function values linear functionals latent variables likelihood see section obtain example non parametric extensions 
penalised likelihood obtained adding penalty functional likelihood just determined coecients representer theorem proved argument spline case 
general iterative methods required nd values coecients 
adjoint 
pg 
construction green functions di erent involving gm 
going details may help consider analogue nite dimensional case vectors matrices functions operators gg uniquely de ned spectrum 

bayesian view spline smoothing close section reviewing equivalence spline smoothing bayesian estimation gp model pointed kimeldorf wahba 
positive semide nite kernel corresponding erential operator dimensional null space construct rkhs follows 
null space represented orthonormal basis rkhs direct sum 
consider model zero mean gp covariance function independent 
furthermore ai priori 
hand regularised risk functional kp fk orthogonal projection kimeldorf wahba show lies span fp mg fk 
ng give numerical procedure computing coecients 
de ne show lim xed proof see chap 
straightforward application duality rkhs hilbert space described section 
procedure dealing improper prior awkward necessary rkhs induced rich 
note parametric extension non parametric gp model sensible rich principle leading semiparametric models partial splines 
details models refer chap 
chap 

maximum entropy discrimination 
large margin classi ers regard gps building blocks statistical models way parametric family distributions see section examples 
statistical methods estimate unknown parameters models follow di erent paradigms machine learning popular 

probabilistic bayesian paradigm introduced section 
noted section intractable posterior process typically approximated gp 
case spline kernels constrained boundary conditions 

large margin discriminative paradigm posterior process obtained associating margin constraints observed data searching process ful ls soft constraints time close prior gp sense concrete section 
constraints linear latent outputs posterior process gp covariance prior 
relationship bayesian methods penalised likelihood generalised spline smoothing methods discussed section 
large margin methods special cases spline smoothing models particular loss function correspond probabilistic noise model :10.1.1.35.5318
attempts express large margin discrimination methods approximations bayesian inference paradigm separation suggested somewhat convincing 
connection paradigms formulated section exposition 
large margin paradigm popular empirical success support vector machine svm see background material :10.1.1.11.2062
bayesian gp setting see section likelihood ju observed data seen impose soft constraints predictive distribution sense functions signi cant probability posterior violate strongly 
large margin paradigm probabilistic view called minimum relative entropy discrimination mred constraints enforced explicitly 
introduce set latent margin variables datapoint 
gp prior 
latent function choose prior margin prior encourages large margins discussed detail 
minimum relative entropy distribution dq 
de ned subject soft margin constraints 
just case likelihood function constraints depend values random process 

known information theory sect 
solution constrained problem dq 
exp dp 

exp value lagrange multipliers obtained minimising convex function log called dual criterion constraints 
right hand side 
holds prior see way 
furthermore immediate 
gaussian process covariance kernel 
mean notational simplicity bias term 
modi cations straightforward 
original svm formulation seen uniform improper prior 
function diag due factorised form 

eu ky form depends choice prior margin variables 
jaakkola give examples priors encourage large margins 
example drop quickly order penalise small especially negative margins empirical errors 
order soft constraint margin violations mimic svm situation 
complete dual criterion log log ky potential term log identical svm dual objective see 
called hard margin svm margin constraints enforced allowing violations obtained 
converges training data separable prone complicated solutions 
ect potential term solution limited see 
keeps saturating exactly happens svm misclassi ed patterns 
dual criterion optimised ecient algorithms smo nonlinear potential term introduces minor complications 
just svm sparsity encouraged observed practice 
conclude mred gives complete probabilistic interpretation svm close approximation thereof 
note svm classi cation seen map approximation bayesian inference probabilistic model loss function correspond proper negative log likelihood :10.1.1.35.5318:10.1.1.19.8785
interestingly mred view points limitations framework opposed bayesian treatment gaussian process model proper likelihood 
recall margin constraints linear latent outputs leading fact mred posterior process 
covariance kernel prior 
constraints enforce predictive mean move priori predictive variances simply prior ones independent data 
suggests predictive variances error bars estimated simply performing discrimination svms large margin discriminative methods may appropriate probabilistic gp models 
details argument see sect :10.1.1.135.8784:10.1.1.135.8784:10.1.1.19.8785

important lack practical methods model selection svm 
bayesian gp methods general model selection strategy detailed section 
alternatively svm setup choice margin width arbitrary distance re scaled terms prior variance 
potential term acts logarithmic barrier enforce constraints 
smo fact svm criterion quadratic linear constraints 
hyperparameters marginalised approximately mcmc techniques 
contrast model selection svm typically done variants cross validation severely limits number free parameters adapted 
claimed learning theoretical foundations count distinctive advantage svm similar superior guarantees approximate bayesian gp techniques :10.1.1.19.8785
kriging important early application gaussian random eld models termed kriging south african mining engineer developed methods predicting spatial ore grade distributions sampled ore grades 
optimal spatial linear prediction roots earlier wiener kolmogorov closeness space may replaced closeness time mainly concerned time series 
fundamental ideas developed elds kriging meteorology name objective analysis see chap 

go details refer chap 
follow 
basic model semiparametric smoothing known feature map zero mean random eld covariance function nutshell kriging minimum mean squared error prediction method linear functionals observations spatial locations example measures ore grade interested predicting dx area focus error properties general kriging methods typically depend second order properties process assumed gaussian eld 
furthermore restrict linear predictors optimal predictor error sense conditional expectation linear gaussian known 
unknown simple procedure plug generalised squares estimate 
procedure motivated angles 
restrict attention linear predictors unbiased sense suggested approach minimises error unbiased predictors 
called best linear unbiased predictor 
bayesian motivation constructed way mentioned section 
gaussian prior covariance matrix scales priori gaussian 
posterior mean converges prior uninformative 
equations known long rediscovered areas statistics 
practice kriging methods concerned inducing appropriate covariance function stationarity assumption observed data 
empirical frequently method estimating covariance function close origin 
theoretical side stein advocates usefulness xed domain asymptotics growing number observations located xed compact region understand relationship covariance model behaviour kriging predictors 
theorem stationary covariance function characterised spectral distribution 
stein points xed domain asymptotics depend strongly spectral masses large high frequency components low frequency ones mean function smooth polynomials 

spectral density fourier transform 
general lighter tails 
smoother sense 
stein advocates expected smoothness central parameter gp prior smooth analytic covariance functions rbf gaussian kernel see section 
important concept highlighted stein see chap 
equivalence orthogonality gps 
essentially gps covariance functions di erent form equivalent case possible unambiguously decide nite amount observations xed region 
basis argue parametric family covariance functions inducing equivalent gps parameters just xed priori consistent estimation possible 
hand parameters di erent values lead orthogonal gps learned data xed priori 
note kriging models generally concerned intrinsic random functions irf generalisations stationary processes frequently spline smoothing context 
nutshell irf non stationary random eld spectral density integral diverges neighborhood origin nite pointwise variance 
generalised divided di erence sense polynomials total degree variance nite serves de ne covariance function conditionally positive semide nite stein restricts analysis interpolation situations predictions required locations principle supported observations contrast extrapolation studied time series context 
confused distinction interpolation smoothing section 
non trivial kriging techniques smoothing methods 
probability measures equivalent null sets mutually absolutely continuous see section 
orthogonal null set mass 
gaussian measures orthogonal equivalent 
practice uses semi parametric models latent process interest sum irf polynomial total degree coecients parametric latent variables 
fact add generality high frequency behaviour process 
integrable complement neighborhood irf written uncorrelated sum stationary non stationary part 
outside neighborhood smooth 
discussed detail see 
choice kernel 
kernel design tendency machine learning community treat kernel methods black box techniques sense covariance functions chosen small set candidates 
family kernels typically comes small number free parameters model selection techniques crossvalidation applied 
approaches surprisingly problems interest machine learning experience invariably shown gained choosing designing covariance functions carefully depending known characteristics problem example see sect 

establishing clear link kernel functions consequences predictions non trivial theoretical results typically asymptotic arguments 
opposed parametric models process prior ects predictions non parametric model xed domain asymptotic situations see section 
sole aim section introduce range frequently kernel functions characteristics give methods constructing covariance functions simpler elements show techniques obtain insight behaviour corresponding gp 
gives extensive material accessible review 
nal part discuss kernel methods discrete spaces noted positive de niteness arbitrary symmetric form function hard establish general 
example sensible approach constructing distance patterns depending prior knowledge proposing covariance function general need positive semide nite simple general criterion prove covariance function 
represented euclidean space kernel see 
note form kernel 
kernels property called nitely divisible 
sch managed characterise nitely divisible kernels property unfortunately just hard handle positive semide niteness :10.1.1.19.8785
fact maps basis mentioned obtained posterior expectation uninformative prior parametric coecients 
stationary try compute spectral density analytically tractable general 
true general see 
conditionally positive semide nite degree see section 
standard kernels provide list frequently standard kernels 
variance scaling parameter practice set parameter uses scales variance process comes uncertainty bias parameter added process 
applications kernel matrix directly linear systems advised add jitter term kernel improve condition number amounts small amount additive white noise chosen quite small confused measurement noise modelled separately see section 
modi cations omitted sequel simplicity 
gaussian rbf covariance function exp kx isotropic 
inverse squared length scale parameter sense determines scale expected change signi cantly 
analytic analytic 
stein points 
quadratic mean similar formula holds predicted perfectly knowing derivatives depend neighborhood 
wide spread gaussian covariance function strong smoothness assumptions unrealistic physical processes particular predictive variances unreasonably small data 
spectral density 
exp light tails 
hand smola recommend gaussian covariance function high dimensional kernel classi cation methods high degree smoothness 
interesting note context gps time series prediction girard report problems unreasonably small predictive variances gaussian covariance function consider kernels comparison 
shows smoothed plots sample paths 
note ect length scale high degree smoothness 
consider anisotropic version called squared exponential covariance function exp positive de nite 
typically diagonal matrix inverse squared length scale parameter dimension 
full matrices considered factor analysis type matrices useful intermediate 
important application additional compared gaussian kernel automatic relevance determination ard discussed 
note covariance function diagonal seen product dimensional reasons numerical stability large 
context kriging see section adding proposed math model called ect see sect 
authors criticised practice 
gaussian rbf smoothed sample paths gp gaussian covariance function 
variance 
dash dotted 
solid dashed gaussian kernels di erent length scales corresponding rkhs tensor product space built rkhs dimensional functions see section 
mat ern class covariance functions called modi ed bessel covariance functions kx modi ed bessel function sect 

show isotropic important feature class smoothness regulated directly 
example times di erentiable spectral density 
obtain process rational spectral density continuous time analogue ar time series model 
de nes ornstein uhlenbeck process stationary analogue wiener process independent increments 
general polynomial order sect 

note 
converges gaussian covariance function appropriate re scaling 
mat ern class generalised anisotropic family way gaussian kernel 
show sample function plots values 
note ect roughness sample paths 
paths erratic length scale horizontal region shown 
process di erentiable twice 
nu ornstein uhlenbeck nu nu nu smoothed sample paths gp mat ern covariance function 
variance 
upper left ornstein uhlenbeck mat ern 
upper right mat ern dash dotted solid 
lower left mat ern dash dotted solid 
lower right mat ern dash dotted solid 
exponential class covariance functions positive de niteness proved mat ern class see sect 

ornstein uhlenbeck covariance function gaussian 
kernel varies smoothly processes quite di erent properties regimes 
continuous sample paths ensured di erentiable sample paths obtained case analytic 
positive de nite 
shows sample path plots 
exponential smoothed sample paths gp exponential covariance function 
variance solid 
dashed 
dash dotted gaussian 
derived spline covariance function rst principles 
kernel interest posterior mean functions gp models variational problem rkhs splines order piecewise polynomials see section associated computations number training points knots 
hand technical complications arise spline kernels rks subspaces wm functions satisfy boundary conditions see section 
operator induced spline kernel null space spanned polynomials practice necessary adjoin corresponding nite dimensional space 
spline kernels stationary supported obtain spline kernels circle imposing periodic boundary conditions wm leading stationary kernel cos representation follows spectral density 
discrete 
note sample functions periodic probability 
wahba chap 
shown construct splines sphere statements hold probability usual 
iterated laplacian quite involved 
equivalent splines sense de ned thin plate spline conditionally positive de nite functions see section see details 
kernel discrimination methods polynomial covariance functions kxk kx popular unsuitable regression problems 
denominator kernel 
normalisation done applications recommended general 
polynomial kernels normalising denominator seen induce nite dimensional feature space polynomials total degree 
interesting note exactly rkhs adjoin conditionally positive de nite kernel order thin plate spline covariance function 
hand spline case polynomial parts usually regularised 
karhunen loeve expansion see section write expansion monomials total degree gaussian random coecients 
regularisation operator see section polynomial kernels worked 
note covariance function kernel nitely divisible 
shows sample path plots 
polynomials analytic 
polynomial sample paths gp polynomial covariance function 
variance 
solid 
dashed 
euclidean inner product referred linear kernel feature space normalised polynomial kernel consists polynomials total degree divided kxk machine learning literature 
gp models kernel straightforward linear models linear regression logistic regression 
clear weight space view see section linear model regarded gp model kernel technique sense number training points 
furthermore svm linear kernel variant perceptron method maximal stability studied statistical physics 
give example function covariance function called sigmoid kernel tanh ax positive semide nite see shipped svm packages know :10.1.1.19.8785
springs desire kernel expansions look restricted layer neural networks 
correct link mlps gp models neal see section involves limit nitely large networks 
covariance function corresponding layer mlp limit williams 
practice course possible expansions kernels data covariance functions 
underlying theory minimisation rkhs see sections breaks view inference gp model 
practical side awed results negative predictive variances pop expected 
worse optimisation techniques including svm algorithms rely positive semide niteness matrices may break 
fact svm optimisation problem convex local minima general constructing kernels elementary parts construct complicated covariance functions simple restricted ones easier characterise stationary isotropic covariance functions see section 
large number families elementary covariance functions known reviewed section 
generalisation stationary kernels conditionally positive semide nite ones stationary elds frequently models see section discussed 
class positive semide nite forms formidable closure properties 
closed positive linear called conic combinations pointwise product pointwise limit 
covariance function nite 
important special case 
example kernel positive variance modi ed constant diagonal choosing normalisation discussed context polynomial kernel 
note hagan localised regression model section special case 
general way creating non stationary covariance function parametric model linear assume gp prior integrate parameters see details :10.1.1.19.8785:10.1.1.19.8785
furthermore suppose sequence models priors obtain sequence running kernel algorithm wasteful awkward due singular kernel matrix 
kernels 
priors appropriately scaled pointwise limit exists kernel 
standard kernels obtained way 
neal showed model size goes nity prior variances tend accordingly layered models non gaussian priors tend gp due central limit theorem see section 
important modi cation embedding 
covariance function arbitrary map covariance function special case 
example kh euclidean space valid kernel induced gaussian rbf kernel 
fisher kernel mutual information kernels examples :10.1.1.44.7709:10.1.1.19.8785
embedding put rigid constraints gp 
example stationary surely 
cos sin sample paths periodic functions 
embedding create non stationary kernels elementary stationary ones 
powerful mechanism starts viewing di erent way 
squared exponential kernel suppose input subject noise di erent observed locations independent noise variables independent process 
process gaussian mean covariance function determined easily form squared exponential kernel covariance matrix depends similar construction create non stationary covariance functions 
idea generalised considerably shown 
de ne note mahalanobis distance covariance matrix depends isotropic correlation function recall section shown js js valid correlation function 
proof uses characterisation df 
correlation xed variables mean variance 
see section 

df integral written dr df 
positive semi de nite special case 
equation create new families non stationary kernels isotropic ones 
note elds estimate 


principle speci ed gps see inference costly 
hand simpler parametric models may sucient 
unlabelled data abundant possible learn second eld source see :10.1.1.19.8785:10.1.1.19.8785
interesting note 
smooth properties 
deducible transferred gp correlation function 
guidelines kernel choice choosing kernel task depends intuition experience 
high dimensional tasks suitable prior knowledge available best option may explore simple combinations standard kernels listed 
invariances known may encoded methods described sect 

approximate bayesian gp inference principle combinations di erent kernels lot free hyper parameters adapted automatically 
low dimensional obtain insight 
stein points usefulness studying xed domain asymptotics see section 
respect tail behaviour spectral density see section important 
degree di erentiability degree smoothness process depends rate decay 
stein recommends kernel families mat ern class come degree smoothness parameter 
stresses importance concept equivalence orthogonality gps see section 
arguments asymptotic nature example clear mat ern class learned accurately limited amount data 
predictions equivalent processes di erent kernels di erent 
ways getting feeling behaviour process visualisation option low dimensional 
draw samples process plot follows plots section produced way 
ne grid domain interest jxj applies especially diagonal kernels positive 
case stein argues citing je reys di erences important lead consistency large data limit xed domain 
ne grids smooth kernels gaussian cholesky technique described fails due round errors 
singular value decomposition svd case concentrating leading determined reliably 

sample lv ll cholesky decomposition 
large approximated incomplete cholesky factorisation see 
process isotropic grid regularly spaced toeplitz structure cholesky decomposition computed see 
repeatedly sampling plotting give idea degree smoothness average length scales euclidean distance expected vary signi cantly special features learning kernel promising approach choosing covariance function learn data prior knowledge 
example parametric family covariance functions choose parameters order corresponding process model observed data 
model selection xed family done empirical bayesian method marginal likelihood maximisation generic approximation case gp models section 
procedure typically scales linearly number hyperparameters elaborate heavily parameterised families employed 
important special case termed automatic relevance determination ard mackay neal :10.1.1.31.4284
idea introduce hyperparameter determines scale variability related variable interesting prior mean 
example set linear model throwing host di erent features components place weights diagonal matrix positive hyperparameters 
place hyperprior diag encourages small values priori incentive small inducing variance close ectively switches ect predictions 
balanced need components model data leading automatic discrimination relevant irrelevant components 
context covariance functions implement ard anisotropic kernel see section form isotropic diagonal positive de nite 
example covariance function 
determines scale variability prior eld moves th coordinate axis 
imagine eld restricted line parallel axis length scale restriction distance expected change process signi cant 
length scale large eld constant direction regions interest 
ard discriminate relevant irrelevant dimensions input variable automatically predictions uenced signi cantly 
spatial statistics techniques see sect 
frequently 
stationary process semi var 
estimated averaged squared distances groups datapoints roughly matrix toeplitz diagonals main diagonals constant 
proper bayesian solution integrate parameters approximated mcmc techniques outcome mixture covariance functions leading expensive predictors 
distance apart tted parametric families maximum likelihood 
stein empirical single input choosing covariance function suggests range techniques including empirical bayesian approach mentioned 
classi cation models idea local invariance certain groups transformations important 
example recognition handwritten digits uenced translations small angle rotations bitmap 
process latent function classi cation problem representing log probability ratio classes see section starting applying small transformations group discrimination remain invariant lead signi cant changes process output sense 
relate notion ard varying invariant directions induce coordinate non linear general irrelevant prediction 
chapter gives number methods modifying covariance function order incorporate invariance knowledge degree reviewing direction omit 
minka pointed instances learning learn prior learning paradigm seen learning gp prior multi task data see 
fact setup standard hierarchical model frequently bayesian statistics implement realistic prior distributions 
access noisy samples assumption sampled di erent realisations latent process turn sampled process prior 
data sort valuable inferring aspects underlying covariance function 
simple multi task scenario multi layer perceptron samples penalised maximum likelihood sharing input hidden weights di erent sets hidden output weights sample 
idea hidden units discover features important general combination uppermost layer speci place gaussian priors hidden output weights gp model covariance function determined hidden units 
generally start parametric family covariance functions learn hyperparameters multi task data marginal likelihood maximisation hierarchical sampling model 
approximate implementation idea reported 
kernels discrete objects mentioned section principle input space restricted group 
example gaussian processes lattices important vision applications form gaussian markov random eld sparse structured inverse covariance matrix 
gaussian likelihoods posterior mean determined eciently conjugate gradients solver embedded trees algorithm wainwright sudderth willsky compute marginal variances 
kernel methods methods covariance matrices variables determined spatial relationship associated covariates proposed number degree rotation results 
loopy belief propagation renders correct mean converges slower numerically unstable 
problems involving discrete spaces nite countably nite 
aim section give selected examples 
kernels de ned set nite length strings nite alphabet 
string kernels proposed try review 
important applications string kernels distance measures sequences arise problems dna rna biology statistical models built nucleotide sequences 
proposed string kernels special cases convolution kernels introduced haussler 
interesting case discussed extension hidden markov random eld hmrf 
markov random eld mrf observed variables latent variables clique potentials subsets components marginalised 
replace clique potential positive de nite kernels marginalise result covariance kernel seen unnormalised joint generative distribution 
original mrf structure allows tractable computation algorithm evaluate covariance function eciently 
example hidden markov model hmm sequences extended pair hmm way emitting observed sequences sharing latent sequence string kernels arise special cases construction 
practice string kernels generally kernels obtained joint probabilities pair su er ridge problem larger priori attain signi cant correlation especially long sequences compared 
example models involving dna sequences sequences correlate strongly homologous encode proteins similar function 
standard string kernel sequences strongly correlated obtained common ancestor latent sequence operations insertions substitutions ancestor model motivated evolution genes gives example pair hmm setup 
homologous sequences di er quite substantially regions structure functional part protein depend strongly 
remote homologies really interesting ones close homologies detected simpler statistical techniques process models string kernels 
hand may possible spot homologies going string kernels pair hmrf constructions example building general framework kernels obtained nite transducers 
conceptually simple way obtain kernel embed euclidean space concatenate embedding known kernels example gaussian 
example fisher kernel maps datapoints fisher scores parametric model :10.1.1.44.7709
surge interest automatic methods parameterising low dimensional non linear manifolds local euclidean coordinates 
methods non parametric conventional parametric mixture models order obtain parametric embedding obtain kernel 
kondor la erty proposed kernels discrete objects concepts spectral graph theory di usion graphs 
nite covariance function simply positive semide nite matrix 
symmetric generator matrix corresponding exponential kernel de ned exp 
de ne elements indices matrix positive de nite 
fact eigenvectors transformed exp 
practice general exponential kernels computed feasibly large particular general ecient way computing kernel matrices points interest 
possible approximate sampling 
kernel generator matrices linked heat equation hk interesting note nitely divisible covariance function scale parameter form 
covariance matrix 
kondor la erty interested di usion kernels graphs special cases exponential kernels 
generator negative called graph laplacian 
construction seen stationary markov chain random walk continuous time vertices graph 
kernel probability time state time interpretation requires true negative graph laplacian implies doubly stochastic 
equation describes heat ow di usion initial distribution 
idea describe structure sense closeness close points highly correlated covariance function terms local neighbourhood association induce weighted unweighted undirected graph 
correlation points proportional distribution random walk started time similar ideas ectively non parametric clustering classi cation partially labelled data 
kondor la erty give examples graphs special regular structures di usion kernel determined eciently 
include certain special cases string kernels nite analogue markov chains treated carefully 
situations determined known simple recursive formulae represent representative sample including training set unlabelled data 
generator matrix underlying graph projected representative sample sensible way sparse leading eigenvectors eigenvalues approximated sparse lead approximation low rank optimal frobenius norm 
kondor la erty note graph regular grid generator matrix converges usual laplacian operator gaussian kernel mesh size approaches 
useful uncertainty estimates 
section highlighted number powerful techniques encoding prior knowledge covariance function learning appropriate kernel 
problems machine learning especially classi cation observe big di erence generalisation error range di erent common kernels signi cant di erences arise uncertainty estimates predictive variances bayesian gp techniques 
discussion section suggests additional complexity bayesian gp methods compared svm arise exactly uncertainty estimates desired 
important ask useful estimates practice 
strictly speaking frequentist con dence intervals bayesian uncertainty estimates tied assumptions violated non trivial real world situations 
conditioned null hypothesis certainly violated scale require data generated model 
bayesian setting di erent priors models compared conclude predictions enjoy certain robustness detect mismatches trigger re nement 
case gp models choice covariance function signi cant ect uncertainty estimates 
demonstrate fact simple dimensional regression task 
note gp regression gaussian noise error bars depend targets di erent non gaussian likelihoods classi cation 
data sampled noisy sine wave single point noise standard deviation 
compare rbf covariance function mat ern kernel di erent process variance cases 
recall mat ern kernel controls degree di erentiability process rbf process analytic 
shows mean predictions standard deviation error bars noise level set true value 
expected ornstein uhlenbeck prior mean prediction interpolates data error bars grow maximum value rapidly away data 
brownian motion process suitable prior smoothing technique 
tendency interpolate smooth data diminishes growing speed error bars grow away data 
note slim error bars rbf prediction data rich regions expressing strong prior belief underlying function smooth close smooth mean prediction 
stein notes predictions rbf covariance function come unrealistically small error bars 
situations uncertainty estimates importance quality decisions 
bayesian context decisions substituting predictive distribution inferred data unknown truth 
utility values computed expectations predictive distribution bayesian optimal decisions comparing di erent alternatives 
simple example arises binary classi cation task allows reject certain fraction test patterns 
bayesian optimal decision reject patterns target predictive distribution jx uncertain highest entropy 
similar setting treated heuristically svm discriminants rejecting patterns discriminant value closest zero 
note cases interested order relations scores test set numerical values 
study comparing practices gp technique sparse ivm approximation amount running time done sect :10.1.1.12.8842:10.1.1.135.8784:10.1.1.135.8784:10.1.1.19.8785

concludes example considered svm reject strategy shows signi cant weaknesses compared approximate bayesian ivm setup additional obtaining uncertainty estimates pay shown large wrong predictive means accompanied large predictive variances error bars noisy sine regression task di erent covariance functions 
mean prediction solid errors bars dotted true curve dashed data dots 
upper left rbf 
upper right ornstein uhlenbeck mat ern 
lower left mat ern 
lower right mat ern 
note shortcomings svm alleviated transformations discriminant output suggested leave order relations invariant 
summary described central properties gaussian processes statistical models gps ecient generic ways approximate inference model selection 
focus giving algorithmic descriptions concrete inference approximations variational optimisation problems may provided 
hope conveyed basic concepts latent variables gaussian random elds required understand non parametric algorithms highlighted essential di erences parametric statistical models 
explaining superior performance bayesian score combines quantities 
evolution powerful computers development fast sparse inference approximations feel gp models applicable large data problems previously restricted parametric models 
gp models powerful exible simple linear parametric models easier handle complicated ones multi layer perceptrons availability fast algorithms remove remaining obstacles part standard toolbox machine learning practitioners 
chris williams discussions important comments early drafts david barber bernhard sch olkopf corrections improvements bernhard sch olkopf mpi hospitality september furthermore neil lawrence ralf herbrich csat manfred opper carl rasmussen amos michael tipping discussions comments 
author gratefully acknowledges support research studentship microsoft research postgraduate studies 
appendix section describe notational conventions concepts probability theory 
section collect de nitions 
notation vectors column default matrices written bold face 
mg ng index sets denotes jij jj sub matrix formed selecting corresponding entries special vectors matrices de ned follows vectors zero ones th standard unit vector 
kronecker symbol 
furthermore identity matrix 
superscript denotes transposition 
diag matrix diagonal 
diag vector containing diagonal tr sum diagonal elements tr diag 
jaj denotes determinant square matrix kak denotes norm vector kak ja said 

euclidean norm 
relations matlab style scalar functions means distinguish notationally random variable possible values 
vector matrix random variables written way vectors matrices 
distribution density generally notation distribution density function 
random variable denotes expectation expected value event denotes probability 
probability space index sets sets data points assumed ordered notation known unordered sets 
usually clear context clarity additional subscript pr fag meaning 
denote indicator function event true 
note 
delta distribution places mass point mass fx bg sets random variables non empty 
write denote conditional independence conditional distribution depend log denotes logarithm euler base notation means cg constant notation left hand side density 
sgn denote sign sgn sgn sgn 
landau notation de ned exists constant probability theoretic concepts notation unfamiliar reader 
measure denoted lebesgue measure denoted dx 
measurable set event fx ag denotes mass 
measure nite mass space nite probability measure mass 
probability measure denote distribution 
events mass called null sets 
example lebesgue measure usual volume ane spaces dimension null sets 
property surely true event false null set 
called absolutely continuous null sets null sets notation 
theorem radon nikodym states density fx ag measurable case called radon nikodym derivative simply density de nitions de nition relative entropy probability measures space density dq dp exists 
relative entropy de ned eq log dq dp log dq dp dq absolutely continuous set 
non negative equal function 
strictly convex 
order run trouble assume probability space complete meaning sigma algebra contains subsets null sets 
density lebesgue measure dw dq dp ratio densities 
base measure nite need probability entropy de ned 
continuous distributions uniform lebesgue measure nite 
usual remedy subtract nite part entropy depend argument di erential entropy log dw entropy di erential entropy concave functions negative convex ones 

review gaussian random elds correlation functions 
technical report norwegian computing centre 
adler 
geometry random fields 
john wiley sons 

theory reproducing kernels 
trans 
amer 
math 
soc 
barber bishop 
ensemble learning multi layer networks 
jordan kearns solla editors advances neural information processing systems pages 
mit press 
becker thrun obermayer editors 
advances neural information processing systems 
mit press 
appear 
billingsley :10.1.1.19.8785
probability measure 
john wiley sons rd edition 
boyd vandenberghe 
convex optimization 
cambridge university press 
available online www stanford edu boyd html 
christopher burges 
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
chung 
course probability theory 
academic press nd edition 
cortes ha ner mohri 
rational kernels 
becker 
appear 
cressie 
statistics spatial data 
john wiley sons nd edition 
csat ernest manfred opper bernhard ole winther 
ecient approaches gaussian process classi cation 
solla pages 
csat manfred opper 
sparse line gaussian processes 
neural computation 
cybenko berry 
hyperbolic householder algorithms factoring structured matrices 
siam matrix anal 
appl 
dietterich becker ghahramani editors 
advances neural information processing systems 
mit press 
mark gibbs 
bayesian gaussian processes regression classi cation 
phd thesis university cambridge 
girard rasmussen murray smith 
gaussian process priors uncertain inputs application multiple step ahead time series forecasting 
becker 
appear 
green bernhard silverman 
nonparametric regression generalized linear models 
monographs statistics probability 
chapman hall 
geo rey grimmett david 
probability random processes 
oxford university press rd edition 

hilbert space theory spectral multiplicity 
chelsea new york 
david haussler 
convolution kernels discrete structures 
technical report ucsc crl university california santa cruz july 
see www cse ucsc edu haussler pubs html 

information theory continuous systems 
world scienti st edition 
jaakkola haussler :10.1.1.44.7709
exploiting generative models discriminative classi ers 
kearns pages 
tommi jaakkola david haussler 
probabilistic kernel regression models 
heckerman whittaker editors workshop arti cial intelligence statistics 
morgan kaufmann 
tommi jaakkola marina meila tony jebara 
maximum entropy discrimination 
solla pages 
kearns solla cohn editors 
advances neural information processing systems 
mit press 
kimeldorf wahba 
correspondence bayesian estimation stochastic processes smoothing splines 
annals mathematical statistics 
kolmogorov 
foundations theory probability 
chelsea new york nd edition 
trans 
morrison 
kondor la erty 
di usion kernels graphs discrete input spaces 
sammut hofmann editors international conference machine learning 
morgan kaufmann 

statistical approach basic mine valuation problems 
journal chemical mining society south africa 
neil lawrence matthias seeger ralf herbrich :10.1.1.12.8842
fast sparse gaussian process methods informative vector machine 
becker 
see www cs berkeley edu 
leen dietterich tresp editors 
advances neural information processing systems 
mit press 
mackay 
bayesian methods adaptive models 
phd thesis california institute technology 
mackay 
bayesian non linear modeling energy prediction competition 
ashrae transactions volume pages 

principles 
economic geology 

intrinsic random functions applications 
journal applied probability 
nelder 
generalized linear models 
number monographs statistics applied probability 
chapman hall st edition 
minka picard 
learning learn learning point sets 
unpublished manuscript 
available media mit edu papers learning html 
thomas minka 
family algorithms approximate bayesian inference 
phd thesis massachusetts institute technology january 
moore 
properly positive hermitian matrices 
bull 
amer 
math 
soc 
neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr university toronto 
see www cs toronto edu radford 
neal 
bayesian learning neural networks 
number lecture notes statistics 
springer 
radford neal 
monte carlo implementation gaussian process models bayesian classi cation regression 
technical report department statistics university toronto january 
nelder 
generalized linear models 
journal roy 
stat 
soc 

hagan 
curve tting optimal design 
journal roy 
stat 
soc 

hagan 
bayesian numerical analysis 
bernardo berger dawid smith editors bayesian statistics pages 
oxford university press 
opper winther 
gaussian process classi cation svm mean eld results leave estimator 
smola :10.1.1.19.8785
manfred opper ole winther 
gaussian processes classi cation mean eld algorithms 
neural computation 

nonstationary gaussian processes regression spatial modelling 
phd thesis carnegie mellon university pittsburg 
platt 
probabilistic outputs support vector machines comparisons regularized likelihood methods 
smola :10.1.1.19.8785
platt burges weare zheng 
learning gaussian process prior automatically generating music playlists 
dietterich pages 
john platt 
fast training support vector machines sequential minimal optimization 
sch olkopf pages 
poggio girosi 
networks approximation learning 
proceedings ieee 
william press saul teukolsky william vetterling brian flannery 
numerical recipes cambridge university press nd edition 
rosenblatt 
perceptron probabilistic model information storage organization brain 
psychological review 
roweis saul 
nonlinear dimensionality reduction locally linear embedding 
science 

fast method calculating perceptron maximal stability 
journal de physique 
sch olkopf burges smola editors 
advances kernel methods support vector learning 
mit press 
bernhard sch olkopf alexander smola 
learning kernels 
mit press st edition 
sch :10.1.1.19.8785
metric spaces completely monotone functions 
proc 
nat 
acad 
sci volume pages 
sch :10.1.1.19.8785
spline functions problem graduation 
annals 
seeger :10.1.1.19.8785:10.1.1.19.8785
covariance kernels bayesian generative models 
dietterich pages 
seeger :10.1.1.135.8784:10.1.1.135.8784:10.1.1.19.8785
bayesian gaussian process models pac bayesian generalisation error bounds sparse approximations 
phd thesis university edinburgh july 
see www cs berkeley edu 
matthias seeger :10.1.1.19.8785
bayesian methods support vector machines gaussian processes 
master thesis university karlsruhe germany 
see www cs berkeley edu 
matthias seeger :10.1.1.19.8785
bayesian model selection support vector machines gaussian processes kernel classi ers 
solla pages 
matthias seeger :10.1.1.19.8785
covariance kernels bayesian generative models 
technical report institute anc edinburgh uk 
see www cs berkeley edu 
matthias seeger :10.1.1.19.8785
pac bayesian generalization error bounds gaussian process classi cation 
journal machine learning research october 
smola bartlett sch olkopf schuurmans editors :10.1.1.19.8785
advances large margin classi ers 
mit press 
smola ov ari williamson :10.1.1.19.8785
regularization dot product kernels 
leen pages 
smola sch olkopf 
uller 
connection regularization operators support vector kernels 
neural networks 
murray smith rasmussen 
derivative observations gaussian process models dynamic systems 
becker 
appear 
solla leen 
uller editors 
advances neural information processing systems 
mit press 
peter 
probabilistic methods support vector machines 
solla pages 
stein 
interpolation spatial data theory kriging 
springer 

uence kernel consistency support vector machines 
journal machine learning research 
martin szummer tommi jaakkola 
partially labeled classi cation markov random walks 
dietterich pages 
tenenbaum de silva langford 
global geometric framework nonlinear dimensionality reduction 
science 
michael tipping 
sparse bayesian learning relevance vector machine 
journal machine learning research 
williams 
discovering hidden features gaussian process regression 
kearns 
grace wahba 
spline models observational data 
cbms nsf regional conference series 
siam society industrial applied mathematics 
grace wahba 
support vector machines reproducing kernel hilbert spaces randomized 
sch olkopf pages 
wainwright sudderth willsky 
tree modeling estimation gaussian processes graphs cycles 
leen pages 
weiss freeman 
correctness belief propagation gaussian graphical models arbitrary topology 
solla pages 
williams 
computation nite neural networks 
neural computation 
christopher williams 
prediction gaussian processes linear regression linear prediction 
jordan editor learning graphical models 
kluwer 
christopher williams david barber 
bayesian classi cation gaussian processes 
ieee transactions pattern analysis machine intelligence 
wright 
modi ed cholesky factorizations interior point algorithms linear programming 
siam journal optimization 

correlation theory stationary related random functions volume springer 
zhu williams rohwer 
gaussian regression optimal nite dimensional linear models 
bishop editor neural networks machine learning volume nato asi series 
springer 
