mpjava high performance message passing java java nio william pugh jaime university maryland college park md usa pugh cs umd edu 
explore advances java virtual machine jvm technology new high performance libraries java find java increasingly attractive platform scientific message passing codes 
report new technologies allow pure java implementation cluster communication library performs competitively standard mpi implementations 
previous efforts java message passing frameworks focused making functionality message passing interface mpi available java native code wrappers existing mpi libraries mpijava pure java implementations 
previous showed pure java java native mpi hybrid approaches offered substantially worse performance mpi applications written fortran mpi bindings 
built message passing java mpjava pure java message passing framework 
extensive java nio package introduced java 
currently framework provides subset functionality available mpi 
mp java java native interface jni 
jni convenient occasionally necessary violates type safety incurs performance penalty due additional data copies java heaps prevents jvm just time jit compiler fully optimizing methods native calls 
mpjava offers promising results high performance message passing pure java 
cluster linux workstations mpjava provides performance competitive lam mpi java grande forum ping pong microbenchmarks 
framework provides performance comparable fortran lam mpi implementation conjugate gradient benchmark taken nasa advanced supercomputing parallel benchmarks nas pb benchmark suite 
design implementation designed mpjava mpi message passing library implemented pure java making improved capabilities java nio package 
mpjava adheres single program multiple data spmd model mpi 
mpjava instance knows total nodes computation unique processor identification tag pid 
information programmer decide split shared data 
example nodes mpjava computation shared array elements store elements node node data exchanged nodes point point send recv operations collective communications broadcast 
distributing data manner communication routines typical mpi openmp parallel programming paradigms 
functionality mpjava api provides point point send recv functions send int peer int offset int len double arr recv int peer int offset int len double arr high level functions away messy details related tcp allowing user focus application message passing details 
mpjava provides subset collective communication operations typically available message passing library mpi 
example array elements distributed nodes broadcast routine recreate entire array elements node double arr int distribution distribution parameter constant tells mpjava data distributed nodes 
default setting follows array elements split nodes node holding elements node holding elements 
distribution patterns possible employ simple default setting experiments 
bootstrapping mpjava provides series start scripts read list hostnames perform necessary remote logins machine start mpjava processes machine special arguments allow mpjava process find 
result bootstrap process network mpjava processes process tcp connections process network 
tcp connections nodes point point collective communications 
collective communication algorithms explored different broadcast algorithms multi threaded concurrent algorithm pairs nodes exchange data parallel parallel prefix algorithm uses single thread 
concurrent algorithm node separate send receive thread select mechanism multiplex communication processors 
parallel prefix implementation data exchange proceeds log rounds sending pieces data round current round number 
example total nodes node broadcast schedule example broadcast schedule node total nodes round partner data java nio java new apis java nio defined java specification request jsr 
new nio libraries heavily influenced address number issues exposed pioneering matt welsh jaguar chi chao chang thorsten von eicken 
inefficiencies java io java net original java io java net libraries available prior jdk perform client server codes remote method invocation rmi wan environment 
performance libraries suitable highperformance communication lan environment due key inefficiencies design java io libraries process converting bytes primitive types doubles inefficient 
native method allows double treated bit long integer jni required type coercions double long allowed java strong type system 
bit shifts bit masks strip bit segments bit integer write bit segments byte array 
java nio buffers allow direct copies doubles values buffers support bulk operations copying java arrays java nio buffers 
java io operations array bytes allocated java heap 
java pass arrays allocated java heap system level operations objects java heap moved garbage collector 
array allocated heap data copied back forth 
alternatively avoid extra overhead jvm implementations pin byte array java heap operations 
java nio buffers allocated allocated heap subject garbage collection 
allows operations copying required operating system programming language 
prior nio java lacked way single thread poll multiple sockets ability non blocking requests socket 
workaround solution separate thread poll socket introduces unacceptable overhead high performance application simply scale number sockets increases 
java nio adds unix select mechanism addition non blocking sockets 
java nio high performance computing mpjava demonstrates java deliver performance competitive mpi message passing applications 
maximize performance framework careful java nio features critical high performance computing channels select buffers 
channels new abstraction tcp sockets complement socket class available java net 
major differences channels sockets channels allow non blocking calls polled selected calls java nio select mechanism operate java nio byte arrays 
general channels efficient sockets select fairly simple fulfills need self explanatory 
java nio buffers hand slightly complicated careful buffers necessary ensure maximal performance mpjava 
detail experiences buffers 
useful new abstraction provided nio buffer container primitive type 
buffers maintain position provide relative put get methods operate element specified current position 
addition buffers provide absolute put int index byte val get int index methods operate element specified additional index parameter bulk put get methods transfer range elements arrays buffers 
allocated backing store allocated heap subject relocation garbage collector 
results directly passed arguments system level calls additional copying required jvm 
direct buffers expensive allocate garbage collect pre allocate required buffers 
user sees collective communication api call mpi function framework handles scenes necessary copies data user arrays pre allocated direct buffers 
best performance important ensure buffers set native hardware 
java platform independent possible create buffers big endian little endian formats storing multi byte values 
furthermore default byte order buffers big endian regardless native byte order machine jvm executing 
communicate set heterogeneous platforms mixed byte orders need perform extra bookkeeping weather performance overhead process 
experience issues clusters consist homogenous set machines 
provides method returns view chunk backing data shared 
maintaining multiple views piece data important reasons supports operations read write primitive types doubles operation requires checks alignment addition bounds checks typical java 
provide bulk transfer operations primitive types 
socket calls require parameters 
nio solves issues multiple views provides bulk transfer methods doubles require checks alignment endian ness 
furthermore transfers visible view need expensive conversions shares backing storage 
maintaining views buffer cumbersome manageable 
map corresponding take care changing position buffers changes position buffer visible views backing data 
furthermore careful prevent overlap simultaneous calls chunk backing data resulting race condition leads nasty unpredictable bugs 
mpjava api calls framework take normal java arrays parameters 
approach requires data copied arrays buffers data passed system level os calls 
avoid extra copy operations initially implemented framework eye performing calculations directly buffers 
strategy requires complicated syntax buffers manipulated put get methods cleaner square bracket notation java arrays performance penalty repeated put get methods buffer order magnitude worse similar code uses java arrays 
turns cost copying large amounts data arrays buffers send buffers arrays receive cost put get methods required perform computations buffers 
performance results conducted experiments cluster pentium iii mhz machines mb ram running redhat linux 
connected channel bonded mbps links cisco switch capable switching maximum packets gb compared fortran codes compiled linked lam mpi mpjava compiled jdk mpijava linked mpich 
mpich underlying mpi implementation mpijava mpijava supports mpich lam 
chose lam mpich experiments lam designed performance delivers better performance mpich designed primarily portability 
ping pong compare mpjava framework lam mpi java io ping pong benchmark 
benchmark java grande forum ping pong benchmark measures maximum sustainable throughput nodes copying data array doubles processor array doubles processor back 
results 
horizontal axis represents number doubles swapped pair nodes 
avoid performance anomalies occurring powers os networking stack adopt java grande forum convention values similar powers 
vertical axis labeled mbits shows bandwidth calculated total number bits exchanged pair nodes divided total time send receive operations 
report results node initiates send followed receive ensure timing entire round trip transit time 
maximum bandwidth benchmark mbps half hardware maximum 
report median runs single slow outlier impact mean value significant amount especially small message sizes transmission time dominated latency 
mbits pingpong nodes mpjava lam mpi java io bytes java io doubles doubles swapped fig 

ping pong performance mpjava lam mpi mpijava java io 
note java io doubles performs conversions doubles bytes java io bytes different java io implementations java io doubles performs necessary conversions doubles bytes vice versa java io bytes sends equivalent amount data byte arrays conversions 
java io doubles implementation highlights tremendous overhead imposed conversions old model results java io bytes represent upper bound performance old java model 
surprising java nio enabled mpjava framework outperforms java io doubles implementation conversions extremely inefficient 
mpjava outperforms java io bytes implementation data sizes larger doubles 
surmise due inefficiencies java io buffering data 
implementations need copy data java heap heap mpjava needs copy data java array preallocated direct buffer need cleaned java io bytes implementation needs allocate clean space heap 
may expensive operation jvms 
native lam mpi implementation provides better performance mpjava message sizes doubles mpjava provides superior performance sizes larger doubles 
main contribution particular experiment empirical evidence provide java capable delivering sustained data transfer rates competitive available mpi implementation common microbenchmark 
microbenchmark implemented bandwidth utilization microbenchmark java grande forum java 
microbenchmark measures bandwidth utilization realistic manner ping pong 
communication necessary vector shared nodes needs distributed node sending portion node 
nodes vector total elements node communicate elements peers 
mbits mpjava benchmark prefix algorithm doubles exchanged pair nodes nodes nodes nodes nodes nodes fig 

performance mpjava prefix algorithm mbits lam mpi benchmark doubles exchanged pair nodes fig 

performance lam mpi nodes nodes nodes nodes nodes represents results framework parallel prefix algorithm shows results concurrent algorithm 
illustrates performance microbenchmark application written lam mpi mbits mpjava benchmark concurrent algorithm doubles exchanged pair nodes nodes nodes nodes nodes nodes fig 

performance mpjava concurrent algorithm mbits mpijava benchmark doubles exchanged pair nodes fig 

performance mpijava nodes nodes nodes nodes library shows results mpijava bindings mpich 
note chart mpijava performance nodes performance achieved mbps 
values axis represent number doubles exchanged pair nodes 
value axis means total bytes transmitted number nodes 
actual values selected axis ping pong microbenchmark previously reason 
axis charts performance megabits mbps 
chose median value runs single slow outlier negatively impact mean value especially small message sizes runtimes dominated latency 
dips irregularities repeatable 
note figures scale axis theoretical hardware maximum experiment mbps 
mpjava concurrent broadcast algorithm occasionally outperforms parallel prefix algorithm performance concurrent algorithm consis tent useful 
believe due part sub optimal thread scheduling os jvm 
addition able achieve true concurrency experiment machines experiments cpu 
mpjava parallel prefix algorithm outperformed lam mpi implementation large message sizes 
ascribe differences difference broadcast algorithms 
parallel prefix predictable send receive schedule lam mpi uses na algorithm exchanges data pair nodes 
comparison mpijava somewhat unfair mpich underlying native mpi library mpijava gave substantially worse performance lam mpi 
comparison provide evidence performance hurdles overcome java gain acceptance viable platform clustered scientific codes 
possible mpi implementation sophisticated broadcast strategy outperforms current implementation reason strategy incorporated java nio implementation achieve similar performance 
cg final performance results nas pb conjugate gradient cg benchmark 
cg benchmark provides realistic evaluation suitability java high performance scientific computation contains significant floating point arithmetic 
cg algorithm uses inverse power method find estimate largest eigenvalue symmetric positive definite sparse matrix random pattern nonzero values 
kernel cg algorithm consists multiplication sparse matrix vector followed reductions double broadcast vector iteration 
core operations comprise runtime calculation 
kernel iterates times called cg benchmark times approximate solution desired precision 
evaluated cg benchmark class class sizes 
class rows total avg 
row data cg benchmark stored compressed row storage crs format 
na way parallelize algorithm divide rows matrix nodes performing matrix vector multiplication broadcast recreate entire vector node 
implemented approach fortran mpi mpjava provide results approach 
adequately optimize code ran nas cg benchmark pgf portland compiler group optimizing fortran compiler 
performance nearly identical results 
sophisticated compiler optimize face extra layer indirection required crs storage format sparse matrix nas pb implementation cg performs clever dimensional decomposition sparse matrix replaces broadcasts reductions runtime secs conjugate gradient class broadcast reduce mpj mpj mpj mpj mpj nodes runtime secs conjugate gradient class broadcast reduce mpj mpj mpj mpj mpj nodes fig 

conjugate gradient class mpjava mpj simple fortran 
note pair stacked bar charts mpjava leftmost simple fortran rightmost rows decomposed matrix 
resulting communication pattern implemented send recv primitives efficient collective communications 
implemented sophisticated decomposition algorithm nas cg implementation mpjava provide results 
instrumented codes time major contributors run time computation multiplication sparse matrix vector broadcast vector reductions required inner loop cg kernel 
versions code perform number floating point operations 
report results versions na fortran fortran na mpjava mpjava fortran decomposition fortran mpjava decomposition mpjava 
results table class problem size table class problem size 
results na algorithm show mpjava capable delivering performance competitive popular freely available widely deployed fortran mpi technology 
poor performance observable nodes fortran code reflects fact lam mpi collective communication primitive scale 
results highlight importance choosing appropriate collective communication algorithm characteristics codes executed hardware configuration employed 
results decomposition algorithm show mpjava competitive fortran mpi 
mpjava performance slightly worse fortran mpi results 
popular wisdom suggests java performs factor slower fortran 
left field high performance java computing hope results help java case viable platform scientific computing 
results benchmark suggest mpjava capable delivering performance comparable excess performance achievable native mpi applications 
runtime secs conjugate gradient dimensional algorithm class broadcast reduce mpj mpj mpj mpj mpj nodes runtime secs conjugate gradient dimensional algorithm class broadcast reduce mpj mpj mpj mpj mpj nodes fig 

conjugate gradient class mpjava mpj original nas fortran 
note pair stacked bar charts mpjava leftmost nas fortran rightmost addition benchmark provides promising results current state java virtual machine jvm technologies 
results sparse matrix vector multiplications nearly identical simple fortran simple mpjava versions mpjava performs fortran versions 
optimization performed sparse matrix vector multiplication code unrolling loop factor accounted improvement simple mpjava implementation 
assume fortran compilers perform optimization loop unrolling hand effect fortran code compiled pgf 
related large body dealing message passing java 
previous approaches loosely divided categories java native hybrids java io approaches 
mpijava efforts provide native method wrappers existing mpi libraries 
resulting programming style complicated mpijava generally better supported 
approaches provide java programmer access complete functionality supported mpi library mpich 
hybrid approach simple number limitations 
mpi java relies proper installation additional library 
overhead java native interface jni imposes performance penalty native code performance application worse directly implemented mpi bindings 
furthermore jit compiler maximally conservative assumptions presence native code may potential optimizations 
language nodes total runtime broadcast reductions fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fig 

raw results class conjugate gradient benchmark 
times reported seconds 
language nodes total runtime broadcast reductions fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fortran mpj fig 

raw results class conjugate gradient benchmark 
times reported seconds 
java io implementations proposed mpj standard carpenter 
official set mpi bindings java implementation particular advantages disadvantages 
part distributed object groups metacomputing architecture dogma project byu pure java implementation large subset mpi features 
implementation proposed mpi bindings carpenter 
codebase available public download time publication 
steve morin provides excellent overview design 
unable find published results performance 
manta project supports interesting flavors message passing codes java including collective communication java ccj group method invocation gmi ibis 
ccj rmi collective communication library written entirely java 
provides features provide broadcast necessary scientific codes conjugate gradient 
gmi generalization java rmi methods invoked single object group objects results discarded returned normally combined single result 
extremely interesting high level programmatic perspective fully orthogonal group design potentially allows programs break spmd model dominant mpi codes 
ibis harnesses techniques infrastructures developed ccj gmi provide flexible grid programming environment 
mpi soft tech announced commercial endeavor called jmpi effort provide mpi functionality java 
deliver product design goals 
message passing library provides pvm mpi functionality java 
library uses threads udp improved performance utilize java nio 
communications subject inefficiencies older java io package 
time publication alpha version library available windows properly linux 
port pvm java syntactic semantic modifications better suited java capabilities programming style 
port elegant provides additional novel features available pvm implementations fortran 
performance due large part older io libraries proved limiting factor wide adoption 
presents native code mechanism serialization primitive types java 
extremely efficient native serialization primitive types byte arrays violates type safety benefit java nio 
titanium dialect java provides new features useful highperformance computation java immutable classes multidimensional arrays zone memory management 
titanium backend produces code mpi calls 
performance outperform native mpi applications substantially worse 
provide useful overview state distributed java endeavors 
done grid computing 
directly deal issues important grid environment adaptive dynamic scheduling automatic parallelism 
focus developing efficient set communication primitives grid aware library built top 
built pure java message passing framework nio 
demonstrate message passing framework harnesses high performance communication capabilities nio deliver performance competitive native mpi codes 
provide empirical evidence current java virtual machines produce code competitive static fortran compilers scientific applications rich floating point arithmetic 
mpi supports asynchronous messages typically benefit threads cumbersome way programmer 
modified version framework provides abstraction asynchronous pipes 
accomplished separate send receive threads callbacks user defined functions 
evaluate performance asynchronous messagepassing framework problems easily fit spmd model distributed stealing sharing 
clusters increasingly composed interconnected smps 
typically useful schedule multiple mpi tasks smp node additional processes fight shared resources bandwidth memory 
java framework supports interleaving computation communication send receive compute threads better utilize extra processors jvm free schedule threads available processors 
developed threaded version mpjava framework maintains send receive computation thread 
cg algorithm node needs entire vector portion iteration broadcast matrix vector multiply step significant contributors total runtime threads interleave communication computation steps 
preliminary results worse singlethreaded results due poor scheduling threads os jvm 
notion interleaving computation communication especially smp appealing requires study 
multi threading area pure java framework offer substantial advantages mpi codes mpi implementations fully thread safe 
interest mpi java library high years ago interest due performance reported previous implementations 
nio enable high performance communication time interest mpi java 
investigate high level question highperformance message passing framework java target mpi adhere standard 
acknowledgments supported nsa nsf darpa 

mohamed jiang swanson 
comparative study parallel distributed java projects 
ipdps workshop java parallel distributed computing 

jmpi www mpi com publications jmpi html 

baker carpenter ko li 
mpijava java interface mpi 

carpenter judd skjellum fox 
mpj mpi message passing java 
concurrency practice experience 


chang von eicken 
interfacing java virtual interface architecture 
acm java grande 

ferrari 
network parallel computing java 
concurrency practice experience 

vip com 

judd clement snell 
dogma distributed object group management architecture 
concurrency practice experience pages 

lam local area multicomputer www lam mpi org 

maassen bal 
gmi flexible efficient group method invocation parallel programming 
languages compilers runtime systems pages 

manta fast parallel java www cs vu nl manta 


writing programs 
technical report man school computer science university london uk october 

morin koren krishna 
jmpi implementing message passing interface standard java 
ipdps workshop java parallel distributed computing april 

mpich portable implementation mpi mcs anl gov mpi mpich 

bal maassen 
object collective communication java 
joint acm java grande conference 

philippsen 
efficient rmi java 
java grande pages 

jsr new apis platform www jcp org jsr detail jsp 

message passing interface standard 

nas parallel benchmarks www nas nasa gov nas npb 

van bal maassen 
ibis efficient java grid programming environment 
joint acm java grande conference 

welsh culler 
jaguar enabling efficient communication java 
concurrency practice experience 

yelick pike miyamoto krishnamurthy hilfinger graham gay colella aiken 
titanium highperformance java dialect 
acm editor acm workshop java high performance network computing new york ny usa 
acm press 
