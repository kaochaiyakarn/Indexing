collective classification relational dependency networks jennifer neville david jensen department computer science governors drive university massachusetts amherst amherst ma jensen cs umass edu collective classification models exploit dependencies network objects improve predictions 
example network web pages topic page may depend topics hyperlinked pages 
relational model capable expressing reasoning dependencies achieve superior performance relational models ignore dependencies 
relational dependency networks extending dependency networks relational setting 
collective classification model offers simple parameter estimation efficient structure learning 
real world data sets compare ordinary classification relational probability trees show collective classification improves performance 
show autocorrelation improve accuracy statistical models relational data 
autocorrelation statistical dependency values variable related entities common characteristic relational data sets 
previous shown autocorrelation cause bias learning algorithms jensen neville particularly appears concentrated linkage common characteristic relational data sets 
explores methods exploit autocorrelation improve model accuracy 
particular demonstrate accomplished relatively simple approach structure learning class undirected graphical models call relational dependency networks 
relatively easy understand autocorrelation improve predictions statistical models 
example consider problem automatically predicting topic technical neural networks reinforcement learning genetic algorithms 
simple method predicting topics look position citation graph 
may possible predict topic high accuracy topics neighboring papers graph 
possible expect high autocorrelation citation graph papers tend cite papers topic 
scheme prediction assumes labels related entities topics referenced papers known 
cases topics entire set papers may need inferred simultaneously 
approach called collective classification taskar koller requires models inference procedures inferences entity relational data set influence inferences related entities 
similar approaches applications neville jensen chakrabarti dom indyk 
introduce relational dependency networks undirected graphical model relational data 
show learned gibbs sampling collective classification 
undirected graphical models represent cyclic dependencies required express autocorrelation express joint probability distribution single conditional distribution 
addition relatively simple learn easy understand 
show preliminary results indicating collective inference offers improved performance non collective inference term individual inference 
show applied collectively perform near theoretical ceiling achieved labels neighbors known perfect accuracy 
results promising indicating potential utility additional exploration collective inference 
classification models relational data relational models individual inference inferences instance change inference related instances 
example inductive logic programming ilp relational learning uses relational instances represented disconnected subgraphs molecules removing need opportunity collective inference 
models cope complex relational structure instance attempt model relational structure instances 
learning relational models individual inference difficult learning relational data differs substantially learning propositional settings 
example ilp long considered difficult representational issues recursion aggregation variables quantification aspects order higher order logics 
addition examined unique statistical properties relational data influence characteristics learning algorithms 
shown concentrated linkage relational autocorrelation bias relational learners particular features jensen neville 
shown degree disparity cause misleading correlations relational data leading learning algorithms add excessive structure learned models jensen neville hay 
address challenges relational learning new learning algorithms necessary 
example probabilistic relational models prms getoor friedman koller pfeffer form directed graphical model bayesian networks support reasoning complex relational domains 
unfortunately prms directed model autocorrelation represented due acyclicity constraints 
example developed relational probability trees method learning tree structured models encode conditional probability distributions class label depend attributes related instances features surrounding structure neville jensen hay 
methods learning adjust sources bias mentioned 
result learned relatively compact parsimonious representation conditional probability distributions relational data 
applied models individual inference 
algorithm learning adjusts statistical biases caused autocorrelation inference techniques autocorrelation improve inference 
unfortunate autocorrelation nearly ubiquitous phenomenon relational data 
observed relatively high levels autocorrelation relational data sets 
example analysis kdd cup data proteins located place cell cell wall highly autocorrelated functions transcription cell growth 
autocorrelation identified investigators 
example fraud mobile phone networks highly autocorrelated cortes pregibon 
topics authoritative web pages highly autocorrelated linked directory pages serve hubs kleinberg 
know exploiting autocorrelation result significant increases predictive accuracy 
developments relational learning focused exploiting autocorrelation 
example relational vector space rvs model bernstein clearwater provost uses weighted adjacency vectors construct classifiers 
model extremely simple produces accurate classifiers data strong autocorrelation 
examples include web pages chakrabarti uses hyperlink structure produce smoothed estimates class labels prior neville jensen uses iterative classification scheme improve accuracy exploiting inferred class labels related instances 
undirected graphical models capable representing reasoning autocorrelation explored task modeling relational data 
domingos richardson represent market entities social networks develop markov random field models model influence purchasing patterns network 
taskar 
relational markov networks conditional random fields sequence data lafferty mccallum pereira model dependencies web pages predict page type 
undirected models proved successful collective classification relational data 
research models focused primarily parameter estimation inference procedures 
model structure learned automatically prespecified user 
models automatically identify features relevant task 
relational tasks large exception autocorrelation structured additional information temporal constraints 
getoor number features lack selectivity model difficult interpret understand 
relational dependency networks relational dependency networks extend dependency networks heckerman chickering meek kadie relational setting 
dependency networks dns graphical models probabilistic relationships alternative bayesian networks markov random fields 
dns easy learn shown perform number propositional tasks expect offer similar advantages relational setting 
reviewing details dependency networks propositional data describe extend dependency networks relational data 
dependency networks bayesian networks markov random fields dependency networks encode probabilistic relationships set variables 
dependency networks alternative form graphical model approximate full joint distribution set conditional distributions learned independently 
dns combine characteristics undirected directed graphical models 
dependencies variables represented undirected graph conditional independence interpreted graph separation 
directed models dependencies quantified conditional probability distributions cpds variable parents 
primary distinction dns graphical models dns approximate model 
cpds learned independently dn models guaranteed specify coherent probability distribution see learning section details 
dns offer advantages conventional bayesian networks disadvantages heckerman 
bayesian networks dns difficult construct knowledge approach represent causal relationships 
dn models encode predictive relationships dependence independence simple techniques parameter estimation structure learning dns 
characteristics distinguish dn models bayesian networks similar markov random fields 
dns markov random fields undirected graphs represent dependencies variables 
causal relationships variables unknown undirected models interpretable directed models require separation reasoning assess conditional independencies 
undirected graphical models straightforward techniques parameter estimation structure learning della pietra della pietra lafferty 
dn conditional probability distributions generally easier inspect understand markov network clique potentials dns approximate full joint distribution markov networks may produce accurate probabilities 
representation 
dn model consists set conditional probability distributions cpds variable parents 
consider set variables xn joint distribution xn 
dependency network represented graph node corresponds xi parents node xi denoted pai consist nodes markov blanket specifies parents node conditionally independent rest nodes graph pai xi undirected edges graph connect node xi parents nodes pai 
furthermore node graph contains local cpd pi xi pai 
cpds specify joint distribution example dn model set variables 
node conditionally independent nodes graph parents 
example conditionally independent 
node contains cpd specifies probability distribution possible values values parents 
full joint probability product local cpds notice model may cycles 
example form clique graph 
necessitates approximate inference techniques see inference section details 
notice dependent reverse true 
undirected edge placed nodes nodes dependent 
case node parents form markov blanket won minimal set 
fig 

sample dependency network 
learning 
graphical model components learning dn structure learning parameter estimation 
structure learning parameter estimation accomplished learning local cpds variable 
structure dn model defined components local cpds feature specifications define structure undirected graphical model 
edges graph connect node variables local cpd 
parameters model correspond parameters local cpds 
dn learning algorithm learns separate cpd variable conditioned variables data 
dn model fully connected selective learning algorithm 
selective modeling technique build parsimonious dn model desirable selective learner learn small accurate cpds 
example algorithm learning relational probability trees model 
variables included tree designated parents network appropriate edges added graph tree local cpd 
learning full dn require learning models tree variable graph 
dn approach structure learning simple result inconsistent network structurally numerically may joint distribution cpds obtained rules probability 
learning local cpd independently may result inconsistent network edge variables dependent dependent vice versa 
independent parameter estimation may result inconsistencies joint distribution sum 
heckerman 
show dns nearly consistent learned large data sets 
data serve coordinating function ensures degree consistency cpds 
inference 
bayesian networks dependency network graphs potentially cyclic 
due nature structure learning algorithm restriction form cpds 
cyclic dependencies necessitate approximate inference techniques gibbs sampling neal loopy belief propagation murphy weiss jordan 
heckerman gibbs sampling combine models approximate full joint distribution extract probabilities interest 
practice dns produced approximations joint distributions represented directed graphical models heckerman 
relational dependency networks relational dependency networks extend dns relational data 
extension similar way probabilistic relational models getoor extend bayesian networks relational data 
representation 
specify probability model network instances 
set objects links rdn defines full joint probability distribution attribute values objects 
attributes object depend probabilistically attributes object attributes objects relational neighborhood 
defining dependency structure attributes dns define generic dependency structure level object types 
attribute associated object type linked set parents influence value parents attributes associated type attributes associated objects type objects linked objects type dependency relation parent consists set attribute values 
situation uses aggregated features map sets values single values 
example contains example dataset movie domain 
types objects movies studios actors 
object type number associated attributes modeled rdn 
consider attribute actor set potential parents attribute consists actor gender movie receipts studio country 
notice actor star multiple movies associated indirectly multiple studios 
fig 

sample relational data graph 
full rdn model potentially larger original data graph 
model full joint distribution separate node cpd attribute value data graph 
consequently total number nodes model graph ta number objects type data graph number attributes objects type 
models tractable structure parameters cpds tied rdn model contains single cpd template attribute type object 
example model consist cpds actor gender actor movie receipts studio country 
construct model graph set template cpds rolled entire data graph 
object attribute pair gets separate local copy appropriate cpd 
facilitates generalization data graphs varying size 
learn cpd templates data graph apply model second data graph different number objects rolling cpd copies 
approach analogous graphical models tie distributions network hidden markov models prms 
contains possible rdn model graph example discussed 
shows dependencies actor gender actor actor movie receipts movie receipts studio country 
furthermore dependency movie receipts related movies 
notice hyper edges movies associated actor awards 
indicates movie receipts dependent aggregated feature actor exists actor 
aggregation approach ensure template cpds applicable data graphs varying structure 
movie may different number actor award values aggregation map sets values single values 
learning 
learning rdn model consists tasks learning dependency structure estimating parameters conditional probability distributions 
rdn learning algorithm dn learning algorithm selective relational classification algorithm learn set conditional models 
relational probability trees cpd components rdn 
extend standard probability estimation trees relational setting data instances heterogeneous interdependent neville 
rpt models estimate probability distributions possible class label values manner conventional classification trees algorithm looks attributes item class label defined considers effect local relational neighborhood probability distribution 
rpt models represent series questions ask item objects links relational neighborhood 
fig 

sample rdn model graph 
rpt learning algorithm neville uses novel form randomization tests adjust biases particular features due characteristics relational data degree disparity autocorrelation 
shown learned randomization tests build significantly smaller trees models achieve equivalent better performance 
characteristic crucial learning parsimonious rdn models collection rpt models gibbs sampling size models direct impact inference efficiency 
data graph object type target attribute rpt learned predict attribute attributes object attributes objects links relational neighborhood 
current approach user specifies size relational neighborhood considered rpt algorithm 
efficiency reasons limited algorithm consider attributes objects links away data graph 
possible rpt models consider attributes distant objects sense graph neighborhood 
shows example rpt learned data imdb predict movie opening weekend receipts attributes movies links away actors directors producers studios movies associated objects see section experimental details 
tree indicates movie receipts depend receipts movies studio actor age movie genre 
root node tree asks movies studio studio movie objects receipts case model moves node right branch asks movie actors born 
answer question prediction returned 
fig 

example rpt learned imdb dataset predict movie receipts labels related movies 
inference 
case dns gibbs sampling inference rdn models 
classification tasks want estimate full joint distribution target attributes graph 
inference model graph consists network observed unobserved variables 
example may want network predict movie receipts studio country actor gender actor 
case attribute values observed movie receipts 
values target variable initialized values drawn prior distribution default distribution movie receipts training set 
gibbs sampling proceeds iteratively estimating full joint distribution cycles 
target variable rpt model return probability distribution current attribute values rest graph 
new value drawn distribution assigned variable recorded 
process repeated unobserved variable graph 
sufficient number iterations values drawn stationary distribution samples estimate full joint distribution 
implementation issues improve estimates obtained gibbs sampling chain length burn number samples 
investigated effects decisions rdn performance 
experiments reported burn period fixed length chain samples 
experiments focuses evaluation classification context single attribute unobserved test set assumed observed modeled cpds 
experiments reported intended evaluate assertions 
claim dependencies instances improve model accuracy 
evaluate claim comparing performance models 
model conventional rpt model individual classification model labels related instances reasoning instance independently instances 
call model rpt 
second model collective classification rdn model exploits additional information available labels related instances reasons networks instances collectively 
second claim rdn models gibbs sampling effectively infer labels network instances 
evaluate claim include models comparison 
third model conventional rpt model allow true labels related instances learning inference 
call model rpt ceiling 
model included ceiling comparison rdn model 
shows level performance possible model knew true labels related instances 
fourth model intended assess need collective inference procedure 
call model rpt default 
reports performance achieved round gibbs sampling 
equivalent learning conventional rpt model true labels related instances randomly assigning labels prior distribution labels inference 
test sets connections labeled instances training set see section details possible labels provide information accurate inferences making gibbs sampling unnecessary 
tasks data set drawn internet movie database imdb www imdb com 
gathered sample movies released united states opening weekend receipt information 
resulting collection contains movies 
addition movies data set contains associated actors directors producers studios 
total data set contains objects links 
learning task predict movie box office receipts 
discretized attribute positive label indicates movie garnered opening weekend box office receipts receipts 
created training set test set splits temporal sampling sets year 
links removed sample 
example sample contains links movies vice versa 
trained movies year tested movies year 
notice inference links test set fully labeled instances training set 
approach sampling intended reproduce expected domain application models 
rpt learning algorithm applied subgraphs centered movies 
subgraphs consisted movies links away actors directors producers studios movies associated objects 
attributes supplied models including studio country actor birth year class label related movies links away 
shows example rpt learned class labels related movies 
movie objects tagged studio movie refer movies primary studio associated second data set drawn cora database computer science research papers extracted automatically web machine learning techniques mccallum nigam rennie seymore 
selected set machine learning papers published associated authors journals books publishers institutions 
resulting collection contains objects links 
prediction task identify topic 
machine learning papers divided topics reinforcement learning case reasoning probabilistic methods theory genetic algorithms neural networks rule learning 
imdb created training set test set splits temporal sampling sets year 
rpt learning algorithm subgraphs centered papers 
subgraphs consisted papers authors journals books publishers institutions papers associated authors 
twelve attributes available models including journal affiliation venue topic papers link away links away authors 
shows example rpt learned topics related papers 
rpt learning algorithm randomization tests feature selection bonferroni adjusted value growth cutoff 
rdn algorithm fixed number gibbs sampling iterations 
results discussion table shows accuracy results models imdb classification tasks 
imdb models perform comparably rpt ceiling models 
indicates rdn model realized full potential reaching level performance access true labels related movies 
addition performance rdn models superior rpt models rpt learned labels rpt default models rpt learned labels tested default labeling class values related instances 
example rpt see features refer target label related movies 
indicates autocorrelation data identified rpt models 
performance improvement due successful exploitation autocorrelation 
fig 

example rpt learned cora dataset predict topic topics related papers 
tailed paired tests assess significance accuracy results obtained trials 
tests compare rdn results models 
null hypothesis difference accuracies models alternative difference 
resulting values reported bottom row table column model compared rdn 
results support rdn results significantly better rpt rpt default 
average performance slightly lower rpt ceiling models test indicates difference significant 
table accuracy results imdb task rpt rpt ceiling rpt default rdn avg test cora classification task shown table rdn models show significant gains rpt rpt default models 
indicates predictive information lies topics related pages 
nearly features selected trees involved topic referenced papers 
recall topic twelve attributes available model form features 
experiment rdn models achieve level performance possible true label related papers known 
improvement rpt default models notable 
due paucity predictive attributes target label clearly showing autocorrelation exploited improve model accuracy 
table accuracy results cora task rpt rpt ceiling rpt default rdn avg test difference accuracy rdn rpt ceiling models may indicate gibbs sampling converged trials 
investigate possibility tracked accuracy gibbs sampling procedure 
shows curves tasks number gibbs iterations 
lines represent trials 
iterations limit graph accuracy plateaus iterations 
accuracy improves quickly leveling iterations 
shows approximate inference techniques employed rdn may quite efficient practice 
currently experimenting longer gibbs chains random restarts convergence metrics fully assess aspect model 
fig 

accuracy vs number gibbs iterations 
curve represents separate trial 
conclude learning curves gibbs chain converged didn rdn model perform rpt ceiling model cora 
possible explanation lack predictive attributes topic 
gibbs chain may mix slowly making procedure jump distant portion labeling space 
inference procedure suffer predictive attributes known certainty drive mixing process right direction 
results indicate collective classification offer significant improvement non collective approaches classification autocorrelation data 
performance approach performance possible class labels related instances known 
addition offer relatively simple method learning structure parameters graphical model allow exploit existing techniques learning conditional probability distributions 
chosen exploit prior construct particularly parsimonious models relational data expect general properties retained approaches learning conditional probability distributions approaches selective accurate 
acknowledgments andrew mccallum provided valuable comments suggestions 
michael hay wolfe comments 
research supported national science foundation graduate research fellowship darpa nsf afrl contract numbers eia 
government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation 
views contained authors interpreted necessarily representing official policies endorsements expressed implied darpa afrl government 
bernstein clearwater provost 
relational vector space model industry classification 
working stern school business new york university 
chakrabarti dom indyk 
enhanced hypertext categorization hyperlinks 
proc acm sigmod pp 
cortes pregibon 
communities interest 
proceedings intelligent data analysis 
della pietra della pietra lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence 
domingos richardson 
mining network value customers 
proceedings th international conference knowledge discovery data mining pp 

getoor friedman koller pfeffer 
learning probabilistic relational models 
relational data mining dzeroski lavrac eds springer verlag 
heckerman chickering meek kadie 
dependency networks inference collaborative filtering data visualization 
jmlr 
jensen neville 
linkage autocorrelation cause bias relational feature selection 
machine learning proceedings nineteenth international conference 
morgan kaufmann 
jensen neville hay 
avoiding bias aggregating relational data degree disparity 
proc 
th intl joint conf 
machine learning appear 
kleinberg 

authoritative sources hyper linked environment 
journal acm 
lafferty mccallum pereira 
conditional random fields probabilistic models segmenting labeling sequence data 
proceedings icml 
mccallum nigam rennie seymore 
machine learning approach building domain specific search engines 
proceedings th international joint conference artificial intelligence pp 

murphy weiss jordan 
loopy belief propagation approximate inference empirical study 
proceedings th annual conference uncertainty artificial intelligence 
neal 

probabilistic inference markov chain monte carlo methods 
tech report crg tr dept computer science university toronto 
neville jensen 
iterative classification relational data 
aaai workshop learning statistical models relational data 
neville jensen hay 
learning relational probability trees 
proceedings th international conference knowledge discovery data mining appear 
taskar koller 
discriminative probabilistic models relational data 
proceedings uai 
