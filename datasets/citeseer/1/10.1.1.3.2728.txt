affect sensitive multimodal human computer interaction maja pantic member ieee leon rothkrantz invited ability recognize affective states person communicating core emotional intelligence 
emotional indispensable important successful interpersonal social interaction 
argues generation human computer interaction hci designs need include essence emotional intelligence ability recognize user affective states order human effective efficient 
affective arousal modulates nonverbal communicative cues facial expressions body movements vocal physiological reactions 
face face interaction humans detect interpret interactive signals communicator little effort 
design development automated system accomplishes tasks difficult 
surveys past solving problems computer provides set recommendations developing part intelligent multimodal hci automatic personalized analyzer user nonverbal affective feedback 
keywords affective computing affective states automatic analysis nonverbal communicative cues human computer interaction hci multimodal human computer interaction personalized human computer interaction 
exploration human beings react world interact remains greatest scientific challenges 
perceiving learning adapting world commonly labeled intelligent behavior 
mean intelligent 
iq measure human intelligence best predictor somebody success life 
growing research fields neuroscience psychology cognitive science argues common view intelligence narrow ignoring crucial range abilities manuscript received october revised march 
pantic supported netherlands organization scientific research nwo ew 
authors delft university technology data knowledge systems group department aj delft netherlands mail pantic cs tudelft nl rothkrantz cs tudelft nl 
digital object identifier ieee matter immensely life 
range abilities called emotional intelligence includes ability express recognize affective states coupled ability regulate employ constructive purpose handle affective arousal 
skills emotional intelligence argued better predictor iq measuring aspects success life especially interpersonal communication learning adapting important 
comes world computers need emotional skills probably need skills humans need 
situations man machine interaction improved having machines capable adapting users information important adapt involves information user affective state 
addition people regard computers social agents face inter face interaction may easy 
human computer interaction hci systems capable sensing responding appropriately user affective feedback perceived natural persuasive trustworthy 
findings advances sensing tracking analyzing animating human nonverbal communicative signals produced surge interest affective computing researchers advanced hci 
intriguing new field focuses computational modeling human perception affective states synthesis animation affective expressions design affect sensitive hci 
step intelligent hci having abilities sense respond appropriately user affective feedback detect interpret affective states shown user automatic way 
focuses surveying past done solving problems providing advanced hci key skills emotional intelligence ability recognize user nonverbal affective feedback 
proceedings ieee vol 
september research motivations research natural human hci various research areas technologies brought bear pertinent issues reap substantial benefits efforts model human perception affective feedback computationally 
instance automatic recognition human affective states important research topic video surveillance 
automatic assessment boredom stress highly valuable situations firm attention crucial tedious task essential aircraft control air traffic control nuclear power plant surveillance simply driving ground vehicle truck train car 
advantage affect sensitive monitoring done computer human observers need perform privacy invading monitoring automated tool provide prompts better performance sensed user affective states 
area benefits accrue efforts computer analysis human affective feedback automatic affect indexing digital visual material 
mechanism detecting scenes frames contain expressions pain rage fear provide valuable tool violent content indexing movies video material digital libraries 
areas machine tools analysis human affective feedback expand enhance research applications include specialized areas professional scientific sectors 
monitoring interpreting affective behavioral cues important lawyers police security agents interested issues concerning deception attitude 
machine analysis human affective states considerable value situations informal interpretations 
facilitate research areas behavioral science studies emotion cognition anthropology studies cross cultural perception production affective states neurology studies dependence emotional abilities impairments brain lesions psychiatry studies schizophrenia reliability sensitivity precision currently problems 
outline examining context affective computing research arisen providing taxonomy pertinent problem domain 
survey past done tackling problems machine detection interpretation human affective states 
type human nonverbal interactive signals conveying messages affective arousal areas receive particular attention facial expression analysis vocal expression analysis 
discuss state art consider challenges opportunities facing researchers affective computing 
ii 
problem domain general agreement machine sensing interpretation human affective feedback quite beneficial manifold research application areas tackling problems easy task 
main problem areas concern 
affective state 
question related psychological issues pertaining nature affective states way affective states described automatic analyzer human affective states 
kinds evidence warrant affective states 
words human communicative signals convey messages affective arousal 
issue shapes choice different modalities integrated automatic analyzer affective feedback 
various kinds evidence combined generate affective states 
question related neurological issues human sensory information fusion shape way multisensory data combined automatic analyzer affective states 
section discusses basic issues problem areas 
begins examining body research literature human perception affective states large comes basic emotions universally recognized 
lack consensus implies selection list affective states recognized computer requires pragmatic choices 
capability human sensory system detection understanding party affective state explained 
meant serve ultimate goal efforts machine sensing understanding human affective feedback basis addressing main issues relevant affect sensitive multimodal hci modalities integrated combined 
psychological issues question humans perceive affective states concern crucial importance researchers affective computing 
ironically growing interest affective computing comes time established wisdom human affective states strongly challenged basic research literature 
hand classic psychological research claims existence basic expressions emotions universally displayed recognized happiness anger sadness surprise disgust fear 
implies apart verbal interactive signals spoken words person dependent nonverbal communicative signals facial expression vocal physiological reactions involved basic emotions displayed recognized cross culturally :10.1.1.103.8364
example known study ekman commonly important evidence universals facial expressions spontaneously displayed specific facial actions signal emotions fear sadness disgust surprise happiness occurred virtually frequency japanese pantic rothkrantz affect sensitive multimodal human computer interaction american subjects response watching emotion inducing films 
hand growing body psychological research strongly challenges classical theory emotion 
russell argues emotion general best characterized terms multidimensional affect space terms small number emotion categories 
experimental design flaws applied classic studies single corpus unnatural stimuli forced choice response format 
classic studies claim basic emotions hardwired sense specific neural structures correspond different emotions 
alternative studies study ortony turner suggest emotions components emotions universally linked certain communicative displays facial expressions 
social argue furthermore emotions socially constructed ways interpreting responding particular classes situations explain genuine feeling affect 
lack consensus nature emotion consensus affective displays labeled named 
key issue contradiction classic studies emphasis emotions product evolution culture dependency comprehension emotion label expression related emotion culture dependent 
details issues debated basic research emotion readers referred 
summary available body basic research literature excessively fragmented provide firm safely presumed employed studies affective computing 
due unresolved debate concerning classic emphasis emotions product evolution hand evidence culture dependent hand grounds assume existence set basic emotions displayed recognized uniformly different cultures 
certain express particular affective state modulating communicative signals way certain particular modulation interactive cues interpreted way independently situation observer 
immediate implication pragmatic choices application user profiled choices regarding selection affective states recognized automatic analyzer human affective feedback 
human performance affective arousal modulates verbal nonverbal communicative signals 
shown furnas difficult anticipate person word choice associated intent highly constrained situations different people choose different words express exactly thing :10.1.1.103.8364
hand usual interpersonal face face interaction people detect interpret nonverbal communicative signals terms affective states expressed communicator little effort 
correct recognition affective state depends factors speaker reveal disguise genuine feelings attention speaker familiarity speaker personality face vocal intonation humans recognize nonverbal affective cues apparent ease 
human sensory system uses multimodal analysis multiple communication channels interpret face face interaction recognize party affective states 
channel communication medium modality sense perceive signals outside world 
communication channels example auditory channel carries speech vocal intonation communicative signals visual channel carries facial expression signals 
senses sight hearing touch examples modalities 
detection party affective states usual face face interaction involves simultaneous usage numerous channels combined activation multiple modalities 
analysis communicator affective feedback highly flexible robust 
failure channel recovered channel message channel explained channel mouth expression interpreted smile seen display sadness time see tears hear 
abilities human sensory system define way expectations multimodal affect sensitive analyzer 
may possible incorporate features human sensory system automated system due complexity phenomenon involves intricate interplay knowledge thoughts language nonverbal behavioral cues affect recognition capability human sensory system serve ultimate goal guide defining design recommendations multimodal analyzer human affective states 
taxonomy problem domain taxonomy automatic human affect analysis problem domain devised considering issues 
channels information corresponding human communicative channels integrated automatic affect analyzer 
data carried multiple channels fused achieve human performance recognizing affective states 
temporal aspects information carried multiple channels handled 
automated human affect analyzers sensitive context operate current situation user 
modalities expect automated human affect analyzers include human interactive modalities sight sound touch analyze nonverbal interactive signals facial expressions vocal expressions body gestures physiological reactions proceedings ieee vol 
september fig 

architecture ideal automatic analyzer human affective feedback 
reported research confirm finding 
visual channel carrying facial expressions auditory channel carrying vocal widely thought important human recognition affective feedback 
listener feels liked disliked depends spoken word vocal utterances facial expressions 
indicates judging affective state people rely body gestures physiological reactions displayed observed person rely mainly facial expressions vocal 
far body gestures concerned body gestures associated exclusively speech 
play secondary role human recognition affective states 
far physiological signals concerned people commonly neglect sense times 
order detect heart rate observer physical contact touch observed person 
research psychophysiology produced firm evidence affective arousal range somatic physiological correlates diameter heart rate skin temperature respiration velocity 
integration tactile channels carrying physiological reactions monitored subject automated human affect analyzer requires wiring subject usually perceived uncomfortable unpleasant 
advent nonintrusive sensors wearable computers opened possibilities invasive physiological sensing problem persists currently available skin sensors fragile accuracy measurements commonly affected hand washing amount gel 
summary automated affect analyzers combine modalities perceiving facial vocal expressions attitudinal states 
optionally provided robust nonintrusive sensory equipment include modality perceiving affective physiological reactions see fig 

multisensory information fusion performance multimodal analyzer human affective feedback influenced different types modalities integrated abstraction level modalities integrated fused technique applied carry multisensory data fusion clearly utmost importance 
goal ensure hci approaches naturalness human human interaction ideal model multisensory information fusion human handling multimodal information flow 
insight modalities sight sound touch combined human human interaction gained neurological studies fusion sensory neurons 
concepts relevant multimodal fusion distinguished 
response multisensory neurons stronger multiple weak input sensory signals single strong signal 
context dependency fusion sensory signals modulated signals received cerebral cortex depending sensed context different combinations sensory signals 
handling sensed context sensory malfunctioning handled fusing sensory observations regard individual fast response necessary attempting recalibrate discordant sensors second look suppressing discordant recombining functioning sensors observation contradictory 
humans simultaneously employ tightly coupled modalities sight sound touch 
result analysis perceived information highly robust flexible 
studies confirmed tight coupling different modalities persists modalities multimodal hci 
question remains tight coupling multiple modalities achieved theoretical computational apparatus developed field sensory data fusion 
illustrated fig 
fusion multisensory data accomplished levels data feature decision level 
data level fusion involves integration raw sensory observations accomplished observations type 
monitored human interactive signals different nature sensed different types sensors data level fusion principle applicable multimodal hci 
feature level fusion assumes stream sensory information analyzed features detected features fused 
feature level fusion retains detailed information data level fusion prone noise sensor failures importantly appropriate type fusion tightly coupled synchronized modalities 
feature level techniques kalman fusion artificial neural networks ann fusion hidden markov models hmm fusion proposed decision level interpretation level fusion approach applied multimodal hci 
practice may follow experimental studies shown late integration approach decision level fusion pantic rothkrantz affect sensitive multimodal human computer interaction fig 

fusion multiple sensing modalities 
data level fusion integrates raw sensory data 
feature level fusion combines features individual modalities 
decision level fusion combines data different modalities analysis 
provide higher recognition scores early integration approach 
differences time scale features different modalities lack common metric level modalities add underlying inference features different modalities sufficiently correlated fused feature level 
certainly incorrect decision level fusion people display audio visual tactile interactive signals complementary redundant manner multimodal analysis human interactive signals acquired multiple sensors resembles human processing mutually independent combined feature space context dependent model 
temporal information observation channel general carries information wide range time scales 
longest time scale static signals structure fat deposits metabolism phonetic peculiarities accent 
signals provide number social cues essential interpersonal communication everyday life 
mediate person identification gender provide clues person origin health 
shorter time scales rapid behavioral signals temporal changes physiological activity milliseconds blink minutes respiration rate hours sitting 
types messages communicated rapid behavioral signals affective attitudinal states joy frustration culture specific communicators thumbs manipulators actions act objects environment self manipulative actions scratching lip biting actions accompanying speech finger pointing raised eyebrows regulators conversational mediators exchange look palm pointing head nods smiles 
general rapid behavioral signals recognized ms video frames ms audio frames 
ability discriminate subtle affective expressions requires comparison time 
changes human behavior observed time instance may misinterpreted temporal pattern changes taken account 
example rapid eyebrows denoting difficulty understanding discussed matters misinterpreted expression anger temporal pattern behavioral changes indicating discussed subject taken account 
addition performing time instance time scale analysis information carried multiple observation channels extremely useful handling sensory ambiguities general 
assumption bears existence certain grammar actions physiological reactions certain subclass actions reactions respect currently encountered action reaction time instance previously observed actions reactions time scale plausible 
considering previously observed affective states time scale current data proceedings ieee vol 
september carried functioning observation channels time instance statistical prediction derived current affective state information lost due malfunctioning inaccuracy particular sensor 
context dependency rapid behavioral signals subdivided classes reflex actions control afferent input backward pull hand source heat scratching eyes facing sun rudimentary reflex impulsive actions appear controlled innate motor programs accompany affective expressions wide open eyes encountering unexpected situation differentiated information processing orienting space adaptable versatile culturally individually variable spontaneous actions appear mediated learned motor programs form firm part affective expressions smile greeting joke raised eyebrows wonder malleable culturally individually variable intentional actions controlled learned motor programs form part affective expressions uttering spoken message raising eyebrows shaking hands get acquainted tapping shoulder friend welcome 
rapid behavioral signals demand relatively little person information processing capacity free deliberate control demand lot processing capacity consciously controlled governed complex culturally specific rules interpersonal communication 
rapid behavioral signals belong exclusively class scratching may belong classes eyes 
crucial determine class shown behavioral signal belongs influences interpretation observed signal 
instance eyes may interpreted sensitivity eyes action reflex expression hate action displayed unintentionally friendly anger friendly action displayed intentionally 
determine class observed rapid behavioral signal interpret terms affective attitudinal states know context observed signal displayed 
words necessary know interpretation observed behavioral signals associates signals situation current environment task 
ideal human affect analyzer summary conceive ideal automatic analyzer human nonverbal affective feedback able emulate capabilities human sensory system turn see fig 
multimodal modalities facial expressions vocal physiological reactions robust accurate despite auditory noise sensors occlusions changes viewing lighting conditions generic independent variability subjects sex age ethnicity sensitive dynamics time evolution displayed affective expressions performing time instance time scale analysis sensed data previously combined multisensory feature level data fusion context sensitive performing application data interpretation terms user profiled affect attitude interpretation labels 
iii 
state art section survey current state art machine analysis human affective feedback problem domain 
presenting exhaustive survey section focuses efforts proposed literature reviewed greatest impact community measured coverage problem domain citations received testing 
relatively existing works combine different modalities single system human affective state analysis 
examples works chen de silva ng investigated effects combined detection facial vocal expressions affective states 
virtually existing studies treat various human affective cues separately approaches automatic single modal analysis human affective feedback 
tactile computer sensing modality natural hci explored increasing interest research psychophysiology produced firm evidence affective arousal range somatic physiological correlates single aimed automatic analysis affective physiological signals existing body literature picard 
lack interest research topic part lack interest research sponsors part manifold related theoretical practical open problems 
pertinent problematic issues concern application haptic technology profound impact users fatigue done improperly currently available wearable sensors physiological reactions imply wiring subject usually experienced uncomfortable skin sensors fragile measuring accuracy easily affected 
picard concerns automatic recognition user defined affective states neutral anger hate grief platonic love romantic love joy set sensed physiological signals 
data collected period days actress intentionally expressing affective states daily sessions data obtained days experiments 
physiological signals recorded jaw coding muscular tension jaw blood volume pressure bvp skin conductivity respiration heart rate calculated bvp 
total pantic rothkrantz affect sensitive multimodal human computer interaction features statistical features raw signal calculated statistical features mean standard deviation features mean slope skin conductivity heart rate change power spectral density characteristics respiration signal 
emotional classification algorithm combining sequential floating forward search fisher projection achieves average correct recognition rate 
reported picard features extracted raw physiological signals highly dependent day signals recorded 
signals recorded short min sessions 
survey divided parts 
part dedicated done automation human affect analysis face images second part explores compares automatic systems recognition human affective states audio signal 
third part survey focuses past efforts automatic bimodal analysis human affective feedback facial vocal expressions 
automatic affect recognition face images major impulse investigate automatic facial expression analysis comes significant role face emotional social lives 
face provides conversational interactive signals clarify current focus attention regulate interactions surrounding environment persons vicinity 
noted previously facial displays direct naturally means communicating emotions 
automatic analyzers subtle facial changes natural place various vision systems including automated tools psychological research lip reading videoconferencing animation synthesis life agents human behavior aware generation interfaces 
wide range principle driving applications caused interest research problems machine analysis facial expressions 
exhaustive surveys pertinent problem domain readers referred samal iyengar overview early works tian review techniques detecting micro facial actions action units aus pantic rothkrantz survey current efforts :10.1.1.39.8427
problem machine recognition human affective states images faces includes subproblem areas finding faces detecting facial features classifying data affect categories 
problem finding faces viewed segmentation problem machine vision detection problem pattern recognition 
possible strategies face detection vary lot depending type input images 
existing systems facial expression analysis process static face images face image sequences 
words current studies assume general presence face scene ensured 
posed portraits faces uniform background illumination constitute input data processed majority current systems 
instances systems utilize camera mounted subject head proposed ohya pantic correctness assumption 
progress development vision systems robust face detection arbitrary scenes works reported presently existing systems facial affect recognition perform automatic face detection arbitrary scene :10.1.1.40.359
problem facial feature extraction input images may divided dimensions 
features extracted automatic way 
temporal information image sequence 
features holistic spanning face analytic spanning subparts face 
features view volume dimensional dimensional 
affect analysis facial images directed automatic static analytic facial feature extraction 
proposed systems extract facial information automatic way see table 
techniques facial affect classification employed systems relevant goals systems limited machine human affect analysis analyses human interactive signals fully automatic preferably achieved real time obtain fluent tight efficient hci 
approaches automatic facial data extraction utilized existing systems include analyses facial motion see table holistic spatial pattern analytic spatial pattern :10.1.1.40.359
instances strong assumptions problem facial feature detection tractable images contain portraits faces facial hair glasses illumination constant subjects young ethnicity 
current systems deal rigid head motions examples systems proposed hong method essa pentland handle distractions facial hair beard glasses :10.1.1.40.359
automated facial affect analyzers proposed literature date fills missing parts observed face perceives face part occluded hand object 
generated automated facial expression analyzer affected input data certainty system proposed pantic existing system automatic facial expression analysis calculates output data certainty input data certainty 
eventually automated facial affect analyzers terminate execution translating extracted facial features description shown affective state 
description identical close human description examined facial affect 
explained section ii human interpretation facial affective feedback depends context pertinent affective feedback observed dynamics time evolution displayed affective expressions 
order proceedings ieee vol 
september table properties proposed approaches automatic affect recognition face images achieve human interpretation shown facial affect pragmatic choices application user task profiled time scale dependent choices regarding selection moods affective attitudinal states recognized facial affect analyzer 
instance intended application monitoring nuclear power plant operator facial affect analyzer deployed probably aimed discerning stress 
addi tion facial affect display interpretation rules differ culture culture may differ person person 
interpretation user facial affect strongly dependent affect labels pertinent user associates patterns facial behavior 
example nuclear power plant operator distinguishes frustration stress panic variations generic category stress facial affect analyzer deployed pantic rothkrantz affect sensitive multimodal human computer interaction surveillance affect attitude mood adapt interpretation categories 
automated system proposed pantic classifies facial expressions multiple quantified user defined interpretation classes existing facial expression analyzers perform facial expression classification number basic emotion categories 
classification techniques existing systems include template classification static images template classification image sequences fuzzy rule classification static images fuzzy rule classification image sequences ann classification hmm classification bayesian classification :10.1.1.40.359
mentioned previously shown facial expression may misinterpreted current task user taken account 
example may displayed speaker emphasize difficulty currently discussed problem may shown listener denote understand problem issue 
existing facial affect analyzers perform task dependent interpretation shown facial behavior 
timing dynamics facial expressions critical factor interpretation facial affect 
current systems facial affect recognition analyze extracted facial information different time scales 
proposed interframe analyses handle problem partial data achieve detection facial expression time markers onset apex offset short time scale 
consequently automatic recognition expressed mood attitude longer time scales range current facial affect analyzers 
table summarizes features existing systems facial affect recognition respect issues 
input image provided automatically 
presence face assumed 
performance independent variability subjects sex age ethnicity 
variations lighting handled 
rigid head movements handled 
distractions glasses facial hair handled 
face detected automatically 
facial features extracted automatically 
inaccurate input data handled 
data uncertainty propagated facial information analysis process 
facial expression interpreted automatically 
interpretation categories labels defined 
interpretation labels user profiled 
multiple interpretation labels scored time 
interpretation labels quantified 
input processed real time 
stands stands represents missing entry 
missing entry means matter issue reported pertinent matter applicable system question 
inapplicable issues instance issues dealing variations lighting rigid head movements inaccurate facial information cases input data hand measured 
value column indicates unknown system question handle images arbitrary subject usually consequence fact pertinent system tested images unknown subjects images subjects different ethnicity 
value column indicates surveyed system handle images subjects previously trained 
value column indicates system question limited predefined rigid number interpretation categories dynamic sense new user defined interpretation labels learned experience 
summary current facial affect machine analysis research largely focused attempts recognize small set posed facial expressions basic emotions portraits faces nearly frontal view face image sequences illumination 
humans detect basic emotional facial expressions accuracy ranging significant automated systems achieve accuracy detecting emotions deliberately displayed subjects 
interesting point conclude system achieving average recognition rate performs better system attaining average recognition rate detecting basic emotions face images 
spite repeated need readily accessible set images image sequences provide basis benchmarks efforts automatic facial affect analysis database images exists shared diverse facial expression research communities 
general isolated pieces facial database exist exploited particular facial research community 
best knowledge example facial database facial research community unpublished database ekman hager facial action exemplars 
bartlett donato tian train test methods detecting facial micro actions facial muscle actions face image sequences :10.1.1.39.8427
facial database publicly available tian cohn kanade au coded face expression image database 
database ekman hager facial action exemplars basis benchmarks efforts research area facial micro action detection face image sequences 
proceedings ieee vol 
september table speech correlates basic emotions happiness anger fear sadness auditory variables commonly reported psycholinguistic research studies included glaring lack common testing resource forms major impediment comparing resolving extending issues concerned automatic facial expression analysis understanding represents main incentive avoid labeling surveyed systems better 
believe defined validated commonly database images faces motion necessary prerequisite ranking performances proposed facial affect analyzers objective manner 
benchmark database established reader left rank surveyed systems priorities properties systems summarized table 
automatic affect recognition audio signal auditory aspect communicative message carries various kinds information 
consider verbal part strings words regarding manner spoken important aspects pertinent utterance misunderstand spoken message attending nonverbal aspect speech 
contrast spoken language processing witnessed significant advances decade processing emotional speech widely explored auditory research community 
data show accuracy automated speech recognition neutrally spoken words tends drop concerns emotional speech 
shown case automatic speaker verification systems 
findings triggered efforts automating human affect recognition speech signal researchers field focused synthesis emotional speech 
problem vocal affect analysis includes subproblem areas specifying auditory features estimated input audio signal classifying extracted data affect categories 
research psychology psycholinguistics provides immense body results acoustic prosodic features encode affective states speaker 
table lists speech correlates archetypal emotions happiness anger fear sadness commonly reported studies 
speech measures reliable indicators basic emotions continuous acoustic measures particularly pitch related measures range mean median variability intensity duration 
works automatic affect sensitive analysis vocal expressions literature date commonly finding 
auditory features usually estimated input audio signal see table pitch fundamental frequency acoustic signal delimited rate vocal cords intensity vocal energy speech rate number words spoken time interval words identified time varying spectra harmonics generated vocal cord vibrations filtered pass mouth nose pitch contour pitch variations described terms geometric patterns phonetic features features deal types sounds involved speech vowels consonants pronunciation 
mentioned previously order accomplish human interpretation perceived vocal affective feedback pragmatic choices application user task profiled time scale dependent choices regarding selection affective attitudinal states moods recognized vocal affect analyzer 
existing automated systems auditory analysis human affective feedback perform context sensitive analysis application user task dependent analysis input audio signal 
analyze extracted vocal expression information different time scales 
proposed interframe analyses detection suprasegmental features pitch intensity duration syllable word sentence detection phonetic features 
computer recognition moods attitudes longer time scales input audio signal remains significant research challenge 
virtually existing automatic vocal affect analysis performs singular classification input audio signals emotion categories anger irony happiness sadness grief fear pantic rothkrantz affect sensitive multimodal human computer interaction table properties proposed approaches automatic affect recognition audio signal disgust surprise 
utilized classification techniques include anns hmms gaussian mixture density models fuzzy membership indexing maximum likelihood bayes classifiers 
table summarizes properties current systems vocal affect analysis respect 
spoken input samples handled 
performance independent variability subjects sex age 
auditory features extracted automatically 
pitch related variables utilized 
vocal energy intensity utilized 
speech rate utilized 
pitch contours utilized 
phonetic features utilized 
auditory features utilized 
inaccurate input data handled 
extracted vocal expression information interpreted automatically 
interpretation categories labels defined 
interpretation labels scored context sensitive manner application user task profiled manner 
multiple interpretation labels scored time 
interpretation labels quantified 
input processed real time 
general people recognize emotion neutral content speech accuracy choosing basic affective states 
automated vocal affect analyzers match accuracy recognizing emotions deliberately expressed subjects recorded pronouncing sentences having length words 
instances strong assumptions problem automating vocal expression analysis tractable 
example recordings noise free recorded sentences short delimited pauses carefully pronounced actors express required affective state 
test data sets small words short sentences spoken subjects containing exaggerated vocal expressions affective states 
similarly case automatic facial affect analysis readily accessible set speech material exists provide basis benchmarks efforts automatic vocal affect analysis 
summary state art automatic affective state recognition speech similar speech recognition decades ago computers classify carefully pronounced digits spoken pauses accurately detect digits spoken way previously encountered forming part longer continuous conversation 
proceedings ieee vol 
september automatic bimodal affect recognition today total reported studies bimodal audiovisual interpretation human affective feedback 
works chen de silva ng chen huang 
chen proposed rule method singular classification input audiovisual data emotion categories happiness sadness fear anger surprise dislike 
input data utilized chen video clips spanish speaker video clips speaker 
speakers asked portray emotions times vocal facial expressions 
data set chen defined rules classification acoustic facial features pertinent emotion categories 
speech signals pitch intensity pitch contours estimated acoustic features 
facial features lowering rising eyebrows opening eyes stretching mouth presence furrow wrinkles manually measured input images 
rules emotion classification evaluated mentioned data set 
known rules suitable emotion recognition audiovisual data unknown subject 
quantification recognition results obtained method reported 
clear picture actual performance method obtained research efforts 
de silva ng proposed rule method singular classification input audiovisual data emotion categories chen input data utilized de silva ng long video clips english speakers 
speaker asked portray emotion category displaying related facial expression speaking single english word choice 
pertinent audio visual material processed separately 
optical flow method proposed detect displacement velocity facial features mouth corners top bottom mouth inner corners eyebrows 
speech signals pitch pitch contours estimated method proposed 
nearest neighbor method classify extracted facial features hmm method classify estimated acoustic features emotion categories 
subject results classifications plotted graph 
resulting graphs de silva ng defined rules emotion classification input audiovisual material 
reported correct recognition rate reduced input data set input samples utilized rules yield classification emotion categories excluded data set 
known precision method de silva ng emotion classification similar audiovisual data obtained recording unknown subject 
method proposed represents hybrid approach singular classification input audiovisual data basic emotion categories happiness sadness anger surprise neutral 
utilized video clips female japanese professional announcer 
asked pronounce japanese name emotions times 
input audiovisual material processed follows 
speech signals pitch intensity pitch contours estimated acoustic features 
features classified emotion categories applying hmm method 
utilized visible rays vr camera infrared ir camera obtain ordinary thermal face images respectively 
vr ir part input sample vr corresponding ir images utilized processing 
images correspond points intensity speech signal maximal syllables ta ro respectively 
typical regions interest mouth region eyebrow eye region extracted selected images separately 
image segment compared relevant neutral image segment order generate differential image 
vr ir differential images discrete cosine transformation applied yield vr ir feature vector respectively 
ann approach classify feature vectors emotion categories 
classification obtained speech signal summed decide final output category 
reported correct recognition rate reduced input data set input samples proposed method yield classification emotion categories excluded data set 
similarly works reported chen de silva ng known precision method emotion classification audiovisual data unknown subject 
chen huang proposed set methods singular classification input audiovisual data basic emotion categories happiness sadness disgust fear anger surprise 
utilized video sequences samples long 
data subjects displayed basic emotions times producing appropriate facial expression right speaking sentence appropriate vocal emotion 
single emotion sequences started ended neutral expression 
facial motion tracking chen huang utilized tao huang algorithm piecewise bezier volume deformation model 
facial mesh model embedded multiple bezier volumes constructed manual selection landmark facial feature points video frame frontal view neutral facial expression 
adjacent pair frames motion vectors multiple mesh nodal points estimated multiresolution template matching method 
alleviate shifting problem templates previous frame 
motion vectors rigid head mo pantic rothkrantz affect sensitive multimodal human computer interaction tions nonrigid facial motions computed squares estimator 
algorithm utilized set predefined basic facial movements facial muscle actions describe motions mouth eyes eyebrows 
final output algorithm vector containing strengths facial muscle actions 
classifier sparse network naive bayes output nodes classify vector emotion categories 
speech signals pitch intensity computed get command speech rate recursive convex hull algorithm 
features classified emotion categories modeled gaussian distribution 
utilized video clips pure facial expression occurs right sentence spoken appropriate vocal emotion chen huang applied single modal methods described sequential manner 
performed types experiments person dependent person independent experiments 
person dependent experiments half available data training data half test data 
average recognition rate achieved experiment 
person independent experiments data subjects training data data remaining subject test data 
average recognition rate reported experiment 
brief existing works human affect analysis bimodal data assume general clean audiovisual input noise free recordings closely placed microphone portraits actor speaking single word displaying exaggerated facial expressions basic emotions 
audio image processing techniques systems relevant goals systems need improvements multimodal context sensitive hci clean input known actor announcer expected context independent data interpretation suffice 
iv 
challenges opportunities limitations existing systems human affect recognition probably best place start discussion challenges opportunities researchers affective computing face 
issue strikes surprises advances video audio processing bimodal audiovisual analysis human affective feedback tractable agreed solving problem extremely useful merely efforts aimed actual implementation bimodal human affect analyzer reported date section iii 
record research endeavor inclusion nonverbal modalities single system affect sensitive analysis human behavior 
problem achieving deeper integration detached visual auditory tactile research communities number related additional issues 
visual input acquisition video input machine analysis human affective states concerns detection monitored subject face observed scene 
problematic issue typical visual sensing including gaze lip movement facial gesture tracking scale resolution pose occlusion 
real life situations assumed subject remain rigid head body movements expected causing changes viewing angle visibility illumination tracked facial features 
highly time consuming scale problem solved proposed soulie forming multiresolution representation input image frame performing detection procedure different resolutions 
high spatial resolution original input image frame necessary discrimination subtle facial changes achieved 
standard ntsc pal video camera provides image digitized measures approximately pixels respectively 
images pixels form lower limit detection face expression human observer may sufficient typical detection tracking facial expressions arrange camera setting pixels width subject face 
field view half times width face 
camera setting sufficient machine recognition human facial affect subject seated free move head 
hand camera setting may sufficient pertinent task subject free walk front camera approach move away camera 
highly beneficial develop strategies extending field regard maintaining high resolution 
investigation development strategies subject research field active vision 
general terms objective active camera control focus sensing resources relatively small regions scene contain critical information 
words aim active vision systems observe scene wide field view low spatial resolution determine direct high spatial resolution observations 
analogous human vision fovea small depression retina vision acute 
fovea provides resolution needed discriminating patterns interest periphery provides broad area monitoring altering gaze control 
field active vision extended purposes face detection tracking certain field automatic facial affect analysis affective computing general highly benefit progress area research 
pose occlusion difficult problems initially thought intractable hardest solve 
significant progress methods monitored object representation orientations employing data acquired multiple cameras 
methods currently thought provide promising proceedings ieee vol 
september solution problems pose occlusion 
extensive review methods utilized video surveillance reader referred 
addition interesting progress statistical methods essentially try predict appearance monitored objects image information available 
explained section ii statistical facial expression predictor task user profiled temporal grammar human facial behavior prove purposes handling partial data machine human affect analysis images faces 
standard visual processing problems cumbersome issue typical face image processing universality employed technique detection face features 
employed detection method prone variability current looks monitored subjects 
explained section ii ideal automated system facial affect recognition perform generic analysis sensed facial information independently possibly static facial signals facial hair slow facial signals wrinkles artificial facial signals glasses makeup 
essa pentland proposed method :10.1.1.40.359
audio input mentioned previously virtually done automating vocal affect analysis assumes fixed listening position closely placed microphone actors radio noise free recordings short neutral content sentences delimited pauses carefully pronounced express required affective state 
clean audio input realistic expect especially unconstrained environments human affect analyzers deployed multimodal context sensitive hci ubiquitous computing 
obvious necessary extension current research automatic vocal affect analysis extend range test material include speech samples naturally spoken read actors meaningful semantically neutral content came late time include exclude continuous speech drawn genuine range speakers terms sex age smoking pattern social background range languages 
extension range test material provide solution original problem place acquisition currently heavily constrained speech material 
processing interpreting unconstrained audio input minimal degradation performance remains significant research challenge facing speech understanding research field affective computing research field 
far area affective computing concerned basic problem needs solved defining better computational mapping affective states speech patterns 
specifically necessary find features extracted computer time may discerning affective states constrained speech material 
psycholinguistic research results consistent general neutral speech continuous acoustic correlates basic emotions anger fear happiness sadness see table 
research small set emotions contradictory reports 
example disagreement duration facets anger fear happiness researchers report faster speech rate report slower speech rate see table 
furthermore researchers adhere standpoint features solely acoustic different phonetic features speech understanding adhere standpoint acoustic phonetic features tightly combined uttering speech 
standpoint assumption impossible express recognize vocal expressions affective states considering acoustic features 
number fundamental reasons adhere standpoint 
best known example features signal affective arousal easy confound features determined linguistic rules involves questions 
uttering question gives rise distinctive pitch contours easily taken evidence affective inflection linguistic context ignored 
similar examples turn topic 
reason considering linguistic content connection detection affective arousal concerns dependence performance understood linguistic content 
shown de silva observers speak language recognized different emotions correctly spoken speech average 
research efforts inclusion phonetic features vocal affect analysis reported 
interesting observation information encoded speech signal far meaningful pitch intensity observed duration syllable word phrase 
robust detection boundaries different levels poses significant research challenge speech processing recognition connected speech far recognition discrete words large body literature source help tackling problems locating pauses phrases detecting boundaries words phonemes 
summary establishing reliable knowledge set suitable features sufficient discriminating affective state archetypal unconstrained audio input lags distant 
order approach goal research temporal analysis acoustic phonetic characteristics spontaneous affective speech needed 
large gaps related research headings works surveyed table indicate detachment existing auditory affect research communities 
raise quality research field deeper integration currently detached research communities necessary 
pantic rothkrantz affect sensitive multimodal human computer interaction multimodal input ideal analyzer human nonverbal affective feedback see fig 
generate reliable result multiple input signals acquired different sensors 
number related issues pose interesting significant research challenges 
consider state art audio visual tactile processing noisy partial input data expected 
ideal analyzer human nonverbal affective feedback able deal imperfect data generate certainty associated varies accordance input data 
way achieving consider time instance versus time scale dimension human nonverbal communicative signals 
explained section ii certain grammar actions physiological reactions 
considering previously observed actions reactions time scale respect current data carried functioning observation channels time instance statistical prediction probability derived information lost due malfunctioning inaccuracy particular sensor currently displayed action reaction 
generative probability models hmm provide principled way handling temporal treating missing information 
alternative exploit discriminative methods support vector machines kernel methods classification performance superior generative classifiers combination generative probability models extracting features discriminative classification 
temporal analysis involves grammar human behavior represents unexplored topic psychological sociological research areas 
issue problem difficult solve general case dependency person behavior personality cultural social vicinity current mood context observed behavioral cues encountered 
source help problems machine learning priori rules interpret human behavior potentially learn application user context dependent rules watching user behavior sensed context 
context sensing time needed learn appropriate rules significant problems right benefits accrue adaptive affect sensitive hci tool section 
typical issue multimodal data processing multisensory data processed separately combined section iii 
certainly incorrect people display audio visual tactile communicative signals complementary redundant manner 
chen proven experimentally case audio visual input 
order accomplish human multimodal analysis multiple input signals acquired different sensors signals considered mutually independent combined context free manner intended analysis section ii 
input data processed joint feature space context dependent model 
practice acute problems context sensing developing context dependent models combining multisensory information additional major difficulties size required joint feature space usually huge results heavy computational burden different feature formats timing 
potential way achieve target tightly coupled multisensory data fusion develop context dependent versions suitable method bayesian inference method proposed 
affect sensitive interpretation multimodal input explained section ii accomplishment human interpretation sensed human affective feedback requires pragmatic choices application task profiled choices 
noted currently existing methods aimed automation human affect analysis context sensitive see tables 
initially thought research topic hardest solve context sensing answering questions user doing witnessed number significant advances 
complexity wide ranging problem problem context sensitive human affect analysis significant research challenges facing researchers affective computing 
issue concerns actual interpretation human nonverbal interactive signals terms affective attitudinal states 
existing employs usually singular classification input data basic emotion categories section iii 
approach limitations 
mentioned previously theory existence universal emotion categories nowadays strongly challenged psychological research area section ii 
pure expressions basic emotions seldom elicited time people show blends emotional displays 
classification human nonverbal affective feedback single basic emotion category realistic 
automatic analyzers sensed nonverbal affective cues realize quantified classification multiple emotion categories proposed example automatic facial affect analysis automatic vocal affect analysis 
nonverbal affective cues classified combination basic emotion categories 
think instance frustration stress skepticism boredom attitudinal states 
shown comprehension emotion label ways expressing related affective state may differ culture culture person person section ii 
definition interpretation categories set displayed human nonverbal affective cues classified key challenge design realistic affect sensitive monitoring tools 
lack psychological scrutiny topic problem harder 
source help machine learning integrating rigid generic rules interpretation proceedings ieee vol 
september human nonverbal affective feedback intended tool system potentially learn expertise allowing user define context dependent interpretation categories proposed 
validation issues general validation studies automated system address question developed system complying predefined set requirements 
automated analyzers human affective feedback usually envisioned machine tools sensing analyzing translating human communicative cues description expressed affective state 
description identical close human description pertinent affective state 
validation studies automated analyzers human affective feedback address commonly question interpretations reached automatically equal human observers judging stimulus material 
turn evaluating performance automated human affect analyzer involves obtaining set test material coded human observers ground truth terms affective states shown pertinent material 
order enable comparing resolving extending issues concerned automatic human affective feedback analysis set test material standard commonly database test material 
readily accessible database test material basis benchmarks efforts research area automated human affective feedback analysis established 
fact research facial affect analysis attracted interest researchers hot topics machine vision artificial intelligence ai research glaring lack existing benchmark facial database 
lack common testing resources forms major impediment comparing resolving extending issues concerned automatic human affective feedback analysis understanding 
slowed progress applying computers analyze human facial vocal affective feedback cooperation collaboration investigators affective computing 
benefits accrue commonly audiovisual database human affect expressions numerous 
avoiding redundant collection facial vocal expression exemplars reduce research costs investigators exemplars 
having centralized repository retrieval exchange audio visual training test material improve research efficiency 
maintaining various test results obtained audio visual data set providing basis benchmarks research efforts increase research quality 
reduce currently existing abundance reports presenting insignificant achievements 
establishment readily accessible benchmark audiovisual database human affect expressions acute problem needs resolved fruitful avenues new research affective computing opened 
number issues problem complex turn difficult tackle 
objects included audiovisual database meets multiple needs scientists working field 
facial expression important variable large number studies computer science lipreading audiovisual speech face synthesis see section research behavioral science psychology psychophysiology anthropology neurology psychiatry 
motion records necessary studying temporal dynamics facial behavior static images important obtaining configurational information facial expressions 
benchmark database include motion images faces 
relevance evaluating achievements tackling past challenges automatic human affect analysis images partially occluded faces various poses acquired various lighting conditions included 
order support research efforts directed specific research goals integration research efforts done various fields including lipreading facial vocal audiovisual human affect recognition database include video recordings silent subjects audio recordings speech vocalizations audiovisual recordings speaking subjects 
important variable distinction deliberate actions performed request versus spontaneous actions volitional control 
examples categories included database order study essential question difference expressions 
examples emotional expressions defined classic studies emotion available 
facilitate experiments directed alternative approaches human affect expression interpretation user profiled interpretation expressions components expressions included database 
crucial aspect benchmark database human affect expressions metadata associated database object ground truth validating automated human affect analyzers 
general relevance images scored terms facial muscle actions facial aus defined ekman friesen facs system standard measure activity face 
interpretations displayed facial vocal expressions terms affective state associated database object 
spontaneous expressions associated metadata identify conditions expression elicited 
provision important eliciting circumstances produce different types expressions conversation versus listening watching stimulus material 
pantic rothkrantz affect sensitive multimodal human computer interaction objects related metadata collected inclusion benchmark audiovisual database 
technical considerations database resolved including criteria sensor data rates field sensing spatial resolution frame rate data formats compression methods 
choice enable sharing database different research communities world 
database objects represent number demographic variables including ethnic background gender age provide basis generality research findings 
category database objects audio visual recordings individuals included order avoid effects unique properties particular people 
acquisition deliberate actions performed request recorded subjects experts production facial vocal expressions individuals having formal training facs behavioral scientists individuals instructed experts perform required expressions 
spontaneous expressions decision acquire kind data 
problematic issue hidden recordings promise acquisition truly spontaneous behavior people usual environment privacy 
large number expressions included database provision individual researchers add research material database 
secure handling additions facilitated 
automatic control addition matches specified technical formats defined database objects realized 
number issues taken account generating metadata associated database objects 
humans detect basic emotional expressions face images neutral content speech accuracy ranging 
human observers may disagree judgments may mistakes occasionally 
validation studies automated analyzers human affective feedback address commonly question interpretations reached automatically equal human observers judging stimulus material validation automated system sound validity metadata associated test samples 
necessary consensus experts involved generating metadata associated database objects excessive 
means easily achievable goal 
extremely difficult ascertain human experts involved generating metadata sufficiently concentrated task 
second facial vocal affect analyses tasks yield inconsistencies judgments single human observer successive analyses stimulus material 
source help problems usage accurate means recognizing facial vocal expressions measures muscular electrical activity double check human visual auditory judgments stimulus material 
involves wiring subjects turn results visual occlusions recorded facial expressions 
available means addressing problems involvement human experts task generating metadata associated database objects 
implies occupying valuable time trained human observers longer period turn trigger behavioral science research communities participate process establishing subject audiovisual benchmark database 
construct administer large audiovisual database 
facilitate efficient fast secure retrieval inclusion objects constituting database 
benchmark audiovisual database envisioned section valuable hundreds thousands researchers various scientific areas easy access 
relaxed level security allows user quick access database frees database administrators time consuming identity checks attain easy access 
case journalists hackers able access database 
database contain images available certain authorized users images psychiatric patients emotional disorders comprehensive security strategy 
example mandatory multilevel access control model users get rights database objects various security levels confidential internal security 
usage access control model implies database easily accessible easily usable 
best strategy encourage primary researchers include database just recordings imagery speech samples restriction allow relaxed level access control just described 
important issue problem secure inclusion objects database 
purpose procedures determining recording added matches specified technical formats defined database objects need developed 
important issues resolved involve questions performance tested automated system included database 
relationship performance database objects pertinent evaluation defined 
fast reliable object distribution networks achieved 
summary development readily accessible benchmark audiovisual database meets multiple needs scientists working field involves questions need answered 
believe questions appropriately answered interdisciplinary team computer vision experts spoken language processing experts database designers behavioral scientists set investigate pertinent aspects 
proceedings ieee vol 
september remarked scientists nass pentland multimodal context sensitive user task application profiled affect sensitive hci single widespread research topic ai research community 
breakthroughs hci designs bring radical change computing world change professionals practice computing mass consumers conceive interact technology 
aspects new generation hci technology particular ones concerned interpretation human behavior deeper level mature need improvements 
current approaches computer analysis human affective feedback follows see tables single modal audiovisual analyzers discussed section iii information processed computer system limited acquired face images recorded speech signal context insensitive attention paid current user works current task summary fields machine vision audio processing affective computing witnessed significant advances past years realization robust multimodal adaptive context sensitive analyzer human nonverbal affective feedback lies distant 
problems involved integration multiple sensors pertinent modalities model human sensory system lack better understanding individual context dependent human behavior general additional related issues 
issue jeopardize wide deployment new hci technology concerns efficiency pertinent hci tools 
generally thought pervasive computing devices inefficient user train devices separately 
computers know people environment act capable adapting current user minimum explicit instruction 
long term way achieving 
develop multimodal affect sensitive hci tools conforming recommendations provided section iv able monitor human nonverbal behavior adapt current user grammar behavioral actions reactions context doing point current scenario stress sensed nuclear power plant operator reads mail cause alarm 
adaptive tools commercially available users profile context tools 
withdraw trained systems time combine stored knowledge order derive generic statistical rules models interpretation human nonverbal behavior context environment 
people participate privacy large scale project significant problem right approach resolve intriguing questions 
important resolve social impact interaction electronic media effects computing information technology interaction patterns related behavior social cultural profile 
issue jeopardize wide deployment pertinent hci technology concerns design hci tools question 
computer technology especially affect sensitive monitoring tools perceived big brother watching tools 
remarked schneiderman large proportion population fact vision universal computers coming era ubiquitous computing 
actual deployment context sensitive multimodal hci tools proposed attainable design tools invade user privacy pertinent hci tools capacity monitor concentrate information somebody behavior misused cause user worry unemployed production controllers want computer programs cause help performing job faster accurately reduce user professional responsibility insisting intelligent capabilities computing devices negative effects machines poor performance seeing machines devices tools merely empower retrieving processing information faster preferences context act 
multimodal hci tools envisioned enable development smart perceptually aware environments adapt users recognize context act understand feel respond appropriately 
represent coming human natural hci means determining impact information technology social behavior 
recognize realization human centered hci technology lies relatively distant commercialization actual deployment depends user friendliness trustworthiness 
acknowledgment authors prof huang prof anonymous reviewers helpful comments suggestions 
aloimonos weiss bandyopadhyay active vision int 
comput 
vis vol 
pp 

pantic rothkrantz affect sensitive multimodal human computer interaction amir ron automatic classification emotions speech proc 
icslp pp 

anandan computational framework algorithm measurement visual motion int 
comput 
vis vol 
pp 

acquisition emotions social construction emotions ed 
oxford blackwell 
scherer acoustic profiles vocal emotion expression personality social psychol vol 
pp 

bartlett hager ekman sejnowski measuring facial expressions computer image analysis psychophysiology vol 
pp 

emotion recognition role facial movement relative importance upper lower areas face personality social psychol vol 
pp 

characteristics recognizability vocal expression emotions 
dordrecht netherlands 
black yacoob recognizing facial expressions image sequences local parameterized models image motion int 
comput 
vis vol 
pp 

boyle anderson effects visibility dialogue cooperative problem solving task lang 
speech vol 
pp 

bruce human face tells human mind challenges robot human interface proc 
roman pp 

larsen ito psychophysiology emotion handbook emotions lewis jones eds 
new york guilford pp 

campbell green optical retinal factors affecting visual resolution physiol vol 
pp 

cassell bickmore external manifestations trust interface commun 
acm vol 
pp 

chen huang emotional expressions audiovisual human computer interaction proc 
icme pp 

chen huang multimodal human emotion expression recognition proc 
fg pp 

chen rao audio visual integration multimodal communication proc 
ieee vol 
pp 
may 
chen audiovisual speech processing ieee signal processing mag vol 
pp 
jan 
cohen garg lew huang facial expression recognition video sequences proc 
icme pp 

special section video surveillance ieee trans 
pattern anal 
machine intell vol 
pp 
aug 
frey huang embedded face facial expression recognition proc 
icip vol 
pp 

science emotion 
englewood cliffs nj prentice hall 
cottrell metcalfe face emotion gender recognition holons adv 
neural inf 
process 
syst vol 
pp 

cowie douglas cowie taylor emotion recognition human computer interaction ieee signal processing mag vol 
pp 
jan 
darwin expression emotions man animals 
chicago il univ chicago press 
dasarathy sensor fusion potential exploitation innovative architectures illustrative approaches proc 
ieee vol 
pp 
jan 
ed communication emotional meaning 
new york mcgraw hill 
de silva facial emotion recognition multimodal information proc 
pp 

de silva ng bimodal emotion recognition proc 
fg pp 

dellaert polzin waibel recognizing emotion speech proc 
icslp pp 

donato bartlett hager ekman sejnowski classifying facial actions ieee trans 
pattern anal 
machine intell vol 
pp 
oct 
ikeda nakamura recognition facial expressions automatic detection face proc 
canadian conf 
ece vol 
pp 

edwards cootes taylor face recognition active appearance models proc 
eccv vol 
pp 

ekman universals cultural differences facial expressions emotion proc 
nebraska symp 
motivation cole ed pp 

ekman friesen repertoire nonverbal behavioral categories origins usage coding vol 
pp 

ekman friesen facial action coding system 
palo alto ca consulting psychologist 
ekman huang sejnowski hager eds final report nsf planning workshop facial expression understanding human interaction lab univ california san francisco 
ekman hager irwin ekman hager facial action exemplars human interaction lab univ california san francisco 
essa pentland coding analysis interpretation recognition facial expressions ieee trans :10.1.1.40.359
pattern anal 
machine intell vol 
pp 
july 
taylor cowie douglas cowie emotion recognition faces speech neural networks fuzzy logic assess system proc 
ijcnn vol 
pp 

emotional patterns intonation music 

vol 
pp 

frick communicating emotion 
role prosodic features psychol 
bull vol 
pp 

furnas landauer gomes dumais vocabulary problem human system communication commun :10.1.1.103.8364
acm vol 
pp 

emotional intelligence 
new york bantam books 
hall multisensor data fusion proc 
ieee vol 
pp 
jan 

xu user oriented affective video content analysis proc 
ieee workshop content access image video libraries pp 

hara kobayashi facial interaction animated face robot human beings proc 
smc pp 

hong neven von der malsburg online facial expression recognition personalised galleries proc 
fg pp 

huang huang facial expression recognition model feature extraction action parameters classification vis 
commun 
image vol 
pp 

huber batliner noth warnke niemann recognition emotion realistic dialogue scenario proc 
icslp pp 


review existing techniques human emotion understanding applications hci 
online 
available www image ece ntua gr reports 
jaakkola haussler exploiting generative models discriminative classifiers proc 
advances neural information processing vol 
kearns solla cohn eds pp 

ji yoon park park intelligent system automatic adjustment facial shape model face expression recognition proc 
vol 
pp 

special issue spoken language processing proc 
ieee vol 
pp 
aug 
kanade cohn tian comprehensive database facial expression analysis proc 
fg pp 

kang han lee lee speaker dependent emotion recognition speech signals proc 
icslp pp 

proceedings ieee vol 
september kearney mckenzie machine interpretation emotion memory expert system interpreting facial expressions terms signaled emotions janus cogn 
sci vol 
pp 

ekman facial expression emotion handbook emotions lewis jones eds 
new york guilford pp 

kimura yachida facial expression recognition degree estimation proc 
cvpr pp 

kobayashi hara recognition mixed facial expressions neural network proc 
roman pp 
see pp 

facial individuality expression analysis eigenface method class features multiple discriminant analysis proc 
icip vol 
pp 

li zhao recognizing emotions speech short term long term features proc 
icslp pp 

lyons akamatsu automatic classification single facial images ieee trans 
pattern anal 
machine intell vol 
pp 
dec 
mann wearable computing step personal imaging computer vol 
pp 

flanagan natural communication information systems proc 
ieee vol 
pp 
aug 
mase recognition facial expression optical flow ieice trans vol 
pp 

matsumoto cultural similarities differences display rules 
emotion vol 
pp 

mcneill hand mind gestures reveal thought 
chicago il univ chicago press 
kennedy role fundamental frequency signaling linguistic stress affect evidence dissociation percept 
vol 
pp 

yair super resolution pitch determination speech signals ieee trans 
signal processing vol 
pp 
jan 
communication words psychol 
today vol 
pp 

murray arnott synthesizing emotions speech time get excited proc 
icslp pp 

creation new medium multimedia era proc 
ieee vol 
pp 
may 
nicholson emotion recognition application computer agents spontaneous interactive capabilities knowl syst vol 
pp 

nass steuer tauber computers social actors proc 
chi pp 

wei de silva speech emotion classification proc 
vol 
pp 

mcgee brewster grey putting feel look feel proc 
chi pp 

olson olson trust commerce commun 
acm vol 
pp 

ortony turner basic emotions psychol 
rev vol 
pp 

ohya spotting segments displaying facial expression image sequences hmm proc 
fg pp 

oviatt kuhn integration synchronization input modes multimodal human computer interaction proc 
chi pp 

cottrell representing face images emotion classification proc 
advances neural information processing systems pp 

pan liang huang exploiting dependencies information fusion proc 
cvpr vol 
pp 

pantic rothkrantz expert system automatic analysis facial expression image vis 
comput 
vol 
pp 

automatic analysis facial expression state art ieee trans 
pattern anal 
machine intell vol 
pp 
dec 
affect sensitive multi modal monitoring ubiquitous computing advances challenges proc 
pp 

pantic facial expression analysis computational intelligence techniques ph dissertation delft univ technol delft netherlands 
pentland looking people sensing ubiquitous wearable computing ieee trans 
pattern anal 
machine intell vol 
pp 
jan 
emotion recognition speech signal proc 
icslp pp 

picard affective computing 
cambridge ma mit press 
picard healey machine emotional intelligence analysis affective physiological state ieee trans 
pattern anal 
machine intell vol 
pp 
oct 
polzin detecting verbal non verbal cues communications emotions ph dissertation school comput 
sci carnegie mellon univ pittsburgh pa 
reeves nass media equation people treat computers television new media real people places 
new york cambridge univ press 
russell universal recognition emotion facial expression psychol 
bull vol 
pp 

russell eds psychology facial expression 
cambridge cambridge univ press 
mayer emotional intelligence cogn 
personality vol 
pp 

samal iyengar automatic recognition analysis human faces facial expressions survey pattern recognit vol 
pp 

sato akamatsu emotional speech classification prosodic parameters neural networks proc 
australian new zealand intelligent information systems conf pp 

satyanarayanan pervasive computing vision challenges ieee pers 
commun vol 
pp 
aug 
reilly feature analysis automatic speech reading proc 
ieee int 
workshop multimedia signal processing pp 

ehrlich sheridan face interface facial affect human machine proc 
chi pp 

scherer johnstone automatic verification emotionally stressed speakers problem individual differences proc 
pp 

schneiderman human values technology declaration responsibility sparks innovation human computer interaction schneiderman ed 
norwood nj ablex 
lew cohen garg huang emotion recognition cauchy naive bayes classifier proc 
icpr vol 
pp 

sharma pavlovic huang multimodal human computer interface proc 
ieee vol 
pp 
may 
cultural similarities differences recognition audio visual speech stimuli proc 
icslp pp 

hansen speech stress conditions overview effect speech production system performance proc 
icassp vol 
pp 

stein meredith merging senses 
cambridge ma mit press 
emotional changes human voice superior vol 
pp 

takeuchi nagao communicative facial displays new conversational modality proc 
interchi pp 

tao huang explanation facial motion tracking piecewise bezier volume deformation model proc 
cvpr vol 
pp 

tian kanade cohn recognizing action units facial expression analysis ieee trans :10.1.1.39.8427
pattern anal 
machine intell vol 
pp 
feb 
life communication agent emotion sensing character mic feeling session character muse proc 
int 
conf 
multimedia computing systems pp 

pantic rothkrantz affect sensitive multimodal human computer interaction vapnik nature statistical learning theory 
new york springer verlag 
soulie multi resolution scene segmentation mlp proc 
ijcnn vol 
pp 

wang yachida expression recognition time sequential facial images expression change model proc 
fg pp 

reading human faces 
cogn vol 
pp 

williams stevens emotions speech acoustic correlates acoust 
soc 
amer vol 
pp 

yacoob davis recognizing facial expressions spatiotemporal analysis proc 
icpr vol 
pp 

yang kriegman ahuja detecting faces images survey ieee trans 
pattern anal 
machine intell vol 
pp 
jan 
yang roth ahuja snow face detector proc 
neural information processing systems vol 
pp 

kim effect sensor fusion recognition emotional states voice face image thermal image face proc 
roman pp 

tomita kimura facial expression recognition thermal image processing neural network proc 
roman pp 

zhang lyons schuster akamatsu comparison geometry gabor wavelets facial expression recognition multi layer perceptron proc 
fg pp 

zhao lu jiang wu study emotional feature recognition speech proc 
icslp pp 

zhu de silva ko moment invariants hmm facial expression recognition pattern recognit 
lett vol 
pp 

maja pantic member ieee received ph degrees computer science delft university technology delft netherlands respectively 
assistant professor data knowledge systems group department delft university technology 
research interests pertain application artificial intelligence computational intelligence techniques analysis different aspects human behavior realization perceptual context aware multimodal human computer interfaces 
dr pantic member association computing machinery american association artificial intelligence 
leon rothkrantz received sc 
degree mathematics university utrecht utrecht netherlands ph degree mathematics university amsterdam amsterdam netherlands sc 
degree psychology university leiden leiden netherlands 
currently associate professor data knowledge systems group department delft university technology delft netherlands 
current research focuses wide range related issues including lip reading speech recognition synthesis facial expression analysis synthesis multimodal information fusion natural dialogue management human affective feedback recognition 
long range goal research design development natural context aware multimodal man machine interfaces 
proceedings ieee vol 
september 
