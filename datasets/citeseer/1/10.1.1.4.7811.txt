feature distributional clustering text categorization ron cs department technion haifa israel cs technion ac il ran el yaniv cs department technion haifa israel cs technion ac il naftali tishby school cs engineering center neural computation hebrew university jerusalem israel tishby cs huji ac il winter cs department technion haifa israel winter cs technion ac il describe new powerful text categorization method combination distributional features support vector machine svm classifier 
feature selection approach uses distributional clustering words introduced information bottleneck method generates cient representation documents 
combined classification power support vector machines produce best known multilabel categorization results newsgroups dataset 

text categorization fundamental task information retrieval rich body knowledge accumulated past years 
standard approach text categorization far document representation word input space vector high trimmed dimensional euclidean space relying classification algorithm trained supervised learning manner 
early days text categorization theory practice classifier design significantly advanced strong learning algorithms emerged see 
contrast despite numerous attempts introduce sophisticated document representation techniques higher order word statistics nlp simple minded independent word representation known bag words bow remained popular :10.1.1.42.7488
date best multi class multi labeled categorization results known reuters data set bow representations 
give evidence usefulness sophisticated text representation method applications introduced information bottleneck ib clustering framework :10.1.1.42.7488
specifically approach ib clustering representing document feature cluster space feature space cluster distribution document classes 
show relatively new distributional representation explored context combined support vector machine svm classifier allows best reported result multi class categorization known newsgroups ng dataset 
show categorization ng strong algorithmic word setup dumais achieved best reported categorization results reuters dataset significantly inferior 
outset findings surprising distributional word clusters words representing documents striking advantages 
word clustering performs sophisticated dimensionality reduction implicitly considers correlations various features terms words 
contrast numerous greedy approaches feature selection consider feature individually mutual information information gain tfidf see 
second clustering achieved ib method provides solution statistical sparseness problem common text categorization representation feature space 
clustering words allows extremely compact representations information compromises allow strong classifiers typically lower computational ort 
tested categorization setup word cluster representation reuters dataset modapte split obtain improvement best known categorization results dumais word representation 
hypothesize di erence appears articles reuters dataset categorized basis keywords 
hypothesis correct mean respect data set significant improvement achieved representations sophisticated bag words 
section study question attempts characterize di erences ng reuters datasets 
rest organized follows 
section discuss categorization results datasets consider ng reuters previous attempts word cluster representation text 
section algorithmic components starting mutual information feature selection information bottleneck method distributional clustering deterministic annealing clustering algorithm support vector machines 
components known believe time components applied 
section experimental setup give detailed description results 
section summarize 

related results dumais reported best known multi label categorization reuters dataset modapte split 
dumais method apply support vector machines svm learning scheme reduced bow representation feature reduction greedy word mutual information class 
method leads break result largest categories 
joachims uses svm multi label categorization reuters dataset feature reduction achieves break 
distributional clustering scheme pereira baker mccallum apply distributional clustering words represented distributions classes generate sophisticated representation word clusters 
representation applied ng dataset naive bayes classifier word clusters 
result accuracy uni labeled categorization 
baker mccallum compared methods feature reduction techniques clustering words latent semantic indexing see mutual information markov feature selection classifier naive bayes cases 
word clustering representation led best accuracy 
improved results achieved joachims far know shows best results uni labeled categorization ng dataset 
joachims applied naive bayes classifier rocchio algorithm mutual information reduced feature representation leads accuracy 
investigate strength word clustering approach document representation 
type distributional clustering essentially supervised application information bottleneck ib method tishby 
slonim tishby explore properties word cluster representation motivate general ib method 
authors show categorization representation words improves results bow representation training set small respect naive bayes classifier 

methods algorithms feature selection mutual information feature selection feature reduction general term techniques dimensionality reduction 
considering high dimensional vectorial representation data techniques attempt select optimal subset vector components data points projected 
incentive improve classification quality noise reduction improve performance 
selection optimal feature subset hard problem su ers combinatorial explosion 
despite existence sophisticated methods see authors consider simple greedy approaches 
dumais method mutual information mi 
binary random variables indicating category word occurred 
mutual information defined follows ew ew ec log ew ec ew ec ew ec boolean random variables appearance disappearance word category respectively 
experiment settings described mutual information technique feature selection 
information bottleneck distributional clustering distributional clustering mutual information optimization introduced pereira tishby lee distributions verb object pairs 
original algorithm aimed minimizing average distributional similarity terms divergence conditional verb noun noun centroids distributions 
algorithm turned special case general principle termed information bottleneck method tishby pereira bialek 
question relevant encoding variable respect variable posed formulated general converging algorithm introduced 
relevant encoding random variable relies soft partitioning domains preserve mutual information variable resulting partition clusters constitute approximate su cient partition enable construction optimal code binary tree provides information denoting induced partition set clusters problem simple variational formulation maximize mutual information respect partition constraint 
find optimal tradeo minimal partition maximum preserved information resulting self consistent equations essentially coincides original distributional clustering algorithm written exp ln normalization factor exponential defined implicitly bayes rule terms partition assignment rules 
parameter lagrange multiplier introduced constrained information natural resolution annealing parameter 
distributional clustering deterministic annealing self consistent equations iterated guaranteed converge value 
fact analogous blahut algorithm information theory 
value modified low high temperature correspond poor distributional resolution high low temperature correspond higher resolution clusters 
procedure known deterministic annealing introduced context clustering rose 
employed procedure enabling increase number clusters annealing process increase 
main problem procedure identifying phase transitions correspond clusters splits 
small datasets alternative agglomerative algorithm developed slonim tishby avoids problem 
support vector machines svms support vector machine svm inductive learning scheme proved successful various application domains 
particular pieces evidence indicate svm choice text categorization 
simplest linear svm 
data linearly separable linear svm computes maximum margin linear classifier 
non linearly separable case extension allows cost dependent training errors basic svm quadratic optimization problem includes parameter controls training errors costs 
authors advocated choice linear svm opposed kernel svm due speed training classification time generalization abilities respect textual domains 
experiments linear svm 
implementation svmlight package joachims 
putting straightforward approach dealing multi class categorization classes decompose problem binary problems 
exist decomposition methods powerful see 
simplicity comparison related results chose straightforward decomposition 
algorithmic setups 
feature selection mutual information technique eq 
discriminating features words selected articles projected svm classifier trained projections details see algorithm 
second setup information bottleneck distributional clustering initially words training set clustered clusters pseudo words deterministic annealing implementation information bottleneck method see respectively rest procedure similar setup articles projected pseudo words best words algorithm 
bag words classifier learning input cm set categories train dn training set articles bow representation set categories belongs feature reduction size output hm set binary classifiers wm set selected features category train set words train category word train compute eq sort words train extract top words wk article train project add add run svm algorithm construct binary classifier bag words classification input test article hm set binary classifiers wm set selected features category output set boolean labels means belongs means 
classifier project run obtain algorithm mi feature selection svm 
experimental setup data sets reuters corpus contains articles taken reuters newswire 
article designated zero semantic categories earn trade corn total number categories 
modapte split consists training set containing articles test set containing articles 
training test sets preprocessed article additional information title body removed 
note figures count documents label 
original split contains training documents test documents additional articles labels 
ib classifier learning input cm set categories train dn training set articles bow representation set categories belongs feature reduction size output hm set binary classifiers mapping function words pseudo words train set words train word train build vector vw nw nw cm nw number occurrences category cluster set vectors vw clusters pw pw pw ib method word train map word appropriate pseudo word pw pw article train project pw category article train add add run svm algorithm construct binary classifier ib classification input test article hm set binary classifiers mapping function words pw output set boolean labels means belongs means 
classifier project pw run obtain algorithm ib word clustering svm newsgroups corpus contains articles taken usenet newsgroups collection 
article designated semantic categories total number categories size 
articles semantic tag ones 
extracted list categories article belongs field newsgroups article header 
ignored problem duplicated articles 
preprocessed article additional information sub problem account articles duplicated reported ject body removed 
addition filtered lines part binary files sent attachments 
line considered binary longer symbols contains blanks 
removed binary attachments useless delimiter lines total amount lines 
cross validated training parameter setting standard split reuters fixed apply cross validation 
experiments newsgroups fold cross validation 
split randomly uniformly parts articles part articles category 
random partition training remaining testing 
note split proportional training test set ratios modapte split reuters training set testing set dataset 
order improve results tuned svm algorithm parameters 
linear svm setting parameters tried tune trade training error margin cost factor negative positive examples 
parameters fixed set possible values applied svm classifier combinations 
perform fair test tuned parameters validation subset taken random parts training set corresponds dumais method tuning parameters described 
performance measure measuring performance multi class categorization meaningless standard accuracy measure 
customary break point arithmetic average precision recall measure essentially harmonic average 
specifically considering categorization task classes 
cm binary decomposition classifiers 
hm th classifier responsible discriminating rest classes 
classifier compute confusion matrix entries counts number samples classified category true label sets include counts number samples classified label sets include similarly respectively count number samples classified true label sets respectively contain precision equals recall equals total micro averaged precision recall micro averaged break point defined micro averaged measure break measure may favor trivial results example data categorized properly recall zero precision average harmonic average 
note micro averaged precision recall simply weighted averages weighted class sizes precisions recalls individual classifiers 
dumais simple average precision recall harmonic average simple average precision recall experiments reported computational efforts ran tests pentium iii mhz ram pc windows 
setup mi feature selection svm classification bottleneck svm single run take hours depending parameter values 
general smaller parameters quicker algorithm runs 
example failed run svmlight ng parameter values managed improve run time filtering binary attachments see 
ib method svm classification svmlight runs faster input vectors pseudo words 
clustering take hour entire ng set requires memory ram run 
training test time entire ng hours hours cross validation folds 

results discussion table summarizes categorization results obtained methods reuters largest categories ng data sets 
note result reuters data set established dumais 
result obtained joachims perform entire experiment reuters categories experiments independently chosen categories 
achieved accuracy category wheat uni labeled setting 
results show interesting di erence quality methods described applied reuters ng datasets 
break best reported result multi labeled categorization ng data set 
previous attempts categorize set performed 
computed microaveraged break point corresponding bare word representation setting described obtain results better unfairly allowed algorithm tune parameters respective test sets folds 
result obtained course unrealistic conditions serve upper bound performance algorithmic setup 
repeated unfair experiment reuters data set obtained opposite results 
ib representation lost advantage unfair conditions achieve result successful results bow representation 
performance representation methods di erent data sets 
inferior bow representation outperformed ib representation reuters ng svm mi selection unfair svm ib clustering unfair table break categorization results data sets number features 
figures correspond ng averages fold cross validation 
unfair means classifier parameters chosen unfairly test set ensure better result achieved 
key answer related process generated labeling data sets 
noted lewis see reuters distribution set contains articles appeared reuters newswire assembled indexed categories personnel reuters presumably manual indexing reuters articles relied mainly restricted set keywords indexers looked 
contrast articles ng labeled creators annotation relied full understanding articles context 
order test hypothesis category data sets computed mutual information words appearing category category 
sorted words decreasing values mutual information 
instance show graphs mi behavior seen graph earn reuters goes sharper rec sport hockey ng approves fact words reuters contribute maximum text categorization 
seen scales axis graphs di er order magnitude 
order compare plot percentage scale mutual information value linearly transformed value dynamic range transformed 
consider dynamic range informative words category obtain normalized sorted histograms 
put scale graphs definitely show ng categories distinction bases features reuters 
show learning curves plotting obtained break success rate function number words 
see curves describes learning rate respect reuters second respect ng 
seen break reuters approaches maximum words chosen greedy non optimal mutual information method 
means words contribute 
graph ng constantly goes speed increase constantly lowers 
addition show word category break result entire reuters corpus ng lower 
table earn number features number features rec sport hockey sorted histograms best discriminating features categories 
earn reuters rec sport hockey ng list individual break result categorizing largest categories reuters words 
instance words vs cts loss possible achieve break categorization earn 
note word vs appears articles category earn articles total category 
word appears non earn articles test set vs categorize earn high precision 
phenomenon noticed joachims showed classifier built word wheat lead extremely high accuracy distinguishing category wheat uni labeled setting 

shown cluster representation texts information bottleneck method combined support vector machine classifier leads multi labeled categorization ng dataset superior best known word techniques 
knowledge todate result best reported multi labeled breakeven dataset 
believe results show advantages sophisticated text representations train set vs appears earn articles non earn articles number features rec sport hockey earn earn hockey scale 
category st word nd word rd word earn vs cts loss acq shares vs money fx dollar vs exchange grain wheat grain crude oil trade trade vs cts interest rates rate vs ship ships vs strike wheat wheat wheat corn corn vs table best words terms mi rate categorization 
largest categories reuters 
micro average categories 
plus means word contributes appearance minus means word contributes disappearance word representations conjunction strong classifier svm 
hand advantage technique categorization reuters dataset hypothesize due inherent di erences ways datasets generated 
hypothesis supported research believe text categorization benefit comparative study larger variety datasets 

baker mccallum distributional clustering words text classification proceedings sigir 
roberto basili alessandro maria pazienza language sensitive text classification proceedings riao th international conference recherche information par ordinateur paris france pp 

reuters collection achieved www research att com lewis 
number features reuters newsgroups number features reuters newsgroups learning curves break vs number words reuters ng top top words bow representation svm cortes vapnik support vector networks machine learning 
cover thomas elements information theory john wiley sons 
crammer singer learnability design output codes multiclass problems proceedings colt 
cristianini shawe taylor support vector machines cambridge university press 
deerwester dumais furnas landauer harshman indexing latent semantic analysis journal american society information science 
duda hart stork pattern classification nd ed john wiley sons new york 
dumais platt heckerman sahami inductive learning algorithms representations text categorization proceedings acm cikm 
jacobs joining statistics nlp text categorization proceedings third conference applied natural language processing pp 

joachims probabilistic analysis rocchio algorithm tfidf text categorization proceedings icml pp 

text categorization support vector machines learning relevant features proceedings tenth european conference machine learning pp 

koller sahami hierarchically classifying documents words proceedings icml pp 

svm light software achieved ais gmd de thorsten 
newsgroups collection achieved kdd ics uci edu 
pereira tishby lee distributional clustering english words proceedings th annual meeting association computational linguistics pp 

rocchio relevance feedback information retrieval ch 
pp 
prentice hall smart retrieval system experiments automatic document processing 
rose deterministic annealing clustering compression classification regression related optimization problems proceedings ieee 
salton mcgill modern information retrieval mcgraw hill 
schapire singer boostexter boosting system text categorization machine learning 
slonim tishby agglomerative information bottleneck advances neural information processing systems pp 

power word clustering text classification appear european colloquium ir research ecir 
tishby pereira bialek information bottleneck method invited th annual allerton conference communication control computing 
vapnik nature statistical learning theory springer verlag 
statistical learning theory john wiley sons new york 
yang 
pedersen comparative study feature selection text categorization proceedings icml pp 

