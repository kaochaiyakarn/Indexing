pe cep ua intelligence box martigny switzerland phone fax mail secretariat idiap ch internet www idiap ch theme topic mixture model graphical model document representation keller bengio idiap rr february submitted publication idiap cp martigny switzerland idiap ch idiap cp martigny switzerland bengio idiap ch idiap research report theme topic mixture model graphical model document representation keller bengio february submitted publication 
documents usually represented bag word space 
representation take account possible relations words 
propose graphical model representing documents theme topic mixture model ttmm 
model assumes types relations textual data 
topics link words themes gather documents particular distribution topics 
defines ttmm compares related latent dirichlet allocation lda model reports interesting empirical results 
idiap rr contents document representation theme topic mixture model expectation maximization optimization 
stochastic gradient ascent optimization 
related model lda ttmm vs lda experiments idiap rr order automatically processed textual data represented formally 
basic widely indexing method text categorization supervised related problems bag words document representation :10.1.1.119.8039
document representations proposed literature particular methods graphical models latent dirichlet allocation lda probabilistic latent semantic analysis plsa 
estimate density documents try overcome problems inherent bag words representation 
weakness bag words take account polysemic properties human languages 
respectively high distinction words ocean sea merge di erent meanings word surfing internet sea 
second problem simple representation dimension representation space equal size dictionary order magnitude words 
means lot parameters estimate system bag words documents inputs leads easily curse dimensionality problem 
graphical model theme topic mixture model ttmm plsa lda tries overcome problems 
method leads representation constructed highlight small number concepts topics documents huge number words 
furthermore advantage density estimation methods indexing methods possibility take profit unlabeled data order improve performance supervised tasks 
section quickly explain general document representation problem 
section ttmm section related lda probabilistic model 
section compares models theoretical aspects section reports experiment comparing di erent document representations 
section concludes 
emphasize particular point find words concept theme topic 
commodity order express intuition semantic links textual data components fact simply refer high level statistical correlations 
document representation corpus information access tasks assumption precise order words documents neglected word frequencies su cient information 
implications assumptions reflected preprocessing step document representation 
explained documents represented vector weights assigned word vocabulary representation called bag words representation vector space model :10.1.1.119.8039
weight general function frequency th word document vocabulary extracted training subset targeted corpus 
frequencies words key point representation selected neutral words called words usually high frequency low discriminant properties general removed possible step preprocessing called stemming words corpus replaced stem 
example connecting connected connection connections replaced common stem connect 
step performed reduces vocabulary size attempts reflect fact words stem similar meanings 
stemming information possible links words included representation 
approaches represent documents applied account kind information 
approaches find probabilistic approaches density documents vector space estimated model 
idiap rr document density estimation model proposed high level statistical correlations words corpus assumed 
theme topic mixture model proposed model lot common lda see section inspired 
theme topic mixture model documents assumed sampled mixture latent themes defines particular mixture latent topics distribution words 
graphically displayed fig 
model observed variable document seen set words unobserved variables themes 
topics 
hyper parameters chosen 
parameters represent respectively mixing proportions themes mixing proportions topics themes probability word topic estimated 
word space document space theme topic word ttmm graphical representation 
boxes represent replicates 
outer box represents repeated choice themes inner box represents repeated choice topics theme 
parameters model 
underlying generative process document 
choose document size 

choose theme multinomial distribution parameter mixing proportions 

words choose topic 
multinomial distribution conditioned theme 
choose word multinomial distribution conditioned topic randomness document size modeled example poisson distribution parameter necessary generative process 
independent data generating variables real interest model 
ignored 
generative process word seen mixture topics di erent mixing proportions depending document theme jk kl jk kl 
fact log likelihood form maximizing lead distinct problems 
idiap rr probability document generated theme jk kl frequency term document seen mixture themes jk kl 
corpus documents 
log likelihood corpus model ln jk kl 
depending actual implementation various multinomial distributions represented tables multilayer perceptrons mlps instance maximized expectation maximization em stochastic gradient ascent optimization techniques 
expectation maximization optimization order perform em optimization ttmm get rid sum inside logarithm log likelihood equation 
done easily ij indicator variables specifying theme document generated indicator variables specifying topic word generated theme context 
complete log likelihood written comp ij ln ln jk kl 
notice expected values ij respectively 
em algorithm goes follows 
step complete log likelihood estimated estimating posteriors ij jkl follows ij ij jk kl qk kl idiap rr jkl jk kl jp kp 
step expected log likelihood comp maximized normalization constraints posteriors estimated previous step 
maximum obtained parameter values ij iq ij iq jk ij jkl ij jpl ij jkl ij kl ij jkl ij wm 
size dictionary 
stochastic gradient ascent optimization tables represented mlps gradient ascent optimization algorithm order learn corresponding parameters 
represent tables distributed manner way control capacity tables 
propose stochastic gradient ascent algorithm optimizing log likelihood criterion normalization constraints jk kl 
lagrange multipliers 
document gradient respect log parameters ln ij iq idiap rr ln jk ij jkl jpl ln kl ij jkl ij wm 
parameters model updated document follows represents small learning rate 
related model lda latent dirichlet allocation model lda similar ttmm 
main di erence considering number themes finite lda considered infinite 
infinity choices proportions mixture latent topics obtained dirichlet distribution 
lda model probability document written kl frequency word kl dimensional dirichlet random variable dirichlet probability density 
di erence computation equation intractable exact inference 
order learn parameters model variational approximation proposed 
log probability document approximated lower bound depending variational distribution approximation fixed posterior distribution 
document log probability decomposed follows ln dkl variational parameters ln lower bound dkl kullback leibler divergence 
log likelihood maximization aims reached 
lower bound closest possible log probability obtained maximizing 

log likelihood maximum respect original parameters obtained maximizing 
leads variational em proposed step iterative algorithm run find step optimal computed 
idiap rr ttmm vs lda section compare ttmm lda characteristics ability applied dimensionality reduction task clustering task supervised task time space complexity 
dimensionality reduction application density estimation methods instance dimensionality reduction method bag words representation 
idea considering words basic units document representation consider topic basis hope topics capture information huge amount words 
case lda proposed variational parameter representing document distribution approximates dirichlet parameters provides representation topic space 
case ttmm choose instance posterior topic components document jk 
similarly represent documents theme basis combination 
clustering application contrary lda ttmm density estimation seen soft documents themes 
useful corpus representation example order speed information retrieval task 
supervised task application ttmm directly supervised task text categorization 
identify themes categories instance probability theme freq category 
case similar proposed li yamanishi 
imagine applying lda directly text categorization task learning lda models categories 
parameters ttmm solution probably better estimated lda parameter help solving di erent classification problems data estimate 
time complexity number documents corpus size dictionary associated corpus number words document number topics number themes 
displayed table em iteration maximizing ttmm likelihood complexity time variational em iteration nk 
ttmm lda defined generative models able infer probability new document case ttmm infer exact complexity time jk 
lda infer optimal lower bound probability operation complexity time 
comparing complexities ttmm lda notice number themes ttmm replaced size document lda formula 
imagine corpus containing long documents ttmm better time complexity lda 
space complexity shown table memory complexity point view lda parsimonious ttmm 
lda parameters space complexity km ttmm 
idiap rr lda ttmm em step nk inference kj table time complexities lda ttmm lda ttmm parameters km table space complexities lda ttmm experiments section experiment comparing lda ttmm bag word representation reported 
lda features bag words document representations compared text categorization task support vector machines svms classifiers 
data subset reuters splits experimental protocol experiment repeated ttmm 
experiment authors selected reuters documents training data modapte split stopped stemmed data resulting vocabulary discarded frequent words obtain vocabulary words 
lda model topics trained documents class labels 
proportion training data trained support vector machine svm lda features document representation explained section dimensionality reduction paragraph binary classification problem numbers themes trained documents class labels 
models optimized em algorithm tables represent various conditional distributions 
proportions training data svm gaussian kernel trained ttmm topic features document representation binary classification problem described experimental section 
standard deviation kernel tuned fold cross validation procedure training data 
results remaining proportion data compared ones svms trained bag words representation svms trained lda features reported 
fig 
summarizes results category grain 
note results optimistic comparable text categorization published results vocabulary extracted training test sets 
problem having unseen words test set addressed 
comparison ttmm lda followed experimental protocol described 
reported numbers themes topics features obtained ttmm give general results lda features better proportion 
furthermore see experiment ttmm capture important information data features bag words representation vs results better small values reuters documents labeled categories 
approach considered grain category 
idiap rr grain proportion data training words features lda features themes ttmm features themes ttmm features classification results grain vs grain binary classification problem proportions training data features 
ttmm features computed topics numbers themes em optimization 
new document density estimation model theme topic mixture model ttmm compared lda similar model 
ttmm appears reach reasonable performance close lda 
advantages ttmm lda discussed 
instance contrary lda ttmm inferred exactly viewing ttmm discretized version lda solve applications accessible lda 
ttmm lda document representation bag words representation proved give dimensionality reduction input space performance loss training small corpora 
plan experiments exploring ttmm advantages lda bag words representation 
instance ttmm directly solve text categorization task may promising issue 
acknowledgments research carried framework swiss project im framework pascal european network excellence funded swiss 
bawa roberto bayardo jr rakesh agrawal 
sets search enhanced 
proceedings nd annual acm conference research development information retrieval berkeley california august 
david blei andrew ng michael jordan 
latent dirichlet allocation 
journal machine learning research january 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal statist 
soc 

thomas hofmann 
unsupervised learning probabilistic latent semantic analysis 
machine learning 
idiap rr lecun 
de apprentissage connectionist learning models 
phd thesis universite curie paris june 
hang li kenji yamanishi 
document classification finite mixture model 
philip cohen wolfgang wahlster editors proceedings fifth annual meeting association computational linguistics eighth conference european chapter association computational linguistics pages somerset new jersey 
association computational linguistics 
fabrizio sebastiani :10.1.1.119.8039
machine learning automated text categorization 
acm computing surveys 
