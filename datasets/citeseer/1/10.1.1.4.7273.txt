fast accurate part speech tagging svm approach revisited jes gim enez research center lsi department universitat polit de catalunya jordi girona barcelona lsi upc es simple effective part speech tagger support vector machines svm 
simplicity efficiency achieved working linear separators primal formulation svm greedy left right tagging scheme 
means rigorous experimental evaluation conclude proposed svm tagger robust flexible feature modelling including lexicalization trains efficiently parameters tune able tag thousands words second really practical real nlp applications 
regarding accuracy svm tagger significantly outperforms tnt tagger exactly conditions achieves competitive accuracy wsj corpus comparable best taggers reported date 
automatic part speech pos tagging task determining morphosyntactic category word sentence 
known problem addressed researchers decades 
fundamental problem sense nlp applications need kind pos tagging previous construct complex analysis permanently fashion current applications demand efficient treatment quantities possibly multilingual text 
literature find approaches pos tagging statistical machine learning techniques including hidden markov models weischedel brants maximum entropy taggers ratnaparkhi transformation learning brill memory learning daelemans decision trees rodr iguez adaboost abney support vector machines nakagawa 
previous taggers evaluated english wsj corpus penn treebank set pos categories lexicon constructed directly annotated corpus 
evaluations performed slight variations wide late state art accuracy english pos tagging 
years succesful popular taggers nlp community hmm tnt tagger brants transformation learning tbl tagger brill variants maximum entropy approach ratnaparkhi 
opinion tnt example really practical tagger nlp applications 
available anybody simple easy considerably accurate extremely efficient allowing training word corpora just seconds tagging thousands words second 
case tbl approaches great success due flexibility offer modelling contextual information slightly accurate tbl 
far considered closed problem researchers tried improve results pos tagging task years 
allowing richer complex hmm models harper lee enriching feature set tagger toutanova manning effective learning techniques svm nakagawa voted perceptron training model collins 
complex taggers state art accuracy raised wsj corpus 
complementary direction researchers suggested combination pre existing taggers alternative voting schemes brill wu halteren 
accuracy taggers better ensembles pos taggers complex efficient 
suggest go back tnt philosophy simplicity efficiency state art accuracy svm learning framework 
claim svm tagger introduced fulfills requirements practical tagger offers balance properties 
simplicity tagger easy parameters tune flexibility robustness rich context features efficiently handled overfitting problems allowing lexicalization high accuracy svm tagger performs significantly better tnt achieves accuracy competitive best current taggers efficiency training wsj performed cpu hour tagging speed allows massive processing texts 
worth noting support vector machines svm paradigm applied tagging previous nakagawa focus guessing unknown word categories 
final tagger constructed gave clear evidence svm approach specially appropriate second third previous points main drawback low efficiency running speed words second reported 
overcome limitation working linear kernels primal setting svm framework advantage extremely sparsity example vectors 
resulting tagger accurate nakagawa times faster prototype implemented perl 
rest organized follows section formal svm learning setting 
section devoted explain details approach tagging 
section describes experimental carried order validate svm tagger 
section includes discussion approach comparison related section concludes outlines directions research 
support vector machines svm machine learning algorithm binary classification successfully applied number practical problems including nlp cristianini shawe taylor 
set training examples instance vector gamma class label 
basic form svm learns linear hyperplane separates set positive examples set negative examples maximal margin margin defined distance hyperplane nearest positive negative examples 
learning bias proved properties terms generalization bounds induced classifiers 
linear separator defined elements weight vector component feature bias stands distance hyperplane origin 
classification rule svm sgn hw delta xi example classified 
linearly separable case learning maximal margin hyperplane stated convex quadratic optimization problem unique solution minimize jjwjj subject constraints training example hw delta 
svm model equivalent dual formulation characterized weight vector ff bias case ff contains weight training vector indicating importance vector solution 
vectors non null weights called support vectors 
dual classification rule ff ff hx delta xi ff vector calculated quadratic optimization problem 
optimal ff vector dual quadratic optimization problem weight vector realizes maximal margin hyperplane calculated ff simple expression terms training examples see cristianini shawe taylor details 
advantage dual formulation permits efficient learning non linear svm separators introducing kernel functions 
technically kernel function calculates dot product vectors non linearly mapped high dimensional feature space 
need perform mapping explicitly training feasible dimension real feature space high infinite 
presence outliers wrongly classified training examples may useful allow training errors order avoid overfitting 
achieved variant optimization problem referred soft margin contribution objective function margin maximization training errors balanced parameter called problem setting section details approach pos tagging regarding collection feature codification training examples 
classification problem tagging word context multi class classification problem 
svms binary classifiers binarization problem performed applying 
applied simple class binarization svm trained part speech order distinguish examples class rest 
tagging word confident tag predictions binary svms selected 
training examples considered classes 
dictionary extracted training corpus possible tags word considering occurrence training word tagged example positive example class negative example classes appearing possible tags dictionary 
way avoid generation excessive irrelevant negative examples training step faster sections see word corpus generates training sets examples average 
feature codification example codified basis local context word disambiguated 
considered centered window tokens basic gram patterns evaluated form binary features previous word preceeding tags dt nn table contains list patterns considered 
seen tagger lexicalized word forms appearing window taken see abney discussion efficiency problems learning large pos training sets 
word features gamma gamma gamma pos features gamma gamma gamma ambiguity classes word bigrams gamma gamma gamma gamma pos bigrams gamma gamma gamma word trigrams gamma gamma gamma gamma gamma gamma gamma gamma gamma pos trigrams gamma gamma gamma gamma gamma gamma table feature patterns codify examples 
account 
simple left right tagging scheme tags words known running time 
approach daelemans general ambiguity class tag right context words label composed concatenation possible tags word rb jj nn 
individual tags ambiguity class taken binary feature form word may vbz 
ambiguity classes avoid passes solution proposed nakagawa tagging performed order right contexts disambiguated second pass 
nakagawa suggested explicit gram features necessary svm approach polynomial kernels combination features 
interested working linear kernel included feature set 
section evaluate importance kind features 
experiments section presents experiments carried order evaluate svm approach pos tagging 
works wall street journal data penn treebank iii benchmark corpus 
randomly divided sentence level word corpus subsets training words validation words test words 
tagging experiments reported evaluated complete test set 
validation set optimize parameters 
penn treebank tagset contains tags 
compiling training examples way explained section receive positive negative examples 
model acc 
sv time fs fs fs fs fs fs table accuracy results ambiguous words alternative svm models varying kernel degree feature set 
sv stands average number support vectors tag time cpu time needed training 
svm classifiers trained binarized setting 
unambiguous tags correspond punctuation marks symbols categories wp 
linear vs polynomial kernels experiment explores effect kernel training process generalization accuracy 
trained svm classification models word training set varying degree polynomial kernel 
software package experiments reported svm light simple frequency threshold common experiments filter features 
particular discarded features occur times minimum number total amount features greater 
training setting parameter left default value 
subsections see optimization parameter leads small improvement final tagger svm algorithm quite robust respect parameterization 
results obtained classifying ambiguous words test set table 
test performed batch mode examples ambiguous words taken separately features involving pos tags left contexts calculated correct pos tag assignment corpus 
svm light software freely available url svmlight joachims org 
complex methods filter irrelevant features 
feature selection problem scope simplest alternatives preferred 
regarding accuracy similar results nakagawa drawn 
set atomic features fs best results obtained degree polynomial kernel 
greater degrees produce overfitting training data number support vectors highly increases accuracy decreases 
gram extended set features fs linear kernel competitive compared degree polynomial kernel clearly preferable regarding sparsity solution learning time times faster 
interestingly advantage extended set features noticeable case linear kernel accuracy decreases polynomial kernels 
state linear kernel gram set basic features suffices obtain highly accurate svm models relatively fast train 
section see linear solution additional advantage allowing primal setting sparse vector weights 
fact crucial obtain fast pos tagger 
evaluating svm tagger focus linear svm model 
experiment pos tagger tested realistic situation performing left right tagging sequence words line calculation features making assigned left context pos tags 
simplicity efficiency principles greedy left right tagging scheme applied optimization tag sequence performed sentence level 
implemented pos tagger prototype perl referred 
svm dual representation set support vectors output svm light converted primal form equation explained section 
tagger tested closed vocabulary assumption unknown words allowed 
simulated directly including dictionary words test set occur training set 
results obtained increasing sizes training set table 
learning tagging times graphically 
experiments performed linux words learning time words tagging time learning tagging time plots increasing sizes training set 
ghz pentium iv processor gb ram 
time figures calculated benchmark package perl reflect cpu time 
expected accuracy tagger grows size training set presenting logarithmic behaviour 
regarding efficiency observed training time linear respect number examples training set 
compression svm model respect training set increases training set size goes examples support vectors examples support vectors 
compression level permit efficient tagger dual form thousands dot products needed classify word 
interestingly model primal form quite compact weight vectors resulting compacting support vectors contains features possible features 
provided test examples sparse contain features average irrespective training set size classification rule efficient single dot product sparse vector needed classify word 
basing dot product non null dimensions example classify tagging time invariant 
particular tagging speed corresponding kw words second 
optimizing parameter svm algorithm tradeoff training error margin maximization slightly better results obtained 
optimized parameter validation set maximizing accuracy value properly set accuracy results obtained training set ambiguous words increase points 
idea quality values run tnt exactly conditions including unknown words backup lexicon results obtained significantly lower ambiguous words 
including unknown words tagger results previous section realistic assume closed vocabulary 
order deal problem developed svm model recognize unknown words 
unknown words treated ambiguous words possible pos tags corresponding open class words penn treebank tagset specialized svms learned particular features disambiguate pos tags unknown words 
approach similar nakagawa features table taken works brill nakagawa 
training examples unknown words collected training set way training corpus randomly divided parts equal size 
part extract examples occur remaining nineteen parts corpus known re parameter automatically done iteratively exploring shorter shorter intervals values best accuracies observed validation set 
setting algorithm involves parameters minc maxc log iters segments 
ones minc maxc determine interval examine 
log true iteration approached logarithmically 
arguments iters segments stand total number iterations number intervals explored iteration respectively 
experiments combination true 
words amb 
exs feat 
sv time time table accuracy results closed vocabulary assumption increasing sizes training corpus 
amb columns contains accuracy achieved ambiguous words respectively 
exs sv stand average number examples support vectors pos tag feat total number binary features filtering average number active features training examples average number dimensions weight vectors 
time time refer learning tagging time 
features known words see table prefixes suffixes sn sn sn sn sn sn sn sn sn sn begins upper case upper case lower case contains capital letter contains capital letter contains period contains number contains hyphen word length integer table feature templates unknown words extract examples 
procedure repeated parts obtaining approximately examples corpus 
choice dividing arbitrary 
proportion results percentage unknown words similar test set 
results obtained table 
label corresponds case parameter optimized similar previous section results obtained clearly outperform results tnt tagger comparable accuracy best current taggers range 
parameter provides small increment performance cost increasing training time cpu hours 
suggestion referees experiments replicated different parti parameter value known words unknown words 
amb 
known unk 
tnt table accuracy results compared tnt open vocabulary assumption 
known unk refer subsets known unknown words respectively amb subset ambiguous known words accuracy 
tion wall street journal corpus order compare related previous ones 
sections training validation test respectively 
parameter lead system achieve token accuracy significantly outperforming tnt 
result competitive reported collins little lower toutanova see details section 
perl implementation model achieves tagging speed words second 
type operations computed tagging algorithm fairly believe re implementation speed tagger making efficiency valid massive text processing 
course tnt tagger efficient achieving tagging speed words second conditions 
discussion closely related nakagawa 
svm tagger compared tnt obtaining best accuracy training word corpus 
apart training set size main differences approaches explained 
nakagawa focused tagging unknown words 
certainly ad hoc procedure performed tag word sequence passes 
pass tagger disambiguates sentence second pass previously assigned tags assumed correct order extract right context tag features 
tagging overhead projected training versions right context tag features trained 
additionally argued advantage svm necessary codify complex gram features polynomial kernels succeed doing task 
base best tagger dual solution degree polynomial kernels linear separator primal setting 
kernels svm classification dual setting tagger simply fast running time 
particular tagging speed words second hours needed tag word test set reported 
training slower time required training word set hours probably due way select training examples pos 
time preparing document brought attention reports best results wsj corpus date single tagger 
refer toutanova tagger cyclic dependency network 
tagger achieves accuracy values wsj corpus training set words allows explicitly model left right context features sequence tagging scheme lexicalized 
overfitting extremely large feature spaces induced avoided model regularization 
results tagger partly due outstanding recognition unknown words accuracy values 
treatment unknown words bit tricky including named entity detector ad hoc feature patterns observation errors tagger 
fact may cause pos tagger highly wsj dependent 
regarding efficiency comments included 
training time best model said hours ghz processor iterations minutes iteration tagging times reported 
svm pos tagger suitable real applications provides balance properties nlp tools simplicity flexibility high performance efficiency 
step plan re implement tagger significantly increase efficiency provide software package public regarding study svm approach pos tagging issues deserve investigation 
learning model unknown words experimented think clearly improved 
second applied simplest greedy left right tagging scheme 
svm predictions converted probabilities natural extension consider sentence level tagging model probability sentence assignment maximized 
exploring possibility simplifying models posteriori feature filtering weight vector 
simplification tagging speed accuracy 
experiments models trained words indicate eliminating features lower weights hurt performance see 
little improvement obtained discarding dimensions 
size models may reduced 
particular competitive accuracy obtained discarding dimensions 
interestingly discard dimensions accuracy falls 
observations hold test validation sets prototype version tagger public web address www lsi upc es nlp html 
dimension reduction test set token accuracy behaviour test set reducing size weight vector dimensions discarding weights closer zero 
surely opening avenue increasing tagging speed 
authors want anonymous reviewers valuable comments suggestions order prepare final version 
research partially funded spanish ministry science technology projects hermes tic tic european lc star ist catalan research department consolidated research group abney abney schapire singer 
boosting applied tagging pp attachment 
proceedings emnlp vlc 
brants brants 
tnt statistical part speech tagger 
proceedings sixth anlp 
brill wu brill wu 
classifier combination improved lexical disambiguation 
proceedings 
brill brill 
transformation error driven learning natural language processing case study part speech tagging 
computational linguistics 
collins collins 
discriminative training methods hidden markov models theory experiments perceptron algorithms 
proceedings th emnlp conference 
cristianini shawe taylor cristianini shawe taylor 
support vector machines 
cambridge university press 
daelemans daelemans zavrel 
mbt memory part speech tagger generator 
proceedings th workshop large corpora 
halteren van halteren zavrel daelemans 
improving data driven tagging system combination 
proceedings coling acl 
lee lee tsujii rim 
part speech tagging hidden markov model assuming joint independence 
proceedings th annual meeting acl 
rodr iguez rodr iguez 
automatically acquiring language model pos tagging decision trees 
proceedings second conference 
rodr iguez 
improving pos tagging machine learning techniques 
proceedings emnlp vlc 
nakagawa nakagawa matsumoto 
unknown word guessing part speech tagging support vector machines 
proceedings sixth natural language processing pacific rim symposium 
ratnaparkhi ratnaparkhi 
maximum entropy part speech tagger 
proceedings st emnlp conference 
harper harper 
secondorder hidden markov model part speech tagging 
proceedings th annual meeting acl 
toutanova manning toutanova manning 
enriching knowledge sources maximum entropy part speech tagger 
proceedings emnlp vlc 
toutanova toutanova klein manning 
feature rich part speech tagging cyclic dependency network 
proceedings hlt naacl 
weischedel weischedel schwartz meteer ramshaw 
coping ambiguity unknown words probabilistic models 
computational linguistics 
