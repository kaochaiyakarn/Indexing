learning approximate dynamic programming scaling real world learning approximate dynamic programming scaling real world edited si andy barto warren powell donald wunsch wiley interscience publication john wiley sons new york chichester brisbane singapore toronto contents concurrency partial observability sridhar mahadevan mohammad georgios theocharous background spatiotemporal abstraction markov processes concurrency partial observability summary hierarchical approaches concurrency partial observability sridhar mahadevan mohammad georgios theocharous computer science department mit laboratory cs umass edu ai mit edu university massachusetts amherst cambridge ma editor summary chapter authors summarize research hierarchical probabilistic models decision making involving concurrent action multiagent coordination hidden state estimation stochastic environments 
hierarchical model learning concurrent plans described observable single agent domains combines compact state representations temporal process abstractions determine parallelize multiple threads activity 
hierarchical model multiagent coordination primitive joint actions joint states hidden 
high level coordination learned exploiting task structure greatly speeds convergence abstracting low level steps need synchronized 
hierarchical framework hidden state estimation action multi resolution statistical modeling past history observations actions 
despite decades research models decision making artificial systems remain significantly human level performance tasks involving planning execution concurrent actions tasks perceptual limitations require remembering past observations problems behavior agents needs taken account 
driving see human concurrency partial observability activities involve simultaneously challenges 
date general framework jointly addresses concurrency multiagent coordination hidden state estimation developed essential components framework understood 
chapter provide broad overview previous research hierarchical models concurrency partial observability 
humans learn carry multiple concurrent activities abstraction levels acting concert humans 
illustrates familiar everyday example drivers learn observe road signs control steering manage engage activities operating radio carrying conversation 
concurrent planning coordination essential important engineering problems flexible manufacturing team machines scheduling robots transport parts factories 
tasks involve hard computational problem sequence multiple overlapping interacting parallel activities accomplish long term goals 
problem difficult solve general requires learning mapping noisy incomplete perceptions multiple temporally extended decisions uncertain outcomes 
impressive feat humans able reliably solve problems driving relatively little effort modest amounts training 
keep lane change lanes accelerate brake coast answer tune fm classical music station fig 
driving human activities illustrating principal challenges addressed chapter concurrency partial observability 
drive successfully humans execute multiple parallel activities coordinating actions taken drivers road memory deal limited perceptual abilities 
chapter summarize past research hierarchical approach concurrent planning coordination stochastic single agent multiagent environments 
overarching theme efficient solutions challenges developed exploiting multi level temporal spatial abstraction actions states 
framework elaborated parts 
hierarchical model learning concurrent plans simplicity assumed agents act fully observe state underlying process 
key idea combining compact state representations temporal process abstractions agents learn parallelize multiple threads activity 
hierarchical model multiagent coordination described primitive joint actions joint states may hidden 
partial observability lower level actions blessing allows agents speedup convergence abstracting low level steps need synchronized 
hierarchy memory semi markov chains markov chains hierarchical hmms controllability hidden markov models semi markov decision processes markov decision processes background partially observable smdps partially observable mdps fig 
spectrum markov process models dimensions agents choice action states observable hidden actions unit time single step time varying multi step 
cal approach state estimation multi resolution statistical modeling past history observations actions 
proposed approaches build common markov decision process mdp modeling paradigm summarized section 
previous algorithms largely focused sequential compositions closed loop programs 
earlier mdp approaches learning multiagent coordination ignored hierarchical task structure resulting slow convergence 
previous finite memory partially observable mdp methods state estimation flat representations scale poorly long experience chains large state spaces 
algorithms summarized chapter address limitations previous new spatiotemporal abstraction approaches learning concurrent closed loop programs task level coordination presence significant perceptual limitations 
background probabilistic finite state machines popular paradigm modeling sequential processes 
representation interaction agent environment represented finite automata states partition past history interaction equivalence classes actions cause probabilistic concurrency partial observability transitions states 
state sufficient statistic computing optimal best actions meaning past history leading state abstracted 
assumption usually referred markov property 
markov processes mathematical foundation current reinforcement learning decision theoretic planning information retrieval speech recognition active vision robot navigation 
chapter interested abstracting sequential markov processes strategies state aggregation decomposition temporal abstraction 
state decomposition methods typically represent states collections factored variables simplify automaton eliminating useless states 
temporal abstraction mechanisms example hierarchical reinforcement learning encapsulate lower level observation action sequences single unit levels 
unified algebraic treatment abstraction markov decision processes covers spatial temporal abstraction reader referred 
illustrates markov process models arranged cube axes represent significant dimensions models differ 
detailed description model scope chapter provide brief descriptions models section basic mdp model 
markov decision process mdp specified set states set allowable actions state transition function specifying state distribution ss action 
reward cost function specifies expected reward carrying action state solving mdp requires finding optimal mapping policy maximizes long term cumulative sum rewards usually discounted factor expected average reward step 
classic result mdp exists stationary deterministic optimal policy solving nonlinear set equations state successive approximation method called value iteration max ss mdps applied real world domains ranging robotics engineering optimization game playing 
domains model parameters rewards transition probabilities unknown need estimated samples generated agent exploring environment 
qlearning major advance direct policy learning obviates need model estimation 
bellman optimality equation reformulated action values represent value non stationary policy doing action acting optimally 
learning eventually finds optimal policy asymptotically 
required scaling learning large problems abstraction key components 
factored approaches representing value functions may key scaling large problems 
spatiotemporal abstraction markov processes spatiotemporal abstraction markov processes discuss strategies hierarchical abstraction markov processes including temporal abstraction spatial abstraction techniques 
semi markov decision processes hierarchical decision making models require ability represent lower level policies primitive actions primitive actions level robot navigation task go forward action comprised lower level actions moving corridor avoiding obstacles 
policies primitive actions semi markov level simply treated single step actions coarser time scale states 
semi markov decision processes smdps preferred language modeling temporally extended actions extended review smdps hierarchical action models see 
markov decision processes mdps time transitions may time units depend transition 
smdp defined tuple finite set states set actions state transition matrix defining single step transition probability effect action reward function 
continuous time smdps function giving probability transition times state action pair natural termination 
transitions decision epochs 
smdp represents snapshots system decision points called natural process describes evolution system times 
discrete time smdps transition distribution written specifies expected number steps action take terminating naturally state starting state continuous time smdps probability decision epoch occurs time units agent chooses action state decision epoch 
learning generalizes nicely discrete continuous time smdps 
learning rule discrete time discounted smdps qt qt max qt action initiated state lasted steps terminated state generating total discounted sum rewards frameworks hierarchical reinforcement learning proposed variants smdps including options maxq 
discuss detail section 
concurrency partial observability fig 
example hierarchical hidden markov model 
leaf nodes produce observations 
internal nodes viewed generating sequences observations 
hierarchical hidden markov models hidden markov models hmms widely probabilistic model representing time series data speech 
mdp states perceivable agent receives observation viewed generated stochastic process function underlying state hmms widely applied time series problems ranging speech recognition information extraction bioinformatics 
mdps hmms provide direct way representing higher level structure practical problems 
example hmm spatial representation indoor environments typically environments higher order structures corridors floors explicit underlying hmm model 
case mdps practical problems parameters underlying hmm learned samples 
popular method learning hmm model baum welch procedure special case general expectation maximization em statistical inference algorithm 
elegant hierarchical extension hmms proposed 
hhmm generalizes standard hidden markov model allowing hidden states represent stochastic processes 
hhmm visualized tree structure see types states production states leaves tree emit observations internal states unobservable hidden states spatiotemporal abstraction markov processes represent entire stochastic processes 
production state associated observation vector maintains distribution functions observation defined model 
internal state associated horizontal transition matrix vertical transition vector 
horizontal transition matrix internal state defines transition probabilities children 
vertical transition vectors define probability internal state activate children 
internal state associated child called state returns control parent 
states produce observations activated vertical transition parent 
shows graphical representation example hhmm 
hhmm produces observations follows 
current node root chooses activate children vertical transition vector root children 

child activated product state produces observation observation probability output vector 
transitions state level 
state reached transition state control returned parent state 

child state chooses activate children 
state waits control returned child state 
transitions state level 
resulting transition state control returned parent state 
basic inference algorithm hierarchical hmms modification algorithm stochastic context free grammars runs length observation sequence 
murphy developed faster inference algorithm hierarchical hmms converting hhmm dynamic bayes network 
factored markov processes domains states comprised collections objects modeled multinomial real valued variable 
example driving state car include position accelerator brake radio wheel angle assume agent environment interaction modeled factored semi markov decision process state space spanned cartesian product random variables xn xi takes values finite domain dom xi 
action primitive single step action closed loop policy primitive actions 
dynamic bayes networks dbns popular tool modeling transitions factored mdps 
denote state variable xi time variable time 
denote set underlying primitive actions 
action action network specified layer directed acyclic node graph nodes xt xt xt xt concurrency partial observability denotes parents graph 
transition probability xt xt defined xt xt wi wi vector elements values xt 
shows popular toy problem called taxi problem taxi inhabits grid world 
episodic problem taxi maximum fuel capacity units placed episode randomly selected location randomly selected amount fuel ranging units 
passenger arrives randomly locations marked ed lue select random destination states transported 
taxi go location passenger source pick passenger move destination location destination put passenger 
episode ends passenger transported desired destination taxi runs fuel 
treating taxi position passenger location destination fuel level state variables represent problem factored mdp state variables values explained 
shows factorial representation taxi domain pickup actions 
relatively straightforward represent factored mdps easy solve general solution optimal value function factored 
detailed discussion issue scope article popular strategy construct approximate factored value function linear summation basis functions see 
factored representations useful finding approximate solutions quickly learning factored transition model time 
taxi task illustrated idea investigated express factored transition probabilities mixed memory factorial markov model 
transition probability edge graph represented weighted mixture distributions weights learned expectation maximization algorithm 
precisely action model represented weighted sum cross transition matrices xt xjt associated conditional probability table cpt ij xit parameters ij elementary transition matrices parameters positive numbers satisfy action number state variables 
number free parameters representation opposed ak non compact case 
parameters measure contribution different state variables previous time step state variable current state 
problem completely factored identity matrix ith component independent rest 
amount factorization exists environment different components time step influence spatiotemporal abstraction markov processes th component 
cross transition matrices ij provide compact way parameterize influences 
taxi position passenger location destination fuel taxi position passenger location destination fig 
taxi domain instance factored markov process actions represented compactly dynamic bayes networks 
shows learning factored mdp compared table mdp averaged episodes steps 
point graph represents rms error learned model ground truth averaged states actions 
model error drops quickly early stages learning 
theoretically tabular maximum likelihood approach estimates transition probability ratio transitions states versus number transitions state eventually learn exact model pair states action executed infinitely 
factored approach uses mixture weighted representation able generalize quickly novel states 
rms error taxi domain tabular maximum likelihood steps fig 
comparing factored versus tabular model learning performance taxi domain 
fuel room room concurrency partial observability structural decomposition markov processes room room mts task available action set task task fig 
state action decomposition markov processes 
related techniques decomposition large mdps explored illustrated 
simple decomposition strategy split large mdp sub mdps interact weakly 
example weak interaction navigation interaction sub mdps states connect different rooms 
strategy decompose large mdp set available actions air campaign planning problem conversational robotics 
intriguing decomposition strategy sub mdps interact shared parameters 
transfer line optimization problem manufacturing example parametric decomposition 
concurrency partial observability section summarizes research exploiting spatiotemporal abstraction produce improved solutions difficult problems sequential decisionmaking learning plans involving concurrent action multiagent coordination memory estimate hidden state 
hierarchical framework concurrent action describe probabilistic model learning concurrent plans temporally extended actions 
notion concurrent action formalized general way capture situations single agent execute multiple parallel processes multi agent case agents act parallel 
concurrent action model cam defined set states set primary actions transition probability distribution power set primary actions set natural numbers reward function mapping 
concurrency partial observability concurrent action viewed set primary actions called multi action primary action single step action temporally extended action modeled closed loop policy single step actions 
illustrates toy example concurrent planning 
general problem follows 
agent set primary actions viewed fixed previously learned subroutine choosing actions subspace state space 
goal agent learn construct closed loop plan policy allows multiple concurrent subroutines executed parallel sequence achieve task hand 
multiple primary actions executed concurrently joint semantics defined 
concurrency facilitated assuming states atomic structured collection discrete continuous variables effect actions sets variables captured compact representation dynamic bayes net dbn 
agent water trap goal stochastic primitive actions left right fail times passing water trap fail times passing water trap holding keys multi step navigation options room hallways single step op option stochastic primitive actions keys get key key nop putback key multi step key options pickup key key drops keys times passing water trap holding keys fig 
grid world problem illustrate concurrent planning agent subroutines getting door interior room state opening locked door 
learn shortest path goal concurrently combining subroutines 
agent reach goal quickly learns parallelize subroutine retrieving key reaches locked door 
retrieving key early counterproductive drop probability 
multiple concurrent primary actions may terminate synchronously notion decision epoch needs generalized 
example decision epoch occur actions currently running terminates 
refer termination condition left 
alternatively decision epoch defined occur actions currently running terminate refer tall condition middle 
design termination schemes combining tall example termination scheme called terminates termination scheme allows st concurrency partial observability primary actions terminate naturally keep executing initiating new primary actions going useful right 
dn dn mult opt ion int er ed st op mult opt ion dn dn st dn dn cur ent mult opt ion mult opt ion cont un fig 
left scheme 
middle tall termination scheme 
right termination scheme 
concreteness describe concurrent planning framework primary actions represented options 
treatment restricted options discrete time smdps deterministic policies main ideas extend readily hierarchical formalisms continuous 
formally option consists components policy termination condition initiation set denotes set states option initiated 
state option taken primitive actions selected terminates 
option markov option policy initiation set termination condition depend stochastically current state option semi markov policy initiation set termination condition dependent prior history option initiated 
example option exit room grid world environment shown states different locations room markov option location direction move get door computed current state 
hierarchical policy primary actions options defined follows 
markov policy options set options selects option time function st 
option initiated st terminates random time state st termination condition process repeats st multistep state transition dynamics options defined discount factor weight probability transitioning 
po denote probability option initiated state terminates state steps 
po note transition model stochastic matrix distributions sum 
multi step models options rewards known optimal hierarchical plans solving generalized bellman equation options similar equation 
definition termination event tall result holds 
concurrency partial observability theorem markov decision process set concurrent markov options defined decision process selects multi actions executes termination termination condition forms semi markov decision process 
proof requires showing state transition dynamics rewards concurrent action defines semi markov decision process 
significance result smdp learning methods extended learn concurrent plans model 
extended smdp learning algorithm learning plan concurrent actions updates multi action value function decision epoch multi action taken state terminates specific termination condition max os denotes number time steps initiation multi action state termination state denotes cumulative discounted reward period 
result algorithm simple grid world problem shown 
illustrates difference performance different termination conditions tall 
median trials steps goal sequential options concurrent options concurrent options optimal concurrent options cont fig 
graph compares smdp technique learning concurrent plans various termination conditions slower get door pickup key sequential plan learner 
concurrent learners outperform sequential learner choice termination affects speed quality final plan 
performance concurrent action model depends termination event defined model 
termination event trades optimality learned plan fast converges optimal policy 
seq denote optimal policy primary actions executed sequentially trial concurrency partial observability multi action policies continue multi action policies multi action policies tall policies sequential actions fig 
comparison policies multi actions sequential primary actions different termination schemes 
termination construct tall termination construct respectively 
continue represent policy learned termination construct 
intuitively models termination construct imposes frequent multi action termination tend articulate frequently perform optimally 
due interruption may converge slowly optimal behavior 
definition termination construct prove theorem theorem concurrent action model set termination schemes tall partial ordering holds optimal policy optimal policy tall policy optimal sequential policy seq continue denotes partial ordering relation policies 
illustrates results defined theorem 
optimal multi action policies tall multi action policies dominate respect partial ordering relation defined policies optimal policies sequential case 
furthermore policies multi actions dominate optimal multi action policies tall termination scheme dominated optimal multi action policies termination scheme 
learning multiagent task level coordination strategies second case study uses hierarchical abstraction design efficient learning algorithms cooperative multiagent systems 
illustrates multiagent automated guided vehicle agv scheduling task agv agents maximize performance task learn coordinate 
key idea coordination skills learned efficiently agents learn synchronize hierarchical representation task structure 
particular agv learning response low level primitive actions agv agents instance agv goes forward agv learn high level coordination knowledge utility agv delivering concurrency partial observability material machine agv delivering assembly machine 
proposed approach differs significantly previous cooperative multiagent reinforcement learning hierarchical task structure accelerate learning concurrent temporally extended actions 
pick station drop station machine load parts assemblies unload load put nav forward left right root 
dm dm da da dmi deliver material station dai deliver assembly station navigate load station navigate drop station fig 
multiple automated guided vehicle agv optimization task 
agv agents shown carry raw materials finished parts machines warehouse 
task graph problem shown right hand side 
general approach learning task level coordination extend concurrency model joint state action space base level policies remain fixed 
extension approach agents learn coordination skills base level policies simultaneously 
hierarchical multiagent reinforcement learning algorithm described implemented hierarchical reinforcement learning formalisms sake clarity maxq value function decomposition approach 
decomposition storing value function distributed manner nodes task graph 
value function computed demand querying lower level subtask nodes high level task node needs evaluated 
task decomposed subtasks desired level details task graph constructed 
illustrate idea multiagent agv scheduling problem 
task decomposed subtasks resulting task graph shown 
agv agents task graph homogeneous agents need learn skills 
subtask deliver parts machine navigation drop station perform load put action 
second agents need learn order subtasks instance go pick station machine pick assembly heading unload station 
agents need learn coordinate agents agv deliver parts concurrency partial observability machine agv deliver assemblies machine 
distinguish learning approaches 
selfish case agents learn task graph attempt coordinate 
cooperative case coordination skills agents learned joint actions level immediately root task 
necessary generalize maxq decomposition original sequential single agent setting concurrent multiagent coordination problem 
call extension maxq cooperative maxq 
algorithm agent learns joint action values communicating agents high level subtasks doing 
high level tasks take long time complete communication needed fairly infrequently significant advantage flat methods 
advantage agents learn coordination skills level actions allows increased cooperation skills agents get confused low level details 
addition agent local state information ignorant agent state 
keeping track just local information greatly simplifies underlying reinforcement learning problem 
idea cases state agent roughly estimated just knowing high level action performed agent 

sn 
denote joint state concurrent action si local state ai action performed agent joint action value function represents value concurrent action joint state context executing parent task maxq decomposition function relies key principle reward function parent task essentially value function child subtask 
principle extended joint concurrent action values shown 
salient feature cooperative maxq algorithm top level level immediately root lower levels hierarchy configured store completion function values joint actions agents 
completion function expected cumulative discounted reward completing parent task finishing concurrent action invoked state joint concurrent value function approximated agent local state si si si composite action si sj primitive action action value function agent local state si defined si ai si si term equation ai si refers discounted sum rewards received agent performing action ai local state si 
second term si completes sum accounting rewards earned completing parent task finishing subtask ai 
completion function updated algorithm sample values smdp learning rule 
note correct concurrency partial observability action value approximated considering local state si ignoring effect concurrent actions ak agents agent performing action ai 
practice human designer configure task graph store joint concurrent action values highest lower highest needed level hierarchy 
illustrate decomposition learning multiagent coordination agv scheduling task joint action values restricted highest level task graph root get value function decomposition agv root dm da da dm dm root dm da da dm represents value agv performing task dm context root task agv agv agv executing da da dm 
note value decomposed value agv performing dm subtask completion sum remainder task done agents 
compares performance speed cooperative maxq algorithm learning algorithms including single agent maxq selfish multiagent maxq known agv scheduling heuristics come serve highest queue nearest station 
throughput system operating multi agent maxq selfish multi agent maxq single agent maxq time start simulation sec throughput system operative multi agent maxq come served heuristic highest queue heuristic nearest station heuristic time start simulation sec fig 
compares performance cooperative maxq algorithm learning methods including single agent maxq selfish multiagent maxq known agv scheduling heuristics 
throughput measured terms number finished assemblies deposited unload station unit time 
hierarchical memory agents learn act concurrently real world environments true state environment usually hidden 
address issue need combine methods learning concurrency coordination methods estimating hidden state 
explored multiscale memory models :10.1.1.28.3557
hierarchical concurrency partial observability suffix memory hsm generalizes suffix tree model smdp temporally extended actions 
suffix memory constructs state estimators finite chains observation action reward triples 
addition extending suffix models smdp actions hsm uses multiple layers temporal abstraction form longer term memories levels 
illustrates idea robot navigation simpler case linear chain tree model investigated 
important side effect agent look back steps back time ignoring exact sequence low level observations actions 
tests robot navigation domain showed hsm outperformed flat suffix tree methods hierarchical methods memory 
abstraction level navigation abstraction level traversal abstraction level primitive corner junction dead 
fig 
hierarchical suffix memory state estimator robot navigation task 
navigation level observations decisions occur intersections 
lower corridor traversal level observations decisions occur corridor 
level agent constructs states past experience similar history shown shadows 
partially observable mdps theoretically powerful finite memory models past pomdps studied flat models learning planning algorithms scale poorly model size 
developed new hierarchical pomdp framework termed pomdps see extending hierarchical hidden markov model hhmm include rewards multiple entry exit points states temporally extended actions 
pomdps represented dynamic bayesian networks similar way hhmms represented dbns 
shows dynamic bayesian net representation pomdps 
model differs model described basic ways presence action nodes fact exit nodes longer binary 
particular navigation example shown exit node xt take possible values representing exit north exit east exit south exit west exit 
xt exit horizontal transition concrete level state required remain 
xt exit enter new state state vertical transition new concurrency partial observability fig 
state transition diagram hierarchical pomdp model corridor environments 
large ovals represent states small solid circles represent entry states small hollow circles represent exit states 
small circles arrows represent production states 
arcs represent non zero transition probabilities follows dotted arrows concrete states represent concrete horizontal transitions dashed arrows exit states represent horizontal transitions solid arrows entry states represent vertical transitions 
fig 
level hpomdp represented dbn 
concrete state 
new concrete state depends new state previous exit state xt 
precisely define conditional probability distributions type node dbn follows nodes xt exit root sx concurrency partial observability root sx state representation hpomdp model defines transition probability state exit state state entry state defines type entry exit state north east west south 
parent state transition model 
concrete nodes xt exit sx sx defines probability vertical transition state entry state type concrete state exit nodes xt sx sx transition probability production state state exit state type sensor nodes ot probability perceiving observation sth node state action important differences hierarchical hmms pomdps flat models results inference 
hierarchical model transition state time zero state able produce part remaining observations actions sequence 
inference algorithm state representation hhmms pomdps achieves doing inference possible subsequences observations different states leads time number states level hierarchy depth hierarchy 
dbn representation achieve result cubic time algorithms asserting sequence finished 
particular implementation assert time slice sequence finished uniform probability exit orientations 
dbn representation apply standard bayes net inference algorithm junction tree perform filtering smoothing take worse case time 
empirically depending size cliques formed shown 
due cubic time complexity em algorithm developed various approximate training techniques reuse training submodels trained separately combined hierarchy selective training selected parts model trained sequence 
methods require knowledge part model data outperformed flat em algorithm terms fit test data robot localization accuracy capability structure learning summary higher levels abstraction 
dbn representation allows longer training sequence 
show hierarchical model requires data training flat model illustrate combining hierarchical factorial representations outperforms hierarchical flat models 
addition advantages flat methods model learning pomdps inherent advantage planning 
belief states computed different levels tree uncertainty higher levels robot sure corridor exactly low level state 
number heuristics mapping belief states temporally extended actions move corridor provide performance robot navigation state mls heuristic assumes agent state corresponding peak belief state distribution 
heuristics better pomdps applied multiple levels probability distributions states usually lower entropy see 
detailed study pomdp model application robot navigation see 
normalized entropies normalized entropies global levels number steps global entropy entropy fig 
plot shows sample robot navigation run trace right positional uncertainty measured belief state entropy corridor level product state level 
spatiotemporal abstraction reduces uncertainty requires frequent decision making allowing robot get goals initial positional information 
summary chapter hierarchical models decision making involving concurrent actions multiagent coordination hidden state estimation common thread spanned solutions challenges multi level temporal spatial abstraction actions states exploited achieve effective solutions 
approach phases hierarchical model learning concurrent plans observable single agent domains 
concurrency partial observability concurrency model combined compact state representations temporal process abstractions formalize concurrent action 
multiagent coordination addressed hierarchical model primitive joint actions joint states abstracted exploiting task structure greatly speeds convergence lowlevel steps ignored need synchronized 
hierarchical framework hidden state estimation multi resolution statistical models past history observations actions 
acknowledgments research supported part national science foundation knowledge distributed intelligence program defense advanced research projects agency mars distributed robotics robot programs michigan state university university massachusetts amherst 
bibliography 
barto mahadevan 
advances hierarchical reinforcement learning 
discrete event systems theory applications 

boutilier dearden goldszmidt 
stochastic dynamic programming factored representations 
artificial intelligence 

crites barto 
elevator group control multiple reinforcement learning agents 
machine learning 

dean givan 
model minimization markov decision processes 
proceedings aaai 

dean kanazawa 
model reasoning persistence causation 
computational intelligence 

dietterich 
hierarchical reinforcement learning maxq value function decomposition 
international journal artificial intelligence research 

fine singer tishby 
hierarchical hidden markov model analysis applications 
machine learning july 

freitag mccallum 
information extraction hmms shrinkage 
proceedings aaai workshop machine learning extraction 

mahadevan 
continuous time hierarchical reinforcement learning 
proceedings eighteenth international conference machine learning 

hernandez mahadevan 
hierarchical memory reinforcement learning 
proceedings neural information processing systems 


statistical methods speech recognition 
mit press 

karplus barrett hughey 
hidden markov models detecting remote protein homologies 
bibliography 
knoblock 
analysis 
james hendler editor artificial intelligence planning systems proceedings international conference aips pages college park maryland usa 
morgan kaufmann 

koenig simmons 
xavier robot navigation architecture partially observable markov decision process models 
kortenkamp bonasso murphy 
editors ai mobile robots case studies successful robot systems 
mit press 

koller parr 
computing factored value functions policies structured mdps 
th international joint conference artificial intelligence ij cai pages 

littman 
markov games framework multi agent reinforcement learning 
proceedings eleventh international conference machine learning pages 

mahadevan connell 
automatic programming behavior robots reinforcement learning 
artificial intelligence 
appeared originally ibm tr rc dec 

mahadevan das 
self improving factory simulation continuous time average reward reinforcement learning 
proc 
th international conference machine learning pages 
morgan kaufmann 

mahadevan 
hierarchical multiagent reinforcement learning 
proc 
th international conference autonomous agents pages 
acm press 

mccallum 
reinforcement learning selective perception hidden state 
phd thesis university rochester 

meuleau hauskrecht kim peshkin kaelbling dean boutilier 
solving large weakly coupled markov decision processes 
proceedings conference uncertainty artificial intelligence 

mahadevan 
reinforcement learning model selective visual attention 
fifth international conference autonomous agents 

murphy 
linear time inference hierarchical hmms 
proceedings neural information processing systems 

nourbakhsh powers birchfield 
office navigation robot 
ai magazine 

parr 
hierarchical control learning markov decision processes 
phd thesis university california berkeley 
bibliography 
pineau roy thrun 
hierarchical approach pomdp planning execution 
workshop hierarchy memory reinforcement learning icml williams college ma june 

prieditis 
machine discovery admissible heuristics 
proceedings twelfth international joint conference artificial intelligence pp 


puterman 
markov decision processes 
wiley interscience new york usa 

ravindran barto 
smdp homomorphisms algebraic approach abstraction semi markov decision processes 
proceedings eighteenth international joint conference artificial intelligence 

mahadevan 
decision theoretic planning concurrent temporally extended actions 
th conference uncertainty artificial intelligence 

mahadevan 
incremental learning factorial markov decision processes 
preparation 

russell norvig 
artificial intelligence modern approach 
prentice hall 

saul jordan 
mixed memory markov models decomposing complex stochastic processes mixture simpler ones 
machine learning 

shatkay kaelbling 
learning topological maps weak local odometric information 
proceedings fifteenth international joint conference artificial intelligence pages 

sugawara lesser 
learning improve coordinated actions cooperative distributed problem solving environments 
machine learning 

sutton barto 
reinforcement learning 
mit press cambridge ma 

sutton precup singh 
mdps semi mdps framework temporal abstraction reinforcement learning 
artificial intelligence 

tan 
multi agent reinforcement learning 
cooperative agents 
proceedings tenth international conference machine learning pages 

tesauro 
practical issues temporal difference learning 
machine learning 
bibliography 
theocharous 
hierarchical learning planning partially observable markov decision processes 
phd thesis michigan state university 

theocharous mahadevan 
approximate planning hierarchical partially observable markov decision robot navigation 
proceedings ieee international conference robotics automation icra 

theocharous mahadevan 
learning hierarchical partially observable markov decision robot navigation 
proceedings ieee international conference robotics automation icra 

theocharous murphy kaelbling 
representing hierarchical pomdps dbns multi scale robot localization ijcai workshop reasoning uncertainty robotics 

wang mahadevan 
hierarchical optimization policy coupled semi markov decision processes 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 

watkins 
learning delayed rewards 
phd thesis king college cambridge england 

weiss 
multiagent systems modern approach distributed artificial intelligence 
mit press cambridge ma 
