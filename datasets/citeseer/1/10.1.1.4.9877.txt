annealed competition experts segmentation classification switching dynamics klaus pawelzik jens kohlmorgen klaus robert muller institut fur theoretische physik sfb universitat frankfurt frankfurt germany gmd german national research center computer science berlin germany department mathematical engineering information physics university tokyo ku tokyo japan method unsupervised segmentation data streams originating different unknown sources alternate time 
architecture consisting competing neural networks 
memory included order resolve ambiguities input output relations 
order obtain maximal specialization competition increased training 
method achieves perfect identification segmentation case switching chaotic dynamics input manifolds overlap input output relations ambiguous 
small dataset needed training 
applications time series complex systems demonstrate potential relevance approach time series analysis short term prediction 
neural networks provide frameworks representation relations data 
especially fields classification time series prediction neural networks corresponding author email salk edu temporary salk institute cnl box san diego ca 
substantial contributions 
important prerequisite successful application systems certain uniformity data 
analysis data series stationarity assumed assumed relations remain constant time 
contrary data originate different sources underlying system switches dynamics standard approaches simple multi layer perceptrons fail represent underlying input output relations 
time series originate kinds systems physics biology engineering 
phenomena kind include speech rabiner brain data pawelzik dynamical systems switch attractors kaneko 
method segmentation data streams prior knowledge sources 
consider case different inputoutput samples generated number unknown functions alternate 
task determine functions respective attributions time series functions segmentation considered unknown determined simultaneously correct segmentation unsupervised manner 
mixtures experts architecture proposed jacobs 
potentially offers solution problem represent different functions respective experts 
problems applying mixture experts architecture task identifying alternating sources 
problem arises gating experts input general underlying sources overlapping input domains 
order solve problem ensemble expert networks competition depends relative performance input 
way introducing competition relates clustering vector quantization mclachlan basford contrast mixtures experts architecture uses input dependent gating network jacobs 
sources overlapping arguments problem arises functions may intersect 
case input output pairs identical different functions 
show intersections induce additional ambiguities problem resolved imposing additional constraints 
learning rule performing disambiguation derived simple assumption memory switching process low switching rate 
assumption allows train system experts small data sets require statistics switching events 
particular method identify time series number data just suffices characterize respective functions 
approach provide analysis dynamics switching cacciatore nowlan bengio frasconi discuss relation approaches section 
unique segmentation sample assigned expert 
easily achieved considering respective best performing expert 
hard competition training get stuck local minima simple cases overcome sample dependent ad hoc initializations kohlmorgen muller muller 
general approach propose anneal competition networks training see yuille 
show method networks successively specialize hierarchical manner series phase transitions effect analysed context clustering rose 

section introduce approach section demonstrate features method example alternating functions unit interval intersect 
example input output samples dynamics chaotic maps experts correspond predictors 
relates method common techniques system identification athans time series prediction tong lim 
section apply method benchmark data santa fe time series prediction competition weigend gershenfeld application demonstrates approach may substantially improve predictions time series opens new perspectives signal classification discuss section 
unmixing experts data originating different sources subject ambiguity 
input output relations considered interdependent reasons 
input domains may overlap 
impossible single network map inputs different outputs extra information 
second input output different sources identical subset data 
case information input output pairs required order reassign data sources 
illustrating basic ideas underlying approach discuss extreme case completely overlapping input manifolds 
example input output pairs time step choice maps gamma logistic map gamma tent map ffi double logistic map ffi double tent map 
ffi denotes iteration 
set get chaotic time series fx see fig 
maps alternately input determine appropriate output representation underlying relations necessarily contain division subtasks 
data sets gating network depends input jacobs necessarily fail 
training data drawn chaotic return maps points map 
new map chosen recursions 
values resulting time series shown 
approach adapt set predictors weighted relative performance 
optimal choice function approximators depends specific application 
radial basis function networks rbfn moody darken type moody darken offer fast learning method 
train weights network performing gradient descent deltaw gamma squared errors gamma weighting coefficient corresponds relative probability contribution network constrained 
approach differs previous way incorporate memory switching process 
start assuming outputs distributed gaussians gammafi furthermore assume system switch state time step alternates different subsystems low rates ij weak assumption memory switching process derive simple bias probabilities probability subsequence oe delta gamma delta gamma delta delta delta time series generated particular sequence delta gamma delta delta functions oe delta delta delta delta delta gamma delta delta denotes corresponding sequence errors 
bayes rule case gives delta oe delta delta delta delta delta delta delta delta sum runs possible sequences delta equation strongly simplified case low bound switching rate short sequences small probability contain switching event delta result hard competition prior annealing proper initialization intended net grabbed similar return maps distinction maps longer possible prediction error maps remains high 
annealing inclusion memory allows creation maps jump target map axis 
information consecutive data points belong high probability dynamics utilized 
case neglect sequences contain set delta components equal 
remaining sequences considered equiprobable maximum entropy delta obtain eq delta delta gamma delta gamma delta gamma delta gamma probability generated subsequence oe delta eq provides estimate weighting coefficients gammafi delta gamma delta gamma gammafi delta gamma delta gamma note result equivalently intuitively derived maximizing log likelihood observation assumptions 
delta reduces mixtures gaussians mclachlan basford equivalent mixture experts jacobs gating network 
eq simply low pass filtered errors plain order include memory originates low switching rate 
drastic simplification memory probability sequences length delta include switching led box type filter replaced exponential order model switching probabilities realistically 
knowledge characteristics time series eq simplest time computationally expensive way include memory 
heuristically eq analogous evolutionary inertia predictor performed better competitors advantage temporally adjacent data points 
helps regularize data ambiguities 
example chaotic maps ambiguities emerge intersections additional information required decide branches function belong 
purpose segmentation desirable choose fi large 
consider fi corresponds hard competition winner takes guarantees unambiguous segmentation kohlmorgen muller muller 
hard competition right lead sufficient diversification predictors 
final result general depends choice initial parameters may lead local minima likelihood mixing maps occur see fig 

solve initialization problem increasing degree competition 
fi predictors equally share data training 
increasing fi enforces competition driving predictors specialization different subsets data 
diversification occurs particular temperatures fi network parameters separate abruptly resolving underlying structure detail 
phase transitions indicated drop mean squared error see fig described statistical mechanics formalism rose 
note careful decrease crucial fine differences underlying functions resolved 
yield weighted low pass filter eq 
applications switching chaos illustrate approach time series points chaotic maps introduced fig 
maps alternated iteration steps 
dynamical systems ergodic support distinguished basis arguments 
furthermore small rate guarantees large probability short sequences length contain alternations underlying system justifies simple method memory account setting delta eq 
note parameter crucial 
radial basis function networks moody darken type moody darken predictors decreased temperature fi smaller value temperature taken error saturated 
result shown fig 
error decreases phase transitions fig occur different underlying dynamics abruptly resolved detail fig 
relevant structures algorithm phase transitions occur little decrease error approaches zero 
find networks segmented time series exactly switching points drifted fig 
contribute removed changing performance method applied time series high dimensional chaotic systems simply replacing scalar argument vectors obtained method time delay embedding time series takens corresponding adaptation networks 
example high dimensional chaotic system take mackey glass delay differential equation dx dt gamma gamma gamma originally introduced model blood cell regulation mackey glass 
generated time series points switched delay parameter samples sampling rate chose second samples third 
increase difficulty problem noise added integration step turning system stochastic fig 
creation training set time series embedding dimension casdagli 
temp 
temp 
temp 
training test error annealing process indicate phase transitions 
maps learned rbfn process 
nets specialized dynamics nets dropped contribute segmentation error training phase transitions occurred fig indicating system detected different dynamical systems 
second transition prominent simpler networks 
leads sub optimal prediction results applied 
removal nets increase error significantly fig correctly indicates predictors completely describe source 
segmentation perfect fig 
performance convergence speed segmentation accuracy approach high dimensional mackey glass data better dimensional maps indicates higher dimensions segmentation identification easier possibly weaker overlap manifolds higher dimensions 
prediction assumption stationarity problematic cases data analysis 
approach provides diagnostic tool predictive solution problems shown maps learned predictors phase transitions 
final result training reached hard competition shown fig 
temp 
removed nets removed noisy mackey glass time series includes different dynamics segmentation task 
adiabatic evolution training error mackey glass data 
increase prediction error successively removing predictors training 
training performed predictors removed significant increase error 
remaining predictors specialized dynamics data indicated net shown 
non due random jumps system parameters 
section demonstrate relevance approach prediction time series 
stress obtain prediction solve problem predicting point time system probably switch state 
statistics switching included model obviously higher amount data necessary 
applied method prediction data set santa fe time series competition weigend gershenfeld 
scalar data set generated dimensional periodically driven dissipative dynamical system asymmetrical potential drift parameters 
rbf predictors predict data point preceding points embedding dimension 
training set restricted points data set keep computation time tolerable 
training finished prediction training data shared predictors 
prediction continuation data set simply done iterating particular predictor responsible generation latest training data 
predicted continuation compared true test set originally unknown participants competition 
method quite useful time steps see fig 
steps system presumably performs switch part potential construction foreseen approach switching statistics taken account 
tested ability method predict parts test set predictors performance time steps fig 
prediction fails system apparently jumps different state 
underlying system case stationary results demonstrate divide conquer useful strategy high dimensionality system complex form potential 
quantitative comparison winners santa fe competition zhang hutchinson weigend gershenfeld pp 
demonstrates power method 
authors applied stationary approach uses hours training time connection machine cm processors achieved prediction error rmse root mean squared error computed step predictions prediction broke 
compare prediction short episode find rmse better training took just half hours sun gx 
prediction solid line continuation data set dashed line competing predictors approach 
predictors decompose dynamics time series simpler prediction tasks predictor able predict certain segments data shown 
accuracy step predictions better result zhang hutchinson winners santa fe competition 
known example non stationary dynamics real world speech 
applied method predicting dynamics plain converted speech data 
find experts trained single sentence reliably segment signal unsupervised segmentation dynamics word recognition 
observe clear relation segmentation phonemes suspect requires careful choice experts models vocal tract details refer muller 
summary outlook new approach analysis time series 
applies systems non caused switching dynamics 
salient ingredients method memory derived low switching rate mixing coefficients adiabatic enforcement competition learning 
illustrated performance approach time series alternating chaotic systems 
particular demonstrated approach able resolve ambiguities general case overlapping input output relations assumptions systems generating data leading unsupervised segmentation 
approach estimate switching process serves analysis tool dynamics switching events 
method robust require statistics switching events 
note ansatz obtain model switching dynamics valid segmentation 
point assumption low switching rate essential get desired segmentation overlapping input domains considered 
due fact data stream variety switching dynamical systems conceivable origin 
framework choice models priori limited number predictors predictors allow relatively simple smooth mappings 
possible fit data various ways training process select wrong model get stuck local minima error function 
constraining training process find models relatively low switching rate solves problem course cases dynamics switch low rates 
imposing low pass filter errors 
additional constraint obtain correct segmentation appropriate models underlying sources 
evident compared approach mixtures controllers architecture cacciatore nowlan 
extension mixtures experts jacobs markov assumption switching characteristics advance 
gating network learn switching probabilities dynamics sources 
tested mixtures controllers data 
architecture incidentally came correct segmentation 
cases failed converge correct model ended heavily switching solution 
contrast method yielded correct result 
problem arises mixture controllers approach overlapping input domains considered 
input sequences appear allow unique determination source 
totally overlapping input domains example case 
gating network triggered input data receives information operation mode produce reasonable segmentation 
account time series switching dynamics overlapping input domains really challenging tasks appears considerable disadvantage 
applications demonstrate power approach prediction time series segmentation speech data muller 
approach predict complex dynamics prediction quality improved significantly due divide conquer strategy inherent ensemble experts 
particular significantly improve results santa fe prediction competition weigend gershenfeld data set shows time series efficiently described switching dynamics 
dedicated application method forecasting problems classification continuously spoken words 
interest estimate dynamics order predict inter switch dynamics dynamical changes 
acknowledges support dfg pa 
acknowledges financial support european communities fellowship contract 
bengio frasconi 

credit assignment time alternatives backpropagation 
nips morgan kaufmann 
cacciatore nowlan 

mixtures controllers jump linear non linear plants 
nips morgan kaufmann 
casdagli 

nonlinear prediction chaotic time series physica 
jacobs jordan nowlan hinton 

adaptive mixtures local experts neural computation 
kaneko 

chaotic regular posi switch coded attractors cluster size variation phys 
rev lett 

dimensional time series modes dynamics produce data distinct domains segmentation done merely looking data 
kohlmorgen muller pawelzik 

competing predictors segment identify switching dynamics 
proc 
international conference artificial neural networks icann springer london pp 
ff 
pawelzik schuster 

optimal embeddings chaotic attractors topological considerations europhys 
lett 

muller kohlmorgen pawelzik 

segmentation identification switching dynamics competing neural networks 
iconip proc 
int 
conf 
neural information processing seoul 
muller kohlmorgen pawelzik analysis switching dynamics competing neural networks ieice transactions fundamentals electronics communications computer sciences appear october mackey glass 

oscillation chaos physiological control system science 
mclachlan basford 
mixture models marcel dekker ny basel 
moody darken 
fast learning networks locally tuned processing units 
neural computation 
pawelzik 

detecting coherence neuronal data 
domany van hemmen schulten eds physics neural networks springer 
rabiner 

tutorial hidden markov models selected applications speech recognition proc 
ieee vol pp 

rose gurewitz fox 

statistical mechanics phase transitions clustering 
phys 
rev letters vol 

athans 

gain scheduling potential hazards possible remedies 
ieee control systems magazine 
takens 

detecting strange attractors turbulence 
rand young eds dynamical systems turbulence springer lecture notes mathematics 
tong lim 

threshold autoregression limit cycles cyclical data 
stat 
soc 

waibel hinton shikano lang 

phoneme recognition time delay neural networks ieee int 
conf 
acoustics speech signal processing 
weigend gershenfeld eds 

time series prediction forecasting understanding past addison wesley 
yuille stolorz 

statistical physics mixtures distributions em algorithm 
comp 


