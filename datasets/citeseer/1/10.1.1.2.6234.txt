self organizing context learning marc barbara hammer dept math comp 
science university ck germany marc hammer informatik uni de 
designed contribute deeper understanding proposed merging som 
context model aims representation sequences important subclass structured data 
revisit model focus fractal context encoding convergence recursive dynamic 
experiments artificial real world data support findings demonstrate power model 
recursive data processing challenging task dealing graph structured data sequences special case 
natural domain sequential data series temporally spatially connected observations dna chains articulatory time series examples 
usually vector representation exists individual sequence entries 
kohonen self organizing map known method projecting high dimensional data low dimensional grid enabling analysis visualization 
neural gas ng algorithm yields data representations small number prototypes data space providing minimum quantization error 
temporal spatial contexts series usually taken consideration terms data windows 
windows constructed serialized concatenation fixed number vectors input stream causing problems loss information curse dimensionality usually inappropriate metrics 
partially accounted adaptive metrics 
increasing interest unsupervised recurrent selforganizing networks observed directly deal sequential data 
prominent methods temporal kohonen map tkm recurrent self organizing map rsom recursive som som structured data 
comparisons methods respect accuracy efficiency 
little known formalism underlies storage temporal information 
especially unsupervised approaches thorough understanding emerging representation needed valid interpretation 
focus contribution theoretical experimental investigation efficient promising approach merge som temporal context combines currently pattern sequence past intuitive way 
show model learns fractal encoding recursive data follows successful technique wellestablished supervised learning tasks :10.1.1.16.2478
merge context temporal processing requires directional context historic influence sequence state account 
basic types context models literature self organized learning 
context tkm rsom implicitly expressed data space summing exponentially weighted historic errors neuron sequence element 
context explicit back neuron activations previous step stored vector neuron addition pattern representing weight vector 
type context computationally demanding large networks subject random activations compact version refers previously winning neuron realized 
back implemented pointer location winner regular usually dimensional som grid 
explicit context contains major drawback dependence regular neuron indexing scheme 
merging som context model developed 
general merge context refers fusion properties characterizing previous winner weight context winner neuron merged weighted linear combination 
training context descriptor calculated online target context vector ci neuron target means vector tuple wi ci neuron adapted direction current pattern context hebb learning 
definition merge context winner best matching neuron recursively computed distance dj wj ct cj current sequence entry context descriptor ct minimum 
contributions balanced parameter 
context descriptor ct linear combination properties winner time step 
typical merging value 
merge context known architectures integration merge context self organizing networks neural gas model kohonen self organizing map learning vector quantization model easily possible 
focus combination context model neural gas call merging neural gas mng 
presentation sequence element neuron rank rnk computed providing information neurons closer neuron update amount exponential function rank wj exp rnk wj cj exp rnk ct cj context descriptor ct updated date training keeping track respective winner 
experiments learning rates set identical values 
neighborhood influence decreases exponentially training obtain neuron specialization 
initial contribution context term distance computation ranking order chosen low setting weight context balance parameter small positive value 
weight representations reliable training worth gradually pay attention context refers 
initial weight specialization successively steered final value maximizes neuron activation entropy words highest possible number neurons shall average identically active training 
thinking terms hierarchical neural activation cascades heuristic optimal small number visited root states branching states decreasing probability 
control strategy proved suitable experiments 
properties merge context order shed light convergence properties merge context determine optimum encoding context weight vectors 
best adaptation neuron wj cj yield dj dj wj cj squared summands considered separately 
left trivially minimum wj 
right expands copt ci induction zero context assumption associated 
focusing convergence neuron specialized particular sequence element unique context obtain asymptotically stable fixed points training update dynamic 
analysis iterative weight updates compared target vector yields wi wi wi wi exponential convergence 
analogously ci wi ci ci copt ci copt describes context convergence show wi ci copt wi induction ci copt ci copt copt context associated current symbol binary sequence 
sum copt denotes fractal encoding context vector weight space known compact efficient representation :10.1.1.16.2478
experiments observe emergence cantor set non overlapping fractal context merging parameter 
spreading dynamic zero initialized context weight space self organizing respect density contextual input 
context function previous winner weight context adaptation moving target problem generally policy faster weight context update put influence pattern matching context matching choosing 
experiments exemplary context development displays experimental context space resulting mng training neurons random binary sequence containing independently drawn symbols 
plot reduced non idle neurons represent current symbol lower line zeroes 
context fills input space equidistant spacing symbol presentations 
stacking symbol lines indicates past 
remarkably longest sequences neurons uniquely discriminate arranged stated theory section cantor way context space 
representation grammar experiment refers sequences generated automaton depicted 
symbols encoded dimensional euclidean space 
training testing concatenated randomly generated words produced sequences input vectors respectively 
number neurons merge parameter starting neighborhood graph 
size context vector initialized center gravity embedded symbols 
rate training adaptive parameters 
context information stored ensemble neurons analyzed 
average length strings test sequence leading unambiguous winner selection number neurons develop distinct specialization words 
results hyperbolic neurons average string length number active neurons 
addition test sequence driven statistics network internal backtracking performed average neurons referred best matching contexts single neuron represent symbol different symbol encountered strong support high context consistency proper precedence learning 
backtracking scheme collect strings neuron composed symbols represented recursively visited best matching predecessors 
string assembly stops revisit neuron words average length produced valid grammar 
longest word corresponds neurons perfectly training set driven neuron 
speaker identification posteriori mng labeling experiment processes speaker data uci repository recordings speakers japanese vowel ae sequences dimensional frequency vectors 
utterance comprises number temporally connected vectors 
training set articulations available speaker test set total utterances 
articulation temporal structure different utterances temporal connection neuron added represent context available state wd cd data center default previous winner utterance start 
unsupervised mng training neurons speaker identity neuron assigned bin histogram containing activation frequencies speakers training set 
articulation sequence test set accumulated majority vote bins activated neurons calculated identify speaker 
resulting histograms specific speakers 
applying posteriori labels error training set error test set better error coming data set 
neurons error decreases 
investigated merge context model temporally spatially connected sequential data 
context obtained self organizing training architectures som ng lvq referring back winner compactly described linear combination represented weight context 
recursive context definition leads efficient fractal encoding fixed point dynamics selforganizing methods 
context emerges training preprocessing data partitioning fractal encoding necessary methods implementing merge context 
consequence self organization capacity context representation grows number neurons 
maximum utilization neurons forced maximizing network entropy adjusting context influence parameter 
recurrent neural networks cover supervised learning tasks pretty main applications merge context model unlabeled data 
experiment speaker recognition posteriori labeling indicates trust context representations potential processing labeled data 
kdd ics uci edu databases html transfer merge context standard som straight forward neuron neighborhood function defined ranking neural grid neighborhood 
kept mind regular grids approach efficient 
supervised scenarios preliminary results show unsupervised mng training fine tuning modified merge context lvq possible 
winner selection done neuron smallest distance dj 
case neuron desired class update degenerates neural gas neighbors 
case weight vector wj adapted step opposite direction context cj adopted ct context cases 
general metrics squared euclidean distance considered interplay context influence context update strength investigated detail 
challenging research connected transfer sequential context processing graph structures step 
chappell taylor 
temporal kohonen map 
neural networks 
sperduti tsoi 
self organizing map adaptive processing structured data 
ieee transactions neural networks 
kohonen 
self organizing maps 
springer verlag berlin 
martinetz schulten 
neural gas network vector quantization application time series prediction 
ieee transactions neural networks 
sinkkonen kaski 
clustering conditional distribution auxiliary space 
neural computation 
hammer 
generalized relevance lvq time series 
dorffner bischof hornik editors proceedings international conference artificial neural networks icann pages 
springer 
hammer 
neural gas sequences 
editor proceedings workshop self organizing networks pages kyushu institute technology 
hammer 
unsupervised recursive sequence processing 
verleysen editor european symposium artificial neural networks esann pages 
side publications 
tino dorffner :10.1.1.16.2478
predicting discrete sequences fractal representations past 
machine learning 
del milan 
recurrent self organizing map temporal sequence processing 
proc 
icann th international conference artificial neural networks volume lecture notes computer science pages 
springer berlin 

recursive self organizing maps 
neural networks 
