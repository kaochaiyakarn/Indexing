understanding yarowsky algorithm steven abney university michigan problems computational linguistics suited bootstrapping semi supervised learning techniques 
yarowsky algorithm known bootstrapping algorithm mathematically understood 
analyzes optimizing objective function 
specifically number variants yarowsky algorithm original algorithm shown optimize likelihood closely related objective function 
bootstrapping semi supervised learning important topic computational linguistics 
language processing tasks abundance unlabeled data labeled data lacking expensive create large quantities making bootstrapping techniques desirable 
yarowsky algorithm yarowsky bootstrapping algorithms widely known computational linguistics 
yarowsky algorithm brief consists loops 
inner loop base learner supervised learning algorithm 
specifically yarowsky uses simple decision list learner considers rules form instance contains feature predict label selects rules precision training data highest 
outer loop seed set rules start 
iteration uses current set rules assign labels unlabeled data 
selects instances base learner predictions confident constructs labeled training set 
calls inner loop construct new classifier new set rules cycle repeats 
alternative algorithm training blum mitchell subsequently popular part proven amenable theoretical analysis dasgupta littman mcallester contrast yarowsky algorithm mathematically poorly understood :10.1.1.114.9164
current aims rectify lack increasing attractiveness yarowsky algorithm alternative training 
yarowsky algorithm advantage placing restriction data sets applied 
training requires data attributes separable views conditionally independent target label yarowsky algorithm assumption data 
previous propose assumption data called precision independence yarowsky algorithm shown effective abney 
assumption ultimately unsatisfactory restricts data sets algorithm shown effective additional internal reasons 
detailed discussion take far afield suffice say precision independence property preferable assume derive basic properties data set closer empirical study shows association computational linguistics abney yarowsky algorithm dl em em inner loop uses labeled examples dl em em inner loop uses examples dl near original yarowsky inner loop smoothing dl vs near original yarowsky inner loop variable smoothing ys sequential update anti smoothing ys sequential update smoothing ys fs sequential update original yarowsky smoothing table yarowsky algorithm variants 
dl em reduces reduce precision independence fails satisfied data sets yarowsky algorithm effective 
proposes different approach 
making assumptions data views yarowsky algorithm optimizing objective function 
show variants algorithm algorithm precisely original form optimize negative log likelihood alternative objective function upper bounds ideally show yarowsky algorithm minimizes unfortunately able 
able show variant yarowsky algorithm call dl em decreases iteration 
combines outer loop yarowsky algorithm different inner loop expectation maximization em algorithm 
second proposed variant yarowsky algorithm dl advantage inner loop similar original yarowsky inner loop dl em inner loop bears little resemblance original 
dl disadvantage directly reduce show reduce alternative objective function consider third variant ys 
differs dl em dl sequential update adding single rule iteration parallel update updating rules iteration 
intrinsic interest sequential update ys proven effective exactly smoothing method original yarowsky algorithm contrast dl uses smoothing nonstandard variable smoothing 
ys proven decrease yarowsky algorithm variants consider summarized table 
extent variants capture essence original algorithm better formal understanding effectiveness 
variants deemed depart substantially original algorithm obtained family new bootstrapping algorithms mathematically understood 

generic yarowsky algorithm original algorithm original yarowsky algorithm call table 
iterative algorithm 
begins seed set labeled examples set unlabeled examples 
iteration classifier constructed labeled examples classifier applied unlabeled examples create new labeled set 
discuss algorithm formally require notation 
assume set examples feature set fx set examples feature xf note xf fx 
require series labelings represents iteration number 
abney yarowsky algorithm examples initial labeling 
train classifier labeled examples resulting classifier predicts label example probability example set arg maxj set table generic yarowsky algorithm abney yarowsky algorithm set examples labeled unlabeled current labeling labeling iteration current set labeled examples current set unlabeled examples example index feature indices label indices fx features example label example value undefined unlabeled yx xf vf examples labeled examples unlabeled examples feature fj examples label feature label number features example fx cf 
eq 
number labels labeling distribution eq 
prediction distribution eq 
dl uses eq 
fj score rule view prediction distribution label maximizes eq 
truth value value objective function negative log likelihood eq 
entropy distribution cross entropy log cf 

objective function upper bound eq 
qf precision rule eq 
qf smoothed precision eq 
qf peaked precision eq 
label maximizes precision qf feature eq 
label maximizes rule score fj feature eq 
uniform distribution table summary notation 
write label example labeling unlabeled example undefined case write 
write set unlabeled examples set labeled examples 
useful notation set examples label note disjoint union sets clear context drop superscript write simply yx risk ambiguity write set labeled examples feature trusting index discriminate labeled examples feature labeled examples label 
represent features represent labels 
reader may wish refer table summarizes notation 
iteration yarowsky algorithm uses supervised learner train classifier labeled examples 
call supervised learner base learning algorithm function classifier drawn space classifiers 
assumed classifier confidence weighted predictions 
classifier defines scoring function predicted label example arg max ties broken arbitrarily 
technically assume fixed order labels define abney yarowsky algorithm maximization return label ordering case tie 
convenient assume scoring function nonnegative bounded case normalize conditional distribution labels example write understanding probability distribution labels call distribution prediction distribution classifier example complete iteration yarowsky algorithm recomputes labels examples 
specifically label assigned example score exceeds threshold called labeling threshold 
new labeled set contains examples 
relabeling applies examples labels examples constitutes original manually labeled data opposed data labeled learning algorithm 
algorithm continues convergence 
particular base learning algorithm yarowsky uses deterministic sense classifier induced deterministic function labeled data 
algorithm known converged labeling remains unchanged 
note algorithm stated leaves base learning algorithm unspecified 
distinguish generic yarowsky algorithm base learning algorithm open parameter specific yarowsky algorithm includes specification base learner 
informally call generic algorithm outer loop base learner inner loop specific yarowsky algorithm 
base learner yarowsky assumes decision list induction algorithm 
postpone discussion section 
objective function machine learning algorithms typically designed optimize objective function represents formal measure performance 
maximum likelihood criterion commonly objective function 
suppose set examples labels yx parametric family models represents probability assigning label example model 
likelihood probability full data set model viewed function maximum likelihood criterion instructs choose parameter settings maximize likelihood equivalently log likelihood log yx log yx yx log notation represents truth value proposition true 
define yx note satisfies formal requirements probability distribution labels specifically point distribution mass concentrated yx 
call labeling distribution 
write log abney yarowsky algorithm written distribution leaving dependence implicit 
nonstandard notation called cross entropy 
easy verify entropy kullback leibler divergence 
note point distribution 
particular point distribution restate maximum likelihood criterion instructing choose model minimizes total divergence empirical labeling distributions model prediction distributions extend unlabeled examples need observe unlabeled examples ones labels data provide information 
accordingly revise definition treat unlabeled examples ones labeling distribution maximally uncertain distribution say uniform distribution yx number labels 
equivalently replace expressions longer equivalent 
minimized labeled minimizing forces label unlabeled examples 
labeled examples minimized labels examples agree predictions model 
short adopt objective function seek minimize modified algorithm show modified version yarowsky algorithm finds local minimum modifications necessary 
labeling function recomputed iteration constraint example labeled stays labeled 
label may change labeled example unlabeled 
eliminate threshold equivalently fix result examples remain unlabeled labeling step uniform distribution 
problem arbitrary threshold prevents algorithm converging minimum threshold gradually decreases address problem complicate analysis 
abney yarowsky algorithm 
train classifier result example set arg maxj set table modified generic yarowsky algorithm modified algorithm table 
obtain proof necessary assumption supervised classifier induced base learner step 
natural assumption base learner chooses minimize 
weaker assumption suffice 
assume base learner reduces divergence possible 
assume equality classifier 
note learning algorithm minimizes satisfies weaker assumption inasmuch option setting available 
consider somewhat stronger assumption base learner reduces divergence examples just labeled examples dx base learning algorithm satisfies proof theorem shorter natural condition base learner satisfy 
state main theorem section 
theorem base learning algorithm satisfies algorithm decreases iteration reaches critical point lemma require order prove theorem 
lemma distributions log maxj equality iff uniform distribution 
proof 
definition max abney yarowsky algorithm log log maxj true true take expectation respect log log maxj log maxj equality maxj uniform distribution 
prove theorem 
proof theorem 
algorithm produces sequence labelings 
sequence classifiers 
classifier trained labeling created recall training step hold fixed change labeling step hold fixed change 
show training step minimizes function labeling step minimizes function examples critical point non increasing iteration algorithm strictly decreasing critical point consider labeling step 
step held constant possibly changes show nonpositive show nonpositive guarantee minimizes viewed function definition pj log 
clearly accomplish placing mass pj minimizes log 
minimizer minimized distribution distributes mass minimizers log 
observe wish find distribution minimizes arg min log arg max minimize setting pj say labeling predicted algorithm defines examples labels modifiable excluding 
abney yarowsky algorithm note minimize examples examples remain unlabeled 
algorithm example unlabeled necessarily unlabeled example 
label changes labeling step decreases label changes remains unchanged case increase 
show examples labeling distribution assigned represents critical point example uniform distribution la prediction distribution 
divergence zero minimum 
possible decrease decreasing cost increase directions motion ways selecting labels receive increased probability mass equally 
say gradient zero critical point 
essentially reached saddle point 
minimized respect dimensions non zero gradient 
remaining dimensions local maximum gradient choose direction descent 
consider training step 
step held constant change equal change recall 
hypothesis theorem cases base learner satisfies 
satisfies base learner minimizes function follows immediately minimizes function 
suppose base learner satisfies 
express training step term remains constant 
second term decreases hypothesis 
third term may increase 
show increase third term offset labeling step 
consider arbitrary example unlabeled time know uniform distribution uniform distribution example labeled previous iteration 
value iteration log training step value log log remains unchanged training step new distribution old uniform distribution example remains unlabeled 
change particular non increasing desired 
hand abney yarowsky algorithm change new distribution non uniform example labeled labeling step 
value iteration labeling step log log lemma 
observed consider change find increase training step offset labeling step 
specific yarowsky algorithm original decision list induction algorithm dl speaks yarowsky algorithm mind just generic algorithm algorithm specification includes particular choice base learning algorithm yarowsky 
specifically yarowsky base learner constructs decision list list rules form feature label score fj 
rule matches example possesses feature label predicted example label highest scoring rule matches yarowsky uses smoothed precision rule scoring 
name suggests smoothed precision qf smoothed version raw precision qf probability rule correct matches fj qf set labeled examples possess feature fj set labeled examples feature label smoothed precision defined follows fj write qf clear context 
yarowsky defines rule score smoothed precision fj qf anticipating needs consider raw precision alternative fj qf 
raw smoothed precision properties conditional probability distribution 
generally view fj conditional distribution labels fixed feature yarowsky defines confidence decision list score rule matches instance classified 
equivalent defining max fj fx abney yarowsky algorithm fixed value initialize arrays example label increment feature feature label set fj define maxf fx fj table decision list induction algorithm dl 
value accumulated fj value accumulated 
recall fx set features 
classifier prediction defined equation label maximizes definition implies classifier prediction label highest scoring rule matching desired 
written maximizing fj fx label general yield probability distribution labels scores positive bounded normalizable 
considering final predicted label example normalization effect inasmuch scores fj compared scaled way 
characterized yarowsky decision list contains rules score qf exceeds labeling threshold 
seen purely efficiency measure 
including rules score falls labeling threshold effect classifier predictions threshold applied classifier applied examples 
reason prune list 
represent decision list set parameters fj possible rule cross product set features set labels 
decision list induction algorithm yarowsky summarized table call dl 
note step labeled step induction algorithm specifies decision list compute prediction distribution example unfortunately prove dl stands 
particular unable show dl reduces divergence prediction labeling distributions 
section describe alternative decision list induction algorithm called dl em satisfy apply theorem combination dl em show reduces disadvantage dl em resemble algorithm dl yarowsky 
return section close variant dl called dl show directly reduce reduce upper bound decision list induction algorithm dl em algorithm dl em special case expectation maximization em algorithm 
consider versions algorithm dl em dl em 
differ dl em trained labeled examples dl em trained labeled unlabeled examples 
basic outline algorithm 
dl em algorithms assume definition yarowsky assumed 
mentioned parameters fj thought defining prediction distribution labels feature equation specifies prediction distributions features example combined yield prediction distribution combining distri abney yarowsky algorithm butions maximizing fj fx equation dl em takes mixture fj fx number features possesses sake simplicity assume examples number features 
probability distribution convex combination distributions distribution follows defined probability distribution 
definitions mode guaranteed severely restricted case features labels 
definition prediction determined entirely strongest definition permits bloc weaker strongest 
yarowsky explicitly wished avoid possibility interactions 
definition dl em turns analysis base learners manageable assume henceforth dl em algorithms dl ys discussed subsequent sections 
dl em differs dl dl em construct classifier scratch seeks improve previous classifier 
context yarowsky algorithm previous classifier previous iteration outer loop 
write old fx fj parameters old prediction distributions previous classifier 
conceptually dl em considers label assigned example generated choosing feature fx assigning label feature prediction distribution 
choice feature hidden variable 
degree example labeled imputed feature determined old distribution old fx old fj fx old gj fx old fj old think old posterior probability feature responsible label portion labeled example imputed feature write old xj synonym old 
new estimate fj obtained summing imputed occurrences normalizing labels 
dl em takes form fj old old algorithm summarized table 
second version algorithm dl em summarized table 
dl em uses update rule fj old old old old update rule includes unlabeled examples labeled examples 
conceptually divides unlabeled example equally labels divides resulting fractional labeled example example features 
note variants dl em algorithm constitute single iteration em algorithm 
single iteration suffices prove theorem multiple iterations effective 
abney yarowsky algorithm initialize example labeled fx old gj fx increment old fj feature label set fj table dl em decision list induction algorithm initialize example labeled fx old gj fx increment old fj unlabeled example fx old gj fx increment old fj feature label set fj table dl em decision list induction algorithm abney yarowsky algorithm theorem classifier produced dl em algorithm satisfies equation classifier produced dl em algorithm satisfies equation 
combining theorems yields corollary yarowsky algorithm dl em dl em base learning algorithm decreases iteration reaches critical point proof theorem 
old represent parameter values call dl em represent family free variables optimize old corresponding prediction distributions 
labeling distribution fixed 
set examples change resulting change 
obviously particularly interested cases set examples dl em set labeled examples dl em 
case show equality choice decreases derive expression put shortly old old log log old em algorithm fact divergence non negative strictly positive distributions compared identical 
old xj xj old fx fx xj log old old xj log yields inequality log log old fx written fx old fj constant maximizing xj xj old fj old fj old xj log fj log old fj fx old xj log fj log old fj old xj log fj abney yarowsky algorithm maximize lower bound 
easy see bounded divergence zero distributions simply set fj old fj identical strict inequality best choice old case choice 
remains show dl em computes parameter set maximizes 
wish maximize constraints values fj fixed sum unity choices apply lagrange method 
express constraints form cf cf fj seek solution family equations results expressing gradient linear combination gradients constraints fj old fx cf xk log gk fj derive expression derivative left hand side fj fx similarly right hand side substituting equation xf fj old xk log gk cf fj old xj fj xf constraint cf solving substituting back xf fj xf old xj old xj xf old xj old xf xj xf old xk old xj fj consider case set examples expand obtain fj old xj old xj fj vf abney yarowsky algorithm normalizes hard see update rule dl em computes intermediate values fj vf old xj old xj consider case set labeled examples expand obtain fj old xj fj update rule dl em computes 
see dl em reduces dx dl em reduces 
note closing dl em simplified algorithm inasmuch known fj fx expression simplifies follows vf old xj vf vf fx dependence disappears replace algorithm dl em delete step replace step statement fx increment 
objective function dl em variation yarowsky algorithm show reduce negative log likelihood variants discuss remainder dl ys reduce alternative objective function define 
value precisely value upper bound derive jensen inequality follows define xj log xj fx fx fx fx gj log gj minimizing minimize upper bound principle possible reduce zero 
reduced zero examples labeled feature concentrates prediction distribution single label abney yarowsky algorithm initialize example label pair feature fx increment feature label set fj define fx fj table decision list induction algorithm dl initialize example label pair feature fx increment unlabeled example feature fx increment feature label set set fj define fx fj table decision list induction algorithm dl vs label example agrees prediction feature possesses 
limiting case minimizer minimizer hasten add proviso possible reduce zero data sets 
provides necessary sufficient condition 
consider undirected bipartite graph nodes examples features 
edge example feature just case feature define examples neighbors belong connected component reducible zero label pairs neighbors algorithm dl consider variants dl called dl dl vs differ dl ways 
dl algorithms assume mean definition equation max definition equation 
difference induction algorithm way decision list construct prediction distribution second dl algorithms update rules differ smoothed precision dl 
dl table uses raw precision smoothed precision 
dl vs table uses smoothed precision dl dl vs fixed smoothing constant varies feature feature 
specifically computing score fj dl vs uses vf value 
value dl vs expressed way prove useful 
define xf vf xf abney yarowsky algorithm lemma parameter values fj computed dl vs expressed uniform distribution labels 
fj qf proof 
fj 
dl vs computes fj lemma proved 
need consider case 
show smoothed precision expressed convex combination raw precision uniform distribution 
define 
qf fj fj qf qf show mixing coefficient mixing coefficient lemma vf step dl vs vf main theorem section theorem specific yarowsky algorithm dl decreases iteration reaches critical point 
proved corollary theorems 
theorem shows dl minimizes function holding constant second theorem shows decreases function holding constant 
precisely dl minimizes labeled examples dl vs minimizes examples sufficient effective 
theorem dl minimizes function holding constant 
specifically dl minimizes labeled examples dl vs minimizes examples proof 
wish minimize function constraints cf fj abney yarowsky algorithm minimize constraints cf express gradient linear combination gradients constraints solve resulting system equations cf fj derive expressions derivatives cf variable represents set examples minimizing fj cf fj fj fj fx xj fj xf xk log gk substitute expressions solve fj xj xf fj fj xf xj substituting expression equation cf solving xj xf xf substituting back expression fj fj xf fj xf xj qf update computed dl showing dl computes parameter values fj minimize labeled examples 
fj xf xf fj xf vf xf qf lemma update computed dl vs dl vs minimizes complete set examples vf abney yarowsky algorithm theorem base learner decreases prediction distribution computed algorithm decreases iteration reaches critical point considering function held constant 
proof structure proof theorem give sketch 
minimize function minimizing example separately 
fx xj fx fx fj log gj minimize choose xj concentrate mass arg min fx log gj arg max labeling rule 
base learner minimizes shown increase unlabeled examples compensated labeling step proof theorem 
theorem specific yarowsky algorithms dl dl vs decrease iteration reach critical point 
proof 
immediate theorems 

sequential algorithms family ys yarowsky algorithm variants considered parallel updates sense parameters fj completely recomputed iteration 
section consider family ys sequential variants yarowsky algorithm single feature selected update iteration 
ys algorithms resemble yarowsky cautious algorithm collins singer differ yarowsky cautious algorithm update single feature iteration small set features yarowsky cautious 
ys algorithms intended close dl algorithm consonant single feature updates 
ys algorithms differ dl choice update rule 
interesting range update rules sequential setting 
particular smoothed precision fixed original algorithm dl works sequential setting proviso spelled 
initial labeled set initial classifier consisting set 
selected features initial parameter set fj abney yarowsky algorithm gj initialization set example possesses feature set yx set yx loop choose feature qf add label set fj update example possessing feature set yx table sequential algorithm ys iteration feature selected added selected set 
feature selected remains selected set 
permissible feature selected permits continue reducing features selected 
short sequence selected features st st ft parameters selected feature updated 
iteration parameters gj ft may modified parameters remain constant 
gj gj ft follows gj st parameters features may modified inasmuch play role manually labeled data 
iteration selects feature ft computes recomputes prediction distribution ft selected feature ft labels recomputed follows 
recall arg maxj continue assume mixture definition equation 
label example set feature belongs st 
particular previously labeled examples continue labeled labels may change unlabeled examples possessing feature ft labeled 
algorithm summarized table 
algorithm schema definition update needs supplied 
consider different update functions uses raw precision prediction distribution uses smoothed precision goes opposite direction call peaked precision 
seen smoothed precision expressed mixture raw precision uniform maximum entropy distribution 
peaked precision mixes certain amount point minimum entropy distribution mass label maximizes raw precision qf qf arg max qf abney yarowsky algorithm note peaked precision involves variable amount peaking mixing parameters depend relative proportions labeled unlabeled examples 
note function explicitly represent dependence 
instantiations algorithm ys consider ys peaked fj qf ys raw fj qf ys fs fixed smoothing fj qf show algorithms reduce iteration 
show third algorithm ys fs reduces iterations ft new feature previously selected 
unfortunately unable show ys fs reduces ft previously selected feature 
suggests mixed algorithm smoothed precision new features raw peaked precision previously selected features 
final issue algorithm schema ys concerns selection features step 
schema stated specify feature selected 
essence manner rules selected matter long selects rules room improvement sense current prediction distribution differs raw precision qf 
justification choice theorem 
theorems sections show decreases iteration long rule 
choose greedily choosing feature maximizes gain eq 
section give lower bounds easily computed theorems 
gain point consider single iteration ys algorithm discard variable write old old parameter set labeling iteration write simply new parameter set new labeling 
set resp represents examples labeled resp unlabeled iteration 
selected feature wish choose prediction distribution guarantee decreases iteration 
gain current iteration old old fx gain negative change positive decreases 
considering reduction old old convenient consider intermediate values 
fx fx fx fx old old old abney yarowsky algorithm note vf xj old xj arg max fj new prediction distribution candidate gj old gj new label distribution relabeling 
defined xf xj vf selected feature examples 
follows agree examples vf agree examples unlabeled assigning uniform label distribution 
differ old labeled examples need relabeled addition gain represented sum intermediate gains corresponding intermediate values just defined gv gv gain gv intuitively represents gain attributable labeling previously unlabeled examples accordance predictions 
gain represents gain attributable changing values fj selected feature 
gain represents gain attributable changing labels previously labeled examples labels agree predictions new model 
gain corresponds step algorithm ys changed held constant combined gv gains correspond step algorithm ys changed holding constant 
remainder section derive lower bounds sections show updates ys ys ys fs guarantee lower bounds non negative non negative 
lemma gv proof 
show remains unchanged substitute old 
property need agrees old previously labeled examples 
old need consider examples examples unlabeled iteration features selected abney yarowsky algorithm old gj features fx xj log old gj fx xj log fx old xj log fx old xj log old gj note xj general equal old xj xj shows gv 
lemma old xj equal 
show relabeling old labeled examples setting increase proof structure proof theorem omitted 
lemma equal qf old qf vf log log fj proof 
definition identical examples xf xf fx divide sum partial sums old old vf old xf fx old consider partial sum separately 
old abney yarowsky algorithm old xk log fk log fk log old fk log fk fk log old fk log fk qf log old fk log fk qf old qf vf vf vf old old xk log fk log fk log old fk log fk vf log old log fj fj vf log log fj justification step bit subtle 
new feature previously selected old fk substitution valid 
hand previously selected feature vf substitution may valid innocuous 
old fj xf fx xf fx old old old combining yields lemma 
theorem bounded 
proof 
combining lemmas 
theorem bounded qf old qf proof 
theorem follows immediately theorem show log log fj abney yarowsky algorithm observe log 
recall uniform distribution labels 
lemma know log fj uniform distribution maximizes entropy 
theorem bounded qf old qf proof 
immediate theorem fact qf old qf qf qf old qf qf qf old qf theorem old qf choice yields strictly positive gain 
proof 
old qf setting qf result theorem 
qf old qf old qf qf old algorithm ys results previous section show algorithm ys correct sense reduces iteration 
theorem iteration algorithm ys decreases 
proof 
wish show 
theorem true expression positive 
theorem exist choices positive particular guarantee maximizing 
maximize minimizing minimize minimizing qf vf log fj qf qf qf qf vf log fj abney yarowsky algorithm terms nonnegative 
term qf second term distribution concentrates mass single label symmetric choices decreases monotonically fj approaches 
minimum equal mode qf may peaked qf cost increase term offset decrease second term 
recall arg maxj qf 
reasoning previous paragraph know minimum 
minimize minimizing qf vf log fk compute gradient qf vf log fk fj fj fj qf qf vf log fk qf fj fj fj qf log fk vf qf qf log fj vf fj vf vf log fk fj fj fj log fk log fj derivative constraint cf minimize constraint solving qf fj vf fj fj qf vf substituting constraint qf vf substituting back vf xf fj qf maximizing solution peaked precision update rule ys 
abney yarowsky algorithm algorithm ys show ys decreases iteration 
fact essentially proven 
theorem algorithm ys decreases iteration 
proof 
proof theorem showed choice qf yields strictly positive gain 
update rule ys 
algorithm ys fs original yarowsky algorithm ys dl smoothed precision fixed update rule 
unsuccessful justifying choice update rule general 
able show decrease selected feature new feature previously selected 
theorem algorithm ys fs positive gain iteration selected feature previously selected 
proof 
theorem gain positive qf old qf assumption selected feature previously selected old uniform distribution left hand side equal qf 
easy verify distribution left hand side equal 
ys fs uses smoothed precision update rule qf rewritten qf qf condition hold trivially inasmuch cross entropy divergence unbounded 
show holds particular case 
derive upper bound qf qf 
qf qf qf log qf observe qf log qf qf log qf log qf qf qf qf abney yarowsky algorithm iff qf iff qf know qf uniform distribution maximizes entropy 
know inequality strict reasoning 
new feature old restriction step algorithm ys old qf qf strictly greater qf 
true combining shown true proving theorem 

minimization feature entropy training algorithm mentioned alternative yarowsky algorithm 
fact connection training yarowsky algorithm 
original training blum mitchell suggested algorithm understood seeking maximize agreement unlabeled data classifiers trained different views data :10.1.1.114.9164
subsequent dasgupta littman mcallester proven direct connection classifier error cross view agreement unlabeled data 
current context justification pursuing agreement unlabeled data 
yarowsky algorithm assumption existence conditionally independent views data 
motivation seeking agreement unlabeled data arbitrary pairs features 
recall original objective function expressed sum entropy term divergence term small small necessarily small limit limiting 
intuitively wish reduce uncertainty model predictions improving fit model predictions known labels 
focus uncertainty model predictions 
log log fx gj log gj fx fj log gj fx fx fx fx fj log gj abney yarowsky algorithm fx fx fx fx fx fx fx words decreasing uncertainty prediction distributions individual features simultaneously increasing agreement features decreasing pairwise divergence decrease upper bound 
motivates agreement recourse assumption independent views 

number variants yarowsky algorithm shown optimize natural objective functions 
considered modified generic yarowsky algorithm showed minimizes objective function equivalent maximizing likelihood provided base learner reduces considered families specific yarowsky algorithms 
dl em algorithms dl em dl em minimize disadvantage dl em base learner similarity yarowsky original base learner 
better approximation yarowsky original base learner provided dl dl algorithms dl dl vs shown minimize objective function upper bound ys algorithms ys ys ys fs sequential variants reminiscent yarowsky cautious algorithm collins singer showed minimize extent algorithms capture essence original yarowsky algorithm provide formal understanding yarowsky approach 
deemed diverge original cast light workings represent new family bootstrapping algorithms solid mathematical foundations 
abney steven 

bootstrapping 
proceedings th annual meeting association computational linguistics acl pages 
blum mitchell 

combining labeled unlabeled data training 
proceedings th annual conference computational learning theory colt pages 
morgan kaufmann publishers 
collins michael yoram singer 

unsupervised models named entity classification 
proceedings empirical methods natural language processing emnlp pages 
dasgupta sanjoy michael littman david mcallester 

pac generalization bounds training 
advances neural information processing systems nips 
yarowsky david 

unsupervised word sense disambiguation rivaling supervised methods 
proceedings rd annual meeting association computational linguistics pages 

