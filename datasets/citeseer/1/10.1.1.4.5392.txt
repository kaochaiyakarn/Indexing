appear proceedings icml workshop statistical relational learning connections fields srl banff canada july comparison inference techniques semi supervised clustering hidden markov random fields mikhail bilenko cs utexas edu basu cs utexas edu department computer sciences university texas austin university station austin tx usa number methods proposed semi supervised clustering employ supervision form pairwise constraints 
describe probabilistic model semisupervised clustering hidden markov random fields incorporates relational supervision 
model leads clustering algorithm step requires collective assignment instances cluster centroids constraints 
evaluate known techniques collective assignment belief propagation linear programming relaxation iterated conditional modes icm 
methods attempt globally approximate optimal assignment icm greedy method 
experimental results indicate global methods outperform greedy approach relational supervision limited benefits diminish pairwise constraints provided 

significant interest semisupervised clustering goal improve performance unsupervised clustering algorithms limited amounts supervision form labels pairwise constraints data points wagstaff klein xing bilenko 
probabilistic model semisupervised clustering pairwise relations compare performance inference methods cluster assignment context em algorithm 
existing methods semi supervised clustering fall general categories call constraint distance 
constraint methods rely labels relational constraints guide algorithm appropriate data partitioning wagstaff 
distance approaches existing clustering algorithm uses particular clustering distortion measure employed measure trained satisfy labels constraints supervised data klein xing cohn bar hillel 
bilenko 
proposed integrated framework semi supervised clustering combines constraintbased distance approaches unified probabilistic model 
shown semi supervised clustering framework underlying probabilistic model hidden markov random field hmrf basu 
minimizing integrated clustering objective function equivalent finding maximum posteriori probability map configuration hmrf zhang 
shown hmrf clustering model able incorporate bregman divergence banerjee clustering distortion measure allows framework common distortion measures kl divergence parameterized squared mahalanobis distance 
additionally cosine similarity clustering distortion measure framework useful directional datasets 
hmrf semi supervised clustering model suggests em algorithm minimizes integrated clustering objective function obtain partitioning dataset 
step algorithm mapped inference step probabilistic relational model 
prior fast greedy iterated conditional modes icm inference technique step bilenko basu 
compare icm global approximate inference techniques relational models belief propagation linear programming lp relaxation 
experiments reveal provided sufficient relational supervision icm produces results comparable belief propagation lp relaxation constraintbased semi supervised clustering task fraction computational cost 

background 
hmrf clustering framework set data points sets pairwise link constraints link constraints sets asso observed data hidden mrf 
hidden markov random field ciated violation costs distortion measure points semi supervised clustering task optimally partition clusters 
optimal partitioning minimizes total distortion points cluster representatives keeping constraint violations minimum 
problem formalized task label assignment hidden markov random field hmrf basu 
hmrf model consists hidden field random variables encode cluster assignments data points observable set random variables correspond observed data points 
data point assumed generated conditional probability distribution pr determined value corresponding hidden variable random variables conditionally independent hidden variables pr pr 
note relational model similar proposed segal 
semi supervised clustering constraints utilized constraints incorporate learnable distortion measures 
fig 
shows simple example hmrf 
observed dataset consists points 
corresponding cluster label variables 

link constraints provided link constraint provided 
task partition points clusters 
clustering configuration shown fig 
points put cluster point linked assigned cluster involved constraints put clusters respectively 
link link constraints specify pairs points different clusters respectively 
hidden random variable associated set neighbors link constraints constraints define neighborhood hidden variable points linked linked corresponding data point 
markov random field defined hidden variables 
consider particular cluster label configuration joint event corresponds specific assignment data points clusters 
hammersley clifford theorem probability label configuration markov random field expressed gibbs distribution geman geman pr set neighborhoods normalizing constant label configuration potential function decomposed functions vn denoting potential neighborhood provided pairwise constraints class labels restrict mrfs hidden variables pairwise potentials 
prior probability configuration cluster labels pr exp fm fc non negative function penalizes violation link constraint corresponding penalty function links 
intuitively form pr gives higher probabilities label configurations attempt satisfy link constraints link constraints possible learnable distortion measure adapts distance computations respect user provided constraints 
facilitate learning distortion measure parameters penalty violating link constraint distant points higher nearby points 
reflects fact linked points far apart current distortion measure distance measure parameters need modified bring points closer 
inversely penalty violating link constraint points nearby current distance measure higher distant points 
reflect reasoning penalty functions chosen follows fm ij fc ij max monotonically increasing penalty scaling function distance typically equivalent distortion measure max maximum value dataset 
form ensures penalty violating link constraint remains non negative 
note resulting potential function corresponds metric version previously described generalized potts potential kleinberg tardos 
posterior probability cluster label configuration pr pr pr assuming pr constant 
consider pr exponential form encompasses commonly distortion measures squared euclidean distance kl divergence cosine similarity basu 
finding maximum posteriori map configuration hmrf equivalent maximizing posterior probability pr normalizing constant 
henceforth refer exponential factor pr constraint potential second factor distance potential negative logarithm pr posterior energy 
note map estimation reduce maximum likelihood ml estimation pr pr constant 
model accounts dependencies cluster labels pr constant full map estimation pr required 
logarithms eqn gives cluster objective function minimizing equivalent maximizing map probability eqn equivalently minimizing posterior energy hmrf obj ij ij max log normalizing constant 
task minimize obj parameterized computational efficiency consider log constant clustering iterations 
expectation maximization em algorithm popular solution incomplete data problems dempster 
known means equivalent em algorithm hard clustering assignments kearns banerjee 
means type iterative clustering algorithm find local maximum function 

hmrf kmeans algorithm outline hmrf kmeans algorithm 
initialization performed neighborhoods inferred constraints described bilenko 
basic idea follows step current cluster representatives data points collectively reassigned clusters minimize obj step cluster representatives re estimated cluster assignments minimize obj current assignment 
clustering distortion measure updated step reduce objective function simultaneously transforming space data lies performing metric learning 
note procedure instantiation generalized em algorithm dempster neal hinton objective function reduced necessarily minimized step 
effectively step minimizes obj cluster assignments step minimizes obj cluster representatives step minimizes obj parameters distortion measure step step repeated till specified convergence criterion reached 
algorithm hmrf kmeans input set data points number clusters set link constraints set link constraints distance measure constraint violation costs output disjoint partitioning objective function obj eqn locally minimized 
method 
initialize clusters centroids set 
repeat convergence 
step re assign cluster labels points minimize obj 
step cluster labels re calculate cluster centroids minimize obj 
step re estimate distance measure reduce obj 

hmrf kmeans algorithm relational nature supervision significant step task assign data points clusters current estimates cluster representatives 
simple means interaction cluster labels step simple assignment point cluster representative nearest clustering distortion measure 
contrast hmrf model incorporates interaction cluster labels defined random field hidden variables 
optimal assignment step hmrf kmeans relational inference problem 

inference techniques describe inference methods collective assignment data points clusters step hmrf kmeans 

belief propagation approach global joint assignment points clusters locally minimizes objective function obj 
factor graph hmrf performing approximate inference hmrf belief propagation pearl 
approach similar technique segal 

implement message passing algorithm approximate inference hmrf represent hmrf factor graph model kschischang 
sum product max product algorithm factor graph model shown generalization known inference algorithms graphical models 
interpreting hmrf model factor graph enables perform belief propagation hmrf message passing algorithm corresponding factor graph 
factor graph corresponding example hmrf shown 
factor graph components variable nodes representing data points 
factor nodes encode distance potential components objective function 
distance factor node edge connecting corresponding variable node table containing different values distance potential function 
table entry possible cluster assignment variable th entry exp distance th point th cluster 
factor nodes factor nodes encode cost violating link link constraints respectively 
factor node constraint linked edges variable nodes involved constraint 
constraint potential table associated constraint factor node maps set value pairs corresponding possible cluster assignments pair points constraint potential values 
factor node encoding link constraint potential value entry constraint potential table cluster assignments 
potential value exp ij distance points current estimate distortion measure ij weight constraint 
similarly link factor nodes potential tables values entry exp max ij potential values constraint factor nodes correspond metric version potts potential function explained section 
finding collective assignment points minimize obj step corresponds running max product message passing algorithm factor graph kschischang 
message passing algorithm converges cluster assignment data point obtained value corresponding variable node 

iterated conditional modes approach iterated conditional modes icm inference technique besag greedy strategy sequentially update cluster assignment point keeping assignments points fixed 
preselected random ordering point sequentially assigned cluster representative minimizes point contribution objective function obj obj ij ij max optimal assignment point minimizes distortion point cluster representative term obj incurring minimal penalty constraint violations caused assignment second third terms obj 
point assigned cluster subsequent points sequence determined ordering current cluster assignment calculate possible constraint violations 
points assigned assignment process repeated new random ordering 
process proceeds point changes cluster assignment successive iterations 
icm guaranteed reduce obj keep unchanged obj local minimum step besag 

lp relaxation approach task finding assignment instances clusters minimize objective function posed integer programming problem 
formulation proposed kleinberg tardos context general metric labeling problem considered cost assigning labels instances attempting satisfy set link pairwise constraints kleinberg tardos 
extend formulation include link constraints allows assigning instances clusters step hmrf kmeans 
il set nonnegative binary variables encode membership instances clusters il signifies th instance belongs th cluster 
sets nonnegative binary variables encode violations link link pairwise constraints respectively 
signifies th pairwise constraint violated signifies th link pairwise constraint violated 
objective function optimized step hmrf kmeans obj il xk xk xk xk assigning instance cluster imposes linear constraint variables il consistency pairwise constraint violation variables assignment variables requires satisfaction linear constraints yk yk ek xk xk yk ek xk xk constraints expressed linear program replacing variables corresponding sets auxiliary variables kl iff th link pair violated assigned th cluster 
semantics kl similar kl iff th link pair violated assigned th cluster 
variables expressed variables follows kl ek xk xk kl ek xk xk consistency assignment variables pairwise constraint violation variables achieved introducing linear constraints kl yk yk ek xk xk kl yk yk ek xk xk kl yk yk ek xk xk kl yk yk ek xk xk minimization objective function constraints solve binary variables np hard 
kleinberg tardos proposed linear programming relaxation integer programming problem allowing nonnegative real numbers provided randomized method rounding real solution linear program integers kleinberg tardos 
follow approach allows perform collective assignment instances cluster centroids 

experiments 
methodology datasets experiments conducted datasets iris uci repository protein dataset xing 
bar hillel 
randomly sampled subset letters handwritten character recognition dataset 
letters chose classes sampling data points original dataset randomly 
parameterized squared euclidean distance clustering distortion measure experiments 
pairwise measure evaluate clustering results underlying classes 
measure relies traditional information retrieval measures adapted evaluating clustering considering cluster pairs recision recall measure recision recall recision recall generated learning curves fold cross validation dataset 
point learning curve represents particular number randomly selected pairwise constraints input algorithm 
unit constraint costs constraints datasets provide individual weights constraints 
clustering algorithm run dataset pairwise measure calculated test set 
results averaged runs folds 
trial cluster initialization performed neighborhoods inferred provided constraints bilenko hmrf kmeans algorithm run particular inference technique step metric learning euclidean distance step 

results discussion compared methods described section collective assignment instances clusters 
figures show learning curves datasets 
number constraints icm belief propagation lp relaxation 
iris results number constraints icm belief propagation lp relaxation 
protein results measure number constraints icm belief propagation lp relaxation 
letters results dataset icm faster belief propagation lp relaxation order magnitude agrees relative computational complexities algorithms 
results demonstrate global relational methods belief propagation lp relaxation outperform greedy approaches limited number pairwise constraints provided 
number provided constraints increases returns computationally expensive methods diminish dataset exists number constraints icm performs worse global approximate inference methods 

compared methods global approximate inference belief propagation lp relaxation greedy approximate inference algorithm icm context collective assignment data points clusters semi supervised clustering pairwise relational constraints 
results indicate belief propagation lp relaxation outperform icm limited number pairwise constraints provided 
sufficiently large amount relational supervision greedy algorithm approximate inference performs par global methods 
greedy inference techniques considered scaling semi supervised clustering large datasets due low computational cost 

acknowledgments helpful discussions 
research supported part nsf iis iis 
banerjee dhillon ghosh 

clustering bregman divergences 
proceedings sdm 
bar hillel hertz weinshall 

learning distance functions equivalence relations 
proceedings icml pp 

basu bilenko mooney 

probabilistic framework semi supervised clustering 
submission available www cs utexas edu ml publication 
besag 

statistical analysis dirty pictures 
journal royal statistical society series methodological 
bilenko basu mooney 

integrating constraints metric learning semi supervised clustering 
appear proceedings icml 
cohn caruana mccallum 

semi supervised clustering user feedback technical report tr cornell university 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
geman geman 

stochastic relaxation gibbs distributions bayesian restoration images 
ieee pami 
kearns mansour ng 

informationtheoretic analysis hard soft assignment methods clustering 
proceedings uai pp 

klein kamvar manning 

constraints space level constraints making prior knowledge data clustering 
proceedings icml pp 

kleinberg tardos 

approximation algorithms classification problems pairwise relationships metric labeling markov random fields 
proceedings focs pp 

kschischang frey loeliger 

factor graphs sum product algorithm 
ieee transactions information theory 
neal hinton 

view em algorithm justifies incremental sparse variants 
learning graphical models pp 

mit press 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
segal wang koller 

discovering molecular pathways protein interaction gene expression data 
bioinformatics 
wagstaff cardie rogers 

constrained means clustering background knowledge 
proceedings icml pp 

xing ng jordan russell 

distance metric learning application clustering 
nips pp 

zhang brady smith 

hidden markov random field model segmentation brain images 
ieee transactions medical imaging 
