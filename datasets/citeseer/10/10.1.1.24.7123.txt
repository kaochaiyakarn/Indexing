human observers eye movements automatic image classifiers alejandro jeff tim grabowski jason babcock shih fu chang dept electrical engineering columbia university new york ny center imaging science rochester institute technology rochester ny explore way people look images different semantic categories handshake landscape directly relate results computational approaches automatic image classification 
hypothesis eye movements human observers differ images different semantic categories information effectively automatic content classifiers 
eye tracking experiments show variations eye movements fixations saccades different individuals images different categories handshakes people shaking hands crowd cluttered scenes people landscapes nature scenes people main object uncluttered background airplane flying miscellaneous people lives 
eye tracking results suggest similar viewing patterns occur different subjects view different images semantic category 
results examine empirical data obtained eye tracking experiments different semantic categories integrated existing computational frameworks construct new ones 
particular exa mine visual apprentice system image classifiers learned machine learning user input user defines multiple level object definition hierarchy object parts scene object object part perceptual area region labels examples specific classes handshake 
resulting classifiers applied automatically classify new images handshake non handshake 
eye tracking experiments performed knowledge study specifically compares eye movements categories links eye tracking results automatic image classification techniques 
keywords eye tracking automatic image classification content retrieval 

eye tracking studies performed years earliest reported different purposes 
main goals studies understanding human visual system particular visual process 
understood humans move eyes part visual acuity falls order magnitude degrees central vision 
tasks eyes moved shift point regard regions requiring high spatial resolution 
humans move eyes objects regions interest foveal acuity required immediate task 
eye movements visual attentional targets monitoring eye movements observers provide externally observable marker subjects visual strategies performing tasks manual image indexing passive image viewing 
analyzing eye movements tasks useful understanding humans look images terms recognition strategy areas observed order time person spends looking certain types objects determining deemed important process areas looked probably important areas looked 
years research field content retrieval focused facilitating access multimedia information images video large digital databases 
particular strong interest able automatically classify multimedia data 
images video example placed categories depending visual content levels syntactic low level features colors semantic objects scenes automatically classifying images photographs video important task facilitates indexing allows searching browsing large image collections images internet 
various computational approaches perform automatic classification field computer vision drawn theories functionality human visual system 
order limit amount information processed example techniques detect regions interest regions may relevant problem hand mail ee columbia edu www www ee columbia edu mail cis rit edu www www cis rit edu selected analysis 
understanding selection performed humans visual process deemed useful construction algorithms perform classification 
spite similarities human processing automatic techniques computational approaches general theories directly link specific problem information obtained experiments involving human subjects 
example observe image people shaking hands move eyes specific path fixate areas deem important faces handshake 
areas observe order observations depend highly content image handshake vs landscape task recognize person find object image 
may possible find patterns way different individuals look images category 
information humans perform specific tasks seldom included computational approaches 
analyzing way humans look images lead important improvements construction classifiers class specific observation patterns exist decisions regarding computational recognition process data collected human observers 
study analyze human observers eye movements observing color photographs different semantic categories 
eye movement traces subjects recorded viewed series randomly interleaved images categories handshake people shaking hands crowd people landscape people main object uncluttered background airplane flying miscellaneous people lives 
analyze viewing patterns image variations similar dissimilar patterns image subjects image variations subject pattern depends strongly image image category variations similar dissimilar patterns images category subjects 
addition explore different ways results directly construction automatic classifiers framework visual apprentice 
system image classifiers learned user input user defines multiple level object definition hierarchy scene object object part perceptual area region labels examples specific class handshake interested 
machine learning techniques training data provided user construct classifiers automatically applied new images 

related eye tracking experiments performed years 
studies examined eye movements individuals perform natural tasks 
focused way humans observe pictures photographs paintings 
types studies useful development theories visual perception recognition 
example suggested humans move eyes informative parts image eye movements fixations saccades discussed strongly influenced visual content image task performed observer describe image search object 
studies best authors knowledge tried compare differences eye movements different semantic categories 
computational techniques information eye movements include 
focus studying differences way humans look images different categories usefulness differences construct automatic classifiers categories 
relation computational computer algorithms human selection regions interest areas image deemed important observer studied 
differs goal results category specific eye tracking experiments construct classifiers 

outline section brief overview eye tracking automatic classification 
section describe approach track eye movements 
section presents human observer experiments performed section dis ways results automatic classifiers visual apprentice 
conclude section 

eye tracking 
eye movements automatic techniques array image plane human eye retina highly anisotropic effective receptor density falls dramatically degree central fovea 
acuity demands visual tasks requires high resolution fovea observers move eyes objects regions interest foveal acuity required immediate task 
people eye movements day 
humans move eyes hold gaze stationary point fixations move quickly fixations saccades 
observers gather great deal information images holding fixation subjects free view images instruction regarding movements eyes typically eye movements second 
known time eye movement patterns image dependent degree idiosyncratic 
addition image dependence pattern eye movements spatial distribution fixation points varies instructed task 
demonstrated subjects adopted dramatically different eye movement patterns viewing image instructions changed 
example pattern seen free viewing different subject asked remember location objects image estimate ages people image 
eye movements necessitated limitations peripheral visual field driven scene task general approximately saccadic eye movements second 
eye movements shift point gaze point scene retinal image stabilized ensure high acuity 
observer scene static eyes stationary orbit resulting static image projected retina 
fixations allow high acuity vision 
observer scene motion mechanisms necessary stabilize retinal image 
number oculomotor mechanisms provide stabilization 
objects moving field tracked smooth pursuit eye movements 
large field motion als elicits smooth eye movements stabilize image retina 
image motion due movement head body cancelled reflex produces rotation eyes compensate head body movements 
saccades rapid ballistic movements reach velocities degrees second 
saccades degree extent degrees seen subjects performing number tasks 
duration saccades varies typically completed msec 
speed eyes move saccade retinal image blurred eye movement 
subjects aware blurring caused saccades slight reduction systems sensitivity effect due primarily phenomenon termed backwards masking retinal image captured saccade tends mask blur evident 

automatic techniques approaches proposed automatically determining visual content category images 
techniques roughly separated classify images scenes indoor outdoor city landscape objects depicted images sky faces 
additionally distinction approaches global features classify scene global color texture local features regions 
goal link human observers eye movements automatic classifiers concern mainly approaches local features perform classification 
particular interested approaches local structure see discussion different levels indexing perform classification 
approaches local structure automatic segmentation images 
pixels image automatically grouped color texture low level features 
regions obtained segmentation automatic classifiers 
example configuration regions determine different ways image categories 
mountain scenes example common find blue sky top image directly white regions corresponding snow 
approaches visual apprentice regions grouped structured hierarchies detection objects scenes 
structuring handshake modeled object contains faces handshake closely related way humans move eyes observing images viewer may look faces handshake 
sections discuss eye tracking experiments performed 
section discuss results incorporated automatic techniques visual apprentice 

eye movement recording methods track subject gaze 
systems today offering advantages disadvantages 
system uses coils fine wire held place eye tight fitting annular contact lenses 
eye position tracked monitoring signals induced coils large transmitting coils frame surrounding subject 
coil offer high spatial temporal resolution limit movement require cornea prevent pain due annular contact lens 
system offering high spatial temporal resolution dual 
shine infrared eye monitor reflections surface cornea rear surface second optical element eye 
monitoring images allows eye movements detected independent head translations cause artifacts 
limbus trackers track horizontal eye movements measuring differential reflectance left right boundaries white eye pupil 
vertical eye movements measured tracking position lower eyelid 
limbus tracker provides high temporal resolution eye position signal suffers inaccuracy significant cross talk horizontal vertical eye movements 
class study illuminates eye infrared illumination images eye video camera 
gaze position determined analyzing video fields collected hz 
eye position data collected applied science laboratories model asl remote 
system monitors eye position contact subject important factor consider 
camera lens image eye surrounded infrared emitting diodes providing illumination coaxial optical axis 
infrared video determines point gaze video camera extract center subject pupil point reflection cornea 
tracking pupil reflections cornea allows image processing algorithms distinguish eye head movements motion head respect 
infrared video limited hz sampling rate provides accuracy approximately degree field 
system automatically tracks subjects head movements range approximately cm 
range tracker manually reset 
signals track loss setting horizontal vertical eye positions zero 

remote eye camera left placed just subject line sight right 
lens surrounded infrared emitting diodes provide coaxial illumination 
retina absorbs light enters pupil retina highly reflective far red infrared regions spectrum 
phenomenon leads red eye photographs taken flash near camera lens produces bright pupil eye image 
image iris darkest regions pupil intermediate surface reflection ir source cornea brightest 
eye image processed real time determine pupil corneal reflection centroids turn determine line sight eye respect head 
shows eye image captured asl bright pupil system 
image left shows raw ir illuminated image image right shows image superimposed cursors indicating pupil surface reflection centroids determined thresholding image fitting circle pupil corneal reflection 
observer moves eyes shape pupil reflection changes centroid 
difference centroid pupil centroid corneal reflection points indicated determine actual eye movement 

image eye captured asl system left pupil centroid white cross right corneal reflection centroid black cross right eye position reported horizontal vertical point regard msec 
raw data arbitrary units display scaling viewing distance subject calibration 
data converted image pixel units scaling output calibration points pixel coordinates 
transformation corrected horizontal vertical scaling offset data center image display center image 
represents horizontal top vertical bottom eye position subject reviewing calibration points see 
calibration necessary determine observer position space performed subject long general setup change observer physical location respect camera 
fixation sequence left right top bottom 
repeated step pattern horizontal trace shows sequence horizontal fixation points line calibration target 
vertical record indicates rows scanned turn 
zero slope portions graphs represent fixations calibration points transitions fixations represent rapid saccadic eye movements calibration points 

horizontal vertical eye position point calibration sequence image pixel units 

horizontal vertical position point calibration sequence image pixel coordinates left 
fixation density mask overlaid calibration grid rescaled right 
left shows data plotted dimensions show spatial distribution 
horizontal vertical eye position point calibration sequence seen scaled image display coordinates 
fixation pattern visualized image lightness pixel proportional fixation density region 
right shows data displayed image mask 
fixated regions calibration target visible mask dark regions image locations fixated subject data collection correspond actual calibration points 

experiments goal eye tracking experiments determine individuals scan images category similar ways differences different categories 

image data set vert sec experiments selected color images different categories handshake people standing near shaking hands main object uncluttered background prominent object center image uncluttered background crowd cluttered scenes people landscape natural landscapes people miscellaneous lives people 
handshake crowd main object images vert collected line news service 
landscape miscellaneous images obtained collection photographer authors 
images experiments resolution approximately pixels 
note important parameter angle subtended image discussed section 

example images categories experiments left right handshake main object crowd landscape miscellaneous 

subjects volunteers females males undergraduate students participated experiment 
subjects native english speakers na goals experiment participated experiments past 
experiment subjects read signed informed consent form describing apparatus 
observers told observe images explanations regarding goal experiment number image categories 
images obtained news source possible subjects familiarity persons places photographs 
distinctions regard 
subjects viewed total images 
images interleaved random order viewed seconds 
experiment broken sessions consisting images lasting approximately minutes 
subjects heads restrained sessions instructed maintain gaze tv display 
subjects took self timed break sessions typically lasting minutes second set images 
goal determine primary areas interest image viewing time selected sufficient allow fixations typically fixations long encourage subject scan entire image 
pilot experiments indicated second exposure appropriate 
visual examination image important viewing variable visual angle subtended image 
angular image selected approximate observer viewing photograph magazine newspaper cm wide image field viewed distance cm 
subjects seated meter ntsc television monitor screen dimensions cm cm 
images subtended mean degrees visual angle exact value varied head position stated value 
remote system adjusted just line sight get best view eye obscuring portion monitor 
thresholds pupil corneal reflection discrimination set subject optimize thresholding determine pupil corneal reflection centroids seen 
system calibrated subject instructing subject fixate point rectangular calibration grid display 
raw pupil corneal reflection centroids recorded calibration point location points image coordinate space establish relationship measured pupil corneal reflection position point gaze image plane 
sample image miscellaneous image class 
shows eye position function time left panel dimensions right panel subject 
pattern regular calibration set seen clearly relatively long fixations separated rapid saccadic eye movements 
twodimensional plot shows fixation patterns tied image content eyes moved broad area image majority trial spent looking small number image regions 
left panel shows fixation pattern superimposed target image point represents gaze position video field msec 
right panel shows data mapped image degree circles indicating fixation defined degree regions containing data points msec 
multiple fixations evident left panel result larger indicators 

example image misc class images viewed color 
eye position trace subject image time left image pixel units right 
point right represents msec sample 
overlay display 
left panel indicates gaze position msec intervals 
individual fixations indicated right panel circles approximately degree diameter 

eye tracking results sec experiments resulted fairly large amount data specifically eye tracking results subjects images categories 
subject viewed image approximately seconds sampling rate eye tracker hz average data points subject image total approximately data points entire experiment 
possible mathematically process acquired data mathematically compare fixations different subjects image category discussed section 
section focus observations viewing patterns observed including fixations saccades 
particular discuss viewing patterns image variations similar dissimilar patterns image subjects image variations subject pattern depends strongly image image category variations similar dissimilar patterns images category subjects 
observations experiments described 
general viewing patterns similar subjects viewing image idiosyncratic behavior evident 
shows fixation plots subjects viewed image 
subjects fixated main figures image number fixations common subjects 
example shows image consistent patterns different individuals 
vert vert 
image fixation mask overlay subjects 
shows similar data different image 
fixation density plots subjects similar 

image fixation mask overlay subjects 
previous examples suggest cases possible find consistent viewing patterns different individuals image 
viewing patterns varied single individual depending specific image observed 
example shows fixation patterns single subject viewing different images highlighting strong image dependency viewing patterns 

fixation mask overlays subject image 

dissimilar viewing patterns image 
fixations subjects image 
cases evident wide variations viewing patterns occurred different subjects viewing image 
illustrates images wide variation 
terms categories consistency handshake main object classes little consistency viewing patterns remaining categories landscape crowd miscellaneous 
note illustration purposes plotted data points subjects images categories 
distinction saccades fixations plots fixation concentrations readily seen higher concentration points 
note example handshake class visible clusters resulting faces appear image class patterns different categories 

data points subjects images handshake category left main object right category 
interesting note general handshake class subjects spent time looking face left face right 
additionally somewhat surprising find cases observers look data points occurred handshake 
summary images consistent viewing patterns subjects viewed image similar way images inconsistent viewing patterns subjects viewed image different ways strong image dependence subject different patterns different images 
addition consistent viewing patterns handshake main object image categories differences categories 

automatic classification previous discussed eye movement variations subject images categories 
goals determine results eye tracking experiments construction automatic classifiers 
data analysis categories may useful construct classifiers classes studied handshake 
briefly describe visual apprentice framework va explore different ways data build classifiers framework 
main principles va allow users construct classifiers simple training stage 
user collects example images videos class interested 
images automatically segmented color edge information provided user constructs object definition hierarchy simple graphical user interface labels regions example image video hierarchy see 
hierarchy subjectively defined user models object scene different levels object composed object parts composed perceptual areas turn contain regions obtained segmentation 
result training stage set labeled regions node hierarchy defined user 
example user constructing handshake classifier regions corresponding handshake faces handshake labeled sample image 
system computes feature vectors color texture location example regions uses different learning algorithms decision trees nearest neighbor construct classifiers node hierarchy defined user 
new images going classified automatically segmented classifiers constructed training stage applied hierarchy 
example face region classifiers find candidate face regions grouped face object part classifier details 
level object level object part level perceptual level region perceptual area object object part object part object part perceptual area region region handshake face handshake face regions regions regions 
generic object definition hierarchy visual apprentice left hierarchy handshake classifier right 
training user manually clicks regions correspond object definition hierarchy defined 
possible eye tracking data class manual labeling consists fixation points subjects image select relevant training regions 
expected shown experiments handshake class example observers fixate gaze faces people shaking hands possibly handshake 
preliminary analysis data showed selection trivial complications 
visual acuity accuracy eye tracker pixels considered performing selection fixation points certain dis tance pixels setup indistinguishable 
current va setup user manually clicks inside regions wants label single pixel location image coordinates sufficient accurately select label regions 
fixation point eye tracking experiments corresponds data points eye tracker fixation point defined lasting msecs data points points clustered way fixation center fixation area computed 
shows example fixation areas obtained eye tracking results shows set image regions obtained automatic segmentation automatically selected fixations subjects 
words regions obtained automatic segmentation overlap image fixations selected shown 
clearly seen relevant regions selected selected human training system handshake regions missing image irrelevant regions selected 
errors segmentation algorithm easily corrected human selects correct regions objects parts interested 
eye tracking results directly poses challenge terms selection regions way eye tracking data plays important role 
authors compared automatically selected regions interest regions selected human observers eye movements proposed technique cluster fixations compare different viewers 
experiments possible apply approach cluster points decisions regarding data clustering algorithm criteria group fixations different observers trivial strong impact construction automatic classifiers 
particular framework examining variations criteria result selection different regions 
factors humans may fixate certain areas objects eyes face areas may yield best results terms selecting relevant regions 

example regions selected fixations human observers 
addition selecting regions automatically fixations possible modify algorithm va additional information provided eye tracking experiments 
case fixations give certain regions weight easily included va framework importantly include regions selected saccades 
furthermore order included classification strategy classifiers applied order 
alternatively entirely new regions extracted training data automatically segmented regions masks produced eye tracking experiments construct classifiers 
actual classification approach case modified classification stage regions extracted training data extracting regions class specific knowledge trying classify 

eye tracking experiments show variations eye movements fixations saccades different individuals color photographs different categories handshakes people shaking hands crowds cluttered scenes people landscapes nature scenes people main object uncluttered background airplane flying miscellaneous people lives 
particular viewing patterns image variations similar dissimilar patterns image subjects image variations subject pattern depends strongly image image category variations similar dissimilar patterns images category subjects 
specifically consistent patterns handshake main object categories 
importantly patterns different categories handshake main object 
results experiments suggested ways type data construction automatic image classifiers 
particular examined visual apprentice system image classifiers learned machine learning user input user defines multiple level object definition hierarchy object parts scene object object part perceptual area region labels examples specific classes handshake 
resulting classifiers applied automatically classify new images 
analysis preliminary results experiments encouraging suggest may possible eye tracking data construction automatic classifiers 
analysis data selection clustering fixation points scan order plays important role criteria strong effect classifiers built 
includes analysis data construction automatic classifiers eye tracking results 

acknowledgments authors wish diane amy silver persons participated experiments 

becker metrics neurobiology saccadic eye movements 
wurtz eds 
elsevier science publishers 
people look pictures university chicago press chicago 
chang smith benitez visual information retrieval large distributed line repositories communications acm december 
van der steen effect freeing head eye movement characteristics dimensional shifts gaze tracking chapter head neck sensory motor system berthoz graf vidal eds 
oxford university press 
tom crane hewitt patent 
gaze shift dynamics kinds sequential looking tasks vision res 

eye movements vision behavior john wiley sons new york 
gould looking pictures eye movements psychological processes edited senders john wiley sons new york 
fe benson aj tracking performance sinusoidal stimulation vertical horizontal canals advances aerospace medicine ed 
reidel publ 
dordrecht netherlands 
chang 
automatic selection visual features classifiers proceedings spie storage retrieval media databases vol 
san jose ca january 
chang conceptual framework indexing visual information multiple levels proceedings spie internet imaging vol 

san jose ca january 
land knowledge base oculomotor system 
phil trans soc lond 
lipson context configuration scene classification ph thesis mit electrical computer science department september 
marr vision computational investigation human representation processing visual information 
freeman san francisco 
stark saccadic eye movements viewing recognizing patterns vision research paek chang 
image classification system probabilistic reasoning invited international conference image processing icip special session image content extraction description multimedia vancouver canada september 
babcock silver konno portable study natural eye movements proceedings spie human vision electronic imaging san jose ca 
stark evaluating image processing algorithms predict regions interest pattern recognition letters pp 

stark algorithms defining visual regions interest comparison eye fixations ieee transactions pattern analysis machine intelligence pami sept 
robinson method measuring eye movements search coil magnetic field ieee trans bio med electron bme 
model visual perception recognition vision research 

beinlich hybrid system scene analysis saccadic eye movements learning feature relations european conf 
visual perception oxford england 
smith chang multi stage classification images features related text proceedings fourth delos workshop pisa italy august 
cognition visual arts mit press cambridge mass 
szummer picard indoor outdoor image classification proceedings ieee international workshop content access image video databases pages bombay india 
vailaya figueiredo jain zhang content hierarchical classification vacation images proceedings ieee multimedia computing systems vol 
florence italy june 
eye movements vision new york plenum press 
