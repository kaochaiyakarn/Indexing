networked windows nt system field failure data analysis presents measurement dependability study networked windows nt system field data collected nt system logs servers running production environment month period 
event logs hand contains system reboot information 
study individual server failures domain behavior order characterize failure behavior explore error propagation servers 
key observations study system software hardware failures major contributors total system downtime recovery application software failures usually quick cases reboots required recover failure average availability individual server strong indication error dependency error propagation network reboots unclassified indicating need better logging techniques maintenance configuration contribute system downtime 
network tens hundreds pc servers errors propagate machine single error may affect multiple computation nodes 
servers usually loosely coupled share system resources cooperate providing required services user 
loose dependencies servers difficult impossible identify error failure dependencies networked system 
consequence lot academia industry designing building networked systems characteristics failures occurring systems understood 
study set field failure data 
goals understand nature failures net jun xu ravishankar iyer center reliable high performance computing university illinois urbana champaign mail iyer crhc uiuc edu worked systems identify performance dependability bottlenecks systems 
data collected networked windows nt system running production environment providing services mail file system services resource management 
data study obtained event logs collected month period analysis solely machine reboots time event type collected 
despite fact data includes system reboots able gain useful insights nature observed failure behavior typical problems networked systems 
key findings summarized system software hardware failures major contributors total system downtime total system downtime respectively 
failure categories responsible system software hardware observed reboots 
planned maintenance changes software hardware configuration responsible reboots 
repair times shorter contribute total downtime 
application software failures responsible reboot usually repaired quickly contribute total downtime 
reboots single multiple machines domain tend occur bursts 
indicates problems removed single reboot cases rebooted machine encounters problems hour 
average availability individual servers strong indication error propagation network 
high probability multiple servers failing short interval machines domain fail hour 
organized follows 
section discusses related 
section presents data collection method data format 
section outlines failure classification categorization analyzes dependability parameters different failure categories 
section talks failure behavior individual servers 
section presents failure behavior domain explores error propagation 
conclude section 
related analysis failures computer systems focus active research quite time 
lee analyzed failures tandem guardian operating system 
study processor halts examined memory management software interrupt handlers major causes processor halts 
lee examined software dependability guardian operating system 
study categorized failures underlying software problem uninitialized pointers incorrect update data structures 
evaluation revealed systems able tolerate processor failures due fault tolerance techniques 
tang analyzed error logs pertaining multicomputer environment vax vms cluster 
thakur analysis failures tandem nonstop ux operating system 
study analyzed problems resulted system crashes 
thakur described simple effective methodology collecting analyzing failures network unix workstations 
majority observed failures encountered network related 
analyzed machine reboots collected network windows nt mail servers 
observed average availability machine high typical machine domain provides acceptable service time average 
explored errors recovery ibm mvs operating system 
error logs collected mvs systems semi markov model multiple errors errors manifest multiple ways constructed describe system failure behavior 
measurement software reliability models guardian system vax cluster 
impact workload system failures extensively studied 
castillo developed software reliability prediction model took account workload imposed software 
study examined hardware transient faults occur timesharing systems proposed model software reliability 
iyer examined effect workload reliability ibm operating system 
study proposed validated load hazard model order measure risk system failure increasing workload 
mourad performed reliability study ibm mvs xa operating system error distribution heavily dependent type system utilization 
meyer analysis influence workload dependability computer systems 
maxion useful interesting results anomalies detection 
lin tsao focused trend analysis error logs 
results analysis transient errors computer systems 
study showed transients follow weibull distribution occur constant rate frequently assumed 
gray results census tandem systems 
chillarege study impact failures customers fault lifetimes 
sullivan examined software defects occurring operating systems databases field data 
depth overview experimental analytical techniques analysis computer systems dependability 
field data analysis parameters section presents data collection method data format defines dependability parameters failure data analysis 
data collection method format field field description server name server outage start date time outage started 
boot time date time server rebooted 
reboot ends outage outage time 
shutdown type clean dirty outage reason cause outage reboot time time operator annotation tier operator primary reason reboot tier operator secondary reason reboot 
operator comm operator freeform notes reboot 
table failure data definitions field failure data collected pc servers running microsoft windows nt service pack month period corporate production environment 
servers operate continuously implement major internal services applications file print services mail service enterprise resource planning product support service sales automation corporate marketing 
data collected event log analyst tool 
collects reliability information nt event logs 
runs single server collection server sequentially retrieves event log information nt servers system 
collects events system access data system reboots 
table shows data format 
parameter definitions dependability parameters calculated field failure data 
mean time failure mtbf average time interval hours start time outage start consecutive outages 
time difference mtbf mttr see mean operational time server 
mean time repair mttr mean time hours outage start boot time outage 
mttr measure system downtime 
availability availability defined mtbf mttr mtbf 
parameters evaluate single server entire domain machines mail server perform services geographically closely distributed 
outage failure interchangeably 
failure classification analysis section discuss failure classification distributions time failures 
classifications observations field data provide information errors failures system 
assess impact different failures system dependability recorded failures categorized 
dependability parameters including mtbf mttr evaluated respect individual failure categories 
unclassified hw maintainance distribution number outages unclassified hw maintainance downtime distribution categories reboots sorted outage reason field 
field allows distinguish unique outage reasons 
classified reasons categories shown table 
classification mtbf mttr individual failure categories calculated table 
show distributions failure categories outage numbers downtime respectively 
obtained results came observations due hardware system software failures larger types 
table shows hardware system software failures times larger mttr failures respectively 
categories contribute total downtime constitute failures 
planned maintenance software hardware configuration install add reboots 
shorter contribute total downtime 
consequently better maintainability configurability essential networked system achieve higher availability 
shutdown machine services provides potentially affect servers system cause chained shutdown 
highly desirable system support maintenance hot hardware software upgrades 
failures system log show specific reasons 
better logging techniques required provide accurate information 
enable accurate analysis system availability precise identification system vulnerabilities application software failures usually recover quickly contributing total downtime 
mttr times smaller hardware system software failures see table 
outage reasons description hw ram fail power supply disk fail cpu fail isdn hw modem fail nic fail tape issue hardware failures problems net hw config firmware hw config move server hw install hardware config install invoice sap exch err exch web svc fail sql fail compass exe exch fail exch restore exch store sql svc fail sql tools exch sp sql inst net sw config sw install sp inst outlook inst sw config rename server vm config iis install inst inst nt sp inst sql inst application software problems software install nt hang gamma sys vm rpc dns fail wins ras fail system software failure maintenance sap weekly nt planned maintenance customer req trust sfm video op error outage reason pcd trust print fail troubleshooting miscellaneous unclassified unclassified don reason outages mtbf mttr hw maintenance unclassified table stats different failures unit hour distribution time failures compute distributions recorded reboots selected failure categories system software failures hardware failures contribute total system downtime 
show distribution time failures entire system including unclassified failures entire system excluding unclassified failures system software failures 
data labels show numbers bin histogram 
percent hours interval size table classification outage reasons alpha beta distribution outages percent hours interval size hour alpha beta dist classified outages percent hours interval size alpha beta dist system software failures distributions obtained outages system unclassified failures outages due system software failures follow weibull distribution parameters figures 
find distribution fit outages due hardware failures 
indicate excluding unclassified failures change distribution time failures 
observe system software failures hardware failures time failures days 
failure behavior individual servers shows empirically obtained distribution availability server servers days service pack runtime considered 
individual servers demonstrate high availability servers availability higher 
servers experienced significant number outages month period 
picked servers largest numbers outages conducted server analysis 
table shows results servers 
interesting observation standard deviations selected servers large 
implies outages specific server clustered closely time 
percentage servers total number servers server availability distribution server erm availability mtbf hr stddev mttr hr table individual servers failure statistics show failures distributed time 
experienced reboots month period respectively 
see time axis servers run stable 
machine encounters error fails usually experiences failures reboots returns stable state 
coalesced reboots machine different coalescing interval periodic behavior repeats days days average 
tried determine cause behavior unfortunately reboot reasons unclassified 
detailed data needed conduct thorough analysis conclusive statement 
similar periodic behavior observed servers 
te te outage sequence number server failure behavior outage sequence number server failure behavior periodic failure behavior series reboots recover failure possible reasons identified server runs stable certain amount time crashes possibly due cumulative problems memory leaks file system errors 
crash recurs system shutdown incomplete system cleanup failure leave system inconsistent state 
operating system able recover possibly repair file system inconsistencies system configuration file damages problems quickly usually takes reboots fully recover 
domain behavior error propagation analysis section describe results analysis domain failure behaviors 
analysis techniques goal domain behavior analysis explore failure patterns determine indication 
define domain set closely connected servers providing similar services mail 
servers domain usually depend instance electronic mail domain server perform forwarding directory service servers domain 
example enterprise resource management domain servers cooperate perform resource allocation 
difficult identify error dependencies system servers choose domain analysis approach explore error propagation networked environment 
processed logging data data coalescing techniques identify error dependency 
coalescing algorithms merge errors type occur certain interval tuple approach slightly different 
objective study error propagation dependency coalesce failure entries cluster regardless failure type 
particular coalescing algorithm works follows time previous failure put current cluster create new cluster note interval coalescing algorithm acts sliding window process clustering failure events 
depending failure frequency size cluster terms time larger see section 
choice important 
long short cause collision truncation 
published results interval minutes single processor machines tightly coupled systems 
case networked system loosely coupled consider reboot data choose value larger usual minute 
choice difficult tries chose hour 
error dependency propagation conducted analysis domains performing enterprise resource management erm domain providing mail service mail domain 
due limited space detailed results erm domain 
important statistics mail domain shown table 
distinguished characteristic erm domain servers planned weekly reboots weekend midnight rebooted time 
remove weekly reboots log data distort analysis 
table shows statistics obtained server domain weekly reboots removed 
mtbf mttr columns computed data coalescing mtbf time consecutive clusters mttr duration cluster mttr mttr clusters failures 
table gives number clusters cluster total number reboots reb reboots clusters reb cluster machine 
observe coalescing algorithm set hour mttr larger erm erm erm 
employed sliding window coalescing reboots 
get insight error propagation compute number clusters total number reboots number reboots clusters coalesced data entire domains 
table shows results erm domain mail domain entire system 
number clusters computed server erm domain adds sum cluster column table clust sum column table clusters clust column entire erm domain shown table 
similar situation true mail domain number clusters computed server domain adds clusters entire domain 
clear indication error propagation domain 
errors propagate cumulative number clusters obtained coalescing failure events individual servers domain number clusters entire domain distinguishing individual servers close 
domain clust sum server mtbf mtbf mttr mttr mttr cluster reb reb cluster erm erm erm erm erm erm erm erm erm erm erm erm erm erm erm clust reb reb clust mach clust entire erm mail table domain statistics indication error propagation fact number different servers involved cluster erm domain mail domain shown column mach clust table 
errors dependent propagate probability different machines fail hour window small expected number machines failed single cluster shall close high availability individual server shown 
frequency machine cluster number machines cluster histogram erm domain table erm domain statistics table transition probability matrix erm domain gives number machines cluster histogram 
observed third clusters involve machine clusters machines involved 
studying machines involved cluster reasons outages classify error dependency propagation 
obtain accurate understanding domain failure behavior modeled domain terms state transition matrix 
model reboots domain 
domain state fully functional state servers outage state state partially functional states servers outage 
selection assignment states performed follows 
outages split time windows hour 
window domain assigned state 
assignment states number servers outage 
table shows transition probability matrix different states erm domain 
source states specified column target states column headings 
numbers show transition probability source state target state 
transition probability matrix shows time domain fully functional state 
domain enters state server non negligible probability system stay state servers fail 
example table shows state chance domain stay state chance server fail 
indication possible error propagation network 
presents case study preliminary failure characterization windows nt networked systems 
analysis real world failure data collected error logging mechanism underlying operating system 
results show servers provide high availability clear indication error propagation network 
issues effective system maintainability fast recovery key achieving higher system availability 
study available logs provide useful data failure behaviors nt systems 
hand learned cases information contained logs sufficient definite interpretation 
believe errors interpreting log data conservative side overestimate system 
study starting point complete analysis windows nt systems 
plan continue detailed system logs allow address issues individual server long recovery system software hardware failures 
castillo siewiorek workload dependent software reliability prediction model proc 
th int 
symp 
fault tolerant computing pp june 
chillarege rosenthal measurement failure rate widely distributed software proc 
th int 
symp 
fault tolerant computing july 
gray census tandem system availability ieee trans 
reliability vol 
pp 
october 
hansen models time coalescence event logs proc 
nd int 
symposium fault tolerant computing pp july 
iyer measurement model software reliability production environment proc 
th annual int computer software applications conference pp 
october 
iyer measurement modeling computer reliability affected system activity acm trans 
computer systems vol 
pp 
august 
iyer effect system workload operating system reliability study ibm ieee trans 
software engineering vol 
se pp 
december 
iyer tang experimental analysis computer system dependability chapter fault tolerant computer design pradhan prentice hall pp 
analysis failures windows nt systems master thesis technical report crhc university illinois urbana champaign 
lee iyer analysis software halts tandem system proc 
rd int 
symp 
software reliability engineering pp 
october 
lee iyer software dependability tandem guardian operating system ieee trans 
software engineering vol 
pp 
may 
lin siewiorek error log analysis statistical modeling heuristic trend analysis ieee trans 
reliability vol 
pp 
october 
meyer wei analysis workload influence dependability proc 
th int 
symp 
fault tolerant computing june 
maxion anomaly detection diagnosis proc 
th int 
symp 
fault tolerant computing pp 
june 
maxion feather case study ethernet anomalies distributed computing environment ieee trans 
reliability vol 
pp 
october 
mourad andrews reliability ibm mvs xa operating system ieee trans 
software engineering october 
siewiorek tsao measurement analysis transient errors digital compute systems proc 
th int 
symp 
fault tolerant computing pp 

sullivan chillarege software defects impact system availability study field failures operating systems proc 
st int 
symp 
fault tolerant computing pp 
june 
sullivan chillarege comparison software defects database management systems operating systems proc 
nd int 
symp 
fault tolerant computing pp 
july 
tang iyer analysis vax vms error logs multicomputer environments case study software dependability proc 
third int 
symp 
software reliability engineering research triangle park north carolina pp 
october 
thakur iyer young lee analysis failures tandem nonstop ux operating system proc 
int symp 
software reliability engineering pp 

thakur iyer analyze environment collection analysis failures network workstations ieee trans 
reliability vol 
pp 

tsao siewiorek trend analysis system error files proc 
th int 
symp 
fault tolerant computing pp 
june 
