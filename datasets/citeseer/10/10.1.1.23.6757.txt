journal machine learning research xx submitted published kernel target alignment nello cristianini nello cs berkeley edu technologies berkeley university california berkeley computer science department soda hall berkeley ca usa kandola cs rhul ac uk royal holloway college university london egham surrey tw ex uk andre andre com technologies new york broadway th floor new york new york usa john shawe taylor john cs rhul ac uk royal holloway college university london egham surrey tw ex uk editor kernel methods increasingly data modeling conceptual simplicity outstanding performance tasks 
kernel function chosen trial error heuristics 
address problem measuring degree agreement kernel learning task 
quantitative measure agreement important theoretical practical point view 
propose quantity capture notion call alignment 
study theoretical properties derive series simple algorithms adapting kernel labels vice versa 
produces series novel methods clustering transduction kernel combination kernel selection 
algorithms tested publicly available datasets shown exhibit performance 
keywords kernel methods alignment clustering transduction 

kernel machines embedding data feature space linear algorithms detect patterns images data 
choice right embedding crucial importance kernel create di erent structure embedding space 
able assess quality embedding crucial task theory kernel machines far treated marginally 
propose measure similarity clusterings set points cristianini kandola shawe taylor cristianini kandola elisseeff shawe taylor assess relations embeddings generated di erent kernels assess similarity clustering labeled dataset induced kernel induced labels 
intuitively points class near far points class 
quantity called alignment shown capture notion clustering achieving high similarity clusters low similarity 
formally regarded frobenius inner product normalized kernel matrices name 
convenient properties eciently computed training kernel machine takes place training data information sharply concentrated expected value empirical value stable respect di erent splits data importantly show kernel aligned labels exists separation data low upper bound generalization error 
observations mean possible measure optimize quantity training set information achieve better generalization power test set transductive setting 
give series algorithms aimed exploiting fact 
alignment measure introduce shown related pearson correlation coecient extensively statistics community random variables correlation measured regarded correlation measure di erent descriptions data 
interesting practical application quantity gives clear rule kernel combination alignment target improved combining kernels separately aligned aligned 
rst algorithm give method improve alignment kernel xed set labels acting eigenvalues 
algorithm performs transduction provides nonparametric way perform kernel selection require specify family kernel functions directly acts entries kernel matrix 
similar algorithms making matrices kernel matrix closely related discussed 
aligning kernel labels consider opposite problem choosing labels dataset maximize alignment xed kernel 
de ne notion maximal alignment kernel set data maximal alignment achievable respect possible labelings 
computational cost problem exact solution intractable provide simple algorithm nding approximately maximally aligned set labels dataset upper bound maximal alignment optimal alignment lower upper bounded 
algorithm eigenvectors kernel matrix analogous certain techniques spectral graph theory 
experiments real world data shows optimal clustering performance attained maximum alignment upper lower bounds tight 
kernel target alignment kernels similarity generalized clustering kernel learning methods reviewed section notion kernel matrix gram matrix informally regarded pairwise similarity matrix pairs points dataset 
course necessary de ne notion similarity kernel methods inner product points suitable feature space information obtained little computational cost high dimensional spaces 
resulting matrix symmetric positive semi de nite eigenvalues non negative reals consequently written eigenvectors eigenvalues information needed learning machine resulting data similarity measure contained gram matrix 
properties re ect relative positions points feature space 
example obvious kernel matrix identity correspond having points orthogonal feature space useful notion similarity point similar point way 
split data clear way assign new point class 
generally heavily diagonal kernel matrix known lead tting 
course knew priori speci classi cation clustering target function learned optimal kernel function ij 
labels vector denoted corresponding kernel matrix yy rank 
object key concepts 
corresponds having total knowledge target function choosing truly relevant feature data target 
course real problem access information expectations assumptions derived example prior knowledge 
case choose kernel function form labelings expected 
prove simple argument want assumption hidden target function resulting kernel matrix going diagonal trivial 
general kernel matrices regarded form soft clustering directly assigning points class give similarity level 
similarity measure matches hidden target function easier learning problem 
obvious example relation kernel quality task seen context text categorization 
domain standard choice kernel bag kernel delivers state art performance classi cation documents topics 
choice kernel exists implicit assumption regarding type data partitioning want perform 
kernels useless wanted implement perfectly legitimate separation documents number characters documents odd number characters 
kernel problem anyway easy design length document kernel course perform poorly task topic categorization 
cristianini kandola elisseeff shawe taylor importance measuring level matching kernel learning problem highlighted simple proof showing general purpose kernel bound give trivial results assumptions learning target need incorporated kernel 
alignment results machine learning free lunch ugly duckling theorem luckiness framework wolpert scha er watanabe point direction learning possible prior assumptions solution turn correct 
boils detecting relation input distribution target distribution 
example observing large margin support vector systems guarantee input distribution naturally matches labels received 
model feature selection problems regarded way choosing data representation best matches target function course representation chosen label information paying price 
kernel learning systems input distribution warped nonlinear mapping feature space better talk similarity kernels target 
capture notion similarity kernels kernel target 
reduced assessing similarity gram matrices regarding target kernel yy 
kernels equivalent dataset induce metric gram matrix create clusters 
way capture notion consider frobenius inner product normalized matrices 
quantity maximized identical matrices zero orthogonal ones 
denote frobenius product matrices hm ni ij ij ij mn 
central quantity measure similarity kernel matrices 
formal de nition requires technicality provided section give intuitive rigorous de nition 
kernel labeled dataset kernel matrix vector labels alignment kernel labels written follows hk yy hk ki hyy yy notice measure hk yy measuring class classes distances 
rst part investigate properties quantity deduce algorithms kernel selection 
take di erent viewpoint adapting kernel labels adapt labels kernel 
leads dicult combinatorial problem solved approximate way 
solution automatically provides kernel target alignment assessment quality approximation achieved regarding alignment function labels hk ki ky hyy yy 
approach provide algorithms transduction completing labeling partially labeled dataset unsupervised learning 
discuss important relation eigendecomposition kernel matrix expression kernel combination possible labelings implications 
intuitively natural labelings induced kernel matrix data correspond clusterings induced eigenvectors 
main theoretical results de nition problem alignment proof sharply concentrated expected value 
furthermore establishing connection high alignment generalization 
main practical contributions deducing kernel combination criterion algorithm approximating aligned labelling related method minimizing cost functions series methods kernel matrix adaptation moving step direction nonparametric kernel selection sense choosing parameterized kernel family tuning parameters maximize alignment measures directly adapt entries kernel matrix obtain better 
contribution promising provides natural algorithm problem transduction completing labeling partially labeled dataset 
method summarized chosen suitable kernel built kernel matrix unlabeled data kernel matrix modi ed maximize alignment labels 
optimal labeling induced new matrix approximately calculated 
outline notation section review basic concepts kernel methods 
section prove universal kernel induces trivial kernel matrices leads poor generalization conclude kernel needs match learning task 
section propose alignment measure matching derive theoretical properties including sharp concentration 
section prove alignment high exists classi cation hyperplane 
section give series algorithms kernel combination clustering transduction 
denote yy outer product vector transpose denote function 
rst quantity course empirical instantiation second obtained sampling points distribution 
results proven sections general version clear context simply hk ki hk ki important confuse notation similar di erent denote hk ki inner product weighted distribution corresponding norm denoted jj jj see section details 
number training points simple relation uu hv ui cristianini kandola elisseeff shawe taylor 
kernel methods kernel methods machine learning conceptual simplicity remarkable performance 
exploit information pairwise similarity data points obtained considering inner products images feature space 
map possibly high dimensional feature space performed implicit way explicitly writing image data points space directly obtaining inner product 
statistical learning methods developed ectively limit impact high dimensionality generalization known practitioner tting observed particularly kernel function chosen 
developing algorithms inner products images di erent inputs feature space application possible rich feature spaces provided inner products computed 
key advantages approach modularity decoupling algorithm design statistical analysis problem creating appropriate function feature spaces particular application 
de ning appropriate kernel function allows range di erent algorithms analyze data concerned potentially answering practical prediction problems 
particular application choosing kernel corresponds implicitly choosing feature space kernel function de ned feature map 
training set fx information available kernel algorithms contained entirely matrix inner products referred gram kernel matrix 
kernels knowing feature space implicitly de ned long guarantee space exists kernel regarded inner product space 
characterize valid kernel functions ways 
simplest way probably proposition saitoh function valid kernel nite set produces symmetric positive de nite gram matrices 
explicit feature map equation compute corresponding kernel 
methods sought provide directly value kernel explicitly computing 
enables extremely rich feature spaces nite dimensional computational point view 
working irrelevant features attractive long relevant ones 
see leads fundamental problems 
best known method type referred polynomial kernel 
kernel polynomial construction creates kernel applying polynomial positive coecients example consider kernel target alignment xed values integer suppose feature space feature space indexed tuples features relatively small additional computational cost time inner product computed addition exponentiation required algorithms applied feature space vastly expanded expressive power 
extreme example consider gaussian kernel de ned exp feature space nitely dimensions 

free kernel section consider notion universal kernel meaning kernel completely independent domain knowledge equally suitable domain 
obtain series results emphasizing impossibility learning kernel 
de ne trivial kernels kernels value known observing data 
kernels training set provides information uncertainty matrix entries 
show universal kernels fact trivial 
de nition trivial kernels trivial kernel kernel evaluation depends point 
kernel machine algorithms information contained gram matrix trivial kernels result trivial algorithms particular classi cation functions constant lookup table 
de nition universal kernels universal kernel kernel features incorporate assumptions target concept 
example inner product feature space includes possible feature vectors set random features 
de nition leaves open question kernel de ned particular input space 
apparent attraction universal kernel able learn classi cation classi cations components feature vector 
stated explicitly may clearly optimistic hope frequently hinted example gaussian kernel solve classi cation problem considered universal 
rst result demonstration hope realized turns universal kernels trivial resulting kernel learning algorithms trivial kernel 
theorem free kernel consider nite input space universal kernel ku exists equal kronecker delta function cristianini kandola elisseeff shawe taylor ku 
constant proof jxj distinct feature vectors feature space components vectors exists 
clearly ku jxj components agree 
feature vector vectors agree points take possible combinations values contributions features universal kernel cancel 
true feature vectors inner product feature vectors zero 
theorem closely related watanabe ugly duckling theorem prior knowledge ugly duckling similar swan swan watanabe watanabe proceeds show perverse choice feature space ugly duckling similar 
theorem states distinct points equally similar absence prior information mapped equally distant images feature space 
theorem shows impossible universal notion similarity 
notion encode assumptions 
theorem simply expressed non nite input spaces 
close analogy euclidean spaces consider gaussian kernel size width tend zero 
provided training points separated distances larger possible classi cations data achieved feature space spanned eigenvectors large eigenvalues 
inner product distinct pair inputs close zero gaussian kernel approximates universal kernel 
note value learning result look table generalization con ned points order distant training point 
hope construct universal kernel random features de nition related task hand 
see kernel trivial inner product vectors computed randomly selected subset features concentrated true value inner product kernel constructed randomly selected subset features exhibit structure close universal kernel 
particular ratio diagonal diagonal elements large learning correspondingly hampered 
formal proof omitted space limitations 
draw considerations clear prior knowledge source generating data classi cations inserts knowledge feature representation impossible learn 
dream immensely rich feature space contains possible features realistic 
section examine measure correlation kernels classi cation tasks 
measure give intuition expect generalization performance provide estimate quality generalization simplest classi cation functions 
kernel target alignment 
kernel target alignment observing domain knowledge kernel give trivial results section examine domain knowledge ect performance kernel machine 
particular look matching kernel unknown target function help generalization 
basic idea kernel extract features related target learned 
unknown guess class tasks expected contain target function 
kernel give results functions class 
see larger class harder learning limit class contains possible targets back cases discussed 
level matching prior assumptions implicit kernel choice true target function concept alignment de ned section 
motivation approach consider case required learn function 
case ideal feature vector desired classi cation 
corresponding kernel function learn target example note learn negation function 
functions obvious feature space gives rise kernel distinct functions realized general points images input points feature space xg dichotomies points realized 
related situation arises kernel kernel operator dp input space distribution generating data nonzero eigenvalues corresponding eigenfunctions 
case write cristianini kandola elisseeff shawe taylor note input space nite integral interpreted sum 
case feature space dimensional eigenfunctions constrained outputs equal 
general possible realize functions training set size add possible targets number functions realized training set size bounded minf mg distinct points lying dimensions 
general dimensional feature space number functions bounded 
general consider kernel realizing set possible dichotomies 
natural generalization approach leads gaussian processes kernel prior probability distribution function class general insights give de nition alignment 
de nition inner product functions functions hf dp dp denote function 
de nition alignment alignment kernel kernel quantity hk hk hk follows cosine angle functions consider special case 
express alignment kernel target hy ky yk kkk hy kkk ky yk dp dp dp kernel target alignment intuition de nition inputs labels hope inner products large inputs di erent labels hope inner products small negative 
better suited target kernel larger value alignment 
note ben david simon studied related concept assessing realisability function classes kernels ben david 
value alignment real problems impossible estimate 
obtain empirical estimate sample de nition inner product gram matrices hk corresponding frobenius inner product 
de nition empirical alignment empirical alignment kernel kernel quantity hk hk hk viewed inner product bi dimensional vectors gram matrices 
consider yy vector labels sample hk yy hk yy hk yy hyy yy ki section see de nition provides method selecting kernel parameters combining kernels 

de nition reasonable gives maximal similarity kernel symmetric notice de ne general meta kernel assess similarity kernel task similarity kernels 
rst application quantity provide novel characterization kernel functions 
consider space possible symmetric matrices inner product space frobenius inner product 
positive de nite matrices form cone space 
inner product positive condition sucient characterize set 
crucial property alignment practical applications theorem showing reliably estimated empirical estimate 
result expressed terms concentration inequality 
concentration means probability random empirical estimate deviating mean bounded exponentially decaying function deviation 
theorem result due mcdiarmid 
note estimation operator selection sample 
cristianini kandola elisseeff shawe taylor theorem mcdiarmid independent random variables values set assume satis es sup jf ef exp theorem sample estimate alignment concentrated expected value 
kernel feature vectors norm fs ln non trivial function value 
proof note 
de ne 
mcdiarmid theorem show concentrated 
consider training set bound di erence 
obtain application mcdiarmid theorem fs exp setting ln right hand sides equal 
probability 
inequalities hold ja kernel target alignment 
de nition true alignment possible prove asymptotically tends nity empirical alignment de ned converges true alignment 
wants obtain unbiased convergence necessary slightly modify de nition removing diagonal nite samples biases expectation receiving large weight 
modi cation statement theorem true alignment 
prefer pursue avenue simplicity article just note change signi cant 
section see notion alignment give simple algorithms selecting kernel parameters combining kernels 
interesting consider special case assume orthogonal respect inner product hy dp true label vector 
write kernel evaluate alignment follows hk hk ki hy hy hy observe hy hy ij alignment simply course terms gram matrix written vector labels 
case approximated decomposition eigenvectors kernel matrix words peaked spectrum aligned speci kernel 
lucky eigenvector largest eigenvalue corresponds target labeling give labeling fraction weight allocate di erent possible labelings 
consequence larger emphasis kernel target higher alignment 
notice nally measure degree universality kernel obtained alignment kernel matrix identity range completely speci kernel completely universal diagonal 
notice nally cristianini kandola elisseeff shawe taylor kernel type corresponds estimating volume version space measure hypotheses consistent training points plus volume hypotheses completely inconsistent training points 

generalization observation relates generalization simple classi cation function value function consider computable sample existence implies generalization principle obtained feature space 
consider function dp true labels 
note 
margin point em dp dp dp dp dp generalization accuracy error largest value expected margin correctly classi ed points margin misclassi ed points margin 

theorem follows readily 
theorem 
probability generalization accuracy function dp bounded ln kernel target alignment proof observe theorem probability greater ja reasoning probability greater generalization accuracy lower bounded empirical estimate function parzen window estimate showing function drawn class bounded complexity example bounded fat shattering dimension possible show function converges uniformly function obtaining bound generalization terms empirically estimated alignment 
concentration considered devroye 
alignment regarded measure proportional class similarity inversely proportional class similarity 
possible relate average margin cristianini proving optimal alignment corresponds maximal margin separation 
furthermore concentration eigenvalues proven showing measure matching distribution kernel reliable 

algorithms results obtained previous sections practical implications 
subsections examine algorithms concept alignment kernel target 
kernel selection combination unsupervised transductive learning kernel adaptation 
adapting alignment investigate kernel matrix adapted improve alignment target classi cation 
consider general case classi cations known 
assume subset indices target values known 
method propose solving optimization problem 
optimizing spectrum 
method transduction combining techniques discussed 
consider base kernels eigenvectors kernel matrix labeled unlabeled data 
de ne parametrized class kernels determined equation cristianini kandola elisseeff shawe taylor consider optimization problem nding optimal parameters maximize alignment combined kernel available labels 
alignment written hk ij hv orthonormality uu hv ui write hv yi hyy yy optimization problem maximize hv yi hv yi hv yi gives alignment hv yi hyy yy transduction algorithm designed take advantage optimizing alignment labeled part dataset doing adapt gram matrix unlabeled part 
techniques spectral labeling method described provide partitioning clustering data 
demonstrate performance algorithms binary classi cation datasets 
ionosphere dataset contains inputs single binary output datapoints 
wisconsin breast cancer dataset obtained university wisconsin hospitals 
contains integer valued inputs single binary output benign malignant datapoints bennett mangasarian 
learning algorithms implemented 
parzen window estimator support vector classi er svm svm 
fold procedure nd optimal values capacity control parameters range values svm svm trained times range values 
value gave lowest mean error associated standard deviation 
available uci data repository 
values considered respectively kernel target alignment breast cancer dataset linear kernel test error parameter value table optimal values capacity control parameters svm svm 
mean standard deviation estimated generalisation error runs quoted di erent data partitions training test sets 
test dataset chosen optimal value 
having selected optimal values svm svm re trained times random data splits 
applied transduction algorithm designed take advantage results optimizing alignment labeled part dataset spectral method described adapting gram matrix unlabeled part 
results averaged random data partitions standard deviation brackets 
tables show mean generalisation error associated standard deviation brackets optimal value datasets data partitions considered 
alignment algorithm applied breast cancer ionosphere datasets gaussian linear kernel 
tables show alignments gram matrices label matrix di erent sizes training set 
index indicates percentage training points 
matrices adaptation matrices optimization alignment 
left columns table shows alignment values breast cancer data gaussian kernel performance svm classi er svm trained gram matrix third column 
right columns show performance parzen window classi er test set breast linear kernel left column ionosphere right column 
theory prediction svm performance apparent table training sets reduction generalisation error 
results clearly show optimizing alignment training set increase value case sum standard deviations 
fur cristianini kandola elisseeff shawe taylor breast cancer dataset gaussian kernel test error parameter value table optimal values capacity control parameters svm svm 
mean standard deviation estimated generalisation error runs quoted di erent data partitions training test sets 
ionosphere dataset linear kernel test error parameter value table optimal values capacity control parameters svm svm 
mean standard deviation estimated generalisation error runs quoted di erent data partitions training test sets 
kernel target alignment breast cancer dataset ionosphere dataset train align test align train align test align table mean associated standard deviation alignment values linear kernel breast cancer ionosphere datasets runs 
alignment breast cancer dataset svm error svm error train align test align test error test error table breast cancer dataset alignment values svm error svm error gaussian kernel sigma runs alignment ionosphere dataset svm error svm error train align test align test align test align table ionosphere dataset alignment values svm error svm linear kernel runs 
cristianini kandola elisseeff shawe taylor breast cancer dataset ionosphere dataset test error test error table parzen window estimate breast cancer gaussian kernel sigma ionosphere datasets linear kernel 
quoted values mean standard deviation estimated generalisation error parzen window estimate runs 
thermore predicted concentration improvement maintained alignment measured test set linear gaussian kernels case train linear kernel 
results ionosphere conclusive 
predicted theory larger alignment better performance obtained parzen window estimator 
results applying svm breast cancer data gaussian kernel show slight improvement test error training sets 
clustering maximal alignment subsection introduce related methods clustering extended case transduction 
introduce new quantity depends kernel unlabeled dataset 
depends relation kernel input distribution 
call absolute alignment kernel set data points maximum alignment possible labelings call corresponding labeling optimally aligned optimal 
max max hk yy hk ki hyy yy course solution vector obtain combinatorial optimization problem 
relax constraint simply asking exact problem course special case setting 
solving relaxed problem obtain approximate discrete solution choosing suitable threshold 
order solve problem characterization spectrum symmetric matrix 
theorem courant fischer minimax theorem symmetric max dim min mv min dim max mv kernel target alignment simple change variables possible see approximated problem solved rst eigenvector absolute alignment upper bounded rst eigenvalue 
maximum eigenvalue max matters 
max max kv transform vector vector choosing threshold gives maximum alignment sign max 
de nition value alignment obtained solution lower bound optimal alignment hk ki max max kkk estimate quality dichotomy comparing value upper bound 
absolute alignment tells specialized kernel dataset higher quantity committed speci dichotomy kernel 
leads simple algorithm transduction unsupervised learning set points kernel ll kernel matrix 
labels available insert corresponding entry max min 
choose kernel parameters optimize max kkk 
nd rst eigenvector choose threshold maximize alignment output corresponding cost alignment changing label kkk point isolated equally close di erent classes changing label small ect 
vice versa labels strongly clustered points clearly contribute cost changing label alter alignment signi cantly 
entries rst eigenvector close proceed iterations choosing entries labels certain treating labels modifying kernel matrix de ned 
remaining labels optimized 
rst eigenvector calculated ways example lanczos procedure ective large datasets 
search engines google estimating rst eigenvector matrix dimensionality large datasets may possible develop approximate techniques 
shows actual alignment test error upper bound absolute alignment function threshold breast cancer data algorithm 
notice actual alignment upper bound alignment get closest con dence cristianini kandola elisseeff shawe taylor plot cancer data max hk ki sign max error curve middle 
partitioned data fact test accuracy maximized 
notice choice threshold corresponds maintaining correct proportion positives negatives 
suggests possible threshold selection strategy availability labeled points give estimate proportion positive points dataset 
plotted upper lower bounds alignment error unsupervised classi cation cancer data function threshold rst eigenvector 
see alignment optimal error optimal 
rst eigenvector calculated ways example lanczos procedure ective large datasets 
search engines google estimating rst eigenvector matrix dimensionality large datasets approximation techniques 
applied procedure outlined datasets uci repository 
preprocessed data normalizing input vectors kernel de ned feature space centering shifting origin feature space centre gravity 
achieved transformation kernel matrix jg gj kjj ones vector ones matrix vector row sums rst experiment applied unsupervised technique breast cancer data linear kernel 
shows alignment di erent eigenvectors labels 
highest alignment shown eigenvector corresponding largest eigen kernel target alignment eigenvalue number alignment threshold number alignment success rate plot alignment di erent eigenvectors labels ordered increasing eigenvalue 
plot breast cancer data linear kernel max kkk straight line sign max bottom curve accuracy middle curve threshold number value 
value threshold shows upper bound max kkk straight line alignment sign max bottom curve accuracy middle curve 
notice actual alignment upper bound alignment get closest con dence partitioned data fact accuracy maximized 
notice choice threshold corresponds maintaining correct proportion positives negatives 
suggests possible threshold selection strategy availability labeled points give estimate proportion positive points dataset 
way label information choose threshold 
experiments describe transduction method 
measure naturally data separates procedure able optimize split accuracy approximately choosing threshold maximizes alignment threshold number making labels 
results gaussian kernel 
case accuracy obtained optimizing alignment threshold number resulting dichotomy impressive 
shows results ionosphere dataset 
accuracy split optimizes alignment threshold number approximately 
approach adapt kernel data 
example choose kernel parameters optimize max kkk nd rst eigenvector choose threshold maximize alignment output corresponding cost alignment changing label kkk point isolated equally close di erent classes changing label small ect 
hand labels strongly clustered points clearly contribute cost changing label alter alignment signi cantly 
cristianini kandola elisseeff shawe taylor threshold number alignment success rate threshold number alignment success rate plot breast cancer data gaussian kernel ionosphere data linear kernel max kkk straight line sign max bottom curve accuracy middle curve threshold number method described viewed projecting data dimensional space nding threshold 
projection implicitly sorts data points class nearby ordering 
discuss problem class case 
consider embedding set real line satisfy clustering criterion 
resulting kernel matrix appear block diagonal matrix 
problem addressed case information retrieval berry applied assembling sequences dna 
cases eigenvectors laplacian approach called fiedler ordering 
fiedler ordering variation simple kernel matrix 
coordinate point real line 
consider cost function ij 
maximized points high similarity sign high absolute value points di erent sign low similarity 
choice coordinates optimizes cost rst eigenvector sorting data value entry eigenvector hope nd permutation renders kernel matrix block diagonal 
shows results heuristic applied breast cancer dataset 
grey level indicates size kernel entry 
gure left unsorted data right shows plot sorting 
sorted gure clearly shows ectiveness method 
clustering minimizing cut cost measure separation classes average separation points di erent classes normalized matrix norm 
de nition cut cost 
cut cost clustering de ned ij quantity motivated graph theoretic concept 
consider kernel matrix adjacency matrix fully connected weighted graph nodes data points kernel target alignment gram matrix cancer data permutation data sorting order rst eigenvector cost partitioning graph total weight edges needs cut remove exactly numerator cut cost 
notice relation alignment ij hk ones vector 
notice normalized kernels xed choosing minimize ij equivalent maximizing ij turn equivalent minimizing sum squared distances couples points class furthermore minimising quantity equivalent minimising sum average square distances points class means nc nc ij denote nc 
approach directly aims nding clusters minimal scatter mean appealing properties alignment quantity sharply concentrated mean proven earlier cristianini 
shows expected alignment reliably estimated empirical estimate 
cut cost expressed di erence alignments similarly concentrated expected value 
quantity approximately minimized eigenvectors matrix called laplacian derived kernel matrix de ned ij diag dm 
laplacian matrix positive semide nite smallest eigenvalue corresponding eigenvector equal entries 
eigenvector corresponding smallest eigenvalue approximately gives best balanced split enforces condition weight positives cristianini kandola elisseeff shawe taylor weight negatives orthogonal constant smallest eigenvector 
de nitions imply equations ij ij ij ij ky ij ij ij ij ij ij ky ly nd minimize cut cost problem np hard 
impose slightly looser constraint 
problem min subject identical second eigenvalue optimization max ly summarizing min eigenvector corresponding second eigenvalue laplacian obtain approximate split second eigenvalue laplacian gives lower bound cut cost 
threshold entries eigenvector order obtain vector entries 
choice threshold discussed purposes set 
section see method adapting rst eigenvector making similar chosen 
method incorporate rst eigenvector information relative size positive negative class second eigenvector provide aligned solution chosen ration positives negatives 

easy see number zero eigenvalues rank estimation number clusters data 

consider transition matrix random walk graph probabilities proportional weights 
obtained removing diagonal gram matrix normalizing row 
quantity give information anomalous points 
points extensions alignment kernel target alignment threshold number cut cost error rate threshold number plot breast cancer data linear kernel gaussian kernel kkk dashed curves sign max error solid curve threshold number currently considered 
applied procedure breast cancer data linear gaussian kernels 
results shown 
cut cost select best threshold linear kernel sets accuracy signi cantly worse results obtained optimizing alignment 
gaussian kernel hand method selects threshold accuracy slight improvement results obtained kernel optimizing alignment 
far algorithms unsupervised data 
consider situation partially labelled dataset 
leads simple algorithm transduction semi supervised learning 
idea labelled data improve performance comes observing selection cut cost clearly suboptimal 
incorporating label information hoped obtain improved threshold selection 
vector containing known labels 
set kp zz positive constant parameter 
original matrix generate eigenvector matrix kp measuring cut cost classi cations generated di erent thresholds 
performed random selections data obtained mean success rate standard deviation breast cancer data gaussian kernel marked improvement achieved label information 
kernel selection combination concentration alignment directly tuning kernel family particular task selecting kernel set need training 
probability cristianini kandola elisseeff shawe taylor level alignment observed training set expectation kernels bounded equation ln jn size set kernel chosen 
main consequences de nition kernel alignment providing practical criterion combining kernels 
justify intuitively appealing idea kernels certain alignment target aligned give rise aligned kernel combination 
particular hk yi kk hk yi kk hk yi kk kk kk kk kk shows kernels equal alignment target completely aligned kk kk kk alignment remains combined kernel remains 
hand kernels completely aligned kk kk kk kk alignment combined kernel correspondingly increased 
intuitively illustrated assume fact fact fairly aligned target means strong component corresponding fact aligned means components di erent say averaging emphasize part aligned target de emphasize components 
resulting kernel general focused speci learning problem particular orthogonal kernel target alignment gives set general problem optimizing coecients combination choose di erent kernels subject ongoing research 

problem assessing quality kernel central theory kernel machines deeply related problem model feature selection classical theme machine learning 
able quantify property rst step ective algorithms kernel selection combination adaptation 
hand able adapt labels set kernel provides ective clustering transduction procedures 
proposed measure desirable theoretical practical properties alignment 
quality kernel necessarily absolute property relative learning task 
absolute kernel proved useless 
measure kernel quality measure degree agreement kernel labeling kernels 
proved property related generalization natural learning algorithms important statistical property concentration 
deduced number algorithms demonstrating simplicity quantity versatile conceptual tool 
particular gave criterion decide kernels combined better practice hypertext combining link words information joachims bioinformatics combining gene expression information pavlidis 
cristianini kandola elisseeff shawe taylor gave criteria adapting kernels target tting parameterized kernel family novel non parametric fashion just entries gram matrix adapted 
gave series methods assigning labels set points optimize alignment kernel 
leads novel procedures clustering transduction 
ideas close relatives previously simultaneously appearing publications 
theoretical ideas free kernel theorem clearly related wolpert wolpert free lunch machine learning watanabe ugly duckling theorem watanabe 
foster ben david forster ben david explores problem embedding certain matrix euclidean space order derive inherent limitations kernel representations 
spectral methods clustering rst introduced graph theory pothen 
spectral clustering algorithms discussed analyzed drineas kernel pca sch olkopf 
latent semantic kernels cristianini 
strongly related labeling methods discussed 
algorithms clustering transduction inspired graph theory put forward blum chawla 
theoretically explore connections high alignment generalization larger classes learning machines relations luckiness framework shawe taylor 
notion stability 
general quality measures designed basically kernel gram matrices devoted exploring possible options 
forms kernel combination adaptation studied tool alignment maximization 
currently undertaken adapt kernel matrix optimize complex measures example margin 
ben david eiron simon 
euclidean half spaces 
nips workshop new perspectives kernel methods 
bennett mangasarian 
robust linear programming discrimination linearly inseparable sets 
optimization methods software 
url ftp ftp cs wisc edu tech reports reports tr ps 
berry hendrickson raghavan 
sparse matrix reordering schemes browsing hypertext 
renegar shub smale editors mathematics numerical analysis pages 
american mathematical society 
avrim blum chawla 
learning labeled unlabeled data graph 
international conference machine learning icml 
kernel target alignment cristianini kandola shawe taylor 
kernel target alignment 
advances neural information processing systems volume 
cristianini shawe taylor lodhi 
latent semantic kernels 
international conference machine learning icml 
devroye lugosi 
statistical theory pattern recognition 
springer verlag new york 
drineas kannan vempala frieze vinay 
clustering large graphs matrices 
proc 
th acm siam symposium discrete algorithms 
forster schmitt simon 
estimating optimal margins embeddings euclidean half spaces 
submitted computational learning theory colt 
joachims cristianini shawe taylor 
composite kernels hypertext categorization 
submitted international conference machine learning icml 
mcdiarmid 
method bounded di erences 
surveys combinatorics pages 
cambridge university press 
pavlidis weston cai grundy 
gene functional classi cation heterogeneous data 
proceedings fifth international conference computational molecular biology pages 
pothen simon liou 
partitioning sparse matrices eigenvectors graphs 
siam matrix anal 
saitoh 
theory reproducing kernels applications 
longman scienti technical harlow england 
sch olkopf smola uller 
kernel principal component analysis 
advances kernel methods support vector learning 
mit press pages 
scha er 
conservation law generalization performance 
international conference machine learning icml 
shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee information technology 
watanabe 
pattern recognition human mechanical 
wiley cambridge uk 
wolpert 
connection sample testing generalization error 
complex systems 
wolpert 
training set error priori distinctions learning algorithms 
report fe institute santa fe nm 

