finding generalized projected clusters high dimensional spaces aggarwal philip yu ibm watson research center yorktown heights ny watson ibm com high dimensional data challenge clustering algorithms inherent sparsity points 
research results indicate high dimensional data concept proximity clustering may meaningful 
discuss general techniques projected clustering able construct clusters arbitrarily aligned subspaces lower dimensionality 
subspaces specific clusters 
definition substantially general realistic currently available techniques limit method projections original set attributes 
generalized projected clustering technique may viewed way trying redefine clustering high dimensional applications searching hidden subspaces clusters created inter attribute correlations 
provide new concept extended cluster feature vectors order algorithm scalable large databases 
running time space requirements algorithm adjustable tradeoff better accuracy 
problem clustering data points defined follows set points multidimensional space find partition points clusters points cluster similar 
various distance functions may order quantitative determination similarity 
addition objective function may defined respect distance function order measure quality partition 
method studied considerable detail statistics database communities applicability practical problems customer segmentation pattern recognition trend analysis classification 
overview clustering methods may 
common class methods clustering partitioning methods set seeds representative objects order partition points implicitly 
variations technique exist means medoid algorithms 
medoid techniques points database seeds algorithm tries search optimal set seeds results best clustering 
effective practical technique class called clarans uses restricted search space order improve efficiency 
known class techniques hierarchical clustering methods database decomposed levels partitioning represented dendogram 
methods qualitatively effective practically infeasible large databases performance quadratic number database points 
density clustering methods neighborhood point order find dense regions clusters exist 
related techniques large databases include condensation grid methods conjunction spatial hierarchical structures 
birch method uses hierarchical data structure called cf tree order incrementally build clusters 
efficient approaches low dimensional data requires scan database 
hierarchical method called cure proposed tends show excellent quality uses robust methods order measure distances clusters 
adjusts different shapes clusters 
interesting grid partitioning technique called proposed designed perform high dimensional data 
spite improved techniques high dimensional data continues pose challenge clustering algorithms fundamental level 
clustering algorithms efficiently higher dimensional cross section axis xx xxx xxx xx cross section axis xx illustrations projected clustering spaces inherent sparsity data 
problem traditionally referred dimensionality curse motivation significant amount high dimensional clustering similarity search 
theoretical results shown high dimensional space distance pair points wide variety data distributions distance functions 
circumstances meaningfulness proximity clustering high dimensional data may called question 
solution problem feature selection order reduce dimensionality space correlations dimensions specific data locality words points correlated respect set features correlated respect different features 
may feasible prune dimensions time incurring substantial loss information 
demonstrate help example 
illustrated figures 
top figures correspond set points bottom figures correspond set 
represents cross section set points represents cross section 
patterns points labeled pattern corresponds set points close plane 
second pattern corresponds set points close plane 
note traditional feature selection case dimension relevant clusters 
time clustering full dimensional space discover patterns spread dimensions 
figures illustrate simple case clusters aligned particular axis system 
methods finding projected clusters discussed 
related methods finding locally dense subspaces discussed 
really clustering methods return dense rectangular projections huge point overlaps 
reality methods may general order find true clusters 
clusters exist arbitrarily oriented subspaces lower dimensionality 
examples figures illustrate cases projected clusters exist arbitrarily oriented subspaces 
patterns labeled case planes projection take place normals arrows illustrated figures 
examination real data shows points may tend get aligned arbitrarily skewed elongated shapes lower dimensional space correlations data 
clearly choice axis parallel projections find clusters effectively 
context shall define call generalized projected cluster 
generalized projected cluster set vectors set data points points closely clustered subspace defined vectors subspace defined vectors may lower dimensionality full dimensional space 
figures contain clusters arbitrarily oriented subspaces 
focus algorithm find clusters lower dimensional projected subspaces data high dimensionality 
assume number clusters input parameter 
addition number clusters algorithm take input dimensionality subspace cluster reported 
output algorithm twofold ffl way partition fc og data points partition element form cluster 
points partition element outliers definition cluster 
ffl possibly different orthogonal set vectors cluster points cluster subspace defined vectors 
vectors outlier set assumed empty set 
cluster cardinality corresponding set equal user defined parameter order describe algorithm shall introduce notations definitions 
denote total number data points 
assume dimensionality space fx set points cluster 
centroid cluster algebraic average points cluster 
centroid cluster denote distance points dist 
euclidean distance metric 
point dimensional space fe set orthonormal vectors dimensional space 
orthonormal vectors define subspace 
projection point subspace dimensional point delta delta 
delta denotes dot product points original dimensional space 
projected distance points subspace denoted dist equal euclidean distance projections dimensional space represented projected energy cluster fx subspace denoted mean square distance points centroid cluster points projected subspace fp dist note projected energy cluster subspace full dimensional space 
certain choices subspaces projected energy significantly smaller full dimensional space 
example case figures dimensional subspaces projected energy small clusters planes normal arrows illustrated 
aim algorithm discover clusters small projected energy subspaces user specified dimensionality subspace cluster different 
providing input parameter gives user considerable flexibility discovering clusters different dimensionalities 
discuss methods providing user guidance finding value meaningful clusters may 
examine general concept arbitrarily projected subspaces finding clusters 
dense full dimensional clusters high dimensional data sets method searches hidden subspaces points cluster correlations chosen term energy metric invariant rotation axis system expressed sum energies individual axis directions 
dimensions 
technique considered meaningful re definition clustering problem high dimensional data mining applications 
propose efficient algorithm projected clustering problem scalable vary large databases 
simplified version problem addressed 
projected cluster defined terms sets points subsets dimensions original data set 
framework may necessarily able tackle sparsity problem high dimensional data fact real data contains inter attribute correlations 
naturally leads projections parallel original axis system 
fact possible data continue sparse possible projected subsets attributes original data dense projections arbitrary directions 
empirical results show cases axis parallel projections counterproductive lead loss information 
provides general framework algorithms clusters represented arbitrarily projected space 
organized follows 
section describes clustering algorithm detail 
algorithm known orclus arbitrarily oriented projected cluster generation 
discuss improvements basic algorithm introduce concept extended cluster feature vectors ecf algorithm scale large databases 
show progressive random sampling approach order improve scalability database size 
section discuss empirical results section discusses summary 
projected clusters skews real data contains different kinds skews data subsets dimensions related 
furthermore subsets correlated dimensions may different different localities data 
correlated sets dimensions lead points getting aligned arbitrary shapes lower dimensional space 
distributions points far uniform distribution referred skews 
projected clusters lower dimensional subspaces closely connected problem finding skews data 
orthogonal set vectors defines subspace projected cluster provides idea nature skews correlations data 
example figures arrows represent directions sparsity greatest correlated sets dimensions 
coincidentally direction projection points clusters similar normal planes arrows 
general subspace maximum sparsity point distribution occurs complementary subspace points similar 
distributions points high dimensional space data continue sparse full dimensionality ruling full dimensional clustering algorithms discussed realistic way detecting regions similarity lower dimensional projections cluster 
related method singular value decomposition svd known technique order represent data lower dimensional subspace pruning away dimensions result loss information 
projected clustering method projects data lower dimensional subspace interesting examine relationship techniques 
give brief overview methodology singular value decomposition 
idea transform data new coordinate system second order correlations data minimized 
transformation done step process 
step covariance matrix constructed data set 
specifically entry matrix equal covariance dimensions diagonal entries correspond variances individual dimension attributes 
second step eigenvectors positive semidefinite matrix 
eigenvectors define orthonormal system second order correlations data removed 
corresponding eigenvalues denote spread variance newly defined dimension orthonormal system 
eigenvectors corresponding eigenvalues largest chosen subspace data represented 
results small loss information data show variance remaining dimensions 
problem dimensionality reduction concerned removing dimensions entire data set amount information lost 
hand focus find best projection cluster way greatest amount similarity points cluster detected 
dimensionality reduction problem chooses eigenvectors maximum spread order retain information distinguishes points 
hand novel approach discussed pick directions spread specific cluster retain information similarity points cluster 
obviously directions spread different different clusters 
subspace complementary projection subspace cluster useless information may order retain maximum information distinguishes points cluster technique applications indexing 
projected clustering problem complex dimensionality reduction problem faced simultaneous problem partitioning data set finding directions similarity partition 
covariance matrix diagonalization section discuss properties covariance matrix diagonalization useful clustering algorithm 
mention properties fact may verified 
covariance matrix set points matrix positive expressed format deltap delta diagonal matrix non negative entries 
columns orthonormal eigenvectors diagonal entries delta eigenvalues orthonormal eigenvectors define new axis system matrix delta covariance matrix original set points represented system 
entries zero means secondorder correlations removed 
means eigenvalues correspond variances new set axes 
trace invariant axis transformation defined eigensystem equal trace original covariance matrix equal energy cluster full dimensional space 
shown picking smallest eigenvalues results dimensional subspace eigenvectors sum variances directions possible transformations 
projected energy cluster subspace diagonalization covariance matrix provides information projection subspace cluster minimizes corresponding energy 
generalized projected clustering decided variant hierarchical merging methods algorithm 
unfortunately class hierarchical methods prohibitively expensive large databases algorithms scale quadratically number points 
solution run method random sample points lead loss accuracy 
consequently decided compromise solution applying technique small number initial points techniques partitional clustering order associate current cluster points 
current clusters order robust merging decisions 
effect information entire database hierarchical merging process number merges greatly reduced 
refer initial set points seeds 
stage algorithm associated seed current cluster set points database closest seed subspace associated cluster assume stage algorithm number current clusters denoted 
current subspace subspace points cluster 
dimensionality equal user specified dimensionality initially equal full dimensionality value reduced gradually user specified dimensionality idea gradual reduction iterations clusters may necessarily correspond natural lower dimensional subspace clusters data larger subspace retained order avoid loss information 
noisy subspaces excluded 
iterations clusters refined subspaces lower rank may extracted 
algorithm consists number iterations apply sequence merging operations order reduce number current clusters factor ff 
reduce dimensionality current cluster fi iteration 
significance dividing merging process different iterations iteration corresponds certain dimensionality subspace clusters discovered 
iterations correspond higher dimensionality successive iteration continues peel noisy subspaces different clusters 
values ff fi need related way reduction clusters occurs number iterations reduction jdj dimensions 
relationship log ff log fi satisfied 
description algorithm orclus chosen ff calculated value fi relationship 
description algorithm illustrated consists number iterations steps applied algorithm orclus number clusters number dimensions current cluster set vectors defining subspace cluster kc current number seeds current dimensionality associated seed fs current set seeds number seeds pick points database denote fs kc set initially original axis system ff fi gammalog ff log kc find partitioning induced seeds assign determine current subspace associated cluster knew ectors new reduce number seeds dimensionality associated seed knew kc delta ffg new delta fig knew knew knew merge knew new kc knew new assign return clustering algorithm algorithm assign oe data point determine dist distance point subspace determine seed value dist add return creating cluster partitions algorithm cluster points dimensionality projection determine covariance matrix determine eigenvectors matrix set eigenvectors corresponding smallest eigenvalues return finding best subspace cluster algorithm merge knew new pair satisfying ij ectors new defined eigenvectors new smallest eigenvalues ij centroid ij ij projected energy subspace ij kc knew find smallest value pairs satisfying merge corresponding clusters discard seed seeds clusters indexed larger subtracting values ij ij ij correspondingly cluster new pairwise recomputation different needs done kc gamma recompute ectors new centroid proj 
en 
kc kc gamma return knew knew knew merging algorithm assign database partitioned current clusters assigning point closest seed 
process partitioning distance database point seed measured subspace words database point value dist computed point assigned current cluster value 
procedure seed replaced centroid cluster just created 
procedure serves refine set clusters iteration set seeds associated clusters 
method illustrated 
procedure find subspace dimensionality current cluster done computing covariance matrix cluster picking orthonormal eigenvectors spread eigenvalues 
finds energy subspace rank cluster value reduces iteration iteration 
process illustrated 
merge iteration merge phase reduces number clusters knew gamma ff delta order closest pairs current clusters need merged successively 
easy full dimensional algorithms goodness merge easily quantified full dimensionality 
case somewhat complex 
current cluster exists possibly different subspace decide suitability merging closest pairs 
aim algorithm discover clusters small projected energy design measure testing suitability merging clusters examining projected energy union clusters corresponding spread subspace 
quantitative measure suitability merging pair seeds calculated step process 
step singular value decomposition points find eigenvectors corresponding smallest new eigenvalues 
new projected dimensionality iteration 
eigenvectors define spread subspace points union segmentations 
denote subspace ij second step find centroid ij energy ij ij cluster subspace ij indicator points clusters combine single cluster 
note points clusters combine method spread directions individual clusters similar 
case ij similarly oriented subspaces small projected energies 
idea measure current clusters fit single pattern behavior 
example current clusters consist points data pattern say result similar planes dimensional projection normal plane direction arrows 
consequently points projected similar plane 
value energy ij measured plane small 
step process repeated pair seeds pair seeds 
seeds merged centroid combined cluster added set seeds seeds removed 
current cluster associated new seed current subspace ij agglomeration procedure repeated multiple times number clusters reduced factor ff 
merging procedure described 
algorithm terminates merging process iterations reduced number clusters point dimensionality subspace associated cluster equal algorithm performs final pass database uses assign procedure order partition database 
desired procedure may order determine optimum subspace associated cluster termination 
aspects merging technique needs explicitly set current clusters database size may large extremely cumbersome maintain sets explicitly 
furthermore covariance matrix calculation intensive set covariance matrix calculations require pass database 
covariance matrix may calculated times merge operation see analysis translates passes database 
value times number clusters determined 
level unacceptable 
introduce concept extended cluster feature vectors operations performed maintaining certain summary information clusters 
provides considerable ease calculation covariance matrices 
details technique provided section 
picking projected dimensionality important input parameter algorithm projected dimensionality give guidance user picking parameter design measure motivated reason defined concept generalized projected clustering place 
proved increasing dimensionality distance pair points certain conditions data distributions 
means cluster points compared universal set points high dimensional space 
undesirable situation indicates average spread energy cluster average spread points entire database full dimensional space 
reason instability randomized clustering algorithms high dimensional space different runs lead different clusters equally measures 
subspaces ratio may smaller 
general ratio significantly smaller desirable indicates tightly knit cluster corresponding projected subspace 
define quality measure called cluster sparsity coefficient delta lower value smaller fraction may picked optimum way lower dimensionality reduce energy particular distribution points aggregate set points may continue high energy 
termination algorithm may return cluster sparsity coefficient 
value clear smaller value projected dimensionality needs picked 
fact user may define minimum threshold intuitively interpretable quality measure pick largest value cluster sparsity coefficient returned termination threshold 
outlier handling order take account fact points outliers may need modifications algorithm 
ffi projected distance nearest seed seed subspace consider arbitrary point database assignment phase 
seed database point assigned assignment phase 
point outlier projected distance seed subspace larger ffi addition seeds initially chosen may outliers 
need removed execution algorithm 
simple modification turns quite effective discarding certain percentage seeds iteration corresponding clusters contain points 
outlier handling option implemented value seed reduction factor ff defined percentage reduction iteration due merges discards 
scalability large databases discussed times calculating covariance matrices potentially large especially terms times covariance matrix calculated scratch ectors delta operation 
concept similar cluster feature vector cf vector order find covariance matrix efficiently 
shall refer extended cf vector ecf vector 
ecf vector specific cluster contains entries 
entries kinds entries corresponding pair dimensions 
pair dimensions sum products ith jth components point cluster 
words denotes ith component kth point cluster cluster pair dimensions maintain entry delta entries type 
cluster refer entry ecf ij pair dimensions refer entire set entries ecf 
entries corresponding dimension sum ith components point cluster 
maintain entry cluster shall refer entry ecf corresponding set entries referred ecf 
number points cluster denoted ecf note set entries included standard definition cf vector introduced 
entire cluster feature vector denoted ecf ecf ecf ecf 
important features ecf vector follows observation covariance matrix derived directly ecf vector 
specifically covariance dimensions set points equal ecf ij ecf gamma ecf delta ecf ecf proof average product ith jth attribute ecf ij ecf average ith attribute ecf ecf covariance ith jth attributes subtraction product averages average product result follows 
established zhang cf vector may order calculate centroid radius cluster 
ecf vector superset cf vector measures may calculated ecf vector 
extended cf vector provides ability calculate covariance matrix 
usefulness result covariance matrix calculated efficiently ecf vector 
summary characteristics cluster useful fact satisfies additive property 
observation ecf vector satisfies additive property 
ecf vector union sets points equal sum corresponding 
proof trivially follows fact ecf vector set points expressed sum ecf vectors individual points 
additive property ensures constructing ecf vector union clusters necessary recalculate scratch sufficient add ecf vectors clusters 
ecf vectors order modify orclus way 
entire operation algorithm current clusters associated seed maintained explicitly 
extended cf vectors seeds maintained 
ecf vector cluster sufficient calculate radius centroid covariance matrix cluster 
iteration seeds defined centroids current clusters iteration 
addition additive property ecf vectors ensures merge operation ecf vector merged cluster may calculated easily 
ecf vectors need recalculated iteration assign delta phase simple additive process affect algorithm 
running time requirements running time algorithm depends initial number seeds chosen algorithm 
skeletal structure algorithm contains basic process merging assignment points seeds 
addition outlier handling option reduces running time algorithm merges replaced simple discards 
analysis overestimate running time algorithm 
running time various procedures algorithm merge time merging asymptotically controlled time required iteration reducing value number current clusters ff delta start eigenvectors pairs current clusters calculated 
requires running time pair 
furthermore subsequent gamma ff delta gamma merges eigenvectors pairs clusters need re calculated 
eigenvector calculation follows total time eigenvector calculations merge operation algorithm delta 
furthermore merge operation time required order pick cluster energy 
total running time merges 
assign running time assignment points dimensional space clusters iteration delta delta 
second iteration time delta ff delta delta 
running time assignment process delta delta gamma ff 
subspace determination time eigenvector calculations subspace determinations merge phase included analysis 
remains calculate time subspace determinations iterative phase ectors procedure assign algorithm 
iteration algorithm subspace determinations second iteration delta ff subspace determinations 
entire algorithm gamma ff subspace determinations 
running time strictly dominated subspace determinations merge phase ignored asymptotically 
summing various components running time total time required delta delta delta 
note running time dependent choice initial parameter choice larger value increase running time improve quality clustering 
space requirements ecf vector cuts space requirements algorithm considerably current clusters associated seed need maintained 
iteration algorithm extended cf vectors need maintained 
space requirement delta 
space requirement algorithm delta 
independent database size may easily fit main memory reasonable values progressive sampling techniques possible speed assignment phase algorithm substantially progressive type random sampling 
note component running time dependent caused assign delta procedure algorithm 
component running time bottleneck database large 
observation cpu time performing assign delta procedure reduces factor ff iteration number seeds reduces factor 
purpose assign delta procedure keep correcting seeds iteration seed central point current cluster associated 
possible achieve results progressive kind random sampling 
randomly sampled subset points assigned seeds iteration 
random sample different multiple iterations change size 
size random sample increases factor ff iteration time assignment phase iteration 
possibility start random sample size delta increase factor ff iteration final iteration database points 
time assignment phase iteration delta delta iterations delta delta delta log ff 
kind progressive sampling provide considerable savings cpu time iterations substantially larger furthermore lose terms accuracy seeds refined iterations larger sample sizes 
empirical results simulations performed mhz ibm rs computer memory gb scsi drive running aix 
tested accuracy performance measures 
accuracy clustering respect matching points input output clusters 

scaling running times database size 

scaling performance initial number seeds 
order test accuracy results determined confusion matrix indicated output clusters matched input points 
entry confusion matrix indicates number points belonging output cluster generated part input cluster clustering algorithm performs row column entry significantly larger 
hand case clustering technique bad completely random points evenly distributed different clusters 
synthetic data generation stage synthetic data generation find arbitrarily oriented subspaces associated cluster 
order generate subspace associated cluster generated random matrices th entry matrix random real number range gamma 
addition matrix symmetric th entry equal th entry 
symmetric matrix eigenvalues real orthonormal eigenvector system exists 
orthonormal axis system generate input orientation corresponding cluster 
different matrix clusters order generate corresponding orientation 
number points cluster determined method determine number points cluster determined constant proportionality points cluster constant proportionality determined formula delta constants random number drawn uniform distribution range 
resulting number drawn uniform distribution range 
purpose experiments 
proportionality constants entire space number points cluster determined formula delta step generate anchor points approximation central point cluster 
anchor points chosen uniform distribution points space 
anchor points determined generate clusters points 
discussed axes orientations cluster determined eigenvectors randomly generated symmetric matrices 
remains distribute points cluster corresponding axis system 
axis system corresponds correlations points zero generate coordinates respect transformed subspace independently 
generated points transformed back original space 
cluster picked eigenvectors defined subspace hidden 
points distributed uniform random distribution gamma axes directions 
remains explain point distribution hidden axes directions accomplished 
step determine spread eigenvectors 
determined pair spread parameters fl 
spread eigenvector determined proportional fl drawn exponential distribution mean fl parameter determined level wished create various axes directions 
value fl increased amount values fl increased different values ith coordinate point respect transformed subspace drawn normal distribution mean anchor point variance equal fl experiments fl 
resulting clusters sparse full dimensional space 
cluster sparsity coefficient measured full dimensional space input clusters choice parameters 
furthermore checked sparsity coefficient individual input cluster dimensional axis parallel projection case cluster sparsity coefficient high averaged 
high sparsity coefficients indicate generated data full dimensional axis parallel projected clustering meaningless 
failure axis parallel projections tested axis parallel version algorithm cluster sparsity coefficients higher dimensional projections large minimum sparsity coefficient dimensional subset large maximum sparsity coefficient dimensional subset 
proved additive property energy metric different dimensions 
formal proof omitted 
axis parallel projected clustering algorithm called proclus match input output clusters 
addition proclus classify unusually large number input points outliers 
version orclus slightly stable accurate proclus 
input clusters output clusters table axis parallel projections confusion matrix user specified input clusters output clusters table axis parallel projections confusion matrix user specified data set 
done ensuring ectors delta procedure returned spread axis parallel directions 
state salient observations briefly ffl axis parallel projected version poor matching input output clusters 
method significantly better random clustering technique runs 
illustrated runs data set points varying values projected dimensionality illustrated results tables 
confusion matrix table slightly better confusion matrix table considered clean partitionings input cluster split output clusters vice versa 
part interesting trend observed reducing user specified value projected dimensionality worsened quality clustering axis parallel version 
clusters exist arbitrary subspaces forcing particular kinds projections clusters leads loss information greater projection greater loss 
ffl axis parallel version unstable 
slight changes initialization conditions changing random seed significantly changed matrices obtained tables 
results quite expected observations sparsity coefficients indicate class axis parallel projection methods 
results orclus input clusters output clusters table orclus confusion matrix case userspecified input clust 
output clust 
table orclus confusion matrix case userspecified number records scaling database size scaling running time database size initial number seeds relative running time variation number seeds scaling running time initial number seeds performance scaling passes purpose tests delta mentioned 
generated input data sets computed confusion matrix cases 
confusion matrices representative general trend observed 
ran algorithm different values projected dimensionality tested corresponding cluster sparsity coefficient 
case output cluster sparsity coefficient dropped significantly case case drop relatively small case case 
fact percentage drop sparsity coefficient highest dimensionality subspace input clusters hidden 
criterion pick projected dimensionality cases illustrate 
data set case contained points results indicated table 
data set tables 
see table entries column clearly larger rest entries 
indicates input cluster gets directed output cluster exception points get distributed clusters 
table case illustrates example containing points 
trends similar table 
results generally indicative clean mapping input output clusters 
tested sensitivity clustering variations value input parameter case confusion matrix obtained range slightly worse confusion matrix optimized value 
indication stability technique 
computational scalability results algorithm 
results averaged runs case order smooth curve 
scaling running time number points database illustrated 
see algorithm performance practically linear database size 
note curve interesting case substantially larger initial number seeds 
illustrates cpu performance initial number seeds increased 
know quality clusters better starting larger number seeds cluster covered seed 
correspondingly running time algorithm increases contribution subspace determination operations merge phase 
requirements varied dramatically initial number seeds 
total number iterations log ff pass performed 
illustrates corresponding trend 
curve remains irrespective database size dimensionality final number projected dimensions 
results indicate larger number initial seeds picked interest greater accuracy method bound 
summary discussed concept finding arbitrarily oriented projected clusters high dimensional spaces definition clustering practical effective solution dimensionality curse traditional version problem 
idea eliminating sparse subspaces cluster projecting points subspaces greatest similarity occurs generalized notion clustering full dimensional case special 
sparsity high dimensional data prevents detection natural clusters full dimensional problems modified definition clustering best redefine understanding notion high dimensional clustering 
research show technique effective high dimensional data visualization 
aggarwal fast algorithms projected clustering 
acm sigmod conference 
agrawal automatic subspace clustering high dimensional data data mining applications 
acm sigmod conference 
beyer nearest neighbor meaningful icdt conference 
cheng fu zhang 
entropy subspace clustering mining numerical data 
acm sigkdd conference 
ester density algorithm discovering clusters large spatial databases noise 
kdd conference 
faloutsos 
lin 
fastmap fast algorithm indexing data mining visualization traditional multimedia datasets 
acm sigmod conference 
gionis indyk motwani 
similarity search high dimensions hashing 
vldb conference 
guha rastogi shim 
cure efficient clustering algorithm large databases 
acm sigmod conference 
hinneburg keim 
optimal grid clustering breaking curse dimensionality high dimensional clustering 
vldb conference 
indyk motwani 
approximate nearest neighbors removing curse dimensionality 
stoc 
jain dubes 
algorithms clustering data 
prentice hall new jersey 
jolliffe 
principal component analysis 
springer verlag new york 
kleinberg 
algorithms nearest neighbor search high dimensional space 
stoc 
kohavi sommerfield 
feature subset selection wrapper method overfitting dynamic search space topology 
kdd 
ng han 
efficient effective clustering methods spatial data mining 
vldb conference 
ravi agrawal singh 
dimensionality reduction similarity searching dynamic databases 
acm sigmod conference 
xu distribution clustering algorithm mining large spatial databases 
icde conference 
zhang ramakrishnan livny 
birch efficient data clustering method large databases 
acm sigmod conference 

