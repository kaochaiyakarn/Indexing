support vector machines phoneme classification jesper salomon master science school artificial intelligence division informatics university edinburgh thesis support vector machines svms applied problem phoneme classification 
sequence acoustic observations phoneme targets task classify observation targets 
task involves multiple classes main hurdles svms overcome extend inherently binary svms multi class case 
methods proposed generalisation abilities measured 
generalisation lost transition lead effective classifiers 
addition refinement svms derive estimated posterior probabilities classifications 
speech recognition systems statistical models necessary svms full speech recognition system 
best accuracy competitive best results literature 
simon king guidance support project 
ii declaration declare thesis composed contained explicitly stated text submitted degree professional qualification specified 
jesper salomon iii table contents summary literature 
recurrent neural networks 
support vector machines 
overview thesis 
support vector machines 
statistical learning theory 
binary classification problem 
empirical risk minimisation 
structural risk minimisation 
vc dimension 
linear classifiers 
separable case 
non separable case 
non linear classifiers 
kernel trick 
training 
classification 
kernels 
choosing kernel 
bias svms 
iv practical issues 
multi class svms 
vs classifier 
vs rest 
dagsvm 
binary tree 

experiments toolkits 
timit speech database 
binary svm experiments 
experimental setup 
choice kernel 
including different amounts training data 
selecting multi class methods 
vs classifier 

practical issues 
experiment effects kernels 
adding features 
adding context frames 
consistency experiment 
experiment including additional training data 
generalisation performance vs classifier 

dagsvm classifier 
results 
binary tree classifier 

implementation 

probability outputs estimating posterior probabilities 

kernel requirements symmetry condition 
mercer theorem 
phonemes timit speech database binary tree structure bibliography vi chapter task thesis create learning machines classify sequences acoustic observations 
consider situation shown 
ordinary spoken english sentence labelled sentence split tiny fragments speech known frames 
converted vectors fixed length fed learning machine 
goal learning machine classify vectors targets representing phonemes english language 
overview frame classification 
idea classifying frames widely isolated continuous chapter 
speech recognition known frame frame classication 
predictions learning machine passed statistical model find sequence phonemes construct meaningful sentence 
classification frames seen basic units speech recognition system thesis chosen learning machine task support vector machines svms 
svms fairly new technique pattern recognition created vapnik 
applied different problems successful areas face recognition text categorisation time series prediction hand written digit recognition 
areas svms shown perform established methods neural networks radial basis functions 
thesis considers svms pattern classification applied areas regression novelty detection 
classification svms binary classifiers 
build decision boundary mapping data original input space higher dimensional feature space see data separated linear hyperplane 
mapping non linearly separable training samples input space linearly separable representation feature space 
choosing hyperplane minimises training error svms choose hyperplane maximises margin separability sets data points 
selection viewed implementation structural risk min naturally recognition systems approach 
chapter 
principle seeks minimise upper bound generalisation error 
typically increase generalisation error expected constructing decision boundary higher dimensional space 
maximising margin expected degradation performance referred curse dimensionality 
selection hyperplane feature space requires svm evaluate scalar inner products feature space 
complicated computationally expensive feature space higher dimensions input space 
fortunately explicit calculation needed due functional representation termed kernel 
kernel calculates inner product feature space direct operation training samples input space 
effective kernel done significant increase computational cost 
svms speech recognition hoped performance gains shown svms tasks mapped domain 
limited done apply svms problem 
results far promising significant hurdles overcome svms viewed viable competitive alternative established methods 
problems include problem multi class classification 
svms inherently binary classifiers 
method effectively extending binary classification multi class case needed 
method able retain generalisation ability binary classifier 
estimation posterior probabilities 
speech recognition systems statistical models combine low level acoustic information higherlevel knowledge language 
svm classifier system needs able return class conditional probabilities 
choosing appropriate kernel 
kernel affects performance svm 
method choosing kernel needed 
working real time environment 
speech recognition systems created real time communication complete system able chapter 
process spoken data dictated 
svms function environment extremely effective 
chosen focus problems 
having fast effective method important feel important examine feasibility applying svms task concerned speed 
methods introduced similar fashion 
hidden markov models hmms method commercial speech recognition systems slow optimisation techniques 
techniques quickly removing predictions early recognition process hmms suitable real time recognition difference phoneme recognition frame frame classification 
small box encloses task frame frame classifier big box encloses task phoneme recogniser 
frames classified dynamic programming method finds sequence phonemes 
task frame frame classification typically done context phoneme recognition 
name implies phoneme recognition consists identifying optimisation techniques better known pruning 
chapter 
individual phonemes sentence composed 
differs slightly frame classification sense phonemes typically durations longer durations phoneme spans frames dynamic programming method required transform frame classifications phoneme predictions 
difference shown 
chosen perform complete phoneme recognition need include dynamic programming method 
may fairly straightforward properly involve introducing large areas speech recognition decoding techniques language models 
detract focus thesis 
furthermore concentrating task frame frame classification methods potentially distort performance svms 
summary literature various techniques applied problem frame frame classification 
section review recurrent neural networks 
method lead results substantially better reported literature 
subsequently previous attempts apply svms task described 
reviewed hurdles described 
recurrent neural networks robinson created phoneme recognition system produced best phoneme recognition results reported literature improved system years reported phoneme accuracy frame accuracy best reported result task 
system utilized time delayed recurrent neural network rnn variant typically frames extracted ms intervals 
average phonemes lasts ms 
project involved full phoneme recognition 
frame classification results reported 
result mapping phoneme targets robinson phoneme targets thesis simon king providing result 
chapter 
standard neural networks specifically invented task 
contrast typical neural network predictions time delayed frames considered decision 
previous predictions fed back network add context having feed multiple frames data classifier 
combined numerous metrics evaluating predictions created effective classifier handling segmented speech data 
extended incorporate knowledge language currently competes head head commercial speech recognition packages 
chen jamieson reported results extensive set experiments 
identical approach robinson rnn included new criterion function enabled directly maximise frame classification accuracy 
phoneme recognition rate slightly worse robinson result best frame classification result 
support vector machines efforts apply svms task speech recognition 
clarkson created multi class svm system classify phonemes assumed phoneme boundaries known mapped variable length phonemes fixed length vectors 
done important information retained problem considerably easier frame frame classification reported result extremely encouraging shows potential svms speech recognition 
additionally multi class classifier proved possible successfully extend generalisation performance binary svm multi class case 
hybrid svm hmm system developed 
svms generate predictions fed hmm 
similarly classification task svm classify phonemes phone boundaries known 
mapping 
performance system compared standard hmm system gaussian mixture mod system hybrid frame frame classification phoneme recognition 
result compared results rnns previous section chapter 
els gmms 
ogi speech database svm hmm outperformed gmm hmm system relative 
fifth training data svm hmm hybrid compared gmm hmm system 
additionally resulting svm hmm system order magnitude free parameters gmm hmm system 
practical issues training svms context speech recognition examined 
problems non converging training algorithms solved multi class systems created shown produce results subset timit speech corpus 
addition recognition svms speech sound applications 
include speaker identification musical instrument classification utterance verification web audio classification 
discussed details worth mentioning produced promising results 
papers show potential svms speech recognition 
discussed attempted tackle problems involved svms applied full classification problem assumptions speech data 
furthermore papers created benchmark systems 
main focus papers may may completely optimised results may suboptimal 
focus thesis evaluate svms known recognised classification task allow direct comparison best results literature 
overview thesis rest thesis organised follows chapter explain concepts linear non linear svms feature space role kernel 
additionally describe techniques extending inherently binary svms multi class problems 
chapter experiments multi class methods 
experimental results discussed compared best results literature 
chapter 
chapter extend svm classification process enable svms return posterior probabilities 
chapter summarise findings give directions possible research 
chapter support vector machines chapter support vector machine formulation pattern classification described formulation defines svms maximum margin classifier implements structural risk minimisation principle 
principle seeks minimise upper bound generalisation error 
chapter start explaining statistical learning theory svms 
secondly advancement svms linear maximum margin classifier non linear classifier carefully described 
describe common methods extending binary svms handle multiple classes 
statistical learning theory svms seek solve binary classification problem 
section statistical foundation technique described context problem 
theory compared employed learning machines chapter loosely :10.1.1.10.7171
chapter 
support vector machines binary classification problem general class classification problem stated follows 

data set samples hx hx hx sample composed training example length elements hx target value 

goal find classifier decision function hx performance classifier measured terms classification error defined equation 
error empirical risk minimisation consider learning machine set adjustable parameters binary classification task machine seeks find learns mapping 
result possible mapping 
defines machine 
performance machine measured empirical risk error emp error size training set set adjustable parameters 
risk minimisation principle called empirical risk minimisation emp 
learning machines implements principle including neural networks instance methods 
lead efficient effective classifiers 
problem machines 
complexity machine high tendency overfit data 
minimisation principle consider capacity machine 
chapter 
support vector machines structural risk minimisation contrast emp structural risk minimisation principle srm considers complexity learning machine searches learn mapping 
done minimising expected risk exp error dp prior probability 
unfortunately explicitly calculate expected risk unknown classification tasks 
estimate risk 
showed upper bound error exp emp vc vc vapnik chervonenkis vc dimension explained 
estimate risk computing upper bound 
done calculating emp training data estimating vc learning machine 
vc dimension vc dimension measure capacity learning machine 
definition largest set points shattered learning machine 
explain shattered means consider training set learning machine mapping 
correctly assign possible labels instances learning machine said shatter consequently vc dimension closely related complexity learning machine 
learning machines large number free parameters large vc dimension generally expected low empirical risk model complex decision surfaces formula expected risk conclude confidence empirical risk decreases complexity learning machine increases 
relation shown case 
learning machine high number free parameters low vc dimension vice versa 
chapter 
support vector machines 
srm described trade quality approximation data complexity approximating function 
svms constructed implement srm described sections 
relation empirical risk vc dimension expected risk 
linear classifiers sections svms formulated maximum margin classifier cases linear classifiers consider 
section case perfect mapping 
learned explained 
subsequently section case perfect mapping unattainable described 
separable case consider binary classification problem arrangement data points shown 
denote square samples targets positive examples belonging set similarly define round samples negative examples belonging learning machine called classifier 
chapter 
support vector machines separating hyperplane 
hyperplane maximises margin separability mapping separate sign 
weight vector offset origin 
mapping hyperplane 
defines decision boundary data sets said linearly separable hyperplane pair fw bg chosen mapping equation perfect 
case round square samples clearly separable 
maximum margin separating hyperplane numerous values fw bg create separating hyperplanes 
svm classifier finds hyperplane maximises margin sets 
shown 
data sets said optimally separated boundary 
chapter 
support vector machines find optimal hyperplane notice equation discriminant function 
scale hyperplane 
discriminant function 
choose scale 
choose scaling near fw bg near training example nearest decision plane get 

written compact representation 
looking define margin obvious margin maximal consider positive training example shortest perpendicular distance separating hyperplane marked black square 
example lie hyperplane satisfy equality equation 
similarly find negative training example satisfies 

denote hyperplanes margin hyperplanes reformulated jw 
bj kwk jw 
bj kwk kwk jw 
bj jw 
bj kwk normal parallel 
training points fall hyperplane optimally separates data minimises kwk subject constraints equation 
notice solution independent bias changing move optimal hyperplane direction maximum margin remain 
separating hyperplane longer optimal nearer classes 
chapter 
support vector machines structural risk minimisation optimal hyperplane explain srm principle implemented maximising margin separability suppose upper bound kwk exist kwk equation get separating hyperplanes closer data points sets consider arrangement data points 
number possible separating hyperplanes shown 
consider situation 
hyperplane closer data points reduces number possible separating hyperplanes 
hyperplanes separates data points shown 
number possible separating hyperplanes reduced upper bound put kwk 
vc dimension vc classifier find separating hyperplanes upper bound put kwk chapter 
support vector machines vc min upper bound kwk number dimensions data points radius hyper sphere enclosing data points 
smaller kwk reduces number separating hyperplanes 
consequently minimising kwk equal minimising upper bound vc dimension 
lagrangian formulation task solve minimisation problem kwk set inequality constraints equation theory multipliers known efficiently solve problem 
lagrangian formulation minimisation problem minimise kwk 
subject constraints called primal formulation 
convex quadratic programming problem objective function convex 
constraints equality constraints dual formulation substituting inequality constraints objective function 
resulting dual formulation maximize ld reason norm specifically chosen kwk elegant techniques exist optimise convex functions constraints 
chapter 
support vector machines subject constraints svm training considered problem maximising ld respect subject constraints positivity resulting optimal hyperplane bias solution linear combination important detail ones lie hyperplanes points called support vectors 
name support vector machines 
number support vectors solution typically total number training examples 
referred sparsity solution 
interesting observation training examples support vectors removed training 
case training solution remain 
notice quadratic programming problem convex local minima 
consequently solution find global minima optimal 
gives svms advantage optimisation techniques neural networks local exist 
classification solved optimisation problem optimal separating hyperplane svms attempt predict unseen instances 
instance classified determining side decision boundary falls 
compute sign sign 
chapter 
support vector machines assign target labels representing positive negative examples 
note input instance enters function form inner product 
exploited extension non linear case done 
non separable case far svm formulation restricted case perfect mapping 
learned 
general real world data sets satisfy condition 
extension formulation handle non separable data needed hyperplane resulting mapping best possible 
extension done creating objective function trades misclassifications minimising kwk misclassifications considered adding slack variable training example require 

adjustment allows old constraints equation violated way violation causes penalty 
size penalty misclassified example value typically distance decision boundary training example overview non separable case shown 
new problem faced minimise sum misclassification errors minimising kwk kwk 
regularisation parameter control relation slack variables kwk integer typical values 
minimisation problem convex linearly separable case 
choose advantage lagrange multipliers error measures exist squares distance 
chapter 
support vector machines non separable case 
encircled data point misclassified positive disappear dual problem 
objective function dual formulation maximize ld subject constraints optimised solution chapter 
support vector machines positive example shortest perpendicular distance decision boundary closest negative example 
compare solution linearly separable case difference added constraint equation 
upper bound support vectors solution training examples lie hyperplane boundary 
training examples falls hyperplanes falls wrong side decision surface 
view support vectors modelling error boundary classes 
non linear classifiers section described linear svm handle examples 
extension needed svms effectively handle real world data modelling non linear decision surfaces 
method doing proposed 
idea explicitly map input data higher dimensional space data linearly separable 
mapping 
dimension input space higher dimensional space termed feature space 
feature space technique described section find optimal separating hyperplane 
hyperplane mapped back input space 
non linear mapping resulting hyperplane input space non linear 
process described step process shown 
kernel trick finding optimal hyperplane higher dimensional feature space complicated computationally expensive 
vapnik boser chapter 
support vector machines role kernel 
data mapped input space feature space mapping 
optimal separating hyperplane 
hyperplane mapped back input space results non linear decision boundary 
guyon showed old trick called kernel trick 
trick steps combined 
training phase described previous section notice equation includes training data form scalar inner products 
mapping input space feature space achieved substitution inner product 

fortunately calculating explicitly needed 
chapter 
support vector machines functional representation computes inner product feature space direct operation data samples original input space 
functional representation called kernel svms member broader class kernel methods 
feature space higher dimension input space implicit calculation dot product removes need explicitly perform calculations feature space 
consequently effective kernel finding separating hyperplane done substantial increase computational expense training optimisation problem changes slightly accommodate kernel 
substitute inner product equation kernel function new optimisation problem faced maximize ld subject constraints linear case classification luckily trick classification 
equation unseen examples support vectors included inner product replace get fact cases extra computational expense small takes roughly time find non linear optimal hyperplane find optimal linear hyperplane 
chapter 
support vector machines offset decision boundary origin 
classification unseen examples subsequently done sign kernels functions kernels 
feasible kernels satisfy conditions 
kernel function symmetric 

satisfy mercer theorem 
conditions described details appendix popular kernels includes homogenous polynomial kernel 

input space kernel maps samples fixed dimensional space resulting decision boundary polynomial 
gaussian kernel kx value termed kernel parameter defined user training begins 
chapter 
support vector machines kernel centres gaussian variance support vector 
kernel called radial basis function kernel resulting classifier closely related radial basis function rbf learning machine :10.1.1.10.7171
linear kernel 
results exactly objective function equation 
note kernel simply specific case polynomial kernel choosing kernel kernel best 
select optimal values kernel parameters 
obvious questions arise different mappings choose 
formal metric choosing best kernel provided upper bound vc dimension :10.1.1.10.7171
vc dimension describes complexity flexibility kernel provide practical proof chosen kernel best 
choice validated numerous independent tests different practical problems methods cross validation preferred making kernel selection 
penalty term needs defined user 
easy method selecting value aside evaluating resulting model performance validation set 
similarly user kernel parameter 
chapter 
support vector machines bias svms recall training process linearly separable case section 
mentioned pair fw bg decision boundary placed exactly hyperplanes recall value move decision boundary direction denoted implicit bias learned training process 
cases desired decision boundary placed learned training process 
purpose explicit bias introduced kernels 
done adding term kernel function 
case polynomial kernel typically done 

adding explicit bias leads different decision boundary 
push boundary data sets 
cases lead improvements 
done practical implementations training process slightly efficient stable 
practical issues implement svm theory straightforward 
quadratic optimisation problem equation solved existing packages specifically designed solve problem typically small problems due memory run time restrictions 
techniques overcome problem large scale classification possible chunking 
technique training set smaller sub sets subset easily solvable quadratic programming package 
process described follows initial subset optimised support vectors kept remaining training examples example loqo 
training examples equation 
chapter 
support vector machines thrown away 
subset added working set optimisation performed 
process repeated training data treated working set 
chunking speeds training drastically 
solution sparse working set large training process lengthy 
sequential minimal optimisation 
optimisation procedure training problem decomposed tiny tasks optimising equation 
remaining kept fixed values easy fast find techniques implemented popular toolkits provide drastically reduced training times relation amount training data training time remains non linear 
consequently large scale classification time consuming task 
important issue bear mind normalisation 
toolkits requires training samples normalised run time increase cases optimisation process fail converge 
multi class svms real world data sets involve multiple classes 
svms inherently binary classifiers techniques needed extend method handle multiple classes 
goal technique map generalisation abilities binary classifiers multi class domain 
literature numerous schemes proposed solve problem 
example 
section try describe concentrate ones shown produce generalisation performance practice 
go details optimisation procedure require extensive description 
refer detailed description 
svm light svmtorch 
see 
chapter 
support vector machines vs classifier vs classifier system proposed popular successful multi class svm method 
principle method simple 
creates binary svm combination classes possible unseen example classified class wins binary classifications method called voting scheme binary svm classification assigns credit vote competing classes 
describe classifier formal way set classes furthermore size binary svm class pair resulting number binary classifiers class models 
training models classification unseen examples performed 
recall function classifies unseen examples equation 
denote function associated svm model sign unseen example classified vs argmax advantage splitting multi class problem multiple binary subproblems different decision boundaries creates 
potentially results complex decision boundary 
furthermore class denotes target label 
classes denotes set possible target labels 
terms inter thesis 
class wins classification unseen example classified lie side decision boundary equation chapter 
support vector machines due manner classification carried unseen example misclassified binary svm chance correctly classified binary models class 
classifier obvious disadvantages 
problem contains classes large number binary svms required consequently explode 
correspondingly number binary svms classification slow need evaluated decision 
classifier produced best results multi class tasks svm literature 
vs rest classification scheme binary svms built 
attempts build decision boundary separating class rest 
creating models accomplished assigning label binary target label remaining classes 
classification unseen example done computing function value equation binary svm model 
differs previous classification scheme equation 
motivation decision function avoid draws 
unable predict winner wins corresponding binary svm 
class maximises value equation chosen 
resulting classifier argmax svm model separating th class rest 
advantage method svms involved 
needs build models consequently evaluation faster vs classifier 
drawbacks 
building classifiers resulting decision boundary complex boundary vs chapter 
support vector machines classifier 
furthermore classes involved svm training svms time consuming 
binary svm isolate class rest 
training examples weighted equally uneven distribution training examples isolated class remaining classes separation difficult 
improve separation explicit bias described section 
dagsvm dagsvm stands directed acyclic graph svm 
multi class method proposed platt employs exact training phase vs classifier 
creates binary classifiers 
distinguishes classification phase constructing rooted binary tree structure internal nodes leaves see 
node graph contains binary classifier th th classes 
classification performed shown 
advantage dagsvm needs evaluations classify unseen example 
appreciably retain complex decision surface vs classifier 
superior methods 
disadvantage stability 
classification process binary misclassification happens unseen example misclassified 
mentioned earlier case vs classifier robustness method may high 
binary tree multi class method describe binary tree classifier 
hierarchical structure best described svm version decision tree 
idea growing tree structure data common machine learning learning machines decision trees nearest neighbour chapter 
support vector machines decision graph classification task target classes 
starting root node see svm belonging node evaluated equation 
moves tree left right child depending outcome 
repeats process leaf reached assigns unseen example th class 
trees cart models 
classifier apparent considering binary nature svms classifier mentioned svm literature 
classifier propose rooted asymmetrical tree structure shown 
contrast dagsvm classifier nodes binary tree contain multiple classes splitting equally sized subsets see 
child node contain binary svm classes form subsets see 
move tree number classes contained node decreases class remains see 
problem growing trees discussed extensively literature example 
algorithms exist task example means additive chapter 
support vector machines schematic binary tree class problem 
similarity trees classify unseen example consider 
classification completed follows 
root node binary svm evaluated 
depending side decision boundary falls moves left child right child 
process repeated leaf reached 
leaf indicates classification hoped generalisation ability shown decision trees translated case multi class svms 
speed concern thesis binary tree classifier extremely fast classification phase 
tree structure symmetric requires lnk evaluations classify unseen examples 
substantially evaluations needed methods 
viewed pessimistic eyes separating hyperplanes top nodes boundary detailed need include classes 
additionally dagsvm classifier binary misclassification happens classification unseen example misclassified 
note structure need symmetrical 
depending inter relation classes opt create asymmetrical deep tree 
chapter 
support vector machines may stable vs classifier 
obvious multi class methods best extending generalisation powers binary svms 
theoretical metrics available assess generalisation abilities possible proper comparisons evaluating capabilities real world data sets 
constructing evaluating multi class method easy forget importance underlying binary svms 
done generalisation ability multi class classifier fully dependent generalisation abilities binary svms 
analysis individual binary svms carried extension multi class case done 
ends support vector machines 
numerous aspects considered 
example describing svm context wider family linear discriminant classifiers give better picture placement learning machines furthermore detailed theoretical analysis generalisation bounds binary svms lead better understanding generalisation performance see 
hoped chapter covered aspects give understanding issues involved working svms method practical problem 
details 
chapter experiments chapter evaluate performance svms problem frame classification 
problem easy 
give indication just difficult plotted dimensions frames target phoneme classes data set 
result shown 
seen plot displays extreme amount overlap 
potential learning machine attempt separate classes produce valid predictions 
plot dimensions aa ae vowel speech frames 
sections groups experiments described 
anal chapter 
experiments ysis models trained class pairs performed learn get best generalisation performance individual svm 
subsequently groups multi class experiments carried 
related sense utilize multi class classification methods described section 
toolkits experiments toolkit svm light 
implements chunking algorithm described section 
fastest toolkit available returns lot useful information training problem 
includes estimates generalisation error vc dimensions 
larger problems examples included svmtorch toolkit 
faster due implementation sequential minimal optimisation algorithm described section 
timit speech database data set task timit database 
corpus highquality continuous speech north american speakers entire corpus reliably transcribed word surface phonetic levels 
speech parameterised mel frequency coefficients mfcc plus energy ms frames ms frame shift target labels consists different classes representing phonemes english language 
configuration commonly speech recognition 
corpus divided training utterances test utterances si sx sentences 
speakers training set appear test set making speaker independent 
measure performance validation set utterances subtracted training set 
generating training test samples data sets randomised order utterances description timit database 
note delta acceleration coefficients derived coefficients resulting total coefficients 
chapter 
experiments chosen appendix overview timit database shown 
binary svm experiments binary svm involves separate optimisation problem 
mentioned multi class problem great importance generalise possible 
case matter simple linearly separable problem requires complex decision boundary 
user defined parameters involved training svm model 
need learned discriminate performance svm optimal 
parameters summarised table 
additionally parameters resulting models listed table 
related user defined parameters examined section 
kernel kernel function needs chosen 
choices include linear non linear kernels example ones described section 
kernels involves kernel specific parameters set 
need optimised 
penalty term parameter regulates relation minimisation kwk minimising classification error equation 
composition feature vector feature vectors input data naturally contain information lying problem possible 
amount training data generally performance increases training data included 
unfortunately mentioned section relation training time size training set non linear 
trade generalisation performance run time 
table overview user defined parameters binary svm done allow direct comparison experiments 
chapter 
experiments number support vectors svs number svs resulting model implicitly chosen user parameters listed table 
related factors investigated 
training time training time svms relate amount training data examined 
classification time classification time important classifier system run time critical 
performance validation set important issue thesis find best possible performance 
achieved naturally explored 
table important factors svm model experimental setup default vector contains dimensions described 
created training sets binary class pairs consisting data phoneme classes labels described section 
choice kernel examine effect different kernels performed experiments kernels section 
parameters varied 
results shown plots svm model separating data phoneme classes aa ae 
similar behaviour examined svms looking figures concluded search optimal parameters needed 
easily done linear kernel straightforward non linear kernels 
kernel specific parameter strongly related penalty term possible search parameters individually 
fortunately areas peaks reasonably smooth 
possible rough search find near optimal parameters 
gamma kernel parameter 
relation gamma svm light toolkit uses gamma factor remainder thesis 
total different class pairs examined 
chapter 
experiments techniques automatically learning svm parameters investigated 
schemes exist literature 
examined unfortunately increase performance employing methods marginal 
furthermore run time needed far exceeded run time exhaustive search 
effect kernel parameter validation accuracy 
plot show resulting performance linear kernel varying plot displays performance polynomial kernel varying degree plot shows performance gaussian kernel varying gamma 
notice gaussian kernel shows best performance decision surface smooth surface kernels 
including different amounts training data experiment investigated size training set affected training time number support vectors model classification 
training time increases non linearly size training set number training examples restricted reduce run time 
particularly important considering number binary svms required multi class classifiers 
chapter 
experiments effect varying training set size 
plot shows increase generalisation performance varying size training set 
expected performance increases amount training data increases 
plot show relation training set size training time near quadratic 
fortunately classification time increases linearly increase training set size 
plot displays roughly linear increase number support vectors increasing amount training data 
plots shows effect varying amount training data experiments observation gaussian kernel offer best generalisation performance 
expected linear polynomial uses feature space fixed number dimensions 
contrast gaussian kernel potential map data infinite dimensions intuitively gives greater flexibility 
superior 
cases polynomial kernel yielded comparable performance 
problem near optimal parameter sets rough exhaustive search parameter space due smooth maxima decision surfaces 
applies kernels examined 
similarly experiment plots svm model separating data phoneme classes aa ae gaussian kernel 
comparable plots examined svms 
chapter 
experiments problems scaling svm handle large amounts training data apparent looking 
near quadratic increase training time severely restricts amount training data 
comparison recurrent neural networks clear advantages svms 
training time simply scales linearly size training set 
increasing amount training data number support vectors increased 
classification time contribution individual support vector needs computed equation 
taken considerations multi class experiments performed 
user defined parameter discussed composition feature vector 
experiments carried various feature vectors 
described section benefits different feature vectors apparent multi class problems 
selecting multi class methods section described different methods extending binary svm handle multi class data 
due computational costs running multi class experiments chosen vs rest classifier described section 
commonly methods performance speech data lacking 
preliminary experiments method produced substantially worse results remaining classifiers incapable properly separating data 
supported results 
leaves vs dagsvm binary tree classifiers 
sections start describing vs classification experiments details 
remaining classification schemes explained briefly take advantage drawn group vs experiments 
chapter 
experiments vs classifier recall description vs classifier section classifier creates binary svm combination classes resulting svms 
unseen example classified svms evaluated 
svm vote winning class unseen example classified class votes 
method popular successful multi class schemes literature 
numerous cases including frame classification classifier shown superior generalisation performance 
task focus frame classification 
possible target labels target corresponds phoneme class 
results binary svms 
considerable number svms 
needs constructed achieves optimal generalisation performance 
practical issues easily imagine training evaluating binary svm comprehensive task 
small amounts training data training process take days 
examples contained timit test set evaluated binary svms 
results evaluations take weeks complete normal machines intel pentium pcs sun ultra reason reduced amount test examples class give reasonably accurate measure performance 
means frequencies individual phonemes test set possible give fairly precise estimate full test set accuracy 
mentioned earlier class wins classification unseen example classified lie classes side decision boundary equation see appendix overview phonemes timit test set classes examples full test set see appendix 
consequently class sets reduced 
chapter 
experiments experiment effects kernels experiment done get indication performance vs classifier system time compare generalisation abilities different kernels 
experimental setup chose include training examples class resulting training examples svm 
basic mfcc coefficients described section varied kernel parameters rough exhaustive search 
created models tested test examples class 
advantage having sun unix machines available ultra 
reduce run time split big job numerous sub jobs distributed different machines 
experiment previously need weeks finish reduced experiment total run time approximately hours 
results result experiments shown table looking results performance kernels confirm section gaussian kernel offers superior generalisation due greater flexibility 
best result impressive considering extensive runtime 
compared best results literature poor 
explanation binary note notation table 
mba stands mean binary accuracy 
average accuracies evaluating test validation set binary svms 
mba val mba set mba test mba test set 
ma ma denote multi class accuracies 
ma evaluating classifier test examples class 
ma weighting results ma phoneme frequencies full timit test set 
intuitively ma offer estimate performance complete test set 
top top alternative metrics measuring classifier performance 
top denotes percentage test examples correct class set phonemes votes 
top means test examples correct phoneme frames phonemes votes 
terms remainder thesis 
chapter 
experiments svm performances validation test sets 
notice difference mba val mba test 
apparently validation set test sets different 
reason composition feature vectors 
contain basic mfcc coefficients described section 
observed coefficients large class variance 
risk large differences validation test sets high leads poor performance 
kernel mba val mba test ma ma top top linear polynomial gaussian table performance vs classifier different kernels 
multi class accuracies reported ma ma clear gaussian kernel outperforms linear polynomial kernels 
best result best top 
adding features problem large variances data sets may alleviated adding second order derivatives existing mfcc coefficients 
computed frames comprise dynamics speech help discriminating fast slow changing phonemes 
features commonly speech recognition typically improve classification results 
experimental setup adding new features feature vectors contained existing mfcc coefficients order derivatives second order derivatives 
chose run experiment gaussian kernel shown best generalisation performance 
result shown table compared previous result 
chapter 
experiments features mba val mba test ma ma top top table effect adding derivatives 
generalisation performance increased binary accuracies 
particular notice substantial increase performance binary accuracy 
results seen table performance increased 
notice binary accuracies validation test sets increased 
words classification error individual svm decreased 
significant improvement confirms derivatives contain important information frame increased number dimensions svms capable detecting features characterises data 
demonstrates theoretical claim svms counteracts curse dimensionality section 
furthermore second order derivatives computed frames improvement indicates substantial part frame information lies context frames 
reached extensive examinations 
investigations adding additional context information reasonable 
adding context frames include context information frame 
answer lies neighbouring frames 
speech fairly slowly changing phoneme lasts average ms 
speech frames extracted ms results frames phoneme average 
consequently contextual information added including neighbouring frames feature vectors 
chapter 
experiments experimental setup ways explored adding neighbouring frames context frames feature vector 
feature vector included frame classified denoted target frame various context frames side hc cm denotes context frame positioned frames target frame similarly represent th context frame target frame 
frame features frame described previous section shown outperform basic features 
able compare earlier results gaussian kernel training examples class 
feature vector jxj mba val mba test ma ma top top hti hc hc hc hc hc hc table summary experiments different feature vectors 
results show including context frames substantially improves performance 
best configuration context frames included side target frame 
improves performance previous results 
results results different feature vector compositions shown table 
seen table including context frames substantially improves performance 
explanation improvement improved mba test 
difference mba test virtually disappeared indicates chapter 
experiments class variance data decreased 
concluded information hidden context frames vital generalisation performance 
best result literature may substantially better experiment 
result dismissed easily 
report frame accuracy training data 
experiments approximately amount training data believe promising 
consistency experiment far best result total examples timit test set examples class 
subset full test set need check solidity result certify reproduced different test sets 
experimental setup new separate subsets created timit test set containing examples phoneme class addition test set previous experiments 
sets tested best models far test set mba val mba test accuracy base table results consistency experiment 
mentioned classes sufficient examples full set refer appendix overview timit speech corpus 
case examples sub sets 
features frame context frames side target frame gaussian kernel 
chapter 
experiments results results tests shown table 
show variances binary accuracies marginal proving previous results reliable 
experiment including additional training data previous experiments binary svm included training examples phoneme class resulting training examples svm 
done restrict run time experiments 
recall 
indicates performance binary svms improve additional training data added 
consequently lead better generalisation performance 
experimental setup ran experiments different amounts training data 
experiment contained training examples class second staggering examples class 
training svms take substantial amount time feasible perform parameter search 
reused best parameters section 
optimal solution required reduce run time 
job split numerous sub jobs run parallel 
final experiment involved evaluating example experiment full timit test set 
results results experiments shown table compared previous best result 
looking table 
estimated actual full test set accuracies comparable 
confirms previously computed estimates 
secondly chapter 
experiments examples class mba val mba test ma ma top top full table results experiments additional training data 
expected example experiment shows best estimated accuracy 
confirmed testing model full timit test set 
result accuracy shown row 
accuracy full test set better second best result literature 
proves svms viable capable technique frame classification 
important observation 
notice performance ma decreases amount training data increases 
may come surprise logical explanation exists mentioned section phonemes infrequent training examples training set 
consequently results uneven distribution examples training 
recall objective function equation kwk 
svm training algorithm aims minimise expression partly means minimising classification errors second term 
described section defined distance misclassified training example decision boundary 
set training examples larger majority effect minimise second term hyperplane moved direction smaller class 
creates implicit bias larger phoneme classes larger classes win classifications 
improved classification performance full test set 
result bias smaller classes lose classifications classification rate decrease 
opposed winning 
chapter 
experiments generalisation performance vs classifier measure generalisation performance vs classifier consider accuracy related binary accuracy binary svms 
logical binary svms test accuracy lead accuracy 
reason collected results experiments plotted relationship factors 
relationship displayed 
plot perceived ways 
hand shows depressing fact large improvements binary accuracies lead great improvement accuracy 
hand looking estimated trend line slight increase binary accuracy lead substantial increase accuracy 
binary mean accuracy axis vs accuracy axis voting scheme 
chapter 
experiments experiments seen vs classifier offers great generalisation ability 
completed experiments means performed optimal conditions classifier performed extremely 
best result frame classification rate competitive second best result reported literature 
vs classifier svm method general 
average binary accuracies shows svms capable discriminant classifier generalise 
may concerned extensive number binary svms required build multi class model 
resulting model just components classifying unseen example involves evaluation components 
run time focus thesis classifier may overly cumbersome practical real time speech recognition system 
easy improve classification speed 
currently classifier evaluates binary svms prediction 
classes dismissed earlier number evaluations reduced dramatically 
experiments observed phonologically similar phonemes tended produce similar classification results different binary svms 
example vowel phoneme winning binary evaluation consonant phoneme indicates unseen phoneme vowel 
exploited classification prune number evaluations 
result evaluating chosen svms gives fairly precise indication phonological group unseen example belongs 
resulting number evaluations suddenly fraction evaluations 
new multi class classifier builds principle examined section 
total number support vectors multiplied number features feature vectors 
chapter 
experiments dagsvm classifier platt dagsvm algorithm described section 
briefly recapitulate uses training phase vs classifier distinguishes classification phase constructing rooted binary tree structure internal nodes leaves 
node graph contains binary classifier classification done starting root node moving tree left right branch node depending outcome node svm 
leaf reaches indicates classification unseen examples 
dagsvm algorithm shown perform similarly cases better voting scheme 
hoped performance reproduced section 
implementation issues describes easy way implement algorithm 
represent tree list possible target labels phoneme classes 
classification unseen example composed steps 
evaluate svm outer phoneme classes list 

phoneme class loses classification removed list 

step repeated list contains phoneme class 

unseen example classified remaining phoneme class 
discusses composition target labels list 
observes order phoneme classes put list affect performance 
section run experiments random sorted lists sorted list phonemes sorted phonological groupings 
vowels nasal flaps stops fricatives see details 
chapter 
experiments experimental setup classifier utilises exact training phase vs classifier simply reuse best models vs experiments section 
algorithm tested full timit test set 
composition list mba test accuracy unsorted sorted table classification results dagsvm algorithm 
result improves previous best result 
notice accuracy improvement sorting lists 
results results table seen table dagsvm classifier shows comparable generalisation performance vs classifier 
consistent results reported literature classification problems 
appreciably result sorting list result slightly better previously best result section 
svm method capability phoneme classifier 
binary tree classifier multi class scheme evaluated thesis binary tree classifier 
described section briefly summarise uses rooted tree structure nodes containing binary svms 
dagsvm algorithm nodes tree contain multiple classes 
classes separated subsets model trained build decision boundary 
classification starting table mba test denotes average accuracy evaluated nodes 
chapter 
experiments root node node svm evaluated depending classification moves left right branch node 
repeated leaf reached indicates classification unseen example 
schematic structure 
met literature covering method svms gotten results compare 
implementation important part binary tree classifier problem growing tree structure 
dagsvm algorithm changing tree structure little impact accuracy vital create structure separates classes optimally 
hierarchical clustering algorithms solve problem 
attempt grow structures maximise separation data points 
chosen algorithm called 
implementation additive similarity trees algorithm 
describe briefly grows small trees leaves combines create complete tree see details 
drawback employing algorithm optimised version original algorithm run time scales amount training data 
consequently feasible grow tree structures tiny subset training data examples class resulting tree deemed unsatisfactory 
alternative data set grow structure 
vs classifier section binary svms built separating phoneme classes 
performance validation set evaluated class pair 
resulted values describing separation ability classes 
results section grow tree structure seen appendix looking tree structure notice tree branches root 
reduced branches middle branch containing ng phoneme classes placed branch left part tree 
chapter 
experiments experimental setup binary svm model trained nodes tree 
training data classes contained node clustered groups separating classes sub tree belonged 
setup user defined parameters previous experiments issue training data problematic 
restricted depending number classes separate node 
root node classes included number training examples class restricted 
lead training examples total 
remaining nodes fewer classes include training data class 
svm kept number training examples constant adjusted contribution class accordingly 
svm training examples take long time train opted search parameter space find parameter sets 
choice parameters best parameters previous experiments 
testing performed subset containing examples class 
mba test ma ma table result binary tree experiment 
column average binary accuracy nodes second actual accuracy test set 
column estimate accuracy full test set 
results results experiment table 
classifier uses training data examples vs dagsvm experiments result worse previous best reported result 
impressive considering needs evaluations classify frame 
looking tree structure appendix interesting note structure closely resembles phonologically feature systems literature 
gaussian kernel context frames side target frame 
chapter 
experiments examples sound patterns english uses binary features describe phone 
features way phoneme sounds produced nasal nasal cavity continuant continuing sounds opposed fricatives round describing rounding lips looking upper parts tree structure phonemes grouped binary features nasals middle branch voiced right branch left branch 
tree phonological separations noticeable 
splits branch containing phonemes voiced unvoiced phonemes 
similarities data grown tree structure phonologically groupings demonstrate points firstly structure reasonable phonemes similar voicing parts tree 
secondly groupings literature translate groupings derived data driven method 
experiments proven svms capable performing learning machine frame classification 
best result dagsvm section competitive best results reported literature 
surpass best result literature reported slightly better reported 
noteworthy development years 
mentioned earlier experiments chapter means performed optimal conditions 
major difficulties experiments included number binary svms included models training time bias binary svms 
amount binary svms needed training experimental process extensive 
combined long time required train svm run time experiment seriously affected progress rate experiments delayed parameter choices experi chapter 
experiments solve restrictions training set size variants svm algorithm literature examined 
designed alleviate problem claimed function extreme amounts training data 
return inferior results regarding bias observed built implicit bias large scale binary classifiers increased accuracy 
control inter dependencies models 
recall section possible explicitly incorporate bias svms 
adjusting biases structured method introduced directly optimise generalisation performance multi class classifiers 
ments progress 
solutions simply restricted size working set number support vectors resulted poor solutions 
chapter probability outputs speech recognition systems statistical models 
proposed multi class system complete speech recognition system probabilistic interpretation classification results needed 
recurrent neural networks rnns clear advantage svms output rnns interpreted posterior probabilities 
svms hand clear way interpreting outputs probabilities 
equation return classified instance distance decision boundary 
clear relationship distance posterior class probabilities 
find relationship possible generate best lists class predictions 
estimating posterior probabilities scheme producing posterior probabilities vs multi class svm classifier simple number votes accumulated class roughly correspond confidence measure class 
classes lot votes won lot binary classifications logically correct 
rough estimate posterior probability obtained scaling number sum votes 
values interpreted see 
chapter 
probability outputs probability estimates 
scheme vs classifier 
obtain accurate estimate individual svms considered 
approach fit gaussian class conditional probabilities jy jy resulting posterior probability sigmoid 
unfortunately gaussian assumption needed class conditional probabilities typically satisfied svms asymmetric distributions output distances see 
proposes fit posterior probabilities directly svm 
posterior probability sigmoid sigmoid function fit svm outputs 
posterior probabilities estimated sigmoid fit model estimating parameters done maximum likelihood estimation 
discusses potential problem fitting sigmoid svm parameters data set 
non linear models support vectors typically correspond large subset training data non linear models support vectors defined decision boundary resulting sigmoid biased decision boundary sigmoid steeper 
possible solutions proposed separate validation set 
easy case validation set learn svm parameters 
taken prove usability method speech data 
done validation set train sigmoid parameters svm model aa vs ae 
shows actual posterior probabilities test set 
seen sigmoid offers fairly estimate probability 
result consistent results reported speech data sets 
chapter 
probability outputs histogram posterior probabilities model trained linear kernel 
note non gaussian distributions 
sigmoid fit distance posterior probability estimates aa vs ae 
extension making svm produce posterior probability estimates important necessary method connection full speech recognition chapter 
probability outputs system 
method described chapter maps distance outputs produced svm classifications posterior probabilities means sigmoid function fit data 
shown result fairly accurate estimates possible svms basic units speech recognition system 
chapter goal thesis examine feasibility applying support vector machines problem frame classification 
done svm method create capable learning machine generalisation ability 
tremendous amount overlap classes successfully model speech data produce liable classifications 
maximising margin separability classes implementing structural risk minimisation principle demonstrated counter act curse dimensionality generalise data contained numerous irrelevant features 
chapter described different schemes extending inherently binary svm multi class domain 
results show schemes effectively retain generalisation abilities binary svm 
best reported result competitive best results literature sub optimal conditions 
experiments showed employing gaussian kernel lead better generalisation performance linear polynomial kernels 
reason believed kernel potential map data input space infinite dimensional space 
furthermore attempts include additional information feature vectors revealed substantial amount information frame hidden neighbouring frames 
lastly clear key better generalisation performance amount training data 
chapter method described retrieving posterior probability estimates chapter 
distance outputs svms 
estimates shown fairly accurate estimates actual posterior probabilities 
extension enables svm classifiers discriminative front classifiers full speech recognition system 
hurdles making svms viable options real time speech recognisers solved 
focus thesis think terms speed applicability real time speech recognition systems forgotten 
best reported result dagsvm classifier 
classifier needed evaluate binary svms returned prediction 
required far time allowed real time system 
fortunately ways reduce classification time exist 

reducing amount binary svms evaluate 
dagsvm algorithm drastically reduced number binary svms evaluate compared vs classifier great redundancy classification phoneme classes considered 
pruning technique reduce number exploiting phonological groupings phonemes binary tree classifier described groupings 
reduced number evaluations 
performance classifier competitive forced limit amount training data top nodes tree 

data cleaning 
described section solution svm training algorithm typically sparse 
data sets large amount overlap common speech data solution sparse 
words resulting model contain large amount support vectors consequently classification time high 
cases quite unnecessary 
observed sparse solution similar generalisation performance throwing away support vectors lying decision boundary 
note sparse solutions technique lead drastic reductions support vectors classification time 
example separating vowel consonant phonemes 
chapter 
mentioned key better generalisation performance solve problem training time scales set size 
solution straightforward 
able promptly handle large amounts training data needs consider memory requirements naturally produce optimal solutions 
method discovered unlock full potential svms speech recognition 
appendix kernel requirements theorems taken 
symmetry condition real symmetric function finite input space kernel function matrix components positive semi definite 
mercer theorem theorem satisfied functional pair exist 
compact subset continuous symmetric kernel positive integral operator 
dx dxdy expanded uniformly convergent series eigenfunctions positive eigenvalues appendix kernel requirements number positive eigenvalues 
theorem holds general compact spaces generalises requirements infinite feature spaces 
equation generalises semi positivity condition finite spaces theorem section 
expansion generalisation usual concept inner product reproducing hilbert spaces dimension rescaled note specific cases may easy check mercer condition satisfied 
equation hold function finite norm satisfies 
appendix phonemes timit speech database table overview timit speech corpus 
appendix phonemes timit speech database phoneme training set test set frequency total training validation aa ae ah ao aw ay ch dh eh er ey hh ih iy jh ng ow oy sh sil th uh uw zh total table overview phonemes timit database 
subset full training set separate validation set 
column shows frequencies test set 
note sil represents silence 
appendix binary tree structure binary tree structure grown seen 
appendix binary tree structure tree structure grown algorithm 
bibliography bartlett shawe taylor 
generalization performance support vector machines pattern classifiers 
advances kernel methods support vector learning pages 
mit press 
bridle 
probabilistic interpretation feedforward classification network outputs relationship statistical pattern recognition 
springer verlag 
burges 
tutorial support vector machines pattern recognition 
knowledge discovery data mining 
campbell 
kernel methods 
jain editors radial basis function networks design applications page 
springer verlag berlin 
chen jamieson 
experiments implementation recurrent neural networks speech phone recognition purdue university west lafayette 
chin 
support vector machines applied speech pattern classification 
master thesis engineering department cambridge university 
chomsky halle 
sound patterns english 
mit press cambridge ma usa 
chun 
hierarchical feature phonetic classification 
master thesis mit cambridge 
clarkson moreno 
support vector machines phonetic classification 
acoustics speech signal processing volume ii pages 
cole 
corpus 
bengio 
support vector machines large scale regression problems 
idiap 
bibliography bengio 
support vector machines large scale regression problems 
machine learning research 
cooley 
classification news stories support vector machines 
proc 
th international joint conference artificial intelligence text mining workshop 

pascal program fitting additive trees tversky algorithm 
behavior research methods instrumentation pages 
perez pontil poggio 
bounds generalization performance kernel machines ensembles 
international conference machine learning 
friedman 
approach classification 
technical report stanford university ua 
fung mangasarian 
proximal support vector machine classifiers 
lee srikant editors kdd knowledge discovery data mining pages new york 

support vector machines speech recognition 
phd thesis mississippi state university 

hybrid svm hmm architectures speech recognition 
neural information processing systems 

getting started darpa timit cd rom acoustic phonetic speech database 

combining discriminant models new multi class svms 
technical report loria inria lorraine les nancy cedex france 
gunn :10.1.1.10.7171
support vector machines classification regression 
technical report university southampton image speech intelligent group 
guyon boser vapnik 
automatic capacity tuning large classifiers 
hanson cowan giles editors advances neural information processing systems volume pages 
morgan kaufmann san mateo ca 
guyon stork 
linear discriminant support vector classifiers 
advances large margin classifiers pages 
mit press 
bibliography yang hermansky 
relevancy time frequency features phonetic classification measured mutual information 
icassp 
hsu lin 
comparison methods multi class support vector machines 
technical report national taiwan university taiwan 
platt shawe taylor 
large dags multiclass classification 
technical report microsoft research redmond 
joachims 
text categorization support vector machines learning relevant features 
proc 
th european conference machine learning ecml pages 
joachims 
making large scale svm learning practical 
scholkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
jurafsky martin 
speech processing 
prentice hall 
keerthi 
efficient tuning svm hyperparameters radius margin bounds iterative algorithms 
king taylor 
detection phonological features speech neural networks 
computer speech pages 
leclerc 
consensus classifications case trees 

springerverlag 
lin wahba zhang lee 
statistical properties adaptive tuning support vector machines 
department statistics technical report university wisconsin madison 

theoretical foundations potential function method pattern recognition learning 
automation remote control pages 
ma randolph 
support vector machine rejection technique speech recognition 
icslp beijing china 

data driven methods extracting features speech 
technical report indian technology madras india 
marques moreno 
study musical instrument classification gaussian mixture models support vector machines 
technical report cambridge 
bibliography alpaydin 
support vector machines multiclass classification 
international workshop artificial neural networks 
mitchell 
machine learning 
mcgraw hill 
moreno rifkin 
fisher kernel method web audio classification 
icassp istanbul turkey 

muller smola ratsch scholkopf kohlmorgen vapnik 
predicting time series support vector machines 
scholkopf burges smola editors advances kernel methods support vector learning pages 
mit press 
smith niranjan 
data dependent kernels svm classification speech patterns 
technical report cambridge university uk 
osuna freund girosi 
improved training algorithm support vector machines 
principe morgan wilson editors neural networks signal processing vii proceedings ieee workshop pages new york 
ieee 
osuna freund girosi 
training support vector machines application face detection 
proceedings cvpr puerto rico 
platt 
fast training support vector machines sequential minimal optimization 
scholkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
platt 
probabilities sv machines 
smola bartlett scholkopf schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 
quinlan 
programs machine learning 
morgan kaufmann 
rabiner juang 
fundamentals speech recognition 
prentice hall englewood cliffs new jersey usa 
robinson 
application recurrent nets phone probability estimation 
ieee transactions neural networks vol 
pages 
tversky 
additive similarity trees 
psychometrika pages 
schmidt gish 
speaker identification support vector classifiers 
icassp volume 
bibliography scholkopf 
support vector learning 
oldenbourg verlag munich 
order click 
scholkopf burges smola 
support vector learning 
scholkopf burges smola editors advances kernel methods support vector learning pages 
mit press 
scholkopf burges vapnik 
extracting support data task 
fayyad uthurusamy editors proceedings international conference knowledge discovery data mining menlo park 
aaai press 
smola 
regression estimation support vector learning machines 
master thesis technische universitat munchen 
suykens vandewalle 
multiclass squares support vector machines 
ijcnn international joint conference neural networks washington dc 
tax duin 
data domain description support vector 
esann pages 
vanderbei 
loqo interior point quadratic programming 
technical report university 
vapnik 
nature statistical learning theory 
springer 
vapnik chapelle 
bounds error expectation svm 
smola bartlett scholkopf schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 
wahba lin zhang 
support vector machines 
smola bartlett scholkopf schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 
williams 
support vector machines course notes university edinburgh uk 
