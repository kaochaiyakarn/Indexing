active messages mechanism integrated communication computation thorsten von eicken david culler seth copen goldstein klaus erik schauser culler cs berkeley edu report 
ucb csd march computer science division eecs university california berkeley ca design challenge large scale multiprocessors minimize communication overhead allow communication overlap computation coordinate sacrificing processor cost performance 
show existing message passing multiprocessors unnecessarily high communication costs 
research prototypes message driven machines demonstrate low communication overhead poor processor cost performance 
introduce simple communication mechanism active messages show intrinsic architectures allows cost effective hardware offers tremendous flexibility 
implementations ncube cm described evaluated split phase shared memory extension split 
show active messages sufficient implement dynamically scheduled languages message driven machines designed 
mechanism latency tolerance programming compiling concern 
hardware support active messages desirable outline range enhancements mainstream processors 
lack consensus programming styles usage patterns large parallel machines hardware designers tended optimize specific dimensions general balance 
commercial multiprocessors invariably focus raw processor performance network performance secondary role interplay processor network largely neglected 
research projects address specific issues tolerating latency dataflow architectures reducing latency cache coherent architectures accepting significant hardware complexity modest processor performance prototype solutions 
draws arenas demonstrate utility exotic processors simple mechanism mechanism implemented efficiently conventional message passing machines 
basic idea control information head message address user level instruction sequence extract message network integrate going computation 
call active messages 
surprisingly commercial machines mechanism order magnitude efficient message passing primitives drove original hardware designs 
considerable room improvement direct hardware support addressed evolutionary manner 
smoothly integrating communication computation overhead communication greatly reduced overlap report appeared proceedings th international symposium computer architecture acm press may gold coast australia 
copyright acm press 
easily achieved 
paradigm hardware designer meaningfully address balance required processor network performance 
algorithmic communication model common cost model algorithm design large scale multiprocessors assumes program alternates computation communication phases communication requires time linear size message plus start cost 
time run program nc ts start cost tb time byte lc message length nc number communications 
achieve peak processor performance programmer tailor algorithm achieve sufficiently high ratio computation communication high performance network required minimize communication time sits idle 
communication computation overlapped situation different 
time run program max 
achieve high processor efficiency communication compute times need balance compute time need swamp communication overhead 
examining average time communication phases nc time message transmission easily compute processor bandwidth network required sustain level processor utilization 
hardware designed reflect balance 
essential properties communication mechanism start cost low facilitate overlap ordination communication going computation 
active messages active messages asynchronous communication mechanism intended expose full hardware flexibility performance modern interconnection networks 
underlying idea simple message contains head address user level handler executed message arrival message body argument 
role handler get message network computation ongoing processing node 
handler execute quickly completion 
discussed corresponds closely hardware capabilities message passing multiprocessors privileged interrupt handler executed message arrival represents useful restriction message driven processors 
active messages network viewed pipeline operating rate determined communication overhead latency related message length network depth 
sender launches message network continues computing receiver notified interrupted message arrival runs handler 
keep pipeline full multiple communication operations initiated node computation proceeds messages travel network 
keep communication overhead minimum active messages buffered required network transport 
traditional pipeline sender blocks message injected network handler executes immediately arrival 
tolerating communication latency raised fundamental architectural issue quite correct 
real architectural issue provide ability overlap communication computation turn requires low overhead asynchronous communication 
tolerating latency programming problem communication initiated sufficiently advance result 
sections show programming models programmer compiler respectively control communication pipelining 
active messages new parallel programming paradigm par send receive sharedmemory primitive communication mechanism implement paradigms simply efficiently 
concentrating hardware design efforts implementing fast active messages versatile supporting single paradigm special hardware 
contents concentrate message multiprocessors consider machines similar base technology representing architectural extremes processor network integration 
message passing machines including ncube ipsc ipsc treat network essentially fast device 
message driven architectures including monsoon machine integrate network deeply processor 
message reception part basic instruction scheduling mechanism message send supported directly execution unit 
section examines current message passing machines detail 
show send receive programming models inefficient underlying hardware capabilities 
raw hardware supports simple form active messages 
utility form communication demonstrated terms fast powerful asynchronous communication paradigm 
section examines current message driven architectures 
show power message driven processing active messages costly implement required support implicitly parallel programming languages architectures designed 
section surveys range hardware support devoted accelerating active messages 
message passing architectures section examine message passing machines architecture constructed scale high performance processors 
ncube cm primary examples 
ncube nodes interconnected binary hypercube network 
node consists cpu chip dram chips small double sided printed circuit board 
cpu chip contains bit integer unit ieee floating point unit dram memory interface network interface dma channels routers support cut routing dimensional hypercube 
processor runs mhz delivers roughly mips mflops 
cm nodes interconnected hypertree incomplete fat tree 
node consists mhz sparc risc processor chip set including fpu mmu cache local dram memory network interface hypertree broadcast scan prefix control networks 
node augmented vector units 
evaluate machines traditional programming models 
show active messages suited machines support powerful programming models overhead 
traditional programming models traditional programming model message passing architectures processes communicate matching send request processor receive request 
synchronous crystalline form send receive blocking send blocks corresponding receive executed data transferred 
main advantage blocking send receive model simplicity 
data transferred source destination addresses known buffering required source destination processors 
blocking send receive communication exacerbates effects network latency communication latency order match send receive phase protocol shown required sender transmits request receiver returns executing matching receive operation data transferred 
blocking send receive impossible overlap communication computation network bandwidth fully utilized 
node node compute send compute request send ready receive data compute receive compute phase protocol synchronous send receive 
note communication latency best network trips send receive block network round trip 
avoid phase protocol allow overlap communication computation message passing implementations offer non blocking operation send appears instantaneous user program 
message layer buffers message network port available message transmitted recipient buffered matching receive executed 
shown ring communication example data exchanged computing executing sends computation phase receives 
table shows performance send receive current machines 
start costs order instruction times 
due primarily buffer management 
cm blocking uses phase protocol 
ipsc long messages phase protocol ensure buffer space available receiving processor 
start costs prevent overlap communication computation large messages 
example ncube time second send executed bytes message reached destination 
network bandwidth machines limited difficult utilize fully requires multiple simultaneous messages processor 
call communication latency time initiating send user program processor receiving message user program processor sum software overhead network interface overhead network latency 
node send right send left compute recv left recv right node send right send left compute recv left recv right communication steps required neighboring processors ring exchange data asynchronous send receive 
data exchanged computing executing sends computation phase receives 
note buffer space entire volume communication allocated duration computation phase 
machine ts tb tfp mesg byte flop ipsc ncube ipsc ncube ipsc cm messages bytes blocking send receive table asynchronous send receive overheads existing message passing machines 
ts message start cost described section tb byte cost tfp average cost floating point operation point 
active messages hardware costs message passing machines reasonable effectiveness machine low traditional send receive models due poor overlap communication computation due high communication overhead 
shortcomings attributed base hardware example initiating transmission ncube takes instructions set dma discrepancy raw hardware message initiation cost observed cost explained mismatch programming model hardware functionality 
send receive native hardware hardware allows processor send message cause interrupt occur arrival 
words hardware model really launching messages network causing handler executed asynchronously arrival 
similarity hardware operation programming model respect memory address spaces source address determined sender destination address determined receiver active messages simply generalize hardware functionality allowing sender specify address handler invoked message arrival 
note relies uniform code image nodes commonly spmd programming model 
handler specified user level address traditional protection models apply 
active messages differ general remote procedure call rpc mechanisms role active message handler perform computation data extract data network integrate ongoing computation small amount 
concurrent communication computation fundamental message layer 
active messages buffered required network transport 
primitive scheduling provided handlers interrupt computation immediately message arrival execute completion 
key optimization active messages compared send receive elimination buffering 
eliminating buffering receiving possible storage arriving data pre allocated user program message holds simple request handler immediately reply 
buffering sending side required large messages typical high overhead communication models 
low overhead active message small messages attractive eases program development reduces network congestion 
small messages buffering network typically sufficient 
deadlock avoidance tricky issue design active messages 
modern network designs typically deadlock free provided nodes continuously accept incoming messages 
translates requirement message handlers allowed block particular reply handler busy wait outgoing channel backed 
active messages ncube simplicity active messages closeness hardware functionality translate fast execution 
ncube possible send message containing word data instructions receiving message requires instructions includes interrupt message arrival dispatching user level 
near order magnitude reduction send overhead greater achieved hardware generation 
table breaks instruction counts various tasks performed 
active message implementation reduces buffer management minimum required actual data transport 
ncube dma associated network channel memory buffer ncube hypercube channels independent input output dmas base address count register 
sending receiving message requires loading address count 
shared memory multiprocessor advocates argue major cause programming difficulty machines 
instruction count task send receive compose consume message trap kernel protection buffer management address translation hardware set scheduling crawl user level total table breakdown tasks instructions required send receive message word data ncube 
message composition consumption include overhead function call register saves handler 
protection checks destination node limits message length 
hardware set includes output channel dispatch channel ready check 
scheduling accounts ensuring handler atomicity dispatch 
crawling user level requires setting stack frame saving state simulate return interrupt user level 
channel required 
additionally convenient associate buffers user process compose outgoing message handlers consume arrived message compose eventual replies 
set reduces buffer management swapping pointers channel buffer user buffer 
additional buffers exceptional cases prevent deadlock reply handler blocks long buffered retried incoming messages dispatched 
reply buffering performed message layer reply returns error code user code perform buffering retry 
typically reply original request saved stack handlers incoming messages nested current handler 
breakdown instructions table shows sources communication costs ncube 
large fraction instructions simulate user level interrupt handling 
hardware set substantial due output channel selection channel ready checks 
minimal scheduling buffer management active messages significant 
note instruction counts ncube slightly misleading system call return instructions dma instructions far expensive average 
instruction breakdown shows clearly active messages close absolute minimal message layer crawl active message specific potentially replaced 
observation tasks performed software done easily hardware 
hardware support active messages significantly reduce overhead small investment chip complexity 
active messages cm active messages implementation cm differs ncube implementation reasons actual network interface somewhat complicated described aspects relevant discussion 

cm provides user level access network interface node kernel time shares network correctly multiple user processes 

network interface supports transfer packets bytes including bytes destination node network routing guarantee packet ordering 

cm identical disjoint networks 
deadlock issues described simply solved network requests replies 
way communication 

network interface dma 
contains memory mapped fifos network outgoing messages incoming ones 
status bits indicate incoming fifos hold messages previous outgoing message successfully sent network interface 
network interface discards outgoing messages network backed process time sliced message composition 
cases send retried 

network interface generally interrupts current version due prohibitive cost 
hardware kernel support interrupts usefulness limited due cost 
comparison ncube interrupt costs system call user level access network interface 
sending packet sized active message amounts outgoing fifo message having function pointer head 
receiving active message requires polling followed loading packet data argument registers calling handler function 
network interface status checked message sent check send ok status bit servicing incoming messages send time costs extra cycles 
experience indicates program need poll explicitly enters long computation loop 
sending multi packet messages complicated potential reordering packets network 
large messages set required receiving 
involves phase protocol get phase protocol put discussed 
intermediate sized messages protocol packet holds header information expense payload arrival order irrelevant 
performance active message cm encouraging sending single packet active message function address bytes arguments takes cycles receiver dispatch costs largest fraction time spent accessing network interface memory bus 
prototype implementation blocking send receive top active messages compares favorably fully optimized vendor library start cost vs byte cost identical 
note due phase protocol required send receive order magnitude larger single packet send cost 
different programming models split cost communication brought active message packet cost 
split experimental programming model active messages demonstrate utility active messages developed simple programming model provides split phase remote memory operations programming language 
split phase operations provided put get shown put copies local memory block remote memory address specified sender 
get retrieves block remote memory address specified sender local copy 
operations non blocking require explicit coordination remote processor handler executed asynchronously 
common versions put get increment separately specified flag processor receives data 
allows simple synchronization checking flag busy waiting 
operating blocks memory yield large messages critical performance current hardware seen 
node node put get put message get request remote node remote node put handler remote addr data length flag addr data 
get handler local addr data length flag addr local node remote addr split put get perform split phase copies memory blocks remote nodes 
shown message formats 
implementations put get consist parts message formatter message handler 
shows message formats 
put messages contain instruction address put handler destination address data length completion flag address data 
put handler simply reads address length copies data increments flag 
get requests contain information necessary get handler reply appropriate put message 
note possible provide versions put get copy data blocks stride form gather scatter demonstrate simplicity performance split shows matrix multiply example achieves peak performance large ncube configurations 
example matrices partitioned blocks columns processors 
multiplication processor gets column performs rank update daxpy corresponding elements columns columns balance communication pattern processor computes column proceeds getting columns processor 
note algorithm independent network topology familiar shared memory style 
remote memory access completion explicit 
key obtaining high performance overlap communication computation 
achieved getting column iteration computing current column 
necessary balance latency get time taken computation inner loops 
quantifying computational cost relatively easy get number multiply adds executed nm number local columns multiply add takes help understand latency get shows diagram operations delays involved unloaded case 
split exposes underlying rpc mechanism programmer specialized communication structures constructed enqueue record 
matrices partitioned blocks columns processors 
multiplication processor gets column performs rank update daxpy columns columns balance communication pattern processor computes column proceeds getting columns processor 
network topology independent algorithm achieves peak performance large ncube configurations 
int matrix dimensions see double matrices int indices int dj nj initial delta dj int number processors processor int rp double buffers getting remote columns double nv tv current column column temp column static int flag synchronization flag extern void get int proc void src int size void dst int flag rp starting column get sizeof double nv flag get column dj dj dj loop columns dj nj dj column index check flag wait previous get tv nv nv tv swap current column nj done get column get nj rp nj rp sizeof double nv flag accum 
col scale unroll shown matrix multiply example split 
top curves show performance predicted model measured node ncube respectively number columns processor varied 
kept constant adjusted keep total number arithmetic operations constant 
matrix multiply example computation bound processor holds columns 
bottom curves show predicted measured network utilization 
discrepancy model measurement due fact network contention modeled 
note computational performance low small values joint processor network utilization relatively constant entire range 
program changes communication computation problem performance stable 
utilization predicted processor utilization measured processor utilization measured joint utilization predicted network utilization measured network utilization performance split matrix multiply processors compared predicted performance model shown 
observations existing message passing machines criticized high communication overhead inability support global memory access 
active messages shown hardware capable delivering close order magnitude improvement today right communication mechanism global address space may implemented software 
split example active messages incorporated coarse grain spmd single program programming language 
generalizes shared memory read write providing access blocks memory including simple synchronization 
address naming issues 
active messages guide design possible improve current message passing machines evolutionary revolutionary fashion 
section examine research efforts build hardware uses different approach provide magnitude performance improvement 
get cost model proc compose send receive handle net xmit hops xmit hops receive service reply byte hop overlapping communication computation ns byte byte overlapped computation hop proc compose send compute receive service reply compute receive handle net xmit hops xmit hops compose send compute receive service reply compute receive handle ns byte performance model get 
compose accounts time set request 
xmit time inject message network hops time taken network hops 
service includes copying data reply buffer handle time copy data destination memory block 
message driven architectures message driven architectures machine monsoon expend significant amount hardware integrate communication processor 
communication performance achieved machines impressive processing performance 
glance come fact processor design intimately affected network design prototypes existence utilize traditional processor design know 
truth problem deeper message driven processors context lasts duration message handler 
lack locality prevents processor large register sets 
section argue hardware support communication partly counter productive 
simpler traditional processors built unduly compromising communication processing performance 
intended programming model main driving force message driven architectures support languages dynamic parallelism id multilisp cst 
computation driven messages contain name handler data 
message arrival storage message allocated scheduling queue 
message reaches head queue handler executed data arguments 
handler may perform arbitrary computation particular may synchronize suspend 
ability suspend requires general allocation scheduling message arrival key difference respect active messages 
case machine programming model put forward object oriented language terms handler method data holds arguments method usually names object method operate 
functional language view message closure code pointer arguments closure 
monsoon usually described dataflow perspective messages carry tokens formed instruction pointer frame pointer piece data 
data value operands specified instruction referenced relative frame pointer 
fundamental difference message driven model active messages computation proper performed computation occurs message handlers background handlers remove messages network transport buffers integrate computation 
difference significantly affects nature allocation scheduling performed message arrival 
handler message driven model may suspend waiting event lifetime storage allocated scheduling queue messages varies considerably 
general released simple fifo lifo order 
size scheduling queue depend rate messages arrive handlers executed amount excess parallelism program 
excess parallelism grow arbitrarily conventional call stack impractical set aside fraction memory message queue able grow size available memory 
active message handlers hand execute immediately message arrival suspend responsibility terminate quickly back network 
role handler get message network transport buffers 
happens integrating message data structures ongoing computation case remote service requests immediately replying requester 
memory allocation message arrival occurs far required network transport dma involved scheduling restricted interruption ongoing computation handlers 
equivalently handlers run parallel computation separate dedicated hardware 
hardware description monsoon machine hardware designed support message driven model directly 
machine mesh processing nodes single chip cpu dram 
cpu bit integer unit closely integrated network unit small static memory dram interface floating point unit 
hardware manages scheduling queue fixed size ring buffer chip memory 
arriving messages transferred queue serviced fifo order 
word message interpreted instruction pointer message available handler addressable data segments 
machine supports levels message priorities hardware independent queues maintained 
message handler terminates executing suspend instruction causes message scheduled 
monsoon messages arrive token queue 
token queue kept separate memory proportional size frame store 
provides storage roughly tokens frame average queuing policy allows fifo lifo scheduling 
alu pipeline way interleaved handlers active simultaneously 
soon handler terminates suspends blocking token queue store tokens words frame store expected average frame size words 
synchronization event token popped queue new handler starts executing pipeline interleave 
common characteristic machines amount state available executing handler small data address registers machine accumulator temporary registers monsoon 
reflects fact computation initiated single message small typically arithmetic operations 
small amount utilize registers locality preserved handler useful values carried 
interesting note machine hardware support message driven programming model fully hardware message queue managed fifo order fixed size 
handler run completion message copied allocated region non buffer memory software 
happens roughly messages 
machine hardware support active messages case message queue serves buffering 
close ofthe messages hold request handler immediately replies general allocation scheduling required 
monsoon fact tokens popped queue means storage allocated arriving message deallocated message handler execution 
handler suspends relevant data saved pre allocated storage activation frame machine monsoon implement message driven model cost large amount high speed memory 
tam compiling active messages far argued message driven execution model tricky implement correctly hardware due fact general memory allocation scheduling required message arrival 
hardware implements active messages easy simulate message driven model performing allocation scheduling message handler 
contrary expectation necessarily result lower performance direct hardware implementation software handlers exploit optimize special cases 
tam threaded machine fine grain parallel execution model active messages goes step requires compiler help manage memory allocation scheduling 
currently compilation target implicitly parallel languages id 
compiling tam compiler produces sequences instructions called threads performing computation proper 
generates handlers called messages received computation 
receive arguments function results called child functions responses global memory accesses 
accesses global data structures split phase allowing computation proceed requests travel network 
function call activation frame allocated 
inlet receives message typically stores data frame schedules thread activation 
scheduling handled efficiently maintaining scheduling queue activation frame frame addition holding local variables contains counters synchronizing threads provides space continuation vector addresses currently enabled threads activation 
enabling thread simply consists pushing instruction address continuation vector possibly linking frame ready queue 
shows activation tree data structure 
service requests remote reads typically replied immediately need memory allocation scheduling active messages provides 
exceptional cases requests delayed lack resources servicing inside handler inadequate 
amortize memory allocation requests fixed size queue space allocated chunks 
maintaining thread addresses frames provides natural level scheduling hierarchy 
frame scheduled activated enabled threads executed continuation vector empty 
ready queue activation tree activation frame local variables synchronization counters ready frame link continuation vector code segment function foo inlet thread thread thread tam activation tree embedded scheduling queue 
function call activation frame allocated 
frame addition holding local variables contains counters synchronize threads provides space continuation vector addresses currently enabled threads activation 
processor frames holding enabled threads linked ready queue 
maintaining scheduling queue activation keeps costs low enabling thread simply consists pushing instruction address continuation vector linking frame ready queue 
scheduling thread activation simply pop jump 
message received types behavior observed message currently active frame inlet simply feeds data computation message dormant frame case frame may get added ready queue ongoing computation 
tam scheduling hierarchy compiler improve locality computation synchronizing message handlers enabling computation group messages arrived example prerequisite remote fetches inner loop body completed 
follows realization arrival message enables small amount computation arrival closely related messages enable significant amount computation 
cases power compile time analysis run time scheduling policy dynamically enhances locality servicing frame continuation vector empty 
result tam compilation model typically memory allocation required message arrival 
dynamic memory allocation performed large chunks activation frames global arrays records 
locality computation enhanced tam scheduling hierarchy 
possible implement tam scheduling hardware support uniprocessor cost dynamic scheduling amounts doubling number control flow instructions relative languages performance depends critically cost active messages 
table summarizes frequency various kinds messages current implementation 
average id requires dynamic scheduling uniprocessors 
message sent received tam instructions equivalent roughly risc instructions 
note statistics sensitive optimizations 
example significant changes expected software cache remote arrays 
message types data words frequency frame frame store request fetch request fetch reply table frequency various message types sizes represented number data values transmitted current implementation tam 
average message sent received tam instructions 
statistics sensitive compiler optimizations sense represent worst case scenario 
hardware support active messages active messages provide precise simple communication mechanism independent programming model 
evaluating new hardware features restricted evaluating impact active messages 
parameters feeding design size frequency messages depend expected workload programming models 
hardware support active messages falls categories improvements network interfaces modifications processor facilitate execution message handlers 
subsections examine parts design space points view 
network interface design issues improvements network interface significantly reduce overhead composing message 
message reception benefits improvements requires initiation handler 
large messages support needed large messages superset small messages 
overlap computation large message communication form dma transfer 
set dma receiving side large messages header received 
small messages supported large message viewed small dma transfer 
message registers composing small messages memory buffers inefficient information small message related current processor state 
comes instruction stream processor registers memory 
receiving message header typically moved processor registers dispatch address data 
direct communication processor network interface save instructions bus transfers 
addition managing memory buffers expensive 
machine demonstrates extreme alternative message composition single send instruction contents processor registers appended message 
message reception tied memory buffers albeit chip 
radical approach compose messages registers network coprocessor 
reception handled similarly received message appears set registers 
coprocessor receive instruction enables reception message 
case coprocessor design complex network interface accessed memory mapped device case cm 
reuse message data providing large register set network interface opposed network fifo registers allows message composed portions messages 
example destination reply extracted request message 
multiple requests sent identical return addresses 
keeping additional context information current frame pointer code base pointer network interface accelerate formatting requests 
nic network interface contains input output registers set consume messages 
output registers retain value message sent consecutive messages identical parts sent cheaply 
data moved input output registers help re data replying forwarding messages 
single network port multiple network channels connected node visible message layer 
ncube example message sent correct hypercube link message layer routing network automatic 
network interface allow messages composed simultaneously message composition atomic 
replies message handlers may interfere normal message composition 
protection user level access network interface requires protection mechanisms enforced hardware 
typically includes checking destination node destination process applicable message length 
checks simple range check sufficient 
reception message head handler address possibly process id checked normal memory management system 
frequent message accelerators designed network interface allows frequent message types issued quickly 
example proposal issuing global memory fetch takes single store double instruction network interface memory mapped 
bit data value interpreted global address translated network interface node local address pair 
return address current frame pointer cached network interface handler address calculated low order bits store address 
processor support message handlers asynchronous message handler initiation design issue addressed purely network interface processor modifications needed 
way signal asynchronous event current microprocessors take interrupt 
flushes pipeline enters kernel 
overhead executing user level handler includes crawl handler trap back kernel return interrupted computation super scalar designs tend increase cost interrupts 
fast polling frequent asynchronous events avoided relying software poll messages 
execution models tam message frequency high polling instructions may possible user level handler return directly computation 
inserted automatically compiler part thread generation 
supported little change processor 
example sparc mips message ready signal attached coprocessor condition code input polled branch coprocessor condition instruction 
user level interrupts user level traps proposed handle exceptions dynamically typed programming languages floating point computations 
active messages user level interrupts need occur instructions 
incoming message may currently running user process network interface interrupt kernel case 
pc injection minimal form multithreading switch main computational thread handler thread 
threads share processor resources program counter pc 
normally instructions fetched computation pc 
message arrival instruction fetch switches handler pc 
handler suspends instruction switches instruction fetch back computation pc 
implementation pcs fact symmetrical 
switching pcs performed pipeline bubbles fetching instruction costs cycle 
note approach format message partially known network interface extract handler pc message 
dual processors multiplexing processor computation threads handlers execute concurrently processors tailored computation simple message handlers may floating point 
crucial design aspect communication handled processors 
communication consists data received network written memory activation frames scheduling queue 
dual processor design proposed mit project 
uses mc computation custom message processor 
design processors separate die communicate snooping bus 
processors integrated single die share data cache communication simpler 
appealing aspect design normal uniprocessors quite successfully 
coarse grain models split important overlap computation transmission messages network 
efficient network interface allows high processor utilization smaller data sets 
extreme implicitly parallel language models provide word time access globally shared objects extremely demanding network interface 
modest hardware support cost handling simple message reduce handful instructions 
remote infrequent amount resources consumed message handling significant 
dual processors larger number multiplexed processors superior depends variety engineering issues involves exotic architecture 
resources invested message handling serve maintain efficiency background computation 
related similar character development optimized rpc mechanisms operating system research community 
attempt reduce communication layer functionality minimum required carefully analyze optimize frequent case 
time scales operating system involvement radically different arenas 
rpc mechanisms distributed systems operate time scales microseconds milliseconds operating system involvement communication operation taken granted 
optimizations reduce os overhead moving data user system spaces marshaling complex rpc parameters context switches enforcing security 
furthermore connecting applications system services major operating system rpcs communication partners protected 
contrast time scale communication parallel machines measured tens processor clock cycles elimination os intervention central issue 
security concern communication partners form single program 
difference distributed systems arena communication paradigm rpc stable propose new mechanism parallel processing show primitive subsumes existing mechanisms 
integrated communication computation low cost key challenge designing basic building block large scale multiprocessors 
existing message passing machines devote hardware resources processing little communication bringing 
result significant fraction processor lost layers operating system software required support message transmission 
message driven machines devote hardware resources message transmission reception scheduling 
dynamic allocation required message arrival precludes simpler network interfaces 
message message scheduling inherent model results short computation run lengths limiting processing power utilized 
fundamental issues designing balanced machine providing ability overlap communication computation reduce communication overhead 
active message model minimizes software overhead message passing machines utilizes full capability hardware 
model captures essential functionality message driven machines simpler hardware mechanisms 
active message model node ongoing computational task punctuated asynchronous message arrival 
message handler specified message serves extract message data integrate computation 
efficiency model due elimination buffering network transport requirements simple scheduling non message handlers arbitrary overlap computation communication 
drawing distinction message handlers primary computation large grains computation enabled arrival multiple messages 
active messages sufficient support wide range programming models permit variety implementation tradeoffs 
best implementation strategy particular programming model depends usage patterns typical model message frequency message size computation grain 
research required characterize patterns emerging parallel languages compilation paradigms 
optimal hardware support active messages open question clear matter tradeoffs architectural revolution 
friends mit id monsoon machine stimulated investigation 
ncube time gratefully provided sandia national laboratories ncube ncube provided access vertex kernel sources mini ncube configuration reboot leisure 
guidance encouragement hard working folks thinking machines instrumental development active messages cm 
supported national science foundation pyi award ccr matching funds motorola trw foundation 
thorsten von eicken supported semiconductor research dc 
seth copen goldstein supported sandia national laboratories contract 
klaus erik schauser supported ibm graduate fellowship 
cm computational resources provided nsf infrastructure cda 
arvind 
fundamental issues multiprocessing 
proc 
conf 
par 
proc 
science eng bonn bad germany june 
bershad anderson lazowska levy 
lightweight remote procedure call 
acm trans 
computer systems february 
culler sah schauser von eicken wawrzynek 
fine grain parallelism minimal hardware support compiler controlled threaded machine 
proc 
th int 
conf 
architectural support programming languages operating systems santa clara ca april 
available technical report ucb csd cs div university california berkeley 
culler arvind 
resource requirements dataflow programs 
proc 
th ann 
int 
symp 
comp 
arch pages hawaii may 
dally architecture message driven processor 
proc 
th annual int 
symp 
comp 
arch pages june 
dally machine fine grain concurrent computer 
ifip congress 
dongarra 
performance various computers standard linear equations software 
technical report cs computer science dept univ tennessee knoxville tn december 

performance second generation hypercube 
technical report ornl tm oak ridge nat lab november 
fox 
programming concurrent processors 
addison wesley 
halstead jr multilisp language concurrent symbolic computation 
acm transactions programming languages systems october 
dana henry christopher joerg 
network interface chip 
technical report csg memo mit lab comp 
sci tech 
square cambridge ma june 
chien dally 
experience cst programming implementation 
proc 
acm sigplan conference programming language design implementation 
intel 
personal communication 
johnson 
trap architectures lisp systems 
proc 
acm conf 
lisp functional programming june 
nikhil 
parallel programming language id compilation parallel machines 
proc 
workshop massive parallelism italy october 
academic press 
csg memo mit laboratory computer science technology square cambridge ma usa 
nikhil papadopoulos arvind 
killer micro brave new world 
technical report csg memo mit lab comp 
sci tech 
square cambridge ma january 
papadopoulos 
implementation general purpose dataflow multiprocessor 
technical report tr mit lab comp 
sci tech 
square cambridge ma september 
phd thesis dept eecs mit 
papadopoulos culler 
monsoon explicit token store architecture 
proc 
th annual int 
symp 
comp 
arch seattle washington may 
thekkath levy 
limits low latency rpc 
technical report tr dept computer science engineering university washington seattle wa 

