prioritized token mutual exclusion distributed systems frank mueller humboldt universitat zu berlin institut fur informatik berlin germany mail mueller informatik hu berlin de phone fax number solutions proposed problem mutual exclusion distributed systems 
approaches extended prioritized environment suitable real time applications impose higher message passing overhead approach 
new protocol prioritized mutual exclusion distributed environment 
approach uses token model working logical tree structure dynamically modified 
addition utilize set local queues union resemble single global queue 
furthermore algorithm designed order message delivery handles messages asynchronously supports multiple requests node multi threaded nodes 
prioritized algorithm average overhead log messages request mutual exclusion worst case overhead represents number nodes system 
prioritized algorithm matches message complexity best non prioritized algorithms previous prioritized algorithms higher message complexity knowledge 
concept local queues incorporated arbitrary token protocols priority support reduce amount messages 
performance results indicate additional functionality algorithm comes cost longer response times test environment distributed execution compared algorithm 
result suggests algorithm strict priority ordering required 
common resources distributed environment may require mutual exclusion 
problem similar mutual exclusion shared memory environment 
shared memory architectures generally provide atomic instructions test set exploited provide mutual exclusion provisions exist distributed environment 
furthermore commonly known mutual exclusion algorithms shared memory environments rely hardware support require access shared variables 
distributed environments mutual exclusion provided series messages passed nodes interested certain resource 
algorithms solve mutual exclusion distributed systems developed 
distinguished approaches token non token 
ones may broadcast protocols may logical structures point point communication 
introduce new algorithm provide mutual exclusion distributed environment supports priority queuing 
algorithm uses token passing approach point point communication logical structure 
node keeps local queue records time requests locally 
queues form virtual global queue ordered priority fifo priority level regard property relative fairness 
request request token 
example global logical clock timestamps avoid associated message passing overhead 
main purpose algorithm provide efficient solution problem mutual exclusion distributed systems known average worst case behavior support strict priority scheduling real time systems time 
worst case behavior algorithm determined known bounds message delays 
aiming enhancing distributed programming environments priority support posix manner dsm threads environment 
resulting algorithm works asynchronously order message systems assumed ordered message delivery previous :10.1.1.23.9868
algorithm derived non prioritized algorithm uses single distributed queue 
explain algorithm easily extended handle priorities 
develop algorithm step step introducing priority support local queues token forwarding 
performance results 
shows correctness algorithm 
model assume fully connected network complete graph 
network topologies fully connected protocol pay additional overhead messages relayed intermediate nodes 
second assume reliable message passing loss duplications modifications messages allow order message delivery respect pair nodes messages sent node node may arrive different order sent assumption local operations orders magnitude faster message delivery case today networks gap processor speed network speed widen 
main concern reduce amount messages expense local data structures local operations 
property relative fairness priority request discussed assume bound message delays deterministic latency throughput characteristics communication medium 
algorithm basic idea token passing protocol distributed mutual exclusion stems algorithm 
decentralized algorithm nodes form logical tree pointing probable owners root 
consider 
root holds token mutual exclusion 
request mutual exclusion travels probable owners solid arcs request request request request 
example token requests token holder 
example request send node forwards request chain probable owners sets probable owner request arrives current token holder probable owner pointer set requester sets pointer dotted arc probable owner request sent node forwards request probable owners sets probable owner node sets pointer probable owner token holder returns critical section token sent node points 
example passes token deletes pointer 
consider process request forwarding 
request passes intermediate nodes probable owner set requesting node 
causes transformation tree forest 
traversed nodes form tree rooted requesting node token holder 
nodes traversed part original tree rooted current token holder 
example forwarded request sets probable owner point logical tree rooted tree rooted request reaches separate trees merged nd tree 
requests transit may separate trees forest 
requests arrived current token holders forest collapses single tree rooted requester 
pointers form distributed queue pending requests current token holder requester 
new request simply appended described 
algorithm average message overhead log requests propagated tree 
worst case requests forwarded gamma messages additional message needed send token 
model assumes requester node time requests ordered fifo priority support 
multi threaded environment multiple requests may issued node 
algorithm easily extended support list pointers node pointer represents request different thread 
priority queuing supported easily discussed section 
priority queuing support prioritized environment requests ordered priority priority level fifo 
basic idea priority support accumulate priority information intermediate nodes request forwarding 
request issued node priority passes node probable owner requests priority set 
higher priorities denote important requests 
example depicts sequence token requests priority respectively 
disregard local queues nodes 
token resides node requests arrive 
sake simplicity assume initial logical structure tree edges point token owner priorities labeled top 
request results new forwarding information certain priority levels depicted labeled edges 
example request results edges labeled indicates requests priority handled edges include 


follow algorithm addition priority constraints edges set pointers request 
pointer may modified higher priority requests arrive lower ones served 
indicates algorithm unsuitable support priority queuing 
essence possible insert new requests arbitrary places distributed queue due race conditions 
new entry added head queue request arrives current token holder pointer requester set pointer current token holder 
request transit nodes may registered requests utilizing pointer 
race setting pointer 
deficiency solved local queues 
local queues algorithm uses distributed queue 
replace queue pointers set local queues 
node issues request priority request propagates chain probable owners reaches node pending request priority 
locally queued basic idea node served request 
request may stored locally token arrives 
token arrives request served requests arrived 
depicted local queues associated nodes node stores request priority outstanding request priority served 
problem set distributed queues handling fifo queuing priority level 
consider sending token node entry node head local queue possibly entries lower priority 
receives token queue piggybacked 
node merge local queue queue 
entries priority different queues entry served 
distributed system fifo policies enforced global logical clock timestamps 
timestamps result additional message passing overhead communication processes request certain lock overhead acknowledgments 
utilize local time facilities enforce fifo ordering 
notice local time meaning current node 
token transmitted queue piggybacked request queue request node requests log local initiation time able measure time request initiation ti req token reception local node 
interval called request latency ti latency see 
latency composed request transit time ti req trans token transit time ti token trans token usage time ti usage 
ti latency ti req trans ti token trans ti usage index may replaced indices nodes 
notice queue piggybacked token message contains local requests accumulated token usage time 
token usage time calculated time spent critical sections nodes holding token request reception 
latency req trans token trans usage lock send req critical sections fwd token req 
events times token requests usage measured transit times may inferred indirectly due lack global clocks 
determine approximate usage times nodes local queue due observation 
local queue contains requests current node possibly nodes associated reception time tx receipt request local node 
usage time local requests determined relative request head piggybacked queue tx new usage tx usage gamma tx receipt ti usage ti latency usage time current node request estimated portion time receiving request relative latency 
consequently local usage time smaller accumulated usage time consistent received issued 
usage times determined local requests local queue merged piggybacked queue contains accumulated usage times 
requests ordered descending priority priority level descending usage time 
usage time provides means aging guarantees requests eventually served 
avoids starvation requests discussed detail 
reception time requests set current local time merging 
token may access mutual exclusion local node 
token usage requests may received logged zero usage time current local time reception time 
requests merged existing queue precedence rules descending priority descending usage time priority addition descending reception time priority level usage times 
token forwarding token longer local node may forwarded requester queue 
queue piggybacked forwarded token usage times recalculated forwarding include local usage time tx new usage tx usage gamma tx receipt example continued 
token forwarded nodes previous token holder retains link token holder requests priority level 
notice depict usage times adjustment token forwarding 
request propagation logical tree request node propagates intermediate nodes reaching token holder depicted 
token forwarding may result race condition characterized follows 
request node may pass intermediate node lower priority request issued 
token sent node request transit passed node race request token location exists 
example consider request issued passed forwarded token forwarded new request trail node request passed forming cycle 
returning characterization race condition communication overhead may longer bound number nodes system request may pass repeatedly node avoid situation adding request forward token forward token 
example token forwarding local queue intermediate nodes token passed request reaches passing node forward token higher priority requester depicted 
request logs intermediate nodes way stops propagating edge probable owner path leads logged node 
request stops propagating probable owner sent token request passed addition kill list entries added local queues intermediate nodes passed token requester node ensures request mutual exclusion granted kill list received 
added request intermediate nodes processed token reaches intermediate node time 
subcases 
token passes intermediate node reaching example node 
request merged pending requests locally recorded request served 
case token passes intermediate node reaching kill list indicates entries removed local queues intermediates nodes 
list received holds token indicated dashed edge 
token reaches node kill list entries local queue removed kill list indicates 
submission kill list necessary result request stops propagating higher equally high priority request pending intermediate node path token probable owner request queued locally intermediate nodes way including node addition kill list sent includes intermediate nodes request 
token may received immediately forwarded node way receives token request may issued 
case token forwarded node possibly reaching kill list guarantees entries request eventually deleted local queues intermediate nodes regardless location node message overhead analyze number messages necessary register single request mutual exclusion 
environment nodes algorithm requires average log messages requests algorithm uses logical tree similar algorithm 
el 

fact prioritized algorithm reduces improved version algorithm priority level utilized 
worst case links nodes form chain gamma messages required propagation request 
obvious section ensured token requests propagate request reaches token 
send token request 
send kill list 
send token request trails token 
model token forwarding cycles words pass node twice 
messages required send token intermediate node requester send kill list see 
example token message excluded message overhead request message caused different request 
details overhead 
performance evaluation hand want investigate performance prioritized algorithm 
hand want compare performance algorithm prioritized algorithms requests issued priority 
concept local queues combined local time keeping may enhance algorithm reduce amount messages 
notice logging requests intermediate nodes described section necessary absence priorities 
experimentation environment algorithmic presentation prioritized protocol omitted due lack space may 
implemented algorithm asynchronous version algorithm extended reader writer locks described detail :10.1.1.14.3607
algorithms dsm threads environment guarantee mutual exclusion 
environment similar posix threads sense strict priority scheduling shall enforced fifo scheduling priority level 
distributed threads share physical memory 
distributed virtual shared memory dsm supported :10.1.1.14.3607
details dsm threads scope 
designed test environment lines fu measuring number messages exchanged critical section entry response time elapsed time issuing token request entry critical section 
measurements taken tasks requesting critical section entries varying intervals gamma milliseconds 
intervals randomly distributed gamma sigma milliseconds 
critical section single write operation performed simulate access shared variable 
node releases lock critical section due lack contention token forwarding measurement ignored remote overhead measured 
dsm threads environment currently runs sparc intel platforms 
chose set sparc workstations connected regular ethernet gather performance numbers 
low speed network allowed gather measurements relative comparison algorithms 
designed different experiments 
uses threads process sparc mhz message passing simulated buffers physically shared memory simulates high speed network extremely low latencies exposing overhead bookkeeping approach 
second measures performance processes dual processor sparc mhz tcp messages simulating medium speed network 
third distributes processes different workstations running processor speeds mhz tcp messages representing actual low speed network 
tcp messages may processed different order received due multi threading environment 
allows test protocol order message delivery 
overhead sending single tcp message including overhead dsm threads message layer ranges milliseconds 
tuned message passing layer 
measurements covers experiment simulated communication physically shared memory 
illustrates overhead protocol approach 
messages required mutual exclusion short critical section intervals gamma messages suffice 
short intervals requests result chains owners algorithms requests arrive faster serviced 
high response time small gamma 
response time algorithm third response time algorithm average 
simulate message passing difference response time represents additional computation overhead required protocol 
additional cost caused larger messages processed transmitting 
savings due local queues visible latency messages simulation environment extremely short 
course approach adds functionality 
covers second experiment tcp communication processes machine 
number messages smaller algorithm particular short interval request times 
behavior attributed local queues 
response time smaller algorithm overhead risen fourth algorithm due difference number messages 
requests issued frequently small gamma response time relatively high 
contention larger gamma response time smaller 
depicts third experiment tcp communication processes different workstations 
number messages protocols 
response time approach half approach 
short critical section intervals observed locks acquired locally remote request required token forwarding 
results small gamma may misleading due small sample sizes 
algorithms require messages mutual exclusion algorithm response time thirds approach 
summary approach additional functionality comes price additional processing cost longer response times 
need priority support algorithm enhanced local queues may choice 
multi threaded environment algorithm may increase exploitation potential parallelism 
strict priority scheduling requests mutual exclusion algorithm choice 
related number algorithms exist solve problem mutual exclusion distributed environment 
chang johnson give overview compare performance algorithms 
proposed priority algorithm broadcast requests token passing approach 
chang developed extensions various algorithms priority handling broadcast messages fixed logical structures token passing 
chang singhal liu dynamic tree similar 
fact difference algorithms root tree piggybacked approach older piggybacking 
due similarity simply referred older algorithm 
mutual exclusion algorithms token passing employ global logical clocks timestamps 
algorithms readily extended transmit priorities timestamps 
algorithms raymond message complexity larger log request 
raymond algorithm uses fixed logical structure dynamic allow addition deletion nodes 
furthermore raymond needs log messages send token requester algorithm requires messages priority level 
modified version raymond algorithm fu essence similar local queues just entry 
previous algorithms synchronous message passing assume single request node time algorithm works asynchronously multiple requests node provide concurrency multi threaded environment 
section request interval msec section request interval msec 
simulated comm physically shared memory section request interval msec section request interval msec 
tcp comm smp processors section request interval msec section request interval msec 
tcp comm distributed different workstations main contribution prioritized algorithm distributed mutual exclusion average message overhead log worst case overhead messages request respectively 
algorithm scheme uses local queues form global queue pending requests 
queues ordered priority fifo priority level 
give framework relate requests different local queues terms fifo ordering local time keeping global logical clock timestamps 
local queues merged preserving fifo ordering 
utilize set logical trees associated priority levels requests propagate token holder requester higher priority 
resulting algorithm prioritized mutual exclusion requires forwarding queues local bookkeeping 
works order message delivery supports asynchronous message handling multiple requests node nodes multi threaded 
algorithm message complexity best known non prioritized counterparts previous prioritized algorithms higher message complexity knowledge 
furthermore concept local queues combined local time keeping incorporated arbitrary token protocols priority support reduce amount messages 
performance results indicate additional functionality algorithm comes cost longer response times test environment distributed execution compared algorithm 
result suggests algorithm strict priority ordering required 
chang 
design mutual exclusion algorithms real time distributed systems 
journal information science 
chang 
simulation study distributed mutual exclusion 
parallel distributed computing mar 
chang singhal liu 
improved log mutual exclusion algorithm distributed processing 
parallel processing volume pages 

logical time distributed computing systems 
ieee computer aug 
fu li 
empirical evaluation distributed mutual exclusion algorithms 
international parallel processing symposium pages 

algorithms mutual exclusion real time distributed computer systems 
journal parallel distributed computing pages 
johnson 
performance comparison fast distributed mutual exclusion algorithms 
international conference parallel processing pages 
lamport 
time clocks ordering events distributed systems 
communication acm june 
li hudak :10.1.1.14.3607
memory coherence shared virtual memory systems 
acm trans 
computer syst nov 
lo 
operating systems enhancements distributed shared memory 
advances computers 
mueller 
distributed shared memory threads dsm threads 
workshop run time systems parallel programming pages apr 
mueller 
design implementation dsm threads 
int 
conference parallel distributed processing techniques applications pages june 
invited 
mueller :10.1.1.23.9868
prioritized token mutual exclusion distributed systems 
workshop parallel distributed real time systems apr 
mueller 
prioritized token mutual exclusion distributed systems 
tr inst 
informatik university berlin jan 
www informatik hu berlin de mueller 

improvement log distributed algorithm mutual exclusion 
distributed computing systems 
arnold 
log distributed mutual exclusion algorithm path reversal 
journal parallel distributed computing apr 
raymond 
tree algorithm distributed mutual exclusion 
acm trans 
computer systems feb 
singhal 
heuristically aided algorithm mutual exclusion distributed systems 
ieee transactions computers may 
suzuki kasami 
distributed mutual exclusion algorithm 
acm transactions computer systems dec 
technical committee operating systems application environments ieee 
portable operating system interface posix part system application program interface api 
ansi ieee std edition including amendment threads extension language 
