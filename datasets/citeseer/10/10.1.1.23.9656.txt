
fl kluwer academic publishers boston 
manufactured netherlands 
latent semantic kernels nello cristianini nello cs rhul ac uk department computer science royal holloway university london egham surrey tw oex uk john shawe taylor john cs rhul ac uk department computer science royal holloway university london egham surrey tw oex uk lodhi cs rhul ac uk department computer science royal holloway university london egham surrey tw oex uk 
kernel methods support vector machines successfully text categorization 
standard choice kernel function inner product vector space representation documents analogy classical information retrieval ir approaches 
latent semantic indexing lsi successfully ir purposes technique relations terms inserting similarity measure documents 
main drawbacks ir computational cost 
describe lsi approach implemented kernel defined feature space 
provide experimental results demonstrating approach significantly improve performance impair 

kernel learning methods kms state art class learning algorithms best known example support vector machines svms 
approach data items mapped high dimensional spaces information mutual positions inner products constructing classification regression clustering rules 
modular systems formed general purpose learning module classification clustering data specific element called kernel acts interface data learning machine defining mapping feature space 
kernel algorithms exploit information encoded inner product pairs data items 
somewhat surprisingly information sufficient run standard machine learning algorithms perceptron convergence algorithm principal components analysis pca ridge regression nearest neighbour 
advantage adopting alternative representation efficient method compute inner products complex cases infinite dimensional vectors 
explicit representation feature vectors corresponding data items necessary kms advantage accessing feature spaces expensive complicated represent 
strong model selection techniques statistical learning theory developed systems order avoid overfitting high dimensional spaces 
surprising areas systems naturally text categorization standard representation documents highdimensional vectors standard retrieval techniques precisely inner products vectors 
combination methods pioneered joachims successively explored 
approach documents representation known bag words mapping documents large vectors indicating words occur text 
vectors dimensions terms corpus usually thousands corresponding entries zero term occur document hand positive 
documents considered similar approximately terms 
despite high dimensionality spaces higher training set size support vector machines shown perform 
investigates possible avenue extending joachims incorporating information kernel 
information retrieval ir representation known suffer drawbacks particular fact semantic relations terms taken account 
documents talk related topics different terms mapped distant regions feature space 
map captures semantic information useful particularly achieved semantic kernel computes similarity documents considering relations different terms 
kernel takes fact consideration enable system extract information documents 
possible approach adopted semantic network explicitly compute similarity level terms 
information encoded kernel defines new metric feature space equivalently mapping documents feature space 
propose technique known information retrieval latent semantic indexing lsi 
approach documents implicitly mapped semantic space documents share terms close terms semantically related 
semantic similarity terms inferred analysis occurrence patterns terms occur documents considered related 
statistical occurrence information extracted means singular value decomposition term document matrix way described section 
show step performed implicitly kernel induced feature space amounts kernel adaptation semantic kernel learning step 
fixed dimension new feature space computation equivalent solving convex optimization problem eigenvalue decomposition just global maximum efficiently 
eigenvalue decomposition expensive large datasets develop approximation technique gram schmidt procedure 
practice method perform better lsi method 
provide experimental results text non text data showing techniques deliver significant improvements datasets certainly reduce performance 
discuss advantages limitations relationships methods 

kernel methods text kernel methods new approach solving machine learning problems 
developing algorithms inner products images different inputs feature space application possible rich feature spaces provided inner products computed 
way avoid need explicitly compute feature vector input 
key advantages approach modularity decoupling algorithm design statistical analysis problem creating appropriate function feature spaces particular application 
furthermore design kernels performed modular fashion simple rules exist combine adapt basic kernels order construct complex ones way guarantees kernel corresponds inner product feature space 
main result regarded kernel adaptation procedure 
idea kernel defined feature space new full potential begun realised 
problem considered classification labelled examples called support vector machine corresponding statistical learning analysis described 
turned development portfolio algorithms clustering principal components analysis pca feature space regression novelty detection ordinal learning 
time links statistical learning approach bayesian approach known gaussian processes classical known ridge regression time providing direct link distinct paradigms 
view developments clear defining appropriate kernel function allows range different algorithms analyse data concerned potentially answering practical prediction problems 
particular application choosing kernel corresponds implicitly choosing feature space kernel function defined hoe oe feature map oe 
training set fx information available kernel algorithms contained entirely matrix inner products known gram kernel matrix 
matrix represents sort bottleneck information exploited operating matrix fact virtually recode data suitable manner 
solutions sought linear functions feature space oe weight vector denotes transpose vector matrix 
kernel trick applied weight vector expressed linear combination training points ff oe implying express follows ff 
explicit feature map oe equation compute corresponding kernel 
methods sought provide directly value kernel explicitly computing oe 
show standard information retrieval feature spaces give rise particularly natural set kernels 
best known method type referred polynomial kernel 
kernel polynomial construction creates kernel applying polynomial positive coefficients example consider fixed values integer suppose feature space feature space indexed tuples features relatively small additional computational cost time inner product computed addition exponentiation required algorithms applied feature space vastly expanded expressive power 
extreme example consider gaussian kernel transforms kernel follows exp gamma oe feature space infinitely dimensions 

vector space representations document possible associate bag terms bag words simply considering number occurrences terms contains 
typically words stemmed meaning inflection information contained letters removed 
bag words natural representation vector way 
number dimensions number different terms corpus entry vector indexed specific term components vector formed integer numbers representing frequency term document 
typically vector mapped space word frequency information merged information word importance uninformative words low weight 
way document represented column vector entry records times particular word stem document 
typically tens thousands entries number documents 
furthermore particular document representation typically extremely sparse having relatively non zero entries 
basic vector space model document represented vertical vector indexed elements dictionary corpus matrix columns indexed documents rows indexed terms dm 
call data matrix term document matrix 
define document document matrix term term matrix dd consider feature space defined basic vector space model corresponding kernel inner product feature vectors hd case gram matrix just document document matrix 
generally consider transformations document vectors mapping oe 
simplest case involves linear transformations type oe pd appropriately shaped matrix 
case kernels form pd call representations vector space models 
gram matrix case pd definition symmetric positive definite 
class models obtained varying matrix natural corresponding different linear mappings standard vector space model giving different scalings projections 
note jiang littman framework collection different methods viewing kernels 
rest refer matrix defining vsm 
describe number different models case showing appropriate choice realises vsm 
basic vector space model basic vector space model introduced salton kernel joachims uses vector representation mapping 
words vsm matrix case 
performance retrieval systems simple representation surprisingly 
representation document vector sparse special techniques deployed facilitate storage computation dot products vectors 
common map obtained considering importance term corpus 
vsm matrix diagonal entries weight term methods proposed known strong influence generalization 
function inverse document frequency idf term total number documents corpus divided number documents contain term 
example word appears document regarded informative 
distance uniform distribution estimation importance better methods obtained studying typical term distributions documents corpora 
simplest method doing just log idf 
measures obtained information theoretic quantities empirical models term frequency 
measures label information estimated external larger unlabelled corpus provides background knowledge system 
described previous section soon defined kernel apply polynomial gaussian construction increase expressive power 
joachims dumais applied technique basic vector space model classification task impressive results 
particular polynomial kernels seen including features tuple words degree chosen polynomial 
problems representation treats terms uncorrelated assigning orthogonal directions feature space 
means cluster documents share terms 
reality words correlated synonymous documents common terms potentially closely related topics 
similarities detected 
raises question incorporate information semantics feature map link documents share related terms 
idea perform kind document expansion adding expanded version synonymous closely related words existing terms 
similar method replace terms concepts 
information potentially gleaned external knowledge correlations example semantic network 
ways address problem 
possible statistical information term term cor relations derived corpus external corpus 
approach forms basis latent semantic indexing 
subsections look different methods case showing implemented directly kernel matrix need explicitly feature space 
allow combined kernel techniques polynomial gaussian constructions described 
generalised vector space model early attempt overcome limitations proposed wong name generalised vsm gvsm 
document characterised relation documents corpus measured 
method aims capturing term term correlations looking occurrence information terms semantically related occur documents 
effect documents seen similar share terms 
gvsm technique provide metric easy see constitutes kernel function 
term document data matrix gvsm kernel dd matrix dd term term matrix nonzero ij entry document corpus containing th th terms 
terms occurring document considered related 
new metric takes occurrence information account 
documents mapped feature space indexed documents corpus document represented relation documents corpus 
reason known dual space method 
common case documents terms method act bottle neck mapping forcing dimensionality reduction 
gvsm vsm matrix chosen document term matrix 
method combined polynomial gaussian kernel construction techniques 
example degree polynomial kernel features tuple documents non zero feature document shares terms document tuple 
knowledge combination previously considered polynomial gaussian construction 
semantic smoothing vector space models natural method incorporating semantic information directly external source semantic network 
section briefly describe approach 
buc semantic network word net way obtain term similarity information 
network encodes word dictionary relation words hierarchical fashion synonym hypernym 
example word husband wife special cases hypernym spouse 
way distance terms hierarchical tree provided wordnet gives estimation semantic proximity modify metric vector space documents mapped bag words approach 
buc included knowledge kernel entries square vsm matrix entries ij ji expresses semantic proximity terms semantic proximity defined inverse topological distance graph length shortest path connecting cases deserve special attention 
modified metric gives rise kernel pd distance gamma pd kp gamma gamma gamma buc distance order apply gaussian kernel construction described polynomial construction equally applied kernel 
buc term term similarity matrix incorporate semantic information resulting square matrix possible concept term relation matrix rows indexed concepts terms 
example consider husband wife examples concept spouse 
matrix case longer square symmetric 
notice regarded special case concepts correspond documents corpus term belongs th concept occurs document 
latent semantic kernels latent semantic indexing lsi technique incorporate semantic information measure similarity documents 
construct kernel functions 
conceptually lsi measures semantic information occurrence analysis corpus 
technique extract information relies singular value decomposition svd term document matrix 
document feature vectors projected subspace spanned singular vectors feature space 
dimension feature space reduced control dimension varying define kernel feature space particular choice vsm matrix see computed directly original kernel matrix direct computation svd feature space 
order derive suitable matrix consider term document matrix svd decomposition sigmav sigma diagonal matrix dimensions orthogonal 
columns singular vectors feature space order decreasing singular value 
projection operator dimensions identity matrix diagonal elements nonzero matrix consisting columns new kernel expressed ui pd motivation particular mapping identifies highly correlated dimensions terms occur documents corpus merged single dimension new space 
creates new similarity metric context information 
case lsi possible isometrically re embed subspace back original feature space defining square symmetric ui gives rise kernel uu uu uu pd view term term similarity matrix making lsi special case semantic smoothing described buc 
need explicitly entries term term similarity matrix help semantic network infer semantic similarities directly corpus occurrence analysis 
interesting kernel methods mapping acting term term matrices obtained implicitly working smaller document document gram matrix 
original term document matrix gives rise kernel matrix feature vector document th column svd decomposition related eigenvalue decomposition follows sigmau sigmav sigma th column eigenvector corresponding eigenvalue ii oe feature space created choosing singular values lsi approach corresponds mapping feature vector vector ui gives rise kernel matrix ui ui sigmau ui sigmav sigmai sigmav matrix diagonal entries th set zero 
new kernel matrix obtained directly applying eigenvalue decomposition component matrices having set eigenvalues zero 
obtain kernel corresponding lsi feature space computing features 
relations computation kernel pca immediate 
similar analysis possible verify evaluate new kernel novel inputs explicit feature space 
order evaluate learned functions novel examples show evaluate new kernel new input training example 
function wish evaluate form ff ff uu uu ff uu uu ff sigmau ff sigma ff sigma expression involves feature vector avoid evaluating explicitly 
consider vector sigmau inner products new feature vector training examples original space 
inner products evaluated original kernel 
sigmau sigma sigma showing evaluate follows ff sigma ff evaluate new example create vector inner products original feature space take inner product precomputed row vector ff computation involves working directly feature space 
combination lsk technique polynomial gaussian construction opens possibility performing lsi high dimensional feature spaces example indexed tuples terms 
experiments applying approach reported experimental section 
think polynomial mapping conjunctions terms view lsk step soft disjunction projection links different conjunctions single concept 
combination polynomial mapping followed lsk step produces function form reminiscent disjunctive normal form 
alternatively perform lsk step polynomial mapping just applying polynomial mapping entries gram matrix obtained lsk step obtaining space indexed tuples concepts 
function obtained reminiscent conjunctive normal form 
applied approach ionosphere data obtained improvement performance 
conjecture results obtained depend strongly fit style function particular data 
main drawback approaches computational complexity performing eigenvalue decomposition kernel matrix 
matrix smaller term document matrix usually longer sparse 
difficult process training sets larger examples 
section techniques get round problem evaluating approximation lsk approach 

algorithmic techniques experiments performed eigenvalue decomposition routine provided numerical recipes 
complete eigen decomposition kernel matrix expensive step possible try avoid working real world data 
efficient methods developed obtain approximate lsk solution 
view lsk technique method obtaining low rank approximation kernel matrix 
projection eigenvalues rank approximation minimises norm resulting error matrix 
projection just method obtaining low rank approximation 
developed approximation strategy gram schmidt decomposition 
similar approach unsupervised learning described smola 
projection built span subset projections set training examples 
selected performing gram schmidt training vectors feature space 
vector selected remaining training points transformed orthogonal 
vector selected largest residual norm 
transformation performed feature space kernel mapping represent vectors obtained 
refer method gk algorithm 
table gives complete pseudo code extracting features kernel defined feature space 
lsk method parametrised number dimensions selected 
table 
gsk algorithm kernel training set dm number norm argmax norm index size norm feat gamma gamma feat feat size norm norm gamma feat feat return feat th feature input classify new example gamma gamma feat size return th feature example 
implicit dimensionality reduction interesting solution problem approximating latent semantic solution possible case directly interested low rank matrix information retrieval case plan kernel conjunction optimization problem type minimize ff qff ff hff hessian obtained pre post multiplying gram matrix diagonal matrix containing gamma labels ky diag note eigenvalues kx hy possible easily cheaply modify gram matrix obtain nearly solution obtain expensive low rank approximation 
minimum error function occurs point ff satisfies hff 
matrix replaced minimum moves new point ff satisfies eff 
consider expansion hu expansions ff ff basis ff ff ff ff substituting formulae equating coefficients th eigenvalue gives ff ff implying ff ff fraction equation squashing function approaching zero values approaching ae case ff second case ff ff effect map parameter chosen carefully region spectrum eigenvalues decrease rapidly effectively project solution space spanned eigenvectors larger eigenvalues 
algorithmic point view efficient explicitly performing low rank approximation computing eigenvectors 
derivation provides cheap approximation algorithm latent semantic kernel 
highlights interesting connection algorithm norm soft margin algorithm noise tolerance obtained adding diagonal kernel matrix 
note approximations view example svm solution constrained optimisation ff constrained positive 
case effect may different support vectors nearly orthogonal eigenvectors corresponding large eigenvalues 
fact procedure distinct standard soft margin approach borne experiments described section 

experimental results empirically tested proposed methods text non text data order demonstrate general applicability method test effectiveness different conditions 
results generally positive cases improvements significant worth additional computation 
cases significant advantage latent semantic gram schmidt kernels certainly hurts performance 

experiments text data section describes series systematic experiments performed text data 
selected text collections reuters medline described 
datasets reuters conducted experiments set documents containing stories reuters news agency reuters data set 
reuters newer version corpus 
compiled david lewis publicly available www research att com lewis 
obtain training set test set exists different splits corpus 
modified apte split 
split comprises training test documents 
reuters category contain documents training set 
similarly test set category relevant documents 
medline medline second data set experiments 
dataset comprises medical documents queries obtained national library medicine 
focused query query 
queries contain relevant documents 
selected randomly data training classifier evaluation having relevant documents training set relevant documents test set 
performed random splits data 
experiments reuters documents preprocessed 
removed punctuation words occurring list applied porter stemmer words 
weighted terms variant tfidf scheme 
log tf log df tf represents term frequency df document frequency total number documents 
documents unit length feature space 
preprocessed medline documents removing words punctuation weighted words variant tfidf described preceding paragraph 
normalised documents bias occur length documents 
evaluation performance measure 
pr precision recall 
set experiment conducted subset documents reuters data set 
selected randomly documents training remaining documents test set 
focused top dimension gsk lsk baseline 
generalisation performance svm gsk lsk linear kernel earn 
reuters categories earn acq money fx grain crude 
trained binary classifier category evaluated performance new documents 
repeated process times category 
svm linear kernel baseline experiments 
parameter controls trade error maximisation margin tuned conducting preliminary experiments 
chose optimal value conducting experiments splits category 
ran svm reduced feature space feature space full dimension 
value showed best results full space selected experiments 
medline text corpus selected value conducting experiments split data 
ran svm feature space full dimension 
optimal value showed best results selected 
note split experiments 
choice perfect basis experimental observation reuters conclude method gives optimal value results experiments reuters shown figures 
note results averaged runs algorithm 
started small dimensional feature space 
increased dimensionality feature space intervals extracting features 
figures demonstrate performance lsk method comparable baseline method 
generalisation performance svm classifier varies varying dimensionality semantic space 
increasing value numbers rise reaching maximum falls number equivalent baseline method 
maximum substantially different baseline method 
words obtain modest gain incorporating information kernel matrix 
illustrate results experiments conducted medline queries 
results averaged random runs algorithm 
experiments start small number dimensions 
dimensionality increased intervals extracting features 
results gsk lsk baseline 
generalisation performance svm gsk lsk linear kernel acq 
dimension gsk lsk baseline 
generalisation performance svm gsk lsk linear kernel money fx 
dimension gsk lsk baseline 
generalisation performance svm gsk lsk linear grain 
dimension gsk lsk baseline 
performance svm gsk lsk linear kernel crude 
dimension gsk lsk baseline 
generalisation performance svm gsk lsk linear kernel query 
dimension gsk lsk baseline 
generalisation performance svm gsk lsk linear kernel query 
table 
numbers varying dimensions feature space svm classifier lsk svm classifier linear kernel baseline reuters categories category baseline earn acq money fx grain crude trade interest ship wheat corn micro avg query encouraging showing lsk potential show substantial improvement baseline method 
results reuters medline show cases improvements performance significant improvements 
results reuters medline datasets demonstrates gsk effective approximation strategy lsk 
cases results approximately lsk 
worth noting cases gsk may show substantial improvement baseline method lsk 
results demonstrate gsk approximation strategy lsk 
improve generalisation performance lsk evident results medline data 
extract informative features useful classification 
gsk achieve maximum high dimension situations 
phenomenon may cause practical limitations large data sets 
addressed issue developed generalised gsk algorithm text classification 
furthermore conducted set experiments study behaviour svm classifier semantic kernel svm classifier linear kernel scenario classifier learnt small training set 
selected randomly training data documents 
focused top categories earn acq money fx grain crude trade interest ship wheat corn 
note number relevant documents shown name categories 
binary classifier learnt category evaluated full test set documents 
tuned category 
numbers obtained results experiments reported table 
micro averaged numbers 
set value 
noted gain categories loss performance 
worth noting svm classifier trained semantic kernel perform approximately baseline method dimensions 
results demonstrate proposed method capable performing reasonably environments labelled documents 

experiments non text data 
generalization error polynomial kernels degrees ionosphere data averaged random splits function dimension feature space 
experiments conducted non text ionosphere data set uci repository 
ionosphere contains features points 
measured gain lsk comparing performance svm polynomial kernel 
parameter set conducting preliminary experiments split data keeping dimensionality space full 
tried 
optimal value demonstrated minimum error chosen 
value splits reduced feature space 
note split data tuning parameter experiments 
results shown 
results averaged runs 
experiments setting small value 
increased dimensionality space intervals 
results show test error greatly reduced dimension feature space reduced 
curves demonstrate classification error svm classifier semantic kernel reaches minimum 
peaks valleys showing results equivalent baseline method 
results demonstrate proposed method general applied domains text 
potential improve performance svm classifier reducing dimension 
cases show gain may successful reducing dimension 

generalised version gsk algorithm text classification section generalised version gsk algorithm 
algorithm arose result experiments reported section 
preliminary experiments contributed development algorithm 
gsk algorithm previous section extracts features relative documents irrespective relevance category 
words features computed respect label document 
generally category distribution skewed text corpora 
establishes need bias feature computation relevant documents 
words introduce bias feature extraction process computed features useful informative text classification 
main goal developing generalised version gsk algorithm extract informative features fed classifier show high effectiveness low number dimensions 
achieve goal described preceding paragraph propose algorithm shown 
gsk iterative procedure greedily selects document iteration extracts features 
iteration criterion selecting document maximum residual norm 
generalised version gsk algorithm focuses relevant documents placing weight norm relevant documents 
algorithm transforms documents new reduced feature space set documents 
input underlying kernel function number bias fed algorithm 
number specifies dimension reduced feature space gives degree feature extraction biased relevant documents 
algorithm starts measuring norm document 
concentrates relevant documents placing weight norm documents 
step document maximum norm chosen features extracted relative document 
process repeated times 
documents transformed new dimensional space 
dimension new space smaller original feature space 
note positive data available training equal weights relevant irrelevant documents 
generalised version gsk algorithm provides practical solution problem may occur gsk algorithm 
algorithm may show require kernel training set yn bias number norm norm norm argmax index size norm feat gamma gamma feat feat size norm norm gamma feat feat return feat th feature input classify new example gamma gamma feat size return th feature example 
generalised version gsk algorithm generalisation high dimension training data 
scenario generalised version gsk algorithm shows similar performance lower dimensions 
complete pseudo code algorithm 
experiments generalised gsk algorithm employed generalise gsk algorithm transform reuters documents new reduced feature space 
evaluated proposed method conducting experiments full reuters data set 
version performed experiments categories contain relevant document training set test set 
order transform documents new space free parameters dimension reduced space bias need tuned 
analysed performance svm classifier respect conducting set experiments reuters categories 
results experiments shown table 
set experiments set dimensionality space varied results demonstrate extraction features biased environment informative useful insufficient training data 
basis experiments selected optimal value set experiments 
note selected optimal value conducting preliminary experiments reuters category 
set value 
results set experiments table 
value dimensional space 
microaveraged values shown table 
order learn svm classifier sv light experiments described section 
results show generalised gsk algorithm viewed substantial dimensionality reduction technique 
observation proposed method shows results comparable baseline method dimensionality 
note baseline method employed svm linear kernel 
noted dimensionality slow improvement generalisation performance svm 
micro averaged values svm generalised gsk dimensions micro averaged number svm linear kernel 
results show performance proposed technique comparable baseline method 
results show generalised gsk algorithm practical approximation lsk 
learning algorithm provided positive training data need bias feature extraction process 
learning algorithm positive training data svm may show performance high dimensionality leading practical limitations 
bias relevant documents overcome problem making technique applied large data sets 
table 
numbers acq wheat different values acq money fx wheat table 
numbers top reuters categories 
category baseline earn acq money fx grain crude trade interest ship wheat corn micro avg micro avg 
studied problem introducing semantic information kernel learning method 
technique inspired approach known latent semantic indexing borrowed information retrieval 
lsi projects data subspace determined choosing singular vectors singular value decomposition 
shown obtain inner products derived projection performing equivalent projection eigenvectors kernel matrix 
possible apply technique kernel defined feature space original dimensionality 
refer derived kernel latent semantic kernel lsk 
experimentally demonstrated efficacy approach text non text data 
datasets substantial improvements performance obtained method little effect observed 
eigenvalue decomposition matrix relatively expensive compute considered iterative approximation method equivalent projecting dimension derived gram schmidt data 
perform projection efficiently kernel defined feature space experiments show datasets called gram schmidt kernel gsk effective lsk method 
despite success large imbalanced datasets encountered text classification tasks number dimensions required obtain performance grows quite large relevant features drawn small number positive documents 
problem addressed biasing gsk feature selection procedure favour positive documents greatly reducing number dimensions required create effective feature space 
methods described similar flavour demonstrated impressive performance datasets 
question dataset different semantic focusing methods effective fully understood remains subject ongoing research 
authors thorsten joachims chris watkins useful discussions 
supported epsrc number gr european commission esprit working group neural computational learning neurocolt nr 


st project kernel methods images text nr 
st 

aizerman braverman 
theoretical foundations potential function method pattern recognition learning 
automation remote control 

boser guyon vapnik 
training algorithm optimal margin classifiers 
haussler editor proceedings th annual acm workshop computational learning theory pages 
acm press 

cristianini shawe taylor 
support vector machines 
cambridge university press 

deerwester dumais furnas landauer harshman 
indexing latent semantic analysis 
journal american society information science pages 


dumais platt heckerman sahami 
inductive learning algorithms representations text categorization 
th international conference information knowledge management 

dumais littman landauer 
automatic cross language retrieval latent semantic indexing 
aaai spring cross language text speech retrieval 

herbrich obermayer graepel 
large margin rank boundaries ordinal regression 
smola bartlett scholkopf schuurmans editors advances large margin classifiers 
mit press 

joachims 
making large scale svm learning practical 
scholkopf burges smola editors advances kernel methods support vector learning 
mit press 

fan jiang michael littman 
approximate dimension equalization vector information retrieval 
pat langley editor proceedings seventeenth international conference machine learning 
morgan kauffman 

joachims 
text categorization support vector machines 
proceedings european conference machine learning ecml 

leopold 
text categorization support vector machines 
represent texts input space 
machine learning appear 

miller beckwith fellbaum gross miller 
papers wordnet 
technical report stanford university 

opper winther 
gaussian processes svm mean field leave 
smola bartlett scholkopf schuurmans editors advances large margin classifiers 
mit press 

william press 
numerical recipes art scientific computing 
cambridge university press 

salton wang yang 
vector space model information retrieval 
journal american society information science 

saunders vovk 
ridge regression learning algorithm dual variables 
shavlik editor machine learning proceedings fifteenth international conference 
morgan kaufmann 

scholkopf mika smola ratsch 
muller 
kernel pca pattern reconstruction approximate pre images 
niklasson bod en ziemke editors proceedings th international conference artificial neural networks perspectives neural computing pages 
springer verlag 

scholkopf smola muller 
kernel principal component analysis 
scholkopf burges smola editors advances kernel methods support vector learning pages 
mit press 

scholkopf williamson smola shawe taylor 
sv estimation distribution support 
neural information processing systems 
forthcoming 

shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee transactions information theory 

shawe taylor cristianini 
margin distribution soft margin 
smola bartlett scholkopf schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 

sheridan 
experiments multilingual information retrieval spi der system 
proceedings th annual international acm sigir conference research development information retrieval pages 
association computing machinery 

buc 
support vectors machines semantic kernel text categorization 
proceedings international joint conference neural networks ijcnn como 
ieee 

smola scholkopf 
tutorial support vector regression 
statistics computing 
invited press 

smola mangasarian scholkopf 
sparse kernel feature analysis 
technical report university wisconsin data mining institute madison 

vapnik 
statistical learning theory 
wiley 

wong wong 
generalized vector space model information retrieval 
acm sigir conference research development information retrieval pages 
acm 
