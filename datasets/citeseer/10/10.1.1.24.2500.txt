task parallel programming reconfigurable systems markus wayne luk pact gmbh 
munich germany department computing imperial college london sw bz uk 
presents task parallel programming style application development reconfigurable systems 
task parallel programming enables efficient interaction concurrent hardware software tasks 
particular supports description communication computation tasks running parallel allow effective implementation designs data transfer time hardware software components comparable computation time 
approach permits precise specification parallelism requiring hardware design knowledge 
language extensions task parallel programming inspired occam handel languages 
compilation scheme method described main stages memory mapping channel implementation software generation hardware synthesis 
techniques evaluated video applications rc pp hardware platform 
reconfigurable computing community recognized need efficient tools enable software developers hardware design experience take advantage reconfigurable technology 
compilers developed translate high level program machine code host processor configuration bitstreams field programmable accelerator 
automatically parallelize regular algorithms signal image processing applications map appropriate platforms 
successful exploiting fine grain parallelism hardware part application compilers usually exploit parallelism hardware software coarse grain hardware tasks 
host coprocessor communication normally overlap computation causes significant inefficiency loosely coupled reconfigurable systems copying data system busses pci bus takes longer hardware computation 
shortcomings addressed proposed approach 
contributions include explaining task parallel programming style capture patterns efficient interactions hardware software tasks presenting compilation scheme task parallel programs describing implementation task parallel programming rc pp hardware platform evaluating performance 
supported european union tmr training project uk engineering physical sciences research council limited xilinx frame fr fr get frame fr proc frame fr fr put frame fr frame fr fr configure fpga get frame fr copy hardware fr run hw coprocessor copy host fr put frame fr fig 

video processing loop software call hardware coprocessor added 
section reviews sequential model shortcomings reconfigurable computing 
section presents task parallel programming model 
section outlines compilation techniques generate host program fpga bitstreams 
performance results video processing applications section conclude section 
sequential model typical hardware software compilers sequential programs attempt accelerate application synthesizing dedicated hardware coprocessors computationally intensive parts program :10.1.1.21.7928
coprocessors implemented fpgas respective parts host program substituted coprocessor calls 
amount program code performed coprocessor range instructions loops functions larger computational kernels 
hardware software partitioning determines input data transferred reconfigurable hardware results returned host processor 
amount communication duration transfers determined partitioning user direct control 
transfer time outweighs speedup achieved coprocessor 
model effective computation communication ratio high instance specialized bit level functions supported standard microprocessors closely coupled systems combine reconfigurable hardware processor kernel chip smaller communication overhead 
furthermore semantics sequential input program constrains execution order software instructions hardware coprocessors data transfers 
fine grain parallelism possible local analyses detect independent operations 
instance consider program fragment typical video processing loop 
function get frame repeatedly reads video frame fr proc frame performs processing resulting fr put frame stores displays result 
typical hardware software compiler generates host program perform computation intensive function proc frame hardware 
function calls loop depend predecessor execution phases overlap 
instance copy host needs wait coprocessor execution finished leaving host processor idle 
coprocessor waits software iteration calls run hw coprocessor 
hardware software executions alternate wasting valuable processing time 
note pipelining loop 
starting get frame new iteration run hw coprocessor put frame previous iterations finished change program semantics 
pipelining illegal fr depend fr instance get frame just copies fr back fr 
hand get frame reads instance file pipelining increases latency application change functionality 
conclude application developer needs specify conditions explicitly instance increased latency acceptable possible purely sequential programming language 
task parallelism described section enables keep programming general simple possible specific necessary allow efficient implementation 
task parallel programming different methodologies parallel programming suggested 
mainly differ way processors processes access memory access memory space shared memory individual memories distributed memory 
uniform memory space shared memory systems simpler program 
provisions prevent processes writing memory cell time needed 
parallel processors distributed memory programmed communicating sequential processes csp model implementation occam programming language 
occam parallel processes share variables 
run independently parallel synchronize channel communication 
versatility occam noted specifying hardware software components occam hardware software partitioning 
propose language compiler similar handel facilitates description overlapping computation communication parallel 
captures shared memory parallelism distributed memory parallelism hardware 
main innovations system support hardware software descriptions treat array data single entity channel communications provide automatic optimised allocation array data memory banks 
approach best described task parallel programming specialized loosely coupled reconfigurable systems 
host coprocessor run independently share memory dedicated communicating processes model software hardware part application 
channel communications translated transfers system bus 
extend channel concept entire data arrays image frames example section 
dma transfer instructions generated explained section 
addition hardware software parallelism software hardware parts may contain parallel sub tasks 
programmed shared memory methods 
opposed sequential model section approach enables task level parallelism 
data transfers hardware software partitioning crucial impact system performance explicit predictable controllable 
define define chan unsigned char chan fr chan fr static void software unsigned char fr unsigned char fr sequential block par par get frame fr rcv chan chan fr fr par snd chan chan fr fr put frame fr static void hardware unsigned char fr fr fr fr par parallel task sequential snd chan rcv chan snd chan chan fr fr rcv chan chan fr fr smooth frame fr fr par second parallel task sequential snd chan rcv chan snd chan chan fr fr rcv chan chan fr fr smooth frame fr fr fig 

task parallel program video smoothing 
parallel tasks hardware execute share hardware smooth frame process 
shows example task parallel program applies image smoothing video frame 
special functions software hardware represent main software hardware processes intention maximize available parallelism having double buffer 
processes initialized program start execute concurrently 
communicate channels chan fr chan fr globally declared keyword chan 
statements snd chan rcv chan perform unbuffered channel communications synchronize processes occam channels 
processes stalls send command met receive command channel vice versa 
handel statements enclosed block curly brackets execute sequentially keyword par immediately precedes block 
rules program executes follows 
software process contains loop frame processing specified 
process gets new input frame receives previous result frame hardware chan fr time 
sends new input frame chan fr outputs result frame 
hardware process main processing loop uses pairs arrays input output frames 
pipelines transfer new frame processing transfer back 
image smoothing operator smooth frame transforms fr fr concurrently snd chan sends old transformed frame fr back software process rcv chan receives new frame fr 
shows processing phases software process mapped host hardware process mapped fpga array hardware process mapped memory bank 
mappings automatically performed described section 
note output frames undefined pipeline filled third iteration 
concurrent computation communication require twice storage memory fr fr needed computation communication take place sequentially 
fr fr fr fr second parallel task fr fr parallel task fr fr fig 

parallel tasks running described 
active links indicated solid arrows inactive links dotted arrows 
static void hardware unsigned char fr fr fr unsigned int minmax minmax minmax sequential block par par sequential snd chan rcv chan snd chan chan fr fr rcv chan chan fr fr comp minmax fr minmax stretch fr minmax fr par snd chan chan fr fr rcv chan chan fr fr comp minmax fr minmax stretch fr minmax fr par snd chan chan fr fr rcv chan chan fr fr comp minmax fr minmax stretch fr minmax fr fig 

main hardware task histogram stretching 
task parallel program follows pattern easily generalized cover hardware sub tasks parallel 
illustrate generalization consider video processing algorithm histogram stretching 
smoothing histogram stretching local transformation 
scans entire frame twice determine minimum maximum greyscale values second stretch pixel values entire greyscale range 
hardware implementation performs processing phases parallel different frames internally vectorized tasks comp minmax stretch 
data transfer performed parallel tasks 
results processing phases frame arrays small arrays minima maxima cyclic manner delaying output frames 
note stretch read write array processes pixel independently surrounding pixels 
compilation shows overview compilation process 
target hardware architecture adopt contains memory banks arrays data mapped 
memory mapping communication channels implemented involving software hardware 
hardware synthesized producing netlist software program generated 
case target single processor host software memory mapping hardware synthesis software hardware task parallel program netlist software program channel implementation channel implementation sequentialization software generation fig 

compilation overview 
sequentialization necessary 
describe compilation phases 
memory mapping phase determines program variables stored reconfigurable accelerator 
scalar variables directly mapped fpga registers arrays local chip ram 
allocate arrays memory banks optimization technique determines bank number offset value array 
allocation subject additional constraint array transferred host array accessed fpga stored bank memory bank accessed host bus fpga time 
restriction applies accessed different hardware sub tasks concurrently want optimize memory accesses hardware tasks independently see paragraph hardware synthesis 
arrays hardware allocated banks accessible host chip ram 
arrays fr fr fr fr fr minmax fr minmax fr minmax examples allocated different memory banks overlap data transfer hardware computation 
mentioned shows memory allocation video smoothing algorithm system memory banks 
channel implementation compiler replaces snd chan rcv chan calls software hardware data transfer functions targeted accelerator board 
physical communication channel system bus exists consistency logical channels program checked 
identify channel transfer main hardware task assign unique id value sent software process performing actual channel communication 
statements snd sw hardware rcv hw software initiate handshake protocol transferring id software side consistency logical channels checked program aborts inconsistency detected 
scalar data synthesized hardware design ensures correct fpga register transfer 
array transfers transfer instruction software process contain information 
specifies channels array hardware process transfer refers known 
software controls dma array bank number offset determined memory mapping necessary generate correct dma instruction 
id numbers sent hardware generate conditional statements containing correct dma instruction depending hardware state 
dma transfer finished acknowledgment sent hardware snd hw rcv sw 
hardware process synchronizes software functions array send receive functions removed dma transfers software control 
fpga circuit release memory banks currently 
shows resulting software main function hardware description ready synthesis task parallel program 
id numbers indicate transfers host channel chan fr id numbers indicate transfers accelerator channel chan fr 
hardware channel communication substituted snd sw rcv sw pair 
software rcv hw reads id number assert statement checks channel consistency snd hw sends hardware 
remaining components main explained section 
software generation generated software program contains code configure start hardware process fpga followed code software function channel communication substituted defined 
case target single processor host generate sequential host program multiple software threads 
restrict parallelism allowed software process dma transfers running parallel host computations 
implemented sequential program placing computations dma start instructions 
shows result technique 
placing get frame put frame dma host dma hw wait dma finished execute concurrently dma transfers 
hardware synthesis hardware synthesis generates circuitry evaluate expressions 
assignments fpga registers memory accesses host communications generally performed order input program 
components par block started time synchronized block 
arrays transferred parallel smooth frame 
note consistency checked statically compile time simple regular programs sequences channel transfers combined reducing number communications 
currently consider run time reconfiguration multi fpga architectures configuration bitstream downloaded program start 
main unsigned char fr unsigned char fr unsigned char id configure hw start hardware rcv hw id assert id id id dma host bank fr offset fr fr id dma host bank fr offset fr fr get frame fr wait dma finished snd hw rcv hw id assert id id id dma hw fr bank fr offset fr id dma hw fr bank fr offset fr put frame fr wait dma finished snd hw static void hardware unsigned char fr fr fr fr par snd sw rcv sw snd sw rcv sw smooth frame fr fr par snd sw rcv sw snd sw rcv sw smooth frame fr fr fig 

generated software hardware program fig 

physical host communication channel compiler requires channel communications occur parallel transfers serialized way consistent software 
mentioned section write conflicts prevented shared memory parallel programs 
hardware synthesis detect conflicts defining assignments processes take clock cycle 
approach supported handel language 
possible appropriate subtasks instance pipeline 
way reduce deadlock problems restrict hardware sub tasks communicate shared variables synchronous channels 
sub task access array variables entire sub task execution scalar variables main hardware task read 
memory mapping constraints conditions guarantee subtask exclusive access memory banks requires 
synchronization limited main hardware task sequentially specified sub tasks synthesized independently optimization techniques vectorization developed sequential programs 
enable additional fine grain parallelism sub tasks 
histogram stretching example minimum maximum values computed comp minmax allocated arrays scalar variables order synthesize vectorize comp minmax stretch independently 
implementation ideal smooth stretch smooth stretch smooth stretch sequential fps fps fps fps fps fps task parallel fps fps fps fps fps fps speedup table 
performance skeletonization program 
implementation results currently extending suif pipeline compiler prototype task parallel programs 
targets rc pp board contains single fpga banks mb memories accessible fpga host bus 
annotations extended syntax section suif standard compiler frontend changes 
extensions hardware synthesis channel communication parallel sub tasks 
presents preliminary performance estimates smoothing histogram stretching algorithms rc pp board 
effect overlapping sub tasks obviously application specific 
instance sub tasks histogram stretching example take ms ms frame design runs mhz 
concurrent execution reduces run time ms ms speedup 
general statements speedup achieved overlapping software hardware speedup sw fpga max fpga sw duration iteration software process including dma transfers fpga duration hardware iteration 
maximum speedup achieved sw fpga improvement difference non real time real time performance 
table shows estimated frame rates speedups smoothing histogram stretching coprocessor model task parallel approach 
consider software systems relatively slow program capturing displaying video frames camera ms frame retrieves displays avi files ms frame efficient ideal system ms frame 
results show speedup limited software time largely outweighs hardware time 
situation better achieving smoothing 
fast system balanced hardware software workloads achieve maximum speedup 
describes task parallel programming reconfigurable systems 
combines shared memory distributed memory parallel programming techniques extends loosely coupled hardware software systems 
video processing example show easily applications specified parallel software hardware tasks 
developer require specific hardware design knowledge compiler capable mapping program efficiently host processor reconfigurable accelerator 
system specifically enables overlapping software data transfer hardware computations communication channels parallel execution coarse grain hardware tasks 
compilation scheme task parallel programs optimization vectorization techniques previously developed sequential programs 
run time estimates commercially available accelerator board show effectiveness method 
includes optimizing channel implementation supporting general parallel software tasks 
additionally intend combine task parallel programming run time reconfiguration multi fpga systems partially reconfigurable fpgas 
just communication latency hidden overlapping communication computation reconfiguration latency hidden overlapping reconfiguration computations pre configuring currently unused resources possibly speculative way 
stylized communication patterns task parallel programs shown indicate potential automation objective arrange parallel tasks similar lengths execution time 

gokhale stone 
napa compiling hybrid risc fpga architecture 
proc 
fpgas custom computing machines 
ieee computer society press 

luk 
pipeline vectorization 
ieee transactions computer aided design integrated circuits systems february 

buell arnold 
splash fpgas custom computing machine 
ieee computer society press 

limited 
homepage www com 

athanas silverman 
processor reconfiguration instruction set metamorphosis 
ieee computer march 

hauser wawrzynek 
garp mips processor reconfigurable coprocessor 
proc 
fpgas custom computing machines 
ieee computer society press 

holt 
napa adaptive processing architecture 
proc 
fpgas custom computing machines 
ieee computer society press 

hoare 
communicating sequential processes 
prentice hall international 

hoare page 
hardware software closing gap 
transputer communications june 

barros 
provably correct hardware software partitioning occam 
proc 
int 
workshop hardware software codesign 
ieee computer society press 

page luk 
compiling occam fpgas 
fpgas 
ee cs books 

weeks 
computer imaging recipes prentice hall 

luk 
memory access optimization ram inference pipeline vectorization 
field programmable logic applications lncs 
springer 

stanford suif compiler group 
homepage suif stanford edu 
