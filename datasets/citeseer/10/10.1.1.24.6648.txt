lrpd test speculative parallelization partially parallel loops technical report tr dept computer science texas university francis dang hao yu lawrence rauchwerger cs tamu edu current parallelizing compilers identify significant fraction parallelizable loops complex statically insufficiently defined access patterns 
previously proposed framework identification 
speculatively executed loop doall applied fully parallel data dependence test determine cross processor dependences test failed loop re executed serially 
method exploits doall parallelism cause slowdowns loops cross processor flow dependence re execute sequentially 
existing partial parallelism loops exploited 
propose generalization speculative doall parallelization technique called recursive lrpd test extract exploit maximum available parallelism loop limits potential slowdowns overhead run time dependence test removes time lost due incorrect parallel execution 
asymptotic time complexity fully serial loops equal sequential execution time 
base algorithm analysis different heuristics practical application 
preliminary experimental results loops track spice fma show performance new technique 
research supported part nsf career award ccr nsf aci nsf eia doe asci asap level hewlett packard equipment efficient run time parallelization needed loops achieve high level performance particular program today supercomputers software developers forced hand code optimizations tailored specific machine 
hand coding difficult increases possibility error sequential programming resulting code may portable machines 
restructuring parallelizing compilers address problem detecting exploiting parallelism sequential programs written conventional languages parallel languages hpf 
compiler techniques automatic detection parallelism studied extensively decades see current parallelizing compilers extract significant fraction available parallelism loop complex statically insufficiently defined access pattern 
typical examples complex simulations spice dyna gaussian charmm 
run time techniques succeed static compilation fails access input data 
example input dependent dynamic data distribution memory accesses guarded run time dependent conditions subscript expressions analyzed unambiguously run time 
contrast compile time access pattern programs determined due limitations current analysis algorithms necessary information just available access pattern function input data 
previous taken different approaches run time parallelization 
employed lrpd test speculatively execute loop doall subsequently test execution correct 
loop re executed sequentially 
fully parallel loops method performs partially parallel loops experience slow equal speculative parallel execution time loop re executed sequentially 
second loops presumed partially parallel developed inspector executor technique record relevant memory employ sorting technique construct iteration dependence graph loop 
iterations scheduled topological order 
major limitation method assumption proper inspector loop exists 
dependence cycle data address computation shared arrays proper side effect free inspector traversed address space obtained 
analyzed loop 
furthermore technique requires large additional data structures proportional trace 
method partially parallel loops doacross mechanism enforcing dependencies executing private storage committing order possibility dependence violation passed 
method requires set phase iteration potential dependence causing addresses pre computed broadcast processors 
information set tags advance await type synchronizations 
method properly exploit large amounts parallelism remove need address pre computation inspector iteration 
parallelize loops address data depend 
new technique extract maximum available parallelism partially parallel loop removes limitations previous techniques applied loop requires memory overhead 
propose transform partially parallel loop sequence fully parallel loops 
stage speculatively execute remaining iterations parallel lrpd test applied detect potential dependences 
correctly executed iterations detected dependence committed process recurses remaining iterations 
limitation loop statically block scheduled increasing order iteration 
negative impact limitation reduced dynamic feedback guided scheduling dynamic load balancing technique described section 
additional benefit technique reduction potential slowdowns simple doall speculation incur compiler user guesses wrong 
effect applying new method exclusively remove uncertainty unpredictability execution time guarantee speculatively parallelized program run fast sequential version additional testing overhead 
remainder technique extension lrpd test implementation issues 
introduce performance model guides strategy applying various flavors technique 
validate model experimental results real code 
recursive lrpd test lrpd previous described lrpd test technique detecting doall loops 
compiler perform classical data dependence analysis speculatively transform loop parallel execution 
run time executes loop parallel tests subsequently data dependences occurred 
test fails loop re executed sequentially 
qualify parallel loops array privatization reduction parallelization speculatively applied validity tested loop termination 
simplicity reduction parallelization discussion tested similar manner independence privatization 
previously shown processor wise test reduce overhead test qualify loops parallel checking cross processor dependences loop carried dependences classical data dependence 
shown increase number loops parallel testing copy condition combination privatization 
privatization testing detect read memory location referenced write read sequence iteration remove type dependence allocating private storage processor 
memory location read number times number iterations privatizable pattern form read memory location transformed safe parallel execution privatizing initializing original shared data previous start loop 
formally addition privatization condition need test run time latest consecutive reading iteration maximum read earliest writing iteration minimum write loop 
processor wise test preferable schedule loop statically blocked 
limitation simplifies tested conditions highest reading processor lowest writing processor 
initialization private arrays done start speculative loop preferably demand copy read memory element written 
follows pattern invalidate speculative parallelization flow dependence processors write lower processor matched read higher processor dependences removed privatization copy 
crucial observation loop executed processor wise lrpd test chunks iterations equal source detected dependence arc executed correctly 
processors executing iterations larger equal earliest sink dependence arc need re execute portion 
leads remainder loop needs re executed represent significant saving previously lrpd test method re execute loop sequentially 
re execute fraction iterations assigned processors may worked erroneous data need repair unsatisfied dependences 
accomplished initializing privatized memory data produced lower ranked processors 
alternatively commit copy correctly computed data private shared storage demand copy re execution 
furthermore need re execute remainder loop serially 
re apply lrpd test remaining processors fact speculatively re execute parallel 
procedure applied recursively processors finished correctly 
loops cross processor dependences expect finish parallel steps 
call application test recursive lrpd test 
outlined options implementing basic strategy differ iterations assigned processors 
simplest describe potential optimizations 
non redistribution strategy better understand technique consider loop compiler statically determine access pattern shared array fig 

allocate shadow arrays marking write accesses read accesses loop augmented marking code fig 
enclosed loop repeats speculative parallelization loop completes successfully 
bits read write processor read occurs write bits remain set means privatizable 
write occurs subsequent read set read bit 
repeated type privatization creates processor cooperating execution loop private copies program variables 
shared variable privatizable written iteration read temporary variables 
reduction variable variable operation form exp associative commutative operator occur exp loop 
known transformations implementing reductions parallel 
element processor cause change shadow arrays 
array privatized 
read copy demand content shared array array tested statically analyzable checkpointed 
result marking speculative doall seen fig 

analysis phase copy commit elements computed processors shared counterpart written value 
step insures flow dependences satisfied stage parallel execution read data produced previous stage 
need restore section array modified processors correct state established arrays 
simple example really necessary overwrite 
non redistribution strategy re init step fig 
re initializes shadow arrays processors successfully completed assigned iterations processors case 
new parallel loop started processors remainder iterations case 
final state example shown fig 

point data committed loop finishes total steps iterations 
redistribution rd strategy fig 
adopt different strategy redistribution rd 
re executing processors incorrect data leaving rest idle stage redistribute remainder processors keeping rest procedure 
pros cons approach 
redistribution employ processors time execution time stage decreases staying constant case 
disadvantage may uncover new dependences processors satisfied executing processor 
hidden potentially large cost associated redistribution remote misses loop execution due data redistribution stages test 
section model strategies devise method decide 
enddo pa restore pa analyze success start :10.1.1.24.6648
success re init shadows pa endif commit start start doall start 
success check point success false init shadow arrays iter step rd step iter rd step iter iter success success loop transformed recursive speculative execution operations update appropriate shadow arrays 
test repeated success true 
independent array checkpointed partially restored stage 
pa privatized array initialized partially committed stage 
state shadow arrays lrpd test 
note cross processor dependences 
state shadow arrays second successful lrpd test processors 
state shadow arrays second lrpd test remainder redistributed rd processors 
note newly uncovered dependences 
final state shadow arrays second successful lrpd test redistribution rd 
sliding window sw strategy performance lrpd test dependent distribution type data dependences encountered 
codes display long distance data dependences quite lrpd test far fail 
noted failing restarting detrimental performance 
type situation devised different strategy sliding window strategy applying test 
initially distributing entire iteration space available processors strip mine entire speculative execution process 
means repeatedly perform lrpd test strip contiguous set iterations traverse fast possible entire iteration space 
fig 
illustrate execution stages technique applied loop 
assuming total processors schedule contiguous iterations speculatively execute parallel 
subsequent analysis phase commit iteration blocks iterations dependence processors 
commit point advanced iteration higher iterations scheduled 
new speculative lrpd test performed iterations committed dependences uncovered 
iterations speculatively executed processors 
increase memory locality organize sliding window circular manner iterations re executed necessary originally assigned processor 
trade offs sliding window sw previously strategies 
fully parallel loop rd methods execute stage global synchronization sw synchronization strip 
dependences possible rd techniques need re execute iterations sw 
sw strategy potentially analysis overhead may go shadows memory elements reused iteration 
far devised strategy choose techniques history predictions 
tradeoff considered sw window size size block contiguous iterations super iteration assigned processor 
larger block implies fewer global synchronizations 
limit window comprises iterations loop get rd case 
goal find ideal block size minimizes synchronizations uncovered dependences 
far able tune technique experimentally empirically 
worth mentioning scheduled block sizes dynamically adjusted execution history prediction applied close dependences encountered increase block size 
alternatively start large block equivalent rd dependences uncovered reduce re executions needed 
show section optimal strategy dependence distribution 
st stage st stage nd stage rd stage rd stage nd stage sliding window strategy example 
extracting data dependence graphs ddg lrpd test lrpd test extract parallelism loops proper inspector exist generates minimal additional overhead 
side extract maximum available parallelism 
loops dependences performance unaffected 
loops complex dependence graphs significant parallelism especially run smaller machines lrpd test generate sequential execution schedule 
beneficial iteration data dependence graph ddg help efficient graph partitioning routine generate optimal schedule 
somewhat similar techniques previously literature apply loops proper inspector extracted 
technique employs lrpd test efficiently extract necessary information construct ddg loop 
sliding window implementation lrpd test detect window window edges ddg stored distributed graph data structure 
modification previously described lrpd test seen fig 
shadow arrays organized level fifo structure record current previous iteration type privatizable reduction operand accessing memory element 
stage doall perform sw lrpd test 
cross processor dependence multiply referenced memory elements logged distributed adjacency table ddg graph 
addition log dependence overlapping processor 
stage test record set shadow array 
space recycled fifo order 
insure record element kept loop execution 
noted needed edges recorded graph example edge third stage needed entered ddg 
seen fig 
step re execute iterations may consumed wrong data re initialize shadow arrays 
obtain full information record read read edges 
ddg extracted analyze transform depth length critical path minimized 
important transformations reduce number dependence edges include allocation private memory elements privatizable participate reduction sequence iterations 
insert synchronizations ddg followed copy accumulate copy operations 
scheduling iteration space done ddg 
mention optimizations implemented 
un processed ddg obtain wavefronts sets independent iterations separated global synchronizations parallelize important loops spice see section 
complexity method essentially sw lrpd test additional constant overhead 
important note case sparse patterns spice shadow hash tables shadow arrays 
result increased time logging operation compact representation allows faster analysis 
tuning algorithm means finding right number iterations scheduled block lrpd test 
larger block size results fewer steps possibly poorer parallelism possible overlap 
finer granularity desired depth fifo shadows increased 
example chosen granularity iteration resulted fifo depth 
believe due input sensitivity method experiments necessary produce performance model 
modeling rd strategies previously shown lrpd test passes loop fact fully parallel speedups obtained range nearly ideal best case ideal worst case 
overhead spent performing single stage original lrpd test scales number processors data set size parallelized loop 
break time spent testing running loop lrpd single stage test fully parallel phases initialization shadow structures proportional dimension shadow structures 
dense access patterns initialize shadow arrays dimensioned conform tested arrays 
associated checkpointing state program entering speculation proportional number distinct shared data structures may modified loop 
dense access patterns proportional dimension shared arrays may modified loop execution 
actual time spent saving np pattern 

done doall np get schedule exec iteration mark write shadow pa 
mark read shadow 
pa 
call analysis done call call endwhile instrumented code generate dd graph sliding window lrpd test 
get schedule control iteration window analysis lrpd test iterations window generates dd graph edges iter fifo iter fifo fifo 
adjacency table iter 
proc proc iter iter iter iter iter iter 




distributed execution trace ddg construction example 
step iterations executed parallel flow dependence detected processors recorded adjacency table structure 
iteration re executed parallel iteration 
step data dependences processors 
dependence iteration added adjacency table 
step execution previous mark popped fifo 
dependences recorded distributed adjacency table parallel operation 
state loop stage depend checkpointing implemented separate step loop execution fly loop execution modifying shared variable 
overhead associated execution speculative loop equal time spent marking recording relevant data 
proportional dynamic count memory 
dense access patterns approximated number tested arrays 
final analysis marked shadow structures worst case proportional number distinct memory marked processor logarithm number processors participated speculative parallel execution 
dense access patterns phase may involve merge operation number processors shadow arrays 
recursive application lrpd test adds additional components accounted performance analysis 
breakdown depend fraction successfully completed turn depends data dependence structure loop 
important note dynamic programs data dependence structure loop extremely input dependent varies program execution 
cross processor dependences detected data restoration phase restore state shared arrays modified processors committed 
time proportional number elements shared arrays need copied checkpointed values 
dependences detected re execution needed shadow arrays re initialized 
commit phase transfers data computed value earlier processors private shared memory 
cost proportional number written array elements 
steps fully parallel scales number processors data size 
furthermore commit re initialization shadow arrays restoration modified arrays done concurrently tasks disjoint groups processors performed successful computation restart 
issues explained detail section 
number times re execution performed performed depends strategy adopted redistribution 
mentioned earlier redistribute time complexity technique cost sequential execution worst case 
steps performing number processors number iterations 
redistribution case rd take progressively time execute processors decreasing amount 
guaranteed finish finite number steps guaranteed processor executing correctly 
model carefully tradeoff strategies 
initially iterations equally distributed processors 
computation time iteration yielding total amount useful loop discussion assume know cost useful computation iteration cost redistributing data iteration processor cost barrier synchronization 
purpose efficient speculative parallelization classify loop types dependence distribution classes geometric loops constant fraction current remaining iterations completed speculative parallelization step linear loops constant fraction original iterations completed speculative parallelization step 
redistribution data speculative pay redistribute remaining iterations processors dependence detected speculative parallelization attempt 
overhead redistribution iteration larger iteration 
case total time required parallel execution simply static ks 
number steps required complete speculative parallelization 
determine time static need compute number steps number speculative parallelization attempts needed execute loop 
consider cases loops determine value 
loops assume constant fraction remaining completed speculative parallelization step 
case 
remains completed steps 
final th step occur 
ks 
remaining iterations reside processor redistribute 
solving get log example log constant loops assume constant fraction original completed successfully speculative parallelization step constant number processors successfully complete assigned iterations 
case completed steps 
completed example fully parallel loop static 
sequential loop static 
ps 
redistribution data speculative rd may pay redistribute remaining iterations processors dependence detected speculative parallelization attempt 
difference opposed redistribution case subsequent step processors smaller number iterations assigned 
case total time required parallel execution dyn kd kd number iterations remaining completed start th speculative parallelization step number speculative parallelization steps completed point redistribution 
redistribution initially useful comes point discontinued 
particular occur long time spent processor useful computation larger overhead redistribution synchronization 
redistribution occur long term sum eq 
larger sum terms equivalently long kd ps note condition tested run time involves number uncompleted iterations known run time assume known priori estimated static analysis experimental measurements 
summary steps remaining iterations redistributed processors 
redistribution occur 
point case described static starting kd total time required dyn static kd kd kd defined 
compute actual value need determine substitute eq 

example consider geometric loops constant fraction current completed speculative parallelization attempt 
case kd kd kd kd eq 
solving obtain log log described 
total time required kd kd computed defined known values general may know exactly cases reasonable estimates advance recomputed execution average values observed far 
experimental model validation graph fig 
illustrates loop testing overhead redistribution overhead time due remote cache misses restart recursive lrpd test synthetic loop executed processors hp system 
assume fraction remaining iterations 
initial speculative run assumed incur redistribution overhead 
performed experiments illustrate performance strategies case means strategy redistribute remaining 
adaptive redistribution means redistribution done long previous speculative loop time greater sum overhead incurred delay times previous run 
redistribution means redistribute 
fig 
shows execution time breakdown experiment 
stage lrpd test measure time spent actual loop synchronization redistribution overhead 
fig 
show cumulative times spent test stages 
adaptive redistribution method begins shorter execution times compared redistribution method failure processor 
method redistribute performs worst wide margin 
noted synthetic loop assumes simplicity constant 
practice adjust model parameters stage lrpd test 
implementation optimizations implemented recursive lrpd test rd sw flavors 
applied optimization techniques reduce run time overhead checkpointing reduce load imbalance caused block scheduling parallelized irregular loops 
previously mentioned block scheduling requirement case constant fraction original completed speculative parallelization realistic number iterations processor assigned varies speculative parallelization 
adaptive adaptive adaptive adaptive time seconds model breakdown processors speculative loop synchronization overhead redistribution overhead lrpd stage lrpd stage lrpd stage lrpd stage execution time breakdown strategies lrpd stage redistribution model time completion processors adaptive time completion strategies selection strategy rd re execution technique 
lrpd test load balancing important issue 
implementation code transformations done run time pass polaris automatically apply simple lrpd test additional manually inserted code commit phase execution loop shown fig 

applied technique important loops track perfect code 
remainder section optimizations effective reducing run time overhead technique 
demand checkpointing commit section mentioned need optimize checkpointing approximatively proportional working set loop 
stage test find contiguous number processors processors executing contiguous block iterations executed uncovering dependences remainder block processors re execute 
need save data residing shared arrays modified speculative execution 
types shared variables variables test compiler analyze variables proven compiler independent accessed iteration processor read privatizable 
saving state preserving safe state done ways write un committed private storage commit copying shared area delete 
copy data may modified speculative loop safe memory storage delete commit results speculation copy back original variables case restore state 
copy copy mechanism copying safe area done ways speculative loop entire working set loop saved copied ii demand loop execution 
performing activity loop adds critical path length program case sparse patterns generates consumes memory necessary 
fully parallel operation cost small block copy 
demand strategy advantages performs copy operations needed sparse codes orders magnitude wholesale approach 
done loop execution may add critical path program due exploitation low level parallelism 
operation initiated separately may guarded 
need save data copy write read 
accomplish access filter distinguish variables test variables analyzed compiler shadowed execution shared variables analyzed statically 
variables independent ones need attention read privatized variables don modify state don need restored 
independent variable location iteration processor location extracted compiler 
referenced filter generated compiler peeling case nested loops guard simple shadow tag 
code statement distinct iteration filter trivial 
commit restoration phase needed analysis region stage lrpd test depends strategy checkpointing 
committing data need copy value written sequential semantics 
independent arrays test accomplished compiler generated loop case copy simply deleting corresponding saved data wholesale copy loop strategy 
experiments shown section implemented demand copy value arrays test demand checkpointing release back storage commit phase proved cost effective application studied 
shadow data structures access pattern dense shadows arrays test implemented shadow arrays far cost effective solution 
case sparse applications spice shadow arrays viable 
spice effectively memory management large static array shadow array imply analysis phase traverse entire space program 
shadows large amount memory 
solution problem shadow hash tables 
implemented private processor hash tables hash function 
compact entire access pattern keep algorithm scalable data size number processors 
cost scalability data access expensive 
implementation especially reduction optimizations explained detail 
feedback guided load balancing drawbacks lrpd test requirement speculative loop needs statically block scheduled order commit partial 
due fact target techniques irregular codes load balancing pose performance problems 
independently developed implemented new technique similar adapts size blocks iterations assigned processor load balancing achieved stage lrpd test 
briefly technique works 
instantiation loop measure execution time iteration 
loop finishes compute prefix sums total execution time loop ideal perfect balance execution time processor average execution time processor total time number processors 
prefix sums compute block distribution iterations achieved perfect load balance 
save result order predictor instantiation loop 
iteration space changes instantiation scale block distribution accordingly 
implementation simple instrument loop low overhead timers parallel prefix routine compute iteration assignments processors 
near improve technique higher order derivatives better predict trends distribution execution time iterations 
overhead technique relatively small decreased 
advantage method tendency preserve locality 
experimental results experimental test bed processor hp system running 
gb main memory mb single level caches 
applied techniques important loops track perfect code spice perfect spec code fma spec code 
codes exception loop spice instrumented run time pass developed polaris infra structure 
track missile tracking code simulates capability tracking sites simultaneously 
main loops program subroutine extend 
account sequential execution time modified original inputs small meaningful measurement 
created input files vary degree parallelism loops 
loops study instantiated times 
better gauge obtained speedups define measure parallelism available loop life program parallelism ratio pr total number instantiations total number restarts total number instantiations example fully parallel loop pr partially parallel loop pr 
case strategy fully sequential loop pr rd case lower 
briefly analyze loop subroutine show effect various optimizations applied 

compiler un analyzable array cause dependences 
write guarded loop variant condition 
shadowed marked rules previously explained 
dependences short distance 
fig 
presents effect input sets resulting parallelism ratio pr number processors varied 
important pr dependent number processors interprocessor dependences affect number restarts stages lrpd test 
furthermore feedback guided scheduling performed length iteration blocks assigned processors variable lead variable pr 
fig 
shows best obtained speedups optimizations turned tested input sets 
speedup numbers include associated overhead 
figures importance optimizations quality parallelization 
fig 
compares execution time breakdown method checkpointing done speculative loop demand speculative loop 
quite obvious demand strategy generates overhead drastically reduces execution time 
fig 
compares execution time processor iteration space equally distributed processors time processor feedback guided scheduling employed 
clearly see loop balancing technique flattens execution profile balances irregular loop 
fig 
compares effectiveness various optimization techniques 
input set moderate number dependences uncovered independent number processors experiment 
clearly due large state loop conditional modification important optimization 
load balancing technique important redistribution rd 
rd vs strategy lesser impact processors 
figs show sw rd strategies useful 
depends actual dependence structure loop 
pr obtained technique quite different 
effect obtained speedup bit skewed due different levels optimizations able obtain various methods 
mentioned scaling analysis phase sw technique possible 
graphs show pr speedup varies window size 
adjust adaptive manner particular problem previous instantiations loop 
ideally want largest window size minimum number failures restarts 
extend 
loop independent array 
reads data read part array writes arrays extended iteration 
extends temporary manner slot 
loop variant condition materialize newly created slot track re overwritten iteration 
implies element track arrays needs privatized 
arrays indexed counter incremented conditionally 
fact conditionally incremented induction variable closed form 
index arrays data dependence analysis difficult 
pre computed loop distribution guarding condition loop variant 
solution speculatively processors compute zero offset 
time shadow arrays question collect ranges 
parallel execution obtain processor offsets induction variable prefix sums show read array intersect writes maximum read index minimum write occurs iteration 
write indexed higher strictly monotonic induction variable 
second doall repeat execution correct offsets 
value assignment commits arrays shared storage 
implementation process loop value needs committed computing actual values induction variables 
figures show pr best obtained speedup inputs 
obtain speedup obtainable hand parallelization compiler recognize deal conditional induction variables 

loop similar simpler extend 
array test read front section conditionally extended appending new element 
array test privatized copy value method shadowed 
stage approach extend employed 
figures show pr best obtained speedup inputs 
program track 
execution profile entire track code different input sets fig 
shows input sensitive program regardless input execution time spent previously discussed loops 
speedup input shown fig 
scalable quite impressive especially fact preliminary results minimal compiler support 
spice known circuit simulator spends time distinct loops loop subroutine implements sparse solver decomposition part similar loops subroutine load bjt update matrix circuit current evaluation device models 
arrays compiler analyzed large array value working space program multiple levels indirection 
total workspace aliasing problem 
parallelization profitable chosen larger input decks ones available original perfect spec codes circuit bit adder bjt technology scaled input deck circuit perfect codes 
parallelized important loops spice loops subroutine main loop bjt similar loops called model evaluation routine load 
main loop bjt previously parallelized doall followed cross processor reduction speculatively distributing linked list traversal circuit topology sparse version lrpd test 
detailed description technique available 
loop fully parallel premature exit parallelized techniques described 
loop lu decomposition partially parallel due sparse nature circuit topology 
employ sparse version lrpd test extract data dependence graph 
ddg schedule iterations wavefronts 
schedule reused execution program access pattern change fully amortizes initial cost dependence graph generation 
adder input deck parallelized loop iterations critical path length number wavefronts 
believe improve speedup numbers employing better scheduling technique wavefronts 
input decks studied similar characteristics 
fig 
fig 
shows obtained speedup loop obtained speedup entire code studied input decks 
fma finite element method computer program designed simulate inelastic transient dynamic response dimensional solids structures subjected suddenly applied loads 
important loop accounting sequential execution time contains array stress state arrays indirection call graph levels deep 
complexity quad loop statically un analyzable theoretically loop statically parallelized input independent information needed compiler available compile time 
turns loop fully parallel lrpd test stage 
fig 

shows speedup loop 
shown exploit parallelism loops fully parallel parallelized compile time analysis original lrpd test 
shown overcome overheads associated method 
optimizations general applicability load balancing irregular applications checkpointing various applications 
shown lrpd test build precise dd graphs loops proper inspector extracted 
experimental results confirm power method 
charmm program macromolecular energy minimization dynamics calculations 
journal computational chemistry 
blume eigenmann lawrence lee padua paek pottenger rauchwerger tu 
advanced program restructuring high performance computers polaris 
ieee computer december 
mark bull 
feedback guided dynamic loop scheduling algorithms experiments 
europar september 
de 
optimizations enabling code generation hp class 
master thesis texas university department computer science august 
lilja 
coarse grained speculative execution shared memory multiprocessors 
proceedings th acm international conference supercomputing pages july 
leung zahorjan 
improving performance runtime parallelization 
th ppopp pages may 
li 
array privatization parallel execution loops 
proceedings th international symposium computer architecture pages 
frisch 
gaussian revision 
gaussian pittsburgh pa 
maydan amarasinghe lam 
data dependence data flow analysis arrays 
proceedings th workshop programming languages compilers parallel computing august 
laurence nagel 
spice computer program simulate semiconductor circuits 
phd thesis university california may 
padua wolfe 
advanced compiler optimizations supercomputers 
communications acm december 
rauchwerger amato padua 
scalable method run time loop parallelization 
int 
paral 
prog july 
lawrence rauchwerger david padua 
lrpd test speculative run time parallelization loops privatization reduction parallelization 
ieee transactions parallel distributed systems 
lawrence rauchwerger david padua 
parallelizing loops multiprocessor systems 
proceedings th international parallel processing symposium april 
saltz crowley 
run time parallelization scheduling loops 
ieee trans 
comput may 
tu padua 
automatic array privatization 
proceedings th annual workshop languages compilers parallel computing portland august 
robert bruce 
dyna nonlinear explicit dimensional finite element code solid structural mechanics 
lawrence livermore national laboratory november 
wolfe 
optimizing compilers supercomputers 
mit press boston ma 
yu rauchwerger 
adaptive reduction parallelization 
proceedings th acm international conference supercomputing santa fe nm may 
hao yu rauchwerger 
run time parallelization overhead reduction techniques 
proc 
th international conference compiler construction cc berlin germany 
lecture notes computer science springer verlag march 
zhu yew 
scheme enforce data dependence large multiprocessor systems 
ieee trans 
softw 
eng june 
processors parallelism ratio parallelism ratio optimizations processors speedup speedup optimizations parallelism ratio different inputs 
obtained speedups 
processors parallelism ratio parallelism ratio input sliding window sliding window processors speedup vs sliding window input sliding window sliding window sliding window window sizes vs rd strategy 
input processors parallelism ratio parallelism ratio input sliding window sliding window processors speedup vs sliding window input sliding window sliding window sliding window window sizes vs rd strategy 
input sequential processor processors processors processors processors processors time seconds time breakdown loop time overhead time checkpoint time sequential processor processors processors processors processors processors time seconds time breakdown demand checkpointing loop time overhead time checkpoint time checkpointing loop vs demand 
processor time seconds execution time processor instantiation fb fb processors speedup speedup optimization contribution input optimizations fb rd rd fb rd fb odc feedback guided load balancing optimization contribution 
processors parallelism ratio extend parallelism ratio feedback guided block scheduling redistribution processors speedup extend speedup feedback guided block scheduling redistribution extend parallelism ratio speedup 
processors parallelism ratio parallelism ratio feedback guided block scheduling redistribution processors speedup speedup feedback guided block scheduling redistribution parallelism ratio speedup 
processors speedup track program speedup optimizations input percent time track input profile extend track speedup execution profile entire program number processors speedup spice bit adder bjt number processors speedup spice ext 
perfect bjt spice speedup important loops entire program adder extended perfect input decks 
number processors speedup spice bit adder bjt processors fma quadrilateral loop speedup speedup spice important loops entire program adder fma quadrilateral loop 
