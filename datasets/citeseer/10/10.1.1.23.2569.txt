online prediction experts log scoring rule online expert prediction clarke dawid current draft summary identify class problems log score function bayesian context 
show bayesian mixture density marginal density data approximate solution asymptotically valid members class 
part identify members class objective functions argue close conceptually related 
version canonical invariance sense 
generally show senses various objective functions class optimal values resulting solutions close 
addition characterize di erence shtarkov solutions bayesian mixture 
keywords online prediction shtarkov optimality mixture density ams subject classi cations primary secondary 
formalism imagine sequence outcomes possibly nite 
outcomes members sample space take discrete interval real line 
goal identify stochastic process 
satis es optimality criterion logarithmic scoring rule parametric family 
assume index parameter space nite dimensional open subset ir xj stochastic process means write mean random variable outcomes density result integrating write mean je reys prior parameter space assuming exists prior density parameter space 
dominating measure priors stated 
denote set strings disjoint union sets vectors ranges empty string 
denote arbitrary member leaving unspeci ed 
random variables upper case outcomes lower case 
general write mean maximum likelihood estimator mle mean mode posterior maximum posterior likelihood estimate mple arg sup xj 
information individual outcome denoted fisher information sample size ni 
empirical fisher information log possible evaluate 
note divided sample size assume converges necessarily 
just convenient leave appear asymptotic expressions 
quantity de ned generally je reys prior 
shtarkov rissanen barron xie identify distribution achieves inf sup sup log xj dx normalized likelihood evaluated mle exists 
regard member class criteria 
arbitrary positive function parameter arbitrary positive function density respect je reys prior density respect dominating measure say fold sample space consider general criterion form inf sup sup log xj seen reduces choosing 
letting remain arbitrary choosing leads bayesian criterion optimizing inf sup sup log xj alternatively reduces appropriate rede nition 
absorbing writing xj dx xj xj xj dx rewrite general expression log xj log xj formally expression shtarkov optimised 
optimum xj xj dx xj xj dx exists mle satisfying xj 
uses de nitions xj dx xj xj xj dx gets bayesian form 
optimum bayesian case taken 
key motivation stochastic complexity model selection minimum description length 
extension shtarkov criterion meaningful decision theoretic reasons 
context chosen nature learner chooses wishes minimise regret having learner guided experts indexed incur penalties xj represents penalty nature selecting sequences dicult experts match 
penalty absorbed 
desired reduction 
develop interpretation section 
addition normalised likelihood evaluated mle normalised joint likelihood evaluated mple quantity arises obtaining bounds asymptotic value mixture respect prior write xj dx note posterior jx satis es jx xj 
see appears multiply divide get inf sup sup log jx inf sup sup log xj inf sup log jx replaced 
seen second factor goes constant solution optimisation problem mixture asymptotically 
argue tractable quantity mathematical analysis fully online 
desirable invariance properties 
identify forms closely related 
suitable choice version set suitable choice seen forms solutions close formal sense 
seen mixture densities perturbations arise naturally bounds barron xie provide asymptotics discrete response setting 
show optimal choice modi ed je reys prior verify setting modi cation mixture respect je reys prior converges unmodi ed mixture respect je reys prior 
contrasts rissanen examined asymptotic form xj dx directly 
answers similar asymptotic sense 
argue natural solution attention focus shtarkov solutions stochastic processes 
mixture density course stochastic process optimum class stochastic processes proper subset collection distributions sample size argue shtarkov solutions best regarded approximations converge certain sense reasonable conditions 
asymptotically fact optimal larger class 
contributions interest nite sample behaviour objective function 
addition consistent prequential online paradigm want leave unspeci ed supremum taken set set want restrict density stochastic process 
technical reasons forced approximate suprema limits suprema hope go far wrong 
section motivating analysis bernoulli example turn discrete parameter case ine bounds 
section motivates criteria chosen study justi es forms criteria take canonical 
investigate conditions equivalence criteria solutions 
section gives formal analysis 
section discusses results various contexts 

examples motivation reviewing discrete bernoulli process example recur sections 
state prove result discrete parameter case 
describe game theoretic approach barron xie give general bounds online case ine case 
examine exponential family examples ine case 
bernoulli example consider bernoulli process xj binary string online condition inf sup sup xj empirical fisher information evaluated mle outer supremum varies set distributions set stochastic processes expression expression bernoulli 
shtarkov result optimal choice 
terms sum zero 
interested limit increases 
assume go nity 
deal cases doesn happen section re interpret sup lim sup means want reinterpret inf lim sup nq lim inf qn sup nq inequality follows interaction mum limit denote expression square brackets mum left stochastic processes mum right distributions stochastic processes inf limit sides preserves inequality 
resulting right hand side independent take mum left distributions lower bound left hand side varies set stochastic processes proper subset set distributions 
identi ed shtarkov expression right hand side lim 
uniform stirling approximation argument shows limit 
see abramowitz stegun 
see asymptotic upper bound consider mixture xj 
left hand side gives lim upper bound stirling approximation argument 
mixture stochastic process shtarkov solution 
consequently show unique asymptotic solution 
comment implicitly choosing conjugate prior beta permits get result explicitly similar argument 
note mixture arises obtaining upper bound shtarkov solution arises lower bound 
countably parameter values 
second example see mixture arises naturally consider case countably parameter values 
consider criterion inf lim sup sup log xj discrete prior density respect counting measure probability denoted assumes values 
proposition 
suppose statistic asymptotic distributions concentrate disjoint sets asymptotically mixture xj unique stochastic process achieving 
proof denote stochastic process optimises 
set minimum taken includes upper bound putting xj xj bounded zero xj 
hypothesis statistic discriminates asymptotically parameter values sense sequence real number sets fx xj disjoint union space 
note assumed uniformity 
ease exposition denoted statistic mean taken parameter indexing asymptotic distribution population mean 
suppose choose lower bound aja aja derive analogous upper bound proceed follows 
note 
lower bound disjointness union disjoint 
putting inequalities get aja aja putting aja aja consider set write 
considering large nite collection nd xed jm 
converge probability 
comment wants stronger sense converge instance sup jm impose stronger conditions 
argument proof breaks continuous parameter case extra conditions nd countable collection sets uncountably parameters satisfy criteria sets instance clear nd general countably sets satisfy relation ine case essential online quantity derived inf sup log xj inf sup sup log xj assumed stochastic process supremum de ned 
assume part speci cation problem absorbed 
return point section 
extending reasoning barron xie develop bounds right hand side follows 
denoting stochastic process sup sup log xj inf sup sup log xj sup inf sup log xj sup sup inf log xj sup sup log xj sup em log xj sup log log dx log dx chain inequalities rst follows upper bounding mum particular choice follows minimax maximin outer operations 
lower bound follows fact averages suprema inner operations 
expression follows information inequality rearranged show minimised expression follows lower bound supremum choosing element supremum taken case lower bound follows multiplying dividing xj logarithm separating terms 
lower bound ine replace quantities ine 
note convergence dependent stopping rule game 
rst bounds give bounds 
results techniques extensions techniques clarke barron conjecture asymptotic form iid case 
deal represent permit modi cation 
change inequalities 
results barron xie mind try 
want rst term converge increases 
taylor expanding inner integral second term tend half expectation random variable log cancel part term 
want convergence uniform 
third expression shannon mutual information equipped density conditional density xj 
asymptotically converges log jj constant relative entropy dim je reys prior exists see clarke barron 
want take supremum clear outside particular examples 
expect supremum match upper bound 
deal consider laplace method approach dimensional case simplicity 
mixture respect prior density ji fx fx andj restriction implicit de nition similar restriction fisher information condition rissanen 
restriction implicit de nition similar restriction condition rissanen 
restriction implicit de nition similar parts conditions rissanen 
practice condition means get constant nearly constant fisher information truncated natural parameter space compact set dealing bounded random variables 
upper bound sup log inf log inf log arises taylor expansion log 
continuity gives open ball centred radius mum 
implies small provided bounded away zero 
bounded inf log seen section normal integrals 
large increase domain integration real line 
gives new expression sup log neglecting restriction compare lower bound asymptotic form putative upper bound 
aside error terms assume small log common upper bound minus lower bound roughly sup log jj constant constant comes normalizing root determinant fisher information give je reys prior 
clearly choose expect go zero asymptotically 
comment consistent barron xie obtained je reys prior optimality criterion clarke barron obtained je reys prior relative entropy averaged form optimality criterion 
boundary points parameter space identi ed pose problem laplace method values boundary merely replace smaller ball entirely inside parameter space 
examples give upper bounds examples normal exponential distributions 
suggested section upper lower bounds tight 
investigate lower bounds apart heuristics 
upper bound look treatment left hand side section restriction seen relatively harmless cases 
consider normal mean 
suppose iid normal 
empirical fisher information log divided seen logarithm longer appear 
left hand side essentially inf sup log upper bounds try gives inf sup log nd inf sup log normal denominator mixture normal densities indicated 
heuristically increases converges inf sup log xed value denominator gets closer 
expression forces choose di erent leads value sup strictly greater 
recall examination lower bound led trying choose je reys prior 
second example consider exponential distribution natural parameterization 
consider sup log xj sup log 
consider class prior densities de ned zero normalizing constant 
note family prior densities contains version je reys prior limit point 
suppose write nn approximation follows stirling formula 
gives sup log xj sup log ke nn log sup log log sum terms goes zero log 
minimise want choose possible collection priors 
general nite extreme sensitivity logarithm tail behaviour 
recalling converges true value conjecture emerge quantity characterising limit prior form mixture 

class criteria examine properties motivation 
advocate speci empirical form related theoretical forms choosing speci examine properties objective function optimal values solutions generates 
choice criterion form section regard class experts indexed 
experts continuous somewhat counterintuitive reasonable assumption opinions experts drawn continuous set imagine expert holding opinion 
consider sup log xj log xj want choose small 
easier values 
instance xj independent experts equivalent 
dependence xj increases diculty choosing increases follows peakedness xj increases giving narrower range experts perform nearly best expert 
essentially ect fisher information 
length increases ect strengthens ect compensate di erential discrimination experts increase longer introduce factor 
choose experts 
want smaller xj peaked function xed larger xj peaked 
worse experts smaller xj better better experts larger xj worse 
want ensure compensation ect reverse better worse merely decreases di erential 
consider procedure 
de ne subset log xj log xj measure peakedness probability function shortly choose relate choice prior actual data 
need require probability sensitive location best expert 
instance probability ruled unbounded collection experts 
seen xed choice assigns mass dependent peakedness xj sharp peak high fisher information near relatively small peak spread lower fisher near set relatively larger making larger 
ect choose adjust xj seek strategy achieving inf lim sup sup log xj eliminate dependence looking limiting behaviour 
alternatively may set ask choose choosing 
consistent density see sense close consider ni small density approximately inf lim sup log xj regard fundamental online quantity examine 
note set get obtain choice quantity closely related 
empirical form leads choose get case choosing mimic ect seen intuitively close theoretical forms empirical form close theoretical forms revealing fundamental similarity 
addition discrete typically take 
approach uni es discrete continuous cases 
propose related forms central form class 
justify choice mimics set depends sample space parameter space 
invariance provides separate justi cation explain summarising results 
theorem 
solution inf lim sup sup log xj xj xj dx exists denominator nite 
solution xj xj dx exists nite measure 
solution xj xj dx exists denominator nite 
criterion solution close criterion solution turn close criterion solution 
log xj log xj log xj probability sense absolute value di erence logarithms 
log 
proof solutions claimed exist clear optimal considering log density ration density gives larger value see barron xie shtarkov 
normalizing constant solution dw dx xj bounded xj gives upper bound nite 
criteria close noting taylor expansion evaluate integral de ning gives approximately seen empirical forms fisher information converge limits xed sample size stopping time 
wilks theorem ensures di erences criteria bounded probability examines di erence second third expression 
see solutions close sense consider log log xj xj log xj dx xj dx empirical fisher informations converge probability rst term tends zero 
second term goes zero approximation improves provided integrals exist see theorem rissanen remarks expression 
cases similar 
note invariant criterion 
fisher information invariant transformations parameter space proxy fisher information depends data 
empirical quantities numerator involve mle invariant transformation parameter space quantities densities respect dominating measures invariant transformations underlying measure space 
consequently senses sought got invariance transformations sample space parameter space 
game theoretic interpretation explicit consider player game played learner nature presence experts indexed learner chooses nature produces learner reward penalty log 
reward th expert log xj prior handicap th expert 
learner regret di erence reward reward best expert got sup log xj log exp 
malign nature choose maximise regret worst regret nature produce result supremum case learner best strategy minimises maximal regret best expert 
leads expression inf sup sup log xj necessarily require density get invariance sensitivity location best performing expert indexed mle derivation regret means learner trying perform best expert 
comment appearance discrimination experts regarded penalty nature nature wants mess learner experts choosing dicult experts sense low fisher information hard discriminate various values nature pay price 
price low contribution regret learner experts relatively low 
conclude subsection reiterate identifying canonical empirical representative class invariant parameter space due invariance mle 
invariant transformations sample space 
online criterion 
meaningful game theory interpretation seen empirical fisher information je reys prior dominating measure prior 
written prior jj point invariance gives set invariance similarly density respect dominating measure seen invariant transformations long transform densities 
justi cations mathematical analysis section 
prefer analog theoretical examination section instance 
equivalence criteria examine di erent results di erent members class 
actual objective function want examine log xj suppressed 
interested optimal values inf sup sup solutions optimization problem densities satisfy arg particular interested aspects robustness density inputs wish argue mixture densities respect prior emerges limit optima increases 
parametric family assume absorbed bayesian analog shtarkov theorem see shtarkov 
change replacing mle mple 
statement modi ed barron xie proof extends case 
proposition 
minimax regret equals maximin regret sup inf sup log xj dx equals log xj unique minimax strategy xj xj dx equalizer rule unique favourable maximin distribution 
proof see barron xie 
expression solution studied rissanen 
barron xie part technique give asymptotics minimax regret 
suggests mixture density limiting form 
assess close quantities de ne distance measure 
expressions natural distance de ne functions domain range sup log log metric continuous implies sup log xj log xj write sup log xj sup log xj log sup log xj take inf sides preserve inequalities 
jg note particularly strong metric stronger convergence distribution instance supremum tail behaviour 
importantly want verify solutions depend continuously inputs shtarkov solutions mixture converge 
unable general convergence distribution metrizable 
recall clarke barron prop 
mixture xj asymptotically close 
proposition modify conditions mle 
gives log xj det log probability 
modifying proof rissanen expressions terms mple mle gives conditions ensuring log xj dx log log ji jw clearly controls far apart mixture distributions general controls far apart shtarkov solutions 
want give conditions shtarkov solutions corresponding di erent criteria form close want log xj xj log xj dx xj dx small uniformly subscript indicate di erent versions 
absorbing second term gives di erent models apply 
ect form prior respect fisher information integrated right 
involve fisher information ected 
seen second term goes zero 
rst term numerator denominator absorbing noting probabilities holds longer 
mutually dominating problem 
fisher informations unchanged implies rst term converges probability di erence logarithms mixtures models respect priors 
comment di erence mixtures priors appear 
control far apart shtarkov solution mixture solution 
multiply divide logarithm get absolute value log xj xj dx log xj dx ji log ji det log goes zero probability 
uses stronger mode convergence second term goes zero 
third term goes zero shtarkov solution mixture solution converge probability 
note em converges numerator ji various regularity conditions quantity converge probability 
converges mean probability de nes mode convergence 
seen constant fisher information methods asymptotically indistinguishable probability 
general log xj xj dx log om asymptotic di erence shtarkov mixture solutions 

criterion solutions section examine deriving sucient conditions mixture densities asymptotic solution 
verify asymptotic conditions satis ed smooth parametric families satisfying various regularity conditions 
set consider empirical form criterion write sup jx 
gives inf sup log give sucient conditions asymptotic value dimension achieved mixture 
theorem 
suppose value set de ned 
lim 
kg probability mixture 
lim sup 
kg inf lim sup 


uniquely achieved value proof putting see upper bound 

martingale positive mean 
probability 
lim 


de ne lim 



consider event stochastic process dm 
see di erent get value greater see uniqueness consider set 

lim sup 


lim 



optimal 
see conditions vacuous consider bernoulli process 


suppose suppose contains exactly ones entries zero 
includes ones zeroes sr ro ro 
sr ro ro 

ro ro 
indicates empirical fisher information function 
ro ro gives lim 
similarly contains exactly zeroes choose set sequences zeroes ones 
case see section 
means uniquely achieves inf sup lim sup 


value 
theorem generally give upper lower bounds 
ensure set theorem probability note identity jx jx log xj log integrating rearranging dividing gives ji log xj log xj integral denominator set laplace method 
lower bound denominator 
denominator log xj xj xj drop second term get lower bound 
point arising taylor expansion 
fact log product sum logs factors local suprema get new lower bound sup log sup log xj 
add subtract integral complement 
gives sup log sup sup upper bound integral sup sup sup sup sup sup performing normal integration gives lower bound sup log sup sup sup sup sup log sup seen upper bound sup sup log sup clearly factors expected go 
develop analogous lower bound see bounds asymptotically tight 
upper bound denominator assuming lower bound subsection adapt technique proof dawid get lower bound error term 
clearly error term xj xj admits upper bound useful lower bound 
exponential families assume fisher information bounded 
comment upper bound generally parametric families exponential form possibly cases fisher information goes zero slowly 
require techniques introduce probabilities dependent parameter wish avoid remain strictly prequential bayesian framework 
brie address case subsection 
write xj nb normalizing factor 
log xj xj clearly 
mle get likelihood equation 
analogously mple get 
proposition 
sup fj proof de nition mle mle set supremum taken get evaluated mle rst case 
close supremum occur point closest 
boundary points indicated 
comment second cases evaluation boundary point typical typically close provided invertible 
taylor expanding evaluating get point 
proposition 
rst case proposition holds get bound sup 
xj value 

proof rst case holds proposition evaluate 
line joining 
analogous result holds 
rearranging inequality gives proposition provided fisher information bounded away zero 
proposition putting result error term integrating respect prior gives 
bound error term 
turn gives lower bound local argument subsection adapted give upper bound analogous form replace suprema ma take trivial upper bounds analog factor 
upper bound denominator bounded deal cases bounded options 
fisher information go zero may constant 
retain natural parametrization write error term xj xj xj xj annulus centred inner radius outer radius compactness cover nitely open sets form 
upper bounding rst term sum integrals sets multiplying dividing xj integrand taylor expanding permits get bounds sum form 
increase slowly retain convergence zero rst term 
second term exponential family assumption express convergence zero log log 
write rst factor continuous arguments bounded compact sets assume exponent lower bounded longer dependent take constant small positive 
want increases slowly 
want characterize pairs holds 
bernoulli revisited brie revisit bernoulli case examined section 
assumed went nity 
suppose go nity xed strictly positive 
loglikelihood get log log log second term approximated large 
denote modi ed log likelihood 

section note function smooth quadratic nature spike locally maximum widely approximate rs 
posterior normal jx jx jx 
jx sr asymptotically xed sequences deal zeroes ones 
criterion really xj xj xj string containing zeroes xj xj 
suppose continuous 


criterion sequence 
put sequences set de ned theorem 
optimality sequences 

intended show mixture densities respect prior particular je reys prior right action take log scoring rule 
shown aitchison see clarke barron relative entropy expected version log scoring rule 
general case limit large unsatisfying may necessary get formal results really want permit nite strings 
key distinction online ine case 
log scoring rule online relative entropy involves expectation sample space deals possible strings data obtain 
represented attention admit game nite sample go inde nitely 
fully online case mean version similar entirely empirical nite unknown expectations taken quantities estimated ones problem 
fully ine case means version similar close theoretical population possible include expectations sample spaces may take limits quantities depend true population 
stopping rules included provide nearly continuous spectrum quantities purely ine purely online depending stopping rule instance may converge di erent limits probability 
regard online ine distinction fundamental general bayesian frequentist distinction 
bayesian paradigm largely online due conditioning data 
frequentist paradigm largely ine due assumptions sample spaces leading sampling distribution 
contrast prequential approach combines online prediction ine assessment performance requires independent 
note prediction incorporates conventional model selection parameter estimation 
see dawid compatibility bayesian frequentist approaches 
sense prequential paradigm surpasses bayes frequentist paradigms 
investigated bernoulli example verify possible get results pointwise sense 
special cases unambiguously identify sets points sample space certain behaviour observed 
point get past probability admitting modes convergence probabilities really probabilities de ne sets certain behaviour observed 
just better deal sets points directly 
set assigned di erent probabilities di erent probability measures set need probability order de ned 
results rissanen pointwise sample space generally stronger results stated mixture uses convergence distribution 
decades statistical thought incorporated aspects stochastic complexity 
intended contribution overlap elds 
coding model implications results variants rissanen barron xie informally state general points follow results 
implication coding shtarkov mixture bayes solutions close constant set probability going 
bayes code je reys perfect code shtarkov relative entropy 
implication model selection model selection principle relative entropy model selection principle shtarkov optimization equivalent asymptotic sense 
similar bic 
equivalence class model selection procedures 
suggest concepts complexity grounded probability theory lead naturally online pointwise examination sample space 
statistical application evaluation analyses introduce ine probability 
technique assessment performance independent 
short prequential paradigm sensible way forward 
abramowitz stegun 

handbook mathematical functions 
national bureau standards applied mathematics series 
washington dc nat 
bur 
stand 
aitchison 
goodness prediction biometrika vol 
dec 
barron xie 

asymptotic minimax regret data compression gambling prediction submitted ieee trans 
inform 
theory clarke barron 

information theoretic asymptotics bayes methods technical report department statistics university illinois 
clarke barron 

information theoretic asymptotics bayes methods ieee trans 
inform 
theory 
clarke barron 

je reys prior asymptotically favourable entropy risk statist 
planning inference 
dawid 

strong asymptotic normality posterior distribution parameter exponential family unpublished manuscript 
rissanen 
fisher information stochastic complexity ieee trans 
inform 
theory vol 
jan 
shtarkov yu 


universal sequential coding single messages problems information transmission vol 
pp 

dawid 

ecient point prediction systems statist 
soc 
ser 
vol 


