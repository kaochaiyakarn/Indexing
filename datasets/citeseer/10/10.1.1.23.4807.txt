appear psychological review 
understanding normal impaired word reading computational principles quasi regular domains david plaut james mcclelland carnegie mellon university carnegie mellon university center neural basis cognition center neural basis cognition mark seidenberg patterson neuroscience program mrc applied psychology unit university southern california develop connectionist approach processing quasi regular domains exemplified english word reading 
consideration shortcomings previous implementation seidenberg mcclelland psych 
rev reading nonwords leads development orthographic phonological representations capture better relevant structure written spoken forms words 
number simulation experiments networks new representations learn read regular exception words including low frequency exception words able read nonwords skilled readers 
mathematical analysis effects word frequency spelling sound consistency related simpler system serves clarify close relationship factors influencing naming latencies 
insights subsequent simulations including attractor network reproduces naming latency data directly time settle response 
analyses network ability reproduce data impaired reading surface dyslexia support view reading system incorporates graded division labor semantic phonological processes 
view consistent general seidenberg mcclelland framework similarities important differences standard dual route account 
aspects language characterized relationship inputs outputs systematic admits exceptions 
task mapping written spoken forms english words 
words regular gave mint pronunciations adhere standard spelling sound correspondences 
irregular exception words pint pronunciations violate standard correspondences 
matters worse spelling patterns range pronunciations clearly town brown research supported financially national institute men tal health mh mh national institute aging ag national science foundation asc mcdonnell pew program cognitive neuroscience 
behrmann derek besner max coltheart joe devlin geoff hinton strain helpful discussions comments 
acknowledge derek besner max coltheart michael mccloskey directing attention issues addressed 
correspondence concerning sent dr david plaut department psychology carnegie mellon university pittsburgh pa plaut cmu edu 
crown vs known shown grown thrown cough rough 
face complexity skilled readers pronounce written words quickly accurately knowledge spelling sound correspondences read nonwords rint 
important debate cognitive psychology best characterize knowledge processing quasi regular domains order account human language performance 
view pinker systematic aspects language represented processed form explicit set rules 
rule approach considerable intuitive appeal human language behavior characterized broad scale terms rules 
provides straightforward account language knowledge applied productively novel items fodor pylyshyn 
illustrated domains partially systematic accordingly separate mechanism required handle exceptions 
distinction rule mechanism exception mechanism operating fundamentally different principles forms central tenet called dual route theories language 
alternative view comes research connectionist parallel distributed processing networks computation takes form cooperative competitive interactions large numbers simple neuron processing units mcclelland rumelhart pdp research group rumelhart mcclelland pdp research group 
systems learn adjusting weights connections units way sensitive statistical structure environment influences behavior network 
result sharp dichotomy items obey rules items 
items coexist single system representations processing reflect relative degree consistency mappings different items 
connectionist approach particularly appropriate capturing rapid online nature language specifying processes learned implemented brain understanding normal impaired word reading somewhat level koch churchland discussion 
fundamentally connectionist modeling provides rich set general computational principles lead new useful ways thinking human performance quasi regular domains 
initial debate views language system focused relatively constrained domain english inflectional morphology specifically forming past tense verbs 
past tense formation simple quasi regular task single regular rule add ed walk walked exceptions grouped clusters similar items undergo similar change sing sang drink small number high frequency arbitrary forms go went 
rumelhart mcclelland attempted reformulate issue away sharp dichotomy explicit rules exceptions view emphasizes graded structure relating verbs inflections 
developed connectionist model learned direct association phonology types verb stems phonology past tense forms 
pinker prince pointed numerous deficiencies model actual performance specific assumptions argued generally applicability connectionist mechanisms language fundamentally limited see fodor pylyshyn 
specific limitations rumelhart mcclelland model addressed subsequent simulation cottrell plunkett seidenberg macwhinney plunkett 
possibility remains strong connectionist model provide full account inflection 
furthermore applications aspects language disorders mcclelland language change hare elman press demonstrate ongoing extension approach account wider range language phenomena 
similar issues arise domain oral reading richer empirical database contact 
domain inflectional morphology researchers assume accounting wealth existing data normal impaired word reading requires postulating multiple mechanisms 
particular theorists besner smith coltheart coltheart curtis atkins haller coltheart marshall newcombe meyer morton patterson noel claimed pronouncing exception words requires lexical lookup mechanism separate sublexical spelling sound correspondence rules apply regular words nonwords see humphreys accompanying discussion properties dual route theories 
separation lexical sublexical procedures motivated primarily evidence independently impaired abnormal reading acquisition developmental dyslexia brain damage previously literate adult acquired dyslexia 
phonological read words nonwords appear selective impairment sublexical procedure surface read nonwords regularize exception words sew sue appear selective impairment lexical procedure 
seidenberg mcclelland sm challenged central claim dual route theories developing connectionist simulation learned map representations written forms words orthography representations spoken forms phonology 
network successfully regular exception words implementation separate mechanisms see seidenberg mcclelland demonstration point 
simulation put forward support general framework lexical processing orthographic phonological semantic information interact gradually settling best representations input see stone van orden van orden van orden pennington stone similar perspective word reading 
major strength approach provides natural account graded effects spelling sound consistency words glushko jared seidenberg consistency interacts word frequency andrews seidenberg seidenberg waters barnes tanenhaus taraban mcclelland waters seidenberg 
furthermore sm demonstrated versions model exhibit aspects developmental surface dyslexia patterson patterson seidenberg mc showed damaging normal model reproduce aspects acquired surface dyslexia 
sm model contributes broader enterprise connectionist modeling cognitive processes common set general computational principles applied successfully wide range cognitive domains 
sm serious empirical limitation undermines role establishing viable connectionist alternative dual route theories word reading particular providing satisfactory formulation nature knowledge processing quasi regular domains generally 
specifically implemented model significantly worse skilled readers pronouncing nonwords besner mccann 
limitation broad implications range empirical phenomena accounted model coltheart 
poor nonword reading exactly predicted dual route claim single system connectionist read exception words findings studies cast effects regularity consistency address distinction section 
understanding normal impaired word reading nonwords adequately 
interpretation model simply approximated lexical look procedure read regular exception words separately mastered sublexical rules necessary read nonwords 
alternative interpretation empirical shortcomings sm simulation stem specific aspects design inherent limitations abilities connectionist networks quasi regular domains 
particular seidenberg mcclelland suggested model nonword reading improved adversely affecting properties larger training corpus different orthographic phonological representations 
second limitation sm provide extensive examination underlying theoretical issues 
sm main emphasis demonstrating network operated fairly general connectionist principles account wide range empirical findings normal developmentally impaired reading 
relatively little attention paid articulating general principles evaluating relative importance 
underlying theoretical foundation remained implicit 
despite subsequent efforts explicating principles seidenberg remains considerable confusion regard role connectionist modeling contributing theory word reading cognitive process 
researchers forster mccloskey claimed sm demonstration impressive right extended understanding word reading operation model connectionist networks generally complex understand 
consequently connectionist networks viewed theories human cognitive functions simulations theories demonstrations specific theoretical points mccloskey see massaro olsen caramazza 
reject claim connectionist modeling see seidenberg bases analyzing understanding networks see hanson burr agree theoretical principles constructs developing connectionist explanations empirical phenomena need elaboration 
current develops connectionist account knowledge representation cognitive processing quasi regular domains specific context normal impaired word reading 
draws analysis strengths weaknesses sm dual aim providing adequate account relevant empirical phenomena articulating explicit formal manner theoretical principles underlie approach 
explore alternative representations regularities written spoken words explicit 
simulation experiment network new representations learns read regular exception words includ ing low frequency exception words able read nonwords skilled readers 
results open range possible architectures plausibly underlie human word reading 
mathematical analysis effects word frequency spelling sound consistency simpler related system serves clarify close relationship factors influencing naming latencies 
insights verified second simulation 
simulation develops attractor network reproduces naming latency data directly time settle response obviating need error proxy reaction time 
implication semantic contribution reading considered fourth final simulation context accounting impaired reading behavior acquired surface dyslexic patients brain damage 
damage attractor network provides limited account relevant phenomena better account provided performance network learns map orthography phonology context support semantics 
findings lead view reading system incorporates graded division labor semantic phonological processes 
view consistent general sm framework similarities important differences standard dual route account 
general discussion articulates differences clarifies implications current broader range empirical findings including raised coltheart 
challenges connectionist approach 
brief critique sm model try distinguish central computational properties central aspects design 
analysis representations leads design new representations employed series simulations analogous sm simulation 
seidenberg mcclelland model general framework seidenberg mcclelland general framework lexical processing shown 
orthographic phonological semantic information represented terms distributed patterns activity separate groups simple neuron processing units 
domain similar words represented similar patterns activity 
lexical tasks involve transformations representations example oral reading requires orthographic pattern word generate appropriate phonological pattern 
transformations accomplished cooperative competitive interactions units including additional hidden units mediate orthographic phonological semantic units 
unit interactions governed weighted connections collectively encode system knowledge different types information understanding normal impaired word reading context meaning orthography phonology mak 
seidenberg mcclelland general framework lexical processing 
oval represents group units arrow represents group connections 
implemented model shown bold 
adapted seidenberg mcclelland related 
specific values weights derived automatic learning procedure basis system exposure written words spoken words meanings 
sm framework broadly consistent general view information processing articulated mcclelland context grain networks 
networks embody general computational principles graded propagation activation builds gradually time 
random unit activations subject intrinsic tic variability 
adaptive system gradually improves perfor adjusting weights connections 
interactive information flows bidirectional manner groups units allowing activity levels constrain mutually consistent 
nonlinear unit outputs smooth nonlinear functions total inputs significantly extending computational power entire network purely linear networks 
acronym grain intended convey notion cognitive processes expressed finer grain analysis terms interacting groups neuron units typical box arrow information processing models 
computational principles central sm framework captured acronym distributed representations items domain rep resented patterns activity groups units participate representing items 
distributed knowledge knowledge relation ship items encoded large numbers connection weights encode mappings 
controversy surrounding sm framework associated implementation stems fact breaks traditional accounts lexical processing coltheart morton patterson fundamental ways 
representational status words 
traditional accounts assume words represented structure reading system architecture 
morton known instances type word representation 
contrast sm framework lexical status string letters phonemes reflected structure reading system 
words distinguished nonwords functional properties system way particular orthographic phonological semantic patterns activity interact see van orden 
sm framework second major break tradition concerns degree uniformity mechanism orthographic phonological semantic representations interact 
traditional accounts assume pronouncing exception words nonwords require separate lexical sublexical mechanisms respectively 
contrast sm framework employs far homogeneous processes oral reading 
particular separate mechanisms pronouncing nonwords exception words 
system knowledge spelling sound correspondences brought bear pronouncing types letter strings 
conflicts possible alternative pronunciations letter string resolved structurally distinct mechanisms cooperative competitive interactions letter string relates known words pronunciations 
furthermore semantic representation word participates oral reading exactly manner orthographic phonological representations framework leaves open issue important semantic influences skilled oral reading 
regularity versus consistency 
issue intimately related tension sm framework traditional dual route theories concerns distinction regularity consistency 
broadly speaking word regular pronunciation generated rule consistent pronunciation agrees similarly understanding normal impaired word reading words 
course useful definitions operationalized specific terms 
commonly proposed pronunciation rules frequent grapheme language gpc rules augmented considerable operate adequately see coltheart seidenberg plaut petersen mcclelland discussion 
consistency hand typically defined respect orthographic body phonological rime vowel plus consonants 
choice partly justified grounds empirical data example richmond welty press demonstrated naming data monosyllabic words english cvc pronunciation consistency body vc accounts significantly variance naming latency consistency onset plus vowel cv 
pragmatic reasons restricting consideration body level consistency bodies constitute manageable manipulation designing experimental lists 
experimenters consider consistency orthographic neighborhoods possible levels individual graphemes largest sub word sized chunks selection stimulus words process general notion consistency broader specific instantiation terms body consistency just general notion regularity broader defined particular set spelling sound correspondence rules 
frequent observation coltheart waters seidenberg words regular typical spelling sound correspondences mint produce shorter naming latencies lower error rates words exceptional correspondences pint regularity originally considered critical variable 
glushko argued consistency provided better account empirical results 
mint may regular word gpc rules spelling sound relationship inconsistent orthographic neighbor pint 
extent process computing phonology orthography sensitive characteristics neighborhood performance regular inconsistent word mint may adversely affected 
glushko demonstrate longer naming latencies regular inconsistent words regular words consistent body neighborhoods result obtained subsequent experiments 
jared seidenberg offered sophisticated hypothesis captures aspects results handled previous accounts referring solely regularity consistency 
jared colleagues magnitude consistency effect word depends summed frequency word friends words similar spelling pattern similar pronunciation enemies words similar spelling pattern discrepant pronunciation 
example inconsistent word mint number friends lint tint print just single enemy pint 
strength friends single enemy exert marked influence especially true pint enemy relatively low frequency negative impact computing pronunciation mint small undetectable 
contrast inconsistent word enemies blown shown grown friends brown town gives rise substantial effect 
words roughly balanced support friends enemies termed ambiguous respect pronunciation body bert seidenberg seidenberg 
commonly observed effect regularity finds natural explanation jared account regular words defined gpc rules friends enemies words irregular spelling sound correspondences pint sew typically enemies friends 
correspondence glushko taraban mcclelland refer words enemies friends exception words acknowledging definition excludes words considered exceptional gpc rules ambiguous words 
jared hypothesis supporting data mesh results demonstrating inadequacy simple regular irregular dichotomy degrees regularity effect observed acquired surface dyslexia shallice warrington mccarthy see plaut behrmann patterson mcclelland direct evidence consistency effects surface dyslexia 
kept mind definition consistency solely body neighborhoods provide partial account consistency effects expected operate full range spelling sound correspondences 
example word chef considered inconsistent analysis words english body ef clef ref agree pronunciation 
broader definition consistency chef certainly inconsistent overwhelmingly common pronunciation ch english appropriate chief chef 
broad view consistency important considering called irregular consistent words words kind bold took highly consistent body neighborhoods irregular gpc rules coltheart 

processing items expected sensitive conflict consistency body rime level inconsistency grapheme phoneme level 
follows adopt standard practice body level manipulations empirical tests interpreted providing approximation understanding normal impaired word reading true range consistency effects 
relationship approaches 
cursory inspection suggest sm framework fact dual route system orthography influence phonology directly semantics 
clarify possible source confusion explicit typical assumptions dual route theories concerning structure operation different procedures 
described earlier central distinction theories lexical sublexical procedures 
sublexical procedure applies gpc rules produce correct pronunciations regular words reasonable pronunciations nonwords incorrect regularized pronunciations exception words 
lexical procedure produces correct pronunciations words response nonwords 
outputs procedures conflict exception words models noel assume horse race faster typically lexical procedure generating actual response 
monsell patterson graham hughes suggest output procedures pooled phonological representation sufficient drive articulation achieved specific means pooling occurs rarely explicit 
lexical procedure subdivided direct route maps orthographic word representations directly phonological word representations indirect route maps semantics 
formulations dual route model sense route model researchers typically assume indirect semantic route slow influence skilled word pronunciation coltheart patterson morton 
contrast portion sm framework operate applying gpc rules simultaneous interaction units 
capable pronouncing types input including exception words time takes depends type input 
furthermore semantic portion framework operate terms word representations terms interacting units participates processing words 
addition nonwords may engage semantics degree extent occurs minimal see discussion lexical decision general discussion 
structure operation sm framework fundamentally different existing dual route theories 
may help clarify relationship sm framework approaches word reading theories 
main alternatives lexical analogy theories multiple levels theories 
lexical analogy theories henderson marcel dispense sublexical procedure propose lexical procedure pronounce nonwords synthesizing pronunciations orthographically similar words 
unfortunately way pronunciations generated synthesized rarely fully specified 
multiple levels theories shallice mccarthy shallice dispense direct lexical route incorporate sublexical route assuming spelling sound correspondences represented segments sizes ranging single graphemes phonemes word bodies entire morphemes 
way sm framework thought integration detailed specification lexical analogy multiple level theories see norris connectionist implementation 
pronunciations nonwords generated basis combined influence known word pronunciations similar nonword having strongest effect 
order system pronounce exception words nonwords hidden units learn sensitive spelling sound correspondences range sizes 
framework broadly consistent van orden proposal orthography phonology strongly associated covariant learning sm framework incorporates direct interaction orthography semantics van orden colleagues dispute 
implemented model sm framework clearly represents radical departure widely held assumptions lexical processing plausible account human word reading 
service establishing framework plausibility sm implemented specific connectionist network implicitly claimed embodies central theoretical tenets framework 
network highlighted bold contains groups units orthographic units hidden units phonological units 
hidden units receive connections orthographic units turn send connections phonological units back orthographic units 
network contains semantic context information 
orthographic phonological forms represented patterns activity orthographic phonological units respectively 
patterns defined terms contextsensitive triples letters phonemes wickelgren 
computationally infeasible sm include unit possible triple representations require fewer units preserve relative similarities patterns 
orthography letter triples unit responds defined table randomly selected letters blank positions 
representation letter string orthographic unit active string contains letter triples generated sampling positions unit table 
example gave activate orthographic units capable generating ga gav ave phonological representations derived analogous fashion phonological unit table entries position randomly selected phonemes understanding normal impaired word reading phonemes containing particular phonemic feature defined rumelhart mcclelland 
constraint features third positions come phonetic dimension place articulation 
unit phonology represents particular ordered triple phonemic features termed 
example pronunciation gav activate phonological units representing back vowel front long fricative back features vowel long front fricative 
average word activates orthographic units phonological units 
return analysis properties representations summarizing sm simulation results 
weights connections units initialized small random values 
network repeatedly orthography monosyllabic words trained generate phonology word regenerate orthography see seidenberg mcclelland details 
training set probability word network proportional logarithmic function frequency ku era francis 
processing word involved setting states orthographic units defined computing hidden unit states states orthographic units weights connections computing states phonological orthographic units hidden units 
back propagation rumelhart hinton williams calculate adjust weights reduce differences correct phonological orthographic representations word generated network 
weight changes accumulated sweep training set changes carried process repeated 
network considered named word correctly generated phonological activity closer representation correct pronunciation word pronunciation differed correct single phoneme 
example gave gav competing pronunciations av ga phoneme 
training sweeps corpus amounting word presentations network correctly named words correct low frequency exception words 
considerable amount empirical data oral reading concerns time takes name words various types 
natural analogue model naming latency subjects amount computing time required produce output 
sm measure network takes exactly amount time update unit compute phonological output letter string 
approximated naming latency measure accuracy phonological activity produced sum squared error exception ambiguous regular inconsistent regular consistent low high frequency 
mean phonological error scores produced seidenberg mcclelland network words various degrees spelling sound consistency listed appendix function frequency 
regenerated seidenberg mcclelland 
network phonological error score 
sm showed network distribution phonological error scores various words replicates effects frequency consistency naming latencies wide variety empirical studies 
presents particularly illustrative results regard high low frequency words levels consistency listed appendix current simulations exception words experiments taraban mcclelland average friends sm corpus counting word enemies ambiguous words generated sm matched ku era francis frequency exception words average friends enemies regular inconsistent words taraban mc average friends enemies regular consistent words control items exception words taraban mcclelland study average friends enemies foreign word coup item group pronunciations bass item class 
relevant empirical effects naming latency exhibited sm model specifically 
high frequency words named faster words forster chambers 
understanding normal impaired word reading 
consistent words inconsistent words glushko latencies increase monotonically increasing spelling sound inconsistency approximated relative proportion friends vs enemies jared 
regular inconsistent words cf 
slower named regular consistent words glushko exception words pint sew slowest named seidenberg 
performance ambiguous words cf 
grown falls regular inconsistent words exception words investigated directly respect reading acquisition 

frequency interacts consistency seidenberg seidenberg waters seidenberg consistency effect greater low frequency words high frequency words may absent see seidenberg equivalently frequency effect decreases increasingly consistency absent regular words see waters seidenberg 
considering empirical simulation results important keep mind way classification consistency intended imply existence distinct subtypes words intended help illustrate effects underlying continuum consistency jared 
model shows analogous effects consistency nonword naming latency 
particular nonwords derived regular consistent words faster name nonwords derived exception words glushko taraban mcclelland 
mentioned model nonword naming accuracy worse skilled readers 
besner 
reported nonword lists glushko mccann besner model correct skilled readers correct respectively 
seidenberg mcclelland pointed scoring criterion network strict subjects 
return issue scoring nonword reading performance purposes suffices acknowledge particularly true respect distinction regular inconsistent words ambiguous words differ degree balance friends enemies 
fact number previous studies including taraban mcclelland failed distinction 
result taraban mcclelland regular inconsistent words contain bodies categorize ambiguous dear grow 
unfortunate consequence occasionally words identical bodies assigned different consistency classes 
current context concerned individual items solely pattern means classes illustrate consistency effects 
regard word classes differ appropriate manner average relative numbers friends enemies 
continuity earlier continue taraban mcclelland stimuli 
differences scoring account performance sm model nonwords inadequate 
sm model replicates effects frequency consistency lexical decision waters seidenberg responses orthographic error scores measure degree network succeeds recreating orthography input string 
model accurate lexical decision conditions normal subjects besner besner 
consistency influences ease word naming skills acquired 
skilled readers younger developmentally dyslexic show larger consistency effects skilled readers 
model shows similar effects early course learning trained limited resources hidden units 
damaging model removing units connections results pattern errors somewhat similar brain injured patients form surface dyslexia patterson patterson 
specifically exception words particularly prone regularized see patterson coltheart marshall 
attempts model surface dyslexia sm model satisfactory see behrmann bub coltheart criticism 
consider types developmental acquired dyslexia detail presenting new simulation results normal skilled reading 
evaluation model evaluating sm results important bear mind relationship implemented model general framework lexical processing derived 
ways implemented network poor approximation general framework contains semantic representations knowledge trained limited vocabulary feedforward architecture severely restricts way information interact system 
addition working implementation network inevitably embodies specific representational processing details central theoretical framework 
details include specific orthographic phonological representation schemes logarithmic frequency compression training error scores model naming latencies supervised error correcting training procedure see jordan rumelhart 
implemented network faithful central theoretical tenets general framework see seidenberg network employs distributed orthographic phonological representations reflect similarities words domain computation orthography phonology involve nonlinear cooperative competitive influences governed weighted connections understanding normal impaired word reading units weights encode network knowledge orthography phonology related knowledge acquired gradually basis network exposure written words pronunciations 
important note central principles lacking implemented network interactivity intrinsic variability 
consider implications principles 
focus limitations sm important clear strengths 
foremost general framework supported explicit computational model implements mapping orthography phonology 
course implementing model correct things allow thoroughly adequately evaluated seidenberg 
models reading explicit arrow diagrams accompanied descriptive text processing occur component notable exception implementation coltheart coltheart compared detail current approach seidenberg 
fact sm general framework amounts description 
step implementing portion framework testing identical stimuli empirical studies sm enabled entire approach evaluated greater detail possible previous explicit models 
furthermore overlooked implemented model succeeds accounting considerable amount data normal impaired word reading 
model reproduces quantitative effects empirical studies normal reading basic findings developmental acquired dyslexia 
existing implementation covers close range results 
important bear mind basic computational properties sm framework implementation developed specifically word reading 
derive broader enterprise connectionist modeling cognitive domains 
principles distributed representations interactivity distributed knowledge gradient descent learning applied successfully problems high level vision learning memory speech language reasoning problem solving motor planning control see hinton mcclelland rumelhart pdp research group quinlan examples 
distinctive aspects connectionist approach strong emphasis general learning principles attempt contact neurobiological cognitive phenomena 
neurally plausible learning particularly critical understanding reading brain developed innate dedicated circuitry evolutionarily skill 
sm specific contributions study reading fits general computational approach understanding cognitive processes learned implemented brain 
sm implementation serious limitations accounting empirical data 
limitations doubt stem lack unimplemented portions framework importantly involvement semantic representations visual articulatory procedures 
full consideration range relevant empirical findings better undertaken general discussion context new simulation results 
consideration poor nonword reading performance sm network postponed 
limitation fundamental nonword reading improved addition semantics 
furthermore coltheart 
argued primarily result poor processing nonwords model incapable accounting central issues normal impaired word reading 
fundamentally reading nonwords adequately model fails refute claim dual route theorists reading nonwords reading exception words requires separate mechanisms 
seidenberg mcclelland argued model poor nonword reading fundamental problem general framework result specific limitations implementation 
limited sizeof training corpus 
model exposed words skilled readers compared know approximately times number 
knowledge model available reading nonwords derived words limited training corpus serious handicap 
coltheart 
argued limitations sm training corpus explain model poor nonword reading system learns gpc rules corpus performs better 
argument fallacious effectiveness training corpus depends critically assumptions built training procedure 
fact coltheart colleagues procedure learning gpc rules built considerable amount knowledge specific reading concerning possible relationships graphemes phonemes various contexts 
contrast sm applied general learning procedure representations encode ordered triples letters phonemic features correspondences 
demonstration sm training corpus sufficient support nonword reading context strong domain specific assumptions invalidate claim corpus may insufficient context weaker assumptions 
second aspect sm simulation contributed poor nonword reading represent phonology 
representational scheme known limitations related scheme extended realistic vocabularies see understanding normal impaired word reading pinker prince detailed criticism 
current context seidenberg mcclelland pointed representations adequately capture phonemic structure 
specifically features phoneme bound features neighboring phonemes 
result surrounding context easily introduce inappropriate features producing single feature errors nonword pronunciations tiv 
specific training corpus representation central sm general framework lexical processing 
seidenberg mcclelland correct suggesting aspects simulation responsible poor nonword reading general framework remains viable 
hand actual performance implementation main source evidence sm put forward support view reading system 
mccloskey pointed notoriously difficult determine implementation failings due fundamental incidental properties design predict changes design affect behavior 
support sm connectionist framework viable alternative rule dual route accounts critical develop simulations account range findings original implementation pronounce nonwords skilled readers 
presents simulations 
orthographic phonological representations dispersion problem purposes supporting nonword reading phonological representation fundamental drawback 
problem stems general issue represent structured objects words composed ordered strings letters phonemes connectionist networks 
connectionist researchers networks properties hinton 
knowledge network connection weights units 

support generalization network knowledge capture important regularities domain 

processing fast major constituents item processed parallel 
problem properties difficult reconcile 
consider standard technique units called slot representation mcclelland rumelhart 
letter goes slot second letter second slot similarly table dispersion problem slot representations left justified vowel centered context sensitive triples log lo log og glad gl gla lad ad split sp spl pli lit output phoneme goes slot 
slots words desired length represented 
scheme satisfies properties cost property 
processing done parallel letters phonemes weighted connections cost regularities letters phonemes related 
reason separate copy letter phoneme slot relevant knowledge embedded connections specific units knowledge replicated connections slot 
extent useful domain oral reading pronunciation letter may depend occurs middle word 
slot approach carries extreme unfortunate consequences 
consider words log glad split 
fact letter corresponds phoneme words learned stored separate times system 
generalization learned letters position letter positions 
problem alleviated degree aligning slots various ways centered vowel seidenberg eliminated completely see table 
adequate generalization requires learning regularities separately slots 
alternative scheme apply network single letter time sejnowski rosenberg nettalk model 
knowledge applied pronouncing letter regardless occurs word words arbitrary length processed 
unfortunately properties traded property 
processing slow sequential may satis developed series networks form exhibit impressive performance reading nonwords weak effects word frequency 
coltheart 
take sequential approach solving dispersion problem correspondence learned position applied positions different correspondence learned 
understanding normal impaired word reading factory domains word reading 
note common finding small significant effects word length naming latency butler richardson imply computation orthography phonology operates sequentially letters parallel implementation mapping may exhibit small length effects demonstrated simulation 
representations sm attempt avoid specific limitations slot approach turn version problem 
elements letters phonemes represented terms absolute spatial position relative position word terms adjacent elements left right 
approach originated wickelgren representation element context sensitive rigidly tied position 
unfortunately knowledge spelling sound correspondences dispersed large number different contexts adequate generalization requires training effectively covers 
returning table words log glad split share correspondence triples letters common 
similar property holds phonology triples phonemes phonemic features 
slot approach correspondence cases different units activated 
result knowledge learned context encoded connection weights apply contexts hindering generalization 
notice effect regularities effect limiting size training corpus 
contribution element representation word specific context occurs 
result knowledge learned item beneficial items share specific context 
representations disperse regularities domain number trained mappings support pronunciation effectively reduced 
result generalization novel stimuli pronunciation nonwords knowledge suffers accordingly 
way seidenberg mcclelland suggestions improving model nonword reading performance enlarge training corpus improve representations amount thing 
improved representations minimize dispersion problem effective size training corpus pronunciation increased 
condensing spelling sound regularities hypothesis guiding current idea dispersion problem prevented sm network exploiting structure english spelling sound system fully human readers 
set design representations minimize dispersion 
limiting case approach single set letter units letter alphabet single set phoneme units phoneme 
scheme satisfies hinton desired properties letters word map phonemes simultaneously weighted connections presumably hidden units spelling sound regularities condensed units connections involved particular letter phoneme 
unfortunately approach fatal flaw preserve relative order letters phonemes 
distinguish top pot salt 
turns scheme involving small amount replication sufficient provide unique representation virtually monosyllabic word 
definition contains single vowel set vowel units needed 
may contain initial final consonant cluster consonant occur cluster separate sets consonant units required clusters 
remarkable thing nearly necessary 
reason initial final consonant cluster strong phonotactic constraints arise large part structure articulatory system 
ends syllable phoneme occur order phonemes strongly constrained 
example phonemes occur onset cluster order str 
required specify pronunciation phonemes cluster phonotactic constraints uniquely determine order phonemes occur 
necessary phonotactic constraints expressed simply grouping phonemes mutually exclusive sets ordering sets left right accordance right ordering constraints consonant clusters 
done reading pronunciation involves simply concatenating phonemes active sequence left right including phoneme mutually exclusive set see table 
cases phonemes occur order consonant cluster clasp lapse 
handle cases necessary add units disambiguate order ps 
convention active taken order ps unit active case order reversed 
cover pronunciations sm corpus units required ps ks ts 
interestingly combinations written single letters english german closely related combinations ts dz typically considered single phonemes called 
fact ts treated languages common see ps ks behave similarly 
representational scheme applies understanding normal impaired word reading table phonological orthographic representations simulations phonology onset vowel coda ps ks ts orthography onset ch gh gn ph ps rh sh th ts wh vowel ai au aw ay ea ee ei eu ew ey oa oe oi oo ou ow oy ue ui uy coda bb ch ck dd dg ff gg gh gn ks ll ng nn ph pp ps rr sh sl ss th ts tt zz es ed pot cat bed hit dog keep bike hope boot boy cup ring chin thin 
phonemes represented conventional way bat 
groupings indicate sets mutually exclusive phonemes 
note notation vowels slightly different seidenberg mcclelland 
representations differ slightly plaut mcclelland seidenberg 
particular added ts dz ordering phonemes somewhat different mutually exclusive phoneme sets added graphemes gu qu eliminated 
changes better capture relevant phonotactic constraints simplify encoding procedure converting letter strings activity patterns grapheme units 
phonology english alphabetic language parts written form word correspond parts spoken form 
spelling units correspond phonemes necessarily single letters 
termed relational units called graphemes consist letters th 
spelling sound regularities english primarily correspondences regularities system elegantly captured orthographic units represent graphemes string simply letters word 
unfortunately clear graphemes word 
consider word shepherd 
case suppose word contains ph grapheme fact pronounced apparent input ambiguous cases 
simple procedure translating letter strings correct sequence graphemes 
completely straightforward translate letter sequence pattern activity representing possible graphemes string 
grapheme components activated 
procedure consistent treatment ps ks ts phonology 
point orthographic phonological representations motivated purely computational considerations condense spelling sound regularities order improve generalization 
turning simulations important clear empirical assumptions implicit representations 
certainly full account reading behavior include specification representations develop prior course reading acquisition 
demonstration scope current 
fact model eye mouth avoid making assumptions reading system inputs outputs actuality learned internal representations 
best ensure representations broadly consistent relevant developmental behavioral data 
relevant assumptions phonological representations segmental composed phonemes strongly constrained 
presume phonological structure learned part prior reading acquisition basis speech comprehension production 
deny phonological representations may refined course reading acquisition particularly influence explicit phoneme instruction see cary cary 
simplicity modeling uses fully developed phonological representations outset training 
analogous assumptions apply regard orthographic representations 
assume letters letter combinations ordering obeys constraints english constraints generally weaker phonology 
properties particularly controversial se orthographic representations develop concurrently reading acquisition 
fully articulated orthographic understanding normal impaired word reading representations outset reading acquisition certainly suspect 
complete account orthographic representations develop primitive visual representations scope current 
provide general characterization account 
suppose children learn visual representations individual letters visual objects 
learning read exposed words consist familiar letters various combinations 
explicit representations gradually develop letter combinations occur unusual consequences see mozer 
context oral reading combinations precisely pronunciations predicted components th ph corresponding relational units 
course explicit representations may develop regularly pronounced letter combinations 
limit orthographic representation contain letter combinations occur language 
expanding orthographic representation units additional combinations little consequence little pressure network learn correspondences components learned 
way particular set graphemes employ viewed efficient simplification general orthographic representation develop exposure letter combinations words 
clear claim orthographic phonological representations fully general 
idiosyncrasies stem fact design took account specific aspects sm corpus 
claim principles representations derived particular phonotactic constraints condense spelling sound regularities general 
simulation feedforward network simulation intended test hypothesis representations condensed regularities orthography phonology improve nonword reading performance network trained sm corpus monosyllabic words 
specifically issue single mechanism form connectionist network learn read reasonably large corpus words including exception words read nonwords skilled readers 
network developed undermine claims dual route theorists skilled word reading requires separation lexical sublexical procedures mapping print sound 
phoneme units hidden units grapheme units 
architecture feedforward network 
ovals represent groups units arrows represent complete connectivity group 
method network architecture 
architecture network shown consists layers units 
input layer network contains grapheme units grapheme table 
similarly output layer contains phoneme units 
layers intermediate layer hidden units 
unit real valued activity level state ranges smooth nonlinear logistic function unit total input 
exp weight unit unit realvalued bias unit exp exponential function 
hidden unit receives connection grapheme unit turn sends connection phoneme unit 
contrast sm network grapheme units receive connections back hidden units 
network maps orthography phonology orthography orthography see phillips hay smith 
weights connections initialized small random values uniformly distributed 
bias terms hidden phoneme units thought weight additional connection unit state learned way connection weights 
including biases network total connections 
training procedure 
training corpus consists monosyllabic words sm corpus augmented monosyllabic words missing corpus word stimuli various empirical studies total words 
sets homographs read plaut mcclelland seidenberg network trained isolated grapheme phoneme correspondences approximation explicit instruction children receive learning read 
correspondences included training networks reported 
understanding normal impaired word reading red read red pronunciations included corpus 
words inflected forms empirical studies rolled days 
orthographic phonological representations intended handle inflected happen capable representing training corpus left 
kept mind network exposure inflected forms extremely impoverished relative skilled readers 
letter string network clamping states grapheme units representing graphemes contained string states grapheme units 
processing input hidden units compute states grapheme units weights connections equations phoneme units compute states hidden units 
resulting pattern activity phoneme units represents network pronunciation input letter string 
word processed network training back propagation rumelhart hinton williams calculate change connection weights reduce discrepancy pattern phoneme activity generated network correct pattern word derivative error respect weight 
standard measure discrepancy sm summed squared error generated correct output phoneme states 
correct target value 
new representation phonology unit interpreted independent hypothesis particular phoneme output pronunciation 
case appropriate error measure generated correct activity patterns see hinton rumelhart durbin golden chauvin press termed asymmetric divergence kullback leibler distance kullback leibler 
state phoneme unit log log notice contribution cross entropy unit simply log target log target 
practical point view cross entropy advantage summed squared error comes precisely true procedure determining pronunciation phoneme unit activities soon described consider units independently states determined independently set hidden unit states 
approximation sufficient cross entropy appropriate error measure summed squared error 
correcting output units completely incorrect opposite flat portion logistic function 
particular concern tasks output units inputs network eliminate error task turning output units regardless input including input 
problem unit state falls flat portion logistic function large weight changes required change state substantially 
unit state diverges target change cross entropy increases faster summed squared error exponentially vs linearly cross entropy better able generate sufficiently large weight changes 
training weights slight tendency decay zero 
accomplished augmenting cross entropy error function term proportional constant current simulation sum squares weight critical weight decay tends aid generalization constraining weights grow extent needed reduce error task hinton 
sm simulation probability word network training epoch logarithmic function written frequency ku era francis 
current simulation compressed frequency values scale error derivatives calculated back propagation 
manipulation essentially effect frequent words stronger impact frequent words knowledge learned system 
fact frequencies manner exactly equivalent updating weights sweep expanded training corpus number times word proportional compressed frequency 
new procedure adopted reasons 
presenting entire training corpus epoch learning rates connection adapted independently jacobs see sutton developed line version 
second implementing frequencies multiplication sampling range frequencies investigate effects actual ku era francis frequencies simulations 
sm constrained logarithmic compression derivative cross entropy respect output unit total input simply difference unit state target 
procedure adjusting connection specific learning rates called delta bar delta jacobs works follows 
connection learning rate initialized 
epoch error derivative connection calculated back propagation compared previous weight change 
direction sign connection rate incremented current simulation decreased multiplicatively current simulation 
understanding normal impaired word reading severe compressions meant lowest frequency words network 
actual weight changes administered epoch combination accumulated error derivatives proportion previous weight changes 
epoch number global learning rate current simulation connection specific learning rate cross entropy error function weight decay contribution past weight changes termed momentum epochs current simulation 
momentum introduced initial epochs avoid magnifying effects initial weight gradients large word activity phoneme units active produces large amount error plaut hinton 
testing procedure 
network described learns take activity patterns grapheme units produce corresponding activity patterns phoneme units 
behavior human subjects oral reading better described terms producing phoneme strings response letter strings 
accordingly direct comparison network behavior subjects need procedure encoding letter strings activity patterns grapheme units procedure decoding activity patterns phoneme units phoneme strings 
encoding procedure generate input network word training corpus 
convert letter string activity pattern grapheme units string parsed onset consonant cluster vowel final coda consonant cluster 
involves simply locating string leftmost contiguous block composed letters non initial block letters encoded vowel graphemes listed table grapheme contained vowel substring activated left inactive 
substrings right left vowel substring encoded similarly onset coda consonant graphemes respectively 
example word school activates onset units ch vowel units oo coda unit notice words guest queen suede parsed vowel functions consonant cf 
queue sue 
issue ph shepherd ambiguity left network cope 
analogous encoding procedure phonemes generate training patterns words simpler monosyllabic pronunciations contain exactly vowel 
decoding procedure producing pronunciations phoneme activities generated network likewise straightforward 
shown table phonemes grouped mutually exclusive sets sets ordered left right top bottom table 
grouping ordering encode phonotactic constraints necessary disambiguate pronunciations 
response network simply ordered concatenation active phonemes state active set 
exceptions rule 
monosyllabic pronunciations contain vowel active vowel included network response regardless activity level 
second exception relates units ps ks ts 
described earlier units active components order components response reversed 
simplicity encoding decoding procedures significant advantage current representations sm 
case reconstructing unique string phonemes corresponding pattern activity triples phonemic features exceedingly difficult impossible see rumelhart mcclelland mozer 
fact sm confront problem simply selected best set alternative pronunciations error scores 
sense sm model produce explicit pronunciations enables procedure select alternatives 
contrast current decoding procedure require externally generated alternatives possible pattern activity phoneme units corresponds directly unambiguously particular string phonemes 
kept mind encoding decoding procedures external network constitute additional assumptions nature knowledge processing involved skilled reading discussed earlier 
results word reading 
epochs training network correctly words training corpus 
homographs network produces correct pronunciations typically competing phonemes alternatives equally active 
example network lead led activation activation 
differences reflect relative consistency alternatives pronunciations words 
nature network level performance training corpus optimal 
network deterministic produces output input 
fact impossible network learn produce pronunciations homographs 
note determinacy intrinsic limitation connectionist networks see movellan mcclelland 
merely reflects fact general principle intrinsic variability included simulation practical reasons keep computational demands simulation reasonable 
understanding normal impaired word reading purposes important finding trained network reads regular exception words correctly 
interested network replicates effects frequency consistency naming latency 
return issue consider pressing issue network performance reading nonwords 
nonword reading 
tested network lists nonwords empirical studies 
lists come experiment glushko compared subjects reading nonwords derived regular words dean reading nonwords derived exception words deaf 
glushko originally termed regular nonwords exception nonwords respectively appropriately characterized terms body neighborhood consistent refer consistent inconsistent nonwords 
third nonword list comes study mccann besner compared performance set set control nonwords 
control nonwords investigation believe effects mediated aspects reading system semantics articulatory system implemented simulation see general discussion 
nonwords definition novel stimuli exactly counts correct pronunciation nonword matter considerable debate see seidenberg 
complexity issue apparent momentarily 
purposes initial comparison consider pronunciation nonword correct regular defined adhering gpc rules outlined 
table presents correct performance skilled readers reported glushko mccann besner nonword lists corresponding performance network 
table lists errors network lists 
consider glushko consistent nonwords 
network single minor mistake items just failing introduce transitional 
fact inclusion varies dialects english dun vs 
training corpus words une june prune tune coded 
case network subject difficult relatively easy nonwords 
situation different inconsistent nonwords 
network subjects produce non regular pronunciations significant subset items network slightly prone 
closer examination responses cases reveals 
consider nonword 
grapheme oo frequently corresponds boot correct regular pronunciation 
body ook pronounced took 
exception words ook training corpus 
suggests correct pronunciation 
issue network pronunciation correct relevant issue network behaves similarly subjects 
fact subjects network sensitive context vowels occur evidenced greater tendency produce non regular pronunciations inconsistent nonwords compared consistent nonwords 
glushko subject non regular responses inconsistent nonwords consistent pronunciation nonword body occurs ku era francis corpus leaving responses actual errors 
network non regular responses inconsistent nonwords match pronunciation training corpus body half frequent pronunciation body 
network responses inconsistent nonwords actual errors 
network performs slightly better subjects glushko nonword lists 
appendix lists pronunciations accepted correct glushko nonwords 
subjects network find mccann besner control nonwords difficult pronounce surprising lists contain number orthographically unusual nonwords 
network performance slightly worse subjects 
network errors understood terms specific properties training corpus network design 
word training corpus body medial ow pronounced bowl bol considered reasonable response 
second errors inflected forms previously acknowledged network minimal experience inflections intended apply 
instances training corpus words containing grapheme coda network possibly learned map phonology 
way nonword effective input network network response jin correct 
applies nonword 
excluding inflected forms scoring considering correct network performs correctly remaining control nonwords slightly better subjects 
remaining errors network involve correspondences infrequent variable training corpus ph yu 
acknowledged failure model inflected forms coda real shortcomings addressed completely ade understanding normal impaired word reading table percent regular pronunciations nonwords glushko mccann besner consistent inconsistent control nonwords nonwords nonwords subjects network table errors feedforward network pronouncing nonwords glushko mccann besner nonword correct response nonword correct response consistent nonwords control nonwords un inconsistent nonwords tol bost bost bost ks koz kos ks lom mon fa pl vez ez put put pr ks pr sk ks sud sud jin st wed wed note pot cat bed hit dog keep bike hope boot boy cup ring chin thin 
activity levels correct missing phonemes listed parentheses 
cases actual response falls outside parentheses 
words marked remain errors considering properties training corpus explained text 
understanding normal impaired word reading account word reading 
purpose separating items analysis simply acknowledges model limitations easily understood terms specific properties training corpus 
dual route model 
possibility consistent dual route theories network partitioned sub networks reads regular words reads exception words 
case hidden units contribute exception words nonwords contribute nonwords exception words 
test possibility measured contribution hidden unit pronouncing letter string amount increase cross entropy error unit removed network 
network partitioned negative correlation hidden units number exception words number nonwords hidden unit substantial contribution defined greater 
fact taraban mcclelland exception words set orthographically matched nonwords listed appendix moderate positive correlation numbers exception words nonwords hidden units contribute see 
units important task important network partitioned system learns rules system learns exceptions 
frequency consistency effects 
important verify addition producing nonword reading new model replicates basic effects frequency consistency naming latency 
sm network current network takes amount time compute pronunciation letter string 
resort error score analogue naming latency 
particular cross entropy network generated pronunciation word correct pronunciation measure network trained minimize 
examine effects frequency consistency directly settling time equivalently trained recurrent network pronouncing various types words 
shows mean cross entropy error network pronouncing words varying degrees consistency function frequency 
highfrequency words produce error low frequency words 
frequency interacts significantly consistency 
post hoc comparisons word type separately reveal effect frequency reaches significance level exception words effect regular inconsistent words significant 
effect frequency regular words consistent inconsistent just fails reach significance 
main effect consistency error number nonwords number exception words 
numbers exception words nonwords listed appendix hidden unit significant contribution indicated increase cross entropy error unit removed network 
circle represents hidden units size circle proportional number hidden units making significant contributions indicated numbers exception words nonwords 
cross entropy low high frequency exception ambiguous regular inconsistent regular consistent 
mean cross entropy error produced feedforward network words various degrees spelling sound consistency listed appendix function frequency 
understanding normal impaired word reading network pronouncing words 
furthermore collapsed frequency post hoc pairwise comparisons word types significant 
specifically regular consistent words produce error regular inconsistent words turn produce error ambiguous words turn produce error exception words 
interestingly effect consistency significant considering high frequency words 
pairwise comparisons significant exception words ambiguous words 
contrasts performance normal subjects typically show little effect consistency high frequency words seidenberg seidenberg 
summary feedforward connectionist network trained extended version sm corpus monosyllabic words orthographic phonological representations condense regularities domains 
training network reads regular exception words reads nonwords glushko mccann besner essentially skilled readers 
minor discrepancies performance ascribed nonessential aspects simulation 
critically network segregated course training separate mechanisms pronouncing exception words nonwords 
network directly refutes claims dual route theorists skilled word reading requires separation lexical sublexical procedures mapping print sound 
furthermore error produced network various types words measured cross entropy generated correct pronunciations replicates standard findings frequency consistency interaction naming latencies subjects andrews seidenberg seidenberg taraban mcclelland waters seidenberg 
notable exception subjects sm network current network exhibits significant effect consistency high frequency words 
analytic account frequency consistency effects empirical finding naming latencies exception words slower far sensitive frequency regular words interpreted requiring explicit lexical representations grapheme phoneme correspondence rules 
recasting regularity effects terms spelling sound consistency glushko jared sm network previous section reproduce empirical phenomena properties 
properties networks human language system account give rise observed pattern frequency consistency effects 
relevant empirical pattern results described way 
general high frequency words low frequency words words greater spelling sound consistency named faster words consistency 
effect frequency diminishes consistency increased effect consistency diminishes frequency increased 
natural interpretation pattern frequency consistency contribute independently naming latency system subject termed gradual ceiling effect magnitude increments performance decreases performance improves 
frequency consistency set words sufficiently high produce fast naming latencies increasing factor yield little improvement 
close analysis operation connectionist networks reveals effects direct consequence properties processing learning networks specifically principles nonlinearity adaptivity distributed representations knowledge referred earlier 
connectionist network weight changes induced word training serve reduce error word definition naming latency 
frequency word reflected network previous simulation explicit scaling weight changes induces 
word frequency directly amplifies weight changes helpful word 
consistency spelling sound correspondences words reflected similarity orthographic phonological units activate 
furthermore words induce similar weight changes extent activate similar units 
weight changes induced word superimposed weight changes words word tend helped weight changes words spelling sound correspondences consistent conversely hindered weight changes inconsistent words 
frequency consistency effects contribute independently naming latency arise similar weight changes simply added training 
course training magnitudes weights network increase proportion accumulated weight changes 
weight changes result corresponding increases summed input output units active decreases summed input units inactive 
due nonlinearity input output function units changes translate directly proportional reductions error 
magnitude summed inputs output units increases states gradually asymptote 
result increase summed input unit yields progressively smaller decrements error course training 
understanding normal impaired word reading output input 
simple network analyzing frequency consistency effects sigmoidal input output function units 
frequency consistency contribute weights summed input units effect error subjected gradual ceiling effect unit states driven extremal values 
frequency consistency equation see effects frequency consistency connectionist networks directly help consider network embodies general principles sm feedforward networks simple permit closed form analysis anderson silverstein ritz jones see stone 
particular consider nonlinear network hidden units trained correlational hebbian error correcting learning rule see 
network specific instantiation van orden covariant learning hypothesis 
simplify presentation assume input patterns composed output patterns specified terms connection weights initialized zero units bias terms 
derive equation expresses concise form effects frequency consistency network response input 
learning trial involves setting states input units input pattern orthography word setting output units desired output pattern phonology word adjusting weight input unit output unit learning rate constant state output unit ij state input unit weight connection 
input output training pattern manner value connection weight simply sum weight changes individual pattern indexes individual training patterns 
training network performance test pattern determined setting states input units appropriate input pattern having network compute states output units 
computation state output unit assumed nonlinear monotonically increasing function sum input units state input unit times weight connection test pattern nonlinear input unit function 
example function standard logistic function commonly connectionist networks shown 
input output function output units need particular function certain properties vary monotonically input approach extremal values diminishing rate magnitude summed input increases positively negatively 
call functions sigmoid functions 
substitute derived expression weight equation equation pull constant term summation obtain equation indicates activation output unit reflects sigmoid function learning rate constant times sum terms consisting activation input units test pattern times sum training patterns activation input unit times activation output unit 
formulation input unit activation sum reflects extent output unit activation tends equal input unit activation equal 
specifically exactly equal number times output unit equal input unit equal minus number times output unit equal input unit equal 
see equation entire ensemble training patterns consistent value activation output unit input unit active connection weights come reflect 
training patterns come completely regular environment output activation depends input unit completely uncorrelated understanding normal impaired word reading activation input unit weights output unit equal weight particular input unit depends 
training patterns sampled randomly larger space patterns sample true correlations exactly scattered approximately normally true value 
learning procedure discovers output units depend input units sets weights accordingly 
purposes understanding quasi regular domains dependencies discrete character weights come reflect degree consistency input unit output unit entire ensemble training patterns 
equation written different way reflect relationship particularly relevant word reading literature frequency particular word consistency pronunciation pronunciations similar words known influence accuracy latency pronunciation 
rearrangement expresses revealing relationship output test similarity test pattern input pattern expression shows relationship state output unit test function states training similarity test input pattern training input pattern measured terms dot product input patterns consisting measure amounts number patterns common refer overlap training pattern test pattern designate substituting previous expression find state output unit test reflects sum training patterns unit output pattern times overlap pattern test pattern 
notice product measure input output consistency training test patterns 
see suppose inputs training testing patterns considerable overlap 
contribution training pattern depends sign output unit state pattern 
sign agrees appropriate state test pattern patterns consistent training pattern help move state output unit appropriate extremal value test pattern 
signs states training test patterns disagree patterns inconsistent performance test pattern worse having learned training pattern 
input training pattern similar test pattern reducing test performance diminishes 
clarify implications equation help consider simple cases 
suppose network trained pattern tested variety patterns 
state output unit testing monotonic function value training pattern times overlap training test input patterns 
long overlap patterns test output sign training output magnitude increase overlap test pattern training pattern 
response output unit varies similarity test pattern pattern training 
second example suppose test training pattern vary number training trials pattern 
case summation training patterns equation reduces count number training presentations pattern 
state output unit pattern approach correct asymptotic value number training presentations increases 
consider general case different input output patterns training number times 
elaborating equation state output unit test written impact consistency pattern refer equation frequency consistency equation 
relating equation word nonword reading simply involves identifying input network representation spelling word output network representation pronunciation 
assumption stronger activations correspond faster naming latencies frequency consistency equation derive predictions relative naming latencies different types words 
particular equation provides number frequency training presentations basis understanding naming latency depends frequency word consistency spelling sound correspondences words accounts fact effect consistency diminishes frequency word increases vice versa high frequency words push value sum tail input output function influences factors reduced see 
quantitative results simple corpus implications frequency consistency equation concrete suppose output unit value word pronunciation contains vowel dive contains vowel give 
suppose trained network set words understanding normal impaired word reading output hfe input inconsistent consistent low frequency inconsistent high frequency consistent 
frequency consistency interaction arising applying output activation function additive input contributions frequency solid arrows consistency dashed arrows 
notice particular identical contribution consistency weaker effect high frequency words low frequency words 
top half logistic activation function shown 
hf high frequency lf low frequency rc regular consistent exception 
ive contain vowel 
frequency consistency equation tells immediately response test input reflect influence words degree 
holding constant higher frequency word closely output approach desired value 
holding frequency word constant similar words agree pronunciation higher frequency closely output approach correct extremal value 
distance desired value vary continuously difference total influence neighbors agree word neighbors disagree contribution neighbor weighted similarity word frequency 
word high frequency tend push activation close correct extreme 
near extremes slope function relating summed input state output unit relatively shallow influence neighbors diminished 
illustrate effects shows cross entropy error particular output unit vary frequency word tested consistency overlapping words see van orden 
simplicity assume words frequency overlap test word true example input units represented letters words differed single cross entropy frequency exception ambiguous regular inconsistent regular consistent 
effects frequency consistency network hidden units trained correlational hebbian learning equation 
letter 
degrees consistency examined exception words give neighbors disagree test word value output unit ambiguous words neighbors split evenly agree disagree regular inconsistent words dive neighbors agree disagree give live regular consistent words dust neighbors agree value output unit 
analysis different cases completely characterized terms single variable consistency pronunciation vowel test word pronunciation words overlapping spellings 
analysis clearly reveals graded effect consistency diminishes increasing frequency 
error correction hidden units noted hebbian approach described fact provide adequate mechanism learning spelling sound correspondences english 
require networks hidden units trained errorcorrecting learning rule back propagation 
section take steps direction extending analyses complex cases 
consider implications errorcorrecting learning rule hebbian learning network hidden units 
back propagation generalization rule known delta rule widrow hoff 
observation delta rule change weight due training proportional state pattern input unit times partial derivative error pattern respect understanding normal impaired word reading summed input output unit times correct state unit result equation simply cf 
equation 
matters complex depends actual performance network trial 
sign output unit error sign target long target extremal value activation function affected change input 
unit hebbian case training word consistent test word help unit correct training inconsistent word hurt giving rise consistency effect 
main difference hebb rule delta rule set weights exists allows network produce correct output training pattern learning procedure eventually converge 
generally case hebbian learning results responses cases incorrect 
illustrate consider applying learning rules training set solution exist 
solution delta rule hebb rule 
problem posed framework examining 
specific network consists input units values representing letters word 
input units send direct connections single output unit pronunciation word contains vowel contains vowel 
table shows input patterns target output case net inputs activations result training learning rule 
items training set body int body ine 
ine words take vowel vowel target activation int words take vowel target 
int words include exception word pint takes vowel 
analysis word equal frequency 
table lists weights input unit output unit acquired training learning rule 
hebb rule involved epochs training learning rate 
resulting weights equal extremal targets activation function set finite weights reduce error zero 
case solution consists set weights produces outputs specified tolerance target value output unit training pattern 
solution exists produces outputs correct sign tolerance targets solution exists smaller tolerance multiplying weights large constant push output sigmoid arbitrarily close extreme values affecting sign 
number epochs times learning rate times number training items letter vowel minus number items letter vowel 
specifically letters occur weight letters occur times time weights 
final final largest magnitude weights strongly positive occurs times strongly negative occurs times 
weakly positive occurs onset weakly negative occurs 
moderately positive occurs twice pine pint 
weights directly reflect occurrences letters phonemes 
outputs network weights produced hebb rule shown table illustrate consistency effect net inputs activations 
example net input fine stronger line line similar inconsistent lint net input pine stronger line pine benefits similarity pint correspondence 
weights completely solve task word pint net input minus passing logistic function results activation quite different target value 
happened pint neighbors cast slightly votes 
consider results obtained delta rule 
case trained network epochs learning rate 
magnitude weights comparable hebb rule case epochs delta rule weight changes get smaller error gets smaller cumulative effect generally tends 
importantly delta rule general effects consistency observed response pint weaker responses right sign 
reason cumulative weight changes caused pint larger caused items epoch error larger pint items 
error correcting learning eventually compensates learning completely converged effects consistency apparent 
error correcting learning process causes alteration relative weighting effects neighbors assigning greater relative weight aspects input pattern differentiate inconsistent patterns see table 
weight tends accumulate distinguishes pint inconsistent neighbors hint lint mint tint 
correspondingly weights slightly negative relative hebb weights accentuate differentiation hint tint pint 
effect consistency delta rule precisely biggest understanding normal impaired word reading table input patterns targets activations training hebb rule delta rule letter inputs hebb rule delta rule word target net act net act hint lint mint pint tint fine line mine pine note net net input output unit act activation 
table weights letter units output unit training hebb rule delta rule letter units hebb rule delta rule understanding normal impaired word reading changes errors greatest delta rule tends counteract consistency effect 
related implication error correcting learning concerns degree output unit comes depend different parts input 
particular input output correspondence perfectly consistent onset state output unit predicted perfectly states particular input units delta rule set weights input units partially correlated output unit 
contrast correspondence variable vowel vs input unit predict state output unit delta rule develop significant weights parts input consonants disambiguate correspondence 
componential correspondence consonants partial correspondences exploited breaks vowels greater reliance context greater consistency effect 
tasks including english word reading set weights layer network maps letters phonemes training patterns see minsky papert 
cases hidden units mediate input output units needed achieve adequate performance 
things considerably complex networks hidden units equation provides guidance 
complexity comes fact output unit reflects similarities patterns activation training pattern test pattern hidden units input units 
hidden units tendency output units give similar output similar inputs activation function 
fact equation applies interpreted partial derivative error output units respect summed input hidden unit values particular weights nonlinearity activation function hidden units relatively sensitive dimensions similarity relatively insensitive allow hidden units respond particular combinations inputs similar combinations 
perspective output units hidden units re represent input patterns alter relative similarities 
critical learning complex mappings english spelling sound system 
phoneme units respond basis hidden layer similarity respond quite differently exception words alternative strategy increasing range tasks solved layer network add additional input units explicitly code relevant combinations original input units see gluck bower marr rumelhart hinton williams examples 
domain word reading higher order units hand specified experimenter input units norris hand specified activated input units separate pathway reggia marsland berndt learned hidden units separate pathway zorzi houghton butterworth 
consistent neighbors order pronounced correctly 
altering effective similarities input patterns network hidden units overcome limitations input output units 
process learning sensitive relevant input combinations occurs relatively slowly goes network inherent tendency making similar responses similar inputs 
fact hidden units sensitive higher order combinations input units important implications understanding body level consistency effects 
layer network hidden units contribution input unit total signal received output unit summed input unconditional contribution input unit independent state input units 
mentioned earlier pronunciations vowels typically predicted individual letters graphemes 
correlations vowel graphemes phonemes highly conditional presence particular consonant graphemes 
example mapping inconsistent mapping perfectly reliable context coda consisting letter pin win thin 
english vowels conditional generally greater vowels conditional onsets press 
consequently multi layer network aided generating appropriate vowel pronunciations developing hidden units respond particular combinations orthographic vowels word bodies 
coda taken account correlation vowel pronunciation may perfect context nt mint vs pint 
case choice vowel conditioned onset coda correspondence reliable 
fact hidden units tend similar responses similar inputs hidden units respond entire input pattern contribute nonstandard vowel pronunciation context nt tend partially active similar words mint 
tend produce interference phoneme level giving rise consistency effect 
important note multi layer network exhibit consistency effects trained tasks partially inconsistent quasi regular layer networks delta rule training environment involves componential correspondences hidden units learn ignore irrelevant aspects input 
summary broad range connectionist networks trained quasi regular environment exhibit general trends observed human experimental data robust consistency effects tend diminish experience specific items frequency entire ensemble patterns practice 
factors important determinants speed accuracy people read words aloud 
understanding normal impaired word reading balancing frequency consistency results analyses concur findings empirical studies sm feedforward network simulations effect consistency diminishes increasing frequency 
furthermore details analytic results revealing 
particular extent effect consistency eliminated high frequency words depends just frequent relative words lower frequency 
fact effect may help explain discrepancy findings feedforward network sm network existence consistency effects high frequency words generally empirical studies 
glance appear pattern observed feedforward network matches high frequency words lower frequency relative low frequency words frequency sm network frequency 
literally true logarithmically compressed word frequencies simulations 
better interpretation feedforward network effect consistency stronger sm network relative effect frequency appears weaker 
described earlier orthographic phonological representations sm context sensitive triples letters phonemes disperse regularities written spoken forms words 
relevant effects current context 
reduce extent training word improves performance words share spelling sound correspondences impairs performance words violate correspondences 
illustrated earlier words log glad split correspondence may set words may activate different orthographic phonological units 
mentioned weight changes induced word help extent activate similar units function overlap 
effect particularly important low frequency regular words performance depends primarily support higher frequency words training word 
contrast new representations condense regularities orthography phonology weight changes high frequency words improve performance low frequency words correspondences greater extent 
effect frequency regular words sm network feedforward network 
reason sm network performance exception word hindered training regular words inconsistent 
regular words sm network behave regular inconsistent words feedforward network exception words behave ambiguous words support interference receive similar words somewhat re duced see 
sm representations reduce effect consistency indirect manner improving performance exception words 
arises orthographic representations contain units explicitly indicate presence context sensitive triples letters 
triples correspond onset vowel combinations word bodies pin int directly contribute pronunciation exception words pint 
contrast new orthographic representations contain graphemes include consonants vowels consonants onset coda 
example orthographic units contribute independently hidden representations 
hidden layer network develop context sensitive representations order pronounce exception words correctly learn basis exposure words varying frequency 
remains true pattern frequency consistency effects sm network better replicates findings empirical studies pattern feedforward network 
skilled readers exhibit high level proficiency reading nonwords matched sm network alternative representations better capture spelling sound regularities 
effect frequency consistency reconciled nonword reading 
answer may lie fact sm feedforward networks trained word frequency values logarithmically compressed true frequencies occurrence language 
sm network replicates empirical naming latency pattern achieves appropriate balance influence frequency consistency suppressed relative effects subjects 
suppression revealed nonword reading examined task primarily network sensitivity consistency dictates performance 
contrast virtue new representations feedforward network exhibits sensitivity consistency comparable subjects evidenced nonword reading 
logarithmic frequencies effects frequency consistency unbalanced network fails replicate precise pattern naming latencies subjects 
interpretation leads prediction feedforward network exhibit nonword reading appropriate frequency consistency effects trained words actual frequencies occurrence 
simulation tests prediction 
simulation feedforward network actual frequencies frequent word ku era francis list frequency understanding normal impaired word reading cross entropy cross entropy ambiguous regular inconsistent frequency exception regular consistent frequency 
data frequency consistency equation equation test words frequencies plotted separately regular inconsistent ambiguous words upper graph regular consistent exception words lower graph 
upper pattern similar regular exception words sm network see lower similar pattern feedforward network see 
correspondences approximate due simplifying assumptions frequency consistency equation 
frequent words frequency 
training procedure sm probability word network training proportional logarithm frequency actual frequency 
compresses effective frequency range 
network experiences variation frequency occurrence words normal readers 
sm put forward number arguments favor logarithmically compressed frequencies actual frequencies training network 
readers experience words approximate actual frequency range language 
low frequency words disproportionately suffer lack inflectional derivational forms training corpus 
main reason compressing frequency range practical consideration limitations available computational resources 
highest frequency word epoch lowest frequency words average epochs 
actual frequencies sm trained network long sufficient exposure low frequency words 
compound matters sm point basic properties network training procedure serve progressively weaken impact frequency course training 
error correcting training procedure backpropagation weights changed extent doing reduces mismatch generated correct output 
high frequency words mastered produce mismatch induce progressively smaller weight changes 
effect magnified fact due asymptotic nature unit input output function weight changes smaller smaller impact units approach correct extremal values 
result learning dominated lower frequency words inaccurate effectively compressing range frequency driving learning network 
sm considered important verify results depend critically severe frequency compression 
trained version network probability word epoch square root frequency logarithm resulting frequency range 
basic pattern frequency consistency effects naming latency taraban mcclelland words larger effect frequency regular words virtually effect consistency high frequency words early training 
shift corresponds predictably pattern influence frequency stronger relative influence consistency 
sm data network accuracy reading words nonwords 
current simulation train version understanding normal impaired word reading ward network new representations actual frequencies occurrence words 
training procedure current avoids problem sampling low frequency words frequency directly scale magnitude weight changes induced word equivalent sampling limit small learning rate allows range frequencies employed 
goal test hypothesis balancing strong influence consistency arises representations better capture spelling sound regularities realistically strong influence frequency network exhibit appropriate pattern frequency consistency effects naming latency producing accurate performance word nonword pronunciation 
method network architecture 
architecture network simulation see 
training procedure 
major change training procedure simulation described values scale error derivatives computed back propagation proportional actual frequencies occurrence words ku era francis logarithmic compression frequencies 
sm words training corpus listed ku era francis assigned frequency assigned listed frequency plus 
values divided highest value corpus generate scaling values training 
weight changes produced word unscaled scaling value 
comparison word highest frequency occurrences value 
contrast relative frequencies words extremely low 
mean scaling value entire training corpus median value 
taraban mcclelland high frequency exception words average value low frequency exception words average 
words ku era francis list value just addition parameters training procedure modified compensate changes word frequencies 
global learning rate equation increased compensate fact summed frequency entire training corpus reduced actual logarithmic frequencies 
second slight tendency weights decay zero removed prevent small weight changes induced low frequency words due small scaling factors overcome tendency weights shrink zero 
modifications network trained exactly way simulation 
testing procedure 
procedure testing network procedure words nonwords simulation 
results word reading 
weight changes caused words small considerably training required reach approximately level performance logarithmically compressed frequencies 
epochs training network words corpus bas cache gent correct homographs considered correct elicited correct pronunciation 
words inconsistent spelling sound correspondences low frequencies average scaling value 
network mastered exception words lowest frequency 
nonword reading 
table lists errors network pronouncing lists nonwords glushko mccann besner 
network produces regular responses glushko consistent nonwords inconsistent nonwords mccann besner control nonwords 
criterion closely corresponds subjects considering response correct consistent pronunciation word training corpus considering inflected nonwords coda network achieves correct consistent inconsistent nonwords correct control nonwords 
network performance sets nonwords comparable subjects network trained logarithmic frequencies 
consistency effects 
shows mean cross entropy error network pronouncing words varying degrees spelling sound consistency function frequency 
main effect frequency main effect consistency interaction frequency consistency 
post hoc comparisons show effect frequency significant level words level consistency considered separately 
effect consistency significant low frequency words high frequency words 
post hoc comparisons low frequency words revealed difference error exception words ambiguous words significant difference regular consistent inconsistent words marginally significant difference ambiguous words regular inconsistent words fails reach significance 
pattern results matches empirical studies fairly 
training regime understanding normal impaired word reading table errors feedforward network trained actual frequencies pronouncing nonwords glushko mccann besner nonword correct response nonword correct response consistent nonwords control nonwords wos ns inconsistent nonwords tol bled bled bost bost bost ks koz kos gaf gat ks hove lom pl han put put fa pov sud sud jin wed wed won note pot cat bed hit dog keep bike hope boot boy cup ring chin thin 
activity levels correct missing phonemes listed parentheses 
cases actual response falls outside parentheses 
words marked remain errors considering properties training corpus explained text 
understanding normal impaired word reading cross entropy low high frequency exception ambiguous regular inconsistent regular consistent 
mean cross entropy error produced feedforward network trained actual frequencies words various degrees spelling sound consistency listed appendix function frequency 
balances influence frequency consistency network replicates pattern interaction variables naming latency reading words nonwords accurately skilled readers 
training moderate frequency compression sm argued training actual frequencies monosyllabic words provide best approximation experience readers 
example words consistent spelling sound correspondences base forms various inflections derivations training monosyllabic words underestimate reader exposure spelling sound regularities 
training compressed frequency range compensates bias exception words tend higher frequency regular words disproportionately affected compression 
seen severe logarithmic compression reduces effect frequency extent network representations amplify consistency effects fails exhibit exact pattern naming latencies empirical studies 
appropriate test severe compression results better match empirical findings 
mentioned earlier sm presenting words training probability proportional square root frequency replicates basic frequency consistency effects network data accuracy network performance 
accordingly worthwhile comparison purposes train network new representations square root compression word frequencies 
analogous actual frequencies scaling value word square root ku era francis frequency plus divided square root frequency plus 
value 
mean corpus median 
taraban mcclelland high frequency exception words average low frequency exception words average 
words ku era francis list value 
compression frequency severe logarithms substantial 
summed frequency training corpus accordingly global learning rate adjusted 
training procedure identical training actual word frequencies 
word reading 
epochs network correctly words training corpus homograph house states final final just fail active 
network word reading essentially perfect 
nonword reading 
network errors glushko consistent nonwords 
inconsistent nonwords network responses non regular pav consistent word training corpus correct 
network mccann besner control nonwords 
remain errors scoring criterion subjects ignoring inflected forms coda correct 
network trained square root frequencies nonwords slightly better network trained actual frequencies 
consistency effects 
shows mean cross entropy error network pronouncing words varying degrees spelling sound consistency function frequency 
significant effect frequency consistency interaction frequency consistency 
effect frequency significant level words level consistency considered separately 
high frequency words regular inconsistent ambiguous exception words significantly different regular consistent words 
low frequency words difference regular inconsistent words ambiguous words significant pairwise comparisons 
network replicates basic empirical findings effects frequency consistency naming latency 
summary sm simulation replicates empirical pattern frequency consistency effects appropriately balancing relative influences factors 
unfortunately reduced relative strength skilled readers 
fact understanding normal impaired word reading cross entropy low high frequency exception ambiguous regular inconsistent regular consistent 
mean cross entropy error produced feedforward network trained square root frequencies words various degrees spelling sound consistency listed appendix function frequency 
orthographic phonological representations disperse regularities spelling sound serves diminish relative impact consistency 
likewise logarithmic compression probability word presentations serves diminish impact frequency 
result reduced effectiveness consistency nonword reading suffers 
current uses representations better capture spelling sound regularities increasing relative consistency 
effect improve nonword reading level comparable skilled readers 
logarithmic frequency compression continues relative impact frequency weak network exhibits consistency effects high frequency words empirical studies 
appropriate relative balance frequency consistency restored maintaining nonword reading actual frequencies words training 
fact square root frequency compression moderate logarithmic replicates empirical naming latency pattern consistency effect high frequency words begins emerge 
way networks far trained logarithmic frequencies square root frequencies actual frequencies provide clear points comparison relative influences word frequency spelling sound consistency naming latency 
analytical results previous section findings suggest central empirical phenomena word nonword reading interpreted naturally terms basic principles operation connectionist networks exposed appropriately structured training corpus 
simulation interactivity componential attractors generalization outlined earlier current approach lexical processing basedon number general principles information processing loosely expressed acronym grain graded random adaptive interactive nonlinear 
principles distributed representations knowledge approach constitutes substantial departure traditional assumptions nature language knowledge processing pinker 
noted simulations far involve deterministic feedforward networks fail incorporate important principles interactivity randomness intrinsic variability 
part simplification necessary practical reasons interactive stochastic simulations far demanding computational resources 
importantly including relevant principles simulation enables detailed analysis specific contribution behavior system 
illustrated clearly current regard nature distributed representations orthography phonology relative influences frequency consistency network learning adaptivity 
network constitutes approximation abstraction complete simulation incorporate principles 
methodology considering sets principles separately relies assumption unforeseen problematic interactions principles findings simplified simulations generalize comprehensive ones 
current simulation investigates implications interactivity process pronouncing written words nonwords 
interactivity plays important role connectionist explanations number cognitive phenomena mcclelland elman mcclelland rumelhart mcclelland constitutes major point contention alternative theoretical formulations massaro 
processing network interactive units mutually constrain settling consistent interpretation input 
possible architecture network generalized allow feedback recurrent connections units 
example interactive activation model letter word perception mcclelland rumelhart rumelhart mcclelland letter units word units bidirectionally connected partial activation word unit feed back support activation letter units consistent 
common way interactivity employed networks making particular patterns activity stable attractors 
attractor network units interact update states repeatedly way initial pattern understanding normal impaired word reading activity generated input gradually settles nearest attractor pattern 
useful way conceptualizing process terms multidimensional state space activity unit plotted separate dimension 
instant time pattern activity units corresponds single point space 
units change states response input point moves state space eventually arriving attractor point corresponding network interpretation 
set initial patterns settle final pattern corresponds region attractor called basin attraction 
solve task network learn connection weights cause units interact way appropriate interpretation input attractor basin contains initial pattern activity input 
domain word reading attractors played critical role connectionist accounts nature normal impaired reading meaning hinton sejnowski hinton shallice plaut shallice 
accounts meanings words represented terms patterns activity large number semantic features 
features support structured frame representations minsky units represent conjunctions roles properties role fillers hinton 
small fraction possible combinations features correspond meanings actual words natural network learn semantic patterns attractors 
deriving meaning word orthography network need generate initial pattern activity falls appropriate semantic attractor basin settling process clean pattern exact meaning word 
system damaged initial activity word may fall neighboring attractor basin typically corresponding semantically related word 
damaged network settle exact meaning word resulting semantic error cat read dog 
fact occurrence errors hallmark symptom type acquired reading disorder known deep dyslexia see coltheart patterson marshall details full range symptoms deep dyslexia plaut shallice connectionist simulations replicating symptoms 
way attractors obviate need word specific units mediating orthography semantics see hinton mcclelland rumelhart discussion 
applied mapping orthography phonology interactivity form attractors characterization deriving word meanings necessarily oversimplified 
words multiple distinct meanings map number separate semantic attractors 
shades meaning contexts expressed semantic attractors regions semantic space single points 
notice conditions seen ends continuum involving various degrees similarity variability semantic patterns generated word contexts see mcclelland st john taraban 
appear problematic 
particular correct pronunciation nonword typically correspond pronunciation word 
network develops attractors word pronunciations expect input nonword captured attractor basin similar word resulting incorrect 
generally attractors appropriate tasks semantic categorization object recognition correct response novel input familiar output 
contrast oral reading correct response novel input typically novel output 
true attractors support sort generalization applicability reading specifically cognitive science generally fundamentally limited 
current simulation demonstrates concerns ill founded appropriately structured representations principle interactivity operate effectively phonological pathway semantic pathway see 
reason learning map orthography phonology network develops attractors componential substructure reflects common sublexical correspondences orthography phonology 
substructure applies words nonwords enabling pronounced correctly 
time network develops attractors exception words far componential 
hindrance attractors particularly effective style computation quasi regular tasks word reading 
advantage attractor network feedforward network modeling word reading provides direct analogue naming latency 
far followed sm error measure feedforward network account naming latency data subjects 
sm offer justifications approach 
assumption accuracy phonological representation word directly influence execution speed corresponding articulatory motor program see zorzi simulations embodying assumption 
assumption consistent view time required orthography phonology computation vary systematically word frequency spelling sound consistency 
case feedforward network sort sm takes amount time process input reasonable rendition nature phonological pathway subjects 
alternative justification error scores model naming latencies mentioned briefly sm view actual computation orthography phonology involves interactive processing time settle appropriate phonological representation vary systematically word type 
naming latencies exhibited subjects function settling time conjunction articulatory effects 
accordingly understanding normal impaired word reading phoneme units hidden units grapheme units 
architecture attractor network 
ovals represent groups units arrows represent complete connectivity group 
feedforward implementation mapping orthography phonology viewed abstraction recurrent implementation accurately approximate actual word reading system 
studying feedforward implementation informative properties including sensitivity frequency consistency depend computational principles operation apply recurrent implementation adaptivity distributed representations knowledge nonlinearity 
principles merely manifest differently influences reduce error feedforward network serve accelerate settling recurrent network 
error feedforward network valid approximation settling time recurrent network arise underlying causes additive frequency consistency effects context nonlinear gradual ceiling effect 
arguments important verify recurrent implementation reads words nonwords accurately skilled readers reproduces relevant empirical pattern naming latencies directly time takes settle pronouncing words 
method network architecture 
architecture attractor network shown 
numbers grapheme hidden phoneme units feedforward networks attractor network additional sets connections 
input unit connected hidden unit turn connected phoneme unit 
addition phoneme unit connected phoneme unit including phoneme unit sends connection back hidden unit 
weights connections pair units hidden unit phoneme unit trained separately need identical values 
including biases hidden phoneme units network total connections 
states units network change smoothly time response influences units 
particular unit state input input input input input time 
state time continuous unit initialized governed equation fixed external input units varying magnitude 
curves state values negative external input exact mirror images curves approaching 
instantaneous change time input unit proportional difference current input summed contribution units 
state unit standard logistic function integrated input ranges see equation 
clarity call summed input units plus bias external input unit distinguish integrated input governs unit state 
equation unit integrated put perfectly consistent external input derivative zero unit integrated input state ceases change 
notice activity point exactly standard unit computes state external input instantaneously feedforward network see equations 
illustrate provide sense temporal dynamics units network shows activity time single unit initialized governed equation response external input varying magnitude 
notice time unit state gradually approaches asymptotic value equal logistic function applied external input 
purposes simulation digital computer convenient approximate continuous units finite difference equations time discretized ticks understanding normal impaired word reading duration explicit superscripts discrete time rewritten equation unit input time tick weighted average current input dictated units weighting proportion 
notice limit discrete computation identical continuous 
adjustments affect accuracy discrete system approximates continuous alter underlying computation performed 
considerable practical importance computational time required simulate system inversely proportional relatively larger extensive training period current simulation minimizing computation time critical smaller testing accurate approximation desired 
long remains sufficiently small approximations adequate manipulations fundamentally alter behavior system 
training procedure 
training corpus network feedforward network trained actual word frequencies 
simulation frequency value word scale weight changes induced word 
network trained version back propagation designed recurrent networks known back propagation time rumelhart hinton williams williams peng adapted continuous units pearlmutter 
understanding back propagation time may help think computation standard back propagation layer feedforward network occurring time 
forward pass states input units clamped time 
hidden unit states computed input unit states output unit states computed hidden unit states 
backward pass error calculated output units states 
error hidden units weight changes hidden output connections calculated error output units states hidden units 
weight changes temporal dynamics somewhat different plaut mcclelland seidenberg network 
network unit input set instantaneously summed external input units unit state weighted average current state dictated instantaneous input 
input hidden connections calculated hidden unit error input unit states 
feedforward back propagation interpreted pass forward time compute unit states followed pass backward time compute unit error weight changes 
back propagation time exactly form recurrent network arbitrary connectivity unit receive contributions unit time just earlier layers forward pass layers backward pass 
means unit store state error time tick values available units needed 
addition states non input units affect units immediately need initialized neutral value current simulation 
respects back propagation time computationally equivalent feedforward back propagation 
fact back propagation time interpreted unfolding recurrent network larger feedforward network layer time tick composed separate copy units recurrent network see minsky papert rumelhart hinton williams 
order apply back propagation time continuous units propagation error backward pass continuous pearlmutter 
designate derivative error respect input unit feedforward back propagation cross entropy error function derivative logistic function 
discrete approxima tion back propagation time continuous units weighted average backwards time current value contribution current error unit 
way standard back propagation backward pass analogous forward pass cf 
equation 
output units interact units course processing stimulus indirectly affect error output units 
result error output unit sum terms error due discrepancy state target error back propagated units 
term referred error injected network training environment second term thought error internal network 
states output units vary time targets specify states understanding normal impaired word reading particular points time 
back propagation time error injected time ticks just feedforward back propagation 
targets vary time define trajectory output states attempt follow see pearlmutter demonstration type learning 
targets remain constant time output units attempt reach targets quickly possible remain 
current simulation technique train network form stable attractors pronunciations words training corpus 
possible states units change quickly receive large summed input units see 
large summed input units typically require amount time approach extremal value may completely reach 
result practically impossible units achieve targets immediately stimulus 
reason current simulation stringent training regime adopted 
network run units time error injected second unit time units receive direct pressure correct unit time back propagated internal error causes weight changes encourage units move appropriate states early possible 
addition output units trained targets error injected unit exceeds target reaches state target 
training criterion achieved units moderately large summed input see curve input 
feedforward network actual frequencies attractor network trained global learning rate adaptive connection specific rates momentum 
furthermore mentioned network trained discretization 
units update states times forward pass back propagate error times backward pass 
result computational demands simulation times feedforward simulations 
attempt reduce training time momentum increased epochs 
improve accuracy network approximation continuous system near training reduced epoch reduced epoch additional epochs training 
final stage training unit updated state times course processing input 
testing procedure 
fully adequate characterization response generation distributed connectionist networks involve stochastic processing see mcclelland scope 
approximation deterministic attractor network measure time takes network compute stable output response input 
specifically network responds average change states phoneme units falls criterion results 
point network naming latency amount continuous time passed processing input naming response generated basis current phoneme states procedure feedforward networks 
results word reading 
epochs training network correctly words training corpus correct 
half errors regularizations low frequency exception words sieve sev suede tow tw 
remaining errors classified visual errors fall ps merely consonants failed reach threshold ba ar wound und 
network come close mastering training corpus performance slightly worse equivalent feedforward network 
network settles representation phonemes word parallel time takes increases length word 
demonstrate entered naming latencies network words correctly multiple linear regression predictors orthographic length number letters phonological length number phonemes logarithmic word frequency measure spelling sound consistency equal number friends including word divided total number friends enemies highly consistent words values near exception words values near 
collectively factors account variance latency values 
importantly factors account significant unique variance factoring consistency log frequency orthographic length phonological length respectively 
particular orthographic length positively correlated naming latency accounts uniquely variance 
convert correlation increase rt letter network mean rts taraban mcclelland high low frequency exception words regular consistent controls regressed subject means reported taraban mcclelland resulting scaling msec unit simulation time intercept msec 
scaling effect orthographic length network msec correlation rt factoring predictors msec letter direct correlation rt 
length effects magnitude low specific criterion chosen gives rise mean response times units time network trained criteria produce qualitatively equivalent results 
understanding normal impaired word reading range empirical studies effects vary greatly subjects reading skill butler specific stimuli testing conditions see henderson 
nonword reading 
table lists errors network pronouncing lists nonwords glushko mccann besner 
network produces regular pronunciations glushko consistent nonwords inconsistent nonwords mccann besner control nonwords 
accept correct pronunciation consistent word training corpus body ignore inflected words coda network correctly inconsistent nonwords control nonwords 
performance network consistent nonwords somewhat worse feedforward networks equal level performance glushko reported subjects see table 
ability attractor network pronounce nonwords comparable skilled readers 
frequency consistency effects 
shows mean latencies network pronouncing words varying degrees spelling sound consistency function frequency 
low frequency exception words taraban mcclelland list withheld analysis pronounced incorrectly network 
remaining words significant main effects frequency consistency significant interaction frequency consistency 
effects obtain comparison regular exception words frequency consistency frequency consistency 
considering level consistency separately effect frequency significant exception words ambiguous words marginally significant regular inconsistent words 
effect frequency regular words 
naming latencies network show significant effect consistency low frequency words high frequency words 
low frequency words regular consistent words significantly different types regular inconsistent ambiguous exception words significantly different comparison regular inconsistent exception words significant 
high frequency words pairwise comparisons significant regular exception words 
naming latencies network replicate standard effects frequency consistency empirical studies 
naming latency low high frequency exception ambiguous regular inconsistent regular consistent 
naming latency attractor network trained actual frequencies words various degrees consistency listed appendix function frequency 
network analyses network success word reading demonstrates training developed attractors pronunciations words 
capable reading nonwords novel pronunciations 
isn input nonword captured attractor orthographically similar word gave move 
carried number analyses network gain better understanding ability read nonwords 
nonword reading involves recombining knowledge derived word pronunciation primarily concerned separate parts input contribute correctness parts output hidden representation word 
naming latency item withheld analyses network 
componential attractors 
analysis measures extent phonological cluster onset vowel coda depends input orthographic cluster 
specifically word activity active grapheme units particular orthographic cluster gradually reduced network rerun phonemes particular phonological cluster longer correct 
boundary activity level measures important input particular orthographic cluster correctness particular phonological cluster value means graphemes cluster completely active value means phonemes completely insensitive graphemes cluster 
state space boundary level corresponds radius word attractor basin particular direction assuming state space includes dimensions grapheme final considered part orthographic vowel cluster 
understanding normal impaired word reading table errors attractor network pronouncing nonwords glushko mccann besner nonword correct response nonword correct response consistent nonwords control nonwords hod kaz wus inconsistent nonwords vol bled bled ond bost bost bost koz kos fa lom yam jin put put pov pav sud sud sul wed wed won wus note pot cat bed hit dog keep bike hope boot boy cup ring chin thin 
activity levels correct missing phonemes listed parentheses 
cases actual response falls outside parentheses 
words marked remain errors considering properties training corpus explained text 
understanding normal impaired word reading units 
procedure applied taraban mcclelland regular consistent regular inconsistent exception words corresponding set ambiguous words see appendix 
words excluded analysis lacked orthographic onset coda 
resulting boundary values combination orthographic phonological clusters subjected anova frequency consistency item factors orthographic cluster phonological cluster item factors 
regard frequency high frequency words lower boundary values low frequency words vs respectively 
frequency interact consistency orthographic phonological cluster respectively 
consider high low frequency words remainder analysis 
strong effect consistency boundary values effect interacts orthographic cluster phonological cluster 
presents average boundary values orthographic cluster function phonological cluster separately words level consistency 
type word set bars phonological cluster indicates sensitive cluster input orthographic cluster 
considering regular consistent words shows phonological cluster depends entirely corresponding orthographic cluster little clusters 
instance vowel coda graphemes completely removed affecting network pronunciation onset 
slight interdependence vowel coda consistent fact word bodies capture important information pronunciation see press 
phonological vowel coda cluster depends orthographic onset cluster 
regular word alternative onset substituted pronounced depending affecting pronunciation body producing correct pronunciation nonword 
similarly regular inconsistent ambiguous exception words correctness phonological onset coda relatively independent non corresponding parts orthographic input 
pronunciation vowel increasingly dependent orthographic consonants consistency decreases main effect consistency pairwise comparisons 
fact spelling sound inconsistency english involves unusual vowel pronunciations 
interestingly exception words vowel pronunciation sensitive orthographic vowel surrounding consonant context orthographic onset vs vowel orthographic activity boundary orthographic activity boundary orthographic activity boundary orthographic activity boundary regular consistent orthographic onset orthographic vowel orthographic coda onset regular inconsistent vowel coda onset ambiguous vowel coda onset exception vowel coda onset vowel phonological cluster coda 
degree activity orthographic cluster required activate phonological cluster correctly words various spelling sound consistency listed appendix 
words lacking onset coda consonant cluster orthography excluded analysis 
understanding normal impaired word reading coda vs vowel 
sense orthographic vowel exception word misleading indicator phonological vowel 
contrast regular consistent words words ambiguous exceptional vowel pronunciations depend entire orthographic input pronounced correctly 
effects understood terms nature attractors develop training different types words 
relative independence onset vowel coda correspondences indicates attractor basins regular words consist separate orthogonal sub basins cluster 
word network settles region state space sub basins overlap corresponding word pronunciation 
sub basin apply independently spurious attractor basins exist sub basins parts words overlap see 
combinations corresponds nonword network pronounce correctly appropriate orthographic input 
arises directly degree network representations explicit structure task 
minimizing extent information replicated representations condense regularities orthography phonology 
small portions input output relevant particular regularity allowing operate independently regularities 
attractor basins exception words contrast far componential regular words unfortunately depicted adequately dimensional diagram 
way network pronounce exception words generalize nonwords 
important note attractors exception words exceptional aspects monolithic way 
particular consonant clusters exception words combine correct vowel phoneme depends entire orthographic input 
word pint sense quarters regular consonant correspondences contribute pronunciations regular words nonwords just items 
traditional dual route characterization lexical look procedure exception words fails justice distinction 
development learning 
gain insight development returning simple layer hebbian network formed basis frequency consistency equation see see van orden related discussion 
expressed equation value weight network equal sum training patterns weighted learning rate product state input unit state output unit patterns input state contribute sum contribute value output state formulation 
value onset trained attractor word spurious attractor nonword bo vowel dy ny 
depiction componential attractors words recombine support pronunciations nonwords 
attractor basins words consist orthogonal sub basins clusters depicted 
spurious attractors nonwords exist sub basins parts words overlap 
support aspects attractors exception words sub basins vowels region relevant consonant clusters distorted somewhat dimensions state space ones depicted 
understanding normal impaired word reading weight re expressed terms counts number consistent patterns states units positive number inconsistent patterns positive negative 
patterns differ frequency occurrence counts simply cumulative frequencies see equation clarity presentation leave see reggia simulation directly frequencies 
consider word pint pint 
entire set words onset typically occur cf 
phone large small weight units strongly positive 
contrast occurs example onset large leading strongly negative weight 
onset letters cooccur positive resulting weight negative 
going step onset occur virtually orthographic vowel coda relevant connections larger weight closer zero 
phoneme inactive words weights graphemes non corresponding clusters tend moderately negative hebbian learning 
error correcting learning weights remain near zero weights corresponding clusters sufficient effective due higher unit correlations eliminating error 
properties hold coda 
unit correlations entire corpus give rise componential pattern weights consonant phonemes significant values connections units corresponding orthographic cluster see smolensky additional relevant simulations 
situation bit complicated vowels 
far variability words pronunciation vowels compared consonants see 
consequently connections vowel graphemes phonemes generally smaller larger corresponding onset coda con 
critical issue concerns exceptional vowel pronunciations words pint 
correspondence small overwhelmed large comes common corre state 
furthermore hebbian learning correlations consonants weak help 
error correcting learning compensate degree allowing weights consonant units grow larger dictated correlation pressure eliminate error 
note reduces vowel phoneme weights 
cross cluster weights provide general solution pronouncing exception words diverse corpus consonants able exist vowel pronunciations pant 
order network achieve correct pronunciations exception words maintaining regular words nonwords error correction combined hidden units order re represent similarities words way reduces interference inconsistent neighbors discussed earlier 
internal representations 
analysis established attractors regular words behaviorally second showed arises nature learning simpler related system 
know simultaneously supporting componential aspects word reading system requires hidden units error correction characterize accomplished 
obvious possibility raised feedforward networks network partitioned sub networks fully componential regular words nonwords componential exception words 
case 
apply criterion hidden unit important pronouncing item removal increases total error item significant positive correlation numbers exception words numbers orthographically matched nonwords listed appendix hidden units important 
hidden units specialized processing particular types items 
questions remains attractor network single mechanism implements componential attractors regular words nonwords componential attractors exception words 
second analysis attempts characterize degree hidden representations regular versus exception words reflect differences attractors 
specifically attempted determine extent contribution orthographic cluster hidden representation depends context occurs words componential representations 
example consider onset exception word pint 
onset need generate pronunciation 
context int contribute altering vowel 
contrast regular word pine onset plays role context ine isolation 
hidden representations regular words componential exception words contribution onset greatly affected presence exception context int regular context ine 
measured contribution orthographic cluster particular context computing hidden representation generated cluster context pint subtracting unit unit baseline condition hidden representation generated context int 
contribution cluster isolation computed understanding normal impaired word reading contribution correlation exception regular words words onset vowel orthographic cluster coda 
similarity correlations contribution orthographic cluster hidden representation context remaining clusters versus isolation taraban mcclelland exception words regular consistent control words 
similarly baseline condition case representation generated network input grapheme units set 
correlation vector differences measure similarity contribution cluster conditions 
high correlation indicates contribution cluster hidden representation independent presence clusters reflects high degree 
contribution correlations computed separately onset vowel coda clusters taraban mc exception words frequency matched regular consistent control words 
words lacking onset coda withheld analysis 
correlations remaining words subjected anova frequency consistency item factors orthographic cluster item factor 
main effect frequency significant interaction frequency consistency orthographic cluster factor considered 
shows average correlations regular exception words function orthographic cluster 
significant interaction consistency orthographic cluster 
significant main effect cluster vowel cluster producing lower correlations consonant cluster vowel vs onset vowel vs coda 
importantly regular words higher correlations exception words means standard deviations vs respectively 
contributions orthographic clusters hidden representations independent context regular words exception words 
sense representations regular words componential 
surprising average correlations exception words lower regular words quite high considerable overlap distributions 
furthermore representations regular words completely componential correlations significantly 
apparently hidden representations words slightly reflect spelling sound consistency 
alternative possibility representations capture predominantly orthographic information range levels structure individual graphemes combinations clusters cf 
shallice mccarthy 
case low order orthographic structure individual graphemes clusters support componential attractors regular words 
presence higher order structure representation clusters regular exception words somewhat sensitive context occur 
importantly higher order structure particular useful pronouncing exception words overriding phonological layer standard spelling sound correspondences individual clusters 
way aspects attractors exception words exist componential attractors regular words 
provide evidence bearing explanation final analysis carried determine extent hidden representations organized basis orthographic opposed phonological similarity 
hidden representations set items organized orthographically phonologically extent pairs items similar hidden representations similar orthographic phonological representations 
put generally sets representations groups units structure extent induce relative similarities items 
control contribution orthography possible analysis involved triples consisting nonword regular inconsistent word exception word share body mint pint listed appendix 
item triple computed similarity hidden representation hidden representations items type measuring similarity correlation unit activities 
similarities orthographic representations phonological representations computed analogously 
orthographic hidden phonological similarity values item correlated pairwise fashion orthographic phonological hidden orthographic 
presents means correlation values nonwords regular words exception words function pair representation types 
consider correlation orthographic understanding normal impaired word reading correlation regular nonwords exception words words orth phon hidden orth representation pair hidden phon 
correlations orthographic hidden phonological similarities body matched nonwords regular inconsistent words exception words listed appendix 
phonological similarities 
values reflect relative amounts structure spelling sound mappings different types items 
values relatively high systematicity english word pronunciations exception words consonant clusters tend map consistently 
mappings exception words structured nonwords regular words paired respectively 
words orthographic similarity related phonological similarity exception words items 
sense defining characteristic exception words finding simply verifies representations simulations appropriate similarity structure 
interesting comparisons involve hidden representations 
shows similarities hidden representations types items highly correlated orthographic similarities phonological similarities pairwise comparisons 
representations nonwords regular words behave equivalently regard 
representations exception words show effect strongly having significantly phonological structure item types exception vs nonword paired exception vs regular paired 
may due reliance words high order orthographic structure override standard spelling sound correspondences 
consistent explanation offered hidden representations organized orthographically phonologically 
summary interactivity implementing attractors important computational principle connectionist accounts wide range cognitive phenomena 
tendency attractors capture similar patterns appear inappropriate tasks novel inputs require novel responses pronouncing nonwords oral reading current simulation shows appropriately structured representations leads development attractors componential structure supports effective generalization nonwords 
time network develops componential attractors exception words violate regularities task 
series analyses suggests componential aspects attractors supported hidden representations reflect orthographic information range levels structure 
way attractors provide effective means capturing regularities exceptions quasi regular task 
advantage attractor network domain temporal dynamics settling response provide direct analogue subjects naming latencies error feedforward network 
fact time takes network settle stable pronunciation response words varying frequency consistency replicates standard pattern empirical studies 
simulation surface dyslexia division labor phonological semantic pathways central theme current processing words nonwords exist connectionist networks employ appropriately structured orthographic phonological representations operate certain computational principles 
kept mind sm general lexical framework current contains pathways orthographic information influence phonological information phonological pathway semantic pathway see 
far ignored semantic pathway order focus principles govern operation phonological pathway 
view phonological semantic pathways support normal skilled reading 
example semantic involvement clearly necessary correct pronunciation homographs wind read 
furthermore semantic variable influences strength frequency consistency interaction naming latencies errors skilled readers strain patterson seidenberg press 
traditional dual route theories see coltheart coltheart lexical procedure influence output sublexical procedure account consistency effects regular words nonwords glushko 
understanding normal impaired word reading sm framework implied computational principles provides natural formulation contributions semantic phonological pathways integrated determining pronunciation written word 
critically formulated connectionist terms integration important implications nature learning pathways 
connectionist systems learning driven measure discrepancy error correct response generated system 
extent contribution pathway reduces error pathway experience pressure learn 
result may master items finds easiest learn 
specifically semantic pathway contributes significantly pronunciation words phonological pathway need master words 
tend learn best words high frequency consistency low frequency exception words may learned completely 
especially true intrinsic pressure network prevent overlearning example weights slight bias staying small 
course combination semantic phonological pathways fully competent 
readers equivalent overt skill may differ division labor pathways see baron strawson 
fact semantic pathway continues improve additional reading experience phonological pathway increasingly specialized consistent spelling sound mappings expense higher frequency exception words 
point brain damage reduced eliminated semantic pathway lay bare latent inadequacies phonological pathway 
way detailed consideration division labor phonological semantic pathways critical understanding specific patterns impaired preserved abilities brain damaged patients acquired dyslexia 
particular relevance context finding brain damage selectively impair nonword reading exception word reading leaving relatively intact 
phonological dyslexic patients read words regular exception better nonwords surface dyslexic patients marshall newcombe patterson read nonwords better exception words 
phonological dyslexia natural interpretation sm framework terms selective damage phonological pathway phonology see patterson marcel reading accomplished primarily exclusively patients semantic pathway 
pathway pronounce words provide useful support pronouncing nonwords definition items semantics 
lines mentioned previous section plaut shallice see hinton shallice series implementations semantic route provide com account deep dyslexia coltheart marshall newcombe form acquired dyslexia similar phonological dyslexia involving production semantic errors see friedman press glosser friedman arguments deep dyslexia simply severe form phonological dyslexia 
question exact nature impairment gives rise reading semantics phonological dyslexia interpretation account relevant findings taken general discussion 
surface dyslexia hand involve reading primarily phonological pathway due impairment semantic pathway 
purest fluent form mp behrmann bub bub kt mccarthy warrington shallice patients exhibit normal accuracy latency reading words consistent spelling sound correspondences reading nonwords exception words particularly low frequency giving pronunciation consistent standard correspondences sew sue 
ascribe errors influences consistency conventionally termed regularizations coltheart retained terminology 
frequency consistency interaction accuracy mirrors interaction latency exhibited normal skilled readers andrews seidenberg seidenberg taraban mcclelland waters seidenberg 
relevance semantic impairment surface dyslexia supported finding cases semantic dementia graham hodges patterson patterson hodges schwartz marin alzheimer type dementia patterson graham hodges surface dyslexic reading pattern emerges lexical semantic knowledge progressively deteriorates 
simulations phonological pathway sm similar surface dyslexic patients read aid semantics 
simulations provide direct account surface dyslexia read exception words skilled readers 
possibility surface dyslexia arises partial impairment phonological pathway addition severe impairment semantic pathway 
interesting possibility division labor ideas development operation phonological pathway shaped important way concurrent development semantic pathway surface dyslexia arises intact phonological pathway operates isolation due impairment semantic pathway 
sets simulations employed test adequacy 
set investigates effects damage attractor network developed previous simulation 
second involves new network trained context support semantics 
understanding normal impaired word reading phonological pathway lesions patterson 
investigated possibility surface dyslexia arise damage isolated phonological pathway 
lesioned sm model removing different proportions units connections measured performance regular exception words various frequencies 
damaged network pronunciation word compared correct pronunciation plausible alternative exception words regularized pronunciation 
patterson colleagues damage regular exception words produce equal amounts error effect frequency reading exception words 
exception words regular words produce alternative pronunciation comparison phonemic features errors revealed network showed greater tendency produce regularizations errors differ correct pronunciation number features 
damaged network failed show frequency consistency interaction high proportion regularization errors exception words characteristic surface dyslexia 
detailed procedure analyzing responses patterson removing hidden units produced better performance regular versus exception words trend frequency interaction 
shows analogous data instances lesions replication sm network hidden unit probability removed 
plotted severity damage percent correct taraban mcclelland low frequency exception words regular consistent control words percent errors exception words regularizations percent correct glushko nonwords counting correct pronunciation consistent word body training corpus 
shown corresponding data surface dyslexic patients mp behrmann bub bub kt mccarthy warrington 
milder lesions produce match mp performance taraban mcclelland words 
severe lesions fail simulate dramatic effects shown kt 
damaged network kt perform equally highfrequency exception words network impaired low frequency exception words impaired high low frequency regular words 
addition severe damage third net errors exception words regularizations just half nonwords pronounced correctly severe damage figures lower 
contrast mp kt produce regularization rates near perfect nonword reading 
attempts percent correct hf reg lf reg hf patient patient mp sm kt sm exc lf exc reg nonwords 
performance surface dyslexic patients mp behrmann bub bub kt mccarthy warrington replication sm model lesioned removing hidden unit probability results averaged lesions 
correct performance taraban mcclelland high frequency hf low frequency lf regular consistent words reg exception words exc glushko nonwords 
reg approximate percentage errors exception words regularizations 
account surface dyslexia damaging sm model satisfactory see behrmann bub coltheart criticisms 
possible explanation failing parallels explanation sm model poor nonword reading due representations relevant structure orthography phonology sufficiently explicit 
essence influence spelling sound consistency model weak 
weakness contributing inability simulate surface dyslexia severe damage regular word reading nonword reading regularization rates low 
interpretation leads possibility network trained appropriately structured representations damaged successfully replicate surface dyslexic reading pattern 
method 
attractor network lesioned removing hidden unit connection groups units probability adding noise weights connections groups units 
case severity damage depends standard deviation sd noise higher sd constitutes severe impairment 
form damage advantage permanent removal units connections reducing possibility idiosyncratic effects lesions particular units connections 
shallice pointed effects network simulation little understanding normal impaired word reading interest study cognitive effects damage brain vast difference scale systems see plaut press 
general simulation studies comparing effects adding noise weights effects removing units connections hinton shallice procedures yield qualitatively equivalent results 
instances type lesion range severities administered main sets connections attractor network graphemes hidden hidden phonemes phonemes hidden phonemes phonemes hidden units 
lesion operation network input procedure determining response exactly simulation 
evaluate effects lesions network tested taraban mcclelland high low frequency regular consistent words exception words glushko nonwords 
words addition measuring correct performance calculated percentage errors exception words correspond regularized pronunciation 
full list responses accepted regularizations appendix 
network word item included calculation regularization rates 
nonwords pronunciation accepted correct consistent pronunciation word training corpus body see appendix 
results discussion 
shows data attractor network weights main sets connections corrupted noise varying severities 
milder lesions graphemes hidden connections top left produce clear interactions frequency consistency correct performance word reading 
instance adding noise sd network correctly regular words high frequency exception words exception words 
addition lesions errors exception words nonwords pronounced correctly 
compared results lesions hidden units sm network show stronger effect consistency better match performance mp regularization rate somewhat low see 
predicted representations better capture spelling sound structure produces stronger frequency consistency interaction regularizations better nonword reading 
see case imagine larger network role weight smaller network accomplished collective influence large set weights 
instance replace connection small network set connections weights positive negative sum weight original connection 
randomly removing proportion connections large network shift mean set weights effect adding random amount noise value corresponding weight small network 
sm network severe lesions replicate pattern shown kt 
lesions reduce correct performance high frequency exception words equivalent levels sd network kt impair low frequency exception words sufficiently network kt kt impair high regular words network kt respectively 
furthermore kt substantial drop regularization rate network kt performance nonwords network kt 
lesions sets connections produce broadly similar weaker results frequency consistency interactions weaker especially severe lesions impairment regular words severe hidden lesions regularization rates lower note different range lesion severities hidden phonemes connections sensitive noise 
summary mild grapheme hidden lesions attractor network account mp behavior severe lesions reproduce kt behavior 
negative findings specific noise network removing units connections produces qualitatively equivalent results regularization rates lower 
illustrate table presents data patients attractor network mild severe lesions graphemes hidden connections hidden units hidden phonemes connections 
levels severity chosen approximate performance mp kt low frequency exception words 
summary types lesion network implementation phonological pathway may able approximate impaired pattern performance shown mp unable account dramatic pattern results shown kt 
findings suggest impairment phonological pathway may play role behavior surface dyslexic patients provide complete explanation patients particular normal nonword reading severely impaired exception word reading 
phonological semantic division labor consider alternative view surface dyslexia reflects behavior isolated phonological pathway learned depend support semantics normal reading 
previous simulations phonological pathway trained fully competent 
explanation surface dyslexia holds entails relationship simulations normal skilled word reading system 
current simulation involves training new network context approximation contribution semantics 
including full implementation semantic pathway understanding normal impaired word reading percent correct percent correct hf reg lf reg hf lesion severity sd sd sd sd sd hf reg lf reg hf lesion severity sd sd sd sd sd exc lf exc reg nonwords graphemes hidden lesions exc lf exc reg nonwords phonemes hidden lesions percent correct percent correct hf reg lf reg hf lesion severity sd sd sd sd sd hf reg lf reg hf lesion severity sd sd sd sd sd exc lf exc lf exc reg nonwords hidden phonemes lesions exc reg nonwords phonemes phonemes lesions 
performance attractor network lesions various severities main sets connections weights corrupted noise mean zero standard deviation sd indicated 
correct performance taraban mcclelland high frequency hf low frequency lf regular consistent words reg exception words exc glushko nonwords 
reg percentage errors exception words regularizations 
understanding normal impaired word reading table performance attractor network lesions units connections correct performance hf reg lf reg hf exc lf exc reg nonwords patient mp patient kt attractor network lesions graphemes hidden hidden units hidden phonemes note probability specified units connections removed network lesion results averaged instances lesions 
correct performance taraban mcclelland high frequency hf low frequency lf regular consistent words reg exception words exc glushko nonwords 
reg percentage errors exception words regularizations 
bub 
see behrmann bub 
patterson mccarthy warrington 
approximate patterson 
course scope 
characterize operation pathway solely terms influence phoneme units phonological pathway 
specifically extent semantic pathway learned derive meaning pronunciation word provides additional input phoneme units pushing correct activations 
accordingly approximate influence semantic pathway development phonological pathway training presence amount appropriate external input phoneme units 
difficult issue arises immediately context approach concerning time course development semantic contribution training phonological pathway 
presumably mapping semantics phonology develops large part prior reading acquisition part speech comprehension production 
contrast orthography semantics mapping orthography mapping obviously develop learning read 
fact semantic pathway substantial contribution oral reading phonological pathway developed part phonological nature typical reading instruction part english orthography phonology mapping far structured orthography semantics mapping 
degree learning semantic path way sensitive frequency words encountered 
accordingly coarse approximation assume strength semantic contribution phonology reading increases gradually time stronger high frequency words 
acknowledged characterization semantics fails capture number properties actual word reading system certainly important contexts lexical factors influence contribution semantics phonology interactivity phonology semantics relative time course processing semantic phonological pathways 
manipulation external input phoneme units allows investigate central claim proposed explanation surface dyslexia partial semantic support word pronunciations alleviates need phonological pathway master words support eliminated brain damage surface dyslexic reading pattern emerges 
method 
apparent necessary simulation requires times training epochs corresponding previous simulation 
attractor network trained actual word frequencies developed due limitations available computational resources 
simulation involved training feedforward network square root compression word frequencies 
network understanding normal impaired word reading produces pattern results word nonword reading quite similar attractor network see simulation 
importantly specific feedforward nature network necessary produce results reported attractor network trained analogous conditions expected produce qualitatively equivalent results 
network trained learning parameters corresponding network simulation change small amount weight decay reintroduced weight experiences slight pressure decay zero proportional constant current magnitude 
mentioned context simulation provides bias small weights prevents network overlearning encourages generalization see hinton 
demonstrated weight decay alter ability network replicate patterns normal skilled performance words nonwords 
course training magnitude input phoneme units putative semantic pathway word set log ku era francis frequency word training epoch 
parameters external input phonemes contribution semantic pathway low high frequency frequency training epoch 
magnitude additional external input supplied phoneme units putative semantic pathway function training epoch taraban mcclelland high low frequency words 
perfect epoch 
point words read correctly 
significant log main effects frequency consistency significant interaction frequency consistency cross entropy error produced words means hfe 
network exhibits determine asymptotic level input time asymptote respectively 
values current simulation generally specific analytic function approximate development semantic pathway affect quantitative qualitative aspects results reported 
shows mean values function training epochs taraban mcclelland high low frequency words 
word correct state phoneme unit external input positive magnitude negative 
purposes comparison second version network trained semantics exactly learning parameters initial random weights 
results discussion 
learning network trained semantics reached asymptote epoch point pronounced correctly words training corpus correct 
shows performance network taraban mcclelland high low frequency exception words regular consistent control words glushko nonwords course training 
performance regular words nonwords improves quite rapidly epochs reaching words nonwords point 
performance high frequency exception words improves somewhat slowly 
contrast performance standard pattern normal skilled readers weight decay training substantially altered basic influences frequency consistency network 
current context network trained concurrently increasing contribution semantics shown direct analogue normal reader 
surprisingly performance improves rapidly case 
high frequency exceptions pronounced correctly epoch exceptions correct 
epoch low frequency exceptions correct nonword reading correct assume nonwords receive contribution semantics 
point network semantics exhibits standard effects frequency consistency cross entropy error means hfe frequency consistency frequency consistency 
considerable amount additional training epoch division labor semantic phonological pathways changes considerably shown overt behavior normal combined network shows pattern effects nonword reading correct word cross entropy error means hfe frequency consistency frequency consistency 
low frequency exception words improves far slowly finding may help explain previous sim understanding normal impaired word reading percent correct training semantics training epoch lf hf reg hf reg lf exc nonwords exc 
correct performance network trained semantics function training epoch taraban mcclelland high frequency hf low frequency lf regular consistent words reg exception words exc glushko nonwords 
networks trained fully competent replicate effects frequency consistency naming latency current perspective simulations fully adequate characterizations isolated phonological pathway skilled readers 
reason performance near asymptote due extended training semantic support word frequency spelling sound consistency affect relative effectiveness processing different words way 
asymptotic behavior follows frequency consistency equation see equation 
increasing training increasing equation adding additional semantic term sum serves equally drive units extremal values see general discussion 
shows performance network point training contribution semantics eliminated complete semantic lesion data reflect underlying competence phonological pathway trained context concurrently developing semantic pathway 
notice simulation involves training epochs bulk overt reading acquisition occurs epochs 
effects network thought reflecting gradual improvement skill reading experience human system spans decades 
initially performance nonwords types words improves phonological pathway gains competence task network trained semantics see 
semantic pathway increases strength characterized curves percent correct effects semantic lesion lf hf reg hf reg lf exc regularizations exc nonwords training epoch 
performance network trained semantics semantic lesion function training epoch semantics eliminated taraban mcclelland high frequency hf low frequency lf regular consistent words reg exception words exc glushko nonwords approximate percentage errors exception words regularizations 
understanding normal impaired word reading accuracy combined network pronunciations words improves faster recall combined network perfect taraban mcclelland words epoch 
pressure continue learn phonological pathway diminished 
eventually epoch pressure balanced bias weights remain small 
point error remains comes low frequency exception words 
error reduced semantic pathway continues increase contribution pronunciation words 
result pressure weights decay longer balanced error weights smaller 
causes deterioration ability phonological pathway pronounce low frequency exception words 
semantic improvement processing high frequency exception words phonological pathway begins suffer 
virtually errors exception words result process regularizations plotted asterisks 
larger weights particularly important exception words override standard spelling sound correspondences implemented smaller weights 
furthermore high frequency words susceptible degradation decrement overt performance induces larger weight changes compensate 
contrast processing regular words nonwords relatively unaffected gradual reduction weight magnitudes 
low frequency regular words just affected epoch 
extended reading experience redistribution labor model semantic phonological pathways 
semantic pathway gains competence phonological pathway increasingly specializes consistent spelling sound correspondences expense exception words 
notice extended training phonological pathway continues able read exception words particularly highfrequency 
way quite sublexical procedure traditional dual route theory read regular words exception words 
important keep mind normal overt performance supported combination phonological semantic pathways fully accurate early continues improve naming latency reflected indirectly error 
interpretation surface dyslexia differences patients ability read exception words may reflect differences severities brain damage 
may reflect differences division labor pathways patients exhibiting severe impairment relied greater extent semantic support 
illustrate directly presents data mp kt data network different points training semantics eliminated 
network epoch provides close match mp performance network epoch percent correct hf reg lf reg hf patient patient mp epoch kt epoch exc lf exc reg nonwords 
performance surface dyslexic patients mp behrmann bub bub kt mccarthy warrington network different points training semantics eliminated 
correct performance taraban mcclelland high frequency hf low frequency lf regular consistent words reg exception words exc glushko nonwords 
reg approximate percentage errors exception words regularizations 
matches kt performance 
substantial discrepancy conditions network rate regularizations higher corresponding patient patient data approximate see patterson 
far assumed surface dyslexic patients fluent type lesion completely eliminates contribution semantic pathway reading 
assumption may reasonable mp kt patients severe impairments written word comprehension 
mp chance selecting written words semantically related word picture bub see bub black 
kt severe word comprehension deficit prevented scoring vocabulary similarities wechsler adult intelligence scale wais bed bed know bed mccarthy warrington 
patients fluent surface dyslexia appear partial impairment semantic pathway 
particular patients semantic dementia reading tested detail large majority exhibit surface dyslexic pattern severity reading disorder correlated degree semantic deterioration graham patterson hodges see warrington 
similar finding applies patients alzheimer type dementia patterson 
cases natural interpretation current context terms performance understanding normal impaired word reading percent correct strength semantics lf hf reg reg hf lf exc exc regularizations nonwords percent correct 
effect gradual elimination semantics correct performance network epochs training semantics taraban mcclelland high frequency hf low frequency lf regular consistent words reg exception words exc glushko nonwords approximate percentage errors exception words regularizations 
network partial complete elimination contribution putative semantic pathway 
illustrate effect shows performance network trained semantics epoch strength semantic contribution phoneme units parameter equation gradually reduced 
semantics degrades performance low frequency exceptions affected followed high frequency exceptions 
contrast performance regular words nonwords relatively unaffected semantic deterioration performance low frequency regular words somewhat impaired semantics completely eliminated data identical epoch 
fact semantic dementia patients exhibit drop performance low frequency regular words semantic impairment severe patterson hodges 
course patient progressive dementia may amount deterioration phonological pathway 
table illustrate impairment tend degrade performance exception words affect performance regular words nonwords degree 
observation surface dyslexic reading association degraded semantics disrupted mapping semantics phonology account effect common reported languages english including dutch italian miceli caramazza japanese patterson suzuki press 
im portant note cases suggest may individual differences extent pronunciation low frequency exceptions depends contributions semantics 
patient wlp schwartz marin thoroughly studied cases disease history cognitive neuropsychology 
wlp began regularization errors low frequency exception words stage disease period testing semantic disorder marked exception word reading largely intact 
dramatically warrington reported patient substantial loss meaning low frequency words comprehension high frequency words measured difficult task producing word definitions intact 
performance reading low frequency exception words perfectly intact reported regularization errors shoe show 
account observations suggest individuals phonological pathway developed relatively high degree competence assistance semantics post hoc interpretation clearly requires independent source evidence 
final comment respect phonological dyslexia appropriate 
recall phonological dyslexic patients able read words better nonwords 
current simulation external input phoneme units represents contribution semantic pathway sufficient support accurate word reading nonword reading 
hand severe damage phonological pathway certainly impairs nonword reading see table 
limit complete lesion orthography phonology nonword reading impossible 
lesion network severely impaired phonological pathway leaving contribution semantics phonology relatively intact replicate basic characteristics phonological dyslexia 
summary detailed patterns behavior acquired dyslexic patients provide important constraints nature normal word reading system 
relevant patients current context fluent surface dyslexia networks read aid semantics 
patients read nonwords normally exhibit frequency consistency interaction word reading accuracy low frequency exception words particularly error prone typically produce regularization errors 
patterson 
patterson relatively unsuccessful replicating surface dyslexia reading pattern damaging sm model 
current simulations employ appropriately structured representations damaged fail produce surface dyslexia particularly severe form exhibited kt mccarthy warrington 
understanding normal impaired word reading findings call question interpretation surface dyslexia arising partial impairment phonological pathway addition extensive impairment semantic pathway 
better match surface dyslexic reading pattern mild severe forms produced normal operation isolated phonological pathway developed context support semantic pathway 
finding supports view normal word reading system division labor phonological semantic pathways pathway completely competent support skilled word nonword reading 
general discussion current develops connectionist approach processing quasi regular domains exemplified english word reading 
approach derives general computational principles processing graded random adaptive interactive nonlinear representations knowledge distributed mcclelland 
instantiated specific domain oral reading principles lead view reading system learns gradually sensitive statistical structure orthographic phonological semantic representations representations simultaneously constrain interpreting input 
support view series connectionist simulations normal impaired word reading 
consideration shortcomings previous implementation seidenberg mcclelland reading nonwords led development orthographic phonological representations capture better relevant structure written spoken forms words 
simulation feedforward network employing representations learned pronounce large corpus monosyllabic words including exception words pronounced nonwords skilled readers 
analysis effects word frequency consistency related simpler system formed basis understanding empirical pattern naming latencies reflecting appropriate balance factors 
simulation feedforward network trained actual word frequencies exhibited word nonword reading replicated frequency consistency interaction amount error produced words various types 
simulation recurrent network replicated effects frequency consistency naming latency directly time required settle stable pronunciation 
critically attractors network developed words course training componential structure supported nonword reading 
simulation role semantic pathway oral reading considered context acquired surface dyslexia patients read nonwords exhibit frequency consistency interaction naming accuracy typically regularizing low frequency exception words 
view symptoms particularly severe form reflect operation partially impaired phonological pathway supported behavior attractor network variety types damage 
simulation supported alternative interpretation surface dyslexia reflects normal operation phonological pathway fully competent learned rely support semantic pathway subsequently impaired brain damage 
alternative perspectives word reading raise consider light results summarized issues concerning nature reading process 
general agreement pathways contribute reading words nonwords aloud leaves open number fundamental questions 
underlying explanatory principles determine existence character different pathways 
operation arise fundamental principles particular principles pathway adheres 
different pathways combine contribute word nonword reading 
consider different approaches questions 
view called dual route view holds fundamental explanatory principle domain word reading distinctly different mechanisms necessary reading nonwords hand exception words 
mechanisms operate fundamentally different ways 
assembles pronunciations phonemes generated application grapheme phoneme correspondence rules 
maps orthographic inputs phonological outputs lexical lookup procedure formulations associative network pinker mcclelland rumelhart interactive activation model coltheart coltheart 
alternative view connectionist approach holds fundamental explanatory principle domain word reading underlying mechanism employs nonlinear similarity activation process conjunction frequency sensitive connection weight adjustment process 
pathways necessary reading different principles apply items different types different tasks performed 
pathway termed phonological pathway performs task transforming orthographic representations phonological representations directly 
semantic pathway performs tasks 
specific reading transformation orthographic representations semantic representations 
second general aspect language transformation semantic representations phonological representations 
understanding normal impaired word reading glance views may appear similar deciding hardly worth effort 
lexical procedure dual route account semantic pathway connectionist account read words nonwords sublexical procedure phonological pathway critical nonword reading better regular words exception words 
tempting conclude explanatory perspectives converging essentially processing system 
neglects subtle important differences theoretical empirical consequences approaches 
case point sublexical gpc procedure account sensitive word frequency storage lexical items 
contrast connectionist approach phonological pathway maintains intrinsic sensitivity word frequency spelling sound consistency see monsell 
sensitivity captured approximate form frequency consistency equation equation expresses strength response simple layer network test pattern terms frequency overlap training patterns 
connectionist approach reflected equation predicts complete dissociation frequency consistency effects phonological pathway exhibit sensitivity 
sensitivity takes specific form items frequent consistent advantage items frequent consistent items frequent consistent may enjoy large additional advantage frequent consistent frequency consistency increases sensitivity differences decreases 
relationship previously discussed approximately characterized frequency consistency equation reproduce form elaborated include term contribution semantic pathway separating contributions training patterns outputs consistent test pattern called friends jared outputs inconsistent enemies 
accordingly output phoneme unit test pattern state reported apparent dissociation frequency consistency naming latencies patients alzheimer type dementia increasing levels severity impairment 
patients substantial numbers errors usual relationship frequency consistency holds accuracy data see patterson 
furthermore dissociation naming latencies younger older normal subjects 
written logistic activation function applied contribution semantic pathway plus contribution phonological pathway sum terms scaled learning rate cumulative frequency training pattern sum frequencies friends indexed times overlap test pattern sum frequencies enemies indexed times overlap test pattern 
kept mind equation approximate networks hidden units trained error correction 
aspects implemented networks critical help overcome interference enemies negative terms equation enabling networks achieve correct performance exception words words enemies friends regular words nonwords 
basic phenomena domain word reading seen natural consequences adherence frequency consistency equation 
general factor serves increase summed input activation function equation improves performance measured naming accuracy latency 
frequent words read better forster chambers higher values words greater spelling sound consistency read better glushko jared positive sum friends outweighs negative sum enemies 
nonlinear nature activation function dictates factors subject diminishing returns performance improves 
reading experience accumulates increasing proportionally equivalently increasing absolute magnitudes frequency consistency effects diminish see seidenberg 
principle applies different types stimuli reader skill level performance stimuli strong factor relatively insensitive variation factors 
regular words show little effect frequency high frequency words show little effect consistency shown 
result standard pattern interaction frequency consistency naming low frequency exception words disproportionately slow inaccurate andrews seidenberg seidenberg taraban mcclelland waters seidenberg 
unit target signs simply reversed 
alternatively equation interpreted reflecting correlation activation output unit target may case 
understanding normal impaired word reading elaborated version frequency consistency equation provides basis understanding effects semantics naming performance 
approximation expressed equation contribution semantic pathway word simply term summed input output phoneme unit 
just frequency consistency stronger semantic contribution moves input activation function diminishing effects factors 
result words relatively weak semantic contribution low words jones schwartz marin exhibit stronger frequency consistency interaction particular naming latencies error rates disproportionately high items weak dimensions exception words strain press 
course simulations demonstrate networks hidden units trained error correction learn pronounce correctly types words help semantics 
context general framework full competence required combination semantic phonological influences 
semantic pathway develops increases contribution required phonological terms equation achieve level performance correspondingly reduced 
additional assumption system intrinsic bias unnecessary complexity limiting effective degrees freedom weight decay extended reading experience leads redistribution labor 
specifically semantic pathway improves phonological pathway gradually loses ability process words learned weakly low frequency consistency 
context contribution semantics severely weakened eliminated brain damage summed input output unit reduced 
output units significant negative terms summed input words enemies manipulation may cause summed input output change sign 
result incorrect response 
errors tend regularizations reduced summed input affects output units correct activations inconsistent word neighbors 
furthermore frequency independent positive contribution summed inputs errors high frequency exception words 
contrast reduction contribution semantics little effect correct performance regular words positive contribution friends sufficient give output units appropriately signed summed input 
resulting pattern behavior corresponding fluent surface dyslexia bub mccarthy warrington shallice seen exaggerated manifestation influences frequency consistency give rise normal pattern naming latencies 
pattern joint nonlinear combined effects frequency consistency connectionist account assumptions contribution semantics lead number predictions shared traditional dual route accounts 
frequency consistency trade detrimental effects spelling sound inconsistency overcome sufficiently high word frequency 
consequently connectionist account strong prediction english language surface dyslexic patient reads exception words regular words read normally performance high frequency exceptions 
contrast dual route framework account patient quite easily terms damage eliminates lexical route leaving gpc route operation 
fact putative separation routes framework predict existence patients 
connectionist account differs dual route account claiming consistency regularity se adherence gpc rules determining variable regularization errors formulated consistency depends types orthographic overlap solely word bodies cf 
glushko 
connectionist account predicts close relationship impairments contribution semantics phonology surface dyslexic reading pattern graham patterson hodges relationship subject individual differences reading skill division labor semantic phonological pathways 
patients highly developed phonological pathways may exhibit pattern semantic impairment severe warrington schwartz 
contrast dual route theories include lexical pathway coltheart coltheart predict selective semantic damage affect naming accuracy 
connectionist account believe important advantage simplicity dual route approach 
advantage goes basic point provides single set computational principles account exception word nonword reading dual route model rely separate sets principles 
additional advantage lies fact boundary regular exception words clear attempts draw boundaries lead unfortunate consequences 
marking items exceptions looked wholes lexicon ignores fact letters items take standard grapheme phoneme correspondences 
pint letters take regular correspondence 
second marking items exceptions ignores fact parts exceptional admit regularity example exceptional pronunciation pint occurs words containing understanding normal impaired word reading ind represents consonant 
third exceptions come clusters share word body 
special word body rules may invoked capture clusters word conforms usual correspondence exceptional 
treat oo followed regular takes common correspondence oo exception 
explicit treatment virtually word exception neglects partial regularity prevents word partial regularity contributing patterns consistency enters items 
connectionist approach contrast avoids need impose unfortunate divisions leaves mechanism exhibits sensitivity partially regular aspects called exception words 
fact exceptions subject processes items system allows explain virtually completely arbitrary exceptions 
hand dual route approach leaves fact system completely unexplained 
fact dual route models provide basis accounting effects consistency reading words nonwords 
dual route theorists coltheart coltheart appealed partial activation lexical items basis effects 
assumption moves part way view consistency effects arise influence lexical items 
add connectionist model exhibits effects requisite sensitivity general grapheme phoneme correspondences stipulating separate rule system system exhibits broad range consistency effects 
additional empirical issues proponents dual route theories raised number empirical issues believe challenge connectionist account normal impaired word reading 
example coltheart 
see besner raise questions concerning reading process exception word reading deem problematic sm framework 
remaining nonword reading acquired surface dyslexia addressed extensively current 
discuss remaining issues acquired phonological dyslexia developmental dyslexia lexical decision may accounted light findings 
consider empirical findings interpreted providing evidence current approach effects buchanan besner besner mc besner pugh katz stimulus blocking effects besner coltheart monsell finding naming latencies exception words influenced position exceptional correspondence coltheart 
acquired phonological dyslexia 
mentioned earlier straightforward sm framework account central characteristic acquired phonological dyslexia substantially better word reading nonword reading terms relatively selective impairment phonological pathway 
apparent difficult arises considering patients virtually unable read nonwords suggesting complete elimination phonological pathway additional semantic impairment render semantic pathway insufficient account observed proficiency word reading 
patients described literature wb wt 
explain word reading patients dual route theorists claim necessary introduce third route lexical 
point fact coltheart 
explicitly considered alternative explanation think rejected 
patient impaired semantic system semantic errors reading comprehension severely impaired reading system avoid making semantic errors reading aloud making poor information pronunciation word yielded reading system 
semantic system may longer able distinguish concept orange concept lemon avoid semantic errors reading aloud route needs deliver just phoneme written word complete representation phonology 
coltheart colleagues argued account entirely basis findings wb pronounce correctly single list written nonwords give correct phonemic correspondence single printed letters 
claimed wb reading route just severely impaired completely abolished 
argument 
unwise base strong theoretical claim empirical observations especially little information required phonological pathway account 
pronounce nonword correctly phonemes derived accurately 
wb inability read nonwords taken definitive evidence phonological pathway completely 
furthermore wb fact semantic errors oral reading train plane girl boy see appendix 
errors relatively rare comprising lexical error responses error responses completely unrelated stimulus 
effect semantic relatedness errors difficult ascribe chance responding see ellis marshall shallice understanding normal impaired word reading mcgill 
generally fully wb lexical errors semantic component typically combination visual phonemic morphological relatedness 
critically coltheart colleagues fail take account fact wb exhibited deficits purely phonological tasks nonword repetition phoneme stripping blending patterson marcel suggesting additional impairment phonology 
argued phonological impairment explain wb nonword reading deficit repeated nonwords successfully read achieved success blending phoneme words auditory presentation individual phonemes 
note failure repeat fully half set simple single syllable word nonwords certainly represents prominent phonological deficit 
auditory blending test words target responses wb partial success task especially germane issue 
patterson marcel assessed wb blending performance nonword targets unable produce single correct response auditory presentation consisted individual phonemes simple nonword onset rime 
patterson marcel argued phonological deficit non reading task sufficient account wb complete inability read nonwords 
pattern performance exhibited wb explained sm framework terms mildly impaired semantic reading pathway possibly impaired phonological reading pathway particular impairment phonology 
similar explanation applies wt patient performance phonological blending tasks reported severely equally impaired ability read repeat set nonwords 
point passing deep dyslexia coltheart remaining major type acquired central dyslexia closely related phonological dyslexia see glosser friedman accounted terms computational principles employed current see plaut shallice 
developmental dyslexia 
focus current characterizing computational principles governing normal skilled reading acquired dyslexia brain damage literate adults 
believe principles provide insight nature reading acquisition normal form developmental dyslexia children fail acquire age appropriate reading skills 
general agreement number distinct patterns developmental dyslexia exist exactly patterns gives rise matter ongoing debate 
common viewpoint de analogues acquired forms dyslexia see baddeley ellis miles lewis harris coltheart marshall 
clearest evidence comes coltheart compared dyslexic children age matched normal readers ability pronounce exception words nonwords 
majority dyslexic children abnormally poor sets items 
selectively impaired exception word reading corresponding developmental surface dyslexia selectively impaired nonword reading corresponding developmental phonological dyslexia 
coltheart interpret findings supporting dual route theory word reading lexical sublexical procedure selectively fail develop properly offer suggestion 
seidenberg doi mcbride chang peterson press compared dyslexic children controls matched age matched reading level 
confirmed existence separate surface phonological dyslexic patterns dyslexic children showed general reading impairment 
critically performance developmental surface dyslexic children remarkably similar reading level matched controls suggesting developmental delay 
contrast phonological dyslexic children performed set controls suggesting deviant developmental pattern 
findings incompatible dual route account colleagues contend naturally accounted terms different impediments development single phonological pathway 
specifically suggest sm delayed acquisition developmental surface dyslexia may arise limitations available computational resources phonological route 
consistent interpretation sm version network trained half normal number hidden units showed disproportionate impairment exception words compared regular words performance items poorer consistent finding generalized deficits common 
nonword reading capability network tested coltheart 
point performance worse normal network impaired nonword reading 
just normal skilled reading limitation sm model stems inappropriately structured orthographic phonological representations 
demonstrate trained feedforward network hidden units identical fashion hidden units simulation semantics 
network chosen comparison simply relevant acquisition data networks expected show similar effects 
corresponding data version hidden understanding normal impaired word reading percent correct training hidden units training epoch lf hf reg hf reg lf exc nonwords exc 
correct performance feedforward network hidden units taraban mcclelland high low frequency exception words regular consistent control words glushko nonwords function training epoch 
network trained exactly corresponding data shown 
units 
comparison figures reveals limiting number hidden units selectively impairs performance exception words particularly low frequency 
contrast nonword reading affected slightly 
notice performance dyslexic network epoch quite similar normal network epoch 
limiting computational resources available learning spelling sound task reproduces basic delayed pattern developmental surface dyslexia 
manipulations impede learning weak noisy weight changes expected yield similar results 
regard developmental phonological dyslexia 
press suggest selective impairment nonword reading may arise phonological representations poorly articulated due peripheral disturbances see liberman rack olson 
consideration normal sm model instructive 
network employed representations argued poorly capture relevant structure orthography phonology 
result model correct reading words regular exception correct subset glushko nonwords scored appropriately see seidenberg mcclelland 
sense model behaved mild phonological dyslexic see besner similar arguments 
way performance model provides evidence system adequate computational resources fails develop appropriately componential orthographic particularly phonological representations fail acquire normal proficiency sublexical spelling sound translation 
kept mind extent semantic pathway develops contributes reading acquisition dissociation word nonword reading exacerbated 
final point contention regard implications developmental reading disorders sm framework concerns existence children oral reading ability exception words far surpasses comprehension called huttenlocher huttenlocher siegel silverberg silverberg 
typically children moderately severely retarded standardized intelligence tests may totally lack conversational speech 
tend devote considerable amount time attention reading studied thoroughly 
suggest due abnormally poor development semantic pathway children may phonological pathways networks trained semantics 
limit networks learn pronounce types words nonwords accurately comprehension 
lexical decision 
final coltheart objections sm model concerns ability perform lexical decisions 
sm establish stimulus conditions model discriminate words nonwords basis measure accuracy regenerating orthographic input besner colleagues besner besner demonstrated accuracy doing worse human subjects conditions 
coltheart 
mistakenly claim sm orthographic error scores yield false positive rate waters seidenberg nonwords word error rates equated subjects fact numbers result phonological error scores besner sm employ suggest learning phonological attractors words help 
actual false positive rate lower besner colleagues report rate orthographic phonological error scores summed orthographically strange words excluded unsatisfactory 
course sm claimed orthographic phonological information completely sufficient account lexical decision performance conditions pointing may cases subjects consult information provided computation orthography semantics 
semantics natural source information distinguish words nonwords fact string letters phonemes defined word virtue having meaning 
coltheart colleagues raise concern full implementation sm framework presentation orthographically understanding normal impaired word reading regular nonword activate semantics degree word care precluding lexical decision 
simulation clearly required address full range lexical decision data adequately comments may serve specific concern 
imagine semantic representations words relatively sparse meaning word activates possible semantic features semantic feature participates meanings small percentage words 
connectionist networks sort investigating learn set base activation level output unit expected value correct activations entire training corpus values minimize total error absence information input 
case sparse semantic representations means semantic features completely inactive specific evidence orthographic input active 
notice nature evidence specific order prevent semantic features word care activated presentation orthographically similar words scare car extreme sensitivity small orthographic distinctions prevent semantic features activated nonword 
account computational requirements connectionist system maps orthography semantics entail ability perform lexical decision 
blocking effects 
somewhat overlapping sets empirical findings viewed problematic current approach effects buchanan besner besner mccann besner pugh blocking effects besner coltheart monsell 
set involves demonstrations variety conditions nonwords pronunciations match word processed differently orthographically matched nonwords 
example subjects faster name slower accurate reject lexical decision mccann besner 
second set problematic findings involves demonstrations subjects performance sensitive context orthographic stimuli occur usually operationalized terms stimuli blocked experiment 
example subjects slower regularization errors pronouncing exception words intermixed nonwords pronouncing pure blocks exception words monsell 
sets phenomena handled particularly sm implementation natural formulations general framework includes semantics 
effects may stem articulatory advantage initiating familiar pronunciations seidenberg petersen macdonald plaut press interactions phonology semantics occur control nonwords 
blocking effects may reflect adjustments stimulus driven strategic control subjects relative contribution semantic phonological pathways lexical tasks 
interpretations supported findings pugh 
investigated effects spelling sound consistency semantic relatedness lexical decision function nonword include 
faster latencies consistent words inconsistent words context purely nonwords effect consistency 
similarly dual lexical decision paradigm obtained facilitation visually similar word pairs phonological consistent tribe inhibition inconsistent couch touch meyer eliminated consistency effect 
semantic relatedness ocean water yielded facilitation regardless nonword context 
findings suggest subjects normally semantic phonological pathways lexical decision avoid phonological pathway lead inappropriate semantic activity included 
effects position exceptional correspondence 
coltheart argue determinants naming rt exception words position counting graphemes phonemes left right word deviates rule governed correspondences 
claim effect incompatible parallel approach word naming dual route cascaded drc model coltheart 
predicts simulates effect gpc procedure drc model operates serially input string 
monosyllabic words provide simulation data drc model chef glow 
account critical factor chef model requires largest number processing cycles irregular grapheme phoneme requiring intermediate number cycles breaks rules second grapheme phoneme glow yields fastest time model irregular third position 
account critical difference words may position irregularity proportion known words similar spelling patterns agree conflict target word pronunciation see jared seidenberg elaboration argument 
concise oxford dictionary lists monosyllabic words starting ch pronunciation ts chair pronunciation chef pronounced chord 
chef highly inconsistent word 
word somewhat difficult know neighborhood words choose similar understanding normal impaired word reading analysis 
take words common pronunciations top tone third pronunciation exemplars pronunciations ton took common 
body level friend enemies bomb comb 
moderately inconsistent word 
words ow gpc procedure coltheart 
considers ow regular ow glow irregular fact monosyllabic words english ow rhyme glow coltheart colleagues regular pronunciation 
glow inconsistent frequent correspondence 
consistent interpretation attractor network developed simulation produces naming latencies chef glow 
experiment human readers performed coltheart revealed predicted relationship position irregularity naming rt slowest rts words chaos irregular grapheme phoneme correspondence fastest rts words irregular position 
stimulus words syllables prevents evaluating performance networks materials 
inspection words appendix suggests confounding position degree consistency 
take items analysis irregular position half words syllable words syllable stress second syllables silent 
gpc procedure coltheart 
applies rules independent syllable position assigns vowel grapheme second syllable vowel grapheme second syllable 
despite fact model able treat words nature operation ensures sensitive fact words sort pattern tense long vowels second syllable 
great majority words ive active passive motive native final vowel making relatively consistent word 
reinterpretation coltheart effect turns give adequate account results remains seen empirical modeling 
furthermore position effect properly controlled stimuli may consistent parallel computation phonology orthography decision initiate articulation depends initial phoneme kawamoto kello jones 
incompatible approach coltheart findings may fact relate simple properties networks develop representations time 
extensions approach approach taken extended number different directions 
obvious natural extension reading words 
pronunciation words exhibits kind quasi regular structure level jared seidenberg regularities apply just grapheme phoneme correspondences assignment stress involve sensitivity linguistic variables word derivational status factors smith baker 
challenge arises extending approach words finding better method condensing regularities positions word 
representations condense regularities onset coda monosyllabic word experience particular correspondences onset affect processing correspondence coda vice versa 
model completely separate sets weights implementing correspondences failures consonant coda attributable fact knowledge transferred onsets 
ultimately solution problem condensing regularities involve sequential processing level 
paradigm case approach nettalk sejnowski rosenberg see letters processed sequentially proceeding text left right 
input shifted window slots wide letter mapped corresponding phoneme falls central slot 
allows successive letter processed set units regularities extracted processing letters position available processing letters position 
time presence letters slots flanking central slot allows network context sensitive exhibit consistency effects 
drawback letter letter approach onset pronunciation word completely insensitive consistency vowel consistency affect vowel correspondences come play pronunciation onset completed 
presents problem empirical finding consistency effects naming latencies main motivations connectionist approach word reading 
reason great deal coarticulation successive phonemes taken view fluent skilled reading involves parallel construction pronunciation phonemes time 
possibility skilled readers attempt process word parallel redirect attention remaining part try see plaut mcclelland seidenberg press simulation illustrating approach 
way early learning reading strictly sequential nettalk skill de understanding normal impaired word reading parallel models 
result system fall back sequential approach allows application knowledge regularities acquired reading units size applied entire length utterance 
approach extends naturally words length size window parallel computation completely dependent experience 
moving single word reading approach taken applicable believe wide range linguistic cognitive domains essentially structure sense systematicity arbitrariness exceptions 
domain approach applied inflectional morphology rumelhart mcclelland 
stated application certainly remains controversial pinker colleagues marcus pinker pinker prince continue maintain single mechanism fully capture behavior regular inflectional process handling exceptions 
claim existing connectionist simulations fully addressed valid criticisms raised point see little criticisms stands applicability connectionist approach principle 
arguments raised papers general reflect full appreciation capabilities connectionist networks quasi regular domains 
example pinker acknowledge connectionist models sound shown sm inflectional morphology seidenberg show frequency regularity interaction takes key indicators operation frequency insensitive rule system frequency sensitive lexical lookup mechanism 
aspects empirical data domain inflectional morphology appear point favor interpretation terms single connectionist system sensitive frequency consistency 
consider aspect historical evolution english past tense system 
hare elman press reviewed pattern change early old english eoe period circa 
eoe main types verbs strong weak consisting subtypes 
period different types weak verbs single type current regular past 
strong verbs regularized persist day various irregular verbs modern english 
coalescence various types weak verbs single type pattern susceptibility regularization strong verbs occasional occurrence particular weak verb took characteristics cluster strong verbs traced workings single connectionist system sensitive frequency consistency 
hare elman approach language change iterative application new generation learners simulated new untrained networks output previous generation learners simulated old networks trained output older networks 
generation imposes distortions corpus elimination subtle differences variations weak past apply similar forms regularization low frequency irregular forms friends 
gradually course generations system transformed highly complex system circa simpler system today 
remaining irregular verbs highly consistent neighbors highly frequent frequent consistent strong verbs absorbed regular system 
crucially argument regular weak system exception strong system show effects frequency consistency expected single system account 
derivational morphology presents rich quasi regular domain approach apply 
morphemes partially productive ways similar quasi regular correspondences inflectional morphology spelling sound appear governed set soft constraints 
second meaning morphologically complex word related completely determined constituent morphemes partial complete regularity mapping meaning sound see discussion points 
graded influences frequency consistency appear operate just level individual words level sentences evidenced findings lexical semantic contextual effects syntactic ambiguity resolution see macdonald taraban mcclelland trueswell tanenhaus 
example consider temporary main verb reduced relative ambiguity associated word examined sentence evi dence examined lawyer useless ferreira clifton 
degree subjects slowed sentence comprehension encountering ambiguities subject number influences including previous disambiguating context trueswell semantic plausibility head noun main verb reading cf 
evidence vs animate noun witness relative frequency verb simple past tense person examined object opposed past participle object examined person macdonald 
verbs consistently simple past tense lead stronger garden path effects reduced relative interpretation required verbs ambiguous usage 
effects natural interpretation terms constraint satisfaction process variety sources lexical knowledge conspire produce coherent sentence interpretation including graded influences strength depends consistency understanding normal impaired word reading word form usage see tanenhaus press mac donald pearlmutter seidenberg discussion kawamoto pearlmutter macdonald seidenberg st john mcclelland connectionist simulations illustrating principles 
generally domains encompassed semantic episodic encyclopedic knowledge quasi regular facts experiences partially arbitrary partially predictable characteristics related facts experiences see mcclelland mcnaughton reilly press discussion 
consider robin example 
properties largely predictable properties birds color exact size sound color eggs relatively arbitrary 
rumelhart rumelhart todd shows connectionist network learn contents semantic network capturing shared structure set concepts allow generalization new examples time mastering idiosyncratic properties particular examples 
example consider john kennedy assassination 
arbitrary aspects date time event understanding happened depends knowledge derived events involving presidents spies understanding things informs pervades memory kennedy assassination 
understanding similar events ultimately influenced learn kennedy assassination 
st john provides example connectionist network learns characteristics events applies similar events just learning mechanism governed principles combined frequency consistency sensitivity spelling sound simulations 
summary quasi regular systems english spelling sound system appear pervasive initial indications connectionist networks sensitive frequency consistency provide insight way systems learned represented 
coltheart 
reach inescapable ability deal linguistic stimuli previously encountered 
explained postulating learned systems general linguistic rules ability time deal correctly exceptions rules 
explained postulating existence systems word specific lexical representations 
formulated connectionist approach knowledge processing quasi regular domains instantiated specific domain english word reading demonstrated account basic abilities skilled readers handle correctly regular exception items generalizing novel items 
approach proficiency humans quasi regular domains stems existence separate rule item specific mechanisms fact cognitive system adheres certain general principles computation neural systems 
connectionist approach addresses general reading abilities provides insight detailed effects frequency consistency naming latency normal readers impaired naming accuracy acquired developmental dyslexic readers 
mathematical analysis simplified system incorporating relevant principles forms basis understanding intimate relationship factors particular inherently graded nature spelling sound consistency 
general lexical framework word reading current contains semantic pathway addition phonological pathway 
contrast lexical sublexical procedures dual route theories operate fundamentally different ways pathways current approach operate common set computational principles 
result nature processing pathways intimately related 
particular consideration pattern impaired preserved abilities acquired surface dyslexia leads view partial division labor pathways 
contribution phonological pathway graded function frequency consistency items weak measures processed particularly poorly 
overt accuracy items compromised semantic pathway contributes pronunciation words nonwords 
relative capabilities pathways open individual differences differences may manifest pattern severity reading impairments brain damage 
needless say remains done 
current simulations specific limitations restriction lack attention paid development orthographic representations need remedied 
furthermore nature processing semantic pathway characterized coarsest way 
wide range related empirical issues including phonological dyslexia developmental dyslexia lexical decision blocking effects addressed general terms 
results reported similar approaches clearly suggest computational principles connectionist modeling lead deeper understanding central empirical phenomena word reading particular quasi regular domains generally 
understanding normal impaired word reading appendix stimuli simulation studies regular regular consistent inconsistent ambiguous exception nonword high frequency low frequency best base brown big bone clear came dead break class catch choose dark cool come days gone mo fact dear got head done group flat foot flew know give main form known great go love page goes low move bove place grow near pull see put sut soon home said meat says tell paid shall week plant show want roll shown watch root stood sand town dat small year word write speak beam brood blown bowl broke cook brow broad bus cord cone bush crown deaf dots dive doll fade dare flood float flour gross grape gull gear lose mose lunch harm glove pear peel hoe glow phase pitch pint pump leaf grove ripe loss hood rouse mad lone sew slam moose shoe choe slip pour mouse prone swamp swarm trunk spear touch wake pose stove wad nad wax strive wand mand weld rave wash wing tint thread wool bool wit zone worm note regular consistent words regular inconsistent words exception words experiments taraban mcclelland 
studies regular consistent words control words exception words 
addition regular inconsistent word shares body exception word 
ambiguous words contain bodies associated pronunciations occurs words 
generated seidenberg mcclelland matched frequency ku era francis taraban mcclelland high low frequency regular consistent exception words 
nonwords generated altering onsets exception words 
understanding normal impaired word reading appendix accepted pronunciations glushko nonwords consistent nonwords inconsistent nonwords nonword pronunciation nonword pronunciation bed bint bint bint bled bled bud bud bost bost st bost br kat kos koz dold dold dar der dur dun dom dum dam dor dr fel fed fed gode god gom hen hod hen hin hyl hove lal lom mak mek mar mer mon mon mun muf muf st nus pet pl pom pum pam pod put put pov puv pr pred sed sed sed sod sod sud sud st spat suff stat st sul taz tav tav wat wet wet wed wed won won wus note pot cat bed hit dog keep bike hope boot boy cup ring chin thin 
phonemes represented conventional way bat 
understanding normal impaired word reading appendix regularizations taraban mcclelland exception words high frequency exceptions low frequency exceptions word correct regularization word correct regularization ar ar bowl bol bwl bot bot broad break bush bus choose cus deaf def def come kom doll dal dol du da flood fl dos gross gros gros done don lose luz los foot fut fut pear par give giv giv phase fas great pint pint pint hav hav plw plo move muv mov rouse rws pull sew su put put shoe su said sed sad says saz sas swamp swamp sw mp shall sal sol swarm swarm want want nt touch watch wac wad wad wer wand wand nd wash wos word word wool worm worm note pot cat bed hit dog keep bike hope boot boy cup ring chin thin 
phonemes represented conventional way bat 
understanding normal impaired word reading anderson silverstein ritz jones 

distinctive features categorical perception probability learning applications neural model 
psychological review 
andrews 

phonological recoding regularity effect consistent 
memory cognition 
bert seidenberg 

acquisition spelling sound information reading 
journal experimental child psychology 
baddeley ellis miles lewis 

developmental acquired dyslexia comparison 
cognition 


dissociation frequency regularity effects pronunciation performance yong adults older adults individuals dementia alzheimer type 
journal memory language 
besner 

visual word recognition evidence strategic control lexical routines oral reading 
journal experimental psychology learning memory cognition 
baron strawson 

orthographic knowledge reading words aloud 
journal experimental psychology human perception performance 


phonological 
journal neurology neurosurgery psychiatry 
behrmann bub 

surface dyslexia dual routes single lexicon 
cognitive neuropsychology 
besner smith 

models visual word recognition obscuring stimulus yields clearer view 
journal experimental psychology learning memory cognition 
besner mccann 

connection connectionism data words necessary 
psychological review 
smolensky 

virtual memories massive generalization connectionist combinatorial learning 
proceedings th annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
bub 

word analytic translation spelling sound non semantic reader 
patterson coltheart marshall eds surface dyslexia pp 

hillsdale nj lawrence erlbaum associates 
bub black 

semantic encoding pictures words neuropsychological observations 
cognitive neuropsychology 
buchanan besner 

reading aloud evidence word pathway 
canadian journal experimental psychology 


representation learning generalization damage neural network models reading aloud 
manuscript submitted publication 
butler 

individual differences word recognition latency 
memory cognition 


morphology study relation meaning form 
philadelphia pa john benjamins 


rules schemas development english past tense 
language 
coltheart 

varieties developmental dyslexia 
cognition 
warrington 

semantic memory reading abilities case report 
journal international neuropsychological society 
coltheart 

lexical access simple reading tasks 
underwood ed strategies information processing new york academic press 
coltheart 

disorders reading implications models normal reading 
visible language 
coltheart 

cognitive neuropsychology study reading 
posner marin eds attention performance xi pp 

hillsdale nj lawrence erlbaum associates 
coltheart curtis atkins haller 

models reading aloud dual route parallel distributed processing approaches 
psychological review 
coltheart patterson marshall 
eds 

deep dyslexia 
london routledge kegan paul 
coltheart 

serial processing reading aloud evidence dual route models reading 
journal experimental psychology human perception performance 


read write idea evidence third reading mechanism 
brain language 
cottrell plunkett 

learning past tense recurrent network acquiring mapping meaning sounds 
proceedings th annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
seidenberg 

rules connections 
past tense revisited 
proceedings th annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum associates 


mundane reasoning settling plausible model 
artificial intelligence 


impaired preserved semantic memory functions dementia 
ed memory functioning dementia 
amsterdam elsevier science publishers 
ellis marshall 

semantic errors statistical 
note allport knowing meanings words unable report 
quarterly journal experimental psychology 
besner 

process lexical decision words parallel distributed processing model 
journal experimental psychology learning memory cognition 
ferreira clifton 

independence syntactic processing 
journal memory language 
fodor pylyshyn 

connectionism cognitive architecture critical analysis 
cognition 
forster 

computational modeling elementary process analysis visual word recognition 
journal experimental psychology human perception performance 
forster chambers 

lexical access naming time 
journal verbal learning verbal behaviour 
understanding normal impaired word reading 

spelling sound approaches internal lexicon 
journal experimental psychology human perception performance 
friedman 
press 
recovery deep phonological 
brain language 


phonological processing reading new evidence acquired dyslexia 
british journal psychology 
glosser friedman 

continuum deep phonological 
cortex 
gluck bower 

evaluating adaptive network model human learning 
journal memory language 
glushko 

organization activation orthographic knowledge reading aloud 
journal experimental psychology human perception performance 
graham hodges patterson 

relationship comprehension oral reading progressive fluent aphasia 
neuropsychologia 
hanson burr 

connectionist models learn learning representation connectionist networks 
behavioral brain sciences 
hare elman 

connectionist account english inflectional morphology evidence language change 
proceedings th annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
hare elman 
press 
learning morphological change 
cognition 
harris coltheart 

language processing children adults 
london routledge kegan paul 
henderson 

orthography word recognition reading 
london academic press 
hinton 

implementing semantic networks parallel hardware 
hinton anderson eds parallel models associative memory pp 

hillsdale nj lawrence erlbaum associates 
hinton 

connectionist learning procedures 
artificial intelligence 
hinton 

mapping part hierarchies connectionist networks 
artificial intelligence 
hinton 
ed 

connectionist symbol processing 
cambridge ma mit press 
hinton mcclelland rumelhart 

distributed representations 
rumelhart mcclelland pdp research group eds parallel distributed processing explorations microstructure cognition 
volume foundations pp 

cambridge ma mit press 
hinton sejnowski 

learning relearning boltzmann machines 
rumelhart mcclelland pdp research group eds parallel distributed processing explorations microstructure cognition 
volume foundations pp 

cambridge ma mit press 
hinton shallice 

attractor network investigations acquired dyslexia 
psychological review 


rules thing past 
acquisition verbal morphology attractor network 
proceedings th annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
mcclelland 

perceptual processing deficit explain impairment inflectional morphology developmental 
computational investigation 
proceedings th annual child language research forum pp 

stanford ca 
humphreys 

independent lexical routes word processing 
evaluation dual route theory reading 
behavioral brain sciences 
huttenlocher huttenlocher 

study children 
neurology 
jacobs 

increased rates convergence learning rate adaptation 
neural networks 
jared seidenberg 

basis consistency effects word naming 
journal memory language 
jared seidenberg 

naming words 
journal experimental psychology human perception performance 
jones 

deep dyslexia ease predication 
brain language 
jordan rumelhart 

forward models supervised learning distal teacher 
cognitive science 
tanenhaus 
press 
constraint account subject object attachment preference 
journal psycholinguistic research 
kawamoto 

nonlinear dynamics resolution lexical ambiguity parallel distributed processing approach 
journal memory language 
kawamoto kello jones 
november 
locus exception effect naming 
proceedings th annual meeting psychonomic society 
st louis mo kawamoto kello jones 

temporal spatial loci exception consistency effects naming 
manuscript submitted publication 
ku era francis 

computational analysis day american english 
providence ri brown university press 
kullback leibler 

information sufficiency 
annals statistics 


relation linguistic structure theories language learning constructive critique connectionist learning models 
cognition 


mean squared error reaction time connectionist model word recognition 
touretzky hinton sejnowski eds proceeedings connectionist models summer school pp 

san mateo ca morgan kauffman 


phonology basic concepts 
cambridge cambridge university press 
liberman 

phonology problems learning read write 
remedial special education 
macdonald 

probabilistic constraints syntactic ambiguity 
language cognitive processes 
macdonald pearlmutter seidenberg 

lexical nature syntactic ambiguity resolution 
psychological review 
understanding normal impaired word reading macwhinney 

implementations conceptualizations revising verb learning model 
cognition 


patterns sounds 
cambridge cambridge university press 
seidenberg doi mcbride chang peterson 
press 
bases subtypes developmental dyslexia 
cognition 
marcel 

surface dyslexia reading revised hypothesis pronunciation print impairments 
coltheart patterson marshall eds deep dyslexia pp 

london routledge kegan paul 


constraints plasticity connectionist model english past tense 
journal cognitive neuroscience 
marcus pinker ullman hollander rosen xu 

language acquisition 
monographs society research child development 
marr 

theory cerebellar cortex 
journal physiology 
marshall 

rational taxonomy developmental 
whitaker eds dyslexia global issue pp 

hague nijhoff 
marshall newcombe 

syntactic semantic errors 
neuropsychologia 
marshall newcombe 

patterns psycholinguistic approach 
journal psycholinguistic research 
massaro 

criticisms connectionist models human performance 
journal memory language 
massaro 

testing trace model fuzzy logical model speech perception 
cognitive psychology 


read non words data different populations 
patterson coltheart marshall eds surface dyslexia pp 

hillsdale nj lawrence erlbaum associates 
mccann besner 

reading implications models pronunciation locus word frequency effects word naming 
journal experimental psychology human perception performance 
mccarthy warrington 

phonological reading phenomena paradoxes 
cortex 
mcclelland 

case language processing 
coltheart ed attention performance xii psychology reading pp 

hillsdale nj lawrence erlbaum associates 
mcclelland 

stochastic interactive processes effect context perception 
cognitive psychology 
mcclelland 

grain model framework modeling dynamics information processing 
meyer eds attention performance xiv synergies experimental psychology artifical intelligence cognitive neuroscience pp 

hillsdale nj lawrence erlbaum associates 
mcclelland elman 

trace model speech perception 
cognitive psychology 
mcclelland mcnaughton reilly 
press 
complementary learning systems hippocampus neocortex insights successes failures connectionist models learning memory 
psychological review 
mcclelland rumelhart 

interactive activation model context effects letter perception part 
account basic findings 
psychological review 
mcclelland rumelhart pdp research group eds 

parallel distributed processing explorations microstructure cognition 
volume psychological biological models 
cambridge ma mit press 
mcclelland st john taraban 

sentence comprehension parallel distributed processing approach 
language cognitive processes 
mccloskey 

networks theories place connectionism cognitive science 
psychological science 


exceptional reading brain damaged children 
neurology 
siegel 

patterns atypical reading development attributes underlying reading processes 
eds handbook neuropsychology vol 

amsterdam elsevier science publishers 
meyer 

functions phonemic codes visual word recognition 
memory cognition 
miceli caramazza 

assignment word stress oral reading evidence case acquired dyslexia 
cn 
minsky 

framework representing knowledge 
winston ed psychology computer vision pp 

new york mcgraw hill 
minsky papert 

perceptrons computational geometry 
cambridge ma mit press 
monsell 

nature locus word frequency effects reading 
besner humphreys eds basic processes reading visual word recognition pp 

hillsdale nj lawrence erlbaum associates 
monsell patterson graham hughes 

lexical sublexical translation spelling sound strategic anticipation lexical status 
journal experimental psychology learning memory cognition 
cary 

literacy training speech segmentation 
cognition 
cary 

awareness speech sequence phones arise spontaneously 
cognition 
morton 

interaction information word recognition 
psychological review 
morton patterson 

new attempt interpretation attempt new interpretation 
coltheart patterson marshall eds deep dyslexia pp 

london routledge kegan paul 
movellan mcclelland 

learning continuous probability distributions symmetric diffusion networks 
cognitive science 
mozer 

discovering faithful representations connectionist network 
proceedings th understanding normal impaired word reading annual conference cognitive science society 
hillsdale nj lawrence erlbaum associates 
mozer 

perception multiple objects connectionist approach 
cambridge ma mit press 
norris 

quantitative multiple levels model reading aloud 
journal experimental psychology human perception performance 
olsen caramazza 

role cognitive theory 
eds handbook neuropsychology pp 

amsterdam elsevier 
noel 

dual route models print sound horse race 



phonological recoding lexical decision effects spelling sound regularity depend regularity defined 
memory cognition 
patterson 

neural nets 
japanese journal neuropsychology 
patterson coltheart marshall 
eds 

surface dyslexia 
hillsdale nj lawrence erlbaum associates 
patterson graham hodges 

reading alzheimer type dementia preserved ability 
neuropsychology 
patterson hodges 

deterioration word meaning implications reading 
neuropsychologia 
patterson marcel 

phonological phonological 
jun de eds analytic approaches human cognition pp 

new york elsevier 
patterson morton 

orthography phonology attempt old interpretation 
patterson coltheart marshall eds surface dyslexia pp 

hillsdale nj lawrence erlbaum associates 
patterson seidenberg mcclelland 

connections disconnections acquired dyslexia computational model reading processes 
morris ed parallel distributed processing implications psychology neuroscience pp 

london oxford university press 
patterson suzuki 
press 
progressive aphasia surface japanese 

pearlmutter 

learning state space trajectories recurrent neural networks 
neural computation 
pearlmutter macdonald seidenberg 

modeling frequency contextual biases sentence processing 
proceedings th annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
phillips hay smith 

pronunciation simulated neural net technical report 
stirling scotland centre cognitive computational neuroscience university stirling 
pinker 

language learnability language development 
cambridge ma harvard university press 
pinker 

rules language 
science 
pinker prince 

language connectionism analysis parallel distributed processing model language acquisition 
cognition 
plaut 
press 
double dissociation modularity evidence connectionist neuropsychology 
journal clinical experimental neuropsychology 
plaut behrmann patterson mcclelland 
november 
impaired oral reading surface dyslexia detailed comparison patient connectionist network 
proceedings th annual meeting psychonomic society 
washington dc 
plaut hinton 

learning sets filters back propagation 
computer speech language 
plaut mcclelland 

generalization componential attractors word nonword reading attractor network 
proceedings th annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum associates 
plaut mcclelland seidenberg 
press 
reading exception words pseudowords routes really necessary 
ed proceedings nd neural computation psychology workshop 
edinburgh scotland 
plaut shallice 

deep dyslexia case study connectionist neuropsychology 
cognitive neuropsychology 
plunkett 

shaped learning frequency effects multi layered perceptron implications child language acquisition 
cognition 
plunkett 

rote learning system building acquiring verb morphology children connectionist nets 
cognition 
pugh katz 

evidence flexible coding visual word recognition 
journal experimental psychology human perception performance 
quinlan 

connectionism psychology psychological perspective new connectionist research 
chicago university chicago press 
rack olson 

nonword reading deficit developmental dyslexia review 
reading research 
reggia marsland berndt 

competitive dynamics dual route connectionist model print sound transformation 
complex systems 
richardson 

effects stimulus attributes latency word recognition 
british journal psychology 
rumelhart 

brain style computation learning generalization 
davis lau eds neural electronic networks chap 

new york academic press 
rumelhart durbin golden chauvin 
press 
backpropagation basic theory 
rumelhart chauvin eds backpropagation theory practice 
cambridge ma mit press 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland pdp research group eds parallel distributed processing explorations microstructure cognition 
volume foundations pp 

cambridge ma mit press 
rumelhart hinton williams 

learning representations back propagating errors 
nature 
rumelhart mcclelland 

interactive activation model context effects letter perception part 
understanding normal impaired word reading contextual enhancement effect tests extensions model 
psychological review 
rumelhart mcclelland 

learning past tenses english verbs 
mcclelland rumelhart pdp research group eds parallel distributed processing explorations microstructure cognition 
volume psychological biological models pp 

cambridge ma mit press 
rumelhart mcclelland pdp research group eds 

parallel distributed processing explorations microstructure cognition 
volume foundations 
cambridge ma mit press 
rumelhart todd 

learning connectionist representations 
meyer eds attention performance xiv synergies experimental psychology artifical intelligence cognitive neuroscience pp 

cambridge ma mit press 
schwartz marin 

deep dyslexia reflect right hemisphere reading 
coltheart patterson marshall eds deep dyslexia pp 

london routledge kegan paul 
schwartz marin 

language function dementia case study 
brain language 
schwartz marin 

reading process dementia evidence word specific sound associations 
coltheart patterson marshall eds deep dyslexia pp 

london routledge kegan paul 
seidenberg 

time course phonological code activation writing systems 
cognition 
seidenberg 

connectionist models cognitive theory 
psychological science 
seidenberg mcclelland 

distributed developmental model word recognition naming 
psychological review 
seidenberg mcclelland 

words lexicon reply besner 

psychological review 
seidenberg mcclelland 

connectionist models explanatory theories cognition technical report pdp cns 
pittsburgh pa carnegie mellon university department psychology 
seidenberg petersen macdonald plaut 
press 
effects models word recognition 
journal experimental psychology human perception performance 
seidenberg plaut petersen mcclelland 

nonword pronunciation models word recognition 
journal experimental psychology human perception performance 
seidenberg waters barnes tanenhaus 

irregular spelling pronunciation influence word recognition 
journal verbal learning verbal behaviour 
sejnowski koch churchland 

computational neuroscience 
science 
sejnowski rosenberg 

parallel networks learn pronounce english text 
complex systems 
shallice 

neuropsychology mental structure 
cambridge cambridge university press 
shallice mccarthy 

phonological reading patterns impairment possible procedures 
patterson coltheart marshall eds surface dyslexia pp 

hillsdale nj lawrence erlbaum associates 
shallice mcgill 

origins mixed errors 
ed attention performance vii pp 

hillsdale nj lawrence erlbaum associates 
shallice warrington mccarthy 

reading semantics 
quarterly journal experimental psychology 
silverberg silverberg 

specific word recognition skills young children 
exceptional children 


connectionism reading cognition 

smith baker 

influence english spelling patterns pronunciation 
journal verbal learning verbal behaviour 
st john 

story gestalt model processes text comprehension 
cognitive science 
st john mcclelland 

learning applying contextual constraints sentence comprehension 
artificial intelligence 


exploration consistency effect word nonword pronunciation 
memory cognition 
stone 

analysis delta rule learning statistical associations 
rumelhart mcclelland pdp research group eds parallel distributed processing explorations microstructure cognition 
volume foundations pp 

cambridge ma mit press 
stone van orden 

words represented nodes 
memory cognition 
stone van orden 

building resonance framework word recognition design system principles 
journal experimental psychology human perception performance 
strain patterson seidenberg 
press 
semantic effects single word naming 
journal experimental psychology learning memory cognition 
sutton 

adapting bias gradient descent incremental version delta bar delta 
proceedings th national conference artificial intelligence pp 

cambridge ma mit press 
taraban mcclelland 

conspiracy effects word recognition 
journal memory language 
taraban mcclelland 

constituent attachment thematic role assignment sentence processing influences content expectations 
journal memory language 


onset rime units printed words 
coltheart ed attention performance xii psychology reading pp 

hillsdale nj lawrence erlbaum associates 
richmond welty 
press 
special role description understanding normal impaired word reading acquisition english orthography 
journal experimental psychology general 
trueswell tanenhaus 

semantic influences parsing thematic role syntactic disambiguation 
journal memory language 
van orden 

rows rose spelling sound reading 
memory cognition 
van orden 

interdependence form function cognitive systems explains perception printed words 
journal experimental psychology human perception performance 
van orden pennington stone 

word identification reading promise subsymbolic psycholinguistics 
psychological review 


dyslexia 
cambridge ma mit press 


structure english orthography 
hague mouton 
waters seidenberg 

spelling sound effects reading time course decision criteria 
memory cognition 
wickelgren 

context sensitive coding associative memory serial order speech behavior 
psychological review 
widrow hoff 

adaptive switching circuits 
institute radio engineers western electronic show convention convention record part pp 

williams peng 

efficient gradient algorithm line training recurrent network trajectories 
neural computation 
zorzi houghton butterworth 

routes reading aloud 
connectionist dual process model technical report ucl rrg 
london uk department psychology university college university london 
