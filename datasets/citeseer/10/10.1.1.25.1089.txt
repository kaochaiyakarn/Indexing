journal machine learning research submitted published sparse bayesian learning relevance vector machine michael tipping microsoft com microsoft research st george house street cambridge cb nh editor alex smola introduces general bayesian framework obtaining sparse solutions regression classi cation tasks utilising models linear parameters 
framework fully general illustrate approach particular specialisation denote relevance vector machine rvm model identical functional form popular state art support vector machine svm 
demonstrate exploiting probabilistic bayesian learning framework derive accurate prediction models typically utilise dramatically fewer basis functions comparable svm ering number additional advantages 
include bene ts probabilistic predictions automatic estimation nuisance parameters facility utilise arbitrary basis functions non mercer kernels 
detail bayesian framework associated learning algorithm rvm give illustrative examples application comparative benchmarks 
er explanation exceptional degree sparsity obtained discuss demonstrate advantageous features potential extensions bayesian relevance learning 

supervised learning set examples input vectors fx corresponding targets ft real values regression class labels classi cation 
training set wish learn model dependency targets inputs objective making accurate predictions previously unseen values real world data presence noise regression class overlap classi cation implies principal modelling challenge avoid tting training set 
typically base predictions function de ned input space learning process inferring parameters function 
exible popular set candidates form output linearly weighted sum generally nonlinear xed basis functions analysis functions type facilitated michael tipping 
tipping adjustable parameters weights wm appear linearly objective estimate values parameters 
detail bayesian probabilistic framework learning general models form 
key feature approach ering generalisation performance inferred predictors exceedingly sparse contain relatively non zero parameters 
majority parameters automatically set zero learning process giving procedure extremely ective discerning basis functions relevant making predictions 
range models type address extremely broad concentrate specialisation denote relevance vector machine rvm originally introduced tipping 
consider functions type corresponding implemented sparse linearly parameterised model support vector machine svm boser vapnik sch olkopf 
svm predictions function kernel function ectively de ning basis function example training set 
key feature svm classi cation case target function attempts minimise measure error training set simultaneously maximising margin classes feature space implicitly de ned kernel 
highly ective mechanism avoiding tting leads generalisation furthermore results sparse model dependent subset kernel functions associated training examples support vectors lie margin wrong side 
state art results reported tasks svm applied 
despite success identify number signi cant practical disadvantages support vector learning methodology relatively sparse svms unnecessarily liberal basis functions number support vectors required typically grows linearly size training set 
form post processing required reduce computational complexity burges burges sch olkopf 
predictions probabilistic 
regression svm outputs point estimate classi cation hard binary decision 
ideally desire estimate conditional distribution tjx order capture uncertainty prediction 
regression may take form error bars particularly crucial classi cation posterior probabilities class membership necessary adapt varying class priors asymmetric misclassi cation costs 
posterior probability estimates coerced svms post processing platt argue estimates unreliable appendix 

note svm predictor de ned explicitly form emerges implicitly consequence kernel function de ne dot product notional feature space 
sparse bayesian learning relevance vector machine necessary estimate error margin trade parameter regression insensitivity parameter 
generally entails cross validation procedure wasteful data computation 
kernel function satisfy mercer condition 
continuous symmetric kernel positive integral operator 
relevance vector machine rvm bayesian treatment su er limitations 
speci cally adopt fully probabilistic framework introduce prior model weights governed set hyperparameters associated weight probable values iteratively estimated data 
sparsity achieved practice nd posterior distributions weights sharply nitely peaked zero 
term training vectors associated remaining non zero weights relevance vectors principle automatic relevance determination motivates approach mackay neal 
compelling feature rvm capable generalisation performance comparable equivalent svm typically utilises dramatically fewer kernel functions 
section introduce bayesian model initially regression de ne procedure obtaining hyperparameter values weights 
framework extended straightforwardly classi cation case section 
section give examples application rvm scenarios illustration potentially powerful extensions basic model ering benchmark comparisons svm 
er theoretical insight reasons observed sparsity technique section summarising section 
streamline presentation main text considerable theoretical implementational details reserved appendices 

sparse bayesian learning regression detail sparse bayesian regression model associated inference procedures 
classi cation counterpart considered section 
model speci cation data set input target pairs fx considering scalar valued target functions follow standard probabilistic formulation assume targets samples model additive noise independent samples noise process assumed mean zero gaussian variance jx jy notation 
restriction relaxed slightly include conditionally positive kernels smola sch olkopf 

note approach bayesian treatment svm methodology se area seen interest seeger kwok treat kernel function simply de ning set basis functions de nition dot product space 
tipping speci es gaussian distribution mean variance function de ned svm identify general basis functions kernel parameterised training vectors 
due assumption independence likelihood complete data set written exp kt wk wn design matrix xn xn clarity omit notate implicit conditioning set input vectors fx subsequent expressions 
parameters model training examples expect maximumlikelihood estimation lead severe tting 
avoid common approach impose additional constraint parameters example addition complexity penalty term likelihood error function 
implicitly ected inclusion margin term svm 
adopt bayesian perspective constrain parameters de ning explicit prior probability distribution 
encode preference smoother complex functions making popular choice zero mean gaussian prior distribution wj vector hyperparameters 
importantly individual hyperparameter associated independently weight strength prior thereon 
complete speci cation hierarchical prior de ne nal remaining parameter model noise variance quantities examples scale parameters suitable priors gamma distributions see berger gamma ja gamma jc gamma ja dt gamma function 
priors non informative parameters small values 
note characteristic parameter prior general case rvm consider implied prior functions data dependent due appearance xn basis functions xn 
presents practical diculty take care interpreting error bars implied model 
appendix consider detail 
sparse bayesian learning relevance vector machine setting parameters zero obtain uniform logarithmic scale 
scales equally pleasing consequence improper scale invariance predictions independent linear scaling basis function outputs example results depend unit measurement targets 
completeness detailed derivations ered appendix consider case general gamma priors main body analysis results assume uniform scale priors 
formulation prior distributions type automatic relevance determination ard prior mackay neal 
priors neural network individual hyperparameters typically control groups weights associated input dimension idea applied input variables gaussian process models 
evidence data support hypothesis broad prior hyperparameters allows posterior probability mass concentrate large values variables consequence posterior probability associated weights concentrated zero ectively switching corresponding inputs irrelevant 
assignment individual hyperparameter weight basis function key feature relevance vector machine responsible ultimately sparsity properties 
introduce additional parameters model may counter intuitive conceded parameters bayesian perspective correctly integrate nuisance parameters approximate procedure suciently accurately presents problem methodological perspective see neal pp 

subsequently observed failure learning attributable form parameterisation prior functions 
inference having de ned prior bayesian inference proceeds computing bayes rule posterior unknowns data jt new test point predictions corresponding target terms predictive distribution jt jw jt dw familiar familiar bayesian methods may come surprise learn perform computations full analytically seek ective approximation 
compute posterior jt directly perform normalising integral right hand side dw decompose posterior jt wjt jt tipping note compute analytically posterior distribution weights normalising integral tj wj dw convolution gaussians 
posterior distribution weights wjt wj tj exp posterior covariance mean respectively diag 
forced adopt approximation representing second term right hand side hyperparameter posterior jt mode probable values mp mp basis point estimate representative posterior sense functions generated utilising posterior mode values near identical obtained sampling full posterior distribution 
important realise necessitate entire mass posterior accurately approximated delta function 
predictive purposes requiring jt mp mp desire mp mp jt approximation 
notion may visualised thought experiment consider utilising identical basis functions 
follows shortly mode jt unique comprise nite ridge constant value 
delta function considered reasonably approximate probability mass associated ridge point implies identical predictive distribution holds 
evidence experiments suggests predictive approximation ective general 
relevance vector learning search hyperparameter posterior mode maximisation jt tj respect 
evaluating explicitly quicker way obtain weight posterior marginal likelihood 
bayes rule simply write wjt tj wj 
expanding known right hand side quantities gather terms appear exponential complete square introducing new terms give inspection posterior gaussian distribution wjt 
combining remaining terms gives tj 

alternative approach iteratively maximise variational lower bound factorised approximation jt joint posterior distribution model parameters bishop tipping 
computationally intensive technique practice gives expected values hyperparameters identical point estimates obtained method described 
sparse bayesian learning relevance vector machine case uniform consider general case appendix need maximise term tj computable tj wj dw exp related bayesian models quantity known marginal likelihood maximisation known type ii maximum likelihood method berger 
marginal likelihood referred evidence hyperparameters mackay maximisation evidence procedure 
optimising hyperparameters values maximise obtained closed form summarise formulae iterative re estimation 
details concerning hyperparameter estimation including alternative expectation maximisation re estimates appendix di erentiation equating zero rearranging approach mackay gives new th posterior mean weight de ned quantities ii ii th diagonal element posterior weight covariance computed current values 
interpreted measure determined corresponding parameter data mackay 
large highly constrained prior ii follows 
conversely small ts data 
noise variance di erentiation leads re estimate new kt note denominator refers number data examples number basis functions 
learning algorithm proceeds repeated application concurrent updating posterior statistics suitable convergence criteria satis ed see appendix implementation details 
practice re estimation generally nd tend nity fact numerically indistinguishable nity machine accuracy implies jt highly princi 
true case uniform hyperparameter priors adopted 
general gamma priors detailed appendix typically lead large nite values implying small non zero weights 
sparsity realised thresholding weights 
tipping ple nitely peaked zero posteriori certain zero 
corresponding basis functions pruned sparsity realised 
making predictions convergence hyperparameter estimation procedure predictions posterior distribution weights conditioned maximising values mp mp compute predictive distribution new datum jt mp mp jw mp wjt mp mp dw terms integrand gaussian readily computed giving jt mp mp jy mp predictive mean intuitively basis functions weighted posterior mean weights typically zero 
predictive variance error bars comprises sum variance components estimated noise data due uncertainty prediction weights 
practice may choose set parameters xed values purposes point prediction retain required computation error bars see appendix 

sparse bayesian classi cation relevance vector classi cation follows essentially identical framework detailed regression previous section 
simply adapt target conditional distribution likelihood function link function account change target quantities 
consequence introduce additional approximation step algorithm 
class classi cation desired predict posterior probability membership classes input follow statistical convention generalise linear model applying logistic sigmoid link function adopting bernoulli distribution tjx write likelihood fy fy probabilistic speci cation targets 
note noise variance 
regression case integrate weights analytically denied closed form expressions weight posterior wjt marginal likelihood tj 
choose utilise approximation procedure mackay laplace method sparse bayesian learning relevance vector machine 
current xed values probable weights wmp giving location mode posterior distribution 
wjt wj equivalent nding maximum log fp wj log log aw fy standard procedure penalised regularised logistic log likelihood function necessitates iterative maximisation 
second order newton methods may ectively applied hessian required step explicitly computed 
adapted ecient squares algorithm nd wmp 
laplace method simply quadratic approximation log posterior mode 
quantity di erentiated twice give log wjt wmp diag diagonal matrix fy fy 
negated inverted give covariance gaussian approximation posterior weights centred wmp 
statistics wmp place gaussian approximation hyperparameters updated identical fashion regression case 
mode wjt fact rw log wjt wmp write wmp bt equations equivalent solution generalised squares problem mardia 
compared seen laplace approximation ectively maps classi cation problem regression data dependent noise inverse noise variance fy fy 
accurate laplace approximation 
bayesian treatment multilayer neural networks gaussian approximation considered weakness method single mode wjt wmp unrepresentative posterior mass particularly multiple modes case 
linearly parameterised model know wjt log concave hessian negative de nite 
posterior unimodal log concavity implies tails heavier exp jwj expect better accuracy 

alternative gaussian approximation realisable variational bound jaakkola jordan exploited variational rvm bishop tipping 
tipping classi cation number classes greater likelihood generalised standard multinomial form fy nk conventional target coding classi er multiple outputs parameter vector associated hyperparameters hyperparameters shared outputs desired 
modi ed hessian computed see inference proceeds shown 
need heuristically combine multiple classi ers case example svm 
size scales highly disadvantageous consequence computational perspective 

relevance vector examples section rst visualisations relevance vector machine applied simple example synthetic data sets regression section classi cation section followed synthetic regression example demonstrate potential extensions approach section 
er illustrative benchmark results section 
relevance vector regression sinc function function sinc sin popular choice illustrate support vector regression vapnik vapnik place classi cation margin insensitive region introduced tube function errors penalised 
case support vectors lie edge outside region 
example univariate linear spline kernel xm min xm xm min xm min xm approximation sinc uniformly spaced noise free samples utilises support vectors shown left 
rvm model data kernel utilised de ne set basis functions typically tackling problems target function additive noise component variance attempt estimate purposes comparison function approximation svm example model sinc function relevance vector machine noise variance case re estimate 
setting noise standard deviation intended analogous approximate sense setting insensitivity value svm 
xed rvm approximator plotted right requires relevance vectors 
largest error compared support vector case obtained dual bene increased accuracy sparsity 
sparse bayesian learning relevance vector machine representative real problems illustrates case uniform noise corresponding rvm noise model added targets 
linear spline kernel 
trained rvm uses relevance vectors compared svm 
root mean square rms deviation true function rvm svm 
note model necessary tune parameters case fold cross validation 
rvm analogues parameters automatically estimated learning procedure 
support left relevance right vector approximations sinc noise free examples linear spline basis functions 
estimated functions drawn solid lines support relevance vectors shown circled 
support left relevance right vector approximations sinc noisy samples 
estimated functions drawn solid lines true function grey support relevance vectors shown circled 
tipping relevance vector classi cation ripley synthetic data utilise arti cially generated data dimensions order illustrate graphically selection relevance vectors classi cation 
class denoted class generated mixtures gaussians ripley classes overlapping extent bayes error 
relevance vector classi er compared support vector counterpart gaussian kernel de ne xm exp width parameter chosen 
value svm selected fold cross validation training set 
results example training set randomly chosen ripley original 
test error associated example test set rvm slightly superior svm remarkable feature contrast complexity classi ers 
support vector machine utilises kernel functions compared just relevance vector method 
considerable di erence sparsity methods typical results benchmark data sets support 
svm left rvm right classi ers examples ripley data set 
decision boundary shown dashed relevance support vectors shown circled emphasise dramatic reduction complexity rvm model 
interest fact svm relevance vectors distance decision boundary space appearing prototypical anti boundary character 
analysis observation seen consistent hyperparameter update equations form posterior induced laplace approximation section 
qualitative explanation output basis function centred near decision boundary unreliable sparse bayesian learning relevance vector machine indicator class membership output poorly aligned data set space see section illustration concept basis functions naturally penalised deemed irrelevant bayesian framework 
course implication utilisation boundary located located functions correct sense 
extensions giving example results benchmark data sets synthetic example demonstrate potential advantageous features sparse bayesian approach ability utilise arbitrary basis functions facility directly optimise parameters kernel speci cation moderate input scales 
feature considerable importance svm rvm necessary choose type kernel function determine appropriate values associated parameters input scale width parameter gaussian kernel 
examples section follow estimated crossvalidation svm rvm 
sensible practical approach single parameter inapplicable desired associate individual input variable 
multiple input scale parameters kernels basis functions inherently sensible seen ective way dealing irrelevant input variables 
consider problem estimating quite simple dimensional function sinc examples additive gaussian noise standard deviation 
evident problems direct application support relevance vector model data function linear modelled superposition nonlinear functions 
nonlinear element sinc function simply add irrelevant noise input output basis functions re ected approximator 
features function dicult learn accurately function svm approximation rvm gives similar marginally superior results shown 
improve results shown right implement modi cations 
emphasised earlier rvm really specialisation bayesian procedure de ned arbitrary basis sets free modify type number 
gaussian kernel input scale parameter explicit non trivial kernel functions typically incorporate parameter sensitive scaling input variables 
may considered input scale parameter associated input dimension implicitly assumed equal unity 
tipping left function sinc training data right approximation support vector model gaussian kernel 
basis functions feel useful problem 
mirroring general approach taken gaussian process regression rasmussen introduce input variables extra functions 
achieved simply appending extra columns design matrix containing values introducing corresponding weights hyperparameters updated identically 
hope weight associated zero pruned corresponding approximately 
fact complicate problem introduced additional quadratic terms hope similarly prune 
second modi cation directly optimise marginal likelihood respect kernel input scale parameters 
problem introduce parameters kernel function xm exp estimate parameters iteration hyperparameter updates cycle maximisation marginal likelihood respect performed gradient method 
give implementation details appendix modi cations nal accurate rvm approximating function shown 
error sparsity modi ed model compared svm table estimates interesting rvm parameters shown table 
table indicate qualitatively quantitatively improvement obtained modi ed rvm table con rms result model learning correct values newly introduced parameters 
approximation true function value candidates pruned 
second xm exp basis functions depend approximately input variable 
fear procedure sparse bayesian learning relevance vector machine approximation relevance vector model additional linear inputs optimisation input scales 
noise estimate 
model error functions svm rvm table root mean square error number basis functions required approximation function sin svm rvm additional linear input functions optimised input scale parameters 
parameter estimate parameter estimate table left rvm parameter estimates weights associated additional basis functions 
right estimates scale parameters 
set large values shrink widths gaussian zero 
er schematic insight occur section 
underline additional parameters successfully estimated directly training set noisy samples time estimating model parameters including data noise level 
cross validation required 
tipping result quite compelling state limitations approach interleaved stage training procedure leaves open question exactly combine optimisation see appendix 
optimisation computationally complex 
generally apply single data set practical apply single benchmark experiment subsection 
benchmark comparisons tables follow summarise regression classi cation performance relevance vector machine example benchmark data sets comparing results illustrative purposes equivalent support vector machines 
number training examples number input variables data set details regarding data appendix prediction error obtained number vectors support relevance required generally averaged number repetitions models 
way summary rvm statistics normalised svm average displayed 
gaussian kernel utilised single case detailed shortly single input scale parameter chosen fold cross validation 
regression errors vectors data set svm rvm svm rvm sinc gaussian noise sinc uniform noise friedman friedman boston housing normalised mean friedman data set additional benchmark result obtained input scale parameters gaussian kernel optimised rvm designated rvm 
errors vectors data set svm rvm rvm svm rvm rvm friedman dramatic improvement rvm consequence fact target function deliberately constructed friedman depend input variables uence distractor variables suppressed low estimates corresponding parameters 
unfortunately computational resources limit repetition extended optimisation procedure complete set benchmark experiments sparse bayesian learning relevance vector machine 
typically observe improvements individual regression classi cation experiments generally dramatic shown 
classification examples classi cation performance table details data appendix problems class exception handwritten digit set computational reasons mirrored svm strategy training separate dichotomous classi ers multinomial likelihood 
errors vectors data set svm rvm svm rvm pima diabetes banana breast cancer titanic waveform german image normalised mean 
perspectives sparsity illustrative benchmark examples indicate bayesian learning procedure capable producing highly sparse models 
purpose section er insight causes sparsity rst look detail form weight prior distribution 
adopt gaussian process perspective order give graphical explanation 
prior weights bayesian viewpoint relevance vector machine sparse posterior probability mass distributed solutions small numbers basis functions learning algorithm nds solution 
sparse solutions posteriori relies course prior signi cant probability sparse models priori 
gaussian speci cation wj appear utilising prior 
hierarchical nature prior character need integrate hyperparameters discover true identity prior weights 
section marginal likelihood obtained weights 
alternatively gamma prior hyperparameters possible integrate independently weight obtain marginal tipping considered true weight prior 
gamma function de ned earlier 
equation corresponds density student distribution marginal weight prior product independent student distributions visualisation student prior alongside gaussian 
case uniform hyperprior obtain improper prior jw intuitively looks sparse prior sharply peaked zero popular laplace prior exp jw utilised obtain sparsity bayesian contexts williams negative log regulariser jw chen smola 
implication super cially appear utilising non sparse gaussian prior weights truth hierarchical formulation implies real weight prior clearly recognised encouraging sparsity 
gaussian student left example gaussian prior wj dimensions 
right prior hyperparameters integrated give product student distributions 
note probability mass concentrated origin spines weights zero 
unfortunately continue bayesian analysis route compute wjt marginal longer gaussian 
pose question integrate explicitly maximise nd mode wjt vice versa detailed section 
alternative approach equivalent sparse bayesian learning relevance vector machine maximisation penalised likelihood function form log jw note presence log di regularisation 
fact discount alternative inference strategy typically nd wjt signi cantly multi modal extremely 
modes occur likelihood form gaussian space overlaps spines see right prior 
remind reader conditional posterior wjt maximise step classi cation case section log concave unimodal 
implication marginalised weight posterior mode highly unrepresentative distribution posterior probability mass illustration phenomenon mackay context single hyperparameter models 
conversely discussed section experimental evidence relevance vector learning suggests mp representative posterior jt 
nally note model single hyperparameter governing inverse prior variance weights place probability sparse models relevance prior 
prior specify independence weights magnitudes coupled weights common hyperparameter priori large weights probable large small example 
gaussian process view rst note relevance vector learning regression maximising probability dimensional vector target values model 
gaussian process model rasmussen mackay williams tj re write vector containing output basis function evaluated training examples earlier denoted vector basis functions evaluated single datum 
implicit formulation bayesian model adopted explicitly contexts realise sparsity example sparse kernel pca tipping 
illustrate schematically idealised dimensional projection gaussian process 
basis functions specify directions space outer product contribution covariance modulated inverse hyperparameters adjustment changes size shape adjusting grows shrinks equally directions 
rvm learning procedure iteratively updates noise level contributions vectors order observed data set probable 
tipping gaussian process view regression rvm showing dimensional projection dimensional data set space 
data set modelled marked cross weighted directions basis vectors shown grey arrows emanating origin noise component covariance denoted dashed grey circle form covariance illustrated ellipse 
see sparsity arise consider trivial example just single basis function aligned left possible shown contributes signi cantly 
right explained noise 
cases normalisation term jcj unit mahalanobis covariance ellipses noise explanation probable 
intuitively occurs wastes probability mass direction course practice vectors competing noise explain data dimensional space 
general point holds consider increasing contribution spread orthogonal greater direction data better explained increasing noise variance increases identically directions increases jcj 
convergence optimisation deleted basis functions noise level optimised set advance better explain data 
intuition may gleaned pictorial perspective expect lie direction relevant classi cation 
unfortunately simplify learning relevance vectors correspond closely aligned vectors 
sparse bayesian learning relevance vector machine potential explanations data set left basis function conjunction noise 
right noise 
covariance ellipses depicting unit mahalanobis distances shown 
gaussians jcj 
basis functions centred near decision boundary retained 
note rule functions located signi cantly wrong side boundary contributes outer product relevant aligned fact example relevance vector may seen right earlier 
may glean understanding may pro table optimise marginal likelihood respect input scale parameters outlined section 
provides approximate representation projected dimensions marginal likelihood model results gaussian kernel 
narrow kernel width seen gaussian process covariance diagonal 
extreme large width implies data set modelled isotropic noise 
follows data set probable intermediate width 
necessarily case optimal width terms model accuracy seen marginal likelihood optimised inappropriately shrinking width kernel zero occur maximising conventional likelihood 
ect width gaussian kernel marginal probability data set covariance ellipses depicting lines equal probability shown intermediate gaussian width centre giving greatest likelihood tipping 
discussion sought provide technical detail experimental justi cation bayesian approach sparse learning linearly parameterised models er understanding mechanism sparsity realised 
concentrated comparison popular support vector machine underline method general 
seek state approach de superior regression classi cation tool worthy serious consideration ers quite compelling advantageous features summarise generalisation typically 
intention comprehensive comparison methods demonstrated section results comparable state art obtained 
learned models typically highly sparse 
examples section models appear optimally compact 
classi cation model gives estimates posterior probability class membership 
highly important overlooked feature practical pattern recognition system 
nuisance parameters validate type ii maximum likelihood procedure automatically sets regularisation parameters noise variance similarly estimated regression 
constraint number type basis functions may note computational implications large number thereof 
optimise global parameters moderate input variable scales 
powerful feature impossible set scale parameters cross validation 
single benchmark task isolated problems highly ective computational reasons extended experimental assessment technique 
potential advantage bayesian approach fully marginalised probability model basis set tj may considered measure merit model provide criterion selection kernel 
indicated quantity computed analytically simple practical numerical approximations employed proved ine ective 
practice cross validation approach nding criterion require signi cantly computational overhead ective approximation remains open research question 
nally note primary disadvantage sparse bayesian method computational complexity learning algorithm 
update rules simple form required memory computation scale respectively square cube number basis functions 
rvm implies algorithm practical training examples number sparse bayesian learning relevance vector machine 
light developed ecient strategy maximising marginal likelihood discussed brie appendix intend publish details shortly 
acknowledgments author wishes chris bishop chris williams neil lawrence ralf herbrich anita john platt bernhard sch olkopf helpful discussions svm code 
addition author excellent comments reviewers 
appendix details relevance vector learning relevance vector learning involves maximisation product marginal likelihood priors convenience 
equivalently straightforwardly maximise log quantity 
addition choose maximise respect log log convenient practice assume uniform logarithmic scale derivatives prior terms vanish space 
retaining general gamma priors maximise log tj log log log log log log ignoring terms independent noting log gives objective function log log log note terms disappear set zero 
consider robustly eciently compute outline derivation hyperparameter updates consider numerical diculties involved 
computing log objective function matrix appears rst terms size computation terms interest may written function posterior weight covariance matrix number basis functions model 
initially practice basis functions deleted optimisation see shortly decrease considerably giving signi cant computational advantages optimisation progresses 
compute rst term exploiting determinant identity see mardia appendix tipping giving log log log log jaj woodbury inversion identity second data dependent term may expressed posterior weight mean 
note may re expressed kt kt kt corresponds penalised log likelihood evaluated posterior mean weights 
terms referred ockham factors 
computation determinant achieved robustly cholesky decomposition procedure 
derivatives updates hyperparameters derivatives respect log log ii setting zero solving gives re estimation rule new ii equivalent expectation maximisation em update see guaranteed locally maximise setting zero mackay de ning quantities ii leads update new observed lead faster convergence bene guarantee local maximisation practice encounter optimisation diculties downhill steps utilising 
sparse bayesian learning relevance vector machine noise variance derivatives respect log log kt tr tr re written setting derivative zero rearranging re expressing terms gives new kt expectation maximisation em updates strategy maximise exploit em formulation treating weights hidden variables maximise wjt log wj operator wjt 
denotes expectation respect distribution weights data hidden variables posterior wjt 
ignoring terms logarithm independent thereof equivalently maximise wjt log wj di erentiation gives update hw posterior hw wjt ii equivalent ecient gradient update earlier 
corresponding procedure noise level maximise wjt log gives new kt old appendix computational considerations numerical accuracy posterior covariance computed inverse hessian matrix positive de nite theory may numerically singular practice 
optimisation hyperparameters progresses range values typically highly extended tend large values 
typically tend nity machine precision permitted 
fact hessian matrix problem approximately ratio smallest largest values order machine precision 
tipping consider case single convenience presentation choose rst hyperparameter 
expression inverse partitioned matrix shown subscript denotes matrix appropriate th row column removed 
term course posterior covariance computed basis function pruned 
furthermore follows consequence model intuitively exactly equivalent basis function excluded 
may choose avoid ill conditioning pruning corresponding basis function model point deleting appropriate column 
cation model optimisation implies typically experience considerable advantageous acceleration learning algorithm 
potential disadvantage believed marginal likelihood increased deleted basis functions reducing nity stage permanent removal suboptimal 
optimisation eciently compute sign gradient marginal likelihood respect corresponding deleted basis functions 
negative gradient imply reducing nite corresponding basis function improve likelihood 
far case 
intuitive reliable criterion deletion basis functions weights iteration regression remove factors fell machine precision case smallest 
result presumably inaccuracies introduced laplace approximation step classi cation slightly robust method required weights deleted note action deleting basis function theory change value objective function practice change negligibly nite part terms log log log log jaj log ja log cancel 
algorithm complexity update rules hyperparameters depend computing posterior weight covariance matrix requires inverse operation fact cholesky decomposition order complexity memory storage number basis functions 
typically pruning discussed rapidly reduces manageable size problems rvm model initialisation may large 
course leads extended training times disadvantage signi cantly set lack necessity perform cross validation nuisance parameters svm 
example exception larger data sets roughly benchmark results section obtained quickly rvm observation depends exact implementations cross validation schedules course 
sparse bayesian learning relevance vector machine large data sets computation scaling approximately full rvm algorithm prohibitively expensive run 
developed alternative algorithm maximise marginal likelihood constructive 
starts single basis function bias adds basis functions deletes current ones appropriate starting possible candidates pruning 
ecient approach number basis functions included step algorithm tends remain low 
greedy optimisation strategy preliminary results show little loss accuracy compared standard algorithm 
appears promising mechanism ensuring sparse bayesian approach remains practical large basis function sets 
appendix adapting input scale parameters section considered parameters kernel speci cation adapted improve marginal likelihood 
note general may prefer select individual kernel parameter example width parameter gaussian crossvalidation 
consider case multiple input scale parameters cross validation option optimising parameters considerable performance gains may achieved 
bene typically notable regression problems exempli ed friedman data set benchmark section 
assume basis functions take form input vector comprises variables corresponding scale inverse squared width parameters 
notate nm elements design matrix 
gradient likelihood respect th scale parameter rst written form nm nm note refers bias independent enter 
rst term independent basis function parameterisation convenient collect terms matrix dnm nm evaluating derivatives gives tt form intended intuitive re writing 
set gaussian basis functions shared scale parameters nm exp mk nk tipping dnm nm mk nk expression basis gradient local optimisation 
exactly performed represents primary implementation diculty 
joint nonlinear optimisation example conjugate gradient methods prohibitively slow 
update equation ective chose interleave updates cycles optimisation simple hill climbing method 
exact quality results somewhat dependent ratio number updates nearly cases signi cant improvement generalisation error observed 
clearly satisfactory er de nitive method optimising expect better mechanism doing employed 
results friedman task toy example section indicate simplistically implemented potentially powerful extension method 
appendix probabilistic outputs error bars regression note alluded earlier care exercised interpreting error bars equation predicated prior functions turn depends basis 
case rvm basis functions data dependent 
see implication re visit link gaussian process models developed section 
gaussian process model speci ed usually zero mean multivariate gaussian distribution observations tj cgp cgp qmn xm covariance function de ned pairs 
weights model 
predictions terms predictive distributions obtained bayes rule jt 
covariance function give rise positive svm kernel adhere mercer condition 
gaussian negative squared exponential common choice xm exp clearly rvm gaussian process de ned predictions recourse model weights 
corresponding covariance function read rvm xm denotes bias 
feature prior data dependent prior covariance points xm depends fx sparse bayesian learning relevance vector machine training set 
implications computing error bars rvm regression 
see consider gaussian covariance function compute predictive error bars away data expect signi cant 
gaussian process williams gp gp evaluated training data points assume test point suciently distant training data elements small 
identical kernel relevance vector model gives predictive variance depends noise variance 
contribution function prior may odd course consistent speci cation 
posterior probabilities classi cation performing classi cation test example prefer model give estimate cjx posterior probability example membership class features quantity expresses uncertainty prediction principled manner facilitating separation inference decision duda hart 
practical terms posterior probability estimates necessary correctly adapt asymmetric misclassi cation costs nearly apply real applications varying class proportions allowing rejection uncertain test examples desired 
importantly bayesian classi cation formulation incorporates bernoulli output distribution equivalent log domain cross entropy error function conjunction logistic sigmoid squashing function enables fy interpreted consistent estimate posterior probability class membership 
provided suciently exible nite data limit estimate exact see bishop 
contrast test point svm outputs real number thresholded give hard binary decision class target absence posterior probabilities svm acknowledged de ciency proposed technique tackling involves posteriori tting sigmoid function xed svm output platt give approximate probability form fa bg 
approach produce predictive outputs range important realise imply output sigmoid necessarily approximation posterior probability 
success post processing strategy predicated original output svm appropriate shifting rescaling approximation inverse sigmoid desired posterior probability 
quantity jx jx referred log odds 
tipping result nature svm objective function expected reliable model 
simple example follows underlines 
gure shows output trained classi ers simple class problem dimension 
class uniform class overlap implying minimal bayes error 
correct log odds shown gure 
zero nite outside region 
output svm rvm sigmoid function case shown classi ers trained generous quantity examples gaussian kernel width 
note models optimal generalisation sense decision boundary may lie 
correct log odds rvm svm output rvm svm trained overlapping uniformly distributed data true log odds membership svm output rescaled prejudice ease comparison rvm 
key feature rvm ers reasonable approximation svm output highly unrepresentative true log odds 
evident setting sigmoid parameters correct 
exempli ed region vicinity decision boundary posterior accuracy arguably important log odds constant 
practically note real application asymmetric misclassi cation losses example medical diagnosis insurance risk assessment absence lack accuracy posterior estimates costly 
sparse bayesian learning relevance vector machine appendix benchmark data sets regression noisy sinc 
data generated function sin equally spaced values added noise distributions rst gaussian standard deviation second uniform 
cases results averaged random instantiations noise error measured true function example test set equally spaced samples 
friedman functions 
synthetic functions taken friedman sin tan inputs generated random dimensional unit hypercube note input variables contribute function gaussian noise unit standard deviation added 
random input samples generated uniformly intervals 
additive gaussian noise generated output standard deviation third signal 
friedman functions average results repetitions quoted randomly generated training examples utilised noise free test examples 
boston housing 
popular regression benchmark data set obtained statlib archive lib stat cmu edu datasets boston 
comprises examples variables results task predicting median house value remaining variables 
averages repetitions randomly chosen examples training remaining testing 
classi cation pima diabetes 
data utilised example set ripley re williams barber 
single split data training test examples available www stats ox ac uk pub prnn 
postal service database images digits commonly utilised benchmark svm related techniques sch olkopf 

data utilised obtained authors comprises training examples example test set 
banana breast cancer titanic waveform german image 
data sets taken collection utilised 

details concerning original provenance sets available highly comprehensive online repository ida gmd de 
total training test splits provided authors results show averages rst 
tipping berger 
statistical decision theory bayesian analysis 
springer second edition 
bishop 
neural networks pattern recognition 
oxford university press 
bishop tipping 
variational relevance vector machines 
boutilier goldszmidt editors proceedings th conference uncertainty arti cial intelligence pages 
morgan kaufmann 
boser guyon vapnik 
training algorithm optimal margin classi ers 
proceedings fifth annual workshop computational learning theory pages 
burges 
simpli ed support vector decision rules 
saitta editor proceedings thirteenth international conference machine learning pages bari italy 
morgan kaufmann 
burges sch olkopf 
improving accuracy speed support vector machines 
mozer jordan petsche editors advances neural information processing systems pages 
mit press 
chen donoho saunders 
atomic decomposition basis pursuit 
technical report department statistics stanford university 
duda hart 
pattern classi cation scene analysis 
john wiley 
friedman 
multivariate adaptive regression splines 
annals statistics 

absolute shrinkage equivalent quadratic 
niklasson bod en editors proceedings eighth international conference arti cial neural networks icann pages 
springer 
jaakkola jordan 
bayesian logistic regression variational approach 
madigan smyth editors proceedings conference arti cial intelligence statistics ft lauderdale fl 

kwok 
evidence framework applied support vector machines 
ieee transactions neural networks 
mackay 
bayesian interpolation 
neural computation 
mackay 
evidence framework applied classi cation networks 
neural computation 
mackay 
bayesian methods backpropagation networks 
domany van hemmen schulten editors models neural networks iii chapter pages 
springer 
sparse bayesian learning relevance vector machine mackay 
gaussian processes 
bishop editor neural networks machine learning pages 
springer 
mackay 
comparison approximate methods handling hyperparameters 
neural computation 
mardia kent bibby 
multivariate analysis 
probability mathematical statistics 
academic press 

ecient training rbf networks classi cation 
proceedings ninth international conference arti cial neural networks icann pages 
iee 
neal 
bayesian learning neural networks 
springer 
platt 
probabilistic outputs support vector machines comparisons regularized likelihood methods 
smola bartlett sch olkopf schuurmans editors advances large margin classi ers 
mit press 
rasmussen 
evaluation gaussian processes methods non linear regression 
phd thesis graduate department computer science university toronto 
onoda 
uller 
soft margins adaboost 
machine learning 
ripley 
pattern recognition neural networks 
cambridge university press 
sch olkopf 
kernel trick distances 
advances neural information processing systems 
mit press 
sch olkopf burges smola editors 
advances kernel methods support vector learning 
mit press 
sch olkopf mika burges 
uller smola 
input space versus feature space kernel methods 
ieee transactions neural networks 
seeger 
bayesian model selection support vector machines gaussian processes kernel classi ers 
solla leen 
uller editors advances neural information processing systems pages 
mit press 
smola sch olkopf 
uller 
connection regularization operators support vector kernels 
neural networks 
smola sch olkopf 
linear programs automatic accuracy control regression 
proceedings ninth international conference arti cial neural networks icann pages 
tipping 
probabilistic methods support vector machines 
solla leen 
uller editors advances neural information processing systems pages 
mit press 
tipping 
relevance vector machine 
solla leen 
uller editors advances neural information processing systems pages 
mit press 
tipping 
sparse kernel principal component analysis 
advances neural information processing systems 
mit press 
vapnik 
statistical learning theory 
wiley 
vapnik smola 
support vector method function approximation regression estimation signal processing 
mozer jordan petsche editors advances neural information processing systems 
mit press 
williams 
prediction gaussian processes linear regression linear prediction 
jordan editor learning graphical models pages 
mit press 
williams barber 
bayesian classi cation gaussian processes 
ieee transactions pattern analysis machine intelligence 
williams 
bayesian regularisation pruning laplace prior 
neural computation 

