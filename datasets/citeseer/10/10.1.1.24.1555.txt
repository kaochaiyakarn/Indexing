years flurry works learning bayesian networks data 
hard problems area effectively learn structure belief network incomplete data presence missing values hidden variables 
introduced algorithm called structural em combines standard expectation maximization em algorithm optimizes parameters structure search model selection 
algorithm learns networks penalized likelihood scores include bic mdl score various approximations bayesian score 
extend structural em deal directly bayesian model selection 
prove convergence resulting algorithm show apply learning large class probabilistic models including bayesian networks variants thereof 
belief networks graphical representation probability distributions 
arguably representation choice uncertainty artificial intelligence successfully applied expert systems diagnostic engines optimal decision making systems 
eliciting belief networks experts laborious expensive process 
years growing interest learning belief networks data 
current methods successful learning structure parameters complete data data record describes values variables network 
unfortunately things different data incomplete 
learning methods exclusively adjusting parameters fixed network structure 
inability learn structure incomplete data considered main problems current state art technology reasons 
real life data contains missing values cited advantages belief networks allow principled methods reasoning incomplete data 
unreasonable time require complete data training 
second learning concise structure crucial avoiding overfitting efficient inference learned model 
introducing hidden variables appear explicitly model learn simpler models 
introduced new method searching structures presence incomplete data 
key idea method best estimate distribution complete data procedures current address institute computer science hebrew university nir cs huji ac il 
ram jerusalem israel bayesian structural em algorithm nir friedman computer science division soda hall university california berkeley ca nir cs berkeley edu efficiently complete data completed data 
follows basic intuition expectation maximization em algorithm learning parameters fixed parametric model 
call method structural em 
name ms em 
roughly speaking structural em performs search joint space struc ture parameters 
step find better parameters current structure select new structure 
case standard parametric em step structural em step 
show penalized likelihood scoring functions bic mdl score procedure converges local maxima 
drawback algorithm applies scoring functions approximate bayesian score 
indications theoretical empirical exact bayesian score provides better assessment generalization properties model data 
bayesian score provides principled way incorporating prior knowledge learning process 
compute bayesian score network need integrate possible parameter assignments network 
general data incomplete integral solved closed form 
current attempts learn incomplete data bayesian score stochastic simulation laplace approximation approximate integral see 
methods tend computationally expensive methods imprecise 
particular laplace approximation assumes likelihood function unimodal cases know function exponential number modes 
introduce framework learning probabilistic models bayesian score standard assumptions form prior distribution 
structural em method idea completion data best guess far 
case search space structures space structures parameters 
organized follows 
section describe class models call factored models includes belief networks multinets decision trees decision graphs probabilistic models 
review learn complete data problems posed incomplete data 
section describe bayesian structural em algorithm settings discuss convergence properties 
algorithm section directly implemented need worth noting structural em procedure applicable scores include priors parameters 
scores incorporate extent prior knowledge learning map parameters maximum likelihood ones 
approximate quantities 
section discuss adapt algorithm learning factored models 
results approximate approach different standard ones literature 
open question accurate 
derivation approximation computational consideration search space network structures 
framework propose suggests possible improvements 
section describe experimental results compare performance networks learned bayesian structural em algorithm networks learned bic score 
preliminaries section define class factored models includes various variants bayesian networks briefly discuss learn complete incomplete data problems raised case 
factored models start notation 
capital letters variable names lowercase letters denote specific values taken variables 
sets variables denoted boldface capital letters assignments values variables sets denoted boldface lowercase letters learning data interested finding best explanation data set possible explanations 
explanations specified sets hypotheses willing consider 
assume class models model parameterized vector legal choice values possible data sets denotes hypothesis underlying distribution model 
shorthand model clear context 
require intersection models zero measure treat disjoint events 
examine conditions algorithms described particularly useful 
defines probability distribution pr assumption considers form models factored model parametric family parameters defines joint probability measure form pr factor value depends variables factored model separable space legal choices parameters cross product legal choices parameters words legal parameterization different factors combined restrictions 
assumption 
models separable factored models 
assumption strong probability model represented single factor 
examples non trivially factored models separable 
example belief network annotated directed acyclic graph encodes joint probability distribution formally belief network tuple component directed acyclic graph vertices correspond random variables encodes set conditional independence assumptions variable independent non descendants parents second component tuple set local models local model maps possible pa values pa set parents probability measure local models pa parameters belief network defines unique joint probability distribution pa straightforward see belief network factored model 
separable combination locally legal parameters defines probability measure 
example specific example consider belief networks variables finite set values 
standard representation local models networks table 
assignment values pa local models product multinomial factors table contains conditional distribution networks decompose pa pa pa pa pa vector contains pa parameters value pa pa pa pa pa pa 
case write joint probability distribution pa pa pa pa easy verify model separable combination legal choices pa results probability distribution 
examples separable factored models include multinets mixture models decision trees decision graphs combination representations belief networks 
example class models factored non trivial sense separable non chordal markov networks 
probability distribution defined networks product form 
change parameters factor requires changing global normalizing constant model 
combination parameters results legal probability distribution 
assumption involves choice factors factored models 
require factor exponential family factor exponential specified form vector valued functions dimension inner product 
example easy verify multinomial factors example exponential 
rewrite standard definitions exponential family include additional normalizing term represent distribution term easily accounted adding additional dimension setting pa pa pa exponential form log pa log pa pa values match values assigned 
pa pa possible values examples exponential factors include univariate multivariate gaussians standard distributions see example 
assumption 
models contain exponential factors 
pr pr bayesian learning assume input dataset number examples 
want predict events generated distribution define bayesian learning problem assume learner prior models parameters model bayesian learning attempts predictions conditioning prior observed data 
prediction probability event seeing training data written pr pr pr pr pr pr pr pr pr pr pr pr pr usually afford sum possible models 
approximate maximum posteriori map model sum models highest posterior probabilities 
justified data sufficient distinguish models expect posterior distribution put weight models 
learning complete data data complete example assigns value variables learning exploit factored structure models 
need assumptions prior distributions parameters model 
assume priori parameters factor independent parameters factors depend form factor 
assumptions called parameter independence parameter modularity heckerman 
assumption 
model factors prior distribution parameters form pr assumption 
pr pr assumptions denote prior parameters factor pr pr practice useful require prior factor conjugate prior 
example dirichlet priors conjugate priors multinomial factors 
types exponential distributions conjugate priors lead closed form solution posterior beliefs probability data 
important property learning assumptions probability complete data model factored form mirrors factorization model 
proposition assumptions data set complete assignments score model consists factors pr pr exponential representation important stress terms score proposition depend accumulated sufficient statistics data 
evaluate score model summary data form accumulated sufficient statistics 
example complete description learning problem multinomial belief networks 
dirichlet priors 
dirichlet prior multinomial distribution variable specified set hyperparameters values say pr dirichlet dirichlet prior parameters pr probability values sufficient statistics gamma function 
details dirichlet priors see 
learn multinomial bayesian networks dirichlet priors need keep counts form pa families intend evaluate 
score network product terms form multinomial factor model see 
particular score form bde score experiments 
learning factored models data done searching space models model models maximizes score 
proposition shows change factored model locally replacing factors score new model differs score old model terms 
caching accumulated sufficient statistics various factors easily evaluate various combinations different factors 
example consider examples search procedures exploit properties 
search current procedures learning belief networks complete data 
search procedure considers arc additions removals reversals 
operations changes factors involved conditional probabilities distributions variables 
execute hill climbing search consider approximately search 
change score due local modification remains modified unrelated part network 
step search procedure needs evaluate modifications involve changes parts model changed previous iteration 
example search procedure exploits factorization properties standard divide conquer approach learning decision trees see example 
decision tree factored model factor corresponds leaf tree 
replace leaf subtree replace subtree leaf factors model remain unchanged 
formal property justifies independent search structure subtree decide root tree 
neighbors point learning incomplete data learning factored models incomplete data harder learning complete data 
mainly due fact posterior parameters longer product independent terms 
reason probability data longer product terms 
posterior distribution parameters model longer product independent posteriors usually represent closed form 
implies exact predictions model integral 
attempt approximate integral 
simplest approximation map parameters 
roughly speaking believe posterior parameters sharply peaked integral dominated predication small region posterior peak 
approximate pr pr vector parameters pr maximizes pr pr find approximation parameters gradient ascent methods em 
probability data model longer decomposes need directly estimate integral 
stochastic simulation extremely expensive terms computation large sample approximations laplace approximation 
approximation assumes posterior parameters peaked gaussian fit neighborhood map parameters estimate integral 
refer reader discussion approximations technique 
approximations requires find map parameters model want consider score 
search model space requires expensive evaluation candidate 
searching large space possible models type search infeasible procedure invest large amount computation making single change model 
thorough investigations properties various approximations bayesian score empirical reports experiments learning structure domains search restricted small number candidates 
structural em algorithm section bayesian structural em algorithm structure selection 
algorithm attempts directly optimize bayesian score asymptotic approximation 
presentation somewhat general settings factored models 
section see specialize factored models 
assume input dataset number examples 
rest section assume dataset fixed denote value supplied missing data random variable 
example dealing standard learning problem training data consists instances possibly partial assignment variables random variables describe training data 
denote set observable variables set variables values determined training data 
similarly denote set hidden unobserved variables variables observed 
assume class models model parameterized pr vector legal choice values defines probability distribution assume prior models parameter assignments model 
sake clarity discussion assumes variables take values finite set 
results section easily apply continuous variables standard continuity smoothness restrictions likelihood functions models find map model suffices pr maximize pr normalizing term models compare 
seen previous section contains missing values usually pr evaluate efficiently 
discussion assume compute estimate complete data likelihood pr seen previous section assumption true class factored models satisfying assumptions 
assume particular model perform predictive inference efficiently 
seen true factored models efficiently compute approximations predictions map approximation 
tools describe general outline bayesian structural em algorithm 
bayesian sem procedure loop convergence compute posterior pr step compute log pr pr log pr step choose maximizes return main idea procedure iteration attempts maximize expected score models actual score 
immediate questions ask 
easier 
buy 
answer question depends class models 
shall see efficiently evaluate expected score factored models 
address second question 
theorem shows procedure progress iteration 
theorem examined bayesian sem procedure 
proof pr log log pr pr log pr sequence models log pr pr log pr pr pr pr pr pr log pr pr log pr pr transformations algebraic manipulations inequality consequence jensen inequality 
theorem implies pr pr choose model maximizes expected score iteration provably making better choice terms marginal score network 
important note theorem implies weaker version step step choose analogous generalized em algorithm 
variant need evaluate expected score possible models step 
fact shall see practice evaluate expected score small subset models 
theorem implies procedure converges improvement objective score 
immediate consequence show procedure reaches point fairly general conditions 
theorem sequence models examined bayesian sem procedure 
number models finite constant models parameters pr limit lim exists 
pr unfortunately say convergence points 
recall standard em algorithm convergence points stationary points objective function 
corresponding notion discrete space models searching 
fact problematic aspect converge sub optimal model 
happen model generates distribution models appear worse examine expected score 
intuitively expect phenomena common ratio proof carries case continuous variables 
simply replace summation integration 
apply jensen inequality mild assumptions density function defined models 
missing information higher 
practice want run algorithm starting points get better estimate map model 
bayesian structural em factored models consider apply bayesian structural em algorithm factored models 
issues need address order translate algorithm concrete procedure 
recall iteration algorithm requires expected score model examine 
term inside expected score assignments pr evaluate complete data 
proposition linearity expectation get property 
proposition training set consist incomplete assignments assumptions consists factors log pr log random variable repre sents accumulated sufficient statistics factor possible completions data 
immediate consequence proposition expected score decomposability properties score complete data local changes model result changes terms score 
complete data search procedures exploit property ones discussed example 
address evaluation terms form log choices 
simplest approximation form log log approximation exact log linear ar 
unfortunately case members exponential family 
cases approximation reasonably accurate 
cases correct non linearity log section expand issues outline possible approximations log approximations covariance matrix vector computing expectations variances raises issue compute probability assignments 
bayesian sem procedure need pr incomplete data usually evaluate posterior efficiently 
address discussed problem map approximation 
want compute expectation attempt learn map parameters 
approximation fairly standard done quite efficiently 
computation map parameters done em done experiments described gradient ascent extensions methods 
fix map parameters standard inference procedure model 
remember approximation imprecise ignores information posterior 
possible way improving approximation considering better approximation posterior ensemble methods 
map approximation get procedure structure procedure factored bayesian sem loop convergence compute map parameters perform search models evaluating model score log model highest score encountered search 
score score return completely specify procedure decide search method structures 
depends class models interested 
classes models class chow trees algorithms construct best scoring model 
see nice idea approach similar structural em 
cases resort heuristic search procedure ones discussed 
general search procedure exploits decomposition properties factored models complete data factored bayesian sem algorithm 
mentioned need estimate moments mean variance distribution order evaluate score factor models share similar factors cache results computations 
consequence evaluation models require additional inference 
cases schedule computation advance know factors examined search 
simple example idea algorithm learning chow trees 
case know advance need evaluate factors involve pairwise interactions variables 
compute necessary information pass training data 
see nice idea 
addition caching strategy fact classes exponential families multinomials gaussians marginalize sufficient statistics factor factor 
upshot discussion efficient search techniques inside bayesian structural em loop 
search algorithms evaluate candidates candidates explore share factors 
new candidate require evaluation expected score factors 
cases examining new model requires new factors evaluated 
computing log examine approximate value log purpose discussion assume factor question fixed omit denote start analysis examining distribution accumulated sufficient statistics recall sum form denotes completion th instance possible completions data 
joint distribution defined model product independent distributions instance data variables independent 
central limit theorem approximated gaussian distribution mean covariance matrix associated functions 
lated performing computation instance training data 
usually compute covariance matrix computations order compute expected sufficient statistics observation implies distribution sharply peaked expected number effective samples data grows 
effective samples samples probability sensitive changes parameters factor 
formally samples zero 
example learning multinomial bayesian networks effective samples pa factor pa pa assigned value completions data 
mentioned simplest approximation log 
approximation precise log linear fairly accurate log approximated linear function vicinity density assigned values region results approximation 
formally taylor expansion get log log log log point line take expectation right hand side second term cancels 
difference log log integration quadratic term taylor expansion 
show norm hessian log bounded region high density bound error 
conjecture factors regular exponential family norm hessian asymptotes expected number effective samples grows 
easily verified multinomial factors 
case simple approximation derivatives log get elements hessian roughly form size expected counts grows hessian matrix vanishes 
implies multinomial factors cases expected counts far safely linear approximation 
hope provide definitive characterization conditions approximation close full version 
cases linear approximation log suffice get better approximation gaussian approximation distribution values approximate log integral gaussian log log multivariate gaussian mean covariance matrix 
note central limit theorem implies normal approximation fairly relatively small number instances 
methods evaluating right hand side 
dimension small numerical integration techniques directly evaluate integral 
dimension large laplace approximation 
reasons believe log behaved integration unimodal function laplace approximation 
perform laplace approximation case need find maximum point alarm insurance method bde bde bde la bde li bic bde bde bde la bde li bic bde bde bde la bde li bic bde bde bde la bde li bic table experimental results learning various percentage missing values 
number cell indicates mean standard deviation kl divergence learned network true network different training sets smaller better 
variants bde score correspond summation integration laplace linear approximations respectively log evaluate hessian log point 
step done standard optimization methods gradient ascent second straight forward application laplace approximation 
due lack space go details 
reminder section discuss apply approximations dirichlet factors 
log log log log log immediately follows linearity expectations log log constant term depends prior 
see approximate expectations individually 
involves log count simplify notation somewhat 
assume mean variance count prior count event 
minimal maximal values take data 
easily recorded computation expected sufficient statistics 
consider approximations log summation approximation iterate possible integral values 
value estimate probability gaussian function integrating range extreme values include volume tail distribution 
log approximate log method scale take values 
baseline evaluate approximations 
integration 
continuous approximation sum log log truncated function necessary grows goes 
evaluate integral numerical integration procedures called hermite gaussian particularly suitable integrals form encoded quite efficiently 
experiments described integration procedure evaluation points 
suspect suffice smaller number control points 
laplace approximation approximate integral gaussian finding mode inte function log mentation find value binary search 
laplace approximation get integral approximated log log log log log imple standard approximations compute second derivatives log experimental results methods section describe results experiments indicate effectiveness general approach evaluate alternative methods computing scores discussed 
networks learning hidden variables 
shaded nodes correspond hidden variables 

addition compare resulting networks networks learned structural em bic score described 
variants procedure general architecture 
search module performs greedy hill climbing search network structures 
evaluate network search procedure calls module aware metric current completion model 
module keeps cache expected sufficient statistics case bayesian score variances bounds avoid recomputations 
missing values real life data sets contain missing values 
poses serious problem learning models 
learning presence missing data careful source omissions 
general omission values informative 
learner learn model maximize probability actual observations includes pattern omissions 
learning procedures attempt score observable data described ignore sense missing values 
justified data missing random mar 
refer interested reader detailed discussion issue 
circumvent requirement augment data indicator variables record omissions augmented data satisfies mar assumption 
procedures discussed relevant dealing data missing random 
order evaluate bayesian structural em procedure performed experiments examine degradation performance learning procedures function percentage missing values 
experiment generated artificial networks alarm network intensive care patient monitoring variables insurance network classifying car insurance applications variables 
network randomly sampled training sets different sizes randomly removed values training sets get training sets varying percentage missing values 
bayesian bic procedures run random initial networks initial random seeds 
initial networks random chain networks connected variables 
evaluated performance learned networks measuring kl divergence learned network generating network 
results summarized table 
expected degradation performance percent missing values grows 
see bayesian procedure consistently outperforms bic procedure prior parameters 
see results summation approximation consistently finding better networks 
cases finds networks small error linear approximation 
especially noticeable smaller training sets 
integration approximation performs slightly worst significantly better linear approximation 
results match hypothesis linear approximation unsuitable small training sets 
larger training sets small percent missing values see linear approximation performs quite better laplace approximation 
hidden variables domains observable variables describe relevant aspects world 
adverse effect learning procedure marginalization hidden quantities lead complex distribution observed variables 
growing interest learning networks include hidden variables 
structural em approach gives tools learning structure fixed set hidden variables 
need additional mechanism choose hidden variables add 
done simple loop searching linear scale 
experiments section attempt evaluate procedure learning hidden variables compares bic score easier learn penalizes network structures 
experiments networks binary variables topology shown 
network hidden variables groups observed variables 
second topology shown 
variables correlated nicely separated hidden ones 
quantified networks parameters sampled dirichlet distribution 
sampled value parameters run standard belief network learning procedure observable variables see hard approximate distribution 
chose parameter settings led worst prediction independent test set 
sampled network training sets sizes instances observable variables learned networks presence hidden binary variables bayesian structural em algorithm bde metric uniform prior bic structural em algorithm uniform prior parameters 
algorithms started set initial network structure randomized parameters 
experiments procedures initialized structure hidden variables parents observable variable 
see motivation choice structure 
discussed bayesian bic versions structural em converge local structural maxima 
case hidden variables phenomena pronounced case missing value 
cases initial structure close local maxima search 
escape local maxima random perturbations 
procedure uses forms perturbations 
hidden method bde bic bde bde bde la bde li bic bde bde bde la bde li bic bde bde bde la bde li bic bde bde bde la bde li bic table performance independent test set networks learned hidden variables bde bic scores 
reported numbers correspond difference log loss test set generating distribution learned distributions 
mean standard deviation quantity run data sets reported 
labels rows indicate number hidden variables learned procedure 
type perturbations change local neighborhood hidden variables tried 
done adding edge hidden variable variable hidden reversing edge 
single edge change procedure restarts structural em procedure new structure runs convergence 
repeated stage procedure perturbs best structure far 
procedure uses cheeseman stutz score evaluate structures different runs structural em 
bic version uses marginal bic score 
repeated perturbations 
type perturbations tried procedure applies second type perturbation simply random sequence moves edge addition deletion reversal 
experiments procedure applied changes 
procedure restarted basic structural em procedure type perturbations 
random walks time limit reached procedure terminated 
results summarized table show variants bayesian procedure usually better predictions bic score 
performance linear approximation better approximations 
main explanation discrepancies missing data case learning problems main improvements achieved runs initialized right random perturbations 
runs terminated cpu minutes runs bic score bde linear approximation gone random restarts runs 
noticeable cases hidden variables require score evaluations factors incomplete data search space define contain local maxima 
structures learned quite close original structure 
due space restrictions elaborate 
discussion described new approach bayesian model selection belief networks related models 
believe approach exciting attempts directly optimize true bayesian score em iterations 
describes framework building algorithms learn incomplete data 
framework provides guarantees leaves open issues collection sufficient statistics computation expected score factor 
details filled class models 
quite bit related learning incomplete data 
general idea interleaving structure search em iteration appeared papers 
structural em friedman introduced framework established formal convergence results 
singh similar insight procedure somewhat different 
structural em procedure procedure iterative 
iteration generates joint assignments missing values best model previous iterations 
procedure invokes learning procedure cooper herskovits completed datasets 
singh procedure merges learned networks trains parameters merged network standard em procedure reiterates 
approach interpreted stochastic approximation structural em 
analysis gives insight limiting behavior singh algorithm 
precisely completed datasets singh approximates expectation score 
combining estimates single search procedure singh searches structures independently completed datasets 
leads various complications need merge learned networks 
variants structural em proposed meila jordan thiesson :10.1.1.54.3360
variants learn multinets selector variable hidden thought mixtures bayesian networks 
meila jordan learn multinets network chow tree 
exploit restriction collect required statistics pass iteration 
provide formal treatment procedure analysis directly applies approach shows procedure converge local maximum 
thiesson aim learn general multinets cheeseman stutz score :10.1.1.54.3360
examining approximations score motivate learning algorithm terminology seen instance factored bayesian sem linear approximation applied multinets 
thiesson efficient method caching expected statistics variables interest gaussian answer queries structure search single pass training data iteration 
analysis directly applies approach 
restriction structural em algorithm focuses learning single model 
practice want committee high scoring models prediction 
committees provide better approximation eq 
ensure commit particulars single model evidence supports models 
meila jordan thiesson attempt approximate committees learning mixture models mixture component bayesian network 
learning map model larger class models 
useful source data better described mixture 
address dependency single model 
alternatively attempt directly follow basic bayesian principle formulated eq 
perform bayesian model averaging 
approach members committee weighted posterior probability 
turns variant bayesian structural em learn bayesian committees 
roughly speaking run bayesian structural em current candidate stage bayesian committee models model weighted posterior probability 
iteration choose models highest expected score current committee 
formal treatment idea somewhat complex topic current research 
issues require additional understanding 
particular provided convergence proofs version algorithm clear proofs apply approximations need perform algorithm practice 
empirical experience shows procedure consistently converge 
better theoretical understanding called 
additional aspect glossed presentation computation expected statistics 
requires large number computations learning 
main bottleneck applying technique large scale domains 
clear able improve standard inference procedures exploiting fact evaluating set queries large number instances 
stochastic simulation attractive approach examine context sample evaluate queries 
requires careful analysis effect noise estimation convergence properties algorithm 
interesting understand possible combine variational approaches type learning procedures 
major open question decide fashion number hidden variables 
right approach learn models hidden variable hidden variables select network highest score 
clearly blind approach 
qualitative model learned hidden variable depends initial structure structural em procedure 
current research examines combine structural em procedure constraint approaches learn constraints possible positions hidden variables guide hidden variables search 
acknowledgments am grateful danny geiger moises goldszmidt daphne koller kevin murphy ron parr stuart russell zohar yakhini useful discussions relating 
comments prompted lead investigate linear approximation detail 
done sri international 
research supported aro number daah onr number 
abramowitz stegun eds 
mathematical functions 

beinlich suermondt chavez cooper 
alarm monitoring system 
proc 
nd euro 
conf 
ai medicine 
binder koller russell kanazawa 
adaptive probabilistic networks hidden variables 
machine learning 
boutilier friedman goldszmidt koller 
context specific independence bayesian networks 
uai pp 


buntine 
learning classification trees 
hand ed ai stats 
cheeseman stutz bayesian classification autoclass theory results 
advances knowledge discovery data mining pp 

chickering heckerman 
efficient approximations marginal likelihood bayesian networks hidden variables 
machine learning 
chickering heckerman meek 
bayesian approach learning bayesian networks local structure 
uai pp 

cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
degroot 
optimal statistical decisions 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal stat 
soc 
friedman 
learning bayesian networks presence missing values hidden variables 
ml 

friedman goldszmidt 
learning bayesian networks local structure 
jordan ed learning graphical models 
preliminary version appeared uai 
geiger heckerman 
knowledge representation inference similarity networks bayesian multinets 
artificial intelligence 
geiger heckerman meek 
asymptotic model selection directed graphs hidden variables 
uai pp 


heckerman 
tutorial learning bayesian networks 
jordan ed learning graphical models 
heckerman geiger chickering 
learning bayesian networks statistical data 
machinelearning 
lam bacchus 
learning bayesian belief networks approach mdl principle 
computational intelligence 
lauritzen 
em algorithm graphical association models missing data 
data analysis 
mackay 
ensemble learning hidden 
unpublished manuscript wol ra phy cam ac uk mackay 
meila jordan 
estimating dependency structure hidden variable 
nips 

pearl 
probabilistic reasoning intelligent systems 
rubin 
inference missing data 

saul jaakkola jordan 
mean field theory sigmoid belief networks 
journal artificial intelligence research 
singh 
learning bayesian networks incomplete data 
aaai pp 


spirtes glymour scheines 
causation prediction search 
thiesson meek chickering heckerman :10.1.1.54.3360
learning mixtures bayesian networks 
uai 
