university california san diego adaptive scheduling master worker applications distributed computational committee charge resources dissertation submitted partial satisfaction requirements degree doctor philosophy computer science computer engineering professor berman chair professor larry carter professor keith marzullo professor rene cruz professor bill lin professor richard wolski gary shao copyright gary shao rights reserved 
dissertation gary shao approved ac quality form publication university california san diego iii chair dedicated memory father love education learning source inspiration iv table contents signature page 
iii table contents 
list figures 
viii list tables 
xi 
xiv vita publications fields study 
xvi 
xvi 

setting context 

master worker parallel applications 

general distributed computing environments 

portable performance 

goals thesis 

organization dissertation 

contributions dissertation 
application environment performance models 

application model 

model 

mw structural model 

application parameters 

environment model 

processors 

networks 

parameter value acquisition 

computation issues 

communication issues 

application performance model 

resource constraints 

flow model 

rate performance 

performance simulation 

usage example 

summary 
scheduling master worker applications 

applying flow modeling mw scheduling 

master process placement 

selecting processors worker processes 

allocation 

reducing performance bottlenecks 

result output deferral accumulation 

utilizing available communication resources 

hierarchical organization 

network congestion control 

non steady state conditions effecting mw scheduling 

mw applications relatively units 

mw applications dominant communication times 

summary 
developing master worker applications gdc environments 



programming model master worker applications 

application template 

example amwat 

performance statistics extension 

scheduler 

application process management 

message handling 

remote process management 

hierarchical organization 

flexible distribution policies 

portable services 

communication services aps 

information management services aps 

process management services aps 

summary 
experimental results 

experimental environments 

homogeneous clusters workstations 

heterogeneous cluster workstations 

homogeneous clusters workstations linked wan connection 
mw applications 

mandelbrot set image generator 

povray ray tracing program 

nas embarrassingly parallel benchmark ep 

tomography program image reconstruction 

orientation search program macromolecular docking 

genetic algorithm solving traveling salesman problem 

amwat performance emulation program 
vi 
experimental performance results 

performance varying computation communication loads 
application scalability test environment 

application speedup test environment 

effects distribution strategies test environment 

experimental results local homogeneous systems 

environmental characterization homogeneous clusters 

mw application performance homogeneous clusters 

experimental results local heterogeneous system 

environmental characterization heterogeneous clusters 

mw application performance heterogeneous cluster 

experimental results wide area system 

environmental characterization wide area system 

mw application performance wan connection 

summary findings 




resource constraints 

acquiring dynamically changing environment parameters 

representation dynamically changing parameter values 

automatic scheduling regime detection 

automatic selection distribution strategy 

hierarchical mw configurations address resource locality issues 
additional support services 

enabling mw framework compiler target 

extending mw flow model application types 

contributions 
appendix 

example amwat master worker application 

example pvm master worker application 

example master program 

example worker program 
bibliography 
vii list figures performance increasing proportional resource capabilities 
examples resource inefficiency 
shaded bars indicate periods computation unshaded bars idle time 
logical structure mw application 
processor network model illustration 
shows interconnections resources network include performance data 
shows effective configuration resources includes performance information 
values network bandwidth taken env measurements values processor capacity application benchmark data 
example asymmetric link bandwidth behavior 
example network configuration 
example environment configuration 
example execution time results 
example gdc environment configuration 
algorithm finding best processor master 
povray execution time results different master hosts 
mandelbrot execution time results different master hosts 
algorithm finding best processors workers 
comparing different allocation strategies variance load distribution 
example environment study performance bottlenecks 
example environment smp machines connected lan type link 
examples level hierarchical mw organization 
hierarchical configuration experiment 
test hierarchical configuration 
demonstration network congestion effects performance 
load balancing example 
processing serialization example 
transport computation times shown worker master process 
emulated worker performance high process serialization 
emulated worker performance reduced process serialization 
observable effects process serialization 
infrastructure master worker computing 
application programming model master worker computations 
amwat programming interface base functions 
amwat programming interface data transfer functions 
example serial computation parallelized mw computation 
example monitored application performance statistics 
programming interface scheduler module 
viii hierarchical mw organization 
aps components 
aps communication component supported interfaces 
programming interface aps comm 
aps information component supported interfaces 
programming interface aps info 
example information search expansion 
example ies querying 
aps process management component supported interfaces 
programming interface process startup 
programming interface process status 
example mw process organization worker processes 
fractal image output mandelbrot program experiments 
image output povray program experiments 
example progression solutions berlin problem ga tsp 
example mapping application parameters graph behavioral regimes 
mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp cycle oriented data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
mapping application parameters graph behavioral regimes active net cluster 
active net machine behavior application identified upper case letters mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
mapping application parameters graph behavioral regimes cetus cluster 
cetus machine behavior application identified upper case letters mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
mandelbrot set generator program homogeneous environments 
povray ray tracing program homogeneous environments 
nas parallel ep benchmark homogeneous environments 
tomography program homogeneous environments 
dot program homogeneous environments 
level configurations tested dot 
ga tsp solver program homogeneous environments 
ix mapping application parameters graph behavioral regimes heterogeneous cluster master host connected mbps link 
range machine behavior cluster application identified bar upper case letters mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp cycle oriented data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
mapping application parameters graph behavioral regimes heterogeneous cluster master host connected mbps link 
range machine behavior cluster application identified bar upper case letters mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp cycle oriented data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
mandelbrot set generator program heterogeneous environment 
povray ray tracing program heterogeneous environment 
nas ep benchmark heterogeneous environment 
tomography program heterogeneous environment 
dot program varying numbers worker processes heterogeneous environment 
ga tsp solver program varying numbers worker processes heterogeneous environment 
mapping application parameters graph behavioral regimes homogeneous clusters connected wan link master process located cetus cluster 
active net machine behavior application identified lower case letters cetus machine behavior identified upper case letters mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp cycle oriented data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
mandelbrot set generator program wan environment 
povray ray tracing program wan environment 
nas ep benchmark program wan environment 
tomography program wan environment 
dot program different master worker tree configurations wan environment 
tested mw configurations dot application wan link 
list tables application environment model parameters 
sensitivity bandwidth measurements data size 
observed range way messaging rates network latencies 
example application model parameters 
partial list heterogeneous mix machines example 
calculated processor constraint values 
calculated network link constraint values 
rates resulting master placement decision 
processor constraints povray application wups 
network link constraints povray application wups 
processor constraints mandelbrot application wups 
network link constraints mandelbrot application wups 
example constraint values bottlenecks lan type links 
example constraint values bottlenecks heterogeneous links 
example constraint values performance bottlenecks wan type link 
statistics returned performance statistics module 
communication statistics separately application specific performance statistics scheduler components cumulative totals 
process statistics worker processes involved computation 
application function calls supported process modes 
implemented distribution policies 
list heterogeneous machine types experiments 
comparative network performance test environments 
example data table environment oriented view mw application performance varying computation communication requirements pep application 
execution times pep emulator program running homogeneous cluster intel pentium iii workstations range application computation communication parameters 
number processors number units execution times measured seconds 
execution times pep emulator program running homogeneous cluster intel pentium iii workstations range application computation communication parameters 
number processors number units execution times measured seconds 
xi execution times pep emulator program running homogeneous cluster sun ultrasparc workstations range application computation communication parameters 
number processors number units execution times measured seconds execution times pep emulator program running homogeneous cluster sun ultrasparc workstations range application computation communication parameters 
number processors number units execution times measured seconds 
execution times produced mw simulator environmental parameters homogeneous cluster intel pentium iii workstations range application parameters computation communication 
number processors number units execution times measured seconds 
execution times produced mw simulator environmental parameters homogeneous cluster sun ultrasparc workstations range application parameters computation communication 
number processors number units execution times measured seconds 
execution times pep emulator program running heterogeneous cluster workstations range application computation communication parameters 
master host linked mbps ethernet number processors number units execution times measured seconds 
execution times pep emulator program running heterogeneous cluster workstations range application computation communication parameters 
master host linked mbps ethernet number processors number units execution times measured seconds 
execution times pep emulator program running heterogeneous cluster workstations range application computation communication parameters 
master host linked mbps ethernet number processors number units execution times measured seconds 
execution times pep emulator program running homogeneous clusters workstations linked wan connection range application computation communication parameters 
number processors master active net cluster number units execution times measured seconds 
execution times pep emulator program running homogeneous clusters workstations linked wan connection range application computation communication parameters 
number processors master active net cluster number units execution times measured seconds 
xii execution times pep emulator program running homogeneous clusters workstations linked wan connection range application computation communication parameters 
number processors master cetus cluster number units execution times measured seconds 
execution times pep emulator program running homogeneous clusters workstations linked wan connection range application computation communication parameters 
number processors master cetus cluster number units execution times measured seconds 
xiii want advisor fran berman continued guidance encouragement 
journey straight easy support constant appreciated 
want advisor rich wolski inspiration examples outstanding technical leadership 
distance influence remains strong 
extended members committee larry carter keith marzullo rene cruz bill lin support patience seeing completed 
special go jim hayes help turning ideas prototype code may someday fit release rest world 
extended people helped provide maintain computing resources experimental 
particular special go cluster ucsd dave active net cluster ucsd cse department computer support staff department maintained workstations ucsd rich wolski cetus cluster utk 
additional extended people contributed original application code adaptation amwat framework chandra mandelbrot bhatia ga tsp jaime frey tomography jim hayes dot 
appreciation grid computing lab countless expressions support encouragement 
addition therapeutic practical benefits friendly competition ultimate forgotten 
financial support provided part nasa fellow ship 
greatest go family 
loving wife paula son daniel ones hard efforts years truly worthwhile 
mom brothers mike david ll able get 
xiv vita june born missouri electrical engineering university missouri columbia computer engineering university missouri columbia electrical engineering washington university st louis doctor philosophy university california san diego publications master slave computing grid gary shao fran berman rich wolski th heterogeneous computing workshop hcw may 
effective network views promote distributed application performance gary shao fran berman rich wolski international conference parallel distributed processing techniques applications pdpta june 
performance effects scheduling strategies master slave distributed applications gary shao rich wolski fran berman ucsd technical report cs 
predicting cost redistribution scheduling gary shao rich wolski fran berman th siam conference parallel processing scientific computing march 
application level scheduling distributed heterogeneous networks fran berman rich wolski silvia jennifer schopf gary shao supercomputing sc november 
fields study major field computer science studies distributed parallel computing professor berman xv dissertation adaptive scheduling master worker applications distributed computational resources gary shao doctor philosophy computer science computer engineering university california san diego professor berman chair popular ways implement parallel operations distributed memory architectures master worker mw organization concentrates control func tions single master process delegates responsibility computations remote worker processes 
mw approach conceptually simple put prac tice achieving consistent portable performance wide range available distributed computing environments requires variety scheduling capabilities techniques allow application behavior tailored fit specific environmental conditions 
dis addresses problems simultaneously achieving mw application performance portability ease development 
application performance determined combination specific application requirements available capacity system resources meet needs 
dissertation presents flow model mw application performance correctly accounts computation communication performance constraints 
ing flow model developed resource selection algorithm choosing appropriate hosts master worker processes delivers performance levels allowed application specific constraints 
application portability essential ensuring wide array platforms available users targets individual mw applications 
simply providing portability reducing available capabilities common denominator apples master worker application template amwat approach developing mw applications maximizes program portability providing access unique xvi resource capabilities specialized scheduling techniques 
variety basic specialized scheduling techniques mw applications experimentally show different techniques appropriate specific scheduling regimes 
particular effectiveness different distribution strategies experimentally compared set test applications tal conditions 
incorporated scheduling techniques portable reusable performance oriented scheduler module 
ease mw application development specifically addressed amwat ap proach 
mw application implement different common func tions show mw development simplified separating functions provided common components general scheduler module purely application specific functionality 
xvii chapter parallelism technique reduce execution time computation allow computation larger problems reasonably accomplished single processor 
common ways implement parallel operations distributed memory architectures master worker approach single master process maintains centralized control concurrent computations number worker processes 
dissertation addresses problems associated creating deploying scheduling master worker applications goal achieving consistent portable formance wide range available distributed computing environments benefits parallelism suitably realized 
setting context performance parallel applications function specific application requirements characteristics resources environments applications run 
discussion master worker mw paradigm environmental domains targeted 
discuss range performance results dissertation 
master worker parallel applications master worker parallelism widely form parallel application program ming 
conceptually simple involves dividing problem number smaller independent units distributed remote worker processes computation parallel 
single master process centrally controls distribution units worker processes return computed results back mas ter process 
method maintaining collection units central location eventual distribution remote processors referred literature queue task queue task farm scheduling 
mw style suited solving problems characteristics total computational broken pieces results computing piece depend results pieces order pieces computed important embarrassingly parallel problems 
examples problems traditionally fit category large genetic database searches image rendering algorithms computational fluid dynamics cfd codes monte carlo simulations tree search algorithms 
effectiveness solving large grained computations mw simple effective technique parallelizing smaller scale iterative computations program loops inside common serial algorithms iterations independently performed 
mw parallel applications require communication single master process worker processes 
distributed memory architectures mw com munication message passing mw applications shared memory architectures efficiently shared data structures interprocess communication 
developing distributed mw applications involves programming effort devel oping serial code equivalents additional required support communication management remotely computing worker processes 
parallel approaches mw programming selected getting perform difficult equivalent serial codes 
faster machines developing better algorithms employing better optimizing compilers established methods improving serial application performance 
meth ods generally applied parallel applications parallel specific methods improving application performance typically understood supported widely available development tools common programming practices 
mw approaches parallelism provide particular advantages disadvantages application developers users 
centralizing control single master pro cess mw applications allow straightforward implementations functions supporting distribution process management fault tolerance 
heterogeneous mixtures computing resources dynamically changing environments easily han application maintains single centralized point control 
reliance single process providing control functionality creates potential single master process performance bottleneck mw applications 
scalability performance increasing numbers worker processes important issue evaluating appropriateness mw approach 
general distributed computing environments opportunities exist running distributed parallel applications processors networked 
give name general distributed computing gdc environments distributed collection commodity machines networks necessarily administrative domain 
particularly interested investigating mw performance issues environments studies date concentrated general scheduling performance issues mw applications running relatively small numbers commodity machines networks 
gdc environments kind available users parallel computing approaches encouraging performance efficient common distributed resources potential delivering significant performance benefits 
variations environmental characteristics gdc environments ef target environments general distributed computing solutions difficult 
areas environmental differences complicate effective general mw approaches application performance portability 
application performance primarily determined behavior cal resources environment processors communication networks 
physical resource characterized terms distinct functional capacities rate processor perform integer floating point operations specific code sequences maximum data transfer rate obtainable network link 
application performance gdc environment limited ap plication demands physical resources exceed constraints imposed resources capable delivering 
context improving application perfor mance process requires finding ways reduce specific application requirements constrained resources targeting physical resources restrictive constraints performance 
applying approach requires thorough understand ing interactions application requirements resource capabilities contribute actual performance 
application portability issues may limit opportunities applications take advantage available resource capabilities 
divide portability issues general areas concern code base runtime portability 
code base portability application property characterizes easily unmodified source code building application instances different environments 
primarily influenced differences low level programming interfaces application develop ment tools available environment 
problems code base portability arise need proprietary user interfaces access vendor specific features high speed switched interconnection network need adjust differences vendor support basic services unix system calls compatibility development tools compilers debuggers applying tools different vendors common code base 
runtime portability application property characterizes hardware software elements required running application targeted environments 
examples possible problems related runtime portability clude missing incompatible versions dynamically loaded libraries third party software services properly installed configured difficulties working user authorization access privileges multiple administrative domains 
dissertation focus mw applications take advantage available resource capabilities achieve performance portability 
portable performance common way measure mw application performance method dissertation look execution time needed workers perform computations return results back master 
mw applications face primary obstacles achieving optimal execution times utilizing best available resources particular job making efficient resources performing computation 
overcoming obstacle involves solving problems application deployment resource selection overcoming second obstacle involves solving different application specific scheduling problems 
solutions effectively address problem areas necessary call portable formance ability single application instance transparently deliver consistently performance variety different computing environments 
assume term application performance means execution times consistently close minimum times possible application set available resources deliverable capabilities computation 
broadly interpret term scheduling include activities allocate order appropriate combinations hardware software data resources purpose performing single computation 
deployment issue separated discussions im proving mw application performance 
oftentimes effective way improve application performance simply enabling application run faster sets resources illustrated 
shows case performance increas ing proportionally increased resource capabilities 
application portability major factor determining widely application deployed platforms available users targets particular application 
contention maximizing application portability essential part ensuring wide array platforms available users targets individual mw applications turn increase opportunities users achieve better application performance 
simply providing portability reducing available capabilities com mon denominator challenge provide portability simultaneously exposing access unique beneficial resource capabilities 
selection important anytime exists choice set resources job 
traditional parallel computing environments problem selecting best resources important issue limited number homogeneous processor network resources available execute parallel job 
seen available options potential computing platforms tapped targets executing parallel application 
increased range choices comes additional responsibilities deciding resources possible candidates application 
proper selection choice depends correctly evaluating predicting real world performance effects alternative decisions 
note cases performance efficient resources particular type 
useful performance predictions accurate models needed mw applications environmental resources effects performance interactions application requirements resource capabilities 
processors speed processors speed performance increasing proportional resource capabilities 
set resources selected mw application schedul ing activities necessary ensure efficient resources 
common mani inefficient resource utilization shown simple examples load imbalance persistently idle workers remains done 
ficient resource utilization characterized large percentage processor time spent idle state performing useful computations results longer execution times achieved efficient resource 
identification appropriate application specific scheduling techniques balancing load improving poor efficiency essential scheduling task maintaining consistent mw performance wide variety environmental conditions 
goals thesis goals thesis investigate level scheduling effort im portant achieving portable mw application performance broad domain general distributed computing environments develop performance prediction models mw applications provide accurate estimations dynamic multi user gdc envi ronments 
load imbalance idle workers examples resource inefficiency 
shaded bars indicate periods computation unshaded bars idle time 
thesis statement portable performance master worker applications achieved identifying particular scheduling techniques demonstrated effec tive specific combinations application requirements environmental conditions accurately weighing perfor mance effects different scheduling approaches apply ing appropriate scheduling techniques necessary ing available levels application performance 
advancing thesis intend show things relatively small set interactions application requirements environmental conditions influence mw application performance predictable measurable ways specific scheduling techniques beneficial mw performance combinations application requirements environmental conditions scheduling techniques generally implemented widely deployable range mw applications 
organization dissertation organized presentation dissertation main chapters 
main chapters described 
performance models 
chapter introduce terminology rest dissertation describing application environmental performance characteristics 
terminology develop mw application performance model parameterized set measurable properties 
properties shown application specific environment specific de pendent particular combinations application environmental characteristics 
performance model uses flow approach emphasize constraining role resource capacities achievable application performance 
scheduling 
chapter variety scheduling activities performed mw applications close existing gaps mw application formance potential performance observed practice 
activities include basic scheduling functions resource selection distribution improved adapted operation wider range computing envi ronments special scheduling techniques beneficially employed specific circumstances boost application performance 
application development 
chapter demonstrate run time control scheduling functions mw applications separated application specific functionality placed independent reusable modules 
describe implementation mw development framework amwat simultaneously simplifies job creating mw applications provides working implementations body scheduling functions described chapter creates portable meta environment simplifying application targeting deployment 
experimental results 
chapter provide experimental results demon strate presence specific operating regions application parameter space demonstrate mw application performance behavior varies region re 
effectiveness different scheduling techniques shown combinations actual mw applications specific computing environments 
main body concluding chapter includes summary findings final drawn brief discussion research directions 
contributions dissertation course research develop support dissertation original contributions 
flow model mw application performance chapter 
devel oped mw application performance model formulated combining simply mea application environmental property values correctly accounts computation communication constraints achievable mw perfor mance 
effective network view graph environment chapter 
developed useful graph representation distributed computing environments called effective network view env graph attributes discovered env discovery tool uses active probing methods 
flow method mw resource selection chapter 
de algorithm applying flow performance model allowing selec tion processor resources master worker processes mw application delivers performance levels allowed system resource constraints 
specialized mw scheduling techniques chapter 
identified specific combinations mw application characteristics environmental conditions ordinary scheduling approaches produce best achievable results 
special ized scheduling solutions performance limiting circumstances de veloped incorporated general mw scheduler module including support output result deferral accumulation multi protocol communication links multi level hierarchical tree organizations active congestion control 
portable reusable performance oriented mw scheduler chapter 
developed implemented general mw scheduling module allows cleanly separating application specific functionality mw application control scheduling functions 
scheduling module successfully incorporates basic advanced scheduling capabilities discussed dissertation automatically available mw applications written module 
application development framework chapter 
developed amwat application development framework demonstrated effectiveness framework providing portable flexible performance enhancing foundation building mw applications 
characteristic operating regions mw applications chapter 
shown specific regions exist application parameter space applica tion performance follows patterns compute bound communication bound overhead bound combination 
operating regions experimentally quantified different computing environments 
distribution strategies chapter 
experimentally compared effectiveness distribution strategies actual mw applications running different workstation environments 
results comparisons allowed reach appropriateness potential benefits having alternative distribution approaches available different computing environments 
chapter application environment performance models chapter describes set models developed characterize mas ter worker mw applications general distributed computing gdc environments performance behavior expect mw applications running gdc environ ments 
models remainder dissertation deriving basic scheduling principles analyzing new scheduling approaches aimed delivering portable performance mw applications running gdc environments 
application model dissertation targets class parallel distributed applications imple mented master worker programs 
mw programs master process hands units worker processes perform computational task units computed results returned back master 
mw approach popular choice developing parallel processing solutions em parallel problems computational task independently performed multiple workers 
example mw computation generation realistic renderings dimensional scenes ray tracing techniques 
ray traced rendering pixel image computed independently pixel lowing computation easily parallelized faster image generation multiple processors 
class mw applications includes staged mw applications master hands possibly different kinds workers separate stages cycles computation 
subsections describe components application model mw applications 
model elements computing model describing mw applications 
mw applications distribute computation processors performing computations parallel 
say total needed performed application divided units wi result performing computation equal union results produced computing wi units wi 
addition unit assumed independently computable units 
assume order units processed important 
symbolically wi wi staged mw application may phases cycles computation performed 
cycle different sets units wc may different computations fc may performed 
specify units com puted results returned cycle cycle application execution defined successful completion computation cycles application 
mw structural model fc wc logically mw application consists multiple processes pj di vided types masters workers 
master processes perform distribu tion units wi collection computed results wi final processing consistent cycle superstep bsp programming model 
master process ck pathways pj worker processes communication logical structure mw application 
results 
worker processes receive units compute results send results back master process 
data related units computed results passed master worker processes communication pathways ck 
communication pathway represents available route physical network connecting processors master worker processes running 
actual physical route communication pathway subject change happen variable routing data packets internet 
mw application requires communication occur master processes worker processes requirement com munication pathways exist individual worker processes 
illustrates logical structure 
application parameters concepts computation structural models define application performance characteristics terms parameters rest dissertation 
parameter applied single unit identified unit index number cycle number unit parameter applicable 
compute cost 
define parameter compute cost computation fc wi processor processor proportional time needed perform computation unloaded 
set processors involved computation 
input data size 
define parameter input data size size bytes data communicated worker processes perform computation fc wi 
output result size 
define parameter output result size size bytes result data communicated back master processes return results computation fc wi 
master processing cost 
processor define parameter master processing cost 
processor proportional time needed master process process results computation fc wi unloaded 
defining parameters individual unit wi cycle application instance acknowledge possibility actual parameter values may vary unit cycle units different cycles 
note variations unit cycle may due data dependent application behavior 
instance compute cost calculating pixel values parts ray traced image vary dramatically depending complexity scene rendered 
addition multi cycle sorting application require different input output data transfer sizes cycle accommodate difference functions performed 
compute cost master processing cost parameters subject variability dependencies processor characteristics implementation details 
dependencies processor characteristics occur different processors deliver different levels performance areas integer operations floating point operations memory intensive operations relevant computation required applica tion 
implementation details level compiler optimization applied building application play role determining processor related costs 
variations application parameter values due data dependent resource dependent implementation dependent characteristics obtaining full ex pression accurate parameter values application may involve considerable effort 
cases parameters show variance due unit computation cycle designate parameter notation omits processor cycle unit specifiers 
case parameters compute cost master processing cost average value course computation proves useful analysis scheduling purposes designate average value forms 
application model parameters intended capture behavior application performs certain basic mw functions interact physical resources gdc environments 
describe model environ ments section section mw application performance model shows application parameters combined resource parameters help determine achievable application performance 
environment model target platform mw applications consider class general dis tributed computing environments 
gdc environments consist computing platforms interconnected variety network links support distributed par computing 
computing platforms consider include uniprocessor workstations personal computers multiprocessor smp workstations multiprocessor smp clus ters 
forms networking consider include local area networks lans networks lan clusters wide area networks wans 
gdc environments thought limited subclass computational grid environments 
general computational grids envisioned means providing pervasive access wide variety computational resources including limited processors networks instruments databases view gdc environments limited supplying processing re sources performing mw computations networking resources allow distributed operations 
chosen concentrate aspects mw application performance gdc environments computation communication 
emphasis designed environmental models contain basic component types processors communication networks 
developed gdc environment model constructed component types application performance analysis scheduling activities described sections dissertation 
processors processor components represent computing capacity different platforms gdc environment 
model assign processor component available physical cpu computing platform 
means uniprocessor workstation represented single processor component dual cpu smp multiprocessor workstation represented processor components 
processor components characterized set computation capacity parameters describe ability processor deliver aspect computation 
parameter value may needed characterize processor different computations place demands different functional areas processor component 
example computation involving large number floating point operations run cpu designed fast floating point performance computation involving integer operations memory transfers run poorly hardware 
case applications processor capacity may expressed purely application specific value comparisons second computation implementing mw exhaustive search algorithm 
characterization processor capacity depends heavily de mands placed different applications represent computation capacity values processor pi model parameter pi vector containing set processor capacity measures terms user specified operations unit time 
example pi values mflops mips comparisons sec represent capacity processor pi perform benchmarked floating point operations benchmarked integer instructions application specific gene sequence comparisons re spectively 
discuss section manner application specific terms determined 
networks network components represent communication capacity network resources gdc environments 
model network resources shared network processor network model illustration 
links represent pathways physical network resources carry data transfers processors network links multi user systems 
visually illustrates simple processor network model configuration showing processor components interconnected network link components 
network links considered shared resources multiple communication transfers take place parallel links transfer occurring time transfer 
illustrates multiple links may interconnect processor components shown links represent ethernet lan type link processors represent available high speed connection processors multi processor machine 
modeling network behavior requires represent network con figuration physical links system modeled characteristics individual links 
configuration information essential deter mining links utilized transfer processors 
illustration visually see communication transfer pro cessors utilizes single link transfer processors utilizes links 
configuration information network topology shows link loaded transfers occur 
network configuration model represented undirected graph nodes represent processor elements network links edges represent available communication paths nodes 
detailed description form graph takes section 
network link characteristics captured parameters model band width latency 
bandwidth specifies available capacity link support aggregate data transfer rate specified units bytes second 
concerned bandwidth delivered application presence traffic may traversing link 
means accumulated sum network traffic flowing link exceed maximum available band width 
latency specifies minimum time data introduced link data passed link ready leave 
example latency minimum time needed byte message travel sender san diego california receiver knoxville tennessee internet link 
bandwidth model network link nj represented parameter nj latency represented parameter nj 
discuss section appropriate link configuration parameters values determined 
parameter value acquisition having defined parameters comprise models mw applications gdc environments remains shown values parameters assigned model terms 
thing kept mind modeling approach uses scalar value parameters account time varying behavior 
time varying resource behavior important analysis scheduling activity models rely value monitoring services network weather service nws remos return dynamic scalar values real time observations environmental conditions 
nws unique benefit providing value forecasting capabilities time series analysis historical measurement data 
examples parameters monitoring prediction methods may applied time varying resource characteristics processor availability network link bandwidth network link latency values 
table summarizes application environment parameters defined 
application compute cost parameter master processing cost parameter strongly related processor computation capacity values parameter description units application compute cost user defined ops unit application master processing cost user defined ops unit application input data size bytes unit application output result size bytes unit processor computation capacity user defined ops second processor availability fraction network configuration graph undirected graph network link bandwidth bytes second network link latency seconds table application environment model parameters 
discuss acquiring values section focusing computation issues 
similarly application input data size parameter output result size parameter strongly related network graph network link parameters discuss acquiring values section focusing communication issues 
computation issues vector parameter containing set values representing capacity processor perform different forms computation 
examples non application specific measures computation capacity include metrics peak floating point operations second mflops peak instructions second mips results basic architectural benchmarks 
advantage standardized benchmark re sults wide availability results different platforms benchmark figures previously published 
possible disadvantage standardized benchmark values processor capacity processor capacity parameters may hard performance analysis modeling purposes relationship processor capacity values application compute costs master process ing costs easily determined 
example mflops measure processor capacity number floating point operations needed compute unit application needs identified parameter values combined performance estimation purposes 
solve problem may necessary utilize application specific parameter values derived application specific benchmarks standard benchmark results adequately represent interactions application processor resources computations 
units column table parameters shows unit listed user defined ops represents application specific measures operations formed computation 
modeling system allows different sets computation related parameters defined specific combinations mw applications gdc resources 
model supports user defined operations units reasons need support different requirements various applications place physical pro cessor resources need allow definitions new application specific units measure cases general parameters easy relate specific application requirements processor resource capacities 
reason demonstrated simple example different applications performing floating point opera tions performing integer operations 
clear performance application depends different aspects processor performance achieve re sults different methods measuring suitability resources application employed 
second reason demonstrated applications application specific characterization application interacting dif ferent resources available complex computation breakdown basic operation types known easily determined 
application specific benchmarking determining processor capacity ap plication cost terms computation cost terms difficult determine prior execution factors input data dependencies complex interactions application code specific architectural features memory hierarchy different levels compiler optimizations effects enabled debug ging performance monitoring features 
handle complex cases operation defined computation single unit parameters assigned values represent unit basic operation quan parameter 
parameter vector processor contain application specific benchmark results giving capacity processor terms units unit time calculated inverse average time taken worker process running unloaded processor compute single unit 
example operation unit application application specific measured value seconds average computing time unit processor determine operations sec quantity units sec represents computational capacity processor application 
parameter values defined purely terms application specific units advantage acquired direct measurements computation time may easier performance analysis tasks scheduling calculations require breaking computation fundamental opera tions 
disadvantage new instance application including changes input data application require new application specific mea resource considered 
values parameter vector computing capacity peak performance unloaded processor 
reality processors gdc environments shared resources deliverable processor resource capacity values depend time varying factors loading processes applications 
account time varying characteristics define second processor parameter pi processor pi modeling specifies availability processor 
define availability percentage processor time application expected receive values 
availability values acquired real time monitoring forecasting services nws 
communication issues acquiring parameter values related communication modeling approach difficult respect environmental parameters application parameters 
acquiring application parameters values usually determined simple inspection application code calculations analysis algorithms 
applications image rendering data trans fer sizes may selectable run time options allow greater flexibility setting granularity workload division distributed processing 
acquiring values envi parameters straightforward process 
subsections describe parameter values acquired network parameter types 
link bandwidth link bandwidth dynamic time varying characteristic kinds shared networks gdc environments 
characterization net environments account explicitly bandwidth function time rely static bandwidth parameter values supplied ap steady state nature model 
bandwidth values model intended represent aggregate bandwidth behavior expected seen application time running 
possible time dependent func tions provide increased accuracy certain environmental conditions modeling approach preclude extension techniques structural modeling provide additional modeling fidelity 
discuss topic greater detail chapter area investigation 
situations acquiring bandwidth values requires active test measure rate data transfers completed network link 
experience confirmed observations researchers effective bandwidth value acquisition requires just measured values predictions capacities network links 
bandwidth measurements taken time exhibit bursty irregular pattern requires additional analysis mea data forming predictions expected bandwidth behavior 
resource monitoring forecasting services nws provide behavior analysis capabilities rely provide appropriate scalar parameter values dynamically changing resource conditions 
second consideration bandwidth characterization relationship tween results bandwidth measurements size data transfers measurement process 
experience shown wide differences observed network bandwidth resulting transfers small sized data blocks compared observed transfers large sized data blocks 
observation confirmed su study file transfer performance 
example discrepancy observed seen table bandwidth measurements machines connected mbps ethernet link 
table shows varying size data blocks transferred bandwidth measurement test significant impact results test observed bandwidth byte blocks times greater byte blocks 
measurements data size measured bandwidth bytes mbps table sensitivity bandwidth measurements data size 
running program averaged results trials trial involved consecutive transfer blocks test size transmitting machine receiving machine 
measurements time difference block transmitted arrival block receiver side 
characterization bandwidth includes effects hardware software elements including speed network interfaces efficiency various layers networking software actual load conditions ethernet link tested 
effects consolidated single bandwidth represent effective bandwidth experienced communicating processes running machines tested 
machine running non dedicated mode external loading outside processes observed absent tests 
results suggest better correlation bandwidth values obtained testing bandwidth achieved application execution bandwidth tests transfer sizes close transfer sizes expected application 
link latency acquisition values network link latency shares diffi culties acquisition link bandwidth values 
latency dynamic time varying characteristic shared network resources 
reasons scalar bandwidth values represent latency values scalar values model 
assumption available monitoring prediction capabilities better job supplying accurate scalar parameter values known time dependent func tional representations 
complicating acquisition process problems accurately measuring way message latency values absolute transfer start times easily obtained high precision distributed environments 
precision im portant latency times may values measured units microseconds milliseconds 
way gauge magnitude network latency observing time required complete cycles word messages sent back forth processes 
measuring time required multiple cycles allows achieve reasonable precision generating longer periods measurement 
cycle time passing word messages back forth processes obtained dividing time taken message cycles number cycles performed 
cycle time forms lower limit time needed perform way message transfers processes upper limit effective network latency encountered passing messages network total transfer times actual latencies experienced 
bandwidth effects minimized small word message size way message cycle time estimate sum network latencies pair processes 
addition calculate maximum value way message cycles completed period time inverse time needed single way message 
example minimum time observed completing way message cycle milliseconds estimate effective network latency approximately milliseconds expected maximum rate way messages occur messages second 
table shows representative way messaging rates network latencies measured different types network links 
previously explained network latencies estimated half value way messaging times inversely proportional way messaging rates 
results show strong correlation networking distance resulting network latencies experienced 
la observed wide area network wan link extending san diego california knoxville tennessee metropolitan area network man link workstation university personal home computer connected internet cable modem orders magnitude greater lan ethernet links 
measurements wireless network link department area network dan workstation laptop computer fell range wan lan links 
lowest latency values observed obtained measuring way messaging rate processes running different processors source network target network link message rate latency type type type msgs sec msec mbps ethernet mbps ethernet wan mbps ethernet cable modem man mbps ethernet wireless dan mbps ethernet mbps ether lan mbps ethernet mbps ether lan mbps ethernet mbps ether smp table observed range way messaging rates network latencies 
smp machine 
latency results shown lend support idea network latency may effect mw application performance latency times comparable magnitude unit computation times 
mw applications contain relatively large grained units unit computation times measurable units hundreds milliseconds latency times observed smaller lan type links significant factors limiting application performance 
idea explored section discusses performance models developed mw applications 
table shows general trend correlation distance latency times environmental factors networking technology traffic loading packet routing paths introduce variances observed link latencies 
vari ances may introduce additional latency local environments link latency ences normally observed significant result increases link latency wide area environments 
cases indirect measurements link latency similar way messaging tests may estimate actual latency parameter values 
network configuration graph modeling approach calls building representative graph logical network topology basic sets processor network link components 
logical network topology representation real network constructed arranging basic network components configuration expected produce application behavior equivalent observed running application real network 
note logical network topology experienced application physical network topology may differ 
lowekamp hallaron gross focused problem discovering modeling actual network topologies 
available information actual network topology useful determining logical network topology experienced application 
practice discovering accurate logical network topology non trivial problem 
environments relevant information interconnection ration real networks may hard obtain 
person installs services equipment really knows cares exactly things hooked 
cases system administrators may reluctant reveal details network fear releasing information may compromise system security 
com mon user level network tools traceroute pathchar able report network routing switch level produce estimates router congestion switch level may problems things distinguishing hosts common subnet sharing single hub separate subnets 
complete maps physical connections available components network topological information useful analysis scheduling effects application performance derived quantified 
physical configuration maps may contain components bridges routers behavior relevant easily translated specific quantifiable effects application performance 
represent logical topology developed approach create call effective network views represent configurations network resources terms potential application 
env information discovered collection straightforward network performance tests require special system tools access privileges 
ultimate goal develop scheduling methods approaches ordinary user chosen develop techniques require presence special system tools access privileges 
env approach widely portable range gdc environments 
procedure creating env graphs involves series steps designed identify clusters machines significant sharing network resource physical ethernet subnet 
detect sharing looking similar responses network events machines initial test phase 
procedure testing activity originates test machine initiates testing processes group machines tested 
resulting env data relative test machine 
phases env discovery process 
bandwidth test test machine machine test group 
bandwidth tests run measuring results consecutive trials averaging results 
bandwidth tests machines similar bandwidth performance clustered assumption machines dissimilar network performance different network resources 
second phase runs simultaneous bandwidth tests test machine machines cluster effort dis cover machines sharing common network resource 
machines identified logically sharing network link interference simultaneous bandwidth tests reduces individual bandwidth results threshold percentage 
threshold percentage midway expected degradation level ideally independent links expected degradation level identical data streams ideally interfering sharing link 
machines identified sharing link grouped labeled name link identified sharing 
third phase runs pairwise bandwidth tests machines identified group determine alternate communication pathways connecting machines group accessible machines outside group 
final env graph generated discovered groupings machines associated network links creating nodes undirected graph proces sor network link connected edges representing available pathways communication 
total number bandwidth tests required complete phases de machines testing equivalent allowing clustered treated group 
machine belong equivalence group number bandwidth tests needed sharply reduced machine group needs tested represen equivalence group 
case machines tested phase requires bandwidth test machines phases require maximum minimum bandwidth tests maximum occurs equivalence grouping 
gives maximum number bandwidth tests required minimum number bandwidth tests required 
bandwidth test takes seconds run entire env discovery process takes minutes complete moderate sized systems consisting tens processors 
larger systems num ber bandwidth tests run reduced greatly presence machines belonging equivalence group 
network configuration system characteristic expected change frequently env discovery process needs run periodically day change network configuration suspected occurred 
example env discovery process show phys ical topology set workstations located computer science engineering cse department ucsd 
shows env graph set stations generated env discovery process 
test machine located san diego supercomputer center sdsc separated cse department campus wide atm network 
shows network link components pcl net circus net connecting proces sor components 
env discovery process correctly identified processor elements linked different ethernet hubs shown physical layout 
env graph correctly identified presence fast internal network repre sented links 
small cluster machines env discovery process required seconds complete testing 
env discovery process intended acquiring network topology information networked resources located geographical neighborhood belonging administrative domain 
building representations resources spanning domains wan environments extend local env graphs wan links connecting local domains 
parameter values bandwidth latency wan links accomplished conventional point point testing procedures tools nws 
process allows extend configuration modeling cover gdc environments including resources connected wide area networks 
validated env discovery process running various clusters workstations located university california san diego ucsd san diego supercomputer center sdsc university tennessee knoxville utk 
local clusters consisted non dedicated workstations connected different combinations mbps mbps ethernet 
sites configured ethernet networks pcl net hub switch circus net hub switch tandem mbit hub physical topology tandem pcl net mbit effective topology hub circus net mbit mbit shows interconnections resources network include performance data 
shows effective configuration resources includes performance information 
values network bandwidth taken env measurements values processor capacity application benchmark data 
directly connect individual machine digital switch sites connected multiple machines ethernet hubs making connection switch 
ran env discovery utility environments time compared results discovery process known physical configuration information different sites 
environments resulting env graph showed groupings machines logical links consistent known structure physical network 
testing env discovery process observed cases site site bandwidth latency parameters symmetrical respect sender receiver located 
example occurred testing configuration including small cluster connected internet cable modem measured bandwidth going cable modem averaged twice bandwidth measured cable modem 
shows sample bandwidth values recorded test shows bandwidth values differed significantly magnitude variability measured values time testing 
cases asymmetry occasionally appeared testing network routing sites resulted different paths data packets traveling opposite directions machines 
significant asymmetry incoming outgoing traffic ob served occur measuring bandwidth values links different sites generally single bandwidth value represent expected bandwidth data transfers directions 
asymmetry natural property technology underly ing cable modem connections appears occasionally wan scenarios internet routing traffic machines uses different pathways traffic direc tion 
environments containing wan links specifically run extra bandwidth tests directions links identify possible asymmetric behavior determin ing network configurations 
handle severe cases asymmetric point point network link behavior modify graph include unidirectional links bandwidth latency parameter values replace single bidirectional link normally represent point point connections remote sites 
bandwidth kbps bandwidth test cable modem connection cable modem avg 
kbps cable modem avg 
kbps trial example asymmetric link bandwidth behavior 
application performance model described application model environment model 
describe rate performance model relates mw application performance requirements imposed application capabilities deliverable resources gdc environments 
model provides explicit handling contention communica tion resources empirically obtained parameter values capture application resource behavior 
performance model intended static analysis mw application scenarios run time scheduling mw applications 
small set application environment model parameters values easily acquired observable properties 
resource constraints basis performance model idea resource constraints deter mine achievable performance mw application 
easy see rate units computed worker processes limited processing capacity processor doing computation 
similarly rate data results returned master process limited capacity networks carry data 
need ability quantify level performance resource deliver application package information form useful evaluating performance mw applications 
constraints imposed resources related way re source application 
mw application model concern kinds processes masters workers 
application model worker processes ones perform computation units processors running worker processes constraints computation 
similarly processor running master process create constraints performance due result processing 
network links connecting worker processes master processes resources impose performance constraints performance due com munication 
want express constraints common terms allow easily compared combined respect performance analysis 
description mw application model equation represents application return computed results units wi available set units starting idea master completion time occurs workers returned computed results post processing finished desired metric application performance note time needed perform units inversely proportional rate units processed application rate time 
rate unit processing conceptualized flow involving movement units master process set worker processes back master process 
flow expressed units units unit time units second wups standard unit measure 
reduction application completion time restated maximization effective flow application 
evaluate flow different resources resource application 
processors running worker processes define flow number units computed unit time 
processors running master processes define flow number results processed unit time 
network links define flow number data blocks associated units transferred unit time 
definitions allows specify maximum flow supported resource performing main application functions defined application model 
capacity processor perform result processing wups 
capacity processor perform unit computing wups 
wnet capacity network link perform data transfers wups 
order quantify capacity functions combine information individual resources characteristics application uses resource 
capacity functions wups units able relate known parameter values units 
need show parameters previously defined application environment models derive appropriate values resource capacity functions ww orker wnet 
processor capacity function model determined application parameter processor parameters 
application parameter specifies processing requirements master process process result single unit 
processor parameter specifies maximum performance deliverable processor resource performing master result processing unloaded state 
second processor parameter specifies percentage processor resource available application dynamic value determined run time account current loading conditions 
parameters correspond model parameters respectively described earlier sections 
defined individual application units usually take average value units designate 
complete expression determining equation 
similarly processor capacity function model determined application parameter processor parameters 
application parameter specifies processing requirements worker process compute result single unit 
master process case processor parameter specifies maximum performance deliverable processor resource performing worker compute processing unloaded state 
second processor parameter specifies percentage processor resource available application 
parameters correspond model parameters respectively described earlier sections 
defined individual application units usually take average value units designate 
complete expression determining ww orker equation 
ww orker network capacity function wnet model calculated application parameters network link parameters 
application parameters specify sizes data transfers associated unit input data computation result data computation 
noting assumed mw application types data transfers follow path combine parameter values yield total amount data transferred unit processed 
simple case environments latency significant factor lan configurations need single network link parameter specifying bandwidth calculate desired constraint value 
expression wnet case latency significant shown equation 
wnet case latency significant factor wan config need extend network link function definition 
note network latency comes play performance factor interactions master pro cess worker process slowed due latency bandwidth constraints 
condition occurs way messaging latency master process worker process large prevent significant masking latency effects overlapping computation worker process communication ties network link 
processing unit involves transfer direction link link latency penalty encountered twice time unit passes application 
magnitude penalty terms flow inversely proportional amount latency 
modified expression wnet case latency included shown equation 
wnet min having constructed set resource constraint values help model perfor mance mw application gdc environment discuss obvious limitation approach 
functions derived section generated average value expression steady state application performance model 
properties modeled reality exhibit considerable variability time due time varying load conditions data dependent havior application run 
experience performing experiments chapter despite limitations converting variable terms average steady state values approach yields performance model job estimating mw application performance scheduling diverse mw applications 
major factor achieving accuracy modeling resource behavior ability generate scalar predictions dynamically varying resource characteristics cpu availability network link bandwidth 
capability rely predictive tools nws 
problem accurately predicting time varying processor network characteristics long periods time open research question 
environmental scenarios certainly exist model scalar values averages insufficient 
believe model extended handle scenarios possibly including stochastic parameters described schopf 
flow model consider model mw applications primary function master process pass collect set worker processes assume communication patterns simple defined requiring commu nication master process individual worker processes 
particular assume worker processes steal worker processes course computation 
define application divisible set units unit may require input data produce output data 
units completed application progressing stages mw computation stage transmission unit master worker processes net net net example network configuration 
including data needed worker perform computation 
stage computation unit performed worker 
stage transmission results worker back master 
stage immediate processing unit results done master 
time parallel computation multiple units may active various stages mw computation assume unit time may utilize services resource system 
units stage actively utilizing resource considered waiting queue access granted 
passing stage computation particular system re source employed unit period time unit move stage 
example consider simple network topology shown 
processor designated executing master process unit intended worker process processor stage employ network net transfer required data processor proces sor stage unit occupy processor time run computations 
stage unit utilize network net transfer result data stage unit occupy processor time process incoming results prepare initiating additional unit transfers constructing performance model mw applications look steady state rate applications process units 
rate application cycles units measure application performance faster cycle rates correspond directly reduced application execution times 
view performance include effects factors process startup time idle worker time computation provide important indication achievable parallel application performance computation 
consider flow units master process single worker process definition unit completion rate occurring master worker units units unit time 
mw computations communication different worker processes total rate unit completions application sum rates arising unit completions individual workers 
define equation rate unit completions application master process set worker processes define execution time ime application master process set worker processes units total number units application 
rate performance ime units application performance derived values 
way solve values consider system resource constraint functions bound achievable application performance 
illustrate concept go back example system observe processor network link labeled numerical values 
assume numbers diagram represent resource capacities defined previously represent units unit time 
values network links represent wnet values links upper numbers circle represent worker compute capacity ww orker values lower numbers circle represent master result processing capacity values 
consider application uses processor host master process 
solve application performance determine values 
fundamental constraint condition meet total unit flow rates resource exceed capacity value resource 
means unit flow processor pro cessor passes network net example sum capacity net 
assuming graph representing network connectivity diagram allows identify network resources shared different unit flows resource capacity rates resources system form set upper bounds possible values 
tor function aid defining upper bound constraints define set construc takes input network connectivity graph set worker pro cesses master process network resource returns set worker processes share network resource communicating mw applications easily determined net graph master process set workers network resource path graph worker process master process recording path passing resource example network shown assume master process located processor labeled wish find set returned net path network links workers set master noting paths pass network link net 
example set returned 
give bounds form constraints application performance shown 
define pa processor process running 
pm ww orker pi wnet goal find values meet constraints yield largest value 
solution cor respond configuration delivers best achievable application performance represented model 
frame problem determining values yield largest value flow rate problem parameters 
values flows wish solve 
sink flows 
set worker processes sources flows 
flow constraints correspond ww orker wnet capacity values target environment 
flows mw computation form tree rooted master limited investigation considering process hosted processor efficient algorithms maximum flow algorithm exist solving problem 
approach iteratively parallel solve flow rate problem candidate processes finding expected deliver maximum flow best expected application performance 
chapter give implementation maximum flow algorithm find master process location yields largest achievable flow 
performance simulation part efforts validate rate application performance model described section developed mw application performance simulator provide detailed predictions application performance resource behavior ap plications running gdc environments 
core simulator set routines simulate behavior units pass system comprised kinds resources processors networks 
resources modeled single servers input queues 
service times processor resources determine long unit control processor relinquishing resource unit input queue calculated processor availability parameter processor compute capacities application worker compute value application master result processing value flow rate performance model 
service times network resources determine long network resource committed servicing data transfers unit calculated network bandwidth parameters network latency parameters application data input size application result data size flow rate performance model 
network connectivity system simulated input graph representation previously described section 
simulator implemented portable language code discrete event simulation library called sim 
takes input file containing information network configuration simulated command line parameters specifying application characteristics number units average size unit input data average size unit output results worker computation time needed unit master computation time needed unit distribution scheduling policy 
started initial group units introduced simulated master host processor circulating system 
units cycle system return back master host additional units introduced distribution policy 
simulation complete units distributed returned master host 
simulation statistics kept simulation software printed show simulated utilization network processor resources system computation time number blocks processed processor 
looking statistics resource utilization possible determine particular resources heavily loaded computation certain resources significantly underutilized 
simulator tool validating concept resource capacities form ing primary constraints mw application performance 
simulating resource utilization application units pass processor network ele ments able observe limitations resource capacities influence expected application performance 
performance constraints predicted static performance analysis show dynamic execution traces simulator confidence correctness flow model mw application performance increased 
addition simulator important advantages simple static performance analysis 
ability incorporate dynamic interac tion processes running separate processors 
static model assumes contributing worker processes process units limits constraints imposed resources simulator captures idle times worker processes may waiting new units appear previous results sent 
second advantage network contention accurately simulated breaking unit data smaller packets allowing accurate simulation multiple data streams passing network links 
third advantage allowing pa rameters simulation adjusted scalar steady state values flow rate performance model dynamic data inputs sources statistical distributions measured trace values real application runs 
validation simulator accuracy performed comparing performance results predicted simulator actual runs mw applications various environments 
example type comparison shown section execution times predicted simulation measured values 
addition run simulator range application parameters compared results experimental runs pseudo application called pep performance emulation program various environments 
example type comparison shown section chapter execution times predicted simulation differed experimentally measured times maximum compute bound communication bound regions operation 
useful property simulator easily embedded programs application scheduler provide detailed predictions application performance resource utilization levels 
role proven useful observing performance impact changing application resource parameters having actual application runs test possible cases 
usage example parameter value units ops unit ops unit bytes unit bytes unit table example application model parameters 
demonstrate model components described previous sections section consider ray tracing program running cluster heterogeneous workstations connected subnets lan environment 
application modeled parallel version popular ray tracing program povray 
example consider pixel image divided equal sized blocks pixels 
pixel block forms single unit 
code result block requires bytes pixel translates data block size bytes 
application specific benchmarks calculate computation cost determine result processing cost master process computation time neglected 
table summarizes application parameters povray application 
consider test gdc environment consisting non dedicated heterogeneous collection workstations composition seen table 
ran env configuration discovery utility collection machines generated env graph shown 
graph illustrates logical topology test environment shows workstations located different network links 
graph annotated show measured network link bandwidth values inside triangles representing links 
network link latency values determined msec combination application lan environment tested shown 
processor computation capacity represented graph relative capacity values inside circles representing processors test environment 
application specific benchmarks revealed slowest processor achieve peak rate wups processors achieving rates higher proportion ratings shown 
application environment model parameters determined re name processor network os intel pentium pro mbit ethernet linux sun ultrasparc iii mhz mbit ethernet solaris sun ultrasparc mhz mbit ethernet solaris sun ii mhz mbit ethernet solaris intel pentium pro mbit ethernet linux intel pentium ii mbit ethernet linux sojourner intel pentium ii mbit ethernet linux tandem intel pentium ii mbit ethernet linux thing sun ultrasparc mhz mbit ethernet solaris sojourner table partial list heterogeneous mix machines example 
mbit pcl net mbit thing pcl net mbit tandem example environment configuration 
mbit name ww orker sojourner tandem thing table calculated processor constraint values 
name wnet pcl net pcl net table calculated network link constraint values 
source capacity values calculated 
table shows processor resource ties terms wups table shows network resource capacities wups terms 
obvious comparing sets resource constraints combination application environment computational constraints dominating factor 
slowest network link place constraints performance processors deliver maximum rates running worker processes 
demonstrate effectiveness models show re sults running application test environment testing effects selecting different processor host master process 
case shown processor chosen host master process processors running worker processes 
graph shows execution times obtained analysis performance model described section lightest shaded bar running scenario mw application performance simulator described section medium shaded bar running povray application machines test environment darkest shaded bar 
machines run non dedicated mode relatively light loading outside sources 
results agree analytical interpretation application processor resource bound model correctly identifies master process host produces lowest execution time master process host produces highest execution time 
agrees expected results assuming application limited processor capacity fastest execution times produced highest capacity processors assigned running worker processes 
note analytical results simulation results show correlation actual application running times 
maximum error predicted execution time analytical results hosting master process simulated results sojourner hosting master process 
summary chapter rate performance model developed describe behavior mw applications running gdc environments 
develop models application environment application performance models 
performance model differs application performance modeling ap proaches focus modeling flow aspects application measures elapsed time processes network activity 
flow approach particularly useful helping combine effects computation communication unified performance model suit able analytical tool evaluating potential mw application behavior number different gdc environments run time aid scheduling mw applications 
flow works performance paradigm captures idea resource capabilities constraining application performance way naturally handles situations resources shared multiple activities network links having support data streams multiple worker processes time 
idea flows inherently parallel concept multiple flows sharing execution time sec tandem analysis simulation application master selection results sojourner povray hosts thing example execution time results 
common pathways 
models elapsed time activities computation communication harder time handling effects resource sharing automatically include support deciding shared resources capabilities naturally divided competing demands 
models small set parameters requiring values parameters easily acquired direct observation measurement objects modeled 
number parameters kept small part limiting concerns computation communication elements application performance macroscopic view working characterize areas 
example area computation concern defining particular archi features speed floating point processing size data caches contribute different application specific ways performance 
simply assume applications benefit architectural characteristics allow application specific parameter values specify application comput ing demands resource computing capabilities 
similarly modeling networking concerned network link characteristics network links interconnected form communication pathways processors 
modeling performance parallel programs systems long history 
summary current modeling approaches distin analytic structural modeling approaches 
taxonomy modeling modeling approach considered hybrid analytic structural methods 
earlier analytic approaches employ horizontal de composition strategy divide parameter space different levels 
modeling approach uses categories parameters characterizing applications characterizing resources 
earlier structural approaches knowledge specific communication patterns processing characteristics mw ap plications relate application resource parameter values application performance 
approach acquiring parameter values empirical modeling approach uses parameters easily acquired direct measurements application level benchmarking 
value modeling approach judged kinds activities supports users 
primary uses modeling approach support mw application performance analysis 
analysis driven dif ferent demands 
desire application developers design applications perform certain targeted platforms 
modeling application devel oping different environments serve targets application design tradeoffs explored evaluating application performance varying applica tion characteristics expended writing new code 
second impetus modeling performance come application users want know environments application perform best wish explore performance effects certain available options running application 
desire model mw performance may come administrators computing resources interested evaluating capabilities environments meet needs user base 
individual site administrators able model evaluate systems number different application models better able allocate appropriate kinds resources jobs users run 
chapter discuss specific scheduling requirements mw ap plications running gdc environments apply performance model principles developed chapter problem improving mw application performance 
chapter scheduling master worker applications chapter focuses development performance efficient schedules mw applications running gdc environments 
recall scheduling broadly defined process matching application interactions environmental resources achieve desired performance levels 
addition process communication allocation scheduling include adaptation application configuration options match variable resource conditions control program execution minimize performance prob lems 
note different scheduling techniques may effective particular ranges application characteristics resource conditions 
call specific combi nations application characteristics resource conditions respond effectively particular scheduling technique regime technique 
goal scheduling mw applications gdc environments identify appropriate scheduling techniques regimes provide performance wide range mw application resource parameter space values 
cover major classes mw scheduling activities chapter scheduling basic mw operations flow principles scheduling reduce specific mw performance bottlenecks scheduling mw application conditions steady state assumptions flow performance modeling entirely valid 
net net net example gdc environment configuration 
applying flow modeling mw scheduling basic scheduling tasks performed mw application selection processor resource run master process selection set processor resources run worker processes allocation units worker processes master process 
look scheduling ties concepts flow mw performance model concentration relationships application requirements resource constraints largely determine achievable application performance 
master process placement heterogeneous system selection location master process strongly depends deliverable capacity candidate resources 
consider logical gdc configuration shown processors connected system network links 
labeled network resources values representing wnet capacity terms 
processor resources shown circles diagram labeled values ww orker capacity term top capacity term bottom 
capacity terms units units second 
simple example system determine assignment master process processor gives greatest achievable flow 
processor selected host master process processor able provide units sec master location table rates resulting master placement decision 
rate worker 
addition maximum units sec worth data transferred network net rate supplied processor total expected application rate processor hosting master units sec 
consider selecting processor host master process observe processor deliver rate units sec executing worker 
addition transfer maximum units sec worth data network net supplied processor apparent processor constrained achieving higher application rate units second limitations net capacity capacity processor serve master host 
proceed similar manner processors derive expected application rates candidate master process host 
table shows set possible outcomes process 
apparent column table processor best choice master yielding potential application rate units sec 
master selection algorithm generally developed basic algorithm finding best perform ing host master process 
known maximum flow algorithm ford fulkerson 
algorithm keep augmenting estimated flow rate master host adding contributions worker processors 
additional contributing workers selected local network master 
continues workers included worker rates incorporated capacity limitations network resources capacity limitations master processor 
capacity available processors non local networks added accumulated master total additions possible exceeding resource ties 
outlines basic algorithm finding best performing master host 
termination algorithm processor highest calculated rate selected master 
complexity deriving complexity algorithm note simplified logical representation network configuration described chapter reduces entire system sets processors connected local networks 
local networks connected local networks level remote networking 
logical topology data transfers workers local network pass level networking encounter network resource constraint 
data transfers workers located different local networks pass levels networking satisfy networking constraints 
worker rates meet resource constraints master processor 
arrangement tests constraints algorithm checked master worker pairing 
processors system master candidate workers recall worker processes perform computation individual master rate calculation takes time calculate 
calculating max imum rates possible master candidates takes time 
algorithm requires simple compare accumulation operations resource constraint test entire algorithm efficient clusters containing hundreds proces sors 
practice cache aggregated contributions clusters processors evaluating rate contributions flowing master candidate reduce number individual calculations performed factor proportional sizes clusters 
addition greater efficiency achieved algorithm imple recognizing groups homogeneous processor resources exist sites allowing similar resources clustered groups equivalent machines 
clustering allows evaluate single representative entire group consider ation master process host 
simple implementation optimizations allow algorithm efficiently utilized scheduling mw applications large collections computational resources national technology grids developed national partnership advanced computational infrastructure npaci networks calculate maximum network capacity wnet processors calculate maximum master processor capacity calculate maximum worker processor capacity ww orker candidate master processor local network set sum candidate worker rates set set ound empty networks set network utilization sum get maximum capacity wnet local network get maximum master processor capacity wnet select new processor local network largest available ww orker value get worker processor capacity ww orker get fraction ww orker cause utilization exceed wnet add add add processor set ound total candidate rate min total local network utilization wnet select new processor outside local network largest available ww orker value get worker processor capacity ww orker get fraction ww orker cause utilization exceed wnet network add add add network involved communications processors add processor set ound select processor largest master algorithm finding best processor master 
name ww orker sojourner tandem thing table processor constraints povray application wups 
national computational science alliance ncsa 
example implementa tion algorithm running machine mhz pentium ii processor able select master processor candidate processors organized network separate local networks containing processors milliseconds 
master selection application examples illustrate resource selection process master process show results environmental configuration shown section chapter sample applications 
environment consisted non dedicated heterogeneous cluster workstations 
external loading conditions light allowing processor show availability values testing period 
application parallel implementation povray popular ray tracing program 
example section developed processor network constraint values povray repeat tables 
plug constraint values configuration graph master selection algorithm estimate application performance available processors host master process 
algorithm able select processor best estimated performance hosting master process 
shows results povray example application name wnet pcl net pcl net table network link constraints povray application wups 
test environment 
shows largest difference analysis value measured execution time case master host 
analysis correctly identified master host result lowest execution time 
second application mandelbrot generates fractal images mandel sets 
mandelbrot problem involves calculating fractal image pixel area divided equal sized units require kbytes data transfer unit 
measurements computation time machine lowest computational capacity produced average computation time unit seconds equating computing rate wups 
statistics env environmental configuration information section calculate flow constraints shown table table 
negligible processing required master process result data returned worker processes master process capacity values included exam ple calculations 
similarly network latency values measured average millisecond lan environment studied network latency included calculations 
plugging constraint values configuration graph povray example master selection process yields results shown 
shows machine thing selected best master process host lowest execution time analysis choice hosts thing tandem produced best mea execution time experimental environment 
note expected execution times mandelbrot example computed simple flow analysis considerably lower measured execution time sec tandem analysis application master selection results sojourner povray hosts thing povray execution time results different master hosts 
name ww orker sojourner tandem thing table processor constraints mandelbrot application wups 
name wnet pcl net pcl net table network link constraints mandelbrot application wups 
experimental values lower case thing 
may configuration approaching conditions scheduling regime discussed section dominant communication times communication time taken master process significant factor application performance directly addressed flow model 
additional time taken master process result extra idle time worker processes forced wait new units arrive processing sent results back computed units 
case worker processes mandelbrot example time spent computation percentage total execution time typically ranged 
unpredicted idle time cause analytical approach overestimate deliverable performance worker processes problem address subsection discuss strategy selecting worker process hosts 
selecting processors worker processes selecting master processor turn selection processors run worker processes 
issue select set processors hosting worker processes deliver aggregate performance 
approach start set worker processors master selection algorithm yielded highest expected application performance 
algorithm keeps track set ound list list containing workers algorithm calculate maximum rate application processor master host 
master selection algorithm ensures set processors results flows fall constraints imposed resource capacity limitations 
applications set processors ound list delivers performance close maximum supported resource constraints 
execution time sec tandem analysis application master selection results sojourner mandelbrot hosts thing mandelbrot execution time results different master hosts 
experimental trials set processors ound worker hosts observed worker processes delivering maximum rate values expected algorithm 
example run mandelbrot application worker processes resulted average time spent computing total run time worker 
observations selected workers showed reduction worker performance due presence unaccounted idle time periods time worker processes doing useful 
may result manner units distributed worker processes master process 
mw application tested maintained queue available units master process distributed new units individual worker processes request 
contention shared resources network links processor hosting master process delays occurred time worker process finished unit time unit appeared processing 
delays appeared idle time observations workers 
minimum set workers selected achieve desired rate unexpected idle time workers resulted reduction actual total rate achieved 
primary causes relatively large worker idle times process alization discussed fully section 
presence process serialization worker processes wait master process allocate additional compute 
possible solutions alleviating effects process serial ization include common techniques allocating larger amounts request reduce number waiting periods prefetching units previous unit completed 
effective number units relatively large care taken number units small load imbalance problem 
problem load imbalance fully discussed section 
identifying applications sets environmental conditions lead significant amounts worker idle time scheduling techniques may effective situations explored chapter 
way get application performance obtainable levels pres ence relatively large worker idle times add additional worker processes originally selected mix attempt raise total effective rates delivered application 
goal compensate lost performance due idle time individual worker processes keeping number additional processor resources levels flow additional resources hits real physical constraints longer contributes substantial benefits application performance 
flow rate performance model solve problem workers add increase effective performance steady state model account idle times caused workers waiting new units arrive certain conditions 
address shortcoming steady state approach performance analysis utilized mw application performance simulator described section chapter provide additional capabilities dynamic application performance analysis 
applied problem predicting performance effects worker idle time simulator effective capturing idle time behavior 
algorithm finding effective set processors host worker processes starts master process ound set processors hosting workers master selection algorithm 
simulator run machines target environment values resource capacities master selection algorithm 
results simulation checked see idle time simulated workers results significant decrease application performance 
substantial performance decrease resource utilization figures simulation checked see additional processes added exceeding existing resource constraints 
worker processes available added violate known resource constraints added set workers 
new system configuration additional processes added constructed simulated 
process worker additions testing simulation repeats performance gains realized adding worker processes processes placed exceeding known resource capacity constraints 
illustrates algorithm finding final set worker processes 
algorithm simulator results calculate predicted resource utilization values resource system 
values allow quickly identify system worker processes added improve application performance 
practice number times simulation cycle needs run small algorithm quickly converges situation additional performance gains insignificant additions exceeding resource constraint 
simulator analyze small number specific scheduling options greater detail works enhancing scheduling worker processes conditions process serialization basic flow performance model capture significant performance factors 
allocation selecting processor resource host master process selecting processor resources host worker processes scheduling tasks performed determine application configurations enable maximum achievable flow 
allocation scheduling task performed ensure potential application flow formance realized run time 
see successful allocation strategies perform complementary functions mw applications gdc environments allocate proper number units processed worker process mw configuration maximize achievable application performance run master selection algorithm get master processor set workers ound predicted application rate run application performance simulator ound get simulated rate worker utilization values check workers ound large simulated idle times find additional processors idle time exceeding wnet constraints add processors ound form ound run simulator ound processors get new simulated rate worker utilization values ound ound return ound worker solution set equal equal set ound equal ound return ound worker solution algorithm finding best processors workers 
knowledge application environment parameter values choose approach effectively minimizes loss expected application performance parameter values subject predictable unpredictable errors 
discuss general allocation problem terms flow model mw application performance introduce specific location strategies designed overcome specific allocation problems 
example illustrating effectiveness allocation strategies depends variances application characteristics 
relation flow performance number units allocate individual processes computa tion directly proportional amount flow supported particular configuration application environmental conditions 
worker process achieving twice flow worker process allocated twice number units compute maximum total flow achieved optimizing flow rates independently computing master worker process pairs 
sys tems application environment parameters known certainty invariant time allocation problem trivial number units allocate pi allocate worker processes pi master process total number directly calculated expression equa tion pi terms flow performance model 
strategy employs static fixed fixed approach allocation problem 
systems fixed strategy appropriate include resources dedicated performing single computation applications exhibit unpredictable variances run time behavior 
pi pi pi pi allocate pi pi real world systems allocation problems accurately quantified 
factors contribute uncertainty acquiring various parameter values including external loading shared resources data dependent application behavior 
approach uses static steady state values representations application characteristics environmental conditions necessarily deviate amount actual dynamic conditions despite best efforts effective predictive techniques available 
result errors parameter prediction actual flow overestimated processes underestimated resulting longer application execution times processes overestimated take additional time finish units assigned processes underestimated wait finish 
performance suffers excessively load imbalance caused uncertainty modeling deliverable flow simple fixed allocation approach equation replaced dynamic approaches tolerant prediction errors 
approach tolerant prediction errors requires predic tions 
common approach self scheduling ss allocation simply allocates single units worker processes request continuing locate units manner allocated workers 
equation shows self scheduling allocation policy 
allocate pi units left allocate principle self scheduling process allocated exactly number units able handle computation running maximum idle time incurred processes waiting process finish limited computation result transfer time single unit computed slowest worker process 
self scheduling potential disadvantage requires large number data transfers unit requiring interaction worker process master process 
configurations additional data transfers reduce effective flow rates increasing needed done master process handle data transfers exposing additional network link constraints particularly latency important factor 
handle cases performance suffer excessively param eter prediction errors fixed allocation strategy overhead costs self scheduling allocation strategy allocation techniques pro posed literature draw advantages techniques various combi nations 
common theme proposed techniques reduce number transfers basic self scheduling approach allocating units groups increasing application tolerance real time variances fixed approach ing groups execution 
primary difference allocation techniques size allocation groups calculated techniques decreasing group size computation progresses reduce maximum idle times processes finished allocated computations process finishes computations 
general allocation strategies represented allocation model shown equation master processor pi worker processor sequence number allocation part series allocations total number units allocated number worker processes 
represents allocation function particular distribution strategy 
discussed allocation strategies simple allocation functions fixed constant value ranges pi pi pj special case ss constant value pi 
section discusses known allocation strate gies greater detail give illustration allocation problem results actual application section 
pi pi pi pi allocate pi pi allocation strategies addition basic allocation approaches described previously interested looking conditions variants prove superior choices 
strategies described literature possible candidates allocation mw applications 
fixed size chunking fsc strategy introduced kruskal weiss attempts place units equal sized groups optimal size 
approximation arrived stated expression allocation function current allocation sequence number total number units total number processors value message overhead time variance process times nh ln apparent expression dependent number allocations sequence number indicating fsc strategy relies single allocation size course computation 
attempting determine single optimal allocation size kruskal weiss included application specific information form message overhead time unit processing time variance 
practice kinds application details easy accurately determine require new measurements may need application system characteristics changed 
remaining allocation strategies discuss require additional application specific information calculating allocation sizes 
guided self scheduling gss strategy introduced polychronopoulos kuck allocates units groups exponentially decreasing size 
expression gives allocation function gss strategy meaning expression fsc allocation function 
max gss class decreasing size allocation strategies 
purpose implementing strategies allocation size gets smaller time trade advantages fewer allocation steps necessary strategies fixed allocate large blocks minimized discrepancies worker process finish time strategies ss allocate small chunks 
primary differences decreasing size strategies size initial allocations rate size decreased 
gss starts relatively large allocation sizes allocation sequence equivalent allocations fixed approach exponentially reduces size sequence 
susceptible unexpectedly slow processors assigned early causing imbalance worker process finishing times 
trapezoidal self scheduling tss strategy introduced ni allocates units groups linearly decreasing size 
suggested initial allocation size final allocation size total number units total number worker processes 
resulting allocation formula stated expression parameters meanings previously stated steps np change steps max change tss decreasing size allocation strategy starts smaller initial allocation size gss uses linear decrease subsequent allocation sizes 
behavior tss expected require total allocation steps compared gss allocating number units 
factoring fac strategy introduced hummel flynn allocates units groups organized rounds 
round consists allocation sequences total number processors 
half remaining units allocated active worker processes round resulting unallocated units decreasing half passing round 
allocation formula stated expression parameters retain previously defined meanings round max round fac decreasing size allocation strategy differs class primarily allocation size decreased processors round individual allocation 
allocation sizes rounds exponentially decreasing factor round 
examples literature alternative allocation strategies foundations similarly trading number allo cation steps required distribute versus possible imbalances processor finishing times caused inappropriately large allocations unexpectedly slow proces sors 
strategies taper bold incorporate additional information application environmental characteristics perform complicated cal order adjust allocation behavior run time application performance complexity difficult implement evaluate 
chosen con attention strategies exception fsc require clearly quantified inputs generate results 
strategies originally introduced assumption set processor resources homogeneous computational capacity clearly case general gdc environment 
flynn hummel showed fac adapted heterogeneous resource conditions addition weighting factors proportional computing capacity processor system 
provided proof showed expected value maximum processor finishing times weighted version fac heterogeneous system expected value maximum processor finishing times weighting values shifted slower processors 
similar arguments thesis weighting results strategies intended homogeneous systems better adapt handling heterogeneous cases 
modified fsc gss tss fac strategies weight results calculating group sizes ratio pi ww orker pi ww orker pi processor pi give corrected proportions heterogeneous set processors 
modification reflected general form allocation calculations shown equation 
depending combination application environmental conditions allocation takes aspects flow related problems load balancing prob lems type described section important define correct scheduling regime considering approach 
results experiments examine effectiveness different allocation strategies chapter 
illustration allocation problem help illustrate impact different allocation strategies mw appli cations ran parallel ray tracing program povray heterogeneous set non dedicated sun workstations located university california san diego ucsd 
running shared resources non dedicated environment system resources ex occasional periods contention user jobs periods relative system stability 
mw application job rendering realistic dimensional representation dimensional scene described specialized mod eling language dividing image equal sized rectangular blocks rendered independently blocks forming units mw applica tion model 
application interesting aspect allocation computational effort required render block highly dependent complexity scene rendered inside block means different scene models exhibit vastly different distribution characteristics 
details povray application chapter 
example ray tracing application considered allocation strategies defined equations called fixed self scheduling ss strategies 
compared performance allocation strategies applied ray tracing different scenes 
test scene referred variable workload case scene commonly benchmark evaluating performance platforms ray tracer ported 
second scene referred uniform workload case designed uniform distribution entire image 
created purpose allowing observation ray tracing scheduling performance free effects workload imbalance input scene 
trials run back back alternating combinations scheduling strategy test scene trial expose combination approximately conditions course experiment 
shows execution time traces representative set trials combination majority trials seconds run 
results show interesting facets allocation problem 
case uniform workload notice relatively stable environmental conditions fixed allocation strategy generally superior better average self scheduling approach 
case variable workload self scheduling strategy generally superior better average fixed strategy indicating advantage adapting uncertainties amount computational needed process unit 
hints scheduling regime dynamic ap proaches self scheduling configurations application environment execution time sec execution time comparison test solaris systems processors fixed dist large workload queue large workload fixed dist uniform workload queue uniform workload trial comparing different allocation strategies variance load distribution 
uncertainty determining characteristic parameters higher regime static approaches configurations uncertainty low 
chose workload distribution uncertainty factor varied illustration easy control simply different scenes inputs test application 
similar drawn sources uncertainty process acquiring characteristic parameters flow performance model 
important note examples shown traces environmental conditions caused allocation strategy tried perform leading obvious question better allocation strategies exist help conditions 
reducing performance bottlenecks mw flow rate performance model chapter identify combinations application requirements resource characteristics lead performance bottlenecks specific constraints deliverable resource capabilities result restrictions achievable application performance 
example performance bottleneck limitation number worker processes beneficially employed application due limited network bandwidth link connecting master process worker processes 
section explores various scheduling techniques employed improve mw applications face common performance bottlenecks encountered running gdc environments 
general improving achievable mw application performance means employing best combination worker processes beneficially contribute execution units distributed master process 
discussed known set constraint parameters proper placement master worker processes result higher achievable system rates ultimately lower application execu tion times 
certain conditions identify scheduling techniques attempt improve application performance modifying aspects constraint parameters 
application performance improved concentrating reducing constraint val ues areas known performance bottlenecks 
discuss techniques subsections 
purposes illustration simple representation gdc environment shown 
environment contains processors network links 
assume selected master process hosted processor shown shaded circle 
configuration consider resource constraint values ww orker wnet nm 
constraint values determine kind performance bottlenecks expected 
result output deferral accumulation common scenario gdc environments occurs application perfor mance limited primarily capacity network link particularly link connects master process worker processes 
example environment consider case wnet limiting constraint 
table shows sets resource constraint values units second wups differ values network capacities wnet wnet 
set table consistent network link performance bottleneck 
think scenario example environment study performance bottlenecks 
resource value constraint wups ww orker ww orker ww orker ww orker ww orker ww orker ww orker wnet wnet wnet resource value constraint wups ww orker ww orker ww orker ww orker ww orker ww orker ww orker wnet wnet wnet table example constraint values bottlenecks lan type links 
set workstations connected lan configuration ethernet subnets 
apply flow analysis constraint values find rate wups delivered worker processes network link total worker computation rate wups available 
way reduce bottleneck characteristics modify application requirements way reduces resource constraint bottleneck location 
mw applications described application model defined chapter possible defer transmission results right unit computation completed units cycle computed 
eliminating large part data sent master process computation effectively increase capacity network links support additional unit flow computation phase cycle 
table shows new set constraint values allows wups available flow delivered worker processes master process 
action deferring result transfers significantly reduce execution times deferred data sent master process cycle 
mw applications results different unit computations accumulated combined ways result deferred result transfer size sum individual result transfer sizes 
examples applications search algorithms need return best results 
common technique reducing effects communication bottle aim achieving high degree overlap time spent workers computation time spent waiting communication 
general solution portable applications achieving effective overlap difficult practice amount computation communication carefully balanced successful 
environments dynamically varying load conditions high degree hetero resource capabilities achieve useful balance arbitrary combinations applications processors network resources open research question 
experiences attempting generally overlap communication computation techniques prefetching units needed worker pro cesses particularly successful conditions application performance highly communication bound quite difficult determine appropri ate amount overlap 
alternative attempting overlap communication computation separating primary computation communication activities dif ferent phases result deferral allows accurate analysis application behavior phase enables techniques reduce amount data trans accumulated results applications 
implementing result deferral requires application written support shifting result transfers cycle accumulating results compu tation 
table shows highlighted values network link capacity improvements network link constraints appear network links tech nique effective eliminating reducing number network related limitations application performance time 
widespread positive impacts achievable performance result deferral technique appears candidate consider designing developing mw application codes 
flow mw performance model gives way easily quantify performance benefits incorporating defer accumulate output technique dif ferent combinations application environmental parameters 
scenario evaluated identify network capacity performance bottleneck application inspected determine output deferral lation feasible productive reducing amount data transferred 
conditions met flow analysis performed reevaluate modified application characteristics predicted performance level 
area result deferral may complicate application scheduling area fault recovery 
workers independent model faults result worker processes failing easily handled simply reassigning processors computation outstanding units 
outstanding units previously sent failed worker processes results returned 
units routinely returned master soon computed non deferral case number outstanding units reduced time results transferred master 
result deferral complete computation cycle finished may outstanding units fault occurs computed results waiting cycle transferred master process 
fault recovery may require redone units remain outstanding process fails 
addition order prepared fault recovery additional information maintained unit status scheduler case output deferral accumulation 
occurs output deferral return results master signal computation completed unit units remain outstanding reporting computation finished 
output results accumulated multiple units output deferral case units state having completed computation having returned results back master process 
fault recovery requires scheduler track status outstanding units difficult output deferral case additional condition completed units may outstanding 
example environment smp machines connected lan type link 
utilizing available communication resources method reducing limitations network link capacities environments choices exist kinds communication resources different resources varying levels performance 
example kind environment seen cast example environment way smp machines connected ethernet lan connection link 
common communication packages pvm mpi develop ing message passing parallel programs typically support communication protocol time may optimized take advantage multiple communi cation resources 
example environment configuration shown assume network links protocol exhibit similar performance limitations shown values table 
sce correspond processors message unix socket communications tcp ip 
applying flow rate analysis resource constraint values shown tells application flow limited wups total wups available processor resources absence network constraints 
possibility improving application flow utilize higher band width available shared memory channels example smp environment 
values table illustrate selection faster communica resource value constraint wups ww orker ww orker ww orker ww orker ww orker ww orker ww orker wnet wnet wnet resource value constraint wups ww orker ww orker ww orker ww orker ww orker ww orker ww orker wnet wnet wnet table example constraint values bottlenecks heterogeneous links 
tion options benefit network related performance bottlenecks particular schedul ing ranges 
example additional network capacities afforded shared memory channels allow application achieve entire wups rate available 
order enable beneficial different communication approaches facility needed support simultaneous multiple communication resource types protocols 
developed portable communication package provide feature 
details communication package chapter 
hierarchical organization environments network capacity links relatively low available flow worker processes may highly constrained 
ex ample restrictive environment geographically separated workstation clusters linked wan connection high network latencies relatively low network bandwidths wan connection impose heavy constraint amount flow pass clusters 
general environment representation shown illustration scenario assigning resource constraints shown left side values table 
analysis mw flow performance model shows network link major performance bottleneck effectively prevents significant resource value constraint wups ww orker ww orker ww orker ww orker ww orker ww orker wnet wnet wnet resource value constraint wups ww orker ww orker ww orker ww orker ww orker ww orker wnet wnet wnet table example constraint values performance bottlenecks wan type link 
examples level hierarchical mw organization 
portion total available processor capacity utilized 
analysis gives effective application rate wups compared total available computational capacity wups 
case network constraints restrict amount flow able pass master process remote worker processes may distribute master process functionality place nearby remote workers 
think creating hierarchical organization master worker pro cesses just level organization single master process communicating worker processes 
concept example environment shown fig ure play role process communicates directly master process local worker processes 
hierarchical organization potential perfor mance benefits 
organization shown worker processes running processors communicate process running processor 
communication activities occur network link potentially ing load network link compared level arrangement flow traffic processors cross network link 
effect network link performance bottleneck reduced allowing master process aggregate messages go individual worker processes send aggregated messages process 
link latency major contributor restricted capacity lowering number round trip messages required effectively increase number units handled unit time 
table shows result reduced link constraints notice processor uses master processor capacity term capacity network link increased wups wups 
rerunning flow performance analysis new constraint values gives achievable application rate wups 
improvement original rate wups due additional wups supplied new process running processor 
general link constraints reduced effective network constraint master process process constraints occur master process group worker processes served process 
case link latencies high computation allows results accumulated reduce total size data transfers constraining link 
flow performance model resulting changes effective resource constraints analyze performance benefits derived implementing multi level hierarchical arrangements mw applications 
illustrate effectiveness hierarchical organization results mw application performance emulator 
emulator run environmental configuration included sun workstations located university california san diego ucsd sun workstations located university tennessee knoxville utk 
execution time mw application recorded size result data blocks varied range kbytes 
experiment mw configurations tested worker set machines ucsd arranged level configuration twelve worker set machines ucsd utk arranged level configuration eleven worker set machines ucsd utk arranged hierarchical master configuration represented 
different sets experimental measurements 
set run emulate application transferring data unit computation second set run emulate application transferred results computation cycle 
computation time emulator set level required approximately seconds single workstation ucsd compute units emulated application 
shows set results 
see graph hierarchical master organization environmental configuration performs better simple level organization worker processes connecting master process wan link stretching country 
level hierarchical configurations involving machines ucsd utk amount data transferred wan link remains cases application performance appears strongly bandwidth limited 
hierarchical organization performed slightly worse cases reflects additional overhead unit results incur having routed additional level processes reaching master process 
note result block sizes increased values larger kbytes configuration subset machines locally connected ucsd begins outperform 
relatively flat execution time graph local ucsd configuration confirms range result block sizes shown mbps bandwidth capacity local link creating bandwidth performance constraint 
conditions show clear distinction scheduling regimes hierarchical level scheduling strategies remote computing resources beneficial detrimental application performance 
shows second set results emulator running application defers sending results back computation cycle 
configuration hierarchical strategy allows continued performance benefits remote computing resources utk large range output block sizes superior level strategies values output block size larger kbytes kbytes 
output block sizes kbytes level strategy slightly faster 
scenario differs previous wan link bandwidth removed performance bottleneck majority computation wan link latency primary application performance bottleneck 
hierarchical strategy reduces effects latency performance allowing group transfers links reduced number accumulation ucsd ucsd ucsd ucsd ucsd utk utk utk utk utk utk utk utk hierarchical configuration experiment 
intermediate process 
reduction number transfers selected links wan link ucsd utk reduces effects bandwidth limitations return cycle result data shown different slopes execution time graphs level hierarchical strategy cases 
level strategy initial advantage retained cycle result sizes kbytes additional overhead due multiple levels 
results clearly show definite ranges techniques output result deferral accumulation hierarchical organization effective 
help tools flow performance model expect appropriate scheduling ranges strategy identified 
network congestion control experimentation mw application performance various kinds environments discovered potential performance problem appears specific certain shared access ethernet links 
problem manifests severe degradation achievable network capacity link subjected heavy application traffic multiple simultaneous data transfers 
heavy congestion observed data transfers worker processes master process applications sending large blocks data parallel 
shows experimental results demonstrating congestion problem 
scientific mw code named dot run single cluster sun workstations connected mbps ethernet link 
dot application produces relatively large sized result blocks mbytes results allowing accumulation cycle 
additional details dot application chapter 
shows effects application performance number worker processes increased 
shows test application execution time sec execution time sec execution time vs unit result block size ucsd workers ucsd utk workers ucsd utk tree application performance emulator unit result block size bytes return results unit execution time vs cycle output block size ucsd workers ucsd utk workers ucsd utk tree application performance emulator cycle output block size bytes return results cycle test hierarchical configuration 
execution time sec effects network congestion dot app ucsd ape workstations units workers workers unit cycle control result transfer method demonstration network congestion effects performance 
applying technique deferring results cycle accumulating results helped reduce performance problem completely successful performance lost network congestion 
experimentation revealed congestion phenomenon mani certain environments led discovery congestion worst ethernet lans hubs interconnect groups machines 
computing environments switched ethernet technology connect machines exhibit performance problems due congestion 
behavior suggests congestion problem may due timeout retransmission congestion control protocol specified tcp ip results longer retry periods packet transfers failed due collisions shared access link 
investigation cause congestion problem immediately suggested course action help solve 
packet collisions multiple simultaneous trans causes network congestion problems solution eliminate possibility simultaneous transmissions senders network link 
practice accomplished requiring sending process obtain authorization receiving process sending large data blocks 
enforce requirement building protocol message handling logic application scheduler controls sending receiving processes 
environment specific scheduling technique call congestion control proved successful greatly reducing network congestion problem seen third column results 
implementation congestion control calls receiving process assume control result data transfers take place ensure worker process time transmitting data network link 
technique appropriate combinations application characteristics environmental conditions congestion problem manifests 
identified mw applications large data block transfers running limited bandwidth networks employing tcp ip protocols 
right situations technique essential enable acceptable parallel computing performance 
scenarios network congestion serious problem applying congestion control technique reduce application performance reduces amount parallelism system serialization result data transfers 
congestion control example scheduling technique narrowly defined regime applicability 
non steady state conditions effecting mw scheduling scheduling techniques chapter rely perfor mance guidelines derived applying flow modeling techniques described previous chapter 
note combinations mw application char environmental conditions basic flow performance model assumes degree steady state application behavior adequately rep resent expected performance 
identify cases particularly true number units left process small relative number processes employed application communication times larger com putation times 
cases involve exceptions steady state assumptions flow performance model discuss greater detail 
load balancing example 
mw applications relatively units flow mw application performance model steady state model performance 
cases relatively units compute relative number processes doing load balancing issues dominant performance factors steady state model representative performance 
load balancing problem distributing computation communication evenly network single resource committed 
illustrates simple case load balancing predominant performance issue 
show gantt charts processes units processes perform computations twice fast processes chart units evenly distributed processes results total execution time time units 
second chart redistributing units place units faster processes results total execution time time units giving improvement case 
execution time performance metric say second case evenly balanced loads relative capacities processes yielding distribution finishing times 
difference time process finish computing average execution time dividing finishing time taken measure load imbalance 
simple example finishing time time units average finishing time time units diagram finishing time time units average finishing time time units second diagram 
measures load balance right diagram exhibits better load balance load imbalance measure versus load imbalance measure left diagram 
equations defining measure load imbalance equations tf time processor finished average finishing time processors pj finishing time particular processor pj 
tf max pj pj pj tf tf pj pj tf equation see load imbalance minimum value maximum finishing time average finishing time equal maximum asymptotic value ratio average finishing time maximum finishing time decreases 
intuitively see equation number processors fixed number units decreased average number units assigned processor decrease 
ratio units processors decreases contribution unit processor finishing time increases fraction total compute time leads greater potential levels load imbalance relative execution time 
analysis case number units held steady number processors increased performed leads identical result decreasing ratio units processors increases relative contribution individual unit processing times load imbalance 
consider impact computing single unit load imbalance 
note expected additional time process spends computing unit calculated inverses effective rates processes previously defined chapter 
term represents effective rate process master process terms units second 
term expression define expected additional time needed calculate unit worker process master process worst case increasing load imbalance new unit started processor just processors finished 
take tf finishing time new unit introduced give expression expected new maximum value load imbalance caused additional unit 
derive new imbalance value assume new finishing time equal original finishing time tf plus expected processing time pk unit computed processor pk 
new average processor finishing time avg replacing finishing time calculating original average finishing time new finishing time processor pk 
resulting expression equation derived plugging expressions avg defining load imbalance 
equation tf pk avg pi tf pk avg pi tf pk see worst case load imbalance values tend increase number processes goes magnitude time relative finishing time tf goes 
helps explain load imbalance problem important ratio units computing processes low 
load imbalance potentially significant factor application performance different scheduling techniques needed promote application performance 
equation suggests ratio pi expected finishing time tf application gauge potential impact load imbalance processor pi 
problems load balancing situations reduce likelihood especially bad decisions 
example fifth process added example took times long compute unit process effective scheduler assign units process single unit assigned result execution time expanding time units longer best scheduling result 
agrees earlier discussion value larger values additional time pi calculating units processes pi system 
solution reduce load imbalance assign proportionally units processes smaller pi values larger pi values 
effective rates pi known worker processes pi master process best unit distribution approximated expression shown equation percentage distrib pi units allocate process pi 
fractional units allocated slow processes formula reassigned fast processes 
distrib pi pi pj pj effective worker rates accurately estimated processes known list scheduling techniques applied load balancing problem 
note identify scheduling regime calls load balancing techniques analyzing known set application environment conditions 
mw applications dominant communication times flow model inadequately represents situation time needed communication unit data exceeds time needed perform com putation worker high ratio 
case additive property flow rates performance model may longer hold 
condition illustrated fig ure show time line activity workers master process handling result data transfers 
shows data transfer times flows represented bars labeled significantly longer times needed unit results computed represented bars labeled processing results different worker processes master serialized 
happens worker processes orker orker forced spend large amounts idle time wait master process handle result transfers completed units continue processing unit 
conditions single master process results round results worker process results round ready process process process master processing serialization example 
transport computation times shown worker master process 
received 
flow rates worker processes additive produce application rates effective rate proportional inverse total transfer time taken single round computations 
transfer times unit data tp flow worker process ideal application rate ideal equation expected application rate process serialization serial equation 
example shows set conditions worker process serialization occurs single process requiring particularly long data transfer times relative computation times significant negative impact application performance 
ideal tp serial tp time illustrate process serialization manifest ran mw application performance emulation program configuration process serialization easily observed 
performance emulation program generic mw application allows set run time number application parameters unit computation time data input output transfer sizes 
emulation program explore expected behavior applications gdc environments function varying application parameters 
trial ran emulated application master process worker processes 
master process worker processes running sun ultrasparc workstations networked mbps ethernet links second worker process ran ibm thinkpad laptop computer networked mbps wireless lan connection 
shows results running simple experiment computa tion time set values milliseconds result transfer size varied range values bytes 
results show transfer sizes exceed bytes clearly seen lower zoomed view results performance worker processes degraded worse single worker mbps ethernet link 
shows experiment run set machines computation time increased times larger example 
observe result decreasing communication time computation time ratio parallel performance worker processes remains better worker case values data transfer size bytes 
effects process serialization observed ways 
shows results running series performance emulation runs set hetero workstations connected ethernet lan ucsd 
runs emulator varied size unit output blocks bytes computation time set require approximately seconds worker processes process units 
upper graph shows execution time proportional output block size indicating network capacity primary performance constraint 
lower graph shows units distributed processors queue allocation policy units dynamically allocated worker processes time 
results show small block sizes units distributed proportionally capacities processor 
size output blocks increased corresponding increasingly long execution times shown upper graph amount allocated processor homogeneous 
set conditions process serialization occurs results master process performance bottleneck forms scheduling regime specific schedul ing actions called 
general process serialization performance efficient execution time sec execution time sec worker mbps ethernet worker mbps wireless workers combined data block size bytes worker mbps ethernet worker mbps wireless workers combined data block size bytes emulated worker performance high process serialization 
execution time sec execution time sec worker mbps ethernet worker mbps wireless workers combined data block size bytes worker mbps ethernet worker mbps wireless workers combined data block size bytes emulated worker performance reduced process serialization 
execution time sec blocks processed worker worker serialization test performance emulator block size bytes worker serialization test performance emulator block size bytes observable effects process serialization 
property parallel programs exhibit effective parallel performance reduced approximately rate flow slowest communication link 
best course action improve parallel performance reduce communication computation time ratio 
reducing ratio involve performing larger amounts computation unit decreasing amount data transferred unit im proving communicating performance links contributing slowest transfer times combinations 
discussed improvements techniques sections 
cases better performance schedule worker processes linked extremely slow interfaces computations involving workers linked significantly faster interfaces 
summary chapter covered scheduling activities mw applications running gdc environments flow model perspective developed chapter 
flow model mw application performance explains scheduling prob lems mw applications terms performance constraints imposed limited resource capabilities relative application requirements 
shown solutions classical scheduling problems master worker host selection alloca tion application specific scheduling solutions result output deferral accumulation hierarchical organization roots reducing ence identifiable resource related performance constraints 
flow performance model gives better opportunities identify sources primary performance bottlenecks quantify impact various techniques improving performance 
scheduling mw applications framed problem identifying ap inappropriate scheduling approaches employ combinations application requirements environmental characteristics 
specific sets conditions generally identified specific successful scheduling approaches form call scheduling regimes approaches 
attempted identify flow performance model contributes defining effective scheduling regimes scheduling approaches considered 
shown flow performance concepts effective wide range mw scheduling problems application configurations 
flow performance analysis provides valuable hints scheduling regimes operative different application environ mental combinations 
developed algorithms master worker resource selection gdc platforms flow performance model mw applications 
algo rithms enable selection processor resource host master process set processor resources host worker processes enables optimal application flow performance achieved 
application performance prediction problem resource selection addressed weissman zhao 
weissman zhao heuristics select number candidate configurations employ cost functions derive computation communication times configuration 
select configuration yielding lowest total cost 
approach resource selection efficiently evaluates application performance different configurations simple constraint calculations observable measurable parameter values 
lieu lowekamp looked automatically selecting pro cessor nodes applications running high speed networks 
results lieu lowekamp algorithms allow automatically select nodes different goals maximizing computation capacity maximizing communication capacity balancing computation communication 
explain correct goal selected match specific application characteristics order give op performance 
approach focuses determination bottlenecks computation communication constraints determines performance efficient configuration different scheduling ranges 
identified unit allocation scheduling function primary means applications adapt uncertainty inaccuracy acquiring dif ferent performance parameter values 
number researchers including hagerup kruskal weiss polychronopoulos kuck ni hummel schmidt uma wein lucco introduced individual alloca tion strategies shown analysis simulation produce benefits different sets assumptions conditions 
knowledge integrated effort show comparative effectiveness different techniques heterogeneous workloads realistic combinations applications environmental conditions 
particular effectiveness strategies heterogeneous sets resources analyzed experimentally tested fac strategy hummel nishikawa describes implementation general load balancing architecture heterogeneous tasks 
experimental study spring wolski compared hybrid static dynamic allocation policy purely dynamic policy sin gle gene sequence comparison application 
purely analytical studies including harchol balter weber winston performed assumptions workload characteristics easily translate able allocation strategies real world environments 
flow analysis able identify parameter uncertainty ef constraint terms related performance bottlenecks require special uncertainty tolerance strategies handle uncertainty effects parame ters part critical performance bottlenecks allow aggressive allocation strategies successfully employed 
level uncertainty inaccuracy parameter value assignment determines acceptable scheduling ranges different allocation strategies 
applied flow principles certain specific performance prob lems mw applications run gdc environments 
spe cial case shown solution involves reducing critical flow performance constraints 
methods changing effects performance effects include altering appli cation behavior making better available resource capabilities reorganizing flow data mw application hierarchical organization 
chapters details mw application developed implemented put scheduling techniques discussed chapter practice give experimental results real applica tions representative gdc environments show techniques action 
chapter developing master worker applications gdc environments chapter describes software infrastructure designed support develop ment master worker mw applications targeted general distributed computing gdc environments 
goal developing infrastructure improve application performance access scheduling techniques discussed chapter enable general portability mw applications wide range computing environments re duce effort required development performance oriented mw applications 
approach develop mw applications highly adaptable existing services resources possible creating entirely new services programming environments 
conflicts exist achieving program portability application perfor mance due large part desirability utilizing particular hardware software facilities features achieve performance specific circumstances rec facilities features may uniformly available appropriate 
example feature fast inter process communication shared memory reads writes 
feature available multi processor platforms available attain higher levels performance 
imple mentation shared memory features may available may de liver acceptable application performance applications running distributed memory architectures 
application written automatically shared memory running multi processor platforms fast message passing protocol running distributed memory multicomputer environment perform better heterogeneous conditions 
difficulties achieving flexibility arise normal semantics programming interfaces forms interprocess communication compatible require different implementations application take ad vantage 
general maximum application portability performance occurs applications appropriate hardware software features requiring specially written versions code handle case 
solution problem achieving application portability perfor mance detailed knowledge particular implementation features away application code middleware services 
middleware services intended provide stable consistent interface functionality needed ap plications allowing functionality delivered variety software hardware sources 
software components deliver improvements existing function ality additional functionality originally envisioned original software design fit established middleware interfaces available appli cations 
approach functional abstraction particular benefits key areas functionality expect heavily reused applications successfully component architecture approach software engineering 
middleware services bridge functional needs applications application schedulers abilities software hardware resources meet needs 
idea middleware ease complexity building software systems new evidenced specifications toolkits different approaches corba dce globus condor 
middleware successfully employed number large scale software projects mm cactus high throughput monte carlo simulation 
intend show middleware approach providing highly reusable services great value relatively small scale mw applications gdc environments 
shows general organization components form ap plication development infrastructure developed mw applications 
scheduling resources portable services application template infrastructure master worker computing boxes represent logical division software components developed sup port building mw applications meeting twin goals high performance portability gdc environments 
box labeled application template represents development simplified application framework specifies minimal set pro gramming interfaces application specific functions creating mw applications access mw specific scheduling capabilities 
box labeled scheduler represents implementation general scheduling services mw applications including support hierarchical worker arrangements flow control techniques created specif ically achieving improved mw application performance gdc environments 
box labeled portable services represents set middleware services define common interface set communication process management data management services required mw applications effectively operate gdc environments 
region labelled resources represents collection hardware resources software systems available ordinary application gdc environment 
partitioning software functionality components directly supports application programming model discuss section 
application specific functions identified programming model formally specified application template component 
defined application specific function calls scheduling functions scheduling component exercise control mw ap plication computation 
shown chapter developed mw specific scheduling techniques suitable coordinating activities mw ap plications 
implementations scheduling functions scheduler component 
functions application template component scheduler component form complete implementation mw application 
functions application template scheduler components perform inter process com munication message passing protocols 
instance application runs gdc environment vary widely numbers kinds hardware software resources available 
functions portable services component intended form meta environment applications developed application template scheduler components allowing applications targeted single common providing access different hardware software services available spectrum gdc environments 
software components discussed greater detail sections 
developing software components implement middleware approach mw application development emphasize achieving certain goals specific areas 

performance 
applications dynamic resource capabilities contribute increased performance able employ appropriate scheduling strategies leverage specific characteristics variety re source conditions 

portability 
applications need runnable wide variety different hardware software platforms 
harnessing large computational potential resources gdc environments works best applications targeted kinds resources able new resources execution available 

reasonable effort 
applications designed gdc environments re quire significantly greater implementation effort type applications written serial programs 
efforts achieve parallel performance portability different platforms contribute significantly application developers 
programming model master worker applications help accomplish goals developed programming model mw applications isolates application specific functions mw application details accessing system services performing application scheduling 
model develop portable functional modules providing general system services application scheduling modules class mw applications 
mw programming model shown implements mw ap plication model section chapter 
model assume computation divided independent units processing units occurs cycles cycle input data transferred worker processes results computed unit output data transferred worker processes 
simple model useful capturing essential elements computations implemented mw programs 
shown chapter applications conforming mw application model evaluated flow performance model performance simulator developed mw computations 
observe elements programming model shown separated distinct groups base transfer control 
base group containing initialization compute finalization statements performs basic computational activities application 
transfer group containing data transfer statements represents application interface processes distributed application 
control group containing loop control statements represents control activities relevant scheduling distributed mw application 
support mw programming model divided pro gramming groups components created application developers application components common applications model master processes perform computations units performed worker processes 
straightforward extend model cover case master process allowed perform computations 
initialization computation cycles transfer cycle input data units transfer input data compute output data transfer output data transfer cycle output data finalization application programming model master worker computations 
provided independently 
base transfer groups comprising basic compu tation data transfer elements contains code specific mw application supplied application developers 
form application developer supplied elements implemented fixed set functions pre defined function headers 
set developer supplied function headers forms mw application template 
implementing functions base group little different programming similar functions conventional serial program 
cases existing serial code group directly ported mw applications minor changes 
bulk additional coding effort application developers come creating instances data transfer functions transfer group 
research focus mw applications message passing semantics performing data transfers processes 
frequency sizes types data transfers specific application 
control group comprising application control elements contains var ious scheduling functions applied mw applications manage way distributed results returned distributed processes 
form scheduling functions defined fixed set functions predefined func tion headers 
details kinds scheduling activities implemented functions third group chapter number scheduling techniques distribution reduction performance bottlenecks 
programming interface scheduler allows reusable scheduling com ponent mw applications designed implemented developers specializing scheduling issues 
part research developed implementation mw application scheduling component 
discuss elements programming model greater detail sections 
application template developed apples master worker application template amwat addresses problems developing mw applications achieve portable perfor mance gdc environments 
programming model outlined section define set application action functions developer implement instantiate create functional mw application 
amwat eases job application development eliminating need developers worry scheduling resource specific issues allowing concentrate application specific details 
shows form base functions defined amwat 
amwat programming interface specifies high level functionality application developer minimally supply create mw application 
base functions shown similar function complexity routines needed serial programs data transfer functions shown form bulk additional programming needed produce working parallel versions mw application 
example amwat example amwat programming interface consider problem parallelizing simple serial computation shown 
computation calculates vector containing sum integer values weighted index ranging 
observe lines function named sumarray called loop iterations 
consider iteration unit single cycle computation devise mw style application perform equivalent computation distributes done iteration different worker processes 
int app initialize int argc char argv int 
int app unsigned int int int 
app compute int unit 
app int int 
app int int 
int app finalize int 
const char app switches void amwat programming interface base functions 

int app int int 
int app int int sendcount 
int app int int recvcount 
void app int int sendcount 
void app int 
void app int 
void app int 
void app int amwat programming interface data transfer functions 
completion 
lines show initialization elements element float array values 
amwat implementation initialization setting number iterations compute performed master process part app initialize function things done computations take place 
initialized values master process array distributed worker processes implemented app app interface defined amwat 
computation performed line implemented app compute interface amwat results placed result array worker returned master process app app interfaces 
computed results received final output results shown lines implemented app finalize interface 
building implementation application amwat requires creat ing necessary functions perform application specific actions required data trans fer functions support remote computation worker processes 
including addi tional base functions app app app total amwat functions implemented example application 
appendix shows listing containing complete source code implementation example computation amwat interface 
resulting implementation completed linking set libraries providing scheduling system services form run time environment application 
scheduler services discussed section system services discussed section 
contrast amwat implementation shown appendix version computation developed common communications library called pvm shown appendix 
pvm example slightly simplified version ex ample code included standard pvm distribution uses source files defining separate master worker process implementations 
pvm version application scheduling activities spawning worker processes distribution units worker processes explicitly coded code implemen tation 
amwat approach scheduling services spawning worker processes distributing units worker processes provided 
way amwat constitutes user level middleware mw applications targeting gdc environments 
float sumarray int int float data int float sum sum data return sum int main int argc char argv int float data float result data result sumarray data printf result result example serial computation parallelized mw computation 
performance statistics extension example flexibility enabled amwat user level middleware developed extension module generating statistics application run time enabled application performance monitoring tuning activities mw application developed amwat 
extension module provides optional performance monitoring capabilities include logging data transfer statistics number data sends receives performed total number bytes transferred direction processor utilization statistics wall clock time actual cpu time spent worker process startup performing useful computations 
performance statistic extension module demonstrates possible add functionality amwat target application having changes application specific functions 
extension module operates wrapping amwat application function calls second layer function calls implement statistics generating func tions 
making function call interface statistics generating functions application function calls wrapping statistics generating module retains api characteristics amwat extensions 
allows scheduling module function exactly manner extension module enabled 
extended application run performance statistics generator enabled information data transfer occurrences sizes direc tion recorded 
addition performance statistics module sets process timers monitor amount time worker process active performing computation recording number units processed worker 
computation results monitoring automatically returned back master process possible analysis display 
implementation performance statistics module introduces relatively little overhead mw applications 
functions gathering statistics commu nication built communication module amwat applications allows statistics number messages handled sizes message blocks transferred process simply accumulated read back needed 
module opportunity keep track communication statistics implementation provides separate sets statistics application scheduler statistics extension modules amwat 
statistics performance individual worker processes gathered process invoking system timer calls returned master process results computed unit 
return pro cess performance data adds sixteen bytes unit result transmission 
shows example recorded performance statistics mandelbrot image generation application involving single master process worker processes 
table gives brief explanation meaning entry 
checking process performance statistics example shows worker pro cess spending seconds performing computations application required seconds complete execution 
inefficient processor time workers indicative application performance limited processor capacity factors network capacity 
distribution worker processing times shows load imbalance significant factor limiting instance 
checking recorded communication statistics master process shows master process received slightly megabytes data total computation time seconds 
dividing total amount data transferred execution time gives estimate effective bandwidth application approximately mbits second corresponds closely kind network performance expected mbps ethernet technology connecting master host resources 
information performance statistics determine particular instance mandelbrot application performance limited capacity network link master process 
performance statistics helps acquire necessary parameters applying flow performance model specific combinations applications environments 
example determine number units processed summing value blks processor average unit computation time individual processor dividing value user value blks average size data transfers unit dividing sum values send kbytes recv kbytes total number units 
performance analyses enabled availability application run time statistics advantage life cycle mw application 
performed development stage potential algorithmic organizational changes master worker process functions improve performance tested communication statistics send send recv recv module msgs kbytes msgs kbytes application extension scheduler total application time performance process total total hostname wall user wall user util blks ucsd edu ucsd edu ucsd edu ucsd edu ucsd edu ucsd edu example monitored application performance statistics 
communication statistics statistic name description send msgs number messages sent master process send kbytes total size messages sent master process recv msgs number messages received master process recv kbytes total size messages received master process process statistics statistic name description total wall total wall clock time elapsed worker process total user total cpu time expended worker process wall wall clock time elapsed worker computing user cpu time expended worker process computing util ratio user wall times blks number units processed worker process table statistics returned performance statistics module 
communication statistics separately application specific performance statistics scheduler components cumulative totals 
process statistics worker processes involved computation 
measured effectiveness 
simple example mandelbrot program shown performance effects quickly observed employing techniques reduce network bottleneck choosing different location master process reducing total amount network traffic 
production stage preliminary trials performance problems inefficient processor resources significant load imbalances encountered execution identified effectiveness run time adjustments solve problems measured 
example shown worker processes waste resources additional processors result reduced execution time long major performance bottleneck network capacity 
scheduler second amwat software component discuss scheduler module shown 
mw specific scheduling functions scheduling module transparently manage control activities process coordination allocation application distributed set processes performing mw computations 
primary responsibility scheduler operations coordinate execution int schedule init int schedule finalize void int schedule start void int schedule control void const char schedule switches void programming interface scheduler module 
application specific functions shown figures scheduler operations discussed chapter processes involved computation 
man agement remote processes performed message passing communication sig synchronization data transfers needed ensure efficient reliable distributed program execution 
addition basic mw scheduling distributed opera tions developed specialized scheduling capabilities hierarchical master worker process organization explicit congestion control intended im prove performance mw applications running particular gdc sub environments 
specialized techniques discussed section incorporated implementation scheduler module help demonstrate benefits extensible scheduling approach 
modules application development infrastructure interactions scheduler components performed defined pro gramming interfaces 
allow easy modifications scheduler functionality complete replacement scheduler module alternative implementation impacting operation components 
shows program ming interface accessing scheduler functions 
important functions schedule control function responsible driving application execution calls interface functions application template module 
scheduler operations utilize outside resources services communication performed calling interface functions portable services module described sec tion details scheduler activities subsections 
application process management mw applications developed software development infrastructure con tain kinds processes single master process number worker processes potentially number processes 
single common executable processes scheduler determines type process run master worker varies behavior match desired mode 
coordinating ap plication operations invoked amwat interface functions scheduler determines possible modes application process executing serial master worker 
serial mode invoked process running single serial application modes processes parallel application 
parallel application includes exactly master process distribute units receive back results worker processes perform unit computations optional processes act intermediate nodes master worker processes 
process mode responsible different aspects distributed mw computation different sets application specific functions invoked processes mode 
table shows application specific functions invoked process running process modes 
message handling communication amwat takes place messages sent processes 
messages help scheduler maintain synchronization remote pro cesses application running providing means transfer data inputs outputs processes required course performing com putation 
job scheduler ensure messages properly received routed appropriate functions processing updating process status changes result processing 
message handling requirements influenced mode process running 
case single process running serial mode interprocess communication requirement message handling 
processes running worker mode scheduling functions handle messaging remote process running master mode 
processes running master mode message management task complex schedulers modes handle sending receiving messages serial master app initialize app initialize app app app compute app app app app app app finalize app app app app finalize worker app initialize app initialize app app app app app app app app compute app app app app app app app app app app finalize app app app finalize table application function calls supported process modes 
process types master worker 
amwat scheduler module designed automatically handle messages running process modes 
amwat processes organized tree single master process root worker processes ends branch 
branch worker process single master process may zero processes 
communication flows branches means individual worker processes communicate directly 
think processes directly descended process tree children process directly precedes process tree parent 
worker processes communicate parent master process 
scheduler worker process checks messages coming parent sends output messages parent 
master processes may communicate different children worker processes 
master process schedulers scan incoming messages children allow sending messages children 
processes act communicating bridges parent children behaving master process communicating children behaving worker process communicating parent 
remote process management addition managing local process operations amwat scheduler module contains routines needed master processes perform operations related working remote processes 
operations include spawning new pro cesses platforms adapting arrival new process trying join existing computation recovering loss active process computation 
process spawning performed scheduler function application started 
master process initiated invoking executable command prompt similar serial program 
master process automatically spawn child processes needed computation platforms services provided portable services module 
configuration information master process passed child processes allow additional spawns required child processes done automatically allowing application deploy gdc environment resources allow immediate startup application processes 
gdc environments include processing resources allow processes immediately started require processes placed job queue wait available running time 
case high performance platforms facilities san diego supercomputer center sdsc national center supercomputer applications ncsa 
addition supporting normal case processes parallel computation started time job request successfully passes job queue scheduler supports case running computation joined additional processes 
means application developed amwat started machines job queues interactive workstations include processor resources computation transit job queue available 
order accomplish scheduler implementation perform operations including new process registration synchronization incoming processes existing computation new allocation additional processes 
noted allowing additional processes join running computa tion capability normally allowed applications mpi communi cation services normally employed applications pvm communi cation services 
example allowing additional processor resources incorporated dynamically running computation pvm pruyne livny describing application resource management system master worker application toolkit built top condor 
amwat differs combination condor worker processes outside single pvm virtual machine condor pool allowed join running mw application 
changing resource availability conditions gdc environments amwat scheduler implementation includes support automatic redistribution graceful recovery losses processes master process due soft ware hardware problems 
computation fail continue making progress master process disabled allowing greater reliability computations performed relatively unreliable resources 
fault recovery mechanism takes ad vantage independence unit processing allow graceful recovery failure worker process 
failure worker process detected aid secondary process monitoring system called apples process tracker apt 
apt works establishing independent monitoring process machine worker process spawned function periodically check see worker process active machine 
monitoring process detects worker process longer running notification message sent centralized apt server process 
worker process status obtained process querying apt server 
fault recovery amwat applications works ensuring dependently computable unit eventually processed results returned master process 
accomplished keeping list units assigned worker master process detection worker unable continue functioning computation adding units previously allocated failed worker process back general pool units available assigned workers 
way long single worker remains perform computation units progress computation continue 
slight difference way units handled depending results associated individual units results derived accumulation computations performed worker 
case independent results units removed master list soon results returned 
means units assigned worker returned results reassigned case failure worker 
case accumulated results scheduler keep track units assigned worker process computation cycle restart processors detection failure units contribute accumulated result 
implementation scheduler handles cases 
hierarchical organization enhanced scheduling function added support multi level hierarchical organization master worker processes 
illustrate show typical single level topology processors multi level topology processors 
single level topology shown left single master process potential performance bottleneck especially number worker processes increases 
model mw computing master responsible sending required data inputs allocating workers handling output data single level multi level hierarchical mw organization 
returned 
load master reaches point master unable promptly service worker requests workers may forced spend time waiting resulting reduced application efficiency scalable performance 
multi level configuration right side represents way overcome potentially serious bottlenecks single master process introducing processes distribute master process 
arrangement number processes serviced master kept reasonable number adding levels hierarchical tree number worker processes scaled larger numbers 
determination processes organized multi level hierarchies extension resource selection algorithms discussed earlier sections single level worker processes 
approach organizing multiple levels recursive process clustering groups processors supernodes mw tree selecting supernode selection algorithms chapter 
supernode takes form tree rooted performance supernodes terms flow determined performance model described earlier chapter individual processors substituting aggregated performance parameters supernode 
way amwat scheduler supports clustering computational power automatically weighting allocations branch mw process tree values relative combined flow capacities resources branch 
example allocation transfers master process process aggregates policy name description fixed time static allocation ss demand allocation unit time fsc fixed size allocations gss variable sized proportional remaining units processors tss variable sized proportional formula value fac variable sized allocate half units round table implemented distribution policies 
intended worker processes 
aggregation reduce number individual transfers need performed master process computation 
flexible distribution policies enhancement added basic mw scheduling functions flexible set built allocation policies support adaptive selection allocation strategies 
shown chapter different allocation approaches perform differently conditions strategies clearly preferable 
means flexibility selecting allocation strategy provide benefits gdc environments resource conditions highly variable 
amwat scheduler module supports different allocation policies described chapter allowing method distribution selected predicted target environment computation 
table shows policies implemented 
distributing remote worker processes balance mini time spent overhead operations need adaptive changing conditions 
time fixed allocations fixed policy require amount time management overhead data transfers perform distribu tion lead severe processor load imbalances initial allocation decisions inaccurate information 
allocation policies divide smaller chunks distribute computation progresses sive changing conditions incur higher costs time spent overhead data transfers 
general costs increase size chunks smaller number increases 
self scheduling ss fixed size chunking fsc policies call chunks remain size computation ss minimum chunk size unit allocation 
remaining policies variable chunk size decreases size computation progresses fewer units remain distributed 
variable sized chunk policies differ formula determine chunk size allocated 
details formulas policy chapter cited policy 
selection distribution currently done prior program execution user scheduler input parameter 
development automatic mechanism selecting appropriate distribution policy remains objective discussed fully chapter 
portable services third software component discuss portable services module 
module provides portable extensible access hardware software resources components 
develop efficient high performance scheduling ap proaches mw applications identified group common application service types communication information management process management con required achieving performance objectives specific instances services vary widely gdc environment 
providing services hardware software resources gdc environments may leveraged 
supply amwat portable services functionality developed apples portable services aps components provide common interfaces communication information management process management services shown 
set common interfaces forms abstraction layer isolates development applications amwat concerns service availability allows applica tions take advantage services available different environments 
discuss aps components greater detail subsections 
portable services communication information process aps comm aps info aps proc aps components communication services aps parallel programs designed run distributed memory systems tradi developed library specific communication programming protocols pvm mpi unix sockets perform distributed program control applica tion data transfers message passing 
approaches works different configurations hardware software fully satisfies portability performance objectives anticipated gdc scenarios 
addition inclusion programming interfaces may involve significant amount low level programming effort 
employed developers highly ex proficient low level approaches lead increased development time greater chances programming errors introduced 
address problems developed aps communication referred simply aps comm remainder chapter component provides single message passing inter process communication api built top common communication approaches today shown described 
common communication approaches goal aps comm provide amwat applications ability lever age multiple simultaneously active communication protocols restricted single uniform protocol 
section describe multi protocol approach aps comm 
section describe existing single development aps comm component joint jim hayes university california san diego sockets aps comm mpi pvm ipc aps communication component supported interfaces protocol communication approaches 
common approaches parallel program communication listed 

mpi message passing interface standard performing message passing communication jointly developed committee representing vendors implementors members user community 
mpi widely supported vendors parallel machines workstation clusters preferred method accessing special performance features vendor specific hardware 
cause standard requires processors remain available entire time parallel mpi program running provides support adding ad ditional processors computation begun mpi suited environments resource availability dynamic unreliable 
mpi assumes processor mpi computation able communicate processor computation 
mpi defines large feature rich set functions programming interface 

pvm parallel virtual machine message passing communications pack age quite popular parallel programming research groups world 
features include availability large variety platforms explicit support heterogeneous architectures support dynamic process spawning 
compared mpi pvm superior tolerance dynamic processor availability conditions 
critical processor pvm group removed impairing availability processors group 
package ported wide variety hardware platforms specification written updated version mpi called mpi provides additional functionality overcome limitations 
time implementations full mpi specification released general development 
enjoy level hardware support vendors high performance machines may take full advantage vendor specific features max performance machines 
mpi pvm assumes processor pvm computation able communicate processor computation 
similar mpi pvm defines large feature rich set functions programming interface 

unix sockets highly portable way program interprocess communication internet networks 
socket facilities modern operating systems connection oriented connectionless operations supported 
basic programming interface small set low level func tions powerful suited casual inexperienced programmers 
forming complete communication system low level functions employed building blocks systems mpi pvm built 

ipc system inter process communication services collection services interprocess communication include programming support semaphores shared memory message queues 
services appro priate programming communication activities processes running multiprocessor platforms suitable programming communication different distributed memory machines 
programming interface ipc services consists small set low level functions similar sockets interface suited casual inexperienced programmers 
case socket functions ipc functions employed building blocks constructing higher level services 
addition approaches listed methods available point experienced limited 
number vendors cessor computers provide proprietary communication libraries access specialized net working hardware vendor specific features 
nexus communication package developed research group working developing globus 
nexus low level approach originally intended target compilers people writing nexus code 
nexus uses communication paradigm different existing message passing approaches multi threaded mechanisms decouple specification communication destination specification thread control responds communication concepts global pointer gp specify communication destination remote service request rsr specify communication action taken 
globus project added globus library provide integrated interface file stream datagram functions application input output communication functions 
globus library targets broad range application communication activities similar nexus primarily intended support multi threaded applications 
number object oriented systems adopted forms remote procedure method calls forms interprocess communication including java rmi corba 
forms inter process communication employed allow communication object components currently proven deliver acceptable levels performance high performance applications 
aps comm services aps comm component approach providing adaptive multi protocol communication services applications 
aps comm api implemented wrapper existing mechanisms enable communication services performed simul sets different service providers 
aps comm defines api contains limited number high level functions deliver communication ser vices 
api designed easy powerful fully express range data transfer operations majority message passing parallel programs 
shows language programming interface aps comm services 
group api functions shown support functions initial izing shutting communication services supporting communication specific command line switch handling acquiring communication related statistics 
group functions support message addressing handled iden 
actual control communication activities handled group calls set non blocking send functions set blocking receive functions non blocking test message function 
aps comm component uses addressing approach conceptually different existing communication services 
particular aps comm com ponent uses identifiers message addresses locally mapped message sources destinations 
identifiers occurring process context meaning single process 
contrast communication identifier pvm mpi globally consistent mapping processes running environ ment 
assumption systems globally consistent identifier mapping processes environment communicate processes located environment 
local mapping enables organization communicating processes ways possible universally addressable property maintained 
example benefits having locally mapped identifiers mw scheduler supports hierarchical arrangement master worker processes forming tree topology 
ability support communication organizations processes address processes branch tree complicated arrangements involving resources universally accessible firewalls private networks constructed 
aps comm functions dedicated sending receiving data api provides functions sending variety different data types 
data types supported correspond standard language data types char float double int short long un signed int unsigned short unsigned long 
non blocking test function included asynchronously check messages matching address message type received allows non blocking receive functionality expressed explicit blocking receive function 
support simple set transfer functions aps comm interface meets general message passing requirements mw applications enables portable mw applications implemented interface 
implementing interface top existing communication services ipc pvm mpi aps comm enables specialized performance features different gdc environments mw applications providing highly portable implementation unix sockets times services available 
addition communication services nexus easily supported aps comm interface aps comm functionality extended demand additional communication services calls 
allowing multiple communication mechanisms services operate simul aps comm component provides feature approaches support manner requires additional developer workload 
shown section previous chapter utilizing best communication resource int comm init void int comm finalize void const char comm switches void void comm int protocols comm myid void const char comm id comm const char image comm void int comm sendtype const type int int comm const type int int comm double programming interface aps comm 
data transfers worker processes master processes mw applications significantly boost performance specific conditions 
aps comm support multiple services flexible supported communication services enabled needed run time 
implementation application may different configurations communication services different environments need recompiling relinking 
ability multiple protocols especially useful modern cluster architectures contain hierarchies networking mechanisms includ ing shared memory pathways intra node communication switch pathways inter node communication 
information management services aps application schedulers require access wide range information accurate scheduling decisions 
seen chapter information needed scheduling decisions job resource selection includes detailed knowledge resource application characteristics additional data specifying types characteristics interact 
example ap plication scheduler expect receive inputs performing resource selection mw application relative processor performance performing par ticular computation current processor availability current available network bandwidth average size result data transfers total number results calculated 
obtaining values kinds inputs gdc environments may involve different sources information may parameterize performance models scheduling 
problem attempting develop applications portable different environments form information provider may need queried vary environment 
seek provide simplified programming interfaces enable easy ration capabilities existing local resources global information service providers 
basic unit information information entry form borrowed globus metacomputing directory service mds lightweight directory service ldap 
information entries logically related sets names associated values grouped form basic units informa tion 
values may ies complex information relationships easily expressed hierarchical entries 
information represented form compat ible way information represented ldap mds information obtained ldap mds sources easily integrated approach 
information visually represented simple lists name value pairs expression machine machine sojourner proc pentium clock memory 
approach differs ldap mds specify fixed manner organizing individual entries directory information tree dit 
difference stems intent provide information service integrates access variety different sources application level ldap mds enable access information collected organized remote server level 
leaving process data organization open handling process application level seek maximize application flexibility accessing information needed making intelligent scheduling decisions 
defined aps information management referred simply aps info remainder chapter component provides simple access wide variety information services common api 
current implementation cludes support static dynamic information sources illustrated designed expanded needed new sources available 
useful capability aps info component built support meta data attributes de scribe additional characteristics information delivered different sources 
meta data available existing data sources globus metacomputing directory service mds storage resource broker srb network weather service nws provide additional qualitative attributes information results obtained sources 
attributes tag data provided aps info api descriptors specifying additional data characteristics including information concerning data source indications predicted accuracy estimates stability time 
aps info api provides useful abstraction layer information differ ent sources emphasis supporting variety local global information sources interface 
important gdc environments infor mation related resources available single source information service provider 
example globus mds probably comes closest general purpose source information expected provide information lookup update functions concerning resources sites globus installed 
infor mation accessible mw applications globus resources 
gdc resources globus installed including small networks workstations personal computing platforms entries mds 
local information sources private databases information files may mechanisms available logging accessing information resources aps info access various sources practical providing common interface 
information providers limited scope information provided serve general solutions information providers 
srb aps info nws applic files aps information component supported interfaces interface provides flexible access information attributes queries targeted primarily servicing requests concerning large data repositories 
nws set information handling mechanisms narrowly focused providing information resources monitored nws system 
information provided services valuable activities scheduling mw applications incorporating different approaches access information attractive option application developers 
aps info api attempt provide bridge different information sources allowing application developers minimize multiple information query methods 
aps info api particularly simple comprising function calls creating manipulating ies function calls accessing information services shown 
despite simplicity api provides flexibility ap plications able interact various information service providers 
ies aps info services provides support meta information property fields 
examples meta information include source returned data estimate predicted accuracy data time data recorded 
meta information incorporated advanced scheduling techniques allow better response variable conditions missing incomplete knowledge 
example need flexible mechanisms acquire information needed application schedulers consider scheduler needing know relative processing power available particular machine order calculate merits incorporating machine computation 
assume information service providers available information benchmark results combinations applications machines 
information stored number different properties char properties string properties properties prop int properties prop int properties prop char name char value int properties prop char name properties branch int properties prop char int type char properties prop char name properties properties prop char name properties info get properties query int info put properties destination properties source programming interface aps info 
formats ranging complex relational databases simple text files scheduler concerned implementation details 
pseudo code series inquiries find desired information shown 
queries start specific forms continue decreasing specificity expanding effort discover useful answer question 
obvious simple example implementing queries different programming interfaces possible information source quickly awkward impractical 
aps info programming interface example handled straightforward manner 
series queries set set properties shown 
responses queries successful specified name value pairings match exactly query returned result 
instance query obtain successful response information provider contained data application benchmark run test application machine thing 
question marks appearing values query field indicate values app benchmark machine return app benchmark value app benchmark machine model return app benchmark value app benchmark processor type scale app benchmark return scaled benchmark value general benchmark machine return general benchmark value general benchmark machine model return general benchmark value general benchmark processor type scale general benchmark return scaled benchmark value return unknown example information search expansion 
returned successful response 
means successful response query return minimum name value pairs source information result benchmark 
aps info interface developer worry information sources available properly set queries 
query submitted single function call available service providers automatically queried determine supply response affirmative responses returned back caller 
source field included example queries illustrate way handle multiple information sources able satisfy query 
example queries shown requester multiple answers decide accept name provider supplied information 
aps info service currently delivers flexibility support different forms information sources 
single api provides access traditional sources static information applications including shell environment variables initialization files relational databases 
addition provides access dynamic sources information nws information furnished directly application run time 
currently underway incorporate information sources source app test machine thing source app test model ultra source app test cpu ultrasparc clock 
source mflops app test machine thing source mflops app test model ultra source mflops app test cpu ultrasparc clock 
example ies querying 
application development systems including globus mds 
process management services aps running distributed parallel applications gdc environments means mech available start processes monitor state processes running remote resources 
different mechanisms available accomplish process management activities vary depending specific policies practices administrators manage individual resources 
order take fullest advantage wide range available gdc resources applications prepared utilize number different process related mechanisms various sites 
keeping previously stated portability goals seek provide access services simplified programming interfaces encompass easily extensible set essential service functions 
developed aps process management referred simply aps proc remainder chapter component provide uniform interface process management functions gdc application developers 
illustrates aps proc functions developed 
module supports multiple methods interactively starting new application processes 
solution problem providing process management services applica tion developers follows philosophy enabling access available service providers diego development aps proc joint jim hayes university california san ssh fork startup aps proc queue pvm status apt aps process management component supported interfaces int proc spawn const char protocol const char machine const char program const char argv const char environment unsigned int processes ids programming interface process startup 
simplified functional interfaces 
aps proc module contains interfaces areas process management process startup process status monitoring 
process startup service provides single interface function call allows new processes started locally remotely available proto cols 
programming interface shown 
currently implemented protocols fork exec calls local processes secure remote login invocation ssh pvm specific task invocation submission jobs batch queueing system 
planning underway add process invocation globus resource allocation manager gram interface enables access re sources globus installed 
architecturally monitoring service involves separate monitoring pro cess report periodically status monitored process 
monitoring process started processor computation time worker process spawned 
monitoring process performs periodic scans processor active process list determine application process monitored running 
change status monitored process detected report automatically dispatched central process monitoring server 
information status specific application processes obtained application scheduler api service set query update watch functions shown fig ure 
queries sent central process monitoring server results returned application scheduler making request 
process status service developed maximum portability minimum intrusiveness systems monitored 
process monitor simply implemented small perl script active short times performing check process status 
existing alternatives approach include established services nws globus heartbeat monitor perform process monitoring functions 
problem nws process monitoring current im plementation targeted monitoring resource behavior process behavior nws currently suitable sensor implemented monitoring state single process 
globus processes perform function similar process status monitor system typically relies having processes running processor computation begins requires process monitored register local process 
systems globus installed process monitoring facility available 
technical reason processes accessed process status api interfacing globus service may pursued 
summary chapter done create software ture overcome problems areas computing gdc environments perfor mance portability ease development 
issues addressed implementa tions application template scheduler portable services modules 
performance issues addressed capabilities incorporated scheduler module provide effective scheduling operations entire class mw applications fitting proc const char name unsigned short port void proc connect monitor proc server const char pid pid unsigned int update void proc monitor monitor int proc query server const char info ids int proc update server info int proc server const char info int proc server info int proc server const char programming interface process status 
programming model defined section 
capabilities include variety func tions basic process management message handling remote process handling support hierarchical organization flexible allocation policies 
amwat structure eases development mw application code written different languages including fortran 
expand number available target resources amwat libraries support resource related services developed portability top priority 
applications utilizing amwat support libraries implemented utilize wide array resources available gdc environments 
addition modular architecture amwat support libraries allows capabilities new resources easily adopted available 
efforts reduce development efforts parallel mw applications follows lines earlier efforts cilk develop structured framework develop ing certain classes applications 
cilk initially targeted allow easier development distributed applications take advantage shared memory communication efficient process coordination data transfer operations 
cilk particularly suited queue applications achieving provably optimal performance 
approach differs primarily targeting wider range resources envi ronments particularly distributed memory architectures wider range scheduling approaches meet needs scenarios 
research effort closely related mw software framework mw applications 
framework designed facilitate development mw applications requiring reliable delivery large amounts compu tational capacity 
similar approach mw framework provides middleware layer specific application implementation layer condor 
approaches differ heavily emphasis place application performance terms execu tion time mw emphasizes delivery high throughput computing applications require massive amounts computation 
range target amwat applications environments includes smaller jobs requiring processors larger jobs take advantage large clusters machines 
driven addressing performance oriented issues attempting handle wider range scheduling problems mw meet different challenges posed scaling performance small large systems maintaining maximum portability 
portability complexity issues addressed approach em middleware approach architecting software components 
idea developing middleware services allow higher levels functional abstraction simplify software development efforts established different areas 
especially research community working developing tools methodologies computing gdc environments need application middleware discussed 
research projects aimed developing middleware specific types distributed applications cactus project started concentrating applications involving solutions einstein highly coupled nonlinear hyperbolic elliptical partial differential gravitational wave equa tions dimensions 
nimrod apples parameter sweep template apst middleware projects targeted large scale parameter sweep appli cations running computational grid environments 
projects aimed developing middleware specialized environments globe project looking web oriented distributed object applications 
current research efforts globus legion con dor 
focused developing middleware components meant provide general purpose environments distributed computing emphasized different issues working achieve goals 
approach differs efforts primarily efforts focused developing applications perform virtually environment projects aim develop environments attractive application developers benefits systems provide 
globus legion aim define standard metacomputing programming environments consistent interfaces supported system services intending ease development applications encourage wider array distributed resources 
globus concentrated developing services allow users applications secure re liable standardized access wide array high performance computing resources 
legion focused developing sophisticated object software infrastructure creating managing applications entire systems encompassing large numbers objects 
legion term object includes entities system hardware software 
legion primarily targeted clusters workstations 
condor concentrated delivering extremely high throughput applications harnessing idle cycles large numbers workstations 
accomplish condor devel oped significant technology allow computations progress environments resource availability highly unpredictable 
unique services condor provides process checkpointing process migration transparent redirection 
characteristic services provided environments globus legion typically employs single option service providers accessed standard interfaces 
single service provider specific particular environment interface globus provides remote data lookup facility called mds 
mds interface useful environments mds service provider installed 
accomplish goals allowing mw applications run widest range environments minimum preconditions define standard interfaces certain essential services communication information retrieval allow multiple service providers opportunity fulfill service requests 
base im plementation rely resource services available basic components socket communication ssh remote logins file information sources 
attempting supplant services systems approach allows easily incorporate services features projects condor globus legion available needed 
chapter experimental results chapter reports results experiments conducted explore relationships mw application performance specific application requirements environmental characteristics variety mw scheduling options 
chapter describing context experiments conducted 
section specify experimental environments test conditions section set test applications producing results section types goals experiments run 
introductory material sections contain experimental results obtained specific test environments section covers results pair homogeneous workstation clusters section covers results heterogeneous workstation cluster section covers results homogeneous clusters linked wan connection 
results experiments summarized section 
experimental environments experimental concentrates general distributed computing gdc environments consisting clusters workstations 
term workstation cluster mean collection general purpose computing machines linked common local networking infrastructure specialized systems hardware software tightly integrated parallel computing ibm sp cray 
workstation clusters prevalent educational commercial govern mental institution settings prominently investigation computing gdc environments 
available clusters vary widely size ranging machines single ethernet link hundreds machines linked arrays switches bridges routers 
investigation focuses relatively small clusters consisting machines environment available experiments 
investigation looked closely specific types computing envi ronments homogeneous clusters workstations heterogeneous clusters workstations homogeneous clusters workstations linked wide area network wan connec tion 
discuss greater detail subsections 
homogeneous clusters workstations consider homogeneous clusters machines system essentially identical machines linked single network delivering communication capacity machine 
machines considered essentially identical produced vendor employ cpu type operating clock frequency run type version operating system software equipped equal amounts ram memory 
important note machines homogeneous cluster may deliver unequal computational performance application due external factors degree loading programs shared processor network resources 
experiments conducted different homogeneous workstation clus ters 
cluster workstations designated active net cluster located department computer science engineering university california san diego ucsd 
workstation contained intel pentium iii cpus dual processor smp configuration cpu clocked mhz 
standard equipment machine included mb ram memory mbps base ethernet network interface 
machine connected intel express switch equipped series sx gigabit module 
backplane speed intel switch rated gbps 
machines running sun solaris operating system intel ia instruction set processors 
second cluster workstations designated cetus cluster located department computer science university tennessee knoxville utk 
machine sun ultra model workstation containing sun ultra sparc cpu clocked mhz mb ram memory 
network connectivity machines consisted mbps base ethernet interface connected cisco catalyst series switch rated media independent backplane speed gbps 
machines running sun solaris operating system sun sparc processors 
cluster environments expected differ primarily ratio computational capacity versus communication capacity delivered applications 
networking hardware employed homogeneous clusters similar delivers comparable performance processing hardware employed reflects differences performance caused years advancements cpu design production technology 
sun ultrasparc processors machines cetus cluster intel pentium iii processors active net cluster years 
comparison floating point performance sun workstations rated specfp benchmark intel workstations rated specfp 
second difference clusters observed experimental testing presence long running active jobs cetus machines trials ongoing 
effect external jobs variable level ranging decrease processor availability cetus machines tests 
clusters non dedicated mode allowed sharing resources users testing external loading outside jobs observed testing active net machines 
heterogeneous cluster workstations consider clusters heterogeneous underlying physical resources consist non identical processor resources unequal network connectivity ma chines cluster 
experiments assembled heterogeneous cluster workstations ucsd designated ucsd het cluster 
table shows various characteristics machines ucsd het cluster heterogeneous testing 
ucsd het test environment includes resources differ tal ways computational capacity processing units communication capacity data transfers processors things 
experienced capacity application dependent term ucsd het test environment contains processor strictly speaking load variations render homogeneous effectively heterogeneous heterogeneity caused differences hardware heterogeneity caused external factors qty 
processor clock ram network os intel pentium pro mhz mb base linux intel pentium ii mhz mb base linux intel pentium ii mhz mb base linux sun mhz mb base solaris sun ii mhz mb base solaris sun ultrasparc mhz mb base solaris sun ultrasparc mhz mb base solaris sun ultrasparc mhz mb base solaris sun ultrasparc iii mhz mb base solaris sun ultrasparc iii mhz mb base solaris table list heterogeneous machine types experiments 
resources observed deliver peak performance ranges difference fastest slowest performers various applications 
differences due different resources delivering varying levels hardware capabilities integer processing performance versus floating point processing performance applications making different demands capabilities 
test environment contains kinds ethernet connections mbps base mbps base links observed deliver bandwidth performance differ order magnitude communication capacity 
machines test environ ment difference network capacity accompanied difference observed efficiency processors able transfer large blocks data 
reasons lower data transfer efficiency include lower processor clock rates capable architectural features older technology processors 
experiments performed ucsd het cluster running non dedicated mode 
observe long running external processes contributed consistently significant loading resources experimental trials 
occasional short term periods loading outside processes contributed variability measured performance results 
bandwidth way message rate network link mbps kb msgs sec ucsd active net utk cetus ucsd utk wan table comparative network performance test environments 
homogeneous clusters workstations linked wan connection workstation clusters consist resources single network administrative domain wide area computing distributed clusters requires resources network administrative domain employed 
part investigation scheduling master worker mw applications wide area environments conducted experiments homogeneous workstation clusters linked wan connection ucsd utk 
machines forming homogeneous clusters ucsd utk active net cetus machines described homogeneous cluster testing 
selected machines active net cluster ucsd machines cetus cluster utk testing clusters linked wan connections 
wan connection ucsd utk exhibits significantly reduced network bandwidth increased network latency compared lan en vironment 
table shows measured values bandwidth way messaging rate predictor network latency lan networks wan connection ucsd utk 
noted wan connection dependent external routing decisions various locations measurements wan connection may limited period validity 
observations shown network bandwidth latency values wan connections ably differ orders magnitude values measured typical lan connections 
experiments intended show effects mw application performance resulting large differences 
single level level example mw process organization worker processes 
mw applications implemented test suite mw applications amwat development framework described chapter 
applications developed amwat automatically enabled amwat scheduler perform basic mw scheduling operations specialized mw scheduling functions 
scheduler supports mw programming model described chapter includes single multi cycle applications 
addition scheduler supports running applications single multiple levels worker processes arranged logical tree organization feature described greater detail section chapter 
majority experiments described chapter run applications simple single level worker processes organization 
cases advantages multiple levels worker processes explored level mw arrangement placed worker processes groups processes performed computations fourth acted intermediate node computing workers master process 
allowed comparisons performance single level level arrangements worker processes equivalent numbers physical processors multiples common increment processors running experiments varied number processors 
shows visual representation single level level arrangements worker processes case worker processes 
applications test suite mandelbrot povray npb ep classic single cycle mw applications execute repeatedly passing units associated data remote worker processes returning computed results back master process units completed 
single cycle applications differ primarily size data blocks associated transfers workers master process average amount computation required process unit 
fifth application dot single cycle performs major data transfers cycle times reg ular unit exchanges 
sixth application test suite ga tsp multi cycle 
seventh application mw application performance emulation program pep created tool studying effects varying application characteristics performance different environments 
discuss test applications greater detail subsections 
mandelbrot set image generator mandelbrot application experiments amwat implementa tion parallel mandelbrot set image generation program 
program generates fractal images parameters set program invoking command line 
parameters determine region complex number space tested membership mandelbrot set parameters generating function resolution pixels image generated 
experiments involving mandelbrot pixel pixel array integer values accumulated master process represent fractal image shown 
image set parameters experimental trials involving mandelbrot application 
pixels fractal image independently calculated mw implementation mandelbrot define unit rectangular block adjoining pixels image generated 
blocks pixels making unit assigned mw application scheduler remote worker processes computation returned master process assembly final image 
results chapter total image divided units leading unit result blocks sizes pixels respectively 
pixel transfer required sending bytes data 
povray ray tracing program povray application reported experiments version popular ray tracing program developed persistence vision team adapted fractal image output mandelbrot program experiments 
run amwat framework 
ray tracing method rendering realistic dimensional scenes textual model descriptions utilizing basic principles optics light propagation generate visually accurate images 
reported experiments involving povray pixel pixel full color image calculated scene shown 
test scene name benchmark performance povray application different computer platforms 
similar mandelbrot application pixels ray traced image independently calculated units defined contain different numbers pixels 
results chapter image divided blocks pixels 
result image pixel required transfer data bytes 
image output povray program experiments 
nas embarrassingly parallel benchmark ep npb ep application implementation nas ep parallel kernel benchmark 
ep refers program embarrassingly parallel designed measure floating point performance parallel machines requiring signifi cant interprocessor communication 
computations performed application generation pairs gaussian random deviates written specification tabulation number pairs successive square 
operations typical calculations performed monte carlo simulation applications 
applications experiments described chapter im plemented npb ep benchmark amwat development framework 
ap plication runs single computation cycle units computed 
data transfers application consist bytes returned master process worker processes unit computed 
run ep benchmark class problem size values experienced compute times second unit running set workstations tests 
tomography program image reconstruction tomography application mw implementation program cessing data files tomographic reconstruction dimensional images 
data files contain data recorded electron beam scans biological specimens 
electron beam tomography process computing dimensional images biological structures multiple dimensional images called slices obtained electron beam scanning objects incrementally changing angles 
tomography application pre processes raw data files obtained electron beam scanning preparation final tomographic reconstruction 
implemented amwat development framework tomography runs single computation cycle number units equal number input files processed 
tests reported chapter tomography run input files 
unit processed tomography involved master process transferring kb input data worker process computation began receiving kb result data unit computed 
compute times unit ranged widely seconds running workstations test environments 
orientation search program macromolecular docking dot application mw implementation program called dot developed research institute san diego supercomputer center sdsc screen potential docking configurations biological macromolecules detailed analyses performed promising candidates 
dot stands daughter program performs rapid computation electrostatic potential energy proteins charged molecules 
constructs grid approximate interaction energies orientations computing potential energies exhaustively searching rotations molecule full degrees freedom second molecule held fixed developed victoria roberts help study macromolecular docking 
dot improves scaling better solve large problems networks computers geometric fit information closest distance molecules full poisson boltzmann electrostatic potentials electrostatic model 
position 
implemented amwat development framework dot runs single cycle mw application differs previously discussed applications performing data transfers cycle 
computation cycle test cases master process transfers mb initialization data worker process computation cycle worker process transfers mb result data back master process 
compute times unit test cases ranged widely seconds running test environment workstations 
genetic algorithm solving traveling salesman problem ga tsp application implementation distributed genetic algorithm approach solving known traveling salesman problem tsp 
genetic algorithm population candidate solutions problem encoded sequence characters genetic sequence 
sequences adapted stages called generations produce new hopefully better solutions passing generation 
new generation algorithm selects specified percentage best candidates population evaluation function quantify candidate solutions desirable 
algorithm performs kinds evolutionary operations selected sequences combination mutation 
combination selection com elements randomly chosen sequences merging produce third sequence contains subsequences parents represents valid solution problem 
mutation random selection element candidate sequence performing transformation yields second sequence represents valid solution problem 
valid solutions problem allowed population sequences combination mutation operations produce valid solution sequences invalid solution sequences filtered wards 
evolutionary operations performed selected pool candidate sequences entirely new population solutions created 
process repeated succeeding generations fixed number generations processed candidate solution generated meet specific criteria successful solution problem 
genetic algorithm approach solving tsp problems ga tsp application solution sequence list locations problem representing order locations visited form complete tour passes location exactly 
best valid solution tour requires shortest total traveling distance visit city closed cycle 
ga tsp starts generating population randomly created tours visiting location closed cycle applies genetic algorithm produce new generations valid tours search shorter total traveling distances 
search parallelized mw computation making creation new generation computation cycle mw programming model 
cycle process sent selected candidate sequences current generation located master process new generation sequences created 
start cycle worker processes units instruct generate fixed number new sequences combination mutation operations probability rates passed command line parameters 
unit results representing new solution sequences sent back master process sequences form new generation received 
cycle master process selects best sequences new generation seeds generation repeatedly processes cycles way specified maximum number generations processed 
trials ga tsp reported chapter known berlin tsp input list coordinates representing locations city berlin shortest tour passing location determined 
locations tsp problem 
possible unique valid tours making exhaustive search approach finding solution impractical 
shows example genetic algorithm progressing best solution generation best solution generations 
implemented amwat development framework ga tsp multi cycle mw application calculation new generation forms computation cycle 
tests run ga tsp program generations population size sequences unit containing sequences units processed cycle 
berlin tsp known optimal solution choice run time parameters allowed ga tsp typically produce solutions tour lengths optimal result 
generation kb data sent master process worker processes worker processes return kb genetic algorithm tsp solver berlin generation genetic algorithm tsp solver berlin generations example progression solutions berlin problem ga tsp 
data representing new solution sequences unit computed 
compute times unit ranged milliseconds different workstations test environments 
amwat performance emulation program pep performance emulation program pseudo application designed model structure mw applications adjustable application parameters 
adjustable parameters include number compute cycles number units cycle average unit compute time unit input data block size unit result block size cycle setup data block size cycle result block size 
pep designed allow coverage wide range mw application characteristics testing portable application performance different environments 
implemented amwat development framework test ap plications pep inherits mw scheduling functionality provided amwat scheduling module 
application emulator configured emulating data transfer computation behavior different mw application types supported amwat programming model including single cycle multi cycle applications unit oriented cycle oriented data transfer patterns 
experimental performance results sections experimental results mw application formance gdc test environments described previously section homogeneous workstation clusters heterogeneous workstation cluster pair homogeneous workstation clusters connected wide area network link respectively 
measure application performance cases application execution time results different views factors contributed mw application execution times seen experimental trials 
view environment oriented investigation general mw application performance environment data obtained running pep application environment varying relative amounts computation communication performed trial 
views investigate performance effects applying different scheduling approaches specific mw test applications varying number processors involved computation distribution method employed 
views discussed greater detail subsections 
performance varying computation communication loads investigate mw application performance respect environmental fac tors application characteristics ran pep application varying primary application parameters range values unit compute time unit result size 
unit compute time varied pep yield compute times aver aging seconds computation unit unloaded processor target environment 
unit result size varied pep produce result output sizes bytes transferred unit 
tri als conducted applying unique parameter combination pep program running target environment recording time needed finish emu lated computation 
trials run parameter combinations repeated trials run combination 
execution times pa rameter combination averaged trials yield average execution times representing pep performance range application parameters tried 
key shading computation limited regime overhead limited regime communication limited regime mixed factor limited regime output unit block size compute time sec bytes table example data table environment oriented view mw application performance varying computation communication requirements pep application 
time required execute pep trials range tested parameter combinations results tested environment usually represent performance observed testing period hours 
reason presenting results environment oriented view illus presence identifiable regions parameter space defined varying computation communication related application parameters require different scheduling approaches ensure consistently mw performance environment 
call regions scheduling regimes particular environment 
example table shows set results tabular form additional shading added help visually identifying specific regimes 
key shading data table shows different types regimes appear view compute limited performance overhead limited performance communication limited performance mixed factor limited performance 
determining table entries categorized regimes patterns observed execution time varies changes compute time output block size 
system limited computational capacity application execution time simply proportional unit compute time 
table format results behavior appears order magnitude change total execution time moving left right row entries follow pattern assigned computation limited performance regime 
example behavior seen row table values 
entries table deviate behavior may indicate areas parameter space performance constrained factors form regimes 
example execution time change proportionally unit transfer size moving column values seen column table values indicating area communication issues dominant constraint application performance 
regions exhibiting behavior assigned communication limited performance regime 
instance deviation simple computation limited performance appears execution time entries lower left corner table values column table 
execution time region parameter space change proportionally compute times output block sizes appears lower bound observed application performance due basic mw overhead costs incurred distributing units collecting results 
entries exhibiting behavior assigned overhead limited performance regime 
table entries value column table cleanly fall regimes placed mixed factor regime 
specific regimes application parameter space determined environment possible map specific applications graph delineated parameter space obtain expected scheduling regimes applications fall running target environment 
example environment data values table application parameters test applications described section shown 
application scalability test environment way evaluate application performance view scalability different test environments 
scalability parallel system measure capacity increase speedup proportion number processors 
speedup usually defined ratio run time best known sequential algorithm solving problem time taken parallel algorithm solve problem processors 
practice run time best sequential algorithm may known easily determined arbitrary choice parallel algorithm computing data transfer size bytes computation limited communication limited mixed limitations overhead limited computation time sec example mapping application parameters graph behavioral regimes 
mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp cycle oriented data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
environment reporting speedup results substitute time taken perform parallel algorithm processor sequential execution time 
discussions scalability involve ability maintain constant levels efficiency ratio speedup number processors increasing number processors employed size problem solved 
interested studying scalability gdc environments particular application instances limit scope scalability questions considering application sizes fixed allowing number processors increase 
tests application scalability typically conducted running appli cation worker processes varying application parameters environment application run 
exe cution times averaged multiple trials obtained cycling back back changes parameters varied number processors trials run variant depending time required run trials amount variation execution times recorded 
results plotted show execution time function increasing worker processes instance smaller execution times equating better application performance 
example scalability results seen scalability mandelbrot application compared different environments 
application speedup test environment second way evaluate application performance view application speedup function increasing numbers worker processes 
consider showing results view case homogeneous environments speedup values individual processors having different levels deliverable computing capacity easily defined measured 
speedup values different numbers worker processes important indicators additional processors employed improve performance application 
terms efficiency defined speedup number processors divided interested identifying regions operation result small isoefficiency function values nearly constant efficiency values number processors region 
explained earlier practical reasons calculation speedup results generated dividing execution time application worker process execution time application worker processes 
execution times taken experimental trials produce scalability results 
results application speedup typically shown application worker processes varying application parameters environment application run 
results plotted show speedup function increasing worker processes instance 
larger speedup values better maximum speedup worker processes equal corresponding efficiency value 
example speedup results seen speedup mandelbrot application compared different environments 
effects distribution strategies test environment third view application performance results comparison application execution times variety distribution strategies 
experiments conducted help determine distribution strategies self scheduling ss appropriate quantify amount benefit possible employing alternate distribution strategies supported amwat scheduler 
tests comparing different distribution strategies performed run ning application target environment different distribution strategies back back trials keeping application parameters identical 
exper cycled strategy times depending variability observed execution times time needed run set trials recording execution times trial 
results average measured execution times distribution strategy bar graphs shorter bars representing better application performance 
example distribution strategy results seen performance mandelbrot application compared different distribution strategies 
details distribution strategy section chap ter 
ss strategy generally results lower load imbalance cost requiring communicating exchanges worker processes master process 
fixed time fixed allocation strategy minimizes number communicating ex changes workers master high probability failing best initial assignment units workers 
fsc fixed size chunking strategy uses larger fixed size blocks reduce number communicating exchanges larger groups increase potential load imbalance 
strategies tss trapezoidal self scheduling gss guided self scheduling fac factoring variations decreasing sized allocation blocks effort reduce number exchanges provide load balancing behavior 
experimental results local homogeneous systems section contains results experimental investigation mw appli cation performance homogeneous workstation cluster environments 
results evaluated performance criteria impact environmental constraints general mw application performance impact specific mw application charac performance test environments 
environmental characterization homogeneous clusters consider performance general pep mw application ho clusters varying application parameters average unit compute time unit data size 
purposes comparison ran tests different homogeneous clusters active net cluster ucsd cetus cluster utk 
table shows execution times pep emulation runs active net clus ter worker processes active net cluster constructed current general purpose commodity processor networking technology expected de liver best levels performance compared general purpose workstation clusters 
table shows unit compute times sec greater unit result transfer sizes bytes greater mw performance follows pattern expected compute bound execution execution times increasing proportional unit compute times 
transfer sizes approach bytes unit communication costs dominate execution time compute times approach seconds unit 
compute times drop seconds unit execution time appears level minimum value seconds 
value represents close lower bound execution time achievable amwat mw applications active net environment 
table shows results tests worker processes active net cluster 
results consistent table worker processes showing execution times twice long half number worker processes compute bound regions performance showing execution times nearly worker processes parameter space regions performance expected limited communication overhead constraints 
shows graphical representation regimes derived values table 
mapped graph mw applications application test suite 
see applications povray npb ep clearly computation limited regime applications mapping overhead limited regime mixed factors regime 
implications having application table values reflect average performance trials trials conducted worker processes trials conducted worker processes 
key shading computation limited regime overhead limited regime communication limited regime mixed factor limited regime output unit compute block size time sec bytes table execution times pep emulator program running homogeneous cluster intel pentium iii workstations range application computation communication parameters 
number processors number units execution times measured seconds parameters fall different behavioral regimes homogeneous cluster environments covered specific application section 
table shows results set tests previously described time running cetus cluster utk 
active net cetus clusters similar architectures consisting workstations connected base switched ethernet links 
clusters primary difference capacity values physical resources older technology cetus cluster having considerably processor capability somewhat networking capability employed active net cluster 
table reflects results differences 
note range parameter space execution time increases proportionally unit compute time similar clusters unit compute times seconds unit result sizes bytes appears region communication limited performance cetus cluster 
performance cetus cluster compute bound region ideal appears active net cluster 
example column seconds compute time ideal execution time close seconds worker processes equally sharing workload 
output unit block size compute time sec bytes table execution times pep emulator program running homogeneous cluster intel pentium iii workstations range application computation communication parameters 
number processors number units execution times measured seconds data transfer size bytes computation limited communication limited mixed limitations overhead limited computation time sec mapping application parameters graph behavioral regimes active net cluster 
active net machine behavior application identified upper case letters mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp cycle oriented data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
output unit block size compute time sec bytes table execution times pep emulator program running homogeneous cluster sun ultrasparc workstations range application computation communication parameters 
number processors number units execution times measured seconds experimental logs show part reason increased execution time presence long running external processes resulted processors consistently delivering availability trials 
increased constraints due communication issues shown cetus cluster compared active net cluster execution times twice high regions parameter space communication times dominate appear clusters 
table shows results set tests time worker processes cetus cluster 
results remarkable consistently show compute bound regions experience nearly fold increase execution time regions parameter space show results nearly unchanged worker case 
shows graphical representation regimes derived values table 
mapped graph mw applications application test suite 
contrast results active net cluster cetus cluster environment see applications fall computation limited regime 
compare experimental results obtained running pep program homogeneous clusters simulation results obtained running mw appli cation performance simulator described section environment parame ters measured homogeneous clusters 
simulator essentially executable implementation flow performance model described chapter similar results help validate accuracy performance model predicting real world output unit block size compute time sec bytes table execution times pep emulator program running homogeneous cluster sun ultrasparc workstations range application computation communication parameters 
number processors number units execution times measured seconds data transfer size bytes computation limited communication limited mixed limitations overhead limited computation time sec mapping application parameters graph behavioral regimes cetus cluster 
cetus machine behavior application identified upper case letters mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp cycle oriented data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
output unit block size compute time sec bytes table execution times produced mw simulator environmental parameters homogeneous cluster intel pentium iii workstations range application parameters computation communication 
number processors number units execution times measured seconds application behavior 
table shows simulation results environment parameters active net cluster application parameters experimental results shown table active net cluster 
comparing execution times table table shows simulation experimental results close agreement entries 
experimental execution times higher entries representing second compute times result sizes kbytes mbyte entries second compute times 
believe deviations execution time result unique contention effects caused networking equipment active net cluster 
chapter see similar contention effects experimental results running test applications tomography active net environment 
table shows simulation results environment parameters cetus cluster application parameters experimental results shown table cetus cluster 
comparing execution times table table shows simulation experimental results close agreement nearly entries 
homogeneous cluster environments simulator underlying flow performance model appear job predicting pep behavior wide range compute times results sizes 
output unit block size compute time sec bytes table execution times produced mw simulator environmental parameters homogeneous cluster sun ultrasparc workstations range application parameters computation communication 
number processors number units execution times measured seconds mw application performance homogeneous clusters consider results experiments characterizing performance specific mw applications running test homogeneous workstation cluster environments 
applications implemented amwat de velopment framework built scheduling module provided scheduling functions required perform tests 
applications mandelbrot povray npb ep tomography classic single cycle mw applications important parameters considered environmental characterization tests unit compute time unit result size 
results classic style mw appli cations may compared formulate general predictions application behavior simple highly popular form mw computing 
appli cations dot ga tsp illustrate performance different forms mw computing largely cycle oriented data transfers 
mandelbrot mandelbrot application shows execution time scales increasing numbers worker processes homogeneous clusters fig ure shows effect additional workers terms speedup 
difference achievable execution time performance clusters dramatic best average execution time observed active net cluster seconds worker processes best average execution time observed active net cluster times larger seconds worker processes 
fact seconds achieved single worker process active net cluster better worker performance cetus 
consequence high computing capacity processors active net cluster mandelbrot application effectively workers time 
result agrees general characterization results active net cluster shown table mandelbrot running active net exhibited compute time seconds unit result size kbytes unit tests placing outside compute bound region operation workers 
cetus cluster compute time tests measured seconds unit placing compute bound region operation cetus cluster shown table 
shows effects different distribution strategies man execution time running worker processes test cluster 
graph shows simple ss self scheduling strategy clearly performs best cetus cluster fixed strategy performs worst increase execu tion time 
poor performance fixed strategy reflects part problems fixed able adapt variable load conditions cetus machines test period 
active net cluster tests ss gss fac strategies produced average execution times fixed strategy produced average execution time worse best times shown gss fac 
fixed strategy clearly performed better active net cluster environment loading outside jobs performance suffered load imbalance processors inherent variability compute times units mandelbrot application 
povray povray application results similar mandel 
basic difference performance results relative differences application falls parameter space defined unit compute time unit result size 
active net cluster povray compute time seconds unit cetus cluster povray compute time seconds unit 
clusters result size kbytes unit 
values place povray strongly compute bound region cetus cluster povray falls speedup execution time sec mw application scalability mandelbrot ucsd active net utk cetus processors mw application speedup mandelbrot processors speedup ucsd active net utk cetus scalability execution time sec mw application distribution mandelbrot ss gss fac fixed distribution method distribution ucsd active net utk cetus mandelbrot set generator program homogeneous environments 
compute bound region active net cluster 
result see figures povray scales better worker processes mandelbrot environments 
performance active net higher cetus cetus showing average execution time times greater active net 
shows effects different distribution strategies povray execution times achieved homogeneous test cluster worker processes 
graph shows results nearly identical mandelbrot 
deduce similar results mandelbrot discernible benefit strategy ss povray active net cetus environments 
npb ep npb ep application see figures example application appears clearly compute bound regions application parameter spaces identified homogeneous test environments 
result size npb ep low bytes unit 
active net cluster average compute time npb ep measured seconds unit cetus cluster measured seconds unit 
results shown figures confirm expected behavior application shows ideal scalability environments 
shows ss distribution strategy worse tested alternatives fixed strategy clearly worse cetus environment 
poor performance fixed strategy cetus cluster due unaddressed variability processor availability processors hosting worker processes 
active net cluster execution times different distribution strategies 
variability compute times unit load imbalance problem shown case npb ep 
tomography case tomography application figures show values application parameters extremes range con sidered 
result size instance tomography test kbytes speedup execution time sec mw application scalability povray ucsd active net utk cetus processors mw application speedup povray processors speedup ucsd active net utk cetus scalability execution time sec mw application distribution povray ss gss fac fixed distribution method distribution povray ray tracing program homogeneous environments 
ucsd active net utk cetus speedup execution time sec mw application scalability npb ep ucsd active net utk cetus processors mw application speedup npb ep processors speedup ucsd active net utk cetus scalability execution time sec mw application distribution npb ep ss gss fac fixed distribution method distribution ucsd active net utk cetus nas parallel ep benchmark homogeneous environments 
unit 
addition tomography application sent kbytes data ers unit processed included calculations accounting network loading 
active net cluster tomography compute time measured seconds unit cetus cluster compute time measured seconds unit 
figures show tomography performance active net cluster scales reveals speedup cetus cluster workers half active net cluster 
result poor speedup performance cetus cluster combination lower computa tional capacity processors produces average worker execution time cetus times longer produced active net cluster 
examination results reveals part reason lack performance cetus cluster running tomography 
graph shows case discussion fixed distribution strategy results sig better performance alternatives 
cetus cluster tomography ss distribution strategy takes longer average run ing fixed strategy 
examination test logs showed processors hosting worker processes idle execution time cetus cluster processor idle times active net cluster seldom exceeded execution time 
idle time results large part worker processes waiting master process assign additional worker 
fixed strategy minimizes cause idle time assigning initially allowing workers continue computing assigned units completed 
lack similar problem active net cluster suggests faster processors network links allow master process efficiently handle large blocks incoming result data respond promptly requests 
gss fac strategies intended reduce number interactions master worker processes help response time number units processor low tomography test case units divided processors strategies simply produce allocation results similar produced ss 
speedup execution time sec mw application scalability tomography ucsd active net utk cetus processors mw application speedup tomography processors speedup ucsd active net utk cetus scalability execution time sec mw application distribution tomography ss gss fac fixed distribution method distribution tomography program homogeneous environments 
ucsd active net utk cetus dot dot application differs behavior previously discussed ap plications primarily way data transfers performed 
dot program tests network traffic consists mbytes data sent master process worker process computation starts mbytes data returned master process worker process computa tion 
network load proportional number worker processes number units 
active net cluster average compute time measured seconds unit cetus cluster compute time measured seconds unit 
figures show resulting performance dot test environments level level worker organizations 
fig ure shows level organizations tested group worker processes organized workers intermediate node 
organization chosen allow comparisons results single level level organizations equivalent numbers processors 
graphs reveal scalable performance single level workers case strongly limited network lim 
environments number worker processes exceeds total network traffic application increases mbytes execution times start increase increasing numbers workers 
level worker organization case produces lower execution times improved speedup values twelve worker processes active net cluster processors cetus cluster 
observe greater capacity resources active net cluster results significantly better performance running dot application 
dot cetus cluster takes times longer run dot active net cluster single level worker processes 
shows distribution strategy little influence performance dot environment 
consistent low network loading produced application computation phase allows fast response master requests worker processes cases distribution strategy 
processor idle times due workers waiting master distribute initial cycle data worker collect cycle results worker computation 
wait times increase proportionally number worker processes 
execution time sec speedup mw application scalability processors dot ucsd active net layer tree ucsd active net layer tree scalability active net mw application speedup ucsd active net utk cetus ucsd active net layer tree utk cetus layer tree processors dot speedup execution time sec execution time sec mw application scalability processors dot utk cetus layer tree utk cetus layer tree scalability cetus mw application distribution dot ss gss fac fixed distribution method distribution dot program homogeneous environments 
ucsd active net utk cetus ga tsp workers workers workers workers level configurations tested dot 
ga tsp application operates pattern mw data transfers master process worker processes having unit oriented computation cycle oriented data transfers 
application implements genetic algo rithm solving traveling salesman problem 
population problem composed valid tours cities algorithm proceeds generating successive generations tours combination mutation operations 
calculation new gener ation tours occurs mw computation cycle 
result computing unit ga tsp number newly evolved city tours tsp problem tests required kbytes data unit returned master process 
new cycle genetic algorithm master process sends new seed population worker process tours generation evolved 
test problem required transfer kbytes data worker process new generation calculated 
compute times ga tsp application measured seconds unit active net cluster seconds unit cetus cluster 
starting population size tours job creating new population divided units tours unit 
figures show performance ga tsp scales number worker processes increased test environments 
show results traditional single level level organizations worker processes 
level organizations tested shown dot 
note speedup ga tsp stops increasing worker processes utilized case single level worker processes level organization worker processes allows speedup values continue increasing past processors environment 
performance bottlenecks single level case attributed cycle related data transfers new generation worker process 
impact additional kbytes communication cycle added worker significant cycle times drop second case active net cluster 
cetus cluster cycle times remain seconds network constraints manifested quickly 
level organization worker processes number data transfers master process worker processes divided master communicate intermediate nodes worker process 
shows distribution strategy noticeably better ss environments workers ga tsp active net cluster results reduction execution time compared case workers 
execution time sec speedup mw application scalability ga tsp ucsd active net layer tree ucsd active net layer tree processors scalability active net mw application speedup ucsd active net utk cetus ucsd active net layer tree utk cetus layer tree ga tsp processors speedup execution time sec execution time sec mw application scalability ga tsp utk cetus layer tree utk cetus layer tree processors scalability cetus mw application distribution ucsd active net ucsd active net utk cetus ga tsp ss gss fac fixed distribution method distribution ga tsp solver program homogeneous environments 
experimental results local heterogeneous system section contains results experimental investigation mw applica tion performance heterogeneous workstation cluster environment 
case homogeneous clusters results evaluated performance criteria impact environmental constraints general mw application performance impact specific mw application characteristics performance test environment 
environmental characterization heterogeneous clusters discuss results exploration general mw application perfor mance heterogeneous workstation cluster located ucsd remainder section simply refer ucsd het cluster 
mix machine types experimental cluster earlier table 
just homogeneous cluster cases give pep emulation execution times varying ap plication parameters average time compute unit unit result data size 
table show pep results worker ucsd het cluster processor host master process connected mbps ethernet network connection 
master host processor results shown table pentium ii clocked mhz 
results show wide range op erating regions parameter space execution time proportional unit compute times including cases result block sizes bytes 
entries lower left corner table appear regions execution time clearly communication bound compute bound 
proportional unit compute times wide range operating parameters execution times heterogeneous results appear greater magnitude seen neous cases greater ideally expect mix machines 
examination testing logs showed high execution time consistently caused load imbalance single machine 
machine equipped relatively slow sun ii cpu clocked mhz mbps ethernet connection 
shows graphical representation regimes derived val ues table 
mapped graph mw applications application test suite 
see applications parameter values falling key shading computation limited regime overhead limited regime communication limited regime mixed factor limited regime output unit block size compute time sec bytes table execution times pep emulator program running heterogeneous cluster workstations range application computation communication parameters 
master host linked mbps ethernet number processors number units execution times measured seconds computation limited regime mandelbrot dot ga tsp show val ues near regime compute times produced fastest processors ucsd het cluster 
performance implications specific test applications covered section 
table show similar results ucsd het cluster processor master host utilizing machines host worker processes 
performance conditions completely consistent results compute limited regimes shown worker processes approximately doubling execution time half workers 
performance communication bound regions appears slightly worse adding execution times result sizes bytes unit 
table show pep results processor previously master host ucsd het cluster testing workers replaced mbps ethernet connection network 
processor selected master host set trials slow sun machine observed earlier cause significant load imbalance earlier trials 
results interesting output unit block size compute time sec bytes table execution times pep emulator program running heterogeneous cluster workstations range application computation communication parameters 
master host linked mbps ethernet number processors number units execution times measured seconds data transfer size bytes computation limited communication limited mixed limitations overhead limited computation time sec mapping application parameters graph behavioral regimes heterogeneous cluster master host connected mbps link 
range machine behavior cluster application identified bar upper case letters mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp cycle oriented data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
output unit block size compute time sec bytes table execution times pep emulator program running heterogeneous cluster workstations range application computation communication parameters 
master host linked mbps ethernet number processors number units execution times measured seconds observe region parameter space pep performance compute bound appears smaller compute bound region better connected master host performance compute bound regions noticeably better 
attribute better performance compute bound regions removal slow processor source load imbalance ucsd het cluster serving master process host 
bad consequences having poorly connected host master process seen communication bound regions table execution times higher order magnitude higher regions output size bytes previous trials connected master host processor 
shows graphical representation regimes derived values table 
test applications mapped range application parameters machines test environment 
note shows mandelbrot dot ga tsp applications map communication limited regime poorly connected processor master host 
mw application performance heterogeneous cluster consider results experiments characterizing performance mw applications running test heterogeneous cluster environment ucsd 
experiments concentrate quantifying effects certain scheduling approaches improving application performance running environments containing highly heterogeneous resource capacities 
data transfer size bytes computation limited communication limited mixed limitations overhead limited computation time sec mapping application parameters graph behavioral regimes heterogeneous cluster master host connected mbps link 
range machine behavior cluster application identified bar upper case letters mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp cycle oriented data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
mandelbrot mandelbrot application shows execution time perfor mance scales increasing numbers worker processes 
heterogeneous cluster choices processor hosting master process 
shows effects choosing different processors master host 
upper curve host named conundrum reflects machine features relatively slow processor sun cpu clocked mhz slow mbps network connection 
combination low performance hardware produces execution times times produced machine named sojourner master process host 
execution time curve graph host named thing falls generally sojourner conundrum reflects results machine mbps network connection similar conundrum features faster sun ultrasparc cpu clocked mhz 
lowest execution times clearly produced host named sojourner con nected network mbps ethernet connection featured intel pentium ii cpu clocked mhz 
results appear reinforce idea selection proper master host significant effects mw application performance heterogeneous systems processor network capacity play part making right choices 
shows effects various distribution strategies man performance test heterogeneous environment 
results clearly show effectiveness different distribution strategies depends conditions applied 
case shown graph dark bar applies strategies weighting allocations different computational capacities processors 
performance results weighting clearly inferior strategies shown ss definition allocates single unit effected weighting 
second case represented lighter shaded bars applies strategies image divided units containing blocks pixels 
approach gss strategy produces best performance returning execution time lower simple ss strategy 
third case applies strategies image divided units 
best performance produced strategy ss fac strategy returns average execution time lower ss units lower best time returned units 
cases fixed strategy clearly performs worse strategies 
povray povray application shows scalability comparison shown earlier mandelbrot 
tests povray performance different master process hosts showed average execution times produced hosts mbps ethernet connections similar worker processes sojourner host mbps ethernet connection produced average execution times second best thing worker processes 
results show povray application relatively longer computation times smaller result block sizes responds performance sensitive capabilities execution time sec execution time sec mw application scalability mandelbrot ucsd heterogeneous sojourner master ucsd heterogeneous thing master ucsd heterogeneous conundrum master processors scalability mw application distribution mandelbrot ss fsc tss gss fac fixed distribution method distribution units weighting units weighted cpu units weighted cpu mandelbrot set generator program heterogeneous environment 
master process host mandelbrot application 
shows comparison results various distribution strategies worker processes 
graph gives effects average ex ecution time different strategies applied povray cases image partitioned units 
results show distribution strategy fixed increasing number units reducing size unit resulted lower average execution times 
individual differences alternatives fixed strategy small clear pattern results identifies strategy consistently better rest 
npb ep npb ep application shows performance ap plication nearly insensitive choice host master process 
application performs relatively little communication performance primarily determined computational capacity individual worker processes 
comparing results npb ep similar tests mandelbrot povray figures observe range application behavior related relative ratios computation communication activity application performs 
applications mandelbrot performance heavily communication bound highly sensitive capabilities potential master process hosts sustain high transfer rates 
applications npb ep performance primarily computation bound dependent capabilities worker hosts employed 
applications povray exhibit performance traits fall extremes 
discussed section chapter flow model mw application performance accounts behavior see figures experimental confirmation specific environmental configurations produce behavioral regimes agree model 
particular mandelbrot application maps computation limited regime connected master environment depicted communication limited regime poorly connected master environment shown exhibits high sensitivity connectivity master host 
npb ep application cleanly maps computation limited regime cases relatively insensitive connectivity master host 
execution time sec execution time sec mw application scalability povray ucsd heterogeneous sojourner master ucsd heterogeneous thing master ucsd heterogeneous conundrum master processors scalability mw application distribution units weighted cpu units weighted cpu units weighted cpu povray ss fsc tss gss fac fixed distribution method distribution povray ray tracing program heterogeneous environment 
shows npb ep running test environment form significantly better distribution strategy place basic ss 
graph shows increasing number units sig change results indicating potential gains reducing load imbalances offset increased overhead performing extra communication needed support increased number units 
tomography tomography application shows results execution time different numbers worker processes fixed distribution strategy 
test application run input files file forming single unit 
graph shows consistently decreasing average execution times number worker processes increased rate decrease appear smooth varying capacities processors added computation different trials 
order processors added predetermined experimental trials begun ordering experiments varied number worker processors 
worker processes average execution time seconds sixteen worker processes seconds 
reduction average execution time number workers demonstrated tomography capable relatively scalable performance test heterogeneous environment 
shows results running tomography different distribution strategies worker processes heterogeneous environment 
graph ss fsc strategies produced execution times higher fixed strategy gss tss fac strategies produced execution times higher fixed 
results tomography running homogeneous cetus cluster described section strategies reduced number interactions required master worker processes delivered significantly better performance 
results illustrate additional load balancing issues arise scheduling mw applications low unit processor ratios heterogeneous systems 
test instance tomography application units running worker processes resulted units assigned execution time sec execution time sec mw application scalability npb ep ucsd heterogeneous sojourner master ucsd heterogeneous thing master ucsd heterogeneous conundrum master processors scalability mw application distribution npb ep ss fsc tss gss fac fixed distribution method distribution units weighted cpu units weighted cpu nas ep benchmark heterogeneous environment 
worker distributed proportionally computing capacity processor 
circumstances described section chapter strategies reduce load imbalance deliver better performance strategies allow allocations result greater levels load imbalance 
compare results ucsd het cluster homogeneous clusters shown ss strategy produced execution times higher fixed strategy cetus cluster see systems greater degrees heterogeneity benefit strategies directly address load imbalance problems 
dot dot application shows execution time performance results varying number worker processes 
results shown graph running dot traditional single level worker processes communi cates directly master process second running dot groups worker processes communicate fourth process acting intermediary workers master process 
hierarchical organization described section homogeneous cluster cases addition worker process case pattern 
just homogeneous cluster cases dot performance single level organization quickly reaches point additional workers fail reduce execution times 
point occurs twelve workers heterogeneous test case 
recall implementation dot tests single cycle mw application mb cycle setup data transferred worker cycle mb result data transferred back master process worker cycle 
single level worker processes data transfers performed master process increase proportionally number worker processes 
level organization test reduces number transfers factor requires worker processes dedicated intermediary node available computations 
tradeoff results level organization delivering longer execution times single level organization number worker processes trials sixteen worker processes 
graph level organization produces longer execution times single level case twelve worker processes shorter execution times sixteen worker execution time sec execution time sec mw application scalability tomography ucsd heterogeneous ss ucsd heterogeneous fixed processors scalability mw application distribution tomography ss fsc tss gss fac fixed distribution method distribution ucsd heterogeneous tomography program heterogeneous environment 
execution time sec mw application scalability processors dot ucsd heterogeneous level tree ucsd heterogeneous level tree dot program varying numbers worker processes heterogeneous environment 
processes 
level organization shown scalable single level organization greater worker processes net performance boost reduced processor resources acting intermediate nodes performing computation 
result worker level case produced execution times lower average worker single level case worker level case produced execution times lower average best single level times 
ga tsp ga tsp application shows kind comparison tomography application 
results shown traditional single level mw organization level hierarchical mw organization worker processes communicating fourth process acting intermediary node workers master process 
level configurations tested ones described section 
recall ga tsp multi cycle mw application test case execution time sec mw application scalability ga tsp ucsd heterogeneous level tree ucsd heterogeneous level tree processors ga tsp solver program varying numbers worker processes heterogeneous environment 
performs computation cycles units processed cycle 
cycle master process transfers kb cycle setup data containing seeds new generation solution sequences worker workers transfer kb result data computed unit contains newly generated sequences back master process 
problem requiring generations compute total amount cycle setup data equal mb times number worker processes 
just case dot level mw hierarchical organization reduces amount data transfers handled master process factor reduces number processes available perform computations 
ga tsp point level organization starts providing lower execution times worker processes 
worker processes level worker organization produces ex ecution times lower single level case workers lower level case workers 
single level organization worker processes produces execution times longer single level organization workers 
experimental results wide area system section contains results experimental investigation mw applica tion performance resources separated wan type network link 
wan connection chose perform experiments machines located ucsd utk 
case homogeneous heterogeneous clusters re sults evaluated performance criteria impact environmental constraints general mw application performance impact specific mw application char performance test environment 
environmental characterization wide area system discuss section results exploration general mw appli cation performance system half worker processes connected master process wan link 
trial cluster characterization exper selected half worker machines active net cluster ucsd half cetus cluster utk 
ran trials pep mw emulation program scenarios master host processor locations worker worker configuration 
computation time scaled values homogeneous cluster characterization tests cluster master host processor resided allowing comparisons results homogeneous wan cases master host employed 
table show results master process worker processors located active net cluster worker processes located cetus cluster 
employing meaning distinguishing shading tables instructive compare results tables 
comparing worker wan results worker homogeneous results shows wan configuration performs significantly worse region operation 
compute times seconds wan case takes longer execute result size bytes longer result size bytes 
comparing worker wan results worker homogeneous results table shows virtually regions operation local workers active net cluster performed better workers combination workers cetus cluster 
results verify significantly slower processing resources wan link improve mw application key shading computation limited regime overhead limited regime communication limited regime mixed factor limited regime output unit block size compute time sec bytes table execution times pep emulator program running homogeneous clusters workstations linked wan connection range application computation communication parameters 
number processors master active net cluster number units execution times measured seconds performance 
table show results running pep master process worker processes located active net cluster worker processes located cetus cluster 
configuration shown perform worse worker cases region operation 
execution times significantly increased halving processors cases direct proportion number workers 
table show results master process worker processors located cetus cluster worker processes located active net cluster 
note number regions exhibiting primarily compute bound behavior configuration scenarios looked restricted regions higher compute times lower result sizes 
compare results tables 
comparing worker wan results worker homogeneous results shows wan case half workers running faster processors communicating wan link performs significantly better regions significantly worse 
general say wan cases advantage compute times larger result sizes smaller 
output unit block size compute time sec bytes table execution times pep emulator program running homogeneous clusters workstations linked wan connection range application computation communication parameters 
number processors master active net cluster number units execution times measured seconds example compute time seconds result size bytes wan execution times nearly third local homogeneous cluster case 
compute time equal seconds size equal bytes wan execution times twice local homogeneous cluster case 
shows graphical representation regimes derived values table 
test applications mapped application parameters corresponding machine types wan environment 
note shows relatively smaller regimes computation limited behavior rela tively larger areas mixed limitation behavior saw homogeneous heterogeneous cluster cases 
test applications map completely computation limited regimes shown 
povray npb ep application parameters computation limited regime running machines cetus cluster 
comparing worker wan results worker homogeneous results table shows regions operation employing local processors cetus cluster combination faster processors active net cluster beneficial cases simply local workers cetus cluster formed better 
case compute time equal seconds result size equal bytes worker homogeneous case produced execution times nearly times wan case 
performance communication bound regions basically worker worker homogeneous cases 
results suggest significantly faster processing resources wan link beneficial mw output unit block size compute time sec bytes table execution times pep emulator program running homogeneous clusters workstations linked wan connection range application computation communication parameters 
number processors master cetus cluster number units execution times measured seconds output unit block size compute time sec bytes table execution times pep emulator program running homogeneous clusters workstations linked wan connection range application computation communication parameters 
number processors master cetus cluster number units execution times measured seconds application problems higher ratios computation communication requirements 
table show results running pep master process worker processes located cetus cluster worker processes located active net cluster 
configuration shows performance scales proportionally number processors compute bound regions compared worker wan results 
note worker wan configuration outperforms worker worker homogeneous cluster performance narrow range operating conditions compute times greater seconds result sizes bytes 
data transfer size bytes computation limited communication limited fb mixed limitations overhead limited computation time sec mapping application parameters graph behavioral regimes homogeneous clusters connected wan link master process located cetus cluster 
active net machine behavior application identified lower case letters cetus machine behavior identified upper case letters mandelbrot povray npb ep tomography dot ga tsp 
note dot ga tsp cycle oriented data transfers increase proportion number worker processes data transfer sizes shown average transfer sizes unit assuming workers 
mw application performance wan connection consider results experiments characterizing performance mw applications running homogeneous clusters workstations connected wan link 
homogeneous clusters active net cetus clusters described earlier 
wan link internet connection ucsd utk campuses 
experiments concentrate quantifying effects certain scheduling approaches improving application performance running environments contain ing wan connections 
mandelbrot mandelbrot application shows execution time scales increasing numbers worker processes homogeneous clusters linked wan connection 
see results graph different distribution strategies note dividing problem units employing decreasing block size strategy produces significantly lower execution times tested numbers workers 
example processes case ss strat egy units resulted longer execution times case fac strategy units 
compare execution times cetus cluster see wan case better fewer worker processes lower execution times worker processes ally produces nearly performance number worker processes reaches 
homogeneous active net cluster generally produces execution times order magnitude lower best wan results 
shows effects different distribution strategies different unit sizes corresponding dividing output image pieces 
results show varying size blocks significantly change resulting execution time sizes choice distribution strategy significant impact achievable application performance 
important note alternative strategies perform better ss fixed strategies tss strategy producing lower execution times ss computing units pixels 
povray povray application shows comparisons shown mandelbrot application 
see alternative strategy ss concert dividing problem units results signif reductions achievable execution time 
percentage improvement execution time increases number worker processes increases reaching reduction execution time worker processes 
interestingly comparing execution times homogeneous cases notice best wan times comparable produced faster active net cluster better cetus cluster 
higher ratio computation communication execution time sec execution time sec mw application scalability mandelbrot utk ucsd units ss utk ucsd units fac processors scalability mw application distribution mandelbrot ss fsc tss gss fac fixed distribution method distribution utk ucsd units utk ucsd units mandelbrot set generator program wan environment 
mandelbrot povray appears better suited achieving benefits resources connected wan links 
agrees results shown shows mandelbrot falls communication limited regime povray falls computation limited regime tested wan environment 
shows effects different distribution strategies different unit sizes 
results povray differ mandelbrot advantages alternative distribution strategies ss fixed show case problem divided units strongly appear image broken units computation 
tss strategy produces lower execution times ss povray computing units 
npb ep npb ep application shows comparisons shown mandelbrot povray applications 
similar previous cases fac alternative strategy concert larger numbers units results signifi cantly lower execution times 
case reduction nearly uniform range worker processes tested consistently yielding lower execution times 
comparing results homogeneous cluster cases notice resulting execution times wan system active net cetus clusters 
consistent performance processors contributes performance directly proportional natural computational capacity 
shows effects different distribution strate gies execution time performance shows results different unit sizes 
graph shows dividing problem smaller plentiful units aid performance enabling alternative distribution strategies 
example increasing number units provides measurable improvements fsc tss gss fac alternative strategies reduces perfor mance ss fixed 
increasing number units produces significant benefits alternative strategies shows erosion perfor mance ss fixed strategies 
ss particularly sensitive increasing number execution time sec execution time sec mw application scalability povray utk ucsd units ss utk ucsd units fac processors scalability mw application distribution povray ss fsc tss gss fac fixed distribution method distribution utk ucsd units utk ucsd units povray ray tracing program wan environment 
units execution time doubles going units 
best execution time produced gss strategy case units lower time produced fixed strategy allocating number units 
tomography tomography application shows results testing effect execution time increasing numbers worker processes test wide area environment 
trials performed environment ss fixed distribution strategy master process half worker processes located cetus cluster utk half worker processes located active net cluster ucsd 
results similar tests performed dot application heterogeneous cluster environment described section fixed distribution strategy outperforming ss strategy large margins cases tested 
examination testing logs showed reason superior performance fixed strategy heterogeneous environment process serialization ss strategy case causing nearly equal numbers units assigned processes clusters regardless relative computational capacities 
units amounts proportional expected capacities processor resources fixed strategy avoids problems process serialization 
result tomography wide area test produced execution times lower fixed strategy compared execution times ss strategy sixteen worker processes 
shows comparison tomography performance variety distribution strategies sixteen worker processes test wide area envi ronment 
trials master host worker processes located cetus cluster remaining worker processes located active net cluster 
results clearly show fixed preferred distribution strategy tomography wide area environment delivering lower execution times second best gss strategy 
occurs units divided sixteen worker processes non static distribution strategies quickly degenerate behaving ss suffer process serialization performance problems ss strategy 
execution time sec execution time sec mw application scalability npb ep utk ucsd units ss utk ucsd units fac processors scalability mw application distribution npb ep ss fsc tss gss fac fixed distribution method distribution utk ucsd units utk ucsd units utk ucsd units nas ep benchmark program wan environment 
execution time sec execution time sec mw application scalability tomography utk ucsd ss utk ucsd fixed processors scalability mw application distribution tomography ss fsc tss gss fac fixed distribution method distribution utk ucsd wan tomography program wan environment 
dot dot application show comparison performance obtained various master worker organizations master process worker processes separated wan connection 
previous results dot running neous clusters shown cycle oriented data transfers dot limit scalability local networks fewer worker processes produce lower execution times 
markedly decreased bandwidth increased latency values wan connections severe performance bottlenecks communication heavy applications dot 
cycle oriented applications single level mw organization produce total network traffic proportional number worker processes worker process located opposite side wan connec tion master process causes additional network traffic flow wan link 
low capacity wan links additional traffic major constraint application performance 
illustrates problem possible approach reducing effects 
graph represents performance dot master process hosted machine cetus cluster utk worker processes hosted machines active net cluster ucsd 
shows different configurations tested single level worker level worker single level worker level worker 
single level configurations master process communicate worker processes wan link 
level configurations master process communicate worker process majority network transfers occurs local network linking worker processes 
results show increasing number worker processes single level configurations results higher execution times illustrating performance limiting effects wan link dot 
results level configurations shows reducing traffic flow wan link single intermediate process results significant reductions execution time process available computation 
level configuration worker processes delivered lower execution times single level configuration number machines level configuration workers delivered lower execution times comparable single level configuration 
ad execution time sec mw hierarchical organization dot utk ucsd wan mw tree structure dot program different master worker tree configurations wan environment 
level configurations allowed worker case produce reduction execution time compared worker case 
summary findings chapter experimental results trials run different combina tions mw applications test environments 
observations intended summarize important findings trials 
superior mw application performance comes balanced combination powerful processors high bandwidth low latency network connections available 
test results showed running mw applications fast modern workstations locally networked environment active net cluster ucsd outperformed applications running number slower workstations locally networked environment cetus cluster utk 
impact running applications significantly faster meant application data transfers performed shorter periods time network tested mw configurations dot application wan link 
capacity resulting reduced ability scale application performance processors included test environments 
test environment identified regions operation parameter space defined application characteristics unit compute time unit data transfer size 
defined regions correspond operating condi tions application performance predominantly compute bound overhead bound communication bound bound combination factors 
applications falling compute bound performance region run scalability number processors tested deliver predictable levels performance 
applica tions falling communication bound mixed factor region required extra scheduling attention achieve performance potential 
increased variability resource capacities heterogeneous cluster environment resulted need additional scheduling attention solve problems master host selection load balancing serious issues homogeneous environments 
applications mandelbrot povray balancing amount potential load imbalance increased overhead time data transfers shown varying size number units important achieving performance heterogeneous environment 
loss efficiency due process serialization potentially serious problem highly heterogeneous environment applications tomography involve relatively lengthy data transfer times 
results wan linked cluster environment verified limitations imposed high latencies reduced bandwidth wan links effectively limited number scheduling options beneficially employed 
far capable processing resources wan link purely local resources shown provide beneficial results wide range application parameters 
performance benefits shown possible capable processing resources connected wan link narrower range application parameters ratio compute time communication size tended large 
effectiveness distribution strategies helped reduce number required data transfers evident applications mandelbrot povray wan environment 
hierarchical mw organization reduce number data streams passing wan link shown boost achievable performance cycle oriented dot application wan environment 
support thesis statement variety scheduling options nec essary achieving truly portable mw application performance examples applications required different scheduling options different environments max performance 
examples included mandelbrot application running best ss distribution strategy homogeneous clusters favoring alternative strategies tss gss fac heterogeneous wan connected clusters 
applications exhibited performance problems unexpected ways environments required specialized solutions overcome 
case dot application explicitly serializing return output data worker processes cycle decreased execution times reducing network congestion sacrifice par transfers 
case tomography running heavily heterogeneous wan connected clusters fixed distribution strategy best far strategy homogeneous cluster cases performed worst 
case cycle oriented mw applications dot ga tsp multi level organization worker processes enhance application scalability total communication traffic proportional number workers single level organization 
chapter goals dissertation investigate level scheduling effort important achieving portable mw application performance broad domain general distributed computing environments develop performance prediction mod els mw applications provide accurate estimations dynamic multi user gdc environments 
attempted demonstrate required scheduling func tionality effectively separated application specific efforts implemented portable reusable component flexible mw application development frame 
conclude discussing possibilities contributions course research 
happens research process answering set questions leads questions remain answered 
briefly discuss open questions may form basis research efforts extending dissertation branching possible new lines inquiry 
resource constraints question additional physical resource characteristics investigated constraints mw application performance 
research concentrated processing communication resources deriving flow model application performance 
execution time chosen metric evaluating application performance 
open question physical characteristics ram memory size filesystem performance incorporated constraint oriented performance model provide improved model ing accuracy certain types mw applications 
different sets resource properties called different performance metric average application throughput evaluate scheduling effectiveness 
acquiring dynamically changing environment parameters question improve process acquiring dynamically changing environ ment parameter values cpu availability network bandwidth 
currently rely external monitoring prediction services net weather service provide steady state approximations dynamically changing environment parameter values 
approach incorporate application specific information long application expects resource may important making accurate predictions 
open question bet ter resource behavior predictions additional information realized improvements material impact mw scheduling performance 
representation dynamically changing parameter values question representations steady state values accurate modeling parameter values 
question viewed continuation line inquiry suggested previous question concerned varying parameter values acquired 
information acquired converted representative forms suitable scheduling activities 
current approach converts parameter values single value representing expected steady state behavior 
certain time varying parameter values open question alternative value representations time dependent functions stochastic values improve accuracy steady state values ways lead better mw application scheduling 
automatic scheduling regime detection question detection scheduling regimes mw applications automatic 
research shown scheduling regimes determined spe cific environments sets application characteristics process currently requires significant amount data acquisition application level benchmarking test ing 
desired automated way predict boundaries scheduling regimes different gdc environments methods quickly determine regimes particular applications fall 
step required enable fully auto matic selection appropriate scheduling techniques mw applications targeting gdc environments 
automatic selection distribution strategy question automate selection distribution strategies produce shortest possible execution times 
experimental results showed different types distribution strate gies produce significantly better results specific combinations application requirements resource conditions 
clear strategies formed better worse experimental trials run revealed details actual application behavior 
investigate better predictions allocation strategy effectiveness derived ex modeling parameters addition modeling extensions compiler analysis application source code 
hierarchical mw configurations address resource locality issues question multiple level hierarchical arrangements allow mw approaches exploit locality properties commonly behavior applications running hi organized architectures 
applications running distributed resources distinct hierarchical levels resource capabilities locality sensitive performance characteristics 
example hierarchical environment multiple clusters smp machines groups processors lowest level linked shared memory channels processors higher level linked high speed network interfaces highest level linked internet style connections 
remains open question hierarchical mw organization effective enabling increased efficiency improved scalability tailoring application behavior different levels match local capabilities 
additional support services question additional services service providers incorporated general mw application framework 
services provided aps interfaces selected enable deployment amwat applications platforms minimal requirements ssh unix sockets allowing transparent capable service providers globus mpi available 
fully expected time additional needs services expanding choices service providers create demand expanding number service options provided aps amwat applications 
example candidate added support globus heartbeat monitor monitoring remote process status 
activity expansion aps support ongoing developments tools distributed computing 
benefit existing amwat applications access new service providers available requiring changes legacy code 
enabling mw framework compiler target question mw application programming model better target automatic program compilers 
amwat separate application specific functions away general mw application support functions succeeded simplifying job developers interested producing mw applications portable performance characteristics 
exists requirement amwat explicitly specify communication routines passing data results processes 
natural extension current development mechanisms compiler preprocessing directly target amwat automatically generate communication specific routines application data transfers 
amwat aware compilation reduce programming effort mw applications little application specific functions necessary serial codes reduce incidence programming errors related implementing communication specific functions 
extending mw flow model application types question apply flow modeling concepts forms distributed appli cations 
addition mw applications common approaches ing distributed computing problems 
examples approaches include stealing spmd single program multiple data data flow parallel pipeline applications 
different approaches utilize sets processing networking resources differ types data transfer patterns employed 
open question data transfer patterns create constraint performance models different application styles manner similar flow performance model mw applications 
contributions number contributions accomplished course research conducted support original thesis 
restate list contributions introduced chapter 
developed flow model mw application performance 

developed effective network view graph representation distributed computing environments 

demonstrated flow method mw resource selection 

developed specialized mw scheduling techniques 

implemented portable reusable performance oriented mw scheduler 

designed mw application development framework 

identified characteristic scheduling regimes mw applications different cluster environments 

characterized performance different distribution strategies actual mw applications variety cluster environments 
appendix example application source code example amwat master worker application program amwat example application file amwat example author gary shao include stdio include global variables int float static int static int function float sumarray int int float data int float sum sum data return sum amwat data transfer functions int app int recvcount int unit int recvcount unit comm results unit return int app int sendcount int unit int sendcount unit comm results unit return int app int comm comm return int app int comm comm return amwat application initialization function int app initialize int argc char argv int int return amwat cycle setup function int app unsigned int int int return amwat calculation function typedef enum failed succeeded app compute unit result unit sumarray unit return succeeded amwat result processing function typedef enum result continue result cycle completed result app completed result app int int return result cycle completed return result continue return result continue amwat cycle processing function typedef enum cycle failed cycle succeeded cycle app completed cycle app int int return cycle app completed return cycle succeeded amwat application finalization function int app finalize int int printf result result return example pvm master worker application example master program static char id pvm tex exp pvm version parallel virtual machine system university tennessee knoxville tn 
oak ridge national laboratory oak ridge tn 
emory university atlanta ga authors dongarra fagg fischer geist kohl manchek papadopoulos scott sunderam rights reserved notice permission copy modify distribute software documentation purpose fee granted provided copyright notice appear copies copyright notice permission notice appear supporting documentation 
institutions emory university oak ridge national laboratory university tennessee authors representations suitability software purpose 
software provided express implied warranty 
pvm version funded part department energy national science foundation state tennessee 
include stdio include pvm define slave main int task id int tids slave task ids int nproc float data result struct enroll pvm pvm set number slaves start pvm config nproc printf spawning worker tasks 
nproc start slave tasks pvm spawn char nproc tids nproc printf trouble spawning slaves printf aborting 
error codes nproc printf tid tids pvm kill tids pvm exit exit printf successful user program initialize data data data broadcast initial data slave tasks pvm pvm nproc pvm tids nproc pvm pvm data pvm mcast tids nproc wait results slaves nproc pvm recv pvm pvm result nproc printf result result program finished exit pvm stopping pvm exit example worker program static char id pvm tex exp pvm version parallel virtual machine system university tennessee knoxville tn 
oak ridge national laboratory oak ridge tn 
emory university atlanta ga authors dongarra fagg fischer geist kohl manchek papadopoulos scott sunderam rights reserved notice permission copy modify distribute software documentation purpose fee granted provided copyright notice appear copies copyright notice permission notice appear supporting documentation 
institutions emory university oak ridge national laboratory university tennessee authors representations suitability software purpose 
software provided express implied warranty 
pvm version funded part department energy national science foundation state tennessee 
include stdio include pvm float sumarray int int float data simple example slaves sum data times slave number int float sum sum data return sum main int task id int tids task ids int nproc master float data result float sumarray enroll pvm pvm receive data master pvm recv pvm nproc pvm tids nproc pvm pvm data determine slave am nproc nproc tids break calculations data result sumarray data send result master pvm pvm pvm result master pvm parent pvm send master program finished 
exit pvm stopping pvm exit bibliography allen goodale seidel cactus computational collaboratory enabling technologies relativistic astrophysics toolkit solving pde communities science engineering 
proceedings frontiers seventh symposium frontiers massively parallel computation annapolis maryland feb 
bailey barton browning carter dagum frederickson schreiber simon nas parallel benchmarks 
international journal supercomputer applications 
bailey barton browning carter dagum frederickson schreiber simon nas parallel benchmarks 
rnr technical report rnr nasa ames research center nas systems division 
optimized model integrate reduced dimensionality schrodinger equations distributed memory architectures 
generation computer systems 
baru moore rajasekar wan sdsc storage resource broker 
proceedings cascon conference toronto canada dec 
livny deploying high throughput computing cluster 
high performance cluster computing buyya ed vol 

prentice hall ptr may ch 

raman livny high throughput monte carlo 
proceedings ninth siam conference parallel processing scientific computing san antonio texas mar 
raman livny high throughput monte carlo 
proceedings ninth siam conference parallel processing scientific computing san antonio texas mar 
berman high performance schedulers 
grid blueprint new computing infrastructure foster kesselman eds 
morgan kaufmann publishers july ch 

berman wolski scheduling perspective application 
proceedings fifth ieee symposium high performance distributed computing aug pp 

bhatt chung leighton rosenberg optimum strategies cycle stealing networks workstations 
ieee transactions computers may 
blumofe joerg leiserson randall zhou cilk efficient multithreaded runtime system 
journal parallel distributed computing 
bush power wind production flow solver alliance 
aiaa aiaa 
www nasa gov www aiaa aiaa html 
buyya abramson giddy nimrod architecture resource management scheduling system global computational grid 
proceedings fourth international conference exhibition high performance computing asia pacific region beijing china may pp 

casanova berman wolski apples parameter sweep template user level middleware grid 
proceedings supercomputing sc dallas texas nov 
cheng performance analysis hierarchical task queue organization parallel systems 
proceedings th international conference distributed computing systems yokohama japan june pp 

comer reliable stream transport service tcp 
internetworking tcp ip third ed vol 

prentice hall upper saddle river new jersey ch 

earnshaw jones fractals chaos 
springer verlag new york ny 
fishwick sim version 
department computer information science engineering university florida gainesville fl 
czajkowski foster karonis kesselman marin smith tuecke resource management architecture metacomputing systems 
job scheduling strategies parallel processing 
ipps spdp workshop proceedings orlando florida mar pp 

donaldson berman program speedup heterogeneous computing network 
journal parallel distributed computing june 
evans minieka optimization algorithms networks graphs second ed 
marcel dekker ch 
pp 

roberts surveying molecular interactions dot 
proceedings acm ieee supercomputing conference san diego california dec pp 

su wolski berman adaptive performance prediction distributed data intensive applications 
proceedings acm ieee scc conference high performance networking computing portland oregon nov 
fitzgerald foster kesselman von laszewski smith tuecke directory service configuring high performance distributed computations 
proceedings th ieee international symposium high performance distributed computing portland oregon aug pp 

ford fulkerson flows networks 
princeton university press princeton new jersey 
foster geisler kesselman tuecke managing multiple communication methods high performance networked computing systems 
journal parallel distributed computing jan 
foster kesselman globus metacomputing infrastructure toolkit 
international journal supercomputer applications 
foster kesselman computational grids 
grid blueprint new computing infrastructure foster kesselman eds 
morgan kaufmann publishers july ch 

wolski synchronizing network probes avoid measurement intrusiveness network weather service 
proceedings ninth international symposium high performance distributed computing pittsburgh pennsylvania aug pp 

gannon stuckey balasubramanian akman diwan govindaraju developing component architectures distributed scientific problem solving 
ieee computational science engineering apr 
geist beguelin dongarra pvm parallel virtual machine users guide tutorial networked parallel computing 
mit press cambridge massachusetts dec 
glassner ed 
ray tracing 
academic press aug 
globus project 
globus communication globus toolkit 
globus website 
www globus org toolkit communication html 
kulkarni yoder enabling framework master worker applications computational grid 
proceedings ninth international symposium high performance distributed computing pittsburgh pennsylvania aug pp 

grimshaw wulf legion vision worldwide virtual computer 
communications acm 
gropp lusk skjellum mpi portable parallel programming message passing interface second ed 
mit press cambridge massachusetts dec 
gropp lusk thakur mpi advanced features message passing interface 
mit press cambridge massachusetts dec 
hagerup allocating independent tasks parallel processors experimental study 
journal parallel distributed computing dec 
harchol balter crovella choosing task assignment policy distributed server system 
journal parallel distributed computing nov 
robinson official home page 
www com index htm index htm 
hayes apples process tracker 
online documentation 
apples ucsd edu apt 
hey lancaster development performance prediction 
international journal high performance computing applications aug 
hummel schmidt uma wein load sharing heterogeneous systems weighted factoring 
proceedings th annual acm symposium parallel algorithms architectures padua italy june pp 

hummel schonberg flynn factoring method scheduling parallel loops 
communications acm aug 
jacobson pathchar tool infer characteristics internet paths 
mathematical sciences research institute msri april 
slides available ftp ftp ee lbl gov pathchar 
kay internet measurement tool survey 
www caida org tools taxonomy html 
kruskal weiss allocating independent subtasks parallel processors 
ieee transactions software engineering oct 
kumar gupta karypis performance scalability parallel systems 
parallel computing 
benjamin cummings publishing redwood city california ch 
pp 

kumar ramesh rao parallel best search state space graphs summary results 
proceedings national conference artificial intelligence aug pp 

lawler lenstra rinnooy kan shmoys eds 
traveling salesman problem guided tour combinatorial optimization 
john wiley sons chichester west sussex new york new york sept 
lowekamp miller sutherland gross steenkiste resource query interface network aware applications 
proceedings seventh international symposium high performance distributed computing july 
lowekamp hallaron gross direct queries discovering network resource properties distributed environment 
cluster computing 
lucco dynamic scheduling method irregular parallel programs 
proceedings acm sigplan conference programming language design implementation san francisco california june pp 

modeling performance parallel programs 
technical report computer science department university rochester rochester new york june 
mm scalable parallel implementation penn state mesoscale model mm 
parallel computing dec 
national center supercomputer applications 
welcome alliance 
access ncsa uiuc edu index alliance html 
nishikawa steenkiste general architecture load balancing distributed memory environment 
proceedings th international conference distributed computing systems pittsburgh pennsylvania may pp 

atherton clarke layered approach characterization parallel systems performance prediction 
performance evaluation parallel systems 
layered approach parallel software performance prediction case study 
proceedings conference delft netherlands june pp 

stal architectural view distributed objects components corba java rmi com dcom 
software concepts tools 
polychronopoulos kuck guided self scheduling practical scheduling scheme parallel supercomputers 
ieee transactions computers dec 
persistence vision raytracer 
persistence vision development team 
www povray org 
pruyne livny parallel processing dynamic resources 
proceedings workshop job scheduling strategies parallel processing santa barbara california apr pp 

roberts freeman olson electrostatic orientation electron transfer complex 
journal biological chemistry july 
fisher understanding dce 
reilly associates sept 
saltz sussman graham demmel baden dongarra programming tools environments 
communications acm nov 
schopf structural performance models high performance applications 
proceedings cluster computing conference atlanta georgia mar 
schopf berman performance prediction production environments 
proceedings merged international parallel processing symposium symposium parallel distributed processing orlando florida mar pp 

isern parallel distributed genetic algorithm traveling salesman problem 
proceedings high performance computing symposium hpc san diego california apr pp 

shao berman wolski effective network views promote distributed application performance 
proceedings international conference parallel distributed processing techniques applications las vegas nevada june 
shirazi hurson scheduling load balancing parallel distributed systems 
ieee computer society press los alamitos california 
carriero miller parallel computing approach genetic sequence comparison master worker paradigm communication 
computers biomedical research 
cirne frey berman wolski su kesselman young combining workstations supercomputers support grid applications parallel tomography experience 
proceedings th heterogeneous computing workshop cancun mexico may pp 

spring wolski application level scheduling gene sequence comparison 
proceedings th acm international conference supercomputing melbourne australia july 
wagner performance prediction modeling multicomputers 
proceedings th international conference distributed computing systems yokohama japan june pp 

foster kesselman lee von laszewski fault detection service wide area distributed computations 
proceedings th international symposium high performance distributed computing chicago illinois july pp 

stevens unix network programming second ed vol 

prentice hall ptr upper saddle river new jersey 
lieu lowekamp automatic node selection high performance applications networks 
proceedings seventh acm sigplan symposium principles practice parallel programming atlanta georgia may 
tang yew processor self scheduling multiple nested parallel loops 
proceedings international conference parallel processing aug pp 

ni trapezoidal self scheduling practical scheme parallel compilers 
ieee transactions parallel distributed systems jan 
valiant bridging model parallel computation 
communications acm aug 
van steen tannenbaum globe wide area distributed system 
ieee concurrency jan 
corba integrating diverse applications distributed heterogeneous environments 
ieee communications magazine feb 
weber distributed processing environment computer generated animation 
software practice experience 
weber optimal assignment customers parallel servers 
journal applied probability 
weissman zhao scheduling parallel applications distributed networks 
journal cluster computing 
whitt deciding queue join counterexamples 
operations research jan 
winston optimality shortest line discipline 
journal applied probability 
wolski dynamically forecasting network performance network weather service 
proceedings th high performance distributed computing conference aug pp 

wolski spring hayes network weather service distributed resource performance forecasting service metacomputing 
generation computer systems oct 
howes lightweight directory access protocol 
rfc mar 
draft standard 
zhang xu sun performance prediction implicit communication systems 
proceedings sixth ieee symposium parallel distributed processing dallas texas oct pp 


