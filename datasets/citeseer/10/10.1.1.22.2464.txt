cooperative learning neural networks particle swarm optimizers van den engelbrecht department computer science university south africa cs ac za engel cs ac za presents method employ particle swarms optimizers cooperative configuration 
achieved splitting input vector sub vectors optimized cooperatively swarm 
application technique neural network training investigated promising results 
keywords particle swarms cooperative learning optimization computing review categories particle swarm optimizers psos previously train neural networks generally met success 
advantage pso optimization algorithms relative simplicity 
aims improve performance basic pso partitioning input vector subvectors 
sub vector allocated swarm 
section brief overview psos followed discussion cooperative behavior implemented splitting technique section 
implementations applied neural network training function minimization described section followed results obtained various case studies section 
concluded ideas serve research directions 
particle swarms particle swarms described eberhart kennedy best classified heading evolutionary computing 
basic operational principle particle swarm reminiscent behaviour flock birds sociological behaviour group people 
various attempts improve performance baseline particle swarm optimizer 
done eberhart shi focus optimizing update equations particles 
improvements generally leads better performance large class problems 
improvements applied technique 
angeline selection mechanism genetic algorithms attempt improve general quality particles swarm 
approach lead improved performance problems somewhat worse performance 
kennedy cluster analysis modify update equations particles attempt conform center clusters attempting conform global best 
approach resulted improved performance problems slightly worse performance 
inside particle swarm swarm consists particles particle keeps track attributes 
important attribute particle current position dimensional vector corresponding potential solution function minimized 
position particle current velocity keeping track speed direction particle currently traveling 
particle current fitness value analogous population members ga obtained evaluating error function particle current position 
neural network implementation value corresponds forward propagation network assuming position vector corresponds weight vector network 
additionally particle remembers personal best position yielding lowest error potentially solutions guide construction new solutions 
swarm level best position particles recorded termination algorithm serve answer 
epoch particle accelerated personal best direction global best position 
achieved calculating new velocity term particle distance personal best distance global best position 
components personal global velocities randomly weighted produce new velocity value particle turn affect position particle epoch 
particle swarm parameters number factors affect performance pso 
firstly number particles swarm affects run time significantly balance variety particles speed fewer particles sought 
important factor convergence speed algorithm maximum velocity parameter 
parameter limits maximum jump particle step large value parameter result oscillation 
small value hand cause particle trapped local minima 
current research indicates constriction coefficient leads improved performance 
implementation maximum velocity parameter set default value neural network training sessions value suggested earlier works psos increased appropriately function minimization tests typically half range function 
aid convergence algorithm inertia term introduced attenuate magnitude velocity updates time 
attenuation factor linear function current epoch number 
swarm behavior particle fly direction better solution weighted random factor possibly causing overshoot target possibly finding new personal global minimum 
interaction particles swarm creates balance course staying close optimal solution 
type behavior ideal exploring large error surfaces especially relatively large maximum velocity parameter 
particles explore far current minimum population remembers global best solution 
solves problems gradient optimization techniques poor performance regions flat gradients 
appears particle swarm optimizer performs regions error surface highly perturbed case product unit networks 
important note pso guarantee solution partly due stochastic nature 
compared second order optimization algorithm scaled conjugate gradient descent pso typically lower minimum error larger maximum error number experiments performed 
cooperative learning driving force population solutions genetic algorithm competition different members population 
crossover solutions cooperate produce new solutions serves create new members compete top spot 
cooperative learning framework populations considered simultaneously 
solution vector split different populations rule simplest schemes allow overlap space covered different populations 
find solution original problem representatives populations combined form potential solution vector passed error function 
new dimension added survival game cooperation different populations 
concept easily applied psos 
form cooperation elements swarm share global best position 
multiple swarms splitting vector swarms inter swarm cooperative behavior emerges 
new problems introduced approach selection solution vector split parts part optimized swarm particles leaving possible vectors choose 
simplest approach select best particle swarm calculate particle best discussed 
note optimal choice analogous pairing highly fit members ga population highly fit members known detrimental effect training performance 
credit assignment analogous real world problem handing group projects students gets credit final product 
terms swarms credit swarm awarded combined vector built swarms results better solution 
simple solution give swarms equal amount credit 
section argument help explain splitting vector results improved performance 
steps forward step back consider dimensional vector constrained error surface function minimized 
vector components examined 
assume optimal solution problem requires components value say 
consider particle swarm containing particular vectors assume currently global best solution swarm drawn epoch 
assume components values respectively 
equivalent components vector values respectively 
epoch vector updated contains sub vector positions assumed function new yields smaller error value considered improvement 
valuable information contained middle component lost optimal solution requires sub vector 
reason behavior error function computed components vector updated new values 
means improvement components steps forward overrule potentially value single component step back 
way overcome behavior evaluate error function frequently example time component vector updated 
problem remains approach evaluation error function possible complete dimensional vector 
updating component values components vector chosen 
method doing just 
steps forward extreme single swarm consisted number dimensional vectors 
extreme different swarms representing single component dimensional vector 
mentioned error function evaluated dimensional vector representatives swarm combined form vector function evaluated 
simplest way construct vector best particle swarm particular component outlined 
create initialize dimensional psos repeat select best particle resp 
select th particle swarm construct vector error function set fitness particle swarm update best fitness necessary perform normal pso updates stopping criterion met pseudo code split swarm approach assuming particles swarm note algorithm keeps track optimal value component dimensional vector separate swarm 
implies error respect component non increasing function epoch number value specific component forgotten 
care taken keep track global minimum error 
combined vector lower error fitness values best particle swarms updated value particles participated creation improved vector 
solution credit assignment problem mentioned chosen implementation obtain results 
algorithm optimizes variable separately epoch cooperates variables epochs 
implementation optimization method described section involves trade increasing number swarms leads directly proportional increase number error function evaluations function evaluation required particle swarm epoch 
total number error function evaluations summarized number error function evaluations number swarms number particles swarm number epochs allowed training 
extreme example illustrates point consider single swarm dimensional vectors allowed train epochs 
keep number error function evaluations constant execution time system dimensional swarms allowed train epochs 
clearly swarm optimizer require epochs find solution assuming single swarm set able find solution epochs 
way increase number allowed epochs reduce splitting factor 
example splitting dimensional vector swarms allow epochs optimization succeed epochs 
neural network architectures neural network trained minimizing error function dimensional space see 
psos generate possible solutions dimensional space forward propagation neural network obtain value error function particle swarm 
error value directly particle fitness constructing particle lower error value synonymous learning network 
forward propagation network computationally expensive task aim find best possible solution limited number forward propagations network 
test effectiveness cooperative swarm configurations number tests performed layer feedforward neural networks 
network depicted input units sigmoidal hidden units linear output units 
architectures selected testing neural network architecture plain plain architecture takes weights different layers serializes dimensional vector optimized single particle swarm 
architecture baseline benchmarks 
layer split architecture splits network weights swarms 
swarm contains weights layer second swarm optimizes second layer weights 
architecture chosen assumption relative scales weights layers differ 
number allowed forward propagations kept constant halving number allowed epochs training 
split architecture takes serialized dimensional vector plain architecture splits half 
assumption splitting vector configuration happens leave part sufficient freedom maneuver split result optimal way split 
number epochs halved keep number error function evaluations constant 
architecture attempts build approach splitting weights function 
hidden output unit swarm created containing weights entering unit 
results dimensional vectors plus dimensional vectors total swarms 
applying clear large number hidden output units put architecture severe disadvantage number allowed epochs significantly reduced 
plain architecture allowed train epochs architecture allowed epochs 
function minimization architectures architectures test vector splitting approach function minimization environment plain case neural network experiments plain architecture single swarm consisting dimensional vectors 
serves baseline comparison 
architecture extreme opposite plain architecture splitting dimensional vector dimensional swarms 
called lieu avoid confusion definition 
case studies neural network experiments data sets section obtained uci machine learning repository 
experiments performed runs architecture 
tables appearing section show classification error expressed percentage followed confidence interval width 
table lists total error train test training error 
note training error significant attempt prevent fitting 
iris iris classification problem consists patterns falling classes 
considered simple classification example classes linearly separable 
input hidden output network architecture 
table clear way split architectures perform significantly better plain architecture 
early training difference order settling 
presents training errors graphically 
interesting note slope rate improvement caused iterations split architectures steep plain approach 
architecture leading small amount 
table shows results convergence test 
training session said converge fails improve consecutive epochs 
second column table lists average number error function evaluations forward propagations network training network converged 
way split type epochs error total dev 
error train dev 
plain plain table iris plain vs split architectures type fwd 
props 
error total dev 
error train dev 
plain table iris convergence tests forward propagations plain iris classification error train architectures roughly twice function evaluations plain method lower classification errors result 
architecture completely failed converge probably means threshold convergence test sensitive 
failure converge error epochs exceeds approach 
ionosphere ionosphere problem higher input dimension iris problem output classes 
results input hidden output network architecture 
due high input dimension versus output dimension architecture highly unbalanced ratio split 
having train dimensional vector epochs vs dimensional vector epochs plain explains architecture performs worse plain method 
method ground narrowly beating plain architecture 
forward propagations plain classification error train cally illustrates fact 
architecture falls plain architectures 
probably caused higher interdependency variables weights network may correlated 
glass glass problem difficult solve possibly severely skewed class distribution 
input hidden output unit network perform experiments 
table shows method outperformed architectures 
architecture relatively balanced ratio performs similarly sibling 
perform beating architectures low epoch counts drawing high counts 
shows dramatically plain network improved going epochs epochs 
type epochs error total dev 
error train dev 
plain plain table plain vs split architectures type epochs error total dev 
error train dev 
plain plain plain table glass plain vs split architectures forward propagations plain glass classification error train type epochs error total dev 
plain plain table rastrigin function plain vs split architectures function minimization experiments including pure function minimization problems possible investigate properties split swarms detail information available nature error function opposed error function neural network classification problem 
rastrigin function rastrigin function quite easy optimize basically dimensional parabola cosine bumps surface 
formally cos px global minimum 
cross component products possible minimize function component wise 
seen table architecture decidedly superior yielding errors orders magnitude smaller plain architecture 
type epochs error total dev 
plain plain table griewangk function plain vs split architectures benefit approach reduced execution time 
note number error function evaluations approaches overhead error function evaluations reduced architecture deals smaller vectors 
results effective speed factor number error function evaluations 
griewangk function griewangk function easily minimized component wise contains product term cos global minimum 
table shows griewangk function having inter dependencies component variables benefits architecture reduction factor 
example serves characterize split swarm approach optimization problems low interdependency vector components lend split swarm techniques problems high inter dependency tend show little gain 
splitting vector optimized multiple swarms neural network training function minimization appears improve performance problems tested far 
possible explanation phenomenon splitting leads finer grained credit assignment reducing chance neglecting possibly solution specific component vector 
method plain architecture neural network tests convincing victory ionosphere problem 
due high degree interdependency weights network function minimization experiments shows interdependent variables tend reduce effectiveness split approach 
observed results split architectures sensitive degree inter dependency variables compared plain architecture 
unsplit group split swarms conjunction possible gain best worlds 
split swarm continue usual epoch vector formed combining best split swarms injected unsplit swarm serving new attractor especially lower error value previous best particle unsplit swarm 
direction find critical splitting factor number disjoint swarms reduction allowed number epochs fixed error function evaluation budget outweighs benefit having independent sub vectors 
interesting see universal constant governing behavior 
lastly different methods addressing credit assignment selection problems investigated 
angeline 
selection improve particle swarm optimization 
proceedings ijcnn pages washington usa july 
blake keogh merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
russ eberhart kennedy 
new optimizer particle swarm theory 
proc 
sixth intl 
symposium micro machine human science pages nagoya japan 
ieee service center 
russ eberhart shi 
comparing inertia weights constriction factors particle swarm optimization 
proceedings congress evolutionary computing pages 
russ eberhart pat simpson roy 
computational intelligence pc tools chapter pages 
ap professional 
ap engelbrecht ismail 
training product unit neural networks 
stability control theory applications 
kennedy 
improving particle swarm performance cluster analysis 
proceedings congress evolutionary computing pages 
mitchell potter kenneth de jong 
cooperative coevolutionary approach function optimization 
third parallel problem solving nature pages jerusalem israel 
springer verlag 
shi russ eberhart 
modified particle swarm optimizer 
proceedings ijcnn pages washington usa july 
van den 
particle swarm weight initialization multi layer perceptron artificial neural networks 
development practice artificial intelligence techniques pages south africa september 

