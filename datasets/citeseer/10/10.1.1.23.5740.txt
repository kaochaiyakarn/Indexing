intrinsic stabilization output rates spike hebbian learning richard kempter center integrative neuroscience university california san francisco san francisco ca usa kempter phy edu gerstner swiss federal institute technology lausanne laboratory computational neuroscience di ch lausanne epfl switzerland gerstner epfl ch leo van hemmen physik department technische universit unchen bei unchen germany leo van hemmen ph tum de neural comput 
press submitted sept study analytically model long term synaptic plasticity synaptic changes triggered presynaptic spikes postsynaptic spikes time di erences presynaptic postsynaptic spikes 
changes due correlated input output spikes quanti ed means learning window 
show plasticity lead intrinsic stabilization mean ring rate postsynaptic neuron 
subtractive normalization synaptic weights summed presynaptic inputs converging postsynaptic neuron follows addition mean input rates mean input correlations identical synapses 
integral learning window positive ring rate stabilization requires non hebbian component component needed integral learning window negative 
negative integral corresponds anti hebbian learning model slowly varying ring rates 
spike learning strict distinction hebbian anti hebbian rules questionable learning driven correlations time scale learning window 
correlations presynaptic postsynaptic ring evaluated piecewise linear poisson model noisy spiking neuron model refractoriness 
negative integral learning window leads intrinsic rate stabilization positive part learning window picks spatial temporal correlations input 
hebbian learning hebb synaptic plasticity driven correlations pre postsynaptic activity thought important mechanism tuning neuronal connections development particular development receptive elds computational maps see 
von der malsburg sejnowski kohonen linsker sejnowski tesauro mackay miller perrone miller review see wiskott sejnowski 
known simple hebbian rules may lead diverging synaptic weights weight normalization turns important topic kohonen oja miller mackay miller 
practice normalization weights imposed explicit rescaling weights learning step constraint summed weights constant explicit decay term proportional weight 
simulation studies spike learning gerstner kempter song xie seung kempter intrinsic normalization properties synaptic plasticity 
explicit normalization step constraint summed weights needed 
face question understand ndings 
preliminary arguments intrinsic normalization occurs spike learning rules gerstner kempter song 
study normalization properties detail 
particular show spike plasticity realistic learning windows procedure continuously strengthening weakening synapses automatically lead normalization total input strength postsynaptic neuron competitive self organized process stabilization output ring rate 
order phrase problem normalization general framework start rate learning rule form dt corr seen expansion local adaptation rules second order rates see 
bienenstock linsker sejnowski tesauro 
de nition rate 
time scale learning set coecients corr may general depend weights example oja rule oja corr coecients vanish linear neurons weight vector converges unit vector 
study learning rules form coecients depend weights introduce upper lower bounds weights dw dt max excitatory synapse exclude run away individual weight values 
rst issue argue focus normalization weights linear form quadratic form narrow 
broad class learning rules naturally stabilize output rate weights 
example consider learning rule form dt constant rates 
equation special case eq 

excitatory synapses increases precisely wn increasing function learning stops approaches xed point stable 
special case input lines equal rate independent stabilization mean output rate implies normalization sum section argument precise 
disadvantage learning rule eq 
correlation coecient corr 
pure rate description learning rule classi ed anti hebbian hebbian 
second issue show section spike learning rules strict distinction hebbian anti hebbian learning rules questionable 
learning rule realistic time window levy stewart zhang bi poo feldman anti hebbian time averaged sense pick positive hebbian correlations input output spikes gerstner kempter 
correlations input output spikes enter learning dynamics calculated linear poisson neuron noisy spiking neuron refractoriness 
spike rules open possibility direct comparison model parameters experiments 
theory spike learning rules developed gerstner ruf schmitt kempter roberts kistler van hemmen xie seung review see van hemmen 
spike learning rules closely related rules sequence learning herz herz van hemmen gerstner abbott blum gerstner abbott idea asymmetric learning windows exploited 
section theory spike learning outlined applied problem weight normalization rate stabilization 
input scenario consider single neuron receives input synapses weights index synapse 
synapse input spikes arrive stochastically 
spike description section model input spike train synapse inhomogeneous poisson process rate 
rate description section input spike train replaced continuous rate variable 
focus di erent input scenarios 
scenarios input rates de ned statistical ensembles describe 
static pattern scenario static pattern scenario standard framework theory unsupervised hebbian learning hertz 
input ensemble contains patterns de ned vectors ir pattern label time discretized intervals 
time interval 
new pattern chosen randomly ensemble patterns applied input application pattern input rate synapse temporal averaging time yields average input rate synapse dt long intervals 
average input rate constant nd denote temporal averaging long intervals normalized correlation coecient input ensemble static ij 
static ji explicit example static pattern scenario appendix 
input correlations play major role theory unsupervised hebbian learning hertz 
translation invariant scenario developmental learning natural assume translation invariant scenario miller 
scenario mean input rates synapses constant time synapse actual rate time dependent time scale 
corr mean 
words correlation function de ned ij 

ji supposed vanish jsj 
corr 
deviation mean 
essential hypothesis correlation function translation invariant ij ji jj suppose example ensemble stimuli consists objects patterns typical size spanning input pixels correlation function spatial width order objects moved randomly equal probability arbitrary direction stimulus array temporal part correlation function symmetric ji jj ji jj 
static pattern scenario translation invariant scenario able simplify analysis hebbian learning rules considerably rate learning section spike learning section 
plasticity rate description section start preliminary considerations formulated level rates analyze rate description proper 
aim show learning dynamics de ned equation yield stable xed point output rate 
order keep discussion simple possible focus mainly linearized rate model focus input scenarios section 
learning rule eq 

synapses assumed type coecients corr depend synapse index usual theory hebbian learning assume slow time scale learning fast time scale uctuations input neuronal output separated averaging time instance assume 
static pattern scenario 
corr translation invariant scenario 
large implies weight changes time interval duration small compared typical values weights 
right hand side self averaging kempter respect randomness time synaptic changes driven statistically temporally averaged quantities dt corr 
corr 

hebbian learning synapse driven correlations preand postsynaptic neuron term right hand side cf 
sejnowski sejnowski tesauro hertz 
term second order 
small compared term proportional 
really 
speculate moment ideal neuron mean output rate attracted operating point fp corr typical mean input rate identical synapses 
dominant term right hand side covariance term 

terms right hand side eq 
cancel 
idea xed point rate fp precise subsections 
piecewise linear neuron model rst example study piecewise linear neuron model rate function constant slope activation function 
normalization ensures mean output rate stays number synaptic inputs identical input rate increased 
case ensure explicitly non negative 
introduced notation 
assume argument inside square brackets positive drop brackets treat model strictly linear 
calculation check consistency 
check lower upper bounds max discussion uence lower upper bounds individual weights stabilization output rate see section 
learning assumed slow 
corr weights change fast time scale input uctuations 
correlation term 
evaluated constant de nition 
de nition input correlations ij eq 
obtain 
ij want derive conditions mean output rate approaches xed point fp de ne average correlation ij require average correlation independent index condition may look arti cial fact natural assumption 
particular translation invariant scenario eq 
holds 
eqs 
eq 
multiply sum rearranging result obtain dt fp xed point fp corr time constant corr xed point fp asymptotically stable 
translation invariant scenario mean input rates synapses xed point reduces fp ca corr corr generalization eq 
case non vanishing correlations 
require fp assure linear regime piecewise linear neuron model 
excitatory synapses require addition weights positive 
realizable xed point rate fp maxf hand weight values bounded max xed point smaller max reached 
learning dynamics weights reached upper bound 
stability xed point requires 
requirements de ne conditions parameters corr explore examples 
note xed point output rate implies xed point average weight cf 
eq 

example linear terms standard correlation learning usually linear terms 
set 
assume average correlation non negative 
term inside square brackets negative order guarantee asymptotic stability xed point 
stable xed point fp achieved corr anti hebbian learning 
xed point fp occur positive rates require 
addition fp weights negative xed point contradicts assumption excitatory synapses see eq 

note output rate approaches zero neuron silent 
summary linear terms rate model correlation learning show rate stabilization weights negative corr anti hebbian learning 
example hebbian learning standard hebbian learning correlation term positive sign 
set corr 
assume 
obtain condition suciently negative order xed point stable 
particular 
ring rate stabilization rate hebbian learning requires linear non hebbian term 
discussion far focused piecewise linear rate model 
generalize arguments non linear neuron model 
keep arguments transparent restrict analysis translation invariant scenario assume identical mean input rates speci example consider neuron model sigmoidal activation gain function derivative positive vanishing input assume spontaneous output rate 
rate vanishes 
function approaches maximum output rate max see fig 

set mean weight obtain mean input uctuations 
small linearize mean input cf 
fig 

linearization leads back 
max input output rate sigmoidal activation function plot output rate solid line function neuron input spontaneous output rate maximum output rate max dashed line linearization mean input 
note cf 
examples 
discussion output rate stabilization proceeds linear case 
xed point exist 
condition dt yields xed point eq 
replaced 
eq 
implicit equation fp sigmoidal neuron reach xed point lies range fp max local stability analysis change 
xed point implies xed point mean weight statement follows directly dt dw dt 
say stabilization output rate implies stabilization weights 
summarize section 
value xed point fp identical linear linearized rate model eq 

stable eq 
positive 
kept higher order terms expansion eq 
xed point may shift slightly long nonlinear corrections small xed point remains stable due continuity di erential equation 
highlights fact learning rule appropriate choice parameters achieves stabilization output rate true normalization synaptic weights 
plasticity driven spikes section generalize concept intrinsic rate stabilization spike time dependent plasticity 
start reviewing learning rule synaptic modi cations driven relative timing pre postsynaptic spikes de ned means learning window gerstner kempter extensive experimental evidence supporting idea levy stewart bell zhang bi poo bi poo abbott munro feldman 
comparison spike learning rule developed rate learning rule discussed allows give coecients corr precise meaning 
particular show corr corresponds integral learning window 
anti hebbian learning rate model example identi ed negative integral learning window gerstner kempter 
learning rule going generalize eq 
spike learning 
changes synaptic weights triggered input spikes output spikes time di erences input output spikes 
simplify notation write sequence ft spike arrival times synapse form spike train consisting functions real spikes course nite width counts events spike 
similarly sequence output spikes denoted 
notations allow formulate spike learning rule kempter kistler van hemmen van hemmen dt dt dt eq 
coecients learning window general depend current weight value may depend local variables membrane potential calcium concentration 
sake clarity presentation drop dependencies assume constant coecients 
assume upper lower bounds synaptic weight changes zero max see discussion section 
sake clarity remainder section presume weights bounds 
interpret terms right eq 

coecient simple decay growth term drop 
sums functions 
recall integration di erential equation dx dt arbitrary yields discontinuity zero input spike arriving synapse time changes time constant amount variable amount dt depends sequence output spikes occurring times earlier similarly output spike time results weight change dt 
formulation learning assumes discontinuous instantaneous weight changes times occurrence spikes may realistic 
formulation generalized continuous delayed weight changes triggered pre postsynaptic spikes 
provided gradual weight change terminates time small compared time scale learning results derived basically 
follows assume discontinuous weight changes small compared typical values synaptic weights meaning learning small increments achieved large large separate time scale learning neuronal dynamics right hand side eq 
self averaging kempter van hemmen 
evolution weight vector dt hs hs ds hs notation angular brackets plus horizontal bar intended emphasize average spike statistics rates angular brackets ensemble rates horizontal bar 
example section 
weights change slowly statistical input ensemble assumed stationary correlations integrals replaced single integral 
formulation learning rule eq 
allows direct link rate learning introduced eqs 

averaged spike trains hs hs identical mean ring rates respectively de ned sections 
order interpret correlation terms careful 
terms hs describe correlation input output level spikes 
subsection give detailed treatment correlation term 
subsection simplify correlation term assumption slowly changing ring rates 
correlations rate learning subsection relate correlation term corr eq 
integral learning window eq 

order transition eq 
eq 
approximations necessary kempter 
neglect correlations input output spikes apart correlations contained rates 
approximation hs arb 
units ms ms firing rate arb 
units learning window arbitrary units function delay presynaptic spike arrival time postsynaptic ring time schematic 
positive negative synaptic ecacy increased decreased 
increase ecient presynaptic spike arrives milliseconds postsynaptic neuron starts ring 
jsj 
bar denotes width learning window 
qualitative time course learning window supported experimental results levy stewart zhang bi poo feldman see reviews brown linden sejnowski bi poo 
learning window described theoretically phenomenological model microscopic variables gerstner 
spike spike correlations schematic 
correlation function corr full line de ned sum terms causal contribution input spike synapse time output ring rate time time reverted epsp dotted line ii correlations mean ring rates indicated dashed line 
independent inputs stationary mean rate contribution constant equal gure sketched situation rates vary time 
horizontal denoting average learning time 
second assume rates change slowly compared width learning window cf 
fig 

consequently set dt integrand 
assumptions obtain dt ds dt ds term right termed di erential hebbian xie seung 
equation equivalent neglect di erential hebbian term set corr ds 
note 


contains correlations uctuations input output rates assume correlations slow compared width learning window 
condition slowly changing rates dropped section 
seen example rate learning intrinsic normalization output rate achievable corr 
argue corr coined anti hebbian framework rate learning hebbian terms spike learning 
intuitive reason learning window sketched fig 
parts positive negative 
integral learning window negative size weight changes positive part large pick correlations 
words approximations necessary transition eq 
eq 
general justi ed 
example appendix 
order argument precise return spike learning 
spike spike correlations previous section replaced hs temporal correlation slowly changing rates 
replacement leads cation situation take account correlations level spikes fast changes rates 
correlation term general form 
just section require learning slow process 

corr see section de nition 

corr 
correlation term evaluated constant weights input statistical properties change slow time scale learning correlations depend time di erence current weight vector corr hs horizontal bar refers temporal average introduced separation time scales cf 
eq 

substituting obtain dynamics weight changes dt ds corr equation completely analogous correlations rates replaced spikes 
may summarize eq 
saying learning driven correlations time scale learning window 
poisson neuron proceed analysis eq 
need determine correlations corr input spikes synapse output spikes 
correlations depend strongly neuron model consideration 
generalization linear rate neuron model section study inhomogeneous poisson model generates spikes rate 
membrane potential model neuron quiescent 
rate proportional 
call model piecewise linear poisson neuron short poisson neuron 
de nition input neuron consists poissonian spike trains timedependent intensities hs mathematics inhomogeneous poisson process refer appendix kempter 
rate model section normalized correlation function rate uctuations synapse de ned eq 

input scenarios section static pattern scenario translation invariant scenario 
spike arriving synapse evokes postsynaptic potential psp time course assume excitatory epsp 
impose condition ds require causality 
amplitude epsp synaptic ecacy 
membrane potential neuron linear superposition contributions ds causal spike times count 
sums run spike arrival times synapses 
spike train synapse poisson neuron output spikes generated stochastically time dependent rate depends linearly membrane potential threshold constants 
notation square brackets ensures non negative 
brackets dropped 
subscript indicates output rate depends speci realization input spike trains 
note spike generation process independent previous output spikes 
particular poisson neuron model show refractoriness 
drop restriction include refractoriness section 
expectation values order evaluate eqs 
rst calculate expectation values perform averages spike statistics input output rates average time 
de nition rate inhomogeneous poisson process expected input hs advantage eqs 
nd expected output rate hs rate contribution synapses output spike time hs ds nal term convolution input rate 
consider correlations input output hs expectation nd input spike synapse time output spike expectation de ned joint probability density equals probability density input spike time times conditional probability density hs fi tg observing output spike time input spike synapse hs hs fi tg framework linear poisson neuron term hs fi tg equals sum expected output rate speci contribution single input spike synapse cf 
eq 

summarize get kempter hs due causality rst term right inside square brackets vanish 
temporal average order evaluate correlation term need take temporal average corr excitatory synapses rst term gives positive contribution correlation function fig 

recall means presynaptic spike precedes postsynaptic ring 
second term right eq 
describes correlations input output rates 
rates correlations principle vary arbitrary fast time scale modeling papers see 
gerstner xie seung 
assume reasons transparency mean input rates constant see input scenarios section 
mean output rate weights may vary slow time scale learning 
learning equation substituting nd dt ds ij 
ij ds ij ds ds ij important factor eq 
input correlation function ij appears eq 
low pass ltered learning window postsynaptic potential 
input ensembles ij ij viz 
temporal symmetry ij ji matrix diagonalized real eigenvalues 
particular applies static pattern scenario translation invariant scenario introduced section 
apply example sequence learning cf 
herz herz van hemmen gerstner abbott blum gerstner abbott 
get preliminary insight nature solutions eq 
suppose inputs mean see broad parameter range mean output rate attracted xed point rst terms right hand side eq 
cancel 
dynamics pattern formation weights dominated largest positive eigenvalue matrix ij ij ds 
eigenvectors matrix identical matrix ij 
eigenvalues ij positive corr ds negative 
simple example appendix 
summarize section solved dynamics spike time dependent plasticity de ned eq 
poisson neuron piecewise linear spiking neuron model 
particular succeeded evaluate spike spike correlations presynaptic input postsynaptic ring 
stabilization spike learning aiming stabilization output rate analogy eqs 

arrive di erential equation multiply sum similarly assume ij independent index keep arguments simple assume addition inputs mean scenario assumptions veri ed 
simpli cations arrive di erential equation mean output rate identical dt fp xed point fp time constant ds ds contribution due spike spike correlations preand postsynaptic neuron 
comparison eqs 
shows may set corr ds 
average correlation replaced ds previous arguments regarding intrinsic normalization apply 
di erence new de nition factor compared 
di erence important 
sign ds enters de nition eq 

furthermore additional term appears 
due extra spike spike correlations presynaptic input postsynaptic ring 
note order small compared rst term number non zero synapses large kempter 
neglect second term set corr eq 
identical 
hand order input rates order important mean input rates low 
shown appendix realistic set parameters order magnitude study intrinsic normalization properties combinations parameters 
example positive integral consider case ds assume correlations input time scale learning window positive 
furthermore excitatory synapses natural assume 
order guarantee stability xed point require yields condition ds suciently negative output rate stabilization possible integral learning window positive sign 
words positive integral learning window non hebbian linear term necessary 
example linear terms standard hebbian learning linear terms vanish 
set 
example assume 
guarantee stability xed point require ds stable xed point output rate possible integral learning window suciently negative 
xed point fp ds note denominator negative eq 

xed point exist need fp cf 
fig 

emphasize contrast example weights xed point positive case excitatory synapses 
summarize examples stabilization output rate possible positive integral non hebbian term negative integral vanishing non hebbian terms 
spiking model refractoriness subsection generalize results realistic model neuron viz 
spike response model srm gerstner van hemmen gerstner gerstner :10.1.1.36.3372
aspects change respect poisson neuron 
include arbitrary refractoriness description neuron eq 

contrast poisson process events disjoint intervals independent 
second replace linear ring intensity eq 
nonlinear linearize 
start neuronal membrane potential 
input spike evokes excitatory postsynaptic potential described response kernel ds 
output spike neuron undergoes phase refractoriness described response kernel 
total membrane potential tj output spike postsynaptic neuron 
example consider refractory kernel exp parameters heaviside function 
de nition srm related standard integrate re model 
di erence integrate re model voltage reset ring xed value equations membrane potential reset time amount depends time previous spike 
analogy eq 
probability spike ring depends momentary value membrane potential 
precisely stochastic intensity spike ring nonlinear function tj viz tj tj 
example may take exp parameters 
spike ring threshold process occurs reaches threshold 
led back linear model eq 

systematic study escape functions gerstner :10.1.1.36.3372
de nition model turn calculation correlation function corr de ned eq 

precisely interested nding generalization eq 
poisson neurons spiking neurons refractoriness 
due dependence membrane potential eq 
ring time rst term righthand side eq 
complicated 
intuitively speaking average ring times postsynaptic neuron 
correct averaging theory population dynamics studied gerstner :10.1.1.36.3372
keep formulas simple possible consider constant input rates input spikes arrive average postsynaptic integration time set kernel membrane potential uctuations small linearize dynamics mean trajectory membrane potential tj linear order membrane potential uctuations correlations gerstner corr ds sj sj survivor function de ned sj exp dt mean output rate dt tj lter du jx eqs :10.1.1.36.3372
membrane potential mean trajectory 
equations main result 
considerations regarding normalization discussed preceding subsection valid function corr de ned equations output rate de ned 
passing note eq 
de nes gain function corresponding rate model 
discuss eq 
detail 
rst term righthand side eq 
describes immediate uence epsp ring probability postsynaptic neuron 
second term describes reverberation primary ect occurs interspike interval 
interspike interval distribution broad second term right eq 
small may neglected gerstner :10.1.1.36.3372
order understand relation eqs 
eq 
study examples 
example refractoriness want show limit refractoriness spiking neuron model identical poisson model 
neglect refractoriness mean membrane potential output rate cf 
sj exp see 
set df du evaluated nd eq 
reduces eq 

example high low noise limit take exponential escape rate exp high noise limit low noise limit 
high noise limit shown lter relatively broad 
precisely starts initial value decays slowly gerstner :10.1.1.36.3372
decay small set dx 
yields sj sj 
correlation function corr contains term proportional linear model 
low noise limit retrieve threshold process lter approaches function viz 
constant gerstner :10.1.1.36.3372
case correlation function corr contains term proportional derivative epsp ds 
summarize section shown possible calculate correlation function corr spiking neuron model refractoriness 
obtained correlation function learning dynamics 
seen correlation function depends noise level 
theoretical result agreement experimental measurements gustafsson 
high noise levels correlation function contained peak time course roughly proportional postsynaptic potential 
low noise time course peak similar derivative postsynaptic potential piecewise linear poisson neuron introduced section valid model high noise limit 
discussion compared learning rules level spikes level rates 
learning rules applied linear nonlinear neuron models 
discuss results context existing experimental theoretical literature 
experimental results rate normalization experiments performed turrigiano 

blocked mediated inhibition cortical culture initially raised activity 
days ring rate returned close control value time synaptic strengths decreased 
conversely induced ring rate reduction leads synaptic strengthening 
output rate normalized 
turrigiano suggest rate normalization achieved multiplying weights factor 
ansatz weights normalized addition subtraction xed amount weights 
discrepancy experiment model resolved assuming upper lower bounds individual weights learning coecients depend weights 
extended ansatz exceeds scope 
additional mechanisms contributing activity dependent stabilization ring rates refer 
desai turrigiano nelson 
model scenarios intrinsic rate normalization possible depending choice parameters hebbian non hebbian terms 
discuss turn 
sign correlation term 
rate normalization model easily achieved integral learning window negative necessary see examples 
basis neurophysiological data known zhang bi poo feldman sign integral learning window decided see reviews linden bi poo 
shown negative integral corresponds terms rate coding negative coecient corr anti hebbian rule 
learning rule pick positive correlations input contains temporal correlations ij time scale learning window 
example appendix 
ii role non hebbian terms 
seen examples linear terms learning rule necessary rate normalization 
see example negative coecient helps 
value means absence presynaptic activation postsynaptic spiking induces long term depression 
seen example hippocampal slices christo see reviews singer brown linden 
cortical slices reported 
hand positive ect presynaptic spikes synapses absence postsynaptic spiking helps keep xed point range positive rates fp see eq 

experiments suggest presynaptic activity results long term potentiation bliss urban bell see brown 
iii zero order term 
seen eq 
spontaneous weight growth helps go keep xed point rate positive range 
turrigiano 
blocked cortical culture activity increase synaptic weights supporting 
spike time dependent learning window indicated learning window parts positive part potentiation negative depression 
learning window accordance experimental results zhang bi poo feldman 
may ask theoretical reasons time dependence learning window 
excitatory synapses presynaptic input spike precedes postsynaptic ring may cause postsynaptic activity takes part ring hebb 
literal understanding hebb rule suggests excitatory synapses learning window positive di erence times occurrence input output spike 
recall means presynaptic spike precedes postsynaptic spiking see fig 

fact positive learning phase potentiation important ingredient models sequence learning herz herz van hemmen gerstner abbott blum gerstner abbott 
positive phase followed negative phase depression fig 
learning rule acts temporal contrast lter enhancing detection temporal structure input kempter gerstner kempter song xie seung 
phase learning rule potentiation depression certain degree theoretical prediction 
advantages learning rule realized models gerstner kempter experimental results millisecond time scale available zhang bi poo feldman 
structural stability output rates seen synaptic plasticity appropriate combination parameters pushes output rate xed point 
explicitly show existence stability xed point require factor de ned eq 
independent synapse index standard example input scenario mean rate synapse translation invariant ij ji jj case rate normalization equivalent stable xed point mean weight see section 
may wonder happens eq 
exactly true holds approximately 
answer question write linear model eq 
vector notation dw dt mw matrix unit vector stable xed point mean weight implies eigenvector negative eigenvalue 
negative eigenvalue exists condition due local continuity solutions di erential equation dependence parameters eigenvalue stay negative regime holds approximately 
eigenvector corresponding negative eigenvalue close identical words output rate remains stable weight vector longer exactly normalized 
normalization properties learning rule akin identical subtractive normalization miller mackay miller 
point view basic property rule stabilization output rate 
approximate normalization mean weight constant consequence 
normalization exact eq 
independent synapse index mean input rates synapses 
learning rule constant coecients typically unstable weights receiving strongest enhancement grow bounds expense synapses receiving enhancement 
unlimited growth avoided explicitly introducing upper lower bounds individual weights 
consequence weights saturate bounds 
saturated weights longer participate learning dynamics stabilization arguments sections applied set remaining weights 
certain models shown explicitly xed point output rate remains unchanged compared case bounds kempter 
simply check xed point lies parameter range allowed bounds weights 
principal components covariance output rate converges xed point mean weight adapts constant value synapses may grow decay 
synapse speci change leads learning proper sense contrast mere adaptation 
spike learning dominated eigenvector ij largest eigenvalue kempter 
learning detects principal component matrix ij de ned eq 

remarks order 
emphasize spike time dependent learning clear cut distinction hebbian anti hebbian rules dicult 
ij sensitive covariance input time scale learning window rule considered anti hebbian type stimulus hebbian see appendix 
second contrast oja rule oja necessary require mean input vanishes synapse 
intrinsic rate normal ization implies mean input level play role 
initial adaptation phase neuronal plasticity automatically subtracted mean input sensitive covariance input see discussion eqs 

convergence xed point learning rate equivalent sejnowski covariance rule sejnowski sejnowski tesauro stanton sejnowski 
subthreshold regime coincidence detection previously exploited normalization properties learning rules negative integral learning window study barn owl auditory system gerstner 
learning led structured delay distribution sub millisecond time resolution albeit width learning window millisecond range 
intrinsic normalization output rate keep postsynaptic neuron subthreshold regime neuron functions coincidence detector kempter 
song mechanism keep neuron subthreshold regime neuron may show large output uctuations 
regime rate stabilization induces stabilization value coecient variation cv broad input regime 
due stabilization output rates speculate coherent picture neural signal transmission may emerge neuron chain processing layers equally active averaged complete stimulus ensemble 
case set normalized input rates layer transformed average set equal rates layer 
ideally target value rates plasticity rule converges regime neuron sensitive viz subthreshold regime 
regime signal transmission maximal kempter 
plasticity rule important ingredient optimize neuronal transmission properties brenner kempter koch song 
appendix illustrate static pattern scenario case learning window negative integral 
time step length 
pattern vector chosen stochastically data base fx pg 
input rates 

static correlation patterns de ned static ij static ij ij example learning window correlation function ij plotted function time di erence denotes average input patterns 
assume intrinsic order sequence presentation patterns 
time dependent correlation input see fig 
ij static ij jsj 



consider postsynaptic potential 
form potential approximated delayed localized pulse dirac delta function 
learning window take see fig 




ds corr learning rule classi ed anti hebbian 
hand ij ds ds ij static ij gives static correlations 
ij ji ij ji holds matrix ij hermitian 
furthermore ij static ij ds js 

ij ij vectors 
positive de nite matrix eigenvalues positive 
dynamics learning dominated eigenvector static ij largest eigenvalue just standard hebbian learning hertz 
note contrast standard hebbian learning need impose spike time dependent learning rule automatically subtracts mean 
assume ij independent see section 
want compare value ds eq 

nd 
static ij substitute numbers estimate order magnitude fraction 
learning window duration width say order 
ms mean rate averaged stimulus ensemble hz 
typical number synapses 
assume input channel correlated percent value static ij uncorrelated remaining percent 
static ij 
yields 
order magnitude lower value mean rate decrease larger number synapses increase 
acknowledgments werner kistler critical reading rst version manuscript 
richard kempter supported deutsche forschungsgemeinschaft fg ke 
abbott blum 

functional signi cance long term potentiation sequence learning prediction 

cortex 
abbott munro 

neural information processing systems nips 
workshop spike timing synaptic plasticity 
breckenridge www pitt edu pwm ltp 
singer 

long term depression excitatory synaptic transmission relationship long term potentiation 
trends neurosci 
bell han sugawara 

synaptic plasticity structure depends temporal order 
nature 
bi poo 

synaptic modi cations hippocampal neurons dependence spike timing synaptic strength postsynaptic cell type 
neurosci 
bi poo 

distributed synaptic modi cation neural networks induced patterned stimulation 
nature 
bi poo 

synaptic modi cation correlated activity hebb postulate revisited 
annu 
rev neurosci 
bienenstock cooper munro 

theory development neuron selectivity orientation speci city binocular interaction visual cortex 
neurosci 
bliss 

synaptic model memory long term potentiation hippocampus 
nature 
brenner bialek de van 

universal statistical behavior neuronal spike trains 
phys 
rev lett 
brown 

hebbian synaptic plasticity evolution contemporary concept 
domany van hemmen schulten editors models neural networks ii pp 
new york 
springer 
christo 

postsynaptic induction nonassociative long term depression excitatory synaptic transmission rat hippocampal slices 
neurophysiol 
thompson 

long term synaptic plasticity tween pairs individual ca pyramidal cells rat hippocampal slice cultures 
physiol 
desai rutherford turrigiano 

plasticity intrinsic cortical pyramidal neurons 
nat 
neurosci 


coincidence detection changes synaptic ecacy neurons rat barrel cortex 
nat 
neurosci 
pawelzik ernst cowan milton 

dynamics self organized delay adaption 
phys 
rev lett 
feldman 

timing ltp vertical inputs layer ii iii pyramidal cells rat barrel cortex 
neuron 
gustafsson 

relation shapes post synaptic potentials changes ring probability cat 
physiol 

gerstner 

hebbian learning pulse timing barn owl auditory system 
maass bishop editors pulsed neural networks chapter pp 

mit press cambridge ma 
gerstner 

population dynamics spiking neurons fast transients asynchronous states locking 
neural comput 
gerstner abbott 

learning navigational maps potentiation modulation hippocampal place cells 
comput 
neurosci 
gerstner van hemmen 

associative memory network spiking neurons 
network 
gerstner ritz van hemmen 

spikes 
hebbian learning retrieval time resolved excitation patterns 
biol 
cybern 
gerstner kempter van hemmen wagner 

neuronal learning rule sub millisecond temporal coding 
nature 
gerstner van hemmen cowan 

matters neuronal locking 
neural comput 
gerstner kempter van hemmen wagner 

developmental learning rule coincidence tuning barn owl auditory system 
bower editor computational neuroscience trends research pp 

plenum press new york 
gerstner kempter van hemmen 

hebbian learning pulse timing barn owl auditory system 
maass bishop editors pulsed neural networks chapter pp 

mit press cambridge ma 
watts 

spike learning neuron analog vlsi 
mozer jordan petsche editors advances neural information processing systems pp 

mit press cambridge ma 
hebb 

organization behavior 
wiley new york 
van hemmen 

theory synaptic plasticity 
moss editors handbook biological physics vol 
neuro informatics neural modelling pp 

elsevier amsterdam 
van hemmen gerstner herz uhn 

encoding decoding patterns correlated space time 
dor ner editor arti cial intelligence und pp 
springer berlin 
hertz krogh palmer 

theory neural computation 
addison wesley redwood city 
herz uhn van hemmen 

hebb rule representation static dynamic objects neural nets 
europhys 
lett 
herz uhn van hemmen 

hebbian learning reconsidered representation static dynamic objects associative neural nets 
biol 
cybern 
kempter gerstner van hemmen 

hebbian learning spiking neurons 
phys 
rev 
kempter gerstner van hemmen 

spike compared rate hebbian learning 
kearns solla cohn editors advances neural information processing systems pp 
mit press cambridge ma 
kempter gerstner van hemmen wagner 

temporal coding sub millisecond range model barn owl auditory pathway 
touretzky mozer hasselmo editors advances neural information processing systems pp 

mit press cambridge ma 
kempter gerstner van hemmen wagner 

extracting oscillations neuronal coincidence detection noisy periodic spike input 
neural comput 
kempter wagner van hemmen 

formation maps axonal propagation synaptic learning 
proc 
natl 
acad 
sci usa press 
kistler van hemmen 

modeling synaptic plasticity conjunction timing pre postsynaptic action potentials neural comput 
kohonen 

self organization associative memory 
springer berlin 
levy stewart 

temporal contiguity requirements long term associative potentiation depression hippocampus 
neurosci 
linden 

return spike postsynaptic action potentials induction ltp neuron 
linsker 

basic network principles neural architecture emergence spatial opponent cells 
proc 
natl 
acad 
sci 
usa 
mackay miller 

analysis linsker application hebbian rules linear networks 
network 
von der malsburg 

self organization orientation sensitive cells striate cortex 
kybernetik 


regulation synaptic ecacy coincidence postsynaptic aps 
science 
miller 

receptive elds maps visual cortex models ocular dominance orientation columns 
domany van hemmen schulten editors models neural networks iii pp 
new york 
springer 
miller 

synaptic economics competition cooperation synaptic plasticity 
neuron 
miller mackay 

role constraints hebbian learning 
neural comput 
oja 

simpli ed neuron model principal component analyzer 
math 
biol 
sejnowski 

natural patterns activity long term synaptic plasticity 
curr 
opin 

gerstner 

noise integrate re models stochastic input escape rates 
neural comput 
brookes 

long term depression synapses slices rat hippocampus induced bursts postsynaptic activity 
exp brain res 
powers binder 

ects background noise response rat cat excitatory current transients 
physiol 

roberts 

computational consequences temporally asymmetric learning rules di erential hebbian learning 
compu 
neurosci 
ruf schmitt 

unsupervised learning networks spiking neurons temporal coding 
gerstner nicoud editors proc 
th int 
conf 
arti cial neural networks icann pp 

springer heidelberg 
sejnowski 

storing covariance nonlinearly interacting neurons mathematical biology 
sejnowski tesauro 

hebb rule synaptic plasticity algorithms implementations 
byrne berry editors neural models plasticity experimental theoretical approaches chapter pp 

academic press san diego 
stanton sejnowski 

associative long term depression hippocampus induced hebbian covariance nature 
tsodyks 

algorithm synaptic modi cation exact timing pre postsynaptic action potentials 
gerstner nicoud editors proc 
th int 
conf 
arti cial neural networks icann pp 
heidelberg 
springer 
tsodyks 

algorithm modifying release probability pre postsynaptic spike timing 
neural comput 
perrone 

post hebbian learning rules 
arbib editor handbook brain theory neural networks pp 

mit press cambridge ma 
song miller abbott 

competitive hebbian learning dependent synaptic plasticity 
nat 
neurosci 
koch 

voltage dependent conductances adapt maximize information encoded neurons 
nat 
neurosci 
turrigiano 

homeostatic plasticity neuronal networks things change stay 
trends neurosci 
turrigiano leslie desai rutherford nelson 

scaling amplitude neocortical neurons 
nature 
turrigiano nelson 

hebb homeostasis neuronal plasticity 
curr 
opin 

urban 

induction hebbian non hebbian ber long term potentiation distinct patterns high frequency stimulation 
neurosci 
singer 

induction ltp visual cortex neurons intracellular 

gerstner van hemmen 

emergence spatio temporal receptive elds application motion detection 
biol 
cybern 
wiskott sejnowski 

constrained optimization neural map formation unifying framework weight growth normalization 
neural comput 
xie seung 

spike learning rules stabilization persistent neural activity 
solla leen uller editors advances neural information processing systems pp 
xx xx 
mit press cambridge ma 
zhang tao holt harris poo 

critical window cooperation competition developing synapses 
nature 

