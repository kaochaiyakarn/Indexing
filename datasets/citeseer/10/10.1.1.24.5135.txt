random projection dimensionality reduction applications image text data bingham heikki mannila laboratory computer information science helsinki university technology box fin hut finland iki fi heikki mannila hut fi random projections emerged powerful method dimensionality reduction 
theoretical results indicate method preserves distances quite nicely empirical results sparse 
experimental results random projection dimensionality reduction tool number cases high dimensionality data lead burdensome computations 
application areas processing noisy noiseless images information retrieval text documents 
show projecting data random lower dimensional subspace yields results comparable conventional dimensionality reduction methods principal component analysis similarity data vectors preserved random projection 
random projections computationally signi cantly expensive principal component analysis 
show experimentally sparse random matrix gives additional computational savings random projection 
keywords random projection dimensionality reduction image data text document data high dimensional data 
applications data mining high dimensionality data restricts choice data processing methods 
application areas include analysis market basket data text documents image data cases dimensionality large due wealth alternative products large vocabulary large image windows respectively 
statistically optimal way dimensionality reduction project data leave nokia research center permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
kdd san francisco ca usa copyright acm xxxxx xx xx xx 
lower dimensional orthogonal subspace captures variation data possible 
best mean square sense widely way principal component analysis pca unfortunately quite expensive compute high dimensional data sets 
computationally simple method dimensionality reduction introduce signi cant distortion data set desirable 
random projection rp original high dimensional data projected lower dimensional subspace random matrix columns unit lengths 
rp computationally ecient suf ciently accurate method dimensionality reduction high dimensional data sets 
method attracted lots interest empirical results sparse 
give experimental results rp dimensionality reduction tool high dimensional image text data sets 
application areas random projection compared known dimensionality reduction methods 
show despite computational simplicity random projection introduce signi cant distortion data 
data sets di erent natures 
image data monochrome images natural 
image matrix pixel brightness values distribution generally approximately gaussian symmetric bell shaped 
text document data vector space document forms dimensional vector vocabulary size 
th element vector indicates function frequency th vocabulary term document 
document data highly sparse peaked terms vocabulary document entries document vector zero 
document data nonsymmetric positively skewed distribution term frequencies nonnegative 
instructive see random projection works dimensionality reduction tool context di erent application areas 
results images corrupted noise experimental results indicate random projection sensitive impulse noise 
random projection promising alternative existing methods noise reduction median ltering 
organized follows 
discuss related random projections similarity search 
section presents di erent dimensionality reduction methods 
section gives experimental results dimensionality reduction image data section text data 
section gives 
related papadimitriou random projection preprocessing textual data prior applying lsi :10.1.1.131.5084
experimental results arti cially generated set documents 
approach columns random projection matrix assumed strictly orthogonal need case shall see experiments 
kaski experimental results random mapping context websom system 
applies random projection indexing audio documents prior lsi som 
kleinberg indyk motwani random projections nearest neighbor search high dimensional euclidean space theoretical insights 
dasgupta random projections learning high dimensional gaussian mixture models 
applications random projection include 
problems dimensionality reduction similarity search addressed information retrieval literature approaches random projection 
ostrovsky rabani give dimension reduction operation suitable clustering 
agrawal map time series frequency domain discrete fourier transform retain rst frequencies 
keogh pazzani reduce dimension time series data segmenting time series sections indexing section means 
aggarwal index market basket data speci signature table similarity search 
wavelet transforms common method signal compression 

random projection random projection original dimensional data projected dimensional subspace origin random matrix columns unit lengths 
matrix notation xd original set dimensional observations rp rk projection data lower dimensional subspace 
key idea random mapping arises johnson lindenstrauss lemma points vector space projected randomly selected subspace suitably high dimension distances points approximately preserved 
simple proof result see :10.1.1.45.3654
random projection computationally simple forming random matrix projecting data matrix dimensions order data matrix sparse nonzero entries column complexity order :10.1.1.131.5084
strictly speaking projection generally orthogonal 
linear mapping see websom hut fi websom cause signi cant distortions data set orthogonal 
unfortunately computationally expensive 
rely result hecht nielsen high dimensional space exists larger number orthogonal orthogonal directions 
vectors having random directions suciently close orthogonal equivalently approximate identity matrix 
experiments mean squared di erence identity matrix element 
comparing performance random projection methods dimensionality reduction instructive see similarity vectors distorted dimensionality reduction 
measure similarity data vectors euclidean distance inner product 
case image data euclidean distance widely measure similarity 
text documents hand generally compared cosine angle document vectors document vectors normalized unit length corresponds inner product document vectors 
write euclidean distance data vectors original large dimensional space jjx jj 
random projection distance approximated scaled euclidean distance vectors reduced space rx jj original reduced dimensionality data set 
scaling term takes account decrease dimensionality data johnson lindenstrauss lemma expected norm projection unit vector random subspace origin 
choice random matrix key points interest 
elements ij gaussian distributed need case 
achlioptas shown gaussian distribution replaced simpler distribution ij 
probability probability probability fact practically zero mean unit variance distributions ij give mapping satis es lemma 
achlioptas result means computational savings database applications computations performed integer arithmetics 
experiments shall gaussian distributed random matrices sparse matrices show achlioptas theoretical result practical signi cance 
context experimental results shall refer rp projection matrix gaussian distributed srp matrix sparse distributed 
shorthand rp refers random projection 
pca svd lsi principal component analysis pca eigenvalue decomposition data covariance matrix computed columns matrix eigenvectors data covariance matrix diagonal matrix containing respective eigenvalues 
dimensionality reduction data set desired data projected subspace spanned important eigenvectors pca matrix ek contains eigenvectors corresponding largest eigenvalues 
pca optimal way project data mean square sense squared error introduced projection minimized projections dimensional space 
unfortunately eigenvalue decomposition data covariance matrix size dimensional data expensive compute 
computational complexity estimating pca 
exists computationally expensive methods nding eigenvectors eigenvalues large matrix experiments appropriate matlab routines realize 
closely related method singular value decomposition svd usv orthogonal matrices contain left right singular vectors respectively diagonal contains singular values svd dimensionality data reduced projecting data space spanned left singular vectors corresponding largest singular values sv uk size contains singular vectors 
pca svd expensive compute 
exists numerical routines power lanczos method ecient pca sparse data matrices shall svd pca context sparse text document data 
sparse data matrix xd nonzero entries column computational complexity svd order :10.1.1.131.5084
latent semantic indexing lsi dimensionality reduction method text document data 
lsi document data lower dimensional topic space documents characterized underlying latent hidden concepts referred terms 
lsi computed pca svd data matrix dimensional document vectors 
discrete cosine transform discrete cosine transform dct widely method image compression dimensionality reduction image data 
dct computationally burdensome pca performance approaches pca 
dct optimal human eye distortions introduced occur highest frequencies human eye tends neglect noise 
dct performed simple matrix operations image transformed dct space dimensionality reduction done inverse transform discarding transform coecients corresponding highest frequencies 
computing dct data dependent contrast pca needs eigenvalue decomposition data covariance matrix dct orders magnitude cheaper compute pca 
computational complexity order dn log dn data matrix size 

results image data data set consisted image windows drawn monochrome images natural scenes sizes original images pixels windows size randomly drawn images 
image window dimensional column vector 
noiseless image data comparing di erent methods dimensionality reduction criteria amount distortion caused method computational complexity 
case image data measure distortion comparing euclidean distance dimensionality reduced data vectors euclidean distance original highdimensional space 
case random projection euclidean distance reduced space scaled shown methods scaling performed 
rst tested ect reduced dimensionality di erent values 
dimensionality reducing matrix operation computed anew 
shows error distance members pair data vectors averaged pairs 
results random projection gaussian distributed random matrix rp random projection sparse random matrix srp principal component analysis pca discrete cosine transform dct shown cent con dence intervals 
reduced dim 
data error rp srp pca dct error produced rp srp pca dct image data con dence intervals pairs data vectors 
clearly seen random projection rp srp yields accurate results dimensionality reduction random projection distort data signi cantly pca 
dimensions random projection pca give quite accurate results error produced dct clearly visible 
smaller dimensions pca distorts data 
tells variation data captured rst principal components error pca dependent sum omitted eigenvalues equal number eigen available www cis hut fi projects ica data images values retained 
contrast random projection method continues give accurate results 
explanation success random projection scaling term formula takes account decrease dimensionality 
pca scaling useful smallest dimensions straightforward rule dicult give 
point interest computational complexity methods 
shows number matlab oating point operations needed rp srp pca dct dimensionality reduction logarithmic scale 
seen pca signi cantly burdensome random projection dct 
case dct chosen data vectors transformed data set number oating point operations small 
reduced dim 
data flops flops needed pca rp srp dct number matlab oating point operations needed reducing dimensionality image data rp srp pca dct logarithmic scale 
figures conclude random projection computationally inexpensive method dimensionality reduction preserving distances data vectors practically pca clearly better dct 
smallest dimensions rp outperforms pca dct 
dimensionality reduction image data di ers slightly common procedure image compression image transformed economical form transmission transformed back original space 
transformation chosen resulting image looks similar possible original image human eye 
respect discrete cosine transform proven optimal 
see image dimensionality reduced rp look random mapping inverted 
pseudoinverse expensive compute orthogonal transpose approximation pseudoinverse image computed new rp rp result random projection 
obtained image visually worse dct compressed image human eye 
random projection successful applications distance similarity data vectors preserved dimensionality reduction possible data intended visualized human eye 
applications include machine vision possible automatically detect line image surveillance camera changed 
noise reduction images second set experiments considered noisy images 
images corrupted salt pepper impulse noise probability pixel image turned black white 
wanted project data way distance data vectors reduced noisy data space close possible distance vectors high dimensional noiseless data space dimensionality reduction applied high dimensional noisy images 
simple ective way noise reduction especially case salt pepper impulse noise median ltering mf pixel image replaced median pixel neighborhood 
median ected individual noise spikes median ltering eliminates impulse noise quite 
common neighborhood size pixels experiments 
mf computationally ecient order dmn image windows pixels denotes size neighborhood case 
mf require dimensionality reduction result yardstick comparing methods dimensionality reduction noise cancellation 
shows distance noisy image windows distorted dimensionality reduction compared distance original high dimensional noiseless space 
compare di erent dimensionality reduction methods respect sensitivity noise 
see median ltering introduces quite large distortion image windows despite human eye removes impulse noise eciently 
distortion due blurring pixels replaced median neighborhood eliminating noise small details 
pca dct random projection perform quite similarly noiseless case 
conclude random projection promising alternative dimensionality reduction noisy data sensitive impulse noise 
exists course methods noise reduction 
interest mainly dimensionality reduction noise reduction 

results text data applied dimensionality reduction techniques text document data newsgroups newsgroups corpus sci crypt sci med sci space soc religion 
christian 
documents converted term frequency vectors common terms removed mccallum rainbow toolkit stemming 
data zero mean variance entries data matrix normalized 
document vectors normalized unit length 
kind preprocessing di erent applied im available www cs cmu edu available www cs cmu edu mccallum bow reduced dim 
data error rp srp pca mf dct noisy data error produced rp srp pca dct mf noisy image data con dence intervals pairs image windows 
mf dimensionality reduced 
age data 
distinct natures image text data di erences preprocessing yielded slightly different results di erent data sets 
size vocabulary terms data set consisted newsgroup documents 
randomly chose pairs data vectors documents computed similarity inner product 
error dimensionality reduction measured di erence inner products dimensionality reduction 
reduced dim 
data average error rp svd error produced rp svd text document data con dence intervals pairs document vectors 
shows error introduced dimensionality reduction 
results averaged document pairs 
results svd random projection gaussian distributed random matrix shown cent con dence intervals 
reduced dimensionality took values 
seen random projection quite accurate svd applications error may 
johnson lindenstrauss result states euclidean distances retained random projection 
case inner products di erent euclidean distances document vectors probably preserved better 
common practice measure similarity document vectors inner products results 
despite ecient svd routines nding singular vectors sparse matrix svd orders magnitude burdensome rp 
results text document data indicate random projection dimensionality reduction large document collections computational complexity latent semantic indexing svd 
similarly rp speed latent semantic indexing lsi dimensionality data rst reduced rp burdensome lsi computed new lowdimensional space :10.1.1.131.5084
documents generated arti cially random matrix assumed strictly orthogonal experiments show restrictions necessary :10.1.1.131.5084
common problem text document retrieval query matching 
random projection useful query matching query long set similar documents particular document searched 

new promising experimental results random projection dimensionality reduction high dimensional real world data sets 
comparing different methods dimensionality reduction criteria amount distortion caused method computational complexity 
results indicate random projection preserves similarities data vectors data projected moderate numbers dimensions projection fast compute 
application areas quite di erent natures noisy noiseless images natural scenes text documents newsgroup corpus 
application areas random projection proved computationally simple method dimensionality reduction preserving similarities data vectors high degree 
experimental results random projection sparsely populated random matrix introduced 
fact necessary gaussian distributed random matrix simpler matrices obey johnson lindenstrauss lemma giving computational savings 
emphasize random projection bene cial applications distances original highdimensional data points meaningful original distances similarities suspect little reason preserve 
example consider data neural network training 
projecting data lower dimensional subspace speeds training training interpoint distances problems include clustering nearest neighbors consider signi cance dimensions data set 
euclidean space dimension equally important independent process monitoring application measured quantities dimensions closely related interpoint distances necessarily bear clear meaning 
realistic application random projection data mining problem clustering compare results computational complexity mining original high dimensional data dimensionality reduced data topic study 
interesting open problem concerns number dimensions needed random projections 
result gives bounds higher ones suce give results empirical data 
example case image data lower bound experiments 
johnson lindenstrauss result course worst case interesting understand properties experimental data possible get results fewer dimensions 
conclude random projection alternative traditional statistically optimal methods dimensionality reduction computationally infeasible high dimensional data 
random projection su er curse dimensionality quite contrary traditional methods 

achlioptas 
database friendly random projections 
proc 
acm symp 
principles database systems pages 
aggarwal wolf yu 
new method similarity indexing market basket data 
proc 
acm sigmod int 
conf 
management data pages 
agrawal faloutsos swami 
ecient similarity search sequence databases 
proc 
th int 
conf 
data organization algorithms pages 
springer 
vempala 
algorithmic theory learning robust concepts random projection 
proc 
th annual symp 
foundations computer science pages 
ieee computer society press 

berry 
large scale sparse singular value computations 
international journal super computer applications 
dasgupta 
learning mixtures gaussians 
th annual ieee symp 
foundations computer science pages 
dasgupta 
experiments random projection 
proc 
uncertainty arti cial intelligence 
dasgupta gupta 
elementary proof johnson lindenstrauss lemma 
technical report tr international computer science institute berkeley california usa 
deerwester dumais furnas landauer 
indexing latent semantic analysis 
journal am 
soc 
information science 
frankl 
johnson lindenstrauss lemma sphericity graphs 
journal combinatorial theory ser 

golub van loan 
matrix computations 
north oxford academic oxford uk 

wavelets 
ieee computational science engineering 
hecht nielsen 
context vectors general purpose approximate meaning representations self organized raw data 
marks ii robinson editors computational intelligence imitating life pages 
ieee press 
indyk motwani 
approximate nearest neighbors removing curse dimensionality 
proc 
th symp 
theory computing pages 
acm 
johnson lindenstrauss 
extensions mapping hilbert space 
conference modern analysis probability volume contemporary mathematics pages 
amer 
math 
soc 
kaski 
data exploration self organizing maps 
acta mathematics computing management engineering series number 

dr tech 
thesis helsinki university technology finland 
kaski 
dimensionality reduction random mapping 
proc 
int 
joint conf 
neural networks volume pages 
keogh pazzani 
simple dimensionality reduction technique fast similarity search large time series databases 
th paci asia conf 
knowledge discovery data mining 
kleinberg 
algorithms nearest neighbor search high dimensions 
proc 
th acm symp 
theory computing pages 

indexing audio documents latent semantic analysis som 
oja kaski editors kohonen maps pages 
elsevier 
ostrovsky rabani 
polynomial time approximation geometric clustering 
proc 
st symp 
foundations computer science pages 
ieee 
papadimitriou raghavan tamaki vempala :10.1.1.131.5084
latent semantic indexing probabilistic analysis 
proc 
th acm symp 
principles database systems pages 
rao yip 
discrete cosine transform algorithms advantages applications 
academic press 
roweis 
em algorithms pca spca 
neural information processing systems pages 
salton mcgill 
modern information retrieval 
mcgraw hill 
sirovich 
management analysis large scienti datasets 
int 
journal supercomputer applications spring 
sonka boyle 
image processing analysis machine vision 
pws publishing 
vempala 
random projection new approach vlsi layout 
proc 
th annual symp 
foundations computer science 
ieee computer society press 
