technische universitat berlin fb physik institut fur theoretische physik fb informatik fg statistical physics clustering algorithms diplomarbeit graepel nr april prof dr sch oll prof dr klaus obermayer acknowledgments pleasure prof klaus obermayer excellent supervision numerous discussions fruitful suggestions 
matthias burger collaborator chapter chapter 
matthias burger ralf herbrich manuscript provided moral support 
am indebted prof scholl officially supervising thesis lies interdisciplinary boundary statistical physics statistics 
mention partly funded technical university berlin fip 
des deutschen support studies 
go parents best sponsors respect 
thesis come existence place 
contents background 
motivation 
machine learning 
supervised learning 
reinforcement learning 
unsupervised learning 
clustering 
applications clustering 
methods clustering 
non parametric approaches 
generative models 
reconstructive methods 
overview thesis 
primary objective 
structure thesis 
probabilistic autoencoder framework probabilistic 
unsupervised learning 
general folded markov chain 
derivation cost functions 
stage folded markov chain 
cost function tmp 
squared euclidean distance distortion 
cost function 
optimization optimization methods 
general remarks 
stochastic gradient descent 
simulated annealing 
deterministic annealing em 
deterministic annealing 
em algorithm 
soft topographic vector quantization derivation stvq algorithm 
stationarity conditions 
deterministic annealing em algorithm 
derivatives stvq algorithm 
stvq noisy source channel coding 
transmission noisy channel 
results 
phase transitions stvq initial phase transition 
automatic selection feature dimensions 
phase transition discrete case 
continuous gaussian case 
numerical results 
toy problem 
annealing dimensional array cluster centers 
automatic selection feature dimensions chain clusters kernel soft topographic mapping clustering high dimensional feature space 
kernel trick mercer condition 
kernel trick 
mercer condition 
admissible kernel functions 
derivation kernel soft topographic mapping 
topographic clustering feature space 
application kernel trick 
em deterministic annealing 
critical temperature phase transition 
numerical simulations rbf kernel 
effect rbf kernel 
simulation results handwritten digit data 

soft topographic mapping proximity data topographic clustering pairwise dissimilarity data 
derivation soft topographic mapping proximity data 
mean field approximation 
em deterministic annealing 
som approximation stmp 
critical temperature phase transition 
numerical simulations 
toy example noisy spiral 
topographic map cat cerebral cortex 
stmp 


appendix proof fixed point property 
derivation symmetry properties assignment correlations 
evaluation assignment correlation gaussian neighborhood functions derivation mean field equations 
bibliography chapter background motivation advent information age reshaped world brings huge amounts data fingertips 
computers capable performing massive calculations seconds storage capacity increases immense rate networks computers particular internet possible access data world 
profound changes necessary develop new tools dealing avalanche available data 
data comprise value depending user demand data located retrieved processed visualized understood order useful 
due sheer amount data complexity need intelligent methods extract meaningful information data 
machine learning interesting approach field data analysis processing idea machine learning 
valiant program performing task acquired learning acquired means explicit programming 
view described situation abundance available data machine learning paradigm particular appeal allow relatively automatic acquisition knowledge computers ideally assist humans full potential available data 
necessary sufficient amount training examples relevant task learned 
particular neural network approaches employ computational principles nature shown yield generalization ability examples 
researchers identified main paradigms learning correspond different learning situations supervised learning reinforcement learning unsupervised learning see textbook account 
supervised learning supervised learning concerned solution classification regression tasks training examples pairs corresponding input values target values 
task supervised learning learn examples general relation input target able predict target value new input value 
interesting useful example classification recognition handwritten digits gray value images obtained handwritten addresses mail envelopes chapter 
:10.1.1.15.9362
regression problems appear frequently empirical sciences measurements observations sample input space researcher interested continuous relation input target space 
methods range traditional statistics approaches neural network architectures support vector machines 
reinforcement learning reinforcement learning concerned different scenario learner takes actions receives rewards environment 
effectively learner receives feedback actions provided correct answer target value situation 
task select actions strategy way maximize total amount payoff 
paradigm thought suitable modeling animal learning nature explicit teacher 
reinforcement learning successfully applied problems dynamic resource allocation game playing 
popular method temporal difference td learning introduced sutton 
unsupervised learning third paradigm concerned called unsupervised learning 
learner provided samples distribution data receives target values reinforcements 
task find suitable representation underlying distribution data 
different objectives formulated order task concrete 
include redundancy reduction information maximization minimum cross entropy minimum reconstruction error discussed section 
unsupervised learning various applications preprocessing supervised learning data compression density estimation source separation data visualization 
major approach unsupervised learning data clustering focus thesis 
clustering 
clustering methods aim partitioning set data items groups data items belong group alike data items belong different groups 
groups called clusters number may preassigned analyst parameter determined algorithm 
result algorithm injective mapping data items clusters interesting way looking clustering consider classification labels clusters represent different classes class labels treated missing data 
formulation problem clear crucial clustering define appropriate distance dissimilarity measure ij data items 
popular case data items represented euclidean feature vectors squared euclidean distance ij kx gamma serves natural distance measure 
case close connection vector quantization vq cluster cluster center code vector serves representative data space data items assigned cluster vectors quantize continuous vector space see chapter provide condensed representation data 
distance chapter 
measures squared euclidean distance defined feature vectors bring structure 
general approach assume explicit representation data items form feature vectors takes input pairwise dissimilarities ij items data set 
turn impossible determine representative vector quantization result clustering obtained mapping 
extensions minimal clustering paradigm topographic clustering hierarchical clustering 
topographic clustering algorithms see prominent representative try additionally preserve extract information proximities clusters 
hierarchical clustering schemes aim construction tree represents clusters different length scales data 
applications clustering exist applications clustering diverse fields pattern recognition see texture segmentation communications see vector quantization biochemistry see proteins psychology see business :10.1.1.130.3511
applications fall broadly categories data analysis visualization data compression preprocessing 
faced large data set feature vectors pairwise dissimilarity values reasonable strategy look cluster group structure data 
structure seen skeleton data serve basis exploration 
available criterion partitioning data analyzing groups separately 
case euclidean feature vectors obtained cluster centers feature vectors space data interpreted prototypes respective group data 
addition clustering algorithm provides information hierarchical topographic cluster structure interesting aspects data revealed 
communications data storage important compress data bandwidth communication channels capacity storage devices constrained 
way achieve vector quantization special case clustering provides code vectors representatives data vectors codebook transmitted stored data items need longer characterized corresponding feature vectors referred index relevant code vector data represented set code vectors compression ratios achieved 
particular method extended optimization codebook respect transmission errors induced channel noise source channel coding leads topographic vector quantization discussed chapter 
representation robust loss code labels transmission sudden changes bandwidth see neural gas 
hierarchical schemes allow transmission image data progressive resolution levels 
clustering methods serve preprocessing supervised learning techniques control tasks 
idea similar data compression find reduced representation data efficient application supervised methods 
approach texture segmentation robotics application 
clustering techniques initialize basis functions radial basis function networks preprocess data networks 
regard chapter 
preprocessing applications topographic mapping preservation spatial information clusters provide important information shown character recognition robotics application 
methods clustering different methods data clustering tried main approaches identified parametric non parametric clustering 
parametric methods assumptions underlying data structure 
general assumptions incorporated global optimality criterion cost function methods aim minimize 
class algorithms divided generative models reconstructive models focus 
non parametric methods fewer assumptions data structure typically follow local criterion construction clusters 
involve learning sense parameter adaptation discussed reasons completeness 
brief review clustering methods common 
non parametric approaches typical examples non parametric approach clustering agglomerative divisive algorithms produce dendrograms 
agglomerative version takes step clusters smallest dissimilarity merges starting clusters consisting single data items 
procedure depends define dissimilarity clusters consisting data point 
essentially methods single link clustering level pair data items clusters needs closer order merge clusters 
contrast complete link clustering pairs need closer leads clusters 
group averaging method 
divisive version clustering works opposite direction starting cluster containing data items cluster successively split smaller clusters dissimilarity criterion maximize dissimilarity clusters 
method applied non parametric setting applied tree structured vector quantization soft clustering algorithm special case stvq algorithm discussed chapter considered divisive algorithm 
latest developments non parametric clustering employs pleasant analogy physical properties model granular magnet 
potts spin assigned data item interactions spins taken decreasing function distance items 
depending temperature system exhibits distinct phases super 
super phase clusters spins strong interactions ordered spin spin correlation function partition spins corresponding data items clusters 
important advantage non parametric clustering methods assumptions underlying data distribution 
general require euclidean vector representation data items pairwise dissimilarities 
known perform poorly clusters overlap vary shape density size 
chapter 
generative models basic idea generative models clustering define parameterized mixture xjr sufficiently simple probability densities xjr taken gaussians xjr oe gammad exp gamma oe try adjust parameters cluster centers cluster variances oe mixing coefficients way achieve match distribution input data :10.1.1.130.110
achieved maximizing data likelihood ml posterior map additional prior information parameters available 
efficient em schemes exist perform optimization :10.1.1.18.5213
additional smoothness constraints imposed cluster centers approach perform called generative topographic mapping gtm preserves information spatial relations clusters 
generative mixture models offer advantages due fact define proper probability distribution data space 
conditional density estimation due probabilistic nature provide means dealing problem missing data active data selection 
disadvantages data modeled needs incorporated mixture densities approach limited data euclidean feature vectors opposed pairwise dissimilarity data see chapter 
reconstructive methods reconstructive clustering methods generally cost function way incorporates loss information incurred clustering procedure trying reconstruct original data compressed cluster representation 
data form euclidean data vectors task finding cluster representation allows accurate reconstruction original data possible closely relates clustering vector quantization 
cases resulting distortion measured squared euclidean distance original data vectors reconstructions 
basic algorithms optimize cost function described means algorithm uses online learning rule stochastic approximation lbg algorithm batch scheme vector quantization 
algorithms drawback greedy sense tend get stuck local minima cost function issue addressed cost function means lbg special case cost function discussed see chapter 
solution problem local minima stochastic methods close relation statistical physics simulated annealing applied clustering optimization problems slow due stochastic search multidimensional search space see chapter :10.1.1.123.7607
alternative idea deterministic annealing developed uses temperature annealing avoids stochastic search replaces deterministic search local minima free energy temperature see chapter 
leads efficient clustering algorithms additional property annealing natural hierarchy clusters emerges divisive manner 
order impose structural constraints emerging tree hierarchical clustering algorithms developed apply principle minimum cross entropy informative priors approximate chapter 
unstructured clustering solution see different approach output vector quantizer fed hierarchy 
extension basic clustering paradigm topographic clustering algorithms main topic thesis 
kmeans clustering cost function invariant permutation cluster indices topographic clustering symmetry broken predefined neighborhood imposed clusters 
result neighboring clusters represent close volumes data space helps visualization vector quantization representation robust noise cluster indices see chapter section details point connection clustering algorithms neural networks evident 
self organizing map som influential types neural networks interpreted topographic clustering algorithm 
som originally formulated kohonen heuristic online learning scheme served model developmental processes brain 
model consists neurons lattice defines neighborhood relations neurons 
incoming data vector neuron weight vector closest data vector called winner 
weight vector weight vectors neighbors updated direction data vector leading topographic mapping data space 
luttrell showed som seen computationally efficient approximation stochastic optimization cost function topographic clustering see chapter details 
particular provides framework probabilistic terms folded markov chains serves derive cost functions efficient coding mechanisms see chapter details 
aforementioned variants extensions vanilla reconstructive clustering operate euclidean data space directly settings conceivable 
idea perform clustering originally data space feature space related data space nonlinear fashion 
correspond preprocessing data computationally expensive feature space higher dimensionality data space 
problem solved algorithms expressed solely terms scalar products data space application called potential function method generalization called kernel trick support vector machine nonlinear principal component analysis :10.1.1.15.9362
scholkopf points approach clustering see chapter generalization topographic clustering 
mentioned section general representation data set purpose clustering dissimilarity matrix containing dissimilarity values pairs data items 
clustering cost functions pairwise dissimilarity data discussed pattern recognition literature earlier buhmann :10.1.1.130.3511
authors essentially formulate cost function dissimilarity data optimize technique deterministic annealing see chapter extension topographic clustering 
chapter 
overview thesis primary objective thesis presents principled approach clustering formulating problem probabilistic autoencoder framework folded markov chains 
suitable optimization technique deterministic annealing introduced performs robust optimization analogy cooling system statistical physics 
application deterministic annealing derived clustering cost functions leads algorithms soft topographic vector quantization stvq performs topographic clustering euclidean feature vectors kernel soft topographic mapping allows clustering high dimensional euclidean feature space application kernel trick soft topographic mapping proximity data stmp generalizes stvq arbitrary pairwise dissimilarity data mean field fashion 
algorithms analysed annealing process application demonstrated artificial real world data 
structure thesis remainder thesis structured follows 
chapter clustering cost functions pairwise dissimilarity data euclidean data derived probabilistic autoencoder framework folded markov chains chapter 
chapter available optimization techniques introduced focus simulated annealing serves conceptual basis deterministic annealing em algorithm 
chapter deterministic annealing applied cost function soft topographic vector quantization stvq algorithm derived 
stvq shown serve unifying framework known clustering algorithms applied problem compression image data sent noisy transmission channel chapter 
chapter annealing process analyzed detail critical temperatures phase transitions calculated terms covariance matrix data coupling matrix clusters 
analytical results verified computer simulations chapter 
chapter stvq algorithm extended high dimensional feature spaces related data space non linear fashion application kernel trick 
working kernel soft topographic mapping demonstrated handwritten digit data chapter 
chapter stvq algorithm generalized pairwise dissimilarity data approximating corresponding cost function mean field fashion 
resulting algorithm soft topographic mapping proximity data stmp applied artifical euclidean data dissimilarity matrix areas cat cerebral cortex chapter 
chapter summarizes discusses ideas 
chapter probabilistic autoencoder framework probabilistic unsupervised learning way looking unsupervised learning consider autoassociative neural networks 
basic idea force learning machine replicate input train learn identity map 
trivial course constraints obeyed 
paradigm feedforward neural networks constraint typically bottleneck feedforward connections limits bandwidth transmission forces network learn efficient representation input data represent faithfully possible bottleneck layer 
part network bottleneck interpreted encoder bottleneck network bandwidth limited transmission channel part bottleneck decoder 
framework generalized non deterministic case encoder transmission channel decoder conditional probabilities 
communication chain natural assume markov property assume output various stages process depends input history process 
leads framework folded markov chains introduced luttrell attempt unify different unsupervised learning schemes special cases probabilistic multilayer 
chapter folded markov chain framework derive cost functions topographic clustering pairwise dissimilarity data euclidean feature vectors 
cost functions form basis subsequently developed clustering algorithms 
general folded markov chain general folded markov chain fmc stages consists probabilistic transformations input item output item xl gamma intermediate items xl gamma corresponding bayes inverse transformations result reconstructed version input item denote probabilistic encoder stage stage conditional probability jx corresponding decoder jx 
bayes theorem relates conditional probabilities encoder decoder pairs follows jx jx chapter 
probabilistic autoencoder framework marginal probability stage joint probability fmc framework specified source probabilities probabilistic encoders jx gamma 
joint probability expressed xl jx gamma jx gamma theta ffi pl gamma gamma jx jx ffi kronecker delta ensures output encoder equal input decoder 
marginal probability obtained sum preceeding stages fmc gives gamma jx gamma jx gamma sums assumed run states respective summation variable 
note folded markov chain framework just continuum formulation sums replaced integrals probabilities replaced probability densities 
concerned variants vector quantization finite data sets discrete code indices discrete notation appropriate 
introduce measure cost distortion introduced encoding decoding process pair fx summation possible encodings stage folded markov chain provides cost function el reflects total distortion el gamma jx gamma jx gamma ffi theta pl gamma gamma jx jx cost function function encoders decoders yields optimality criterion construction probabilistic coding schemes 
concerned special case depicted 
derivation cost functions stage folded markov chain consider cost function stage fmc special case jx jx ffi jx jx decoders jx jx related corresponding encoders jx jx bayes theorem form jx jx jx jx chapter 
probabilistic autoencoder framework ae oe ae oe jx jx jx jx ffi oe oe illustration probabilistic autoencoder form stage folded markov chain 
data item encoded probabilistic encoders jx jx recovered corresponding decoders jx jx leading data item resulting distortion measured 
equation simplified cancelling summing making ffi 
leads expression depend decoders jx jx anymore jx jx jx jx marginal probability introduced expressed jx jx introducing notation jx jx jx cost function expressed solely terms encoders jx jx formulation cost function derived assumptions distortion measure effectively form basis soft topographic mapping proximity data stmp derived chapter 
chapter 
probabilistic autoencoder framework cost function tmp order contact known clustering results equations introduce specific notation point 
data items labelled indices representations denoted code labels choose hitherto probabilistic encoder jx rji deterministic 
case express encoder terms stochastic matrix ir thetan elements binary assignment variables ir ir may take values set 
accordance som literature fix second encoder express terms matrix rs thetan elements rs sjr subject constraints rs 
literature kohonen som rs called neighborhood function space neurons determines coupling neurons due spatial arrangement neural lattice 
interpretation consistent autoencoder model transition matrix noise internal representation data autoencoder 
clustering framework noise induces transitions cluster indices due channel noise transmission see chapter example source channel coding 
data dissimilarity matrix ij thetad notational conventions written cost function topographic mapping data items clusters tmp ir rs jt ts ku ij dissimilarity values ij factor introduced computational convenience 
note source distributions data items implicit sums data items 
relation topographic clustering seen follows 
cost ij incurred data items associated cluster 
data item cluster determined linear combination corresponding assignment variables ir weighted transition probabilities rs configurations low cost close pairs data vectors assigned cluster pair clusters high transition probability dissimilar pairs vectors assigned different clusters low transition probability 
denominator enforces coherent clusters normalizing cost cluster size 
consider special cases cost function tmp 
second encoder neighborhood matrix taken rs ffi rs cost function recovered equivalent cost function pairwise clustering introduced hofmann buhmann 
normalizing denominator ks introduced heuristic arguments cluster coherency 
derivation appears natural consequence bayes theorem applied probabilistic autoencoder 
ii special case mapping ir obtains form equivalent measure introduced unifying objective function topographic mappings 
iii euclidean data vectors simpler version cost function derived serves basis topographic vector quantization 
clear section 
chapter 
probabilistic autoencoder framework squared euclidean distance distortion starting simplify cost function different way assuming data items corresponding feature vectors euclidean vector space distortion measured squared euclidean distance vectors kx gamma written jx ffi jx kx gamma shorthand notations jx jx jx jx jx jx 
summed ffi bayes theorem form jx jx leads expression cost function jx jx kx gamma expanding norm kx gamma kx kx gamma delta performing summations possible obtains jx kx gamma fl fl fl fl fl jx fl fl fl fl fl expression rewritten order obtain expression involving squared euclidean distance data vector weighted mean data space jx fl fl fl fl fl gamma jx fl fl fl fl fl comparing expression derivation straightforward interpretation average squared euclidean distance vectors drawn independently jx twice variance vectors drawn jx see 
apply bayes theorem form jx jx replace decoder corresponding encoder jx kx gamma jx called cluster center depends formulation cost function euclidean vector representation data items effectively form basis algorithms soft topographic vector quantization kernel soft topographic mapping chapter chapter respectively 
chapter 
probabilistic autoencoder framework cost function adopting similar notation introduced tmp cost function write cost function luttrell called topographic vector quantization ir rs kx gamma encoder expressed terms binary stochastic matrix second encoder fixed 
additionally parameter vector dimension dn introduced concatenation cluster centers data space 
order understand cost function consider special case rs ffi rs reduced cost function central clustering 
relation clustering seen follows terms contribute cost ir cost incurred equals half squared euclidean distance kx gamma data vector cluster center corresponds winner take wta rule data item assigned cluster 
configuration minimal cost achieved distances data vectors cluster centers assigned minimal 
cluster centers live space data vectors serve representatives data space data vectors assigned 
considering vectors data set think divided tesselation cells cluster center 
effectively quantization space vector assigned represented cluster center name vector quantization 
considering full cost function matrix breaks permutation symmetry cluster indices induces coupling cluster centers 
elements looked transition probabilities induced channel noise original model proposed motivated example noisy vector quantization chapter 
transition probabilities closely related elements neighborhood matrix self organizing map algorithm 
context serve mainly visualization purposes due coupling data points close data space assigned clusters close sense high transition probability 
exact relation som pointed chapter 
clustering problem formulated optimization task minimize cost function cluster centers assignments chapter deal question efficiently solve task 
chapter optimization optimization methods general remarks clustering defined terms optimization problem previous chapter section briefly review known optimization techniques 
section devoted deterministic annealing specific procedure 
noted idea optimization closely related engineering problems traced back natural sciences especially physics 
idea physics formulate natural laws terms variational optimization principles equations motion 
probably famous example principle action see feynman entertaining text book account serve basis theoretical mechanics particle takes path action quantity action functional path difference kinetic potential energy integrated path 
description incorporates path opposed local nature differential equations 
example kind principle time optics formulated fermat 
apart principled considerations optimization plays extremely important role disciplines engineering economics logistics methods developed solve optimization problems fields 
problems continuous variables expressed terms linear cost functions appropriate constraints exist standard algorithms commonly referred linear programming lp quadratic programming qp respectively 
interesting cases variables optimized may discrete finite number values cost function constraints may non linear parameters 
cases elaborate schemes searching parameter space 
techniques introduced 
stochastic gradient descent particularly popular neural network implementations simulated annealing versatile optimization tools serves conceptual basis deterministic annealing :10.1.1.123.7607
stochastic gradient descent cost energy function depends parameter vector aim optimization find parameter values minimize 
way doing gradient descent cost surface defined 
starting chapter 
optimization initial guess parameter vector iteratively updated deltaw simple gradient descent steepest descent algorithms deltaw takes form deltaw gammaj re learning rate parameter chosen decreasing function time order speed convergence enforce accuracy near optimum 
batch algorithms exist perform nonlinear optimization gradient information conjugate gradient newton quasi newton methods particular sum squares errors levenberg marquardt algorithm see textbook account 
alternative batch optimization scheme sequential online update 
suppose cost function written sum contributions single data items data items random sequence update parameters deltaw gammaj re sufficiently small values average direction update approximate direction steepest descent 
time dependence analyzed robbins monroe prove convergence probability hold learning rate lim condition ensures successive updates sufficiently decrease magnitude second condition guarantees minimum reached third condition expresses necessity keeping sampling noise low 
practice small constant value effective elaborate schemes suggested 
online procedure advantages algorithm avoid getting stuck local minima effective cost function changes step 
ii data set contains redundancy sampling data lead reduction computational costs compared batch procedure 
iii data items discarded update storage capacity necessary 
iv underlying process changes slowly time behavior tracked online mode 
focus batch methods accessible analytic terms approximated stochastically online mode 
simulated annealing universally applicable optimization schemes simulated annealing 
ideas statistical physics seen application chapter 
optimization metropolis algorithm optimization 
parameter vector cost function assume finite number states fwg sake simplicity 
idea simulated annealing perform stochastic search parameter space fwg 
starting random initialization old candidate parameter vector new generated randomly vicinity old generating distribution specified terms deltaw new gamma old deltaw fi exp gamma fi deltaw dimensionality parameter space 
rule depending difference cost deltae new gamma old parameter vectors applied accepting rejecting new deltae accept deltae accept probability exp gammafi deltae fi called inverse temperature parameter 
rule results markov chain stochastic walk fwg constrained decreasing cost function step escape local minima cost function finite values fi 
probability distribution equilibrium calculated principle detailed balance new jw old old old jw new new yields gibbs distribution exp gammafie partition function fwg exp gammafie simulated annealing fi annealing parameter optimization varied search fi fi starting low values fi search space easily accessible value fi stepwise increased accessible volume search space narrowed global minimum cost function 
order find global minimum cost function careful annealing schedule fi chosen 
geman geman showed generating function schedule fi fi ln leads convergence probability global minimum cost function 
simulated annealing described advantages 
optimization large class cost functions irrespective nonlinearities discontinuities stochasticity 
ii boundary conditions constraints easily satisfied 
iii statistical guarantee finding optimal solution 
advantages prices 
indicated simulated annealing slow generality means difficult incorporate knowledge chapter 
optimization optimization problem hand procedure 
order limit computational costs optimization people frequently try speed convergence annealing process schedule technique referred simulated quenching 
ergodicity system ability reach parameter space lost convergence guarantee longer holds 
section deterministic annealing introduced alternative simulated annealing limited class problems 
deterministic annealing em deterministic annealing slow convergence simulated annealing results stochastic search parameter space costly computational terms 
optimization problems really interested minimum values cost function approximation resulting gibbs distribution 
transition rules clear distribution parameter space value fi going gibbs distribution anyway 
idea deterministic annealing calculate gibbs distribution parameter space directly find minimum 
minimum tracked low high values fi hoped coincide minimum original cost function 
deterministic annealing applied wide variety optimization problems including deformable models matching problems non metric multidimensional scaling supervised learning classification regression travelling salesman problem data clustering application going focus 
technique deterministic annealing works particular class cost functions necessary calculate approximate expression partition function equivalently free energy section focus cost functions form ir ir chapter binary stochastic matrix ir parameter vector real valued parameters 
cost function may depend data constant mentioned list parameters 
assume extra knowledge problem hand lead application principle maximum entropy inference derivation probability distribution parameter space 
principle advocated statistics framework jaynes states average cost fmg probability distribution maximally non committal respect missing data obtained maximizing entropy gamma fmg ln chapter 
optimization additional constraint fmg summations equations runs admissible states fmg 
known probability distribution derived maximum entropy principle maximum stability terms norm temperature parameter fi equivalently average cost changed 
argument obtained information geometry cost function corresponding family gibbs distributions parameterized fi forms trajectory space probability distributions minimal length 
properties ensure maximal robustness noise principle maximum entropy natural choice stochastic optimization scheme 
maximum entropy principle leads known gibbs distribution introduced context simulated annealing exp gammafie partition function fmg exp gammafie partition function free energy calculated gamma fi ln gamma fi ln fmg exp gammafie idea deterministic annealing assume thermodynamic equilibrium consider value fi effective cost function minimized 
context optimization motivated fact smoothed version original cost function recovered case fi optimal values sum fmg case dominated exp gammafie arg free energy limit fi written lim fi parameter vector minimizes free energy satisfies conditions case statistical physics solution problem depends calculation partition function 
assuming cost function partition function calculated fmg exp gammafi ir ir exp gammafie ir results convenient expression gamma fi ln exp gammafie ir chapter 
optimization optimality criterion ir ir ir exp gammafie ir exp gammafie ir probability assignment ir 
context clustering probability interpreted probability membership data item cluster note hm ir ir hm ir expectation value binary variable ir taken gibbs distribution 
deterministic annealing proceeds follows 
starting low values fi free energy minimized equivalently solved standard local minimization procedure gradient descent em algorithm described chapter 
subsequently fi increased annealing schedule resulting free energy minimized starting parameter values obtained previous value fi 
minimum low values fi tracked high values fi free energy coincides see 
minimum coincide low cost minimum original cost function global minimum 
convergence change optimal local minimum established puzicha point convergence global minimum expected general 
plot data space toy example consisting data points crosses dimension cluster centers fw circles 
shown positions cluster centers global local minimum cost function seen plot cost surface illustrate procedure consider toy example cost function special case previous chapter coupling clusters left ir kx gamma data points clusters cluster centers cost surface local global minimum neglecting interchange symmetry illustrated 
resulting cost function shown 
annealing process starts minimizing free energy low value fi minimum exists 
minimum tracked intermediate values fi chapter 
optimization free energy nearly coincides original cost function high values fi 
success deterministic annealing depends course annealing schedule fi 
hand increase fi quickly possible safe computation time hand fi increased high rate risk loosing minimum fi step 
practice exponential annealing schemes fi jfi effective chosen 
em algorithm deterministic annealing reduces problem finding global optimum cost function solution multiple local optimization problems family free energy functions fi parameterized fi 
local optimization achieved expectation maximization em algorithm 
em algorithm established way determining maximum likelihood parameter estimates problems unobserved missing data 
applied wide variety problems including latent variable density models supervised learning incomplete data hierarchical mixtures experts boltzmann machines various problems traditional statistics applications mentioned previous section deterministic annealing 
seminal dempster em algorithm method related information geometry statistical physics convergence properties known :10.1.1.33.2557
algorithm introduced general statistics framework relation optimization problem hand pointed 
suppose observed data missing data parameter vector goal find maximum likelihood estimator observed data joint probability 
em algorithm starts initial guess maximum likelihood parameters iteratively generates successive estimates repeating steps step compute probability missing values gamma 
step find hln maximized 
em algorithm repeatedly estimates probability distribution missing data previous parameter estimate step performs likelihood estimation step joint data expectation deltai taken previous estimate probability distribution missing data 
scheme shown converge sense step increases log likelihood ln local maximum 
relation statistical physics framework optimization identity gamma fi leads formulation em algorithm terms variational free energy hln gamma chapter 
optimization entropy distribution reformulate standard em algorithm terms variational free energy follows step find gamma minimized 
step find minimized 
minimizing common quantity formulation brings symmetry em algorithm optimization parameters side missing data 
temperature fi algorithm minimize free energy deterministic annealing procedure 
cost functions derived chapter essentially lead development different topographic clustering algorithms subsequent chapters 
chapter 
optimization effect fi optimization problem illustrated toy problem fig 

plot shows iso cost lines parameter space fw half due interchange symmetry exhibits local global optimum 
plot shows free energy small value fi 
landscape smoothed global optimum center mass data 
shows free energy intermediate value fi 
symmetry broken global optimum half moved away center mass data 
plot shows free energy large value fi assumed form original cost function 
dashed lines show position global minimum half original cost function 
chapter soft topographic vector quantization derivation stvq algorithm stationarity conditions consider cost function derived chapter squared euclidean distance distortion ir ir partial assignment costs ir rs kx gamma constraints ir rs cost function course exactly form assumed derivation expressions deterministic annealing chapter 
applying principle maximum entropy gibbs distribution obtained partition function involves sum possible states fmg leads free energy gamma fi ln gamma fi ln fmg exp gammafie fact partition function factorizes stationarity conditions cluster centers value temperature parameter fi ir ir chapter 
soft topographic vector quantization inserting partial assignment costs ir solving yields expression rs rs assignment probability data point cluster hm exp gamma fi st kx gamma exp gamma fi ut kx gamma hm expectation value binary assignment variable set fw gibbs distribution 
equation interpreted generalized centroid condition weighted mean data vectors 
optimal cluster centers fw positioned represent average data vectors assigned clusters weighted neighborhood transition matrix data vectors weighted assignment probabilities turn depend cluster centers seen 
assignment probabilities described soft min functions partial assignment costs ir fact calculated separately data vector consequence factorial form partition function 
means assignments data points clusters independent cluster centers 
deterministic annealing em algorithm prescriptions chapter deterministic annealing applied problem determining optimal cluster centers resulting algorithm called soft topographic vector quantization stvq pseudo code gives overview procedure 
soft topographic vector quantization stvq initialize hxi oe oe small calculate lookup table rs choose fi start fi final annealing factor convergence criterion ffl fi fi start fi fi final annealing repeat em step calculate eq 
step calculate new eq 
kw new gamma old ffl fi fi temperature parameter fi introduced control annealing procedure find solutions original cost function parameter interpretation 
analogy gaussian mixture models parameter fi chapter 
soft topographic vector quantization interpreted inverse variance data space determining resolution clustering 
consequently annealing process corresponds stepwise refinement representation data possible determine resolution final representation terminating annealing schedule appropriate value fi 
particularly appropriate avoid fitting data presence noise 
seen chapter increase fi gives rise phase transition split cluster centers related maximal variance data 
phenomenon supports idea think fi scale parameter 
practice interested solution high values fi convenient choose fi start fi fi critical value fi initial phase transition calculated avoid wasting computation time initial phase transition occurs 
derivatives stvq algorithm put derived algorithm stvq familiar context consider certain limits approximations lead family topographic clustering algorithms 
limiting case fi assignment probabilities yields batch version topographic vector quantizer discussed luttrell heskes 
winner take algorithm rs rs ffi st arg min uv kx gamma approximation rs ffi rs assignment probabilities leads new probabilistic version som called soft som 
modification provides important computational simplification omission convolution rs saves considerable amount computation time 
equations rs rs exp gamma fi kx gamma exp gamma fi kx gamma noted luttrell correspond nearest neighbor encoding general minimize cost function 
exact minimization achieved non zero transition probabilities taken account update rule determination winner stvq 
combines limiting case fi approximation rs ffi rs obtains batch version som kohonen original algorithm stochastic approximation discussed chapter 
equations chapter 
soft topographic vector quantization fi fi fi rs ffi rs rs ffi rs rs ffi rs rs ffi rs stvq som sc hc step step step step stvq family clustering algorithms 
rs som rs som som ffi st argmin kx gamma substituting rs ffi rs yields soft clustering procedure sc proposed rose limit fi recovers known lbg algorithm hc online version means clustering 
summarizes family topographic clustering algorithms 
stvq noisy source channel coding transmission noisy channel order demonstrate applicability stvq particular source channel coding algorithms applied compression image data sent noisy channel model decoded transmission 
data transmission scenario seen depicts process encoding noisy transmission decoding distortion measure stvq 
channel model binary symmetric channel characterized bit error rate ber fl number bits transition probabilities rs expressed terms hamming distance dh bit binary representations rs gamma fl gammad fl training set theta pixel gray value images taken different scenes 
images split blocks size theta encoding 
chapter 
soft topographic vector quantization size codebook chosen order achieve compression bit pixel bpp 
applied exponential annealing schedule determined start value fi start just critical fi split calculated chapter equation 
note transition matrix optimization corresponds embedding dimensional hypercube dimensional data space 
resulting codebooks tested encoding test image lena determining codebook simulating transmission indices noisy binary symmetric channel bit error rate reconstructing image codebook 
encoder channel noise rs decoder gamma distortion cartoon source channel coding procedure data communication 
input data grouped groups clusters labeled indices encoding stage 
indices transmitted noisy channel characterized set transition probabilities rs noise process 
soon index received decoder data reconstructed vector decoding stage represents data points assigned cluster encoding 
stvq combined error due clustering channel measured squared euclidean distance original data vector cluster center minimized averaged transitions results results source channel coding experiments summarized shows plot signal noise ratio pixel psnr function bit error rate stvq diamonds vertical crosses lbg oblique crosses 
stvq shows best performance especially high bers naturally far superior take account channel noise 
performs slightly worse approx 
db stvq 
considering fact computationally demanding stvq encoding due omission convolution rs result demonstrates efficiency source channel coding 
chapter 
soft topographic vector quantization ber stvq ber lbg comparison different vector quantizers image compression noisy channel bsc transmission reconstruction 
plot shows signal noise ratio pixel psnr defined log oe signal oe noise function bit error rate ber stvq optimized channel noise optimized ber lbg 
training set consisted theta pixel gray value images block size theta 
codebook size corresponding bit pixel bpp 
annealing schedule exponential convergence parameter ffl gamma lena test image 
shows generalization behavior codebook optimized ber fl rectangles 
codebook optimized fl performs worse appropriately trained codebooks values fl performs better lbg low values fl 
low values trained noisy case outperformed lbg robustness channel noise achieved expense optimal data representation noise free case 
provides visual impression performance different vector quantizers ber fl 
reconstruction stvq slightly better clearly superior reconstruction lbg 
chapter 
soft topographic vector quantization original lbg snr db stvq snr db snr db lena transmitted binary symmetric channel ber fl encoded reconstructed different vector quantization algorithms 
parameter settings 
chapter phase transitions stvq initial phase transition order understand annealing process temperature parameter fi instructive look representation data changes fi 
rose buhmann known cluster centers split increasing fi number relevant clusters resolution fi determined number clusters split point 
stvq permutation symmetry cluster centers broken couplings clusters introduced transition matrix changes stationary states splitting behavior cluster centers 
fi corresponds infinite temperature data point assigned cluster equal probability number cluster centers 
case cluster centers cluster centers located center mass data 
loss generality taylor expansion fw order yields rs rs fw rs rs fw assumption symmetrical rs sr expression evaluated relation fi gamma st gamma tu linearized fixed point equations fi rt covariance matrix data rt rs st gamma chapter 
phase transitions stvq elements matrix acts cluster indices 
system equations decouples transformation covariance matrix data space matrix cluster space 
transformation known principal component analysis pca 
denoting transformed cluster centers designate components new bases data space cluster space prime hat denote pca transformation fi eigenvalues eigenvectors 
non zero solutions fi 
critical fi fi max max center mass solution unstable clusters split new representation data set emerges 
fi depends data largest eigenvalue max covariance matrix eigenvector max denotes direction maximum variance oe max max data 
consequently split clusters occurs principal axis data space 
fi depends transition matrix largest eigenvalue max matrix largest eigenvalue max indicates eigenvector max dominant determines direction cluster space split occurs 
component vector expressed linear combination kr components kr eigenvectors kn matrix development cluster center component linearized fixed point equation depends value component eigenvector max principal axis data space eigenvector max indicates direction axis far cluster center moves relatively cluster centers linear approximation 
order express result terms eigenvectors eigenvalues observed set eigenvectors 
follows max identical eigenvector corresponds second largest eigenvalue max results extended fixed point equations 
matrix elements simply replaced elements rt rt gamma automatic selection feature dimensions similar analysis carried regard phenomenon automatic selection feature dimensions term kohonen context dimension reduction 
consider dimensional data space array clusters labeled dimensional index vectors couplings rs clusters defined array typically chosen monotonically decreasing function gamma simple representation input data achieved data significant variance dimensions 
case vectors lie dimensional subspace excess dimensions effectively ignored see 
variance chapter 
phase transitions stvq phenomenon dimension reduction automatic selection feature dimensions 
states minimal free energy shown phase transition oe transition oe dimensional array cluster centers dimensional data space 
chain clusters dimension data space subject periodic boundary conditions 
direction referred longitudinal dimension direction called transversal dimension 
dots represent data points filled circles locations cluster centers 
cluster centers labels differ connected lines 
transition probabilities rs correspond gaussian neighborhood function standard deviation oe 
parameter values fi ae lead critical standard deviation oe critical mode transition 
chapter 
phase transitions stvq data excess dimensions surpasses critical value original representation unstable array vectors folds excess dimensions represent see 
phenomenon studied formal way employing fokker planck approximation dynamics zero temperature som line learning algorithm 
analysis full stvq family investigating fixed point equations comparing results limiting case som 
phase transition discrete case purpose examine stability known fixed point 
consider case infinite number data points generated underlying probability distribution 
fixed point equations read rs dx rs dx exp gamma fi st kx gamma exp gamma fi ut kx gamma cluster indices dimensional index vectors lie dimensional cubic array delta delta delta ng 
assume rs theta obey rs kr notational convenience data space split subspaces theta embedding longitudinal dimensions elements excess transversal dimensions elements assume probability distribution data space factorize probability distribution transversal dimensions zero mean dx 
longitudinal dimensions data space assume factorization gamma consider system approximation ae finite 
variance longitudinal data space effectively infinite obtains fixed point see appendix ae gamma expanded order fixed point fw just 
assignment probability data point cluster fixedpoint state depends longitudinal components abusing notation write 
consider stability transversal dimensions determines critical parameters phase transition depicted 
rs dx see appendix equation obtains transversal components cluster centers linear approximation rs cs wt fw dx rs dx delta chapter 
phase transitions stvq denominator evaluates gamman see appendix equation average data space fixed point cluster singled 
inserting obtains fi rs st gamma tu dx covariance matrix transversal dimensions data space ae dx essentially correlation function assignment probabilities clusters fixed point state fw taken data space 
depends fi assignment probabilities 
note form rt taken rt rs st gamma tu 
equations decoupled data space transformation denoting components transformed cluster centers 
index respect eigenvector eigenvalue reads 
fi rs st gamma tu 
rs kr follows rs kr see appendix 
defining discrete convolution lattice functions gammas written 
fi gamma 
application discrete fourier transform exp delta leads decoupling cluster space obtains 
fi gamma 
fact modes space depend absolute value kkk due isotropy neighborhood function data distribution fixedpoint state 
equation non zero solutions fi gamma fi 
oe oe variance axis data space clear cluster centers automatically select direction transversal data space maximum variance oe max eigenvector max gives direction data space array cluster centers folds 
critical temperature fi transition occurs implicitly oe max fi gamma fi gamma critical mode mode solution minimal fi 
fi explicit expression critical variance oe max obtained oe max fi gamma fi chapter 
phase transitions stvq arg max gamma fi similar results derived approximation step applied 
resulting equations identical squared fi calculated approximation 
continuous gaussian case analytically determine values oe max fi choose rs gaussian variance oe distance kr gamma sk clusters array 
continuum approximation considered index vectors associated index vectors space real functions previously defined defined corresponding continuum conditions rs expressed rs kr gamma sk oe exp gamma kr gamma sk oe denotes dimensionality cluster array 
inserting replacing sums integrals yields see appendix rs kr gamma sk fi ae exp gamma fi ae kr gamma sk inserting fourier transformations kr kr obtains fi ae log ae fi oe gamma fi ji 
inserting provides critical variance oe max oe max fi oe ae ae ae mode interesting aspect fi oe ae appear play similar role 
interpreting fi inverse variance noise data space essentially sum variance data space fi variance oe noise cluster space scaled data space factor ae gamma results valid case fi corresponds 
obtains lim fi oe lim fi oe max oe ae chapter 
phase transitions stvq equation shows high values oe long ranged coupling clusters suppress high transversal modes 
seen critical variance oe max proportional variance neighborhood function oe scaled data space factor ae gamma stability fixed point state fw variance data transversal direction data space adjusted changing oe results carry som version algorithm oe replaced oe oe denotes variance som neighborhood function 
wavelength critical mode obtains ae oe oe critical variance oe max expressed terms half width homogeneous data distribution obtains oe oe results identical ritter schulten line version kohonen som algorithm gaussian neighborhood function fokker planck approach 
numerical results section numerical results validate analytical calculations illustrate deterministic annealing scheme 
stvq applied toy problem sufficiently simple transition matrix eigenvectors eigenvalues easily calculated 
order demonstrate effects advantages deterministic annealing scheme stvq dimensional array clusters dimensional data space considered 
behavior dimensional chain clusters dimensional data space investigated validate results automatic selection feature dimensions previous section 
toy problem consider dimensional data space data points generated elongated gaussian probability distribution gamma jcj gamma exp gamma gamma diagonal covariance matrix diag 
cluster centers coupled transition probability matrix gamma choice corresponds chain clusters cluster linked nearest neighbor transition probability second nearest neighbors uncoupled transition probabilities vanish 
magnitude governs coupling strength normalization factor included comply condition 
shows coordinates positions cluster centers data space functions inverse temperature parameter fi configuration minimal free chapter 
phase transitions stvq energy 
critical inverse temperature fi cluster centers split axis principal axis distribution data points 
accordance eigenvector max max gamma largest eigenvalue max matrix cluster centers move opposite positions principal axis remains center 
topologically correct ordering established initial phase transition 
plot projections cluster centers principal axis data functions fi toy problem cluster centers nearest neighbor coupling 
data points chosen randomly independently gaussian probability distribution gamma jcj gamma exp gamma gamma diagonal covariance matrix diag 
cluster centers initialized origin stvq applied different values fi 
stvq convergence criterion ffl gamma analytically determined critical value fi fi coupling strength 
corresponds point seen plot 
shows critical value fi temperature parameter function nearest neighbor coupling strength error bars indicate numerical results agreement theoretical prediction solid line 
inset displays average cost rs kx gamma function fi coupling strength 
visible drop average cost occurs fi 
note transition zone finite due finite size effects 
chapter 
phase transitions stvq plot critical value fi temperature parameter function coupling strength stvq toy problem 
error bars denote numerical results 
value cluster centers initialized origin fi linearly annealed fi fi fi fi final monitoring low values fi average cost constant 
lower error margins denote fi values change occurs upper error margins denote fi values large drop occurs 
line shows theoretical prediction calculated max oe max inset plot average cost function fi typical example 
visible drop occurs fi 
annealing dimensional array cluster centers consider dimensional data space set theta clusters labeled twodimensional index vectors delta delta delta 
theta data points lie equally spaced grid unit square 
transition probabilities rs chosen gaussian function distance index vectors rs exp gamma kr gamma sk oe normalization constant needed satisfy 
set transition probabilities corresponds square grid clusters commonly applications som 
shows snapshots combined heating cooling experiment best described terms temperature fi 
heating process annealing starts low temperature randomly initialized cluster centers temperature increased exponential scheme 
figures display series snapshots cluster centers heating 
defects grid indicate local minimum cost function introduced random initialization cluster centers preserved low temperatures 
gradually increased shallow local minima vanish grid chapter 
phase transitions stvq melting topological defects 
plots show snapshots cluster centers dimensional theta cluster array dimensional data space stvq different temperatures dots indicate cluster centers centers connected lines correspond pairs clusters transition probability rs highest 
starting local minimum cost function introduced random initialization preserved low temperature seen temperature increased exponentially theta figures illustrate corresponding melting topological defects 
shows positions cluster centers re cooling 
gaussian neighborhood function standard deviation oe input data consist data points square grid unit square 
ordered 
topologically ordered state reached corresponds global minimum free energy 
governs resolution representation data space localized defects melt away low temperature corresponds high resolution data space global twists melt away 
cooling temperature decreased starting high value corresponds state system cluster centers merged center mass data distribution 
annealing performed reverse heating schedule terminates corresponds global minimum free energy shown 
note ordered dimensional grid cluster centers established initial phase transition remains ordered configuration cooling process 
shows average cost measure quality data representation function temperature annealing experiments heating cooling 
displays du dt derivative average cost respect temperature function heating 
equivalent heat capacity ther chapter 
phase transitions stvq ln semi logarithmic plot average assignment cost hei function temperature cluster array 
upper curve shows exponential heating schedule starting local minimum cost function shown 
steps occur temperatures twists spatial arrangements cluster centers unfold 
lower curve shows scheme applied cooling direction 
cooling cluster centers remain topologically ordered arrangement cf 
figures 
normalization constant parameters 
interpreted measure progress quality data representation change temperature annealing see discussion heat capacity general statistical measure 
exhibits pronounced peaks temperatures correspond steps annealing rearrangements cluster centers occur 
behavior analogous physical systems undergo phase transitions reflects case qualitative change assignment cost triggered small quantitative change heat capacity may serve determine reasonable annealing schedule temperature parameter indicates critical points annealing 
automatic selection feature dimensions chain clusters consider data set data points drawn homogeneous probability distribution defined dimensional rectangular data space length variable width oe oe variance probability distribution axis data chapter 
phase transitions stvq ln semi logarithmic plot heat capacity du dt function temperature heating shown figures upper curve 
temperatures corresponding peaked minima heat capacity indicate transition points array cluster centers observed 
parameters figures 
space 
set clusters labeled indices delta delta delta ng 
transition probabilities rs chosen gaussian function distance indices rs exp gamma min kr gamma sk gamma kr gamma sk oe normalizes probabilities set transition probabilities corresponds linear chain clusters 
dimensional chain dimensional data space constitutes simplest non trivial case derived 
derived longitudinal space infinite size continuum limit periodic boundary conditions imposed longitudinal dimension data space transition probabilities rs cluster centers initialized see ae 
size system examined important aspects 
number clusters chosen large computationally feasible order reduce finite size effects mode spectrum order continuum approximation valid 
number data points chosen local inhomogeneities strongly bias result whilst keeping computation time chapter 
phase transitions stvq tractable 
shows spatial distribution cluster centers variance oe gradually increased oe oe phase transition 
chain folds excess dimension wave shape dominant wavelength illustrated depicts power fourier modes function oe critical value oe critical mode increases power dominates spatial arrangement cluster centers 
plot squared absolute amplitudes kw transversal fourier modes functions standard deviation oe data chain cluster centers shown 
modes largest wavelength shown 
phase transition oe mode selected chain folds curve 
parameters fi ae oe 
data points distributed uniformly data plane gamma theta gamma oe width data distribution direction 
shows average cost hei derivative oe functions oe numerical experiment shown 
critical standard deviation oe occurs derivative 
position obtain numerical results compares theoretical values oe solid line obtained obtained numerical simulations error bars 
numerical results agreement theoretical values obtained previous section hindsight justifies approximations employed derivation 
similar transitions data representation occur annealing fixed oe oe observed shows heat capacity case stepwise decrease leads smooth change representation initial state left inset folded state right inset chain 
observation interest chapter 
phase transitions stvq de ds ds plot average cost hei derivative scaled arb 
const 
functions standard deviation oe data set dimension chain cluster centers 
slope average cost shows clear change critical value oe interpolating oe minimum oe maximum derivative yields critical value oe arrow indicates theoretical prediction critical standard deviation oe 
parameters 
regard neural development biological systems 
interpreting fi noise parameter leads idea development cortical maps may triggered reduction neuronal noise widely accepted view change variance input data 
chapter 
phase transitions stvq plot critical standard deviation oe function temperature parameter fi chain cluster centers 
standard deviation oe data set transversal dimension linearly increased fixed fi critical value oe obtained derivative average cost shown 
upper bound error bars taken position minimum lower bound position maximum doe parameters 
chapter 
phase transitions stvq plot heat capacity du dt function temperature chain cluster centers 
starting initial state chain high temperature left inset temperature reduced linear steps fi fixed oe lowered heat capacity increases average cost reduced faster chain continuously transformed folded configuration cluster centers right inset 
vertical arrows indicate corresponding temperatures left right inset respectively 
parameters oe oe 
chapter kernel soft topographic mapping clustering high dimensional feature space chapter generalization topographic clustering possible effectively carry clustering possibly high dimensional feature space related input space nonlinear map 
idea inspired revival method proposed aizerman context support vector machines nonlinear pca :10.1.1.15.9362:10.1.1.42.1588
basic insight certain conditions inner products phi delta phi high dimensional space related data space nonlinear mapping phi 
computed efficiently kernel function conversely certain kernel functions data space shown correspond inner products phi delta phi space related data space mapping phi 
pointed trick applied possible express algorithm solely terms inner products data vectors 
replacing inner products kernel functions delta delta equivalent carrying algorithm inner product space allows take account higher order correlations input features 
mapping phi seen kind preprocessing bring features data go unnoticed algorithm run simply data space 
kernel methods applied soft topographic vector quantization making new class distance measures available topographic mappings 
idea suggestion scholkopf means clustering 
kernel trick central results chapter introduced derivation algorithm 
kernel trick mercer condition kernel trick kernel trick potential function method efficient way calculating inner products phi delta phi feature space related data space mapping phi phi 
possibly infinite dimensional hilbert space 
idea express inner product delta theta 
feature space terms kernel function theta 
chapter 
kernel soft topographic mapping data space phi delta phi conversely interest kernel function corresponds inner product phi delta phi mapping phi space question answered consider example 
delta easily seen phi relation holds 
mapping illustrated shows data vectors phi occupy submanifold dimension maximum 
note space relation holds 
minimal embedding space minimal dimensionality 
ax illustration mapping data space left higher dimensional feature space right 
calculating kernel function equivalent calculating inner product connected points right images left 
clearly distance points changes mapping phi example phi 
phi shown mapping phi square gamma theta gamma mercer condition general question kernel functions exists pair ff phig described properties answered mercer condition exists mapping phi expansion inner product phi phi phi delta phi chapter 
kernel soft topographic mapping kernel delta delta positive semidefinite fulfills dxdy square integrable function dx may easy general check positive kernel function quite useful kernel functions satisfy condition discussed 
admissible kernel functions typical kernels far include 
polynomial kernel delta 
sigmoidal kernel tanh delta theta 
radial basis function kernel exp gamma yk oe discuss polynomial kernel corresponds finite dimensional feature space polynomial kernel degree acting data delta previous example corresponded case 
introducing see dimension feature space related term powers expansion 
denoting dimensions phi powers appear mapping corresponding expressed follows phi delta delta deltar 
delta delta delta delta delta delta means space corresponding spanned monomials degree minimum embedding space dimensionality gamma 
result hints combinatorial explosion encountered trying operate directly 
sigmoidal kernel introduced solve classification tasks kernel trick bears sigmoidal activation function artificial neural networks 
shown tanh delta theta satisfy mercer condition parameter values theta 
gaussian radial basis function kernel exp gamma yk oe successfully supervised learning tasks support vector machine 
corresponds infinite dimensional feature space expansion polynomials infinite number terms 
type kernel consequently impossible perform algorithm directly 
chapter 
kernel soft topographic mapping derivation kernel soft topographic mapping topographic clustering feature space phi 
mapping data space possibly high dimensional feature space cost function topographic vector quantization see chapter tmk ir rs phi gamma constraints ir rs phi images input vectors dg number clusters cluster centers ir binary assignment variables rs transition probabilities 
note generalization cost function recovered phi taken identity mapping 
derivation chapter obtains stationarity conditions cluster centers phi rs hm rs hm hm ir average binary assignment variable ir constitutes assignment probability data point cluster hm ir exp gammafie ir exp gammafie partial assignment costs ir ir rs phi gamma principle sufficient formulate em algorithm expectation step step maximization step step done chapter stvq algorithm 
feature space high dimensionality procedure explicitly mapping data vectors performing clustering computationally intractable 
order solve problem kernel trick introduced section applied 
application kernel trick avoid explicit mapping 
phi calculation squared euclidean distance cluster centers expressed linear combinations transformed input vectors phi ir phi chapter 
kernel soft topographic mapping restricts subspace spanned phi restriction holds minimum cluster centers outside subspace case increase total cost 
comparing obtains coefficients ir ir rs hm rs hm js partial assignment costs written solely terms inner products data vectors terms kernel functions data space 
obtains ir rs phi gamma rs phi delta phi gamma phi delta js phi phi delta phi js ks rs gamma js js ks kernel trick phi delta phi 
em deterministic annealing formulation just leads efficient em scheme computation assignment probabilities hm ir value fi 
step assignment probabilities hm ir estimated previous estimate partial assignment costs ir step partial assignment costs ir recalculated terms coefficients ir obtained previous estimate assignment probabilities hm ir order avoid convergence local minima deterministic annealing parameter fi employed discussed chapter obtains kernel soft topographic mapping 
diagram gives overview algorithm pseudo code kernel soft topographic mapping initialize ir ir ir random number calculate lookup tables rs choose fi start fi final ffl fi fi start fi fi final annealing repeat em step calculate hm ir eq 
step calculate new ir eq 
calculate new ir eq 
ke new ir gamma old ir ffl fi fi chapter 
kernel soft topographic mapping note som approximation introduced stvq chapter applied 
leave convolution rs save computation time cost convergence guarantee local minimum free energy em algorithm 
critical temperature phase transition discussed chapter soft topographic vector quantization annealing process induces phase transitions cluster representation cluster centers split order represent data space 
splits related qualitative changes optimization problem taken account annealing process 
order avoid wasting computation time start annealing value fi start fi fi value split representation occurs 
essentially derivation chapter initial phase transition critical parameter value fi phase transitions expressed terms largest eigenvalues matrices fi max max elements rs rt ts gamma expression obtained stvq chapter 
covariance matrix phi phi centered images phi phi gamma phi data points 
singular value decomposition seen matrix inner products ij phi delta phi non zero eigenvalues max max elements ij calculated kernel table ij ij ij gamma lj gamma im lm numerical simulations rbf kernel effect rbf kernel numerical simulations focus rbf kernel exp gamma kx gamma yk oe effect width oe outcome topographic clustering examined 
kernel function regular inner product data space thought introducing new distance measure data space 
order get better understanding new distance measure introduce rbf kernel squared euclidean distance determines partial assignment costs 
case phi gives phi gamma gamma exp gamma kx gamma oe chapter 
kernel soft topographic mapping plotted fig 
function distance kx gamma different values oe 
seen small values oe close vectors influence assignment costs vectors away assignment costs constant distance 
high values oe significant influence vectors greater distance 
plot function distance kx gamma data space different values oe 
value oe determines range relevant assignment costs cluster 
distances kx gamma 
oe assignment costs constant 
simulation results handwritten digit data effect parameter oe generation topographic maps handwritten digit data united states postal service usps buffalo studied 
data theta pixel gray value images corresponding dimensional vectors values gamma neighborhood function rs chosen reflects topology theta lattice clusters coupling strength decreasing gaussian function distance rs exp gamma sk oe oe 
transition probabilities rs normalized unit probability clusters chapter 
kernel soft topographic mapping topographic maps generated time vectors smoothed gaussian width usps data input 
algorithm described previous section applied choosing exponential annealing schedule fi start fi 
convergence criterion em algorithm ffl gamma resulting maps shown 
cluster centers high dimensional space hardly visualized plot shows cluster center projection ir upper left map generated polynomial kernel delta corresponding standard result soft topographic vector quantization 
maps generated radial basis function kernel exp gamma yk oe oe 
cases inner products kernel functions scaled dimensionality input space parameters independent 
clearly maps show topography similar data vectors digits large overlaps mapped nearby clusters lattice 
map oe cluster centers tend occupy regions data space high density input points minimize amount data points large area assignment cost constantly maximal 
digit represented top right corner map region high data density digits tend look alike 
opposite true case oe 
data vectors greater distance influence cost cluster position cluster center 
consequence cluster centers positioned way take account data points full oe range 
parameter oe possible fine tune range relevant determination cluster centers 
algorithm introduced chapter able perform topographic clustering high dimensional feature spaces minimal computational extra cost compared stvq 
different point view efficient optimization scheme clustering euclidean data space cost functions involving distance measures squared euclidean distance 
points view beneficial depending applications involved 
possible applications 
possibility document clustering documents represented binary vectors non zero entries related word occurrences words key word list 
polynomial kernel incorporate products word occurrences binary values correspond logical ands 
higher order structure documents accessible clustering algorithm 
possible application unsupervised texture segmentation 
textures characterized higher order statistics pixel representation suitable kernel implement distance measure relates different textures reasonable way 
summary remains seen kernels suitable applications 
clear new flexibility gained wide choice kernels corresponding distance measures 
chapter 
kernel soft topographic mapping polynomial kernel gaussian kernel oe gaussian kernel oe gaussian kernel oe topographic maps handwritten digit data gaussian rbf kernels different width 
clusters coupled oe 
applied data vectors dimension consisting theta gray value images values gamma 
optimization parameters ffl gamma shown projections ir cluster centers arranged lattice induced chapter soft topographic mapping proximity data topographic clustering pairwise dissimilarity data field unsupervised learning researchers focussed analysis methods data vectors space assumed euclidean 
examples kind include principal component analysis pca independent component analysis ica vector quantization vq latent variable models self organizing maps som 
algorithms far stvq chapter chapter belong group 
data items points euclidean data space restrict oneself set pairwise proximities measured particular empirical sciences psychology biochemistry linguistics economics 
strategies data analysis pursued time pairwise clustering detects cluster structure dissimilarity data multidimensional scaling deals embedding pairwise proximity data euclidean space purpose visualization 
approaches combined hofmann buhmann :10.1.1.130.3511
alternative chapter presents generalization stvq pairwise proximity data 
resulting algorithm soft topographic mapping proximity data stmp creates topographic maps pairwise proximity data 
approach mean field approximation possible calculate approximate averages gibbs distribution results application principle maximum entropy cost function derived chapter 
derivation soft topographic mapping proximity data mean field approximation consider cost function tmp derived chapter topographic mapping proximity data tmp ir rs jt ts ku ij chapter 
soft topographic mapping proximity data constraints ir rs order apply deterministic annealing scheme introduced chapter principle maximum entropy applied yields gibbs distribution exp gammafie tmp fi inverse temperature partition function fmg exp gammafie tmp summation partition function legal assignment matrices fmg 
cost function tmp linear assignment variables ir probability distribution factorize consequence difficult calculate averages 
saul jordan cost function linear assignment variables ir ir parameterized partial assignment costs ir thetan leads probability distribution zq exp gammafie factorizes 
partial assignment costs determined minimize kullback leibler kl divergence kl qjp fmg ln kullback leibler divergence expressed terms free energies corresponding original cost function approximation tmp respectively kl qjp fmg log exp gammafie fm exp gammafie tmp exp gammafie tmp fm exp gammafie fi gamma tmp tmp gamma average taken 
kl qjp equality holds known upper bound free energy derived recovered tmp tmp gamma note approach implicitly assumes assignments data items clusters independent sense hm ir jr hm ir ihm jr assumption chapter 
soft topographic mapping proximity data valid case ae known hofmann elaborate mean field approaches tap method conceivable contribute little development applicable algorithm 
minimizing upper bound yields conditions kv tmp gamma obtains tmp kv gamma hm kr kv kr conditions optimal mean fields kr calculated detailed appendix 
note cost function invariant substitution ij ji ij ji 
making simplifying assumption zero self dissimilarity ii neglecting terms order obtains optimal mean fields kr kr rs js kj gamma ij weighting coefficients js hm jt ih ts hm lu ih average assignment variables hm kr exp kr exp gammafie ks please note similarities equations corresponding equations chapter equations respectively 
step algorithms exactly form weighting coefficients ir ir 
noting average cluster assignments invariant transformation ir ir seen equations mean fields stmp partial assignment costs equivalent ij gammak 
sense stmp equivalent dissimilarity value data items corresponds inner product feature representations data items 
hand result surprising stmp derived quite distinct ideas approximation pairwise dissimilarities stmp clustering high dimensional feature spaces inner products kernel trick 
inner products similarity measures euclidean spaces sense thought special case stmp 
em deterministic annealing mean field approximation free energy leads efficient em scheme computation assignment probabilities hm ir value fi 
step assignment probabilities hm ir estimated previous estimate partial chapter 
soft topographic mapping proximity data assignment costs kr step partial assignment costs kr recalculated terms coefficients ir obtained previous estimate assignment probabilities hm ir 
order avoid convergence local minima deterministic annealing parameter fi employed discussed chapter obtains soft topographic mapping proximity data stmp 
diagram summarizes stmp algorithm pseudo code soft topographic mapping proximity data stmp initialize ir ir ir random number calculate lookup table rs prepare dissimilarity matrix ij data choose fi start fi final annealing factor convergence criterion ffl fi fi start fi fi final annealing repeat em step calculate hm ir eq 
step calculate new ir eq 
calculate new ir eq 
ke new ir gamma old ir ffl fi fi som approximation stmp chapter family clustering algorithms derived efficient approximation stvq 
introduce equivalent approximation stmp 
step equation seen soft min function mean fields kr leaving convolution rs leads new prescription calculation mean fields ks js kj gamma ij som approximation 
approximation computationally efficient exact update 
drawback iteration scheme longer performs exact minimization free energy 
robustness som algorithm approximation numerical results demonstrate usefulness approximation 
critical temperature phase transition discussed chapter stvq chapter annealing fi induces changes cluster representation 
cluster centers data space split principal axis data eigen structure coupling matrix chapter 
soft topographic mapping proximity data 
exists euclidean data space dissimilarity approach critical behavior cluster assignments decreasing temperature examined 
consider case infinite temperature fi 
hm kr equation mean fields kr fi kr kj gamma ij linearize right hand side equation kr performing mv gamma mv kr gamma kr kr kr mv gamma mv delta delta delta evaluation expression yields kr gamma kr fi delta km gamma rv mv gamma mv delta km im kj gamma km gamma ij gamma rv rs vs gamma equations decoupled transforming shifted mean fields kr gamma kr eigen bases delta gamma 
denoting transformed mean fields ae arrives ae fi delta gamma ae assuming rs sr equation non vanishing solutions fi delta gamma delta gamma eigenvalues delta gamma respectively 
means fixed point state equation unstable increase fi fi delta max gamma max delta max gamma max denote largest eigenvalues delta gamma respectively 
instability course characterized corresponding eigenvectors delta max gamma max gamma max determines mode index space unstable see chapter details delta max identified principal coordinate classical metric multidimensional scaling 
instructive consider special case delta understand meaning 
assume dissimilarity matrix represents squared euclidean distances ij jx gamma data vectors zero mean dimensional euclidean space 
easy show delta km delta xm chapter 
soft topographic mapping proximity data case theta matrix delta maximum rank singular value decomposition seen non zero eigenvalues delta covariance matrix data 
eigenvectors correspond principal axes data space eigenvalues associated variances maximum variance data space determines critical temperature instability occurs principal axis 
general case dissimilarities concluded delta determines width pseudo variance ensemble items 
results section extended stmp som approximation 
matrix gamma modified omitting convolution rs resulting gamma rv vr gamma numerical simulations toy example noisy spiral section stmp generate topographic representation dimensional noisy spiral left dimensional euclidean space distance data 
data points generated sin cos gaussian noise zero mean standard deviation oe 
data points data points plots noisy spiral left corresponding dissimilarity matrix right 
data points generated oe 
dissimilarity matrix obtained ij jx gamma plotted rows top columns left right correspond data points order generation spiral increasing 
chapter 
soft topographic mapping proximity data dissimilarity matrix calculated squared euclidean distances data points ij kx gamma depicted right 
neighborhood matrix chosen reflects topology chain clusters coupling strength decreasing gaussian function distance rs exp gamma sk oe oe rs normalized unit probability clusters note choice oe corresponds narrow neighbourhood 
annealing lead topological defects representation 
stmp applied som approximation choosing exponential annealing schedule fi start 
exact tmp update som approx 
plot average assignment cost hei function temperature parameter fi stmp som approximation applied dissimilarity matrix noisy spiral 
topology clusters chain equation oe 
annealing schedule exponential convergence criterion em algorithm ffl gamma average cost calculated equation binary assignment variables ir replaced averages hm ir vertical line indicates value fi calculated equation 
seen variants stmp converge final value average cost function low temperature 
split occurs close fi value predicted indicated vertical line plot 
due weak coupling chapter 
soft topographic mapping proximity data som approximation induces slightly earlier transition fi accordance 
stmp detects reduced dimensionality spiral correctly forms groups spiral 
shows assignment matrix data points order generation spiral chain clusters 
high temperature left assignments fuzzy emerging topography visible immediately phase transition 
diagonal structure assignment matrix low temperature right indicates topography map small defects stem gaussian noise data 
neurons data points neurons data points plot average assignments hm ir high temperature fi left low temperature fi right stmp som approximation applied noisy spiral 
dark corresponds high probability assignment 
data parameters 
topographic map cat cerebral cortex consider example sense interpreted representing euclidean space 
input data consist matrix connection strengths cortical areas cat 
data collected text figures available anatomical literature connections assigned dissimilarity values follows self connection strong dense connection intermediate connection weak connection absent connection 
data originally analysed ordinal data stronger assumption dissimilarity values represent ratio scale 
true values connection strength known crude approximation 
serves demonstration purposes shows robustness described method 
original matrix ij completely symmetrical due differences afferent efferent connections application stmp equivalent substitution ij ij ji 
original matrix nearly symmetrical introduces small mean square deviation dissimilarity true matrix ij gamma ij 
topology chosen dimensional map clusters coupled accordance equation dimensional index vectors oe 
shows dissimilarity matrix columns rows sorted stmp assignment results 
dominant block chapter 
soft topographic mapping proximity data data points data points dissimilarity matrix areas cat cerebral cortex 
areas sorted cluster assignments top left right 
horizontal vertical lines show groups areas assigned clusters 
dark means similiar 
topology clusters theta lattice oe 
annealing scheme exponential fi start fi convergence criterion em algorithm ffl gamma chapter 
soft topographic mapping proximity data diagonal structure reflects fact areas assigned cluster similar 
additionally seen areas assigned clusters far apart lattice similar assigned neighboring clusters 
displays areas assigned clusters map stmp 
coherent regions map seen represent cortical systems visual auditory somatosensory 
visual areas ps exception occupy cluster part main visual region 
position justified fact areas connections system 
general observed primary areas areas visual system areas sii somatosensory system areas ai aii auditory system placed corners edge map 
higher areas crosstalk centrally located map 
example epp posterior part posterior gyrus visual auditory association area represented center map direct visual neighbors 
comparison shows solution metric multidimensional scaling dissimilarity matrix 
local dissimilarity structure preserved dimensional embedding regions clearly separated 
summary map plausible visualization connection patterns cat cerebral cortex 
clear arbitrary coarse topography theta square map fully express rich cortical structures 
prior knowledge connection patterns available encoded topology clusters improve representation 
stmp stmp algorithm introduced chapter generalizes topographic clustering arbitrary distance measures mean field approximation 
distance measure data set coupling rs set clusters chosen freely flexibility opens applications empirical sciences psychology economics biochemistry 
generally characterization set data items pairwise dissimilarities natural representation discrimination clustering tasks fewer assumptions vector space representation 
particular structured objects feature representation difficult define characterized pairwise dissimilarities analysed stmp 
similarity stmp hints close relation pairwise dissimilarities vector space representations inner products 
stmp acts dissimilarity data directly avoids preprocessing step embedding data euclidean space 
procedure applied problems classification regression topic ongoing research 
chapter 
soft topographic mapping proximity data dls aes alg sva sii alls bl am bm ai aii vp ssf epp siv rs dp tem ps ig cga er hipp psb il sb ia la pl cgp connection map cat cerebral cortex 
map shows cortical areas mapped lattice theta clusters 
cortical systems visual auditory delta delta delta somatosensory mapped coherent regions visual areas ps occupy cluster apart main visual region 
parameters 
chapter 
soft topographic mapping proximity data alls dls alg aes sva ps ai aii dp vp ssf epp tem sii siv am bm bl ia ig cga cgp la rs pl il psb sb er hipp metric multidimensional scaling solution dissimilarity matrix cat cerebral cortex 
plot shows embedding cortical areas dimensions 
regions areas belong identified letter visual auditory somatosensory 
embedding achieved determining eigenvectors matrix delta projecting column vectors delta corresponding data item eigenvectors largest associated eigenvalues 
shown scatter plot projections eigenvectors associated largest eigenvalues scaled square root respective eigenvalue 
chapter central theme topographic clustering 
starting idea probabilistic autoencoder cost functions clustering derived euclidean data vectors arbitrary dissimilarity data 
multimodality cost functions necessary find appropriate optimization scheme strikes balance optimality computational costs 
soft topographic vector quantization stvq deterministic annealing principles statistical physics introduced combined expectation maximization algorithm known tool statistics finding maximum likelihood solutions problems missing data 
variation temperature parameter optimization leads phase transitions cluster representation 
gave rise detailed analysis annealing process phase transitions involved 
phase transitions characterized terms properties data preassigned cluster structure 
order perform topographic clustering high dimensional feature spaces kernel trick applied possible calculate inner products feature space calculating value kernel function data space 
method possible take account higher order correlations input features change distance measure data space retaining advantages deterministic annealing em kernel soft topographic mapping 
data may represented euclidean feature vectors pairwise dissimilarities data items 
general approach taken develop topographic mapping proximity data deterministic annealing stmp 
interestingly approach mean field approximation turned incorporate kernel algorithm special case dissimilarities chosen negative inner products data vectors euclidean space 
summary ideas statistical physics statistics combined create new ways clustering data unsupervised data analysis visualization data compression 
plethora new ideas grown 
starting point autoencoder concept opens possibilities role 
complex models probabilistic encoders lead ex chapter 
representation data 
form conceptual bridge unsupervised supervised learning 
relation transfer ideas generalization learning dynamics realm supervised learning topics established unsupervised learning 
deterministic annealing scheme optimization proved efficient clustering context nonlinear optimization problems supervised learning 
idea tried different architectures nonlinear optimization widespread problem examine applicability deterministic annealing systematically 
set ideas motivated fact pairwise dissimilarity data matrix grows number data items 
order keep problem computationally tractable incorporate mechanisms deal missing data 
obviously involves assumptions data compatible problem hand 
possibly efficient way dealing large amount data active data selection query data promote learning process 
different criteria suggested certainly interesting direction research 
general idea describing set data items terms mutual dissimilarities direct original representation supervised learning problems classification regression 
assumptions prior knowledge problem incorporated distance measure avoiding implicit assumptions introduced vector space representations 
currently idea actively pursued results way 
standard classification paradigm equivalence relations learned complemented learning objectives 
example learning preference relations investigated purpose information retrieval 
chapter appendix proof fixed point property set cluster centers fw qualifies fixed point satisfies rs dx rs dx exp gamma fi st kx gamma exp gamma fi ut kx gamma consider transversal dimensions 
inserting yields rs dx rs dx obtains numerator rs dx rs dx dx mean assumed zero 
satisfied 
evaluation longitudinal dimensions insert obtain rs dx rs dx ae gamma equation written average dx probability distribution rs rs dx rs second step identity rs dx chapter 
appendix 
equation shown summing sides yielding unity 
demonstrate validity needs show symmetrical ae gamma homogeneous sufficient show rs symmetrical ae gamma equivalent rs rs ae gamma gamma rs exp gamma fi st kx gamma ae gamma gamma exp gamma fi ut kx gamma ae gamma gamma rs kr follows rs gammas substituting gamma gamma write rs rs exp gamma fi gammas kx gamma ae gamma gamma exp gamma fi ut kx gamma ae gamma gamma rs exp gamma fi kx gamma ae gamma exp gamma fi ut kx gamma ae gamma rs probability distribution symmetrical ae gamma consequently dx ae gamma correct fixed point 
derivation symmetry properties assignment correlations shown rs kr follows rs kr starting express rs rs exp gamma fi kr ks fl fl flx gamma ae gamma fl fl fl exp gamma fi ku fl fl gamma ae gamma fl fl jj dx substituting gamma non singular length preserving transformation matrix krk obtains rs exp gamma fi ka gammas gammat kt fl fl flx gamma ae gamma gamma fl fl fl exp gamma fi ku gammat fl fl gamma ae gamma gamma fl fl jj dx chapter 
appendix substituting gamma ae gamma gamma leads rs gamma ae gamma exp gamma fi ka gammas gammat kt fl fl flx gamma ae gamma fl fl fl exp gamma fi ku gammaa gamma sk fl fl gamma ae gamma fl fl jj dx gamma ae gamma exp gamma fi ka gammas gammat kt fl fl flx gamma ae gamma fl fl fl exp gamma fi ku gammat fl fl gamma ae gamma fl fl jj dx comparing seen rs function gamma gamma ae gamma 
case particular choice gamman length preserving linear transformation follows rs kr evaluation assignment correlation gaussian neighborhood functions starting shown calculate approximation rs homogeneous isotropic gaussian neighborhood function continuum approximation 
inserting assignment probabilities ground state gives rs ae exp gamma fi rt st fl fl flx gamma fl fl fl exp gamma fi ut fl fl flx gamma fl fl fl dx evaluate expression rt kx gamma continuum approximation sums replaced integrals property fixed point ae gamma 
gives rt fl fl flx gamma fl fl fl oe exp gamma kr gamma tk oe fl fl flx gamma ae gamma fl fl fl dt oe exp gamma kt oe fl fl flx gamma ae gamma gamma delta fl fl fl dt evaluation integral straightforward yields rt fl fl flx gamma fl fl fl fl fl flx gamma fl fl fl ae gamma oe inserting observing expression exp gamma gammafi ae gamma oe delta appears factor numerator denominator cancels arrives rs ae exp gamma fi fl fl flx gamma ae gamma fl fl fl fl fl flx gamma ae gamma fl fl fl exp gamma fi fl fl gamma ae gamma fl fl jj dx chapter 
appendix denominator integrand approximated exp gamma fi fl fl flx gamma ae gamma fl fl fl exp gamma fi fl fl flx gamma ae gamma fl fl fl du ae fi numerator integrand rewritten exp gamma fi fl fl flx gamma ae gamma fl fl fl fl fl flx gamma ae gamma fl fl fl exp gamma fi fl fl fl gamma ae gamma fl fl fl fl fl gamma gamma fl fl fl inserting exp gamma fi fl fl fl gamma ae gamma fl fl fl dx fi obtains continuum approximation rs rs kr gamma sk fi ae exp gamma fi ae kr gamma sk derivation mean field equations relations kr lu kr lu rs gamma lu lu ku lu gamma ws kw lu lu ws obtains derivative averaged cost function tmp tmp kv rs ts kv ir jt lu ae ij rs ts hm kr kv jt lu rs kj hm kt kv ir lu ts ik ffi rt hm kr kv lu rs ii gamma hm kw kv ws jt ir lu rs ij chapter 
appendix ii ij ji obtains tmp kv hm kr kv rs ts jt lu rs theta kj gamma ws iw lu ws ij comparing equations optimal mean fields kr equation 
bibliography aizerman braverman 
theoretical foundations potential function method pattern recognition learning 
automation remote control 
amari 
em algorithm information geometry neural network learning 
neural computation 
amari 
information geometry em em algorithms neural networks 
neural networks 
barto sutton watkins 
learning sequential decision making 
gabriel moore editors learning computational neuroscience foundations adaptive networks pages 
mit press cambridge ma 
bauer riesenhuber 
phase diagrams self organizing maps 
physical review 
bell sejnowski 
information maximization approach blind blind deconvolution 
neural computation 
bishop 
neural network pattern recognition 
clarendon press oxford 
bishop svens en williams 
em optimization latent variable density models 
touretzky mozer hasselmo editors advances neural information processing systems pages cambridge ma 
mit press 
bishop svens en williams 
gtm generative topographic mapping 
neural computation 
bishop tipping 
hierarchical latent variable model data visualization 
technical report ncrg neural computing research group aston university 
domany 
data clustering model granular magnet 
neural computation 
borg 
multidimensional similarity structure analysis volume springer series statistics 
springer verlag berlin heidelberg 
buhmann 
data clustering learning 
arbib editor handbook brain theory neural networks 
mit press cambridge ma 
bibliography buhmann hofmann 
maximum entropy approach pairwise data clustering 
proceedings international conference pattern recognition volume ii pages 
ieee computer society press 
buhmann 
vector quantization complexity costs 
ieee transactions information theory 
buhmann 
complexity optimized data clustering competitive neural networks 
neural computation 
burger graepel obermayer 
phase transitions soft topographic vector quantization 
gerstner 
nicoud editors artificial neural networks icann volume pages berlin germany 
springer verlag 
burger graepel obermayer 
annealed self organizing map source channel coding 
advances neural information processing systems cambridge ma 
mit press 
press 
burges 
tutorial support vector machines pattern recognition 
submitted data mining knowledge discovery 
choe sirosh miikkulainen 
interconnected self organizing maps hand written digit recognition 
touretzky mozer hasselmo editors advances neural information processing systems pages cambridge ma 
mit press 
cohn ghahramani jordan 
active learning statistical models 
technical report ai memo cbcl artificial intelligence laboratory mit 
cortes vapnik :10.1.1.15.9362
support vector networks 
machine learning 
courant hilbert 
methods mathematical physics 
interscience publishers new york english edition 
csisz ar 
divergence geometry probability distributions minimization problems 
annals probability 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
duda hart 
pattern classification scene analysis 
wiley new york 
titterington 
modification mean field em algorithm factorial learning 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma 
mit press 
bibliography durbin mitchison 
dimension reduction framework understanding cortical maps 
nature 
durbin szeliski yuille 
analysis elastic net approach travelling salesman problem 
neural computation 
durbin willshaw 
analogue approach travelling salesman problem elastic net method 
nature 
feynman leighton sands 
feynman lectures physics 
addison wesley publishing reading ma 
geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
ghahramani hinton 
em algorithm mixtures factor analysers 
technical report crg tr dept computer science university toronto 
ghahramani jordan 
learning incomplete data 
technical report ai memo cbcl artificial lab mit 
ghahramani jordan 
supervised learning incomplete data em approach 
cowan tesauro alspector editors advances neural information processing systems pages san francisco ca 
morgan kaufmann publishers 
gold rangarajan 
softassign versus softmax benchmarks combinatorial optimization 
touretzky mozer hasselmo editors advances neural information processing systems pages cambridge ma 
mit press 
gold rangarajan mjolsness 
learning clustering point graph matching distance measures 
neural computation 
goldfarb 
new approach pattern recognition 
kanal editor progress pattern recognition volume machine intelligence pattern recognition pages 
noth holland amsterdam 
sejnowski 
unifying objective function topographic mappings 
neural computation 
gower 
distance properties latent root vector methods multivariate analysis 
biometrika 
graepel burger obermayer 
deterministic annealing topographic vector quantization self organising maps 
kohonen editor proceedings workshop self organising maps volume proceedings artificial intelligence pages 
infix 
bibliography graepel burger obermayer 
phase transitions stochastic selforganizing maps 
physical review 
graepel burger obermayer 
self organizing maps generalizations new optimization techniques 
submitted neurocomputing 
graepel obermayer 
fuzzy topographic kernel clustering 
brauer editor proceedings th gi workshop fuzzy neuro systems pages 
graepel obermayer 
stochastic self organizing map proximity data 
submitted neural computation 
hadley 
nonlinear dynamic programming 
addison wesley 
haykin 
neural networks comprehensive foundation 
macmillan college new york ny 
hecht nielsen 
networks 
applied optics 
herbrich graepel obermayer 
learning preference relation information retrieval 
working notes icml workshop information retrieval 
press 
hertz krogh palmer 
theory neural computation volume santa fe institute studies science complexity 
redwood city ca 
heskes kappen 
error potentials self organization 
ieee icnn 
heskes kappen 
self organizing nonparametric regression 
fogelman soulie gallinari editors artificial neural networks icann pages paris 
ec cie 
hofmann buhmann 
central pairwise data clustering competitive neural networks 
cowan tesauro alspector editors advances neural information processing systems pages san francisco ca 
morgan kaufmann publishers 
hofmann buhmann :10.1.1.130.3511
pairwise data clustering deterministic annealing 
ieee transactions pattern analysis machine intelligence 
hofmann buhmann 
multidimensional scaling data clustering 
tesauro touretzky leen editors advances neural information processing systems pages cambridge ma 
mit press 
hofmann buhmann 
annealed neural gas network robust vector quantization 
malsburg seelen sendhoff editors artificial neural networks icann pages berlin heidelberg 
springer verlag 
bibliography hofmann buhmann 
inferring hierarchical clustering structures deterministic annealing 
simoudis han fayyad editors proceedings second int 
conf 
knowledge discovery data mining kdd pages 
aaai press 
hofmann puzicha buhmann 
optimization approach unsupervised hierarchical texture segmentation 
proceedings international conference image processing icip santa barbara 
hofmann puzicha buhmann 
unsupervised segmentation textured images pairwise data clustering 
proceedings international conference image processing icip pages lausanne switzerland 
hotelling 
analysis complex statistical variables principal components 
journal educational psychology 

simulated annealing practice versus theory 
journal 
computational modelling 
jaynes 
information theory statistical mechanics 
physical review 
jolliffe 
principal component analysis 
springer series statistics 
springerverlag berlin heidelberg 
jordan xu 
convergence results em approach mixtures experts architectures 
neural networks 
jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 
kirkpatrick gelatt vecchi :10.1.1.123.7607
optimization simulated annealing 
science 
buhmann 
multidimensional scaling deterministic annealing 
pelillo hancock editors energy minimization methods computer vision pattern recognition volume pages berlin heidelberg 
springer verlag 

deterministic annealing density estimation multivariate normal mixtures 
physical review 
kohonen 
analysis simple self organizing process 
biological cybernetics 
kohonen 
self organized formation topological correct feature maps 
biological cybernetics 
kohonen 
self organization associative memory 
springer series information sciences 
springer verlag berlin heidelberg nd edition 
kohonen 
self organizing maps 
springer verlag berlin heidelberg 
bibliography kruskal 
linear transformation multivariate data reveal clustering 
multidimensional scaling theory application behavioral sciences theory new york london 
seminar press 
linde gray 
algorithm vector quantizer design 
ieee transactions communications 
luttrell 
hierarchical vector quantisation 
iee proceedings part volume pages 
luttrell 
derivation class training algorithms 
ieee transactions neural networks 
luttrell 
code vector density topographic mappings scalar case 
ieee transactions neural networks 
luttrell 
self supervised training hierarchical vector 
iee international conference artificial neural networks 
luttrell 
bayesian analysis self organizing maps 
neural computation 
mackay 
information objective functions active data selection 
neural computation 
macqueen 
methods classification analysis multivariate observations 
neyman editors proceedings fifth berkeley symposium mathematical statistic probability pages ca 
university california press 

theoretical results nonlinear principal component analysis 
submitted ieee transactions neural networks 
mclachlan krishnan 
em algorithm extensions 
wiley series probability statistics 
john wiley sons new york ny 
metropolis rosenbluth rosenbluth teller teller 
equations state calculations fast computing machines 
journal chemical physics 
michie spiegelhalter taylor 
machine learning neural statistical classification 
series artificial intelligence 
ellis horwood 
miller rose 
combined source channel vector quantization deterministic annealing 
ieee transactions communications 
miller rose 
hierarchical unsupervised learning growing phase transitions 
neural computation 
miller rao rose gersho 
global optimization technique statistical classifier design 
ieee transactions signal processing 
bibliography cherkassky 
self organization iterative kernel smoothing process 
neural computation 
murata 
muller amari 
adaptive line learning changing environments 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma 
mit press 
neal hinton 
new view em algorithm justifies incremental variants 
technical report department computer science university toronto 
obermayer schulten 
statistical mechanical analysis selforganization pattern formation developement visual maps 
physical review 
obermayer ritter schulten 
large scale simulations self organizing neural networks parallel computers application biological modelling 
parallel computing 
obermayer ritter schulten 
principle formation spatial structure cortical feature maps 
proceedings national academy science 
parra deco 
statistical independence novelty detection information preserving nonlinear maps 
neural computation 

minimum property free energy 
physical review 
puzicha hofmann buhmann 
deterministic annealing fast physical heuristics real time optimization large systems 
proceedings th imacs world conference scientific computation modelling applied mathematics 
rao miller rose gersho 
mixture experts regression modeling deterministic annealing 
ieee transactions signal processing accepted publication 
ripley 
pattern recognition neural networks 
cambridge university press cambridge st edition 
ritter schulten 
convergence properties kohonen topology conserving maps fluctuations stability dimension selection 
biological cybernetics 
ritter martinetz schulten 
neural computation self organizing maps 
addison wesley reading ma 
robbins monroe 
stochastic approximation method 
annals mathematical statistic 
rose gurewitz fox 
statistical mechanics phase transitions clustering 
physical review letters 
bibliography rose gurewitz fox 
vector quantization deterministic annealing 
ieee transactions information theory 
rose gurewitz fox 
constrained clustering optimization method 
ieee transactions pattern analysis machine intelligence 
roweis 
em algorithms pca spca 
advances neural information processing systems cambridge ma 
mit press 
press 
saul jordan 
exploiting tractable substructures intractable networks 
touretzky mozer hasselmo editors advances neural information processing systems pages cambridge ma 
mit press 
young 
analysis connectivity cat cerebral cortex 
journal neuroscience 

specific heat general statistical measure 
zeitschrift fur physik 
scholkopf burges vapnik 
extracting support data task 
fayyad uthurusamy editors international conference knowledge discovery data mining pages menlo park ca 
aaai press 
scholkopf smola 
mller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
scholkopf sung burges girosi niyogi poggio vapnik 
nonlinear component analysis kernel eigenvalue problem 
technical report mpi fur kybernetik tubingen 

neural network analysis russian banks 
kohonen editor proceedings workshop self organising maps pages 
simi 
statistical mechanics underlying theory elastic neural optimisation 
network 
simi 
constrained nets graph matching quadratic assignment problems 
neural computation 
singh bertsekas 
reinforcement learning dynamic channel allocation cellular telephone systems 
advances neural information processing systems cambridge ma 
mit press 
sutton 
learning predict methods temporal differences 
machine learning 
tesauro 
td gammon self teaching backgammon program achieves master level play 
neural computation 
tishby levine 
alternative approach inference 
physical review 
bibliography tipping bishop 
mixtures principal component analysers 
technical report ncrg aston university birmingham 
tipping bishop 
mixtures principal component analysers 
technical report ncrg aston university birmingham 
tresp ahmad 
training neural networks deficient data 
cowan tesauro alspector editors advances neural information processing systems pages san francisco ca 
morgan kaufman publishers 

topology selection self organizing maps 
network computation neural systems 

hyperparameter selection self organizing maps 
neural computation 
valiant 
theory learnable 
communications acm 
van de kappen 
boltzmann machines em algorithm 
technical report department medical physics biophysics university nijmegen foundation neural networks snn 
vapnik 
nature statistical learning 
springer verlag berlin heidelberg germany 
wu 
convergence properties em algorithm 
annals statistics 
xu jordan :10.1.1.18.5213
convergence properties em algorithm gaussian mixtures 
neural computation 
yuille 
generalized deformable models statistical physics matching problems 
neural computation 
yuille kosowsky 
statistical physics algorithms converge 
neural computation 
yuille 
statistical physics mixtures distributions em algorithm 
neural computation 
