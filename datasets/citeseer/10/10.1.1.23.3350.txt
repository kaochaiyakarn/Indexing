ieee transactions pattern analysis machine intelligence vol 
may short papers bayesian method fitting parametric nonparametric models noisy data michael werman daniel simple paradigm fitting models parametric nonparametric noisy data resolves problems associated classical mse algorithms 
done considering point model possible source data point 
paradigm solve problems ill posed classical mse approach fitting segment opposed line 
shown achieve excellent results general curves presence strong discontinuities 
results shown number fitting problems including lines circles elliptic arcs segments rectangles general curves contaminated gaussian uniform noise 
index terms bayesian fitting parametric models nonparametric models 
common practice fit models lines circles implicit polynomials data points minimizing sum squared distances points model mse mean square error approach 
mse algorithm may natural fact implicitly assumes data point version point model closest 
assumption clearly false leads bias instance fitting circles data contaminated strong noise 
mse algorithm suffers drawback differentiate large model small 
instance fitting line segment image data know slope location fitted segment points 
mse criterion differentiate correct segment segment long mse error respect data 
offer simple paradigm fitting parametric models solves problems 
done considering point parametric model possible source data point 
paradigm extended nonparametric models gives results data strong discontinuities 
show results method lines segments circles elliptic arcs rectangles general curves 
gaussian uniform noise models considered 
previous papers describe square techniques fit parameters noisy data different numerical techniques linear approximations needed computations 
see example werman institute computer science hebrew university jerusalem jerusalem israel 
mail werman cs huji ac il 
department computer science university haifa haifa israel 
mail cs haifa ac il 
manuscript received sept revised may accepted may 
recommended acceptance meer 
information obtaining reprints article please send mail tpami computer org ieeecs log number 
ieee ordinary squares estimate shown consistent regression problem 
papers different solutions heuristics fitting circles ellipses parametric curves different statistical optimization techniques see example 
papers related bayesian techniques specific cases parametric nonparametric curve surface fitting 
idea associating cloud influence data point compute better straight line fitting general error criterion point line distance 
interesting approximate solution traveling salesman problem offered nonparametric path pulled cities controlled term tries keep short possible 
differs bayesian formulation treatment parametric models offered 
general differs previous mainly precisely map estimate model usually map estimate model denoised data points computed approximated 
extend fitting general nonparametric case 
suggested algorithm data points parametric model dm defined set parameters common fitting algorithm choose instance model called mse mean square error function defined dist dm pi attains minimum fd dist dm pi squared distance pi model 
bayesian justification minimizing mse function follows wishes maximize probability certain model instance data 
bayes formula assuming uniform distribution different model instances independent data pr pr djm pr pr djm pr yn pr assuming gaussian measurement noise variance common approximate pr const exp dist pi point model closest pi 
multiplying logarithms ignoring constants easy see maximizing approximate probability equivalent minimizing mse function 
approximation fails cases notably instance large values 
correct expression pr const exp dist pi pr dp ieee transactions pattern analysis machine intelligence vol 
may point generally bayes rule pr prob prob prob pi pr pr pr pr noise model pr priori distribution points show examples different priors parametric models uniform prior want concentrate effect integration choice model 
experience taught data points available effect model prior optimal fit small 
fitting parametric models give examples applying proposed method fitting lines segments circles elliptic arcs rectangles 
line proceed apply fitting paradigm described lines chance gives classical mse result assumptions 
priori lines equiprobable 

priori points line equiprobable 

noise additive gaussian value irrelevant 

points independent samples line 
data xi yi denoting line pr pr pr pr yn pr xi yi jl pr xi yi jl prob yn pr xi yi jp pr dp exp dist xi yi pr dp exp dist xi yi dt exp dist xi yi rob lj xi yi yn dist xi yi log prob lj xn yn equal additive constant dist xi yi map estimate line minimum 
argument gives map estimate flat flat sum squared distances data smallest 
case paradigm suggested agrees classical mse paradigm shall show case models 
circle proceed apply fitting paradigm circle 
data xi yi denoting parameters circle center radius assuming noise additive gaussian pr yn pr xi yi ja pr xi yi ja pr xi yi jp pr dp closed form expression integral estimated quickly expressing infinite series swiftly converges proof left due lack space 
general integral gaussian circle expressed exp ds comparison mse algorithm exp circle mse algorithm known biased noise gives estimate radius average larger true radius 
empirically verified method suggested unbiased adding random noise running optimization process times 
results converged true radius 
see fig 
typical examples fitting method offered compared mse fit 
comparison outlier sensitivity vs method moments estimator center circle uniform noise pi pi measured noisy points 
estimator radius compared behavior estimator proposed presence outliers 
points selected circle radius gaussian noise unit variance added 
outliers chosen uniformly square ranged best fit circle computed 
repeated times ieee transactions pattern analysis machine intelligence vol 
may fig 

examples mse suggested fit circle 
cases true circle gold data points red gaussian noise unit variance mse fit blue suggested fit green 
examples reflect typical result noise large regard radius mse fit biased suggested fit 
improvement suggested method apparent right circle radius left circle radius 
average optimal radius mse method computed circles unit radius unit noise variance indicating strong bias 
average radius method suggested 
fig 
average radius plotted type outliers method suggested robust method moments 
method tailored handle outliers better results probably achieved combining various methods realm robust statistics 
elliptic arc elliptic arc determined parameters entire ellipse angles define arc 
algorithm finding map arc proceeds way circle difference integration arc complicated 
fig 
example arc noisy data points sampled reconstructed arc line segment model computed paradigm suggested best fit segment 
done classical mse methods distinguish different length segments mse error see fig 

bayesian paradigm offered naturally solves problem fits big rigorous heuristics penalize area volume fit fig 

average radius method green method moments red function outlier size outliers randomly selected square 
correct radius 
ieee transactions pattern analysis machine intelligence vol 
may fig 

elliptic arc dark data points red elliptic arc fitted suggested method blue 
example 
continuing circle probability pr point pi segment proportional exp dist pi dp integral easily expressed error function erf 
multiplying data points gives probability 
see fig 
example segment fitting 
axis aligned rectangle axis aligned rectangles useful descriptions pattern recognition model cartesian products intervals opposed case circle model rectangle interior 
tenenbaum considered problem learning concepts small numbers positive examples turned axis aligned rectangle fitting problem solved similar model noise 
proceed apply fitting paradigm rectangle 
data xi yi denoting parameters rectangle lower left hand upper right hand corners assuming noise additive gaussian fig 

mse paradigm fitting segment inherently ill posed increase error function segment long 
penalty red portions segment change mse function 
method suggested penalize portions far data value integrand small increase segment length decreasing integration element dp effect smaller value integral 
pr yn pr xi yi ja pr xi yi ja pr xi yi jp pr dp erf erf yi erf xi erf yi xi axis aligned rectangles proceed apply fitting paradigm axis aligned rectangles 
data xi yi denoting rectangles pr jd pr xi yi ja pr xi yi ja equal pr xi yi ja pr xi yi ja just sum probabilities rectangles computed rectangle case 
see fig 
example 
line uniform noise uniform noise shape circle square defined follows pr area pi probability pr positive iff pi ieee transactions pattern analysis machine intelligence vol 
may fig 

straight line segment dark segment points red line segment fitted suggested method blue 
portion best mse fit line 
example fit line noisy points noise uniform unit size circles data easy see integral defining probability data point pi proportional length line intersection unit circle pi 
finding optimal line equivalent finding line circles centered data points product lengths intersections circles maximal 
see fig 
example 
fig 

example fitting axis aligned rectangles set points 
noisy data points fitted rectangles 
red blue points interior points lower upper rectangle respectively 
original rectangles superimposed fit 
ieee transactions pattern analysis machine intelligence vol 
may fig 

fitting line points uniform noise shape circle 
resulting line nice intuitive interpretation 
line maximizes product lengths intersections circles data points 
interesting note standard fitting paradigm uniform noise probability line circles data points identical 
method described yields sharper result albeit necessarily unique 
extending paradigm nonparametric models curve data algorithms described implemented section parametric models extended general nonparametric curves 
previous derivations easy see probability curve sparse data proportional pr pr dp curve represented discrete points close probability may approximated expression ln xm pik exp cjk curve length parametric models define pr 
factor cjk stands length element curve 
combined term standard smoothness term xx xy yy dc arrive optimal solution 
paradigm may viewed standard regularization data term replaced 
worthwhile look see leads curve sticks data portions curve far away data contribute little integrand due presence pik exp term smaller move away data 
portions result larger value leads smaller value entire expression 
amply demonstrated data consists version step function note fitted curve suffer wellknown gibbs phenomena yields spurious curve parts away data see fig 

order force resulting curve continuous parameterized initial point distance consecutive points different different curves fixed specific curve angles consecutive fig 

regularized fit sampled step function demonstrating known gibbs phenomena fit data obtained method suggested 
ieee transactions pattern analysis machine intelligence vol 
may fig 

regularization right curve higher left 
points 
keeping distance consecutive points fixed iteration helps avoid tears restored curve 
forego regularization 
question suggests case fitting general nonparametric curve drop regularization term purely bayesian approach outlined 
demonstrated answer expect obtain reasonable results 
look trivial case data points measurement noise arbitrarily small standard deviation denote bp bq small circles respectively 
curve connecting denote lp lq lengths bp bq respectively length portion lie bp bq 
total length curve denoted lp lq 
recall probability curve assuming measurement noise proportional kc pk exp kc qk dc exp dc making small relative radii bp bq integrand integrals arbitrarily small portion curve length limit expression bounded lp lq lp lq clear order probability large small possible relative lp lq 
curve left fig 
smaller probability right fig 

easy see limit optimal curve wanders unbounded amount time close possible shortest route 
hardly surprising aforementioned case segment model tries stick data 
regularization applied unreasonable curve result 
note convergence fitting paradigm offered usually leads optimization nonconvex functions 
arbitrary starting point optimization converges local minimum 
problem solved initializing optimization mse fit 
powell mead methods convergence fast parametric models 
nonparametric fitting general curves section took minute xp digital workstation 
research bayesian paradigm fitting parametric nonparametric models natural mathematically rigorous superior classical mse method albeit higher computational cost required optimizing nontrivial cost functions fitted model 
hope try alleviate problem extend paradigm models splines 
acknowledgments authors dr alexander department statistics university haifa interesting discussions 
research supported israeli ministry science 
werman sub pixel bayesian estimation albedo height int computer vision vol 
pp 

cheng polynomial regression errors variables royal statistical soc 
vol 
pp 

durbin szeliski yuille analysis elastic net approach travelling salesman problem neural computation vol 
pp 

fisher direct squares fitting ellipses proc 
int conf 
pattern recognition vol 
pp 

gull baysian data analysis straight line fitting maximum entropy bayesian methods pp 

cooper describing complicated objects implicit polynomials ieee trans 
pattern analysis machine intelligence vol 
pp 
jan 
werman variations regularization proc 
th int conf 
pattern recognition pp 

werman full bayesian approach curve surface reconstruction math 
imaging vision vol 
pp 

werman probabilistic analysis regularization ieee trans 
pattern analysis machine intelligence vol 
pp 
oct 
bruckstein set points ieee trans 
pattern analysis machine intelligence vol 
pp 
apr 
meer estimation bilinear contraints computer vision proc 
int conf 
computer vision pp 

pnueli bruckstein hough techniques fast optimization linear constant velocity motion moving influence fields pattern recognition letters vol 
pp 

press flannery teukolsky vetterling numerical recipes 
cambridge univ press 
sampson fitting conic sections scattered data iterative improvement bookstein algorithm computer vision graphics image processing vol 
pp 

bajcsy recovery parametric models range images case superquadrics global deformations ieee trans 
pattern analysis machine intelligence vol 
pp 

taubin estimation planar curves surfaces nonplanar space curves defined implicit equations applications edge range image segmentation ieee trans 
pattern analysis machine intelligence vol 
pp 
nov 
tenenbaum bayesian modeling human concept learning advances neural information processing systems 
haralick performance analysis line circle fitting digital images proc 
workshop performance characteristics vision algorithms 
werman fitting second degree curve presence error ieee trans 
pattern analysis machine intelligence vol 
pp 

zhang parameter estimation techniques tutorial application conic fitting image vision computing vol 
pp 

