bayes net toolbox matlab kevin murphy department computer science university california berkeley berkeley ca october bayes net toolbox bnt open source matlab package directed graphical models 
bnt supports kinds nodes probability distributions exact approximate inference parameter structure learning static dynamic models 
bnt widely teaching research web page received hits may 
discuss broad spectrum issues related graphical models directed undirected describe high level bnt designed cope 
compare bnt software packages graphical models nascent ort 
decade graphical models emerged powerful unifying formalism probabilistic models widely statistics machine learning engineering ranging mixture models hidden markov models hmms factor analysis pca kalman lters 
reason described quotation jor graphical models marriage probability theory graph theory 
provide natural tool dealing problems occur applied mathematics engineering uncertainty complexity particular playing increasingly important role design analysis machine learning algorithms 
fundamental idea graphical model notion modularity complex system built combining simpler parts 
probability theory provides glue parts combined ensuring system consistent providing ways interface models data 
graph theoretic side graphical models provides intuitively appealing interface humans model highly interacting sets variables data structure lends naturally design ecient general purpose algorithms 
eld lacked corresponding general purpose software package 
bun bun rst argue possibility need general tool 
article describes attempt build package called bayes net toolbox bnt 
describe issues arise representation inference learning graphical models 
describe approaches existing software packages adopt issues 
describe bnt overcomes shortcomings existing packages 
reader assumed familiar basics graphical models see example books jen jor 
representation graphical models come main avors directed undirected 
possible combine directed undirected graphs 
discuss turn 
directed graphical models directed acyclic graph dag models known bayesian belief networks popular ai community partly lend causal interpretation pea structure easy design hand expert system 
dag models useful modelling temporal data dynamical systems encode arrow time 
models called dbns dynamic bayesian networks 
dag models popular bayesian statistics community parameters represented explicitly nodes random variables endowed distributions priors 
resulting graph provides concise speci cation model exploited computationally gibbs sampling 
known bugs package 
dag model includes decision utility nodes chance nodes known uence decision diagram optimal decision making 
kind graphical model called dependency network hcm allows directed cycles 
useful data visualization de ne unique joint distribution see hcm details 
parameterization directed models directed model bayes nets dependency nets specifying local conditional probability distributions cpds distributions jpa represents node pa parents 
packages assume nodes represent discrete categorical random variables cpds multinomials represented tables 
tabular cpds simple represent learn inference see section disadvantage requiring number parameters exponential number parents 
representations require linear number parameters proposed including noisy pea generalizations hen sri die mh logistic sigmoid function nea 
decision trees represent cpds variable number parameters useful variable parent selection inside structure learning algorithm see section 
regression trees similar www mrc cam ac uk bugs continuous random variable 
feedforward neural networks multi layer perceptrons mlps conditional linear gaussian clg generalized linear models model cpds continuous nodes 
bayesian modelling need richer variety cpds dirichlet priors multinomial parameters wishart gamma priors variance precision parameters gaussian priors weight matrices 
non parametric distributions 
easy users de ne exotic cpds 
diculties arise wish inference corresponding model try learn parameters data 
discuss issues 
undirected graphical models undirected graphical models known markov networks common physics computer vision communities 
instance ising models markov random fields mrfs just grid structured markov networks 
statistics community undirected models model multiway contingency tables case called hierarchical log linear models 
parameterization undirected models parameters markov network clique potentials 
instance grid correspond edge potentials neighboring pixels variables discrete categorical potentials represented tables 
dag case may require large number parameters clique binary nodes specifying requires parameters 
parameter tying assuming edges grid problem domains image processing 
domains text processing maximum entropy methods popular 
approach de nes potential nodes xn gibbs exponential family distribution set features exp domains support features implicitely de ne graph structure 
example suppose function function graph nodes looks mixed directed undirected graphical models possible combine directed undirected graphs called chain graph 
common example image processing hidden nodes connected undirected grid hidden node child contains observed value pixel see 
far inference concerned link hidden pixel observed pixel represented directed undirected arc directed arc representation preferred comes learning see section 
see www cs cmu edu maxent html excellent collection tutorials papers maxent 
chain graph image processing 
shaded node observed pixel caused hidden parent clear hidden causes correlated modelled markov random field pairwise potentials 
factor graph formalism general way graph structure represent global models necessarily probabilistic terms local terms factors 
possible inter convert representations information lost graph structure process information implicitely represented parameters 
ect computational complexity inference model 
inference inference mean computing jx represents set observed variables represents set hidden variables value interested estimating irrelevant nuisance hidden variables 
instance represent patient disease represent observed symptoms 
represent unknown parameter data 
main kinds inference exact approximate 
discuss 
exact inference exact inference sense having closed form solution possible limited set cases notably hidden nodes discrete nodes hidden observed linear gaussian distributions case network just sparse parameterization joint multivariate gaussian sk rg 
expert systems hidden markov models hmms fall category factor analysis kalman lters fall 
main kinds exact inference algorithms dag models directed undirected graphs 
dag inference algorithms exploit chain rule decomposition joint jx jx essentially push sums inside products marginalize irrelevant hidden nodes eciently ld dec am called variable elimination algorithm 
result computation single marginal jx 
general inference algorithms de ned terms message passing tree 
original graph undirected cycles loops converted socalled junction tree triangulation cutset conditioning pea ps 
tree may directed undirected messages may passed parallel sequentially computation messages may may involve division operation 
instance pearl algorithm pea formulated directed trees division hugin jlo algorithm jlo formulated undirected trees division belief propagation formulated undirected networks division 
algorithms essentially equivalent 
advantage message passing algorithms arises interested computing marginals simultaneously necessary learning dynamic programming avoid redundant computation involved calling variable elimination times variable 
variable elimination somewhat easier implement enables certain optimizations exploit knowledge speci query 
potentials implement inference algorithm works terms sums products variable elimination message passing necessary represent local distribution object supports operations methods summation integration multiplication optionally division 
call object potential just non negative function variables domain 
random variables domain discrete represent potential multidimensional array table 
random variables domain jointly gaussian represent potential multivariate gaussian simply store mean covariance scale factor 
variables discrete gaussian represent potential conditional gaussian cg table gaussians table scalars 
variables discrete random variables discrete utility variables represent potential pair tables useful uence diagrams 
types potentials described see mur 
cg potentials represent nite mixtures gaussians 
unfortunately representation closed 
potential domain discrete variable possible values continuous variable mixture gaussians hasn got smaller 
repeated applications sums products cause size representation blow 
approximation weak marginalisation lau reduces mixture gaussians single gaussian moment matching 
implementation lau numerically unstable improved lj 
see min 
messages passed sequentially scheduling usually uses passes called collect distribute forwards backwards ps 
undirected models parameterized terms potentials cliques conversion necessary 
directed models parameterized terms cpds simply de ne potentials pa jpa 
possible kinds cpds see section details 
level abstraction orded potentials allows reuse code models 
instance simply replacing discrete potentials gaussian potentials code implement forwards backwards algorithm hmms rts smoother linear dynamical systems mur 
approximate inference cases exact inference mathematically possible computationally feasible cost inference depends treewidth graph size maximal clique corresponding optimally triangulated graph 
particular hidden nodes discrete inference takes time 
jointly gaussian distributions inference takes number nodes regardless image processing applications large 
trees graphs undirected cycles treewidth constant maximal fan number parents node graph inference takes time 
graphs especially repeating structure grids treewidth grows number variables grid treewidth exact inference infeasible 
reasons need approximate inference computing exact solution take long closed form analytic solution 
exact inference known np hard general 
kind complexity arises certain kinds graph structures certain kinds distributions 
general distributions continuous random variables yield intractable posteriors fully observed conjugate exponential case notable exception 
list techniques tackle kinds intractability 
sampling monte carlo methods 
simplest kind importance sampling cd draw random samples prior unconditional distribution hidden variables weight samples likelihood yjx evidence 
ecient approach high dimensions called monte carlo markov chain mcmc allows draw samples posterior jy compute normalizing constant yjx 
mcmc includes special cases gibbs sampling metropolis algorithm see nea grs mac 
mcmc dominant method approximate inference bayesian statistics community 
variational methods 
simplest example mean eld approximation exploits law large numbers approximate large sums random variables means 
particular essentially decouple nodes introduce new parameter called variational parameter node iteratively update parameters minimize cross entropy kl distance approximate true probability distributions 
updating variational parameters proxy inference 
mean eld approximation produces lower bound 
sophisticated methods possible give tighter lower upper bounds 
see tutorial 
technique extended approximate bayesian inference technique called variational bayes gb 
belief propagation bp 
entails applying message passing algorithm original graph loops undirected cycles 
originally believed unsound outstanding empirical success bgt shown bp algorithm mmc led lot theoretical analysis shown bp closely related variational methods 
technique extended approximate bayesian inference technique called expectation propagation min 
learning main kinds learning parameter learning called model tting structure learning called model selection 
discuss turn 
parameter learning adopt bayesian approach parameters treated just random variable learning inference 
adopt frequentist approach goal learning nd point estimate parameters maximum likelihood ml maximum posteriori map 
learning problem harder partial observability missing values hidden latent variables model 
case exact parameter posterior generally multimodal settle nding locally optimal solution 
start considering fully observed case compute global optimum 
fully observed case model dag cpds estimated independently problem said fully decompose 
cpds exponential family gaussians tables compute ml estimates closed form 
conjugate priors compute map estimate closed form 
complex cpds trees neural networks local hill climbing methods may necessary 
model undirected ml estimates clique potentials estimated closed form graph triangulated decomposable potentials tabular fully parameterized 
graph triangulated potentials tabular iterative proportional fitting ipf 
potentials represented terms features matter graph structure generalized iterative scaling gis algorithm dr requires features sum constant improved iterative scaling iis algorithm ppl assumptions features 
usually uses monte carlo sampling compute expectations needed scaling algorithms graph implied features small treewidth exact inference faster bj 
alternatively deterministic approximation schemes belief propagation tw 
partially observed case missing values latent variables em algorithm nd locally optimal ml map estimate lau 
step requires calling inference routine exact approximate compute expected sucient statistics step similar fully observed case 
step takes step right direction parameter space nding optimum called generalized em algorithm 
imagine doing partial step nh 
methods handling partial observability bound collapse rs gradient methods course possible 
advantages em compared gradient methods simplicity lack step size parameters fact deals constraints automatically 
combined gradient methods increased speed jj 
structure learning adopt bayesian approach structure learning means returning posterior distribution possible graphs computing expected value functions indicate presence certain features edges respect distribution 
adopt non bayesian approach structure learning means nding single best model 
best mean satis es conditional independencies observed data constraintbased approach maximizes scoring function penalized likelihood mdl bic marginal likelihood 
nd best scoring model principle search space models space discrete gradient methods 
bayes nets model may dag pdag partially directed acyclic graph represents class markov equivalent dags 
observational data possible distinguish members equivalence class 
simple modify scoring function exploit interventional data simply don update parameters nodes set intervention cy 
enables learn causal models data useful domains bioinformatics 
alternative searching graph edge space search feature space gibbs exponential distribution equation de ne model ppl bj 
features provide ner granularity edges lead models perform better terms density estimation predictive power 
interpreted causally 
structure learning large subject see hgc bun mur details 
software issues aside various theoretical issues discussed various practical issues big di erence success software package 
mention 
package free 
lot packages free academic 
packages free versions restricted various ways limit model size 
source code available 
language written 
system easy extend 
instance add new kinds cpds inference algorithms 
api available 
api application program interface package integrated applications standalone program 
gui available 
systems run 
java run computer guis microsoft windows 
comparison existing software packages review existing software packages aware september respect issues mentioned 
mentioned table due lack space nearly packages exact inference junction tree variable elimination pearl algorithm 
limited discrete random variables 
restricts applicability 
packages bugs hydra sampling exible 
unfortunately sampling programs fail exploit tractable substructure rao blackwellization usually slow really big problems domains speech vision text 
see section strengths bnt wide range inference algorithms exact approximate available user 
enables package di erent kinds model 
furthermore inference algorithms plugged learning algorithms modular way 
design bnt discuss design bnt respect desiderata 
representation bnt originally designed bayes nets dag models name 
extended handle uence diagrams 
bayes net represented structure contains graph represented adjacency matrix cpds represented list objects pieces information 
dbns store intra slice topology inter slice topology 
bnt supports variety cpd types class see 
support bayesian models parameters hidden inside cpd object 
class required implement set methods depending inference learning algorithms 
explained 
inference biggest strengths bnt ers variety inference algorithms di erent tradeo accuracy generality simplicity speed algorithms brute force enumeration joint designed pedagogical debugging purposes 
cases bnt ers multiple implementations algorithm matlab version name src api cts gui ug free analytica java discoverer course java bn power constr 
ci bn toolbox matlab bn toolkit bugs business nav 
coco xlisp lisp ci java ergo java genie smile wu hydra java hugin expert ci ideal lisp java bayes java mim wum lisp java tetrad ci web weaver java comparison software packages graphical models 
columns meanings 
src source code available 
api application program interface available program standalone blackbox 
source included api needed windows unix mac cts package support continuous random variables means discretizes 
gui package gui 
package learn parameters 
package learn structure 
uses search score ci uses conditional independence tests package support sampling 
package support utility action nodes decision diagrams 
ug package support undirected graphs 
free package freely available 
restricted means product free non commercial free version limited functionality 
see www cs berkeley edu bayes html online version table complete urls additional information 
tabular softmax discrete generic gaussian root mlp deterministic boolean class hierarchy cpds supported bnt 
root node parameters represents observed exogenous input de ne conditional density models 
mlp stands multilayer perceptron feedforward neural network 
currently classi cation discrete nodes easily extended regression continuous nodes 
softmax essentially special case mlp hidden layers 
netlab implement learning mlps softmax 
strictly speaking deterministic boolean cpds implemented subtypes tabular cpd class constructors create tabular cpd objects 
merely provide syntactic sugar tabular cpd class 
version 
engines api easily interchanged 
instance step em 
algorithm represented inference engine object 
inference engines di er ways including works topologies restrictions 
works cpd types restrictions 
exact approximate inference 
terms topology engines handle kind dag 
exceptions follows fg belief propagation factor graphs fg pearl works polytrees works bn models qmr 
terms cpd types sampling algorithms essentially handle kind node distribution algorithms sum products variable elimination junction tree belief propagation convert cpd potential imposes restrictions see section 
algorithms designed give exact answer 
exceptions belief propagation exact applied trees cases sampling exact limit nite number samples 
situation summarized 
potentials implement inference algorithm works terms sums products variable elimination message passing necessary represent cpd jpa potential discussed section 
bnt supports kinds potentials class discrete gaussian represented engine exact 
cpd types topology approx cg dag fg approx cg fg cond gauss exact cg dag enumerative exact dag gaussian exact dag global joint exact cg dag jtree exact cg dag jtree exact dag likelihood weighting approx dag loopy pearl approx dag pearl exact polytree exact noisy bn var elim exact cg dag inference engines static models supported bnt 
means hidden nodes discrete means hidden nodes linear gaussian cpds cg means hidden nodes conditional linear gaussian cpds 
details algorithms please consult online documentation 
moment form canonical form conditional gaussian utility 
pa discrete enumerate values represent pa conditional probability table 
bnt function cpd cpt 
destroy special structure cpd 
instance jpa noisy distribution requiring jpa parameters resulting cpt entries 
ect sample complexity number training cases needed learn ect computational complexity amount time needed inference 
way avoid explicitly graphically represent conditional independence assumptions noisy distribution rd 
jpa conditional linear gaussian distribution represent pa cg potential 
general mixed discrete continuous distributions represented potentials 
example discrete pa continuous cpd logistic softmax represented cg potential 
pa observed value evaluate function represent result table 
bnt function cpd table 
suggests convert cpds potentials seen evidence idea rst proposed mur 
approach adopted bnt lets apply exact algorithms greater range models mixtures experts input output hmms software packages 
unfortunately technique stable cg algorithm lj 
advantage bnt approach handling evidence follows 
discrete variable observed take possible value observed value 
representing potential sparse table zeroes simply allocate observed variable dimension size implications optimal triangulation bnt lets specify nodes observed evidence arrives 
resulting table smaller faster manipulate 
simpler approaches zero compression ja hd 
belief propagation belief propagation bp implemented converting cpds potentials standard sum product operations 
simple slow 
reasons slow practical theoretical 
practical reason overhead manipulating objects matlab quite high real problem bp iterative algorithm 
theoretical reason structure cpds lost mentioned 
cpds notably noisy implement special purpose methods computing messages needed bp methods called compute lambda msg compute pi 
currently exploited loopy pearl inf engine 
sampling implement likelihood weighting importance sampling need way sampling jpa means parents observed 
generic way convert partially observed cpd potential see section sample 
individual cpd classes may implement ecient methods 
implement gibbs sampling need way sampling markov blanket node jm jpa children jpa implemented partly slow matlab partly bugs system job 
plan add interface bnt bugs 
learning bnt supports parameter structure learning currently dag models 
parameter learning parameter estimation routines bnt classi ed types depending goal compute full bayesian posterior parameters just point estimate maximum likelihood maximum posteriori variables fully observed missing data hidden variables partial observability 
routines listed 
full partial point learn params learn params em bayes bayes update params supported currently tabular cpd class supports bayesian updating useful things sequential line learning dirichlet priors 
map estimate computed tabular cpds 
priors especially important multinomials avoid problems sparse data 
structure learning structure learning routines bnt classi ed types analogously parameter learning case 
algorithms implemented indicated parentheses 
notation explained 
full partial point ic pc mi hill climb structural em hill climb bayes mcmc mcmc mean approximation bayesian scoring function mentioned ch 
structural em algorithm described fri fri 
algorithm ch assumes total ordering nodes 
principle search ordering fk ecient searching larger space graphs 
ic pc algorithm sgs constraint algorithm need search starts fully connected graph removes edges determined conditional independence tests applied data directions arrows added arcs markov equivalence 
disadvantage independence tests need specify threshold avoided bayesian criterion mt 
alternatively mutual information mi criterion cbl mt 
options supported bnt 
software issues summarize relevant attributes bnt 
free 
gnu gpl license 
source code available 
matlab 
easy extend 

adding new cpds inference engines simple creating new class 
api available 

matlab called variety languages 
gui available 
coming soon see section 
past bnt past bnt started summer intern compaq dec cambridge research labs crl 
working jim rehg wanted apply bayes nets computer vision problems 
time software packages available features needed learning ability handle continuous random variables dynamical systems 
crl system written limited 
fact handle learning continuous random variables dynamical systems 
returned berkeley decided rewrite system matlab 
matlab 
chose matlab ease handle gaussian random variables kalman lter implemented lines 
proved decision matlab perceived merely package numerical linear algebra data visualization release version mid matlab fully edged programming language exible data structures cell arrays objects 
prevents bnt running octave open source version matlab lacks features 
identify various pros cons matlab 
positive side matlab scripting language suitable rapid prototyping 
addition provides debugger pro ler 
matlab excellent numerical algorithms svd 
matlab excellent data visualization plotting routines 
matlab excellent neural nets optimization 
instance bnt uses netlab learn mlp softmax cpds 
matlab code high level easy read 
matlab widely education engineering departments 
negative side matlab slow untyped interpreted language compiler ective 
matlab object system advanced say java 
commercial matlab license costs thousands dollars student edition 
matlab pointers 
particular supports pass value pass constant passing large arguments functions result lot copying function updates single pixel image image copied function 
comparison matlab interactive scienti languages mathematica splus gauss available online 
bnt proved popular web site average hits week widely world teaching research elds ranging statistics biology economics mechanical engineering 
rockwell developed commercial product 
www ncrg aston ac uk netlab www cs berkeley edu bayes compare html see www com prod htm 
bnt intel summer intern intel decided bnt new research project graphical models led gary 
short term goal graphical models applications data mining audio visual speech recognition low level vision 
medium term goal create open source graphical models library optimized intel architectures stimulate commercial interest application graphical models 
long term goal identify microprocessor architecture design changed enable large scale applications graphical models 
terms data mining intel team people china add variety structure learning algorithms see section bnt 
intel team people usa developing data visualization tools 
goal apply tools data intel factories 
terms open source library fall intel assemble team people russia may adding extensions bnt fast implementations loopy belief propagation mrfs image processing applications 
bnt currently lines code comments 
ultimate goal open source library entirely written 
callable matlab scripting languages 
intel similar experience opencv intel open source computer vision library developed russia 
bnt january richard founded discuss creation open source bayes net library 
time group lively discussion concrete come 
richard consider bnt truly open source relies matlab free 
bnt code freely available currently gnu library general public license various people contributed mention contributions 
integrating bnt allow glm cpds 
cemgil contributed simple graph visualization routines 
wei hu intel china implemented version junction tree algorithm 
shan huang intel china adding stable cg inference 
implemented ic structure learning algorithm 
ilya implemented graph triangulation yair weiss contributed routines relating loopy belief propagation 
ron zohar added code compute probable explanation 
addition numerous people helped nd bugs philippe translated documentation french 
currently receive code contributions email integrate hand clearly scale handle people simultaneously working www project org www intel com research mrl research opencv see groups yahoo com group www cs berkeley edu index html 
www sci edu au staff dunn html project 
plan move bnt sourceforge open source development environment 
fairly long wish list features see added bnt divided broad categories new functionality speedups issues 
major additional functionality see includes add tree structured cpds 
add support bayesian modeling parameters explicitely represented nodes 
add support online inference learning 
current dbn support designed ine batch processing 
implement approximate inference algorithms expectation propagation min variational bayes gb 
add support non dag graphical models dependency nets mrfs feature maxent models 
model type class methods inference learning discussed section 
resulting program called gmt graphical models toolbox emphasize generality fact completely backwards compatible bnt 
terms see interface bugs hydra implement gibbs sampling metropolis hastings respectively 
le parser handle proposed xml le format 
ken shan implemented web program translate bif le sequence calls bnt api 
program lets users create graphs interactively 
hao wang implemented prototype matlab xu currently extending people intel china 
program automatically lays graphs way look pretty minimizing line crossings 
essential interpreting results structure learning 
implemented graphviz open source graph visualization package 
bob davies intel california developed prototype tom sawyer commercial version graphviz 
prototype lets user interactively edit results layout process essentially wang matlab program mentioned 
program lets users visualize edit cpds 
particularly useful tree structured cpds 
www mrc cam ac uk bugs www washington edu hydra see www cs berkeley edu file formats html list various le formats commonly proposed 
www harvard edu ken bif bnt www research att com sw tools graphviz www com gui ties graph cpd editor visualizer matlab backend inference learning 
terms speed current major bottleneck related creation manipulation potentials see sections 
functionality needed sum product type inference algorithm belief propagation junction tree 
fairly simple improve current implementation matlab ultimately lot code need translated 
looking automatic code generation technology nasa project fs 
goal code fast apply largescale applications real time computer vision 
michael jordan comments earlier draft stuart russell funding developed bnt gary ering internship intel serge belongie introducing matlab numerous people contributed code bug xes see section 
am aji mceliece 
generalized distributive law 
ieee trans 
info 
theory march 
boutilier friedman goldszmidt koller 
context speci independence bayesian networks 
uai 
bgt berrou glavieux 
near shannon limit error correcting coding decoding turbo codes 
proc 
ieee intl 
comm 
conf 
bj bach jordan 
thin junction trees 
nips 
binder koller russell kanazawa 
adaptive probabilistic networks hidden variables 
machine learning 
bun buntine 
operations learning graphical models 
ai research pages 
bun buntine 
guide literature learning probabilistic networks data 
ieee trans 
knowledge data engineering 
cbl cheng bell liu 
learning belief networks data information theory approach 
proc 
th acm intl 
conf 
info 
knowl 
mgmt 
cd cheng druzdzel 
ais bn adaptive importance sampling algorithm evidential reasoning large bayesian networks 
ai research 
cowell dawid lauritzen spiegelhalter 
probabilistic networks expert systems 
springer 
ch cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
ch chickering heckerman 
ecient approximations marginal likelihood incomplete data bayesian network 
machine learning 
cy cooper yoo 
causal discovery mixture experimental observational data 
uai 
dec dechter 
bucket elimination unifying framework probabilistic inference 
uai 
die 
parameter adjustment bayes networks 
generalized noisy gate 
uai 
dr darroch ratcli generalized iterative scaling log linear models 
annals math 
statistics 
edwards 
graphical modelling 
springer 
nd edition 
fk friedman koller 
bayesian network structure 
uai 
fri friedman 
learning bayesian networks presence missing values hidden variables 
uai 
fri friedman 
bayesian structural em algorithm 
uai 
fs fischer schumann 
generating data analysis programs statistical models 
functional programming 
submitted 
gb ghahramani beal 
propagation algorithms variational bayesian learning 
nips 
grs gilks richardson spiegelhalter 
markov chain monte carlo practice 
chapman hall 
hcm heckerman chickering meek kadie 
dependency networks density estimation collaborative ltering data visualization 
technical report msr tr microsoft research 
hd huang darwiche 
inference belief networks procedural guide 
intl 
approx 
reasoning 
hen henrion 
practical issues constructing belief networks 
uai 
hgc heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
technical report msr tr microsoft research 
ja jensen andersen 
approximations bayesian belief universes knowledge systems 
uai 
jen jensen 
bayesian networks decision graphs 
springer verlag 
jordan ghahramani jaakkola saul 
variational methods graphical models 
jordan editor learning graphical models 
mit press 
jj 
conjugate gradient acceleration em algorithm 
jasa 
jlo jensen lauritzen olesen 
bayesian updating causal probabilistic networks local computations 
computational statistics quarterly 
jor jordan editor 
learning graphical models 
mit press 
kschischang frey 
loeliger 
factor graphs sum product algorithm 
ieee trans info 
theory february 
triangulation graphs algorithms giving small total state space 
technical report dept math 
comp 
sci aalborg univ denmark 
lau lauritzen 
propagation probabilities means variances mixed graphical association models 
jasa december 
lau lauritzen 
em algorithm graphical association models missing data 
computational statistics data analysis 
ld li ambrosio 
ecient inference bayes networks combinatorial optimization problem 
intl 
approximate reasoning 
lj lauritzen jensen 
stable local computation conditional gaussian distributions 
technical report dept math 
sciences aalborg univ 
mac mackay 
monte carlo methods 
jordan editor learning graphical models 
mit press 
mh meek heckerman 
structure parameter learning causal independence causal interaction models 
uai pages 
min minka 
expectation propagation approximate bayesian inference 
uai 
mmc mceliece mackay cheng 
turbo decoding instance pearl belief propagation algorithm 
ieee areas comm 
mt thrun 
bayesian network induction local neighborhoods 
nips 
mt thrun 
bayesian multiresolution independence test continuous variables 
uai 
mur murphy 
filtering smoothing linear dynamical systems junction tree algorithm 
technical report berkeley dept comp 
sci 
mur murphy 
inference learning hybrid bayesian networks 
technical report berkeley dept comp 
sci 
mur murphy 
variational approximation bayesian networks discrete continuous latent variables 
uai 
mur murphy 
learning bayes net structure sparse data sets 
technical report comp 
sci 
div uc berkeley 
nea neal 
connectionist learning belief networks 
arti cial intelligence 
nea neal 
probabilistic inference markov chain monte carlo methods 
technical report univ toronto 
nh neal hinton 
new view em algorithm justi es incremental variants 
jordan editor learning graphical models 
mit press 
pea pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
pea pearl 
causality models reasoning inference 
cambridge univ press 
ppl della pietra della pietra la erty 
inducing features random elds 
ieee trans 
pattern analysis machine intelligence 
ps peot shachter 
fusion propogation multiple observations belief networks 
arti cial intelligence 
rd rish dechter 
impact causal independence 
technical report dept information computer science uci 
rg roweis ghahramani 
unifying review linear gaussian models 
neural computation 
rs ramoni sebastiani 
learning bayesian networks incomplete databases 
uai 
sgs spirtes glymour scheines 
causation prediction search 
mit press 
nd edition 
sk shachter 
gaussian uence diagrams 
managment science 
saad opper 
advanced mean field methods 
mit press 
sri srinivas 
generalization noisy model 
uai 
tw 
teh welling 
uni ed propagation scaling algorithm 
nips 
yedidia freeman weiss 
understanding belief propagation generalizations 
ijcai 

