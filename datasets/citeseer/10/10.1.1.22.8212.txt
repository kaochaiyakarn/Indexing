modeling embodied lexical development david bailey jerome feldman srini narayanan george lakoff icsi berkeley edu international computer science institute university california berkeley center street suite berkeley ca usa presents implemented computational model lexical development case action verbs 
simulated agent trained informant giving labels agent actions hand motions system learns label carry similar actions 
computationally system employs novel form active representation explicitly intended neurally plausible 
learning methodology version bayesian model merging omohundro 
verb learning model placed broader context project embodied natural language acquisition 
central questions cognitive science concepts language embodied 
involves seeking answers kinds questions brain neural system represent concepts 
learn concepts 
organize concepts lexically 
learn lexical items 
way making progress ultimate questions ask structured connectionist systems physical neural systems brain hope connectionist results give computational insight brain captures concepts language 
group icsi uc berkeley pursuing research strategy decade feldman 
overview results challenges 
perspective modeling lexical development ideal task studying embodied cognition 
isolate linguistically conceptually simple situations construct test detailed models 
major effort dissertation regier simplified realistic connectionist model visual system plus conventional back propagation learning scheme demonstration lexical items describing spatial relations develop different languages 
languages differ radically spatial relations conceptualized obvious set primitive features built program 
key regier success came directly embodiment people visual system visual concepts arise capabilities 
building simple realistic visual system model regier able program learn spatial terms labeled example movies wide range languages simple back propagation techniques 
lexical development active field methods results 
critical empirical finding child words label things relationships actions internal states tomasello 
addressing broader lexical acquisition problem appreciate role embodiment 
focus learning verbs describing simple hand motions push 
bailey dissertation includes computational model learn produce verb labels actions carry actions specified verbs learned 
entails methodological problem model child learning label actions program able act 
bailey solves incorporating jack badler human simulation system part model 
shortcoming standard view lexical acquisition provides account child learns concepts learns words label 
weakness appears technical consequence back propagation regier pdp models network learns perfectly classify domain mechanism inference action 
new requirement learning algorithms produce usable representations 
outlines entails bailey verb learning system 
conventional concept learning task depicted upward arrows 
training agent executes motor action bottom tries relate features action world state middle layer characterization word meanings top layer produce appropriate label 
impose additional constraint process learned verb meaning function command interface agent depicted downward arrows 
example learns shove labels pushing actions high acceleration able carry action asked 
done interpreting verbal command choosing motor action parameters accordance project reminiscent winograd differs focus learning restriction verbs attention finer semantic distinctions 
siskind considered action verb learning leverage internal state intentions motor commands relying visual features 
linking feature structure verbs labelling interpretation feature extraction execution guidance senses motor actions schemas architecture bailey verb learning system 
world state guide execution 
levels representation basic questions neural cognitive development receiving increasing attention connectionist perspective 
universally assumed genetic environmental factors interact elaborate ways acquisition lexical terms 
deal theoretical modeling acquisition syntax including past tense controversy appear detailed theory lexical development 
hope progress supplying scientific notation expressing mechanisms lexical acquisition 
formalism needs bridge gap embodied experience expression symbols language 
inherently involves multiple levels description 
regier combined levels scientific discourse lowest neural implicit 
complicated domains discussed added computational level connectionist level 
analogous marr computational level comprises mixture familiar notions feature structures novel representation executing schemas 
levels discourse cognitive words concepts computational structs schemas see connectionist structured models learning rules neural implicit intermediate representation levels provide needed scientific language specifying proposed structures mechanisms 
require intermediate representations implementable computational simulations allow test consequences hypotheses 
third requirement computational mechanisms reducible structured connectionist models embodiment realized 
fourth requirement representational formalisms support computational learning algorithms perform experiments acquisition 
executing schemas novel aspect current effort extensive executing schemas schemas short represent actions named distinguish notions schema remind intended really execute invoked 
motivation schemas comes motor control 
simple behaviors grasping involve coordinated movement range muscles stereotyped parameterized manner called synergy bernstein 
higher level motor control involves coordination synergies accompanying perception control 
concept motor schema pervades literature motor control flow chart depiction activity patterns arbib 
currently represent schemas formalism known petri nets computer science murata 
important features clean ways capture concurrency event asynchronous control addition standard ideas sequence hierarchy parameterization 
petri net bipartite graph containing places drawn circles transitions rectangles places hold tokens represent predicates world state internal state 
transitions active component 
places pointing transition contain adequate number tokens usually transition enabled may fire removing input tokens depositing new set tokens output places 
generally side effect firing transition triggers external action motor synergy optional 
simple constructs wide variety control structures built 
bottom third depicts example sliding object tabletop 
slide schema captures fact people shape hand moving arm object large small objects handled differently 
includes loop continues motion goal separate little schema tightening grip slip detected 
feature structures keep things minimal models computational mechanism feature structures structs short drawn row double boxes 
structs static knowledge representation parameter setting binding 
chosen compatible called structures literature unification grammars similar known ai slot filler mechanisms 
version linking features relevant schema world state features push aspect schema posture low indx aspect palm schema posture cube object object button posture schema extend palm high shove extend elbow slide slide schema posture direction aspect grasp palm indx flex extend dn lf rt med hi goal motor parameter features world state features cube button object elbow slide start slide schema small large goal done goal grip palm grip slipping tighten apply move arm force dur dir move arm grasp elbow lbs weight false accel accel accel elaboration computational level showing details slide schema linking features verbs push senses shove sense 
simpler structures nested complex values probabilistic 
verb learning model special struct called linking struct center plays important role sole interface language action 
maintains bidirectional connections schemas schema receives bindings structs produces additional bindings execution 
way actions may translated semantic features 
generally want claim requirements parameterizing schemas principal determiner semantic features get encoded language 
features chosen intention allowing model learn relevant verbs language 
critical linking feature name schema generating action 
include motor parameters force elbow joint motion hand posture 
world state features relevant object shape 
verb representation sense verb represented model struct values feature probability distributions 
features presumed independent representation conjunctive gestalt nature 
scheme necessary allow multiple senses verb capturing meaning single sense cases require overly broad distributions obscure details needed carry corresponding action 
word sense representation compatible rosch prototype theory categorization stands contrast necessary sufficient conditions structure logical formalisms 
central case yields highest response gives graded response feature values differ central case 
top third shows word senses verbs push shove 
upper left ellipse gives structs senses push 
top sense hand motion invokes slide schema 
codes somewhat open palm hand posture codes strongly elbow joint extension 
bottom sense corresponds depressing button finger invokes different schema illustrated 
ellipse upper right shows shove codes slide schema specifies high acceleration 
execution mode verbal command interpreted choosing sense best matches current world state 
sense turn set linking struct determining schema execute parameters 
example shove specifies slide schema high acceleration actual amount force required depends weight object involved specified utterance 
model additional struct world state struct encodes things weights positions objects 
learning recall central question lexical development children learn label experience 
verb learning task assume child acquired various schemas actions hand manipulating object table 
assume agent hears informant labeling actions agent performing 
regier avoid hard feel separable problems assuming informant supplies just verb 
learning mechanism assumes informant usually provide partially appropriate labels 
problem faced model child learn verbs relate actions goals 
principles literature incorporated learning model 
principle children tendency learn action verbs corresponding actions extending meanings actions huttenlocher focus motor schemas 
principle learning occurs explicit negative evidence 
third principle fast mapping carey notes children learn word sensibly little observation 
solution relies intimate relation passive linking struct 
schema executions translated structs vice versa action labeling task reduced manageable proportions 
system needs correlate informant labeling sets feature bindings 
involves determining correct number senses verb relevant features sense probability distributions included feature 
ways doing computational learning literature 
way won suffice standard back propagation regier pdp modelers require learning technique produce invertible solutions 
current experiments version bayesian model merging omohundro 
applied verb learning task model merging proceeds assuming execution example separate word sense model merging senses models provides better description training set 
better formalized bayesian framework aim maximize posterior probability lexicon collection word senses training set 
accomplished applying bayes law yield maximizing product 
term prior assigns higher probability preferred usually compact lexicons case prior exponentially decreasing function total number word senses 
second term likelihood assigns higher probability lexicons generate training data 
stated model merging algorithm collects training data performing series merges 
easy convert algorithm online develop refine lexicon training examples arrive 
case algorithm performs merge phase time new examples accumulated 
set model merging greedy algorithm performance improved setting 
simplified version algorithm handling new training example follows incorporate example struct verb lexicon create new sense add senses created loop best candidate merge similar pair senses old post compute posterior replace merge sums counts feature values new post compute posterior new post old post terminate loop endloop note merging step inevitably leads compact lexicon lowers likelihood training set 
combined effect detrimental 
algorithm key advantages 
prior function explicit tunable mechanism striking balance generating specific senses word generating excessively general senses 
second online version produces sensible results emerge just training example back propagation style algorithms 
gives graphical overview idealized learning run system 
shown left column linking structs summarizing successive example executions labeled push teacher 
training example system assumes word push labels just example involving slide elbow joint undergoing extension posture acceleration level 
second word label similar action results system broadening range accelerations believes denoted push 
third example quite different arise pushing doorbell keypad 
mdl evaluation function model merging algorithm prefers second word sense widening slots existing model 
learned approximately different senses push shown upper left ellipse 
final example closely matches sense merges 
example different posture posture slot broadened allow possibilities probabilities approximating frequencies observed 
acceleration example low algorithm concludes acceleration isn criterial verb 
course just cartoon version complex system operation convey flavor accel palm palm linking struct training examples merge initial sense schema schema schema word senses push schema merge schema schema ex ex ex ex schema new sense elbow schema elbow elbow elbow extend extend extend fixed posture posture posture posture extend posture elbow fixed posture palm elbow extend palm posture elbow palm posture elbow extend slide slide slide slide slide slide accel accel accel accel accel accel index grasp grasp illustration model merging learning verb push 
mechanisms 
full training results reported space briefly review training run simplified version model 
random executions slide generated labeled push pull slide sideways motions 
half chosen training set 
merging algorithm collapsed instances push single sense likewise instances pull 
instances slide collapsed senses gaussians direction feature probability distribution prevented merging leftward rightward motion instances direction feature 
resulting lexicon correctly recognized similarly generated test cases 
errors involved slides pushes direction prototypical values verbs push chosen due greater frequency training set 
verbs executed correctly randomly generated initial world configurations involving varied hand postures hand positions object positions 
training set schemas slide push converged senses slides extending elbow depressing index finger 
pull single sense encoding slides elbow medium force 
similar encoded high force additionally examples involved grasp posture included word sense 
model algorithms involve number tunable parameters 
sensitive parameter deciding word sense probabilistic feature value peaked justify feature schema execution 
connectionist implementation described model computational level stated earlier committed connectionist reducibility important parts model schemas model merging algorithm 
petri nets involve local control schemas straightforwardly modeled connectionist units represent places transitions weren complication passing parameters 
solution problem developed temporal synchrony approach binding 
focal clusters reasoning system represent assertions queries predicates plus arguments extended trigger coordinate primitive synergies plus parameters 
connectionist account model merging algorithm sketched framework recruitment learning feldman 
binder nodes represent conjunction lexical item motor parameters world state features word sense 
compete explain new training examples close match winning binding node slightly modified account features training example effectively merging new example existing sense 
clear winner new binder unit recruited represent new training example effectively creating new sense 
account applies online version model merging 
discussion bailey model lexical development action verbs offers advantages 
bidirectional mapping word senses schemas allows model recognize carry verbs learned 
bidirectionality accomplished intermediary linking struct confers benefits hardwiring relevant features opposed allowing hidden units evolve novel features facilitates translation verbs back set features 
restricted parameterized formalism schemas facilitates translating linking features executing schema bridging declarative procedural gap 
conjunctive probabilistic feature structures represent word senses compatible notion gestalt perception prototype theory 
model offers account pragmatics verb representation simplified fact context dependencies handled schema execution 
learning algorithm operates negative evidence exhibits fast mapping 
provides explanation children ability learn patterns verbs tendency encode manner english vs path korean feature discussed 
model shortcomings 
offers account radial category structure connecting word senses lakoff 
explain schemas interact image schemas regier model phrases push 
account schema learning relates lexical learning desirable 
concepts words course program directly embodied meaning applies limited range concrete concepts lexical items 
cognitive linguists general lakoff particular believe concepts ultimately derive meaning mappings directly embodied ones studied mappings years 
part current effort apply computational techniques described model mappings occur 
key computational mechanisms schemas binding structs core concept story 
narayanan describes implemented program demonstrates simulation schema structure controller abstraction multiple motor actions able offer answers known problems modeling semantics verbal aspect 
project involves model interprets certain concepts inter alia metaphorical mappings concrete source domains 
imagine model reading news stories international economics politics trying understand 
test understanding examining bindings various structs processing story 
currently validating model modeling examples database simple sentence newspaper stories 
critical test system ability understand stories appear validation complete 
summarized narayanan described fully dissertation 
arbib lyons 

schemas integrate vision touch hand control 
arbib hanson editor vision brain cooperative computation 
mit press cambridge ma 
badler phillips webber 

simulating humans 
oxford university press new york 
bernstein 

ordination regulation movement 
pergamon press new york 
carey 

child word learner 
halle bresnan miller editor linguistic theory psychological reality 
mit press cambridge ma 
feldman 

dynamic connections neural networks 
biological cybernetics 
feldman lakoff bailey narayanan regier stolcke 

years automated language acquisition project 
ai review 
special issue integration natural language vision processing 
shastri narayanan feldman 

connectionist encoding schemas reactive plans 
poster th cognitive science society conference 
huttenlocher smiley 

emergence action categories child evidence verb meanings 
psychological review 
lakoff 

women fire dangerous things categories reveal mind 
university chicago press 
murata 

petri nets properties analysis applications 
proceedings ieee 
narayanan 

embodied language understanding modeling semantics causal narratives 
aaai fall symposium embodied cognition action pages 
aaai press tr fs 
narayanan 

talking talk walking walk computational model verbal aspect 
proceedings th cognitive science society conference 
omohundro 

best model merging dynamic learning recognition 
technical report tr international computer science institute berkeley ca 
regier 

human semantic potential 
mit press 
rosch 

human categorization 
warren editors advances cross cultural psychology volume 
academic press new york 
siskind 

naive physics event perception lexical semantics language acquisition 
phd thesis massachusetts institute technology 
tomasello editors 
names things young children acquisition verbs 
lawrence erlbaum associates hillsdale nj 
winograd 

procedural model language understanding 
computer models thought language 
freeman new york 
