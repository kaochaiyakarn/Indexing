infinite raam principled connectionist basis grammatical competence presents infinite raam iraam new fusion recurrent neural networks fractal geometry allowing understand behavior networks dynamical systems 
shown capable generating context free non regular language arbitrary values expands showing capable generating syntactically ambiguous languages capable generating certain context free constructions absent natural languages 
demonstrations support belief provide adequate connectionist model grammatical competence natural language 
natural language issues early extremely influential noam chomsky showed natural languages nl modeled finite state automaton existence center embedded constructions 
second equally important observation minimally adequate nl grammar ambiguous assigning structure interpretation sentences example flying planes 
observation led development chomsky formal hierarchy languages computational resources machines needed recognize 
hierarchy chomsky observation center embedding expressed saying nl non regular generated grammar having rules form simon levy ofer melnik jordan pollack levy melnik pollack cs brandeis edu dynamical evolutionary machine organization center complex systems brandeis university waltham ma usa february non terminal symbols terminal symbol 
nl merely non regular belonging context free cf level chomsky hierarchy powerful belonging hierarchy subject heated debate postal langendoen shieber 
non cf phenomena copying crossed serial dependencies bresnan kaplan peters zaenen suggested powerful approach syntactic transformations chomsky called researchers criticized transformations having arbitrary power failing constrain types languages expressed gazdar 
criticism entire formal approach came observing cf grammars cfgs power generate structures sequence followed mirror image occur nl placed extraordinary burden human parsing mechanism occur bach brown marslen wilson 
connectionism natural language debates complexity nl connectionism awaken fifteen year sleep 
connectionist models researchers way embodying flexibility graceful degradation non rigid properties characterize real cognitive systems nl 
research culminated publication highly controversial rumelhart mcclelland provided connectionist account part grammar english feed forward neural network 
soon criticized traditional cognitive scientists fodor pylyshyn pinker prince cited non generative nature connectionist models fundamental shortcoming entire field 
partly response criticisms connectionists spent past decade investigating network models support generativity recurrent feedback connections lawrence giles fong rodriguez wiles elman williams zipser 
research attempt contribute effort focusing strongly possible natural language issues described 
attempt faces number challenges 
despite analysis network dynamics contribute generativity uncertain dynamics support generation formed strings certain length 
unknown network true competence language learned exemplars merely capable generating finite regular subset language 
second easier model weak strong generative capacity building networks generate recognize strings having certain properties assigning syntactic structure strings 
third lack syntactic structure inhibits formulation account syntactic ambiguity networks making plausible models nl 
fair connectionists cognitive scientists take seriously notion human language infinite generative capacity 
obviously resources argue issue certain model provably infinite competence persuasive cognitive science community model 
sum concerned formulating recurrent network model rigorously addresses set criteria emerged long debate complexity nl 
candidate remainder presents new formulation raam pollack recurrent network model addresses nl issues principled way :10.1.1.30.6437
traditional raam recursive auto associative memory raam pollack method storing tree structures fixed width vectors repeated compression :10.1.1.30.6437
architecture consists separate networks encoder network construct fixed dimensional code combining nodes symbolic tree bottom decoder network decompresses fixed width code components 
decoder applied recursively terminates symbols reconstructing tree 
networks simultaneously trained time varying inputs 
training successful result bottom encoding coincide top decoding 
publication pollack raam gained widespread popularity model nl syntax :10.1.1.30.6437:10.1.1.30.6437
researchers blank marshall attractive way closing gap symbolic sub symbolic paradigms cognitive science 
van gelder saw raam direct simple refutation traditional cognitive scientists connectionism went far show traditional syntactic operations transformations performed directly raam representations chalmers 
power raam model apparent variants began emerge 
included sequential raams kalman showed raam behave linked list labeling raams sperduti encoded labeled graphs containing cycles 
short raam hold great deal promise general connectionist solution encoding just nl syntax sorts structured representations 
raam plagued apparently diverse set problems notably failure scale realistically large structures 
believe problems traced original formulation raam decoder works conjunction logical terminal test answering representation requires decoding 
default terminal test merely asks elements code boolean 
analog binary conversion standard interface back propagation research late calculate binary functions real valued neurons 
enabled initial discovery raam training led basic logical problems prevented scaling raam infinite loop problem representations break decoder terminating 
words trees appear infinitely large simply components pass terminal test 
behavior breaks computer program implementations requires depth checking 
precision vs capacity problem tighter tolerances lead decoding er greater set reliable representations 
terminating non terminal problem arises fusion non terminal terminal decoding encoded tree terminates abruptly 
section new formulation raam networks analysis iterated dynamics decoding resolves problems completely 
formulation leads new natural terminal test natural labeling terminals inherently higher storage capacity 
new raam formulation bias example raam decoder neuron network parameterized weights 
application decoder converts coordinate new coordinates 
consider raam decoder shown 
consists neurons receive input 
output portion network divided right left pair neurons 
operation decoder output pair neurons recursively reapplied network 
raam interpretation recursion implies branching node binary tree represented decoder initial starting point 
network recurrence evaluated context dynamical systems 
network form iterated function system ifs barnsley consisting transforms iteratively applied points dimensional space 
past examined applicability ifs analogy interpretations neural dynamics blair pollack kolen melnik pollack pollack 
context raams main interesting property contractive lies trajectories points space 
contractive space divided sets points 
set consists points located underlying attractor fractal attractor ifs 
second set complement points attractor 
trajectories points second set characterized gravitation attractor 
finite multiple iterations transforms effect bringing points second set arbitrarily close attractor 
noted infinite loop terminating nonterminal problems arise insufficient terminal test 
trajectories leave attractor eventually hit attractor 
terminal test guarantees termination trajectories raam ifs test includes points attractor 
terminal test decoder network attractor problems infinite loops early termination corrected possible extremely large sets trees represented small neural codes 
attractor fractal generated arbitrary resolution 
interpretation possible tree described single point equivalence class initial points sharing tree shaped trajectories fractal attractor 
formulation set trees generated represented specific raam function weights governed initial condition space sampled resolution attractor construction 
note lower resolution attractors contain points higher dimensional counterparts cover coarser terminal set terminate trajectories earlier act prefix trees higher dimensional attractors 
pieces complete new formulation 
encoder network trained constructed directly mathematical inverse decoder 
terminal set leaf tree run inverse left right transforms resultant sets intersected terminals subtracted 
process continued bottom empty set find set initial conditions encode desired tree 
second attractor terminal test allows natural formulation assigning labels terminals 
barnsley noted point attractor associated address simply sequence indices transforms arrive point points attractor 
address essentially infinite sequence digits 
achieve labeling specific alphabet need consider sufficient number significant digits address 
example new raam formulation section describe obtain attractor trees raam decoder sort shown 
decoder weights example obtained hill climbing search aesthetically appealing attractor demonstration valid set decoder weights 
recall treating decoder ifs maps input point range points range 
generate attractor ifs apply mappings transforms entire unit square fixed resolution 
re apply transforms resulting set points 
repeat operation transforms change set points resolution 
visualize behavior decoder unit square examining set points obtained iterated applications transforms 
applied transforms points unit square obtaining large overlapping regions corresponding left right transforms original points 
note points part left right regions 
unit square application transforms 
attractor shown gray dark gray points reachable attractor left transform light gray points reachable right 
small white wedge gray areas overlap contains ambiguous attractor points reachable transforms 
unit square applications transforms 
shows unit square iteration transforms iterations 
shows final galaxy attractor obtained iterations fail produce contraction 
fractal attractor exhibits self similarity longest arms galaxy shapes attractor 
shows derive tree point attractor 
starting point attractor small circle top left transform dashed line takes immediately attractor specifically attractor region labeled indicating region reachable attractor points left transform 
tree far 
right transform point top takes point attractor indicated circle lower left part 
point point goes attractor region labeled left transform goes attractor right transform specifically region labeled indicates region reachable attractor points right second transform 
second point decodes tree parent tree completing derivation 
final attractor showing derivation tree daughter tree 
left transform shown dashed line right transform straight line 
repeating process point attractor map set trees decoded raam resolution 
described earlier tree set corresponds equivalence class points decode tree 
points class tend cluster giving interesting way laying raam language spatially 
shows phenomenon raam hill climbed decode language described section grayscale denoting tree equivalence classes attractor points 
dramatic striping pattern equivalence classes inherent fractal raam model derives comparatively elegant solution hill climbing produced language 
linguistic advantages new raam described earlier new raam formulation thoroughly addresses shortcomings traditional raam model 
infinite loops terminating non terminals eliminated making terminal test test point fractal attractor raam decoder 
furthermore new formulation provides principled account generativity grammatical competence 
treating raam fractal generated arbitrary resolution increase generative capacity raam bound giving model scales perfectly name infinite raam iraam 
shown straightforward matter hill climb weights iraam generates strings language tree equivalence classes tractor points cluster extreme left colored black labeled right colored white labeled 
system 
briefly dynamics network point unit square transforms point guaranteed attractor 
behavior corresponds terminal component recursive grammar chomsky normal form language 
addition left transform point ends left side unit square right transform ends right side 
successive application left right left transforms leads zigzag dynamics balances left right zig zag lands attractor terminates oscillation 
behavior corresponds recursive component grammar 
provide constructive proof obtaining behaviors resolution 
proof gives exact iraam competence model non regular cf language 
specifically show exists set weights raam tractor generated predetermined resolution contains trees language 
performance limitations sizes trees produced derive resolution non attractor unit space sampled arbitrary stipulation breakdown model 
infinite competence thing iraam brings connectionist nl modeling 
iraam method encoding decoding trees just strings strong generative capacity known 
iraam direct model hierarchical linguistic structure 
immediate implication result iraam parser just recognizer 
extent real nl processing involves assignment meaning strings structure merely grammaticality judgments ability represents significant advance application connectionism nl 
interesting way iraam handles syntactic ambiguity 
consider fractal addressing scheme described earlier 
terminal point word attractor associated address simply sequence indices transforms taken arrive attractor point points attractor 
transforms assume digit sequence fall range example binary branching iraam transforms terminals address digits digit address effectively puts word part speech equivalence classes 
story 
path terminal ter attractor terminals ambiguous addresses containing digits range express fact transform taken arrive point sequence 
continuing linguistic analogy ambiguity corresponds word belonging part speech chomsky flying planes example flying verb adjective 
binary branching iraam example point left right inverse attractor digit address point symbol general ary iraam possible digit addresses consisting unambiguous values ambiguous values 
fact great linguistic importance iraam reason typically exclusively iraam decoder favor putting th non ambiguous terminal class th position string terminals set weights generate attractor transients attractor 
nonterminal structure binary branching iraam structures possible occur 
iraam contains ambiguous terminals decode structures 
returning flying planes example assign unambiguous verbs category unambiguous nouns planes category ambiguous flying category 
assignment natural ability binary iraam decode structures gives parses expression flying planes 
existence proof raam deal syntactic ambiguity non deterministic grammars 
short believe iraam solves problems earlier raam model addresses linguistic inadequacies recurrent neural net models discussed earlier 
iraam section outlined linguistic criteria plausible nl model model able handle slightly non cf phenomena copying crossed serial dependencies incapable handling cf phenomena absent deprecated nl mirror image constructions incur relatively high cost producing parsing structures 
investigate point tested ability iraam model shown learn context free training set con exemplars language enumerated languages readers troubled possibility planes singular verb carpenter planes wood substitute cars unambiguous noun 
increasing order length fractal address representing representing hill climbing learn weights 
initial weights noise added weight came gaussian distribution zero mean standard deviation added noise standard deviation scaled fraction training set missed 
resulting weights generate trees iraam resolution attractor generated resolution initial starting point space sampled resolution 
hill climbing produce results languages average success strings covered languages 
instructive look successes achieved 
comparing best hill climbed networks language strings covered strings generated network fit general pattern training set strings fit pattern best network fit pattern words network producing grammatical strings network essentially guessing 
attribute results iraam aforementioned tendency put symbols class left side branch symbols class right side 
words trees form natural iraam trees form precisely types trees building blocks mirror image language bias mirror image language harder iraam learn counting language despite fact expressible simple cfg 
result means proof sort consider interesting reasons 
suggests languages iraam share important formal property nl avoidance mirror image constructions 
second result illustrates iraam imposes constraint terminal symbol semantics nonterminal syntax constraint absent definition cfg grammar chomsky hierarchy terminal symbol appear 
extent individual natural languages favor putting part speech fixed locations sentence phrase english generally object japanese subject object verb iraam appears advantage traditional grammars model nl 
interpretations demonstrated new formulation raam fractal attractor terminal test enables model show competence ambiguity represent variety tree structures represent deprecated mirror image structures 
plan relate new formula number chosen allowed include members language strings length language meant exemplars longer order enumerate 
effect task harder task tion linguistic formalisms tree adjoining grammars joshi schabes categorial grammars steedman having similar properties 
hypothesize relation may achieved multiplicative connections gate lexical varieties naturally recursive dynamics 
means complete mean imply nl grammar represented neurons weights 
hand principle contractive maps emergence fractal attractors limit behavior nonlinear systems mathematical facts successfully image compression systems 
provides evidence relevance principles connectionist modeling natural language 
reason believe principles right interpretation scale support neurally plausible universal grammar 
bach brown marslen wilson 
crossed nested dependencies german dutch psycholinguistic study 
language cognitive processes 
barnsley 

fractals 
new york academic press 
blair pollack 
analysis dynamical recognizers 
neural computation 
blank marshall 
exploring symbolic subsymbolic continuum case study raam 
technical report tr computer science department university indiana 
bresnan kaplan peters zaenen 
cross serial dependencies dutch 
linguistic inquiry 
chalmers 

syntactic transformations distributed 
connection science 
chomsky 

models description language 
ire transactions information theory 
chomsky 

syntactic structures 
mouton 


complexity vocabulary 
linguistics philosophy 
fodor pylyshyn 
connectionism cognitive architecture critical analysis 
cognition 
gazdar 

phrase structure grammar 
jacobson eds nature syntactic representation 
reidel 


english context free language 
linguistic inquiry 
joshi schabes 
tree adjoining grammars 
rozenberg salomaa eds handbook formal languages automata chapter 
berlin springer verlag 
kolen 

exploring computational capabilities recurrent neural networks 
ph 
thesis ohio state 
kalman 
tail recursive distributed representations simple recurrent neural networks 
connection science 
lawrence giles fong 
natural language grammatical inference recurrent neural networks 
ieee transactions knowledge data engineering appear 


copying natural languages context freeness queue grammars 
proceedings th meeting association computational linguistics pp 

melnik pollack 
gradient descent method neural fractal memory 

international joint conference neural networks ieee 
pinker prince 
language connectionism analysis parallel distributed processing model language acquisition 
cognition 
pollack 

recursive distributed representations 
artifical intelligence 
postal langendoen 
english class context free languages 
computational linguistics 
rodriguez wiles elman 
recurrent neural network learns count 
connection science 
rumelhart mcclelland 
learning past tenses english verbs 
rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition volume 
mit 
shieber 

evidence context freeness natural language 
linguistics philosophy 
sperduti 

labeling raam 
technical report tr international computer science institute 
steedman 

categorial grammar 
wilson keil eds mit encyclopedia cognitive sciences 
mit 
pollack 
fractal reconstructive analogue memory 
th annual cognitive science conference pp 



dynamical automata 
technical report tr computer science department cornell university 
van gelder 

compositionality connectionist variation classical theme 
cognitive science 
williams zipser 
learning algorithm continually running fully recurrent neural networks 
neural computation 
