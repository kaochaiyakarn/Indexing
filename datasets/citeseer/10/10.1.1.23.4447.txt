layered learning paul utgoff david department computer science governor drive university massachusetts amherst ma technical report september explore incremental assimilation new knowledge sequential learning 
particular interest network knowledge layers constructed line manner learned units represent building blocks knowledge serve compress representation facilitate transfer 
motivate need layers knowledge advocate sequential learning avenue promoting construction layered knowledge structures 
novel stl algorithm demonstrates efficient method simultaneously acquiring organizing collection concepts functions stream rich unstructured information 
learnability compression artificial neural networks sequential learning domain illustrations sequential tasks teacher designed curriculum knowledge decomposition simple learning mechanisms stl algorithm 
experiment connectivity heuristics 
experiment input dependency connectivity heuristic 
experiment highest layer connectivity heuristic 
experiment reduced set input primitives 
connectivity 
summary 
discussion layered learning learning essential element intelligent behavior 
know humans learn arbitrary piece knowledge time 
agent receptive ideas difficult learn reasonably small amount effort 
ideas remain distant agent knowledge develops rendering difficult knowledge simple absorb 
way knowledge accumulates incrementally lifetime agent 
starting point discussion knowledge accumulates lifetime agent seemingly result basic kind learning mechanism 
discussion focuses computational processes psychological validity neurological plausibility 
knowledge acquired readily presentation constitutes frontier receptivity 
knowledge learned agent provides basis assimilate new knowledge 
knowledge assimilated frontier receptivity advances improving basis understanding 
explore idea knowledge accumulate incrementally virtually unbounded number layers refer view approaches layered learning 
agent process input stream structures knowledge usefully layered organization 
proceed discussion layered knowledge representation essential maximizing knowledge compression generalization 
comment common approaches practiced artificial neural network learning 
short review attempts model learning layers knowledge 
background perspective illustrate important points concrete example 
layered learning benefit input stream result organized curriculum 
second simple learning mechanisms drive knowledge organization process effectively 
important possible design algorithms model sequential learning large number tasks long period time 
learnability compression learning process compressing observations experiences form applied advantageously 
general statement hypothesis may explain great observations succinctly exploits regularity achieve compression excellent predictor events rissanen langdon 
extent hypothesis correct theory help agent predict consequences improve agent projective reasoning 
structural procedural knowledge compressed important implications space consumption time consumption learnability 
critical means achieving compactness refer existing previously acquired knowledge possible replicate place 
finds notion structured programming coding useful procedures functions referring needed 
leads great compression executable code 
results large coding efficiencies functionality needed multiple locales produced debugged just 
approach modular programming led notion data abstraction sharing code libraries general elevation functionality programmable machines 
arbitrary constraint depth functional nesting 
practical purposes maximum depth 
shapiro applied model structured programming learning calling structured induction 
saw reuse knowledge facilitated compression learner benefit applying layered learning idea learning tasks 
decomposing learning problem learning subproblems learn subproblems individually return higher level learning problem workable level abstraction 
example learning king pawn versus king chess position won important criteria pawn opposing king far side board order queen captured 
concept learned discriminates positions 
learning concept primitive original learning problem 
powerful approach respect producing compression 
view feature true false position giving extra dimension discrimination 
modular programming arbitrary limit number layers knowledge nested manner 
shapiro demonstrated dramatic improvements compression compared learning task structural decomposition 
matter saving space 
total time required learn concept concept total time needed learn concept decomposition 
concept independent larger problem 
learned free contexts appears reused needed variety contexts 
equivalent functionality concept learned context appears 
concept constitutes building block element knowledge simplify learning expression higher level knowledge context 
pagallo fringe algorithm attempted find useful subconcepts searching pathological effects omitting 
shapiro provided task concept decomposition pagallo 
noticed decomposition knowledge replicate 
particular fragments replicated structure observed fringe decision tree 
reducing smallest replicated subtree boolean function rebuilding tree function new feature compact tree result 
way important subconcepts identified automatically iterative manner 
significant difficulty see data cause replicated subtrees form context 
suffer consequences obtaining benefit 
remains promising direction research 
reducing replicated structure general approach compression 
cook holder subdue system induces graph grammar guided minimum description length compression measure beam search background knowledge 
arbitrary depth limit grammar 
mention specifically important idea building block knowledge example benzene ring identified chemistry application 
achieves compression able name times overcome small cost maintaining name substructure 
hint algorithm searches decomposition partial function indicated collection labelled training instances 
algorithm considers limited subsets variables subfunctions variables picking configuration compresses data best 
algorithm locks decomposition step greedy manner 
approach designed find functional decomposition differs grammatical structure replacement rules 
practical limitations arity decomposed functions constraints depth decomposition 
summarizing main points section layered learning achieve great compression avoiding replication knowledge 
storing element knowledge single definition kind procedure function concept referring element needed serves eliminate replication 
generalization includes process grouping abstracting data elements classical sense importantly process organizing knowledge usefully entities called building blocks 
generalization pertains widest applicability reusability knowledge 
decomposition facilitate compression 
artificial neural networks variety artificial neural network ann algorithms devised rumelhart mcclelland freeman 
generally impose severe constraint number layers computation causes compression learnability suffer 
refer algorithms approaches limit number layers layered learning 
artificial neural network approaches layers continue receive great deal attention applied useful class problems learn 
artificial neural networks offer reported fits generally category restriction layers 
functionally shallow networks preclude forms knowledge reuse facilitate compression learnability shallow networks unattractive regard particularly goal model lifelong accumulation knowledge 
constraint layers limits ability reuse knowledge learned previously building blocks 
consider boolean function expressible layers combinational logic 
function just layers may require exponential expansion number gates connections 
combinations implicit deeper circuit explicit shallower circuit 
undertake learning boolean function subject constraint just layers learning forcing exponential realm 
principles apply equally layers hard soft threshold functions 
consider simple illustration organizational tradeoff 
suppose wish build boolean logic circuit patterned expression letters indicate boolean input values 
written corresponding circuit layers computation input logic gates fifteen wires counting inputs output 
contrast distributing ands functionally equivalent logic circuit expression corresponding circuit layers computation input logic gates input gate wires 
layered circuit requires hardware layered circuit 
gradient descent global error minimization nested functional forms shown scale deeply nested forms 
anecdotal evidence additional layers may degrade layered learning learning process tesauro 
networks fixed architecture trained global error minimization theoretical results shown loading data network np complete problem independent training algorithm regardless network shallow deep judd blum rivest sma 
network architecture allowed grow learning trained localized signals results apply 
white shown networks grow learning learn arbitrary functions limit 
various shallow network architectures called universal approximators certain classes functions mitchell 
function may representable architecture learnability function representation guaranteed 
similar saying continuous function approximated arbitrarily accurately sum monomials may require large number monomials 
similarly boolean function represented disjunctive normal form form may suffer respect learnability compression 
remain concerned learnable representation 
jacobs modular architecture partitions space tasks way layered network handles disjoint subset tasks 
form decomposition sense tasks partitioned level 
multiple subconcepts building blocks subsequent layers learning 
sophisticated model piecewise linear fit training data nilsson 
linear threshold unit competes classify instance trained accordingly linear discriminant applies subset training instances 
linear discriminant serves purpose provide answer subdomain expertise 
constructive methods add hidden units learning increasing width existing layers ash hanson frean jones utgoff precup 
constructive methods generally bog 
explained stuck local minimum forced traverse shallow error gradient 
potentially exponential learnability requirements imposed layers major contributor 
methods add hidden units manner increases number layers gallant fahlman lebiere driven single goal reducing residual global error single task hand 
kind non constructive approach places multiple related tasks output layer architecture suddarth holden caruana 
enriches error gradient hidden units learning 
suddarth explored putting extra tasks output layer need learned 
mere presence training process sped learning actual task interest 
problems associated layers remain 
summarizing main points section imposing constraint maximum depth knowledge nesting may constrain amount compression generalization achieved 
networks layers known learnable gradient descent error function parameter space 
partitioning large task set special cases necessarily produce building blocks knowledge 
existing constructive methods designed single task learning designed remove layered learning residual error form building blocks 
systems designed solve multiple tasks shared hidden units fixed architecture benefit sharing hidden units suffer having layers computational units 
sequential learning facilitate learnability compression important eliminate particularly constraint number layers knowledge 
mentioned systems constraint solve tasks fixed ahead time 
longer view wish agents learn new tasks terms old 
ability form building block concepts critical 
means forming building blocks learn concept sequential manner old concepts available expressing new concepts 
learning systems attempt learn succession concepts just single target concept 
sammut marvin able previously learned concepts building blocks learning new concept 
system assumes presence wise teacher 
teacher decides concepts teach order 
positive effect progressively improving basis subsequent learning 
clark thornton discuss need layers representation need map representation 
propose specific algorithm discussing problem theoretically 
offer helpful distinction classes learning problems call type type ii learning 
type learning problems studied methods capture regularity directly observable 
type ii learning mapping variables new variables absolutely necessary order uncover unobservable regularity 
course level mapping new variables may needed complicating learning problem 
offline methods searching mappings generally intractable information available guide process definition 
important perspective 
clear implication mappings arise learning variety concepts functions type manner happen provide useful mappings problems arise 
new appear approaches larger learning problems bottom manner learning progression tasks 
spirit shapiro structured induction addresses type ii learning problems learning sequence tasks 
example stone explored layered learning domain robotic soccer 
observed learning tasks tackling intractable standard type methods 
teaching system progression tasks large task learned 
system relies teacher decide teach 
learning algorithm representation layer computational units 
approach nesting learning tasks system rivest 
extended cascade correlation training set networks ahead time solve variety useful tasks 
system add learned network encapsulated single unit network constructed 
produces nested form learning building block hierarchical 
valiant proposed architecture concepts represented layers linear threshold units 
discusses idea unit correspond concept unit trained individually localized training signal 
remains designer organize units layered learning connections decide train 
limit depth nesting goal express concepts terms building block concepts 
somewhat different vein studies developing nervous system impacts learning 
example elman suggested developed mental capacity infants helps learning admitting small chunks knowledge 
simulations language learning artificial neural networks indicated starting small network capable processing short sentences helps accelerate learning 
development continues modeled enlarging network ability learn process longer sentences considerably enhanced having learned handle short sentences 
starting larger network outset impairs learning 
jacobs showed developmental approach learning improves performance 
learning granularity vision spatial frequency range followed learning efficient learning granularities outset 
quartz sejnowski quartz sejnowski discuss patterns neurological growth including axonal dendritic synapse formation 
relate various studies support notion nerve activity correlation signals proximal dendrites promote growth branching 
view development learning driven agent experiences interactions environment 
learning remains nonstationary problem life agent 
main points section problems difficult learn shallow type ii mapping inputs outputs 
accounts common approach manually engineering input representation order reduce learning task simple presently weak algorithms handle 
handful researchers examining nest learning new learning problems easier learned previously 
developmentally limited processing capability facilitate early learning 
processing capability develops new learning build formed 
knowledge attempted design online learning algorithm organizes knowledge layers computational elements serve building blocks subsequent learning 
domain illustrations sections explore aspects layered learning 
interest build online learning algorithm organizes concepts acquires 
ground discussion employ domain advanced concepts kinds card forms card solitaire 
concepts rich purposes illustration 
kind card called column stackable pertains cards play 
card stacked card bottom column conditions hold 
color suit card color suit card differ second rank ace 
king card exactly card shall ignore rules cards may placed head column immaterial 
layered learning second kind card called bank stackable applies cards play stacked bank 
card play may placed card play bank conditions hold 
suit card suit card identical second rank card exactly card shall ignore rules cards may start bank typically unimportant 
notice concepts depend properties card individually collectively 
notions suit suit color rank suit colors differ rank successor suits identical mentioned building block concepts 
humans concepts difficult compute primarily rank suit suit color indicated plainly card 
problem slightly richer understandable suppose deck cards standard indications 
imagine cards solely integers interval indicated rank suit color 
rank card implicitly integer interval suit implicitly integer interval 
suits grouped color suits 
fashioned problem simple understand entirety rich lend layers knowledge 
deeply nested knowledge goal wish hand engineer input representation leaves problem simple layered method 
completeness state definitions formally shall attempt learn exactly way 
define column stackable bank stackable suit rank mod suit color suit mod column stackable suit color suit color rank rank bank stackable suit suit rank rank notice nested set definitions 
compact express target concepts manner 
discussion shall avoid integer modular arithmetic employed 
shows hand designed layered network consisting inputs variety building block units target concepts 
units shown line boxes top units layer shown group 
unit output line descends diagonally right input line diagonally left 
output line unit connected input line unit dot appears intersection 
lines crossing dot connected 
linear threshold unit shown clear box linear combination unit threshold shown shaded box 
example rank inputs input line diagonally left looking connecting dots input club diamond heart spade 
building block concepts functions successively map inputs increasingly useful representations 
notice layers computation indicated groups units 
subconcepts learning problems right 
agent able acquire structure layers represents knowledge 
shall see section units strictly unnecessary 
simply network human reasonably construct serves purpose moment 
layered learning input input club club diamond diamond heart heart spade spade black black club diamond heart spade red red rank rank fewer suits black red suits identical suits red black suit colors differ rank successor bank stackable column stackable 
hand designed layered network propositional concepts card individually symmetric 
looking card concept suit corresponds fixed sub intervals domain interval 
concepts critical sub interval boundaries 
knowledge subintervals straightforward compute suit testing falls particular subinterval smaller 
table indicates suits computed sub intervals integer card value spade heart club diamond compute suit value integer interval input card value compute rank linear combination 
suit 
weight suit unit subtracts corresponding multiple rank unit 
hand designed network suit unit 
boolean units suit 
assuming true maps false maps suitable linear combination compute rank 
diamond 
club 
heart 
suit colors red black simple disjunctions relevant suits 
suits black red suits red black suit colors differ units compute exclusive suit color 
rank successor concept somewhat opaque hard threshold units test relation 
test difference exactly inequalities necessary test simultaneously difference rank difference 
true course difference exactly 
concept suits identical disjunction suit equivalence tests 
column stackable conjunction suit colors differ rank successor concepts bank stackable conjunction layered learning 
target concepts suits identical rank successor concepts 
depicts target concepts column stackable bank stackable matrix 
card indexes row card indexes column 
large solid circle indicates ordered pair column stackable large hollow circle indicates ordered pair bank stackable small solid circle dot indicates pair impossible member target concept 
ordered pair column stackable bank stackable 
see input dimensions concepts requires care specify exactly 
twelve decision regions boundaries aligned axes 
depicted decision boundaries enclosed regions discussed 
sequential tasks teacher designed curriculum design online learning algorithm learns new concepts previously learned concepts additional inputs learning task 
goal mimic process receptive new ideas difficult understand current state knowledge 
model mechanisms kind 
section presents illustration economies accrue learning layer 
hand designed network learned sequentially layer time 
concept function learned layer trained individually supervised manner layered learning independent learning task 
possible learn concept single unit time units layer connected possible take entire layer time 
stream training examples processed delivering training example corresponding unit 
waits concepts layer learned sufficiently proceeding 
supposes teacher mechanism organizing training sequential manner deciding proceed layer 
main goal agent learn concepts regarding difficult learn immediately 
needs learn simpler concepts build satisfactory basis subsequent learning 
domain important learn certain subintervals integer values important recognize 
basis easier agent learn suit concepts 
goes new layer knowledge advancing frontier receptivity making agent ready acquire 
implemented algorithm train layers successively described 
ordered pair cards class label function value included corresponding unit trained 
unit computational layer trained straightforward manner appropriate error correction rule 
unit computational layer units preceding evaluated feed forward manner inputs correct ordered pair cards 
error correction rule applied 
simple experiment exact dependencies knowledge elements concepts functions modeled linear units known eliminating problems connectivity 
shown layers learned successively epochs respectively examples training corpus 
interest memory organization classification accuracy need full corpus 
total cpu time seconds pentium iii 
second experiment run connectivity known advance 
existing units including input units provide outputs inputs new layer units 
case layers learned successively epochs respectively just seconds 
highly connected approach target concepts learned rapidly layers trained sequenced manner 
approach allowing inputs previously learned concepts serve input basis subsequent learning desirable property allowing agent draw knows order understand new find regularity obfuscated 
undesirable property massive connectivity growing dimensionality subsequent learning difficult embrace 
methods learning presence irrelevant subconcepts variables features especially helpful littlestone mechanisms independent tests correlation targets current output layer may needed 
return issue 
conducted experiments variety feed forward artificial neural networks backprop werbos rumelhart mcclelland numerous report detail 
summary putting aside speed issues network topologies layers hidden units failed 
providing topology perfect connectivity hand designed network 
confronting task common layers hidden units convergence elusive 
importantly known layer provides decision boundaries second layer com layered learning groups boundaries form decision regions 
regions specific task hand 
units learned constitute building blocks 
depicts possible decision boundaries implicitly decision regions target concepts 
regions partition instance space provide multiple layers decomposed knowledge 
regions tailor target concepts 
partitioning instance space particular task may satisfactory accomplishment learning take place way provides successive levels mapping earlier concepts possible 
example hidden layer network learn column stackable leaving bank stackable decision boundaries depicted may 
example boundaries bank stackable regions making useless purpose learning bank stackable subsequently 
decision boundaries shown happen task 
alas learning tasks may result serendipitous boundaries useful 
examining see network input units sixteen computational units layer twelve computational units layer output units capable representing target solitaire concepts 
noted ability represent mean learning succeed 
theoretical results computational learning theory show expect global training algorithm perform problem 
notice outputs network architecture described term dnf format 
pitt valiant proved term dnf concept representations efficiently learned 
equivalent cnf representation may learned efficiently converting term dnf cnf causes worst case exponential explosion representation size 
returning imagine bank stackable unit unique predecessors 
bank stackable suits identical spade heart club diamond 
learn bank stackable necessary learn just concepts 
modularity subconcepts apparent making learning bank stackable relatively easy existing knowledge column stackable 
illustrate point suppose wanted reuse networks recognizing concepts poker 
network contains concepts applied new problem 
example rank successor needed evaluate elements possible straight may 
similarly suits identical evaluating flush 
said layered networks discussed 
divisions relevant new concepts forcing learning anew 
sequential multitask learning decomposition targets useful subconcepts take place knowledge learning tasks encountered 
suggests strongly learning simple useful concepts layered organization lay foundation may come 
knowledge decomposition simple learning mechanisms discuss novel stl algorithm demonstrates mechanism organizing concepts terms acquired 
models quite directly notion advancing frontier receptivity 
agent benefit greatly receiving information order conforms layered learning agent receptivity expect life experiences ordered 
learning multiple layers building block knowledge absence ordering experiences 
approach try times sense 
entails great inefficiency account decomposition knowledge useful building blocks 
agents different information receive presumably current knowledge differs giving frontier receptivity 
example people attending lecture hear understand differently 
process modeled 
stl algorithm consider target concepts column bank 
suppose ordered pair instance relations may hold stated positive negative atoms instance 
example consider instance bound bound input spade heart club diamond black red rank input spade heart club diamond black red rank spade heart club diamond black red red black fewer suits identical suit colors differ rank successor column stackable bank stackable 
instance bound bound input spade heart club diamond black red rank input spade heart club diamond black red rank spade heart club diamond black red red black fewer suits identical suit colors differ rank successor column stackable bank stackable 
may information strictly necessary agent may know infer atoms due earlier successful learning building block concepts 
terms level discourse mismatch sender receiver 
information agent infer irrelevant information currently difficult absorb 
providing irrelevant information matter communication inefficiency major concern purpose 
simplicity simply provide truth values possible atoms 
assume stream observations holds atoms correspond concepts 
say important decomposition occurred leaving organization layered structure accomplished 
information represented form 
world provides stream sensory perceptual information 
example assumes knowledge integer values ordering 
stream information agent provide primitives higher level information stl algorithm described suggests way information assimilated organized time 
agent learn subconcepts organize layers correspond computational dependencies 
guided assumption simple learning mechanisms available learn refine knowledge element 
table shows stl stream layers algorithm 
predicate function name observed algorithm updates unit described 
unit layered learning table 
stl algorithm input stream observations observation represented list atoms describing known 
observation 
distinguished atom named input bind argument variable argument constant 

non input atom single numeric argument update linear combination unit corresponding atom name numeric argument target value 

non input atom numeric argument update linear threshold unit corresponding atom name target atom negated target 

linear unit network learned sufficiently see discussion mark 
move linear units learned sufficiently new layer 
unlearned unit new layer connect output learned unit input unlearned unit 
exist created added list unlearned units initially empty 
inputs initially distinguished input values provided observation 
set atoms training instance observation algorithm attempts learn atom linear threshold unit linear combination 
atom bound variables arguments atom indicates boolean concept positive negative value indicated atom absence presence negation connective training label 
atom single numeric argument atom indicates numerical function numeric argument training value 
model function numerical argument 
stl algorithm tries learn concepts functions come way 
course concepts learned easily sooner 
example concepts sees second layer presumably learned reliably 
concept function learned successfully output value connected input concepts functions learned reliably 
effect pushing unlearned concepts deeper layer 
process continues pushing unlearned concepts deeper providing improved basis 
models advancing frontier receptivity 
agent receptive learned simply acquired successfully 
approach embodies assumption concepts learned early considered potential building blocks inputs learning concepts 
stl algorithm intended operate online manner 
algorithm important decisions 
determine unit successfully acquired target concept eligible input unsuccessful units 
criterion successful learning stl unit produced correct evaluation consecutive examples vc unit vc dimension linear unit simply unit inputs 
chose empirically problems hand 
examining formulate principled criterion 
second decision stl determine unit learn target concept sufficiently 
possible approach simply connect trained unit untrained unit soon trained unit available 
unnecessarily aggressive unit may train faster capable learning current input connections 
stl relies sample layered learning input input diamond diamond spade spade black black black red club diamond heart club club heart heart red red red black spade fewer suit colors differ suits identical bank stackable column stackable rank rank rank successor 
layered network stl connectivity heuristic complexity determine unit requires additional input connections 
unit examples satisfying learning criterion unit considered failed new connections added training resumes 
number required examples log vc log confidence parameter accuracy parameter thresholded linear units 
number examples required linear combination described similar formula log log pseudo dimension confidence accuracy 
see anthony bartlett detailed discussion sample complexity context neural networks 
experiment connectivity heuristics distinct pairs corresponding atoms training instances 
stl intended online algorithm finite amount data 
observations 
infinite stream input data simulated treating collection observations circular list 
offline algorithm making multiple passes data pass epoch 
algorithm knowledge epoch operates online manner 
algorithm learned concepts functions instances requiring seconds pentium iii total connections 
constructed network shown computational layers different knowledge organization hand designed network 
spade diamond suits learned easily training integer subinterval units 
spade mastered heart learned readily card value layered learning input input diamond diamond spade spade black black club red red club heart heart rank rank black red club diamond heart spade fewer red black rank successor suit colors differ suits identical bank stackable column stackable 
layered network stl input dependency heuristic spade 
similarly club learned diamond acquired 
subinterval concepts required 
second unexpected outcome rank units placed final computational layer 
fewer short rank fewer units defined inputs 
somewhat unsatisfying effectively tosses building blocks rank units 
occurs rank units take relatively long time learn compared concepts primarily simple boolean functions 
different criterion consider unit having learned change outcome 
experiment input dependency connectivity heuristic basic stl algorithm table connects unsuccessful units successful units preceding layers 
allows unit draw previously acquired knowledge solving problem practical drawback creating unnecessarily high dimensional input space 
simple heuristic reducing number connections dependencies basic inputs unit 
new network unit considered connection unsuccessful unit basic inputs subset unsuccessful unit connection 
example connected heart spade connected red black 
employing heuristic stl algorithm produces network shown 
case layers connections 
stl algorithm required instance observations seconds learn units correctly 
noteworthy difference network rank layered learning input input diamond diamond spade spade black black red red club club heart heart rank rank diamond black red red black club heart spade suit colors differ suits identical fewer bank stackable column stackable rank successor 
layered network stl il heuristic units learned earlier due reduced dimensionality 
concepts functions 
experiment highest layer connectivity heuristic complex connectivity heuristic layering information 
connecting unit existing network units unit initially connected highest layer current network 
decision assumption layer network represents new useful mapping input space 
failure unit learn particular input layer indicates unit probably requires higher level remapping input space 
cases unit fails learn input representation higher level representation available resolved adding inputs previous layer unit 
unit fails learn layer inputs layer available add new connections layer units resume training 
note connections removed unit training 
units initially connected network inputs listed training atom 
new connections added time unit deemed failure 
highest layer connectivity heuristic may connect unlearned unit previously learned units 
heuristic stl takes instance observations seconds complete learning 
layers connections 
notice rank units 
connectivity reduced 
layered learning input input diamond diamond spade spade black black heart red red club club heart rank rank diamond black red club heart spade red black suit colors differ suits identical fewer bank stackable column stackable rank successor 
layered network stl il heuristic simpler experiment reduced set input primitives matter curiosity ran stl leaving superfluous concepts 
shows resulting network learned observations seconds 
layers connections 
notice black learned depending club concept 
sense spade true spade diamond simple test club range suffice 
appealing semantically sensible 
connectivity problem choosing learned units connect unlearned unit instance known feature selection problem liu motoda 
abundance possible features learned network units stl select small subset allow unit acquire target concept 
feature selection algorithms typically fall categories wrapper methods described john kohavi pfleger caruana freitag filter methods described hall smith koller sahami 
wrapper methods perform heuristic search possible feature subsets repeatedly running learning algorithm comparing results computationally demanding purposes stl 
unit network need run feature selection algorithm time new connections deemed necessary 
filter methods select features independent learning algorithm efficient making promising approach stl 
mechanism removing extraneous connections network trained useful 
recognizing connections simple locating connections near zero weight 
layered learning units network may compute related overlapping functions 
input units sufficient compute new function units may receiving significant weight value 
example inputs sufficient computing value heart 
connection may adjusted zero overlaps output 
method sequential backward elimination help remains seen 
summary exposed rich stream information stl algorithm extract learn concepts organize repeatedly advance level receptivity 
decomposition organization occur naturally result simple learning mechanisms 
paradoxical simple mechanisms needed build efficient knowledge structures represent difficult concepts 
needed understand handle connectivity problems 
suggested heuristics resulting networks retain great connections strictly necessary 
compare hand designed version just connections 
discussion examined approaches modeling layered learning 
involves learning curriculum simply illustrates difficult problems learned broken sequence simple problems 
remarkable human academic enterprise devoted organizing knowledge presentation orderly graspable manner 
fits supposition humans frontier receptivity new knowledge layed terms old extent possible 
observe teachers starting semester chapter text away week week waiting subconcepts hidden earlier chapters form 
teachers start quite sensibly chapter progress designed layered presentation 
agent autonomy goal moderation scientists impose serious handicap agents books teachers including parents 
second approach dispensed organized instruction offering possible mechanism extracting structure unstructured stream rich information 
showed stl algorithm adoption simple learning mechanisms drive task decomposition 
simple concepts agent frontier mastered basis understanding grows enabling subsequent mastering concepts difficult 
approach accepts paradox apparent shortcomings unsophisticated learning mechanisms intelligence sophisticated learning mechanisms 
agent benefit greatly curriculum 
stl process stream generated progressively higher level spend great deal trying master concept currently hopelessly difficult 
exposing agent just acquired helps focus effort lead better structuring learned knowledge 
informative explore model learning knowledge layers problems suggest new approaches 
example stl relies kind race produce knowledge organization 
learned simple means achieves status means earned right considered input units learned 
noted drive mechanism organizing knowledge acquired 
strategy necessarily lead best possible organization 
furthermore successfully learned portion organize structure statically cast 
mechanism layered learning unit continue consider units serve best inputs revise selection inputs dynamically 
advocated building block approach designed eliminate replication knowledge structures see quite plainly concepts learned just card learned identically 
mechanism applying learned functions variety arguments highly useful 
inductive logic programming solves problem 
may useful explore variable binding mechanisms modeled networks simple computational devices khardon roth valiant 
main results argument favor layered learning demonstration advantages localized training signals method self organization building block concepts layered artificial neural network 
learning complex structures guided successfully assuming local learning methods limited simple tasks 
acknowledgments supported national science foundation iri iri 
jacobs andy barto connell david jensen provided helpful comments earlier versions 
anthony bartlett 

neural network learning theoretical foundations 
cambridge university press 
ash 

dynamic node creation backpropagation networks 
connection science 
blum rivest 

training node neural network np complete 
proceedings ieee conference neural information pp 

morgan kaufmann 
caruana freitag 

greedy attribute selection 
machine learning proceedings eleventh international conference 
new brunswick nj morgan kaufmann 
caruana 

multitask learning 
machine learning 
clark thornton 

trading spaces computation representation limits uninformed learning 
behavioral brain sciences 
cook holder 

substructure discovery minimum description length background knowledge 
journal artificial intelligence research 
jacobs 

visual development acquisition binocular disparity sensitivities 
proceedings eighteenth international conference machine learning pp 
ma morgan kaufmann 
elman 

learning development neural networks importance starting small 
cognition 
fahlman lebiere 

cascade correlation architecture 
advances neural information processing systems 


fundamentals neural networks architectures algorithms applications 
prentice hall 
layered learning frean 

algorithm method constructing training feedforward neural networks 
neural computation 
freeman 

neural networks algorithms applications programming techniques 
addison wesley 
gallant 

optimal linear discriminants 
proceedings international conference pattern recognition pp 

ieee computer society press 
hall smith 

practical feature subset selection machine learning 
proceedings australian computer science conference 
hanson 

networks 
advances neural information processing systems 
jacobs jordan barto 

task decomposition competition modular connectionist architecture vision tasks 
cognitive science 
john kohavi pfleger 

irrelevant features subset selection problem 
machine learning proceedings eleventh international conference pp 

new brunswick nj morgan kaufmann 
judd 

neural network design complexity learning 
cambridge ma mit press 
khardon roth valiant 

relational learning nlp linear threshold elements 
proceedings sixteenth international joint conference artificial intelligence pp 

koller sahami 

optimal feature selection 
proceedings thirteenth international conference machine learning pp 

littlestone 

learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
mitchell 

machine learning 
mcgraw hill 
liu motoda 
eds 

feature extraction construction selection data mining perspective 
kluwer 
nilsson 

learning machines 
new york mcgraw hill 
pagallo haussler 

boolean feature discovery empirical learning 
machine learning 
pitt valiant 

computational limitations learning examples 
journal acm 
quartz sejnowski 

neural basis cognitive development constructivist manifesto 
behavioral brain sciences 
rissanen langdon 

arithmetic coding 
ibm journal research development 
rumelhart mcclelland 

parallel distributed processing 
cambridge ma mit press 
layered learning sammut banerji 

learning concepts asking questions 
michalski carbonell mitchell eds machine learning artificial intelligence approach 
san mateo ca morgan kaufmann 
rivest 

knowledge speed learning comparison knowledge cascade correlation multi task learning 
proceedings seventeenth international conference machine learning pp 

palo alto ca morgan kaufmann 
shapiro 

structured induction expert systems 
addison wesley 
sma 

loading deep networks hard 
neural computation 
suddarth holden 

symbolic neural systems hints developing complex systems 
international journal man machine studies 
stone veloso 

layered learning 
proceedings eleventh european conference machine learning pp 

springer verlag 
tesauro 

practical issues temporal difference learning 
machine learning 
utgoff precup 

constructive function approximation 
motoda liu eds feature extraction construction selection data mining perspective 
kluwer 
valiant 

architecture cognitive computation 
journal association computing machinery 
werbos 

advanced forecasting methods global crisis warning models intelligence 
general systems yearbook 
white 

connectionist nonparametric regression multilayer feedforward networks learn arbitrary mappings 
neural networks 
jones 

node splitting constructive algorithm feed forward neural networks 
advances neural information processing systems pp 

bratko 

learning discovering concept hierarchies 
artificial intelligence 
