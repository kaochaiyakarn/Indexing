tetra evaluation serial program performance fine grain parallel processors todd austin sohi computer sciences department university wisconsin madison dayton street madison wi cs wisc edu july tetra tool evaluating serial program performance resource control constraints fine grain parallel processors 
tetra primary advantage architect ability quickly generate performance metrics designed architectures 
user needs specify capabilities architecture number functional units issue model implementation 
tetra extracts canonical form program serial instruction trace 
applies control resource constraint scheduling produce execution graph 
control resource constraint scheduling directed processor model specification supplied program 
scheduled tetra provides number ways analyze program performance specified processor model 
include parallelism profiles storage demand profiles data sharing distributions data lifetime analysis control distance branch loop call stack distributions 
report extraction scheduling analysis methodologies tetra 
detail implementation discuss number performance optimizations 
appendix includes user documentation tetra 
fl todd austin sohi 
supported national science foundation ccr 
quantitative analysis program execution essential computer architecture design process 
current trend architecture enhancing performance uniprocessors exploiting fine grain parallelism order metrics program execution operation frequencies sufficient characterizing exact nature dependencies operations essential 
date processors executed instructions sequentially overlapped execution instructions sequential instruction stream pipelining 
processors relevant characteristics dynamic program execution included operation frequencies branch prediction accuracies latencies memory operations cache memory systems 
continuing trend processor architecture boost performance single processor overlapping execution operations fine grain parallel processing models multiscalar vliw superscalar decoupled systolic dataflow complete restricted fashion 
aid design processors simple order metrics dynamic execution operation frequencies operation latencies sufficient 
needed thorough understanding operations program interact nature dependencies operations dynamic execution graph impacted compiler processor model impact performance 
employ dynamic dependence analysis 
dynamic dependence analysis uses canonical graph representation program execution called dynamic dependence graph ddg 
nodes graph represent operations executed edges graph represent dependencies operations 
perform dynamic dependence analysis extract ddg serial program re schedule control resource constraints compiler processor model 
analyze resulting execution graph see impact execution model program performance 
iterating process effectively explore trade offs involved design processors aggressively exploit fine grain serial program parallelism 
primary advantage dynamic dependence analysis program evaluation techniques function simulation timing simulation prototyping ability quickly generate performance metrics designed architectures 
actual implementation compiler processor need known specified perform analysis capabilities model 
report extraction scheduling analysis methodologies dynamic dependence analyzer tetra 
section introduce notion dynamic dependence graph discuss various dependencies exist program executions 
show extracted serial traces bound particular execution model control resource scheduling 
number performance metrics obtained scheduled described 
section algorithms extracting scheduling analyzing serial program 
explore time storage complexities algorithms previous cited 
section implementation tetra discussed detail number successful performance optimizations applied 
summarize report section 
overview primary structure constructed manipulated analyzed dynamic dependence graph ddg 
partially ordered directed acyclic graph representing execution program particular input 
executed operations comprise nodes graph dependencies realized execution form edges graph 
particular operation ddg corresponds single instruction program executable terms operation instruction interchangeably 
reasons 
ddg captures complete execution program mapping instructions program executable operations ddg 
example repeated execution loop see loop instructions represented times ddg number iterations executed program execution 
operation instance particular instruction 
second instructions create values find ddg 
examples instructions include unconditional branches nops 
instructions artifacts particular architecture analyzed program compiled leave actual ddg 
edges ddg force specific order execution dependent operations forming complete ddg weak ordering program required operations 
program dependencies inherent execution program removed changing algorithms program removed usually costs storage possibly execution speed 
classify program dependencies categories data control resource 
data dependencies operations share data dependency operation creates value called read write raw flow dependency 
dependency forces order operations source values created subsequent operations 
data dependencies removed changing algorithms program compiler transformations rewrite code 
examples algorithmic changes include parallel algorithms parallel reduction sequential algorithms serial summation 
examples compiler transformations include loop unrolling invariant loop code motion loop interchange strength reduction 
ddg contains data dependencies constrained control resource limitations called dynamic data flow graph 
execution models extra ambiguous data dependencies may inserted ddg 
ambiguous data dependencies occur static memory disambiguation performed compiler determine memory read write access pointer dereferences access memory conservative assumption ordering enforced compilation write precedes read instruction 
actual execution ordering result hardware support speculative loads multiscalar processor fs 
control dependencies control dependencies introduced conditional branches encountered execution program 
branch predicate resolved known instructions executed branch subsequent instructions share control dependency conditional branch 
view control dependence shown fow applying control dependence analysis possible remove control dependencies re attribute earlier executed instructions 
instruction control flow graph post dominates conditional branch instruction need share control dependence conditional branch path taken conditional branch execute post dominating instruction 
instruction control dependent earlier executed predicate instruction execution invariant respect decisions program 
utilize control dependence analysis section design restrictive issue models 
case control dependencies removed speculative execution 
processor correctly guesses outcome branch execution proceed control dependence existed 
course speculative execution effectiveness processor ability remove control dependencies function program execution branch prediction algorithm processor ability back mis predicted paths 
resource dependencies resource dependencies called structural hazards occur operations delay required physical resource exhausted 
examples limited processor resources include functional units physical storage 
possible remove resource dependencies supplying critical resource 
important resource dependency consider storage dependency 
storage dependencies occur parallel operations compete single physical storage location 
storage dependencies require computation delayed storage location result longer required previous computation 
dependencies classified write read dependencies called anti dependencies war hazards write write dependencies called output dependencies hazards 
different data values reside single storage location lifetime program synchronization required ensure computation accessing correct value storage location 
violation storage dependency result access uninitialized storage location data value stored storage location 
storage dependencies removed renaming 
technique applicable hardware compiler assigns new physical storage location value created making possible values destined logical storage location overlapping lifetimes 
values created assigned different locations program property single assignment location assigned 
course wasteful implement values non overlapping lifetimes renaming implementations determine value longer needed allowing physical storage reclaimed 
extraction ddg representation program execution lends parallelism studies 
lacks total order execution serial stream remains weakest partial order successfully perform computation required algorithms 
machine constructed optimally execute ddg performance represent upper bound performance attainable program 
claim largely independent compilation strategy program ddg contain control resource dependencies 
compiler change data dependencies changes typically local nature 
substantial impact form ddg require changing algorithms program 
compilers generally perform changes say ddg approaches canonical form program 
re map canonical form processor model wish study reveal impact program performance 
shows simple program fragment execution trace resulting ddg 
note loop body instructions original program create instances operations trace ddg 
ddg contains data dependencies shown solid arrows 
fact represent data dependencies ddg control resource dependencies accounted bind ddg processor model control resource scheduling 
scheduling process scheduling operations binds level execution graph 
binding directed execution model specification 
ddg levels horizontal slice ddg 
scheduled ddg thought sequence parallel instructions issue level level processor 
ddg scheduled processor model control resource limitations operation execute soon inputs available 
oracle processor operations execute cycle second cycle 
phases scheduling ddg control scheduling resource scheduling 
phase control scheduling determine level operation issued 
resulting schedule depends compiler processor issue model control structure program 
control scheduling models explored control flow graph cfg scheduling control dependence graph cdg scheduling 
model cfg scheduling traditional issue model instruction issue control dependencies realized traversal control flow graph program 
second model cdg scheduling uses restrictive control dependence graph fow 
scheduling algorithms described section 
second phase scheduling resource scheduling issued operations allocated processor resources scheduling heuristic 
processor resources allocated instruction executed functional unit storage resources 
functional units allocated just long program level transformations recurrence breaking induction variable expansion major impact data dependencies loop intensive programs 
changes quite regular easily simulated ddg extraction algorithmic changes instance identifying transforming serial fft parallel fft attempted compilers widespread applicability 
respect compiler relevant significant control control resource dependencies manifest 
want remind reader execution model includes hardware compiler techniques applied 
loop beq done beq loop done beq done beq loop beq done beq loop beq done sample program execution trace resulting ddg 
solid lines represent true data dependencies node labels correspond instruction instance 
beq reg label 
branches label 
value register reg 
zero falls instruction 
operator computes remainder integer divide 
compute result value stored allocated physical storage register memory 
physical storage reserved indeterminate amount time 
extend resource scheduling include processor resources result buses functional unit input buses instruction window slots methodologies described apply cases 
study unconstrained constrained resource scheduling techniques detailed section 
topological sorting optimal resource unconstrained scheduling technique 
define optimal schedule supply resources produces shortest length schedule 
unfortunately generating optimal schedule dag presence resource limitations np complete problem small resource supplies lk gj 
known algorithm task exponential time exponential respect size trace analyzed 
resources limited scheduling heuristic applied resulting schedule nearly sub optimal 
propose resource constrained scheduling heuristics history schedule history list schedule best fit list bf list schedule fit list ff round robin rnd rbn random random 
heuristics selected study exhibit varying degrees storage space complexity execution return provide equally varying degrees performance respect optimal schedule 
section describe heuristics show relative performance respect 
scheduled derive number interesting metrics execution graph 
analysis ddg metrics fall categories resource demand profiles value metrics 
resource demand profiles show quantitative demand particular resource function level scheduled ddg 
parallelism profile resource demand profile showing number functional units level ddg 
shows parallelism profile gnu gcc compiler compiling source file cexp 
available parallelism defined arithmetic average number operations level parallelism profile viewed speedup attained machine capable extracting executing ddg program 
length parallelism profile critical path length execution 
length identical height scheduled ddg gives minimum number steps required evaluate operations scheduled ddg 
available parallelism critical path length 
available parallelism operations level critical path length approximately levels 
values parallel operations reside physical storage 
physical storage demand profile waiting token profile shows amount storage required level ddg 
profile function parallelism program lifetime values increases physical storage requirements 
metric useful determining amount type temporary storage required realize levels parallelism shown parallelism profile 
aggregate compacted storage demand profile gcc shown 
tetra break storage register memory storage demand 
storage class original register memory likelihood compiler allocate object register memory location 
case register locations reserved high frequency short lived values memory reserved values 
method recognizes storage allocation compiler inextricably bound architecture code intended run host architecture departs architecture register allocation 
frequency lifetime breakdown possible architecture designer get handle storage needed level memory hierarchy achieve level program performance 
value metrics examine characteristics reveal values produced manipulated consumed program execution 
example value lifetime distribution shows distribution value extents 
extent value measured distance respect levels ddg 
shows value lifetime distribution sample gcc execution 
degree sharing distribution frequency distribution indicating nodes ddg level average std dev std dev example parallelism profile 
gnu gcc compiling cexp 
parallelism profile compacted top dashed line average plus standard deviation bottom dashed line average minus standard deviation middle solid line average parallelism levels 
ddg level average std dev std dev ddg level average std dev std dev example aggregate storage demand profile 
gnu gcc compiling cexp 
parallelism profile compacted top dashed line average plus standard deviation bottom dashed line average minus standard deviation middle solid line average parallelism levels 
regs memory value lifetime ddg levels regs memory example value lifetime distribution 
gnu gcc compiling cexp 
graph cumulative distribution function showing lifetime ddg levels register memory values allocated host compiler 
ddg generated values 
distribution important possible dataflow realization execution engine indicating operations fired token created explicit token store machine example conventional multiprocessor execution program 
multiprocessor different processors responsible execution different parts suitably constrained ddg program 
measuring data flows nodes subgraph albeit constrained form ddg matches processor execution model measure degree data sharing processors example 
results directly applicable design directory cache coherency schemes 
shows degree sharing distribution sample gcc execution 
dependency distance distributions show control distance producer value consumer 
examining possible gauge real architecture speculate past program control structures realize level parallelism 
control distances measures terms program control construct 
measure distances control flow graph branch distance distances iterations loop loop distance distances execution call stack call stack distance 
shows branch loop call distance sample run gcc 
branch distance reveals level branch speculation required simple single window execution processor achieve level performance target model 
loop distance metric displays balance loop parallelism functional non loop parallelism 
indicator processing model support type parallelism compiler processor implementation 
call distance indicates parallelism spread separate function bodies reveals effectively architecture support dynamic execution multiple procedures static inlining 
having introduced concept dynamic dependence graph described process ddg analysis extraction scheduling analysis dive head long details processes implemented 
ddg extraction scheduling analysis methodologies designing algorithm extract schedule analyze serial trace virtually unlimited length challenging task 
interesting representative trace easily larger degree sharing memory registers example degree sharing distribution 
gnu gcc compiling cexp 
graph cumulative probability distribution showing total sharing values register memory values allocated host compiler 
branch distance loop distance call stack distance control distance total barriers branch distance loop distance call stack distance example control distance distribution 
gnu gcc compiling cexp 
graph cumulative probability distribution showing total branch loop call distance distributions edges ddg 
stored conventional computer memory 
resulting ddg annotated analysis larger 
algorithms section dominating goal limit storage usage costs time complexity algorithm increased 
describe methodology briefly discuss previous area 
previous plethora measured average parallelism sequential instruction stream particular hardware configuration byp nf tf wal 
studies typically find length critical path computation compute average parallelism total number instructions divided number cycles execution 
measure resource load determine maximum number resources required 
studies typically evaluate average parallelism changes various constraints register renaming various branch prediction strategies memory disambiguation strategies changes operation latencies instruction window sizes resource constraints changes parameter result changes critical path length resource demand consequently average maximum parallelism 
interested single measure average available parallelism aspects ddg need construct entire ddg parts 
early measuring total available parallelism kuck kea statically analyzed hand program dependencies fortran programs resulting available parallelism estimated analyzed code fragments 
kumar kum gathered exact parallelism profiles serial fortran programs 
kumar extracted parallelism profiles rewriting fortran programs comet profile generated execution program 
method lend imperative language applied fortran 
operations ddg fortran statements assumed execute unit time 
method extracting serial traces quite different kumar 
kumar placed fortran statements ddg place machine instructions ddg 
allows precise control relative time taken operations ddg finer grain parallelism fortran statements show results 
technique builds ddg serial execution trace modifications source program applied language compiled interpreted 
kumar implemented control model similar cdg issue model 
specifically allow fortran statement execute surrounding predicates completed evaluation 
lam wilson lw shown assumption overly restrictive speculation allow operations execute prior resolution surrounding predicates 
look effects control model available parallelism look control barrier viewpoint 
measuring control distance dependencies show level parallelism effective speculation expose parallelism 
csy similar kumar comet rewrites programs 
kumar comet limited scheduling program statements ability schedule granularity operation level statement level loop level subprogram level 
limit computing resources available analysis 
implemented list scheduling algorithms nearoptimal identical list bf earliest available identical list ff circular identical random identical random 
expand proposing new powerful scheduling heuristic history 
extend heuristics support scheduling resource non deterministic latency allows examine effects limited storage resources 
number papers dataflow literature included examples ddg analysis ca acm nik 
example culler arvind ca provide detailed parallelism profiles waiting token storage demand profiles dataflow programs 
dataflow processor language environment lends ddg analysis 
instrumenting dataflow processor execution sufficient generate parallelism profiles critical path 
environment lacks storage control dependencies results include effects available parallelism 
clear results extend programs written imperative languages fortran applied processors restricted computation models 
research results agree acm kum ordinary programs algorithms intended execute parallel environments significant amount fine grain parallelism 
larus performed detailed studies loop level parallelism number spec benchmarks lar 
analysis intended primarily directing development application compilers capable parallelizing loops analysis limited intra loop parallelism lexically top level loop 
analysis examines parallelism global view allowing inter loop parallelism analyzed 
number programs results quantitatively close larus supporting programs parallelism intra loop parallelism 
mahlke examined effects compiler optimizations serial program parallelism 
looked specifically classes program optimization classical superscalar multiprocessor 
superscalar optimizations encompass superblock global trace scheduling techniques loop unrolling peeling induction variable expansion 
multiprocessor compiler optimizations primarily memory renaming simulated analysis data migration faster loads 
classical superscalar optimizations expose parallelism especially small instruction windows 
multiprocessor optimizations effective large instruction windows 
running oracle machine resource control constraints perfect memory disambiguation unlimited renaming program transformations exposed little intrinsic parallelism 
supports thesis ddg program part canonical form program 
wilson lam lw studied effects control dependencies available parallelism 
primary result showed combining control dependence analysis multiple flows control speculative execution expose large portion total available parallelism unconstrained ddg 
critical factor development aggressive fine grain parallel processors control dependence resolution factors data dependence resolution 
extend providing framework control resource constraints combined 
theobald examined serial program parallelism 
constraining available computational resources history schedule able explore program sensitivity limited functional unit supplies 
extend evaluating advantages computationally frugal scheduling heuristics showing limitations scheduling heuristic 
examine storage demand combine control resource scheduling 
extend earlier number ways 
provides detailed description methodology describe number invariants algorithm examine time storage complexity 
extend methodology include construction control resource constrained provide full treatment scheduling heuristics complexity limitations 
detail number successful performance optimizations implementation tetra 
results include number new metrics including storage demand profiles value lifetime profiles control distance profiles 
unconstrained ddg extraction analysis construction unconstrained ddg need consider data dependencies realized analyzed program execution 
scheduling control dependencies ignored unlimited supply functional units storage available level ddg 
extend unconstrained ddg methodology account restrictive issue models limited physical resources discuss difficulties handling 
basic extraction scheduling analysis algorithm shown pseudo code 
algorithm iterates instruction trace stream order execution performing extraction control resource scheduling analysis pass 
trace instructions accessed argument trace 
variable typed stream functions similar standard input stream 
trace stream provides built operations eof 
trace eof returns boolean indicating information read trace stream inst trace assigns instruction trace stream variable inst 
extraction analysis performed pass trace stream trace stream piped directly ddg analyzer need store entire instruction trace 
type level integer address inst record pc address src set dest var baselevel level procedure trace stream inst mapping level value schedlevel level inst inst baselevel trace eof inst trace inst dest exists inst dest endif inst inst dest latency inst baselevel schedlevel latency inst baselevel value inst src value exists max value endif endfor schedlevel inst schedlevel inst schedlevel inst dest schedlevel latency inst max schedlevel latency inst inst schedlevel endif inst schedlevel endwhile value value endfor endproc main loop ddg extraction scheduling analysis algorithm 
function inst inst earliest level level earliest procedure inst inst schedlevel level endproc function inst inst earliest level level earliest unconstrained ddg control resource scheduling routines 
operation placement execution graph constrained input values placed earlier latest input value 
level computed innermost loop 
loop iterates inputs current instruction storing restrictive input variable 
initial value value variable baselevel forces lower bound placement operations ddg 
baselevel primarily handling instructions unknown side effect handling pre existing values 
instruction unknown side effect system calls possible directly obvious identify storage read written instruction 
algorithm assume instruction modified live values program pessimistic system call assumption modified optimistic system call assumption 
conservative assumption implemented placing operation levels currently ddg bounding placement operations levels operation unknown side effect 
placement level variable ensures operation access value created previously 
updated time new level previous level 
bounding operations implemented value variable baselevel 
placements earlier baselevel instructions forced read values updated instruction unknown side effect 
identical procedure realize control dependencies 
optimistic system call assumptions simply place instruction topologically earliest level ddg baselevel 
pre existing values reside data segment 
placed compile time compiler 
architectures pre existing values exist stack registers start program execution 
case value placed creating operation execution graph available earliest defined level ddg 
level stored variable baselevel 
determining restrictive input level latest input system call calls constrain placement include effects control resource dependencies respectively 
control scheduling performed resource scheduling operation issued resource requirements determined 
called update control barriers created current instruction 
routines processor model dependent 
detail ones implemented tetra section 
shows scheduling routines schedule ddg resource control constraints 
control resource constraints routines simply return passed levels 
resulting execution graph topologically sorted tar ddg 
schedule optimal respect length represents upper bound performance attained analyzed program 
schedule commonly called eager evaluation schedule operation scheduled soon inputs ready 
possible optimal schedules operations example schedule optimal schedule operation moved level schedule 
operation level execution graph known compute result value available simply operation placement level plus latency create value 
algorithm calls processor dependent function latency determine latency operation 
latency equal total time required compute result total time resource reserved case memory 
level newly created value inserted live address register specifier destination storage 
live acts associative memory allowing algorithm find ddg level operation input values unique value identifier 
algorithm employs mapping variable array domain value identifiers large sparse 
mapping variable supports built function map exists returns boolean value indicating currently exists mapping value 
assignment map value installs new value mapping variable deleting previous value exists 
tetra live implemented hash table 
necessary assign arbitrary unique value identifiers newly created values 
values residing storage location non overlapping lifetimes sufficient value destination register number memory address unique value identifier 
serial semantics trace stream ensure entry live yields correct value information 
algorithm assumed register number obviously different memory address case single bit added distinguish registers mapped unused portion memory address space 
storage requirements live proportional size program active name space 
active name space program complete set names memory addresses register specifiers program produce execution 
de allocate value record touched memory incur memory cost 
reasonable assume value records kept reasonable small analyze nearly program definition program active name space fit memory computer run program 
reduce overhead trimming value memory requirements live reducing live overhead 
section discuss implementation live tetra 
live implemented hash table time complexity accessing values 
generation resource demand profiles occurs call 
point operation just placed ddg 
inspecting instruction determine resources update appropriate profile vector index derived level placed operation 
profile storage size order constructed execution graph height critical path length may necessary increase granularity profile array entries 
words saving resource demand single level array element save total resource demand levels array element producing profile granularity 
unfortunately information average parallelism storage demand computed range 
samples range number operations placed level known analysis 
interesting information exact shape curve maximum minimum variance parallelism storage demand lost 
usually problem long traces little parallelism 
describe section number implementation tricks help mitigate profile storage requirements 
value metric distributions require information stored values live 
value created referenced variables may updated 
example computing degree sharing distribution keep counter initially zero value live increment time variable referenced 
value dead call stored value usage information transferred appropriate frequency distribution 
important call values live completion trace analysis remaining values live dead 
actual mechanics produce value metric distributions generally trivial shall welcome reader describe 
implementation control distance metrics significantly complicated offer complete description define amount time resource reserved operation useful building schedules pipelined resources 
metrics fully describe handling section 
control distance analysis consider sake illustration accepted task constructing machine exploit available parallelism serial program instruction stream 
major obstacle accomplishing task devising mechanism effectively overcome control barriers operations producing values operations consuming values 
values control barriers producer consumer overcome little cycle machine execution 
control barriers program constructs perturb normal inline fetch issue instructions 
quantify difficulty overcoming control barriers number barriers resolved speculated operation produces value operation consumed value 
path represented edge ddg say ddg edges control distance respect serial trace 
building control distance distributions gauge just effectively architecture overcome control barriers fully exploit available parallelism 
examine control distances branch distance loop distance call distance 
branch distance ddg edge defined number conditional branches resolved speculated serial trace consuming operation issued 
loop distance number loop iterations issued consuming operations issued 
commonly referred literature loop carried dependency distance 
call distance distance call stack producing operation consuming operation 
need examine issues edges ddg meaningful examine second procedure measure type control distance 
examine control distances edges gain clearer results concentrating edges control barriers hardest overcome 
edges ones critical path 
value traveling critical path edge transmitted consumer cycle machine edges important hardest resolve 
unfortunately computationally infeasible locate exact set edges critical path property critical path easily generate reasonable superset edges 
edges critical path span exactly level ddg 
analyze edges length trim away easier edges metrics 
just easy edges removed function program execution 
call length edges restrictive edges 
simplest control distance measure branch distance 
state required global branch counter 
counter incremented time conditional branch instruction encountered 
branch counter current value placed live value created 
branch distribution updated time operation uses value time length edge created value examining restrictive edges 
distance computed current branch counter value minus branch counter value stored value record 
loop distance edge defined intra loop iteration distance zero definition value loop instance 
instance loop complete invocation loop structure program execution 
loops executed loop may multiple instances 
shows nested loop example 
example code definition created iteration loop executing second iteration nested loop value third iteration loop outside scope loop loop distance example edge operation creates operation uses shown live 
loop distance edge definition occurs iteration loop third iteration 
instrumentation methodology complicated possibility value may created dynamically nested loop may complete execution require values attributed surrounding loop 
fact know loop attribute value common loop tree dynamic loop instances loop value defined 
intra loop distance computed subtracting iteration loop distance def depth depth loop inst loop iter loop iter loop iter loop iter loop iter live base line 
outer loop inner loop 
definition outer inter inner iter outer iter loop inst nested loop example resulting loop tree 
brackets nested loop example represent high level language iterative construct 
inner loop smaller bracket lexically nested outer loop larger bracket 
execution nested loop example outer loop iterates times 
iteration outer loop inner loop executes iterations creating definition second iteration 
definition outside scope inner loop third iteration outer loop 
loop tree solid lines represent pointers loop instance records dashed lines pointers loop iteration records 
common loop creating iteration 
employ loop tree structure track program dynamic loop instances iterations storage efficient manner 
shows live loop tree resulting code example 
loop tree contains node types loop instance descriptors loop iteration descriptors base line node 
loop instance descriptor created invocation loop program 
loop defined subgraph cfg basic block loop header dominates basic blocks including blocks contain back edge loop header asu 
definition suffices cfg contains irreducible loops 
irreducible loops created languages generally practice 
ddg analyses annotate instruction trace loop start continue signals 
natural loop analysis performed prior executing ddg analyzer 
loop start signal encountered loop instance descriptor created linked current loop invocation 
loop signal encountered current loop invocation reset parent loop instance 
loop instance descriptor contains pointer creating loop iteration unique brand tree search optimization tree depth variable tree searches count 
loop tree descriptors contain count indicating descriptors value records point 
program create virtually unlimited number loop instances iterations imperative loop tree storage released soon known unreferenced value live 
employ counting scheme extent values created program referencing loop instance records follow program structure result loop descriptor free time analysis 
counts just reclaim loop tree storage count zero 
loop iteration descriptors linked loop instance contained 
occurrence loop start loop continue signals cause creation loop iteration descriptors loop tree 
loop iteration descriptors contain pointer loop instance created iteration iteration number count 
loop instance descriptor loop iteration descriptors coalesced kept separate reduce size loop tree 
information loop instance descriptor larger pointer memory 
value referenced edge created ddg 
control distance edge computed walking loop tree starting iteration descriptor definition value common loop instance ancestor 
loop distance computed subtracting iteration number definition iteration number 
base line node reached definition value contained common loop loop distance zero 
optimizations speed tree walk depth counts branding 
loop instance descriptor contains depth respect base line node 
walking tree find common ancestor immediately determine walk tree deeper 
top level loops program form forest loop trees base line node real loop 
top level loop tree assigned unique numeric brand stored loop instance descriptors 
definition reside top level loop tree quickly determine loop distance zero create loop instance descriptors different brands 
storage required store loop tree bounded value proportional size program active name space 
number created loop instance iteration descriptors unbounded counting scheme ensures active loop iteration descriptors number values live 
value live point increment count loop iteration descriptor loop iteration descriptors worse case point loop instance descriptor 
size live loose definition irreducible loop loop contain jumps middle outside loop reducible loop entries loop header 
see asu formal definition 
spec benchmark containing irreducible loops spice 
problem rectified rewriting offending routine structured manner 
proportional size active name space loop tree worse case 
practice values live point iteration descriptors actual storage requirements 
procedure loop distances computed easily adapted measure call distance 
recall call distance distance call stack definition value 
call tree similar loop tree instance descriptors required procedures iterate 
compute call distance locate common ancestor call tree 
procedure definition value occurred may nested procedure instance 
depth value minus depth common ancestor node call distance 
branding optimization apply computation call tree form forest trees 
storage required store call tree bounded value proportional size active name space reason loop tree 
practice significantly smaller loop tree 
having methodologies perform unconstrained ddg extraction scheduling analysis extend described methodology support restrictive issue models limited physical resources 
sections discuss difficulties handling 
control scheduling tetra explore impact processor issue models control flow graph cfg control dependence graph cdg 
model cfg issue traditional issue model 
operations may execute soon trace order branch instruction resolved 
model employs speculation mechanism describing particular edge cfg speculated correctly constrain ddg 
mis speculated edge inserts control barrier ddg instructions placed earlier ddg mis speculated branch instruction 
shows ddg cfg control scheduling 
example speculation employed operations insert control barriers schedule 
cfg control scheduling algorithm shown 
algorithm implements functions called main scheduler loop determining unconstrained placement operation see 
simply constrains operation level control barrier stored variable 
needs update instruction passed creates control barrier 
called passed instruction control resource scheduled 
procedure checks instruction conditional branch attempts speculate past call speculate 
speculate processor model dependent routine implements speculation mechanism level adaptive bit counter static predictor percent correct speculation fails control barrier inserted execution graph updating constraint variable 
reset level mis predicted conditional branch schedlevel 
operations placed mis predicted conditional branch 
second control scheduling model cdg scheduling issues operations relaxed control model specified control dependence graph 
see fow cfr complete description formal definition 
cdg scheduling allows operations issued soon surrounding predicate resolved correctly speculated 
instruction said control dependent surrounding predicate predicate ultimately determines instruction issued 
ddg control scheduled control dependence graph control flow graph shown 
note operations elevated scheduled ddg just surrounding predicate operations respectively 
contrast cfg scheduling case control dependent respectively 
mobility allowed operations post dominate branches issue invariant decisions 
cdg model appears multi threaded respect cfg model cfg control constrained schedule cdg control constrained schedule 
schedules performed ddg 
solid lines represent flow dependencies node labels correspond instruction instance 
control constrained schedule uses control flow graph cfg scheduling model speculation 
second control constrained schedule uses control dependence graph cdg scheduling model speculation 
control barriers produced branch instructions shown horizontal dashed lines labeled instruction instructions creating control barrier 
var level function inst inst earliest level level max earliest procedure inst inst schedlevel level inst speculate inst schedlevel endif endif endproc cfg control scheduling algorithm 
program simultaneously executing multiple non contiguous areas program 
research parallel operations horizontal slices execution graph non local independent operations coming different areas program merit examining issue model 
cdg issue model exploited hardware compiler 
example beckmann bp proposed hardware support multithreaded execution serial programs 
execution conditions fire threads cdg restrictive cfg 
compiler examples include pdg scheduling techniques region scheduling gs 
techniques implement cdg issue model mundane hardware applying speculative non speculative code motion code executes resolution control dependent predicate preceding branch 
shows cdg control scheduling algorithm 
constrains operation time control barrier defined cdg 
level control dependent branch resolved ii 
instruction control dependent instruction current procedure level current procedure began execution 
compute ddg level case procedure calls returns address executed branch inst control dependent 
employ computationally efficient dominance frontier technique annotate conditional branches basic block control dependent cfr 
involves computing post dominator tree lt 
node cfg depth traversal identify nodes cfg basic block longer post dominates successors 
dominance frontier cfg node 
control dependent branches contained basic blocks dominance frontier 
ddg analyzer annotate trace stream basic block list addresses basic blocks current control dependent need performed basic block instructions basic block control dependent branches uses list find correct control dependent branch 
barrier stack variable records issue time procedure current call stack 
call procedure control barrier level calling instruction pushed stack 
schedule instructions statically defined control construction cdg result basic block control dependent number different conditional branches 
dependent multiple branches controlling predicate latest executed current procedure instance 
var mapping address level stack level function inst inst earliest level level inst max earliest top max earliest inst endif procedure inst inst schedlevel level inst speculate inst inst pc schedlevel endif endif inst push inst endif inst pop endif endproc cdg control scheduling algorithm 
scheduling time space shown schedule non deterministic algorithm complexity complexity perform latencies 
find history delete list best fit degrades insert list fit list best fit find log best delete log list fit fit insert log check check list log find delete log fit insert check check log find round robin delete insert check check find random delete insert check check table comparison scheduling heuristics 
time space complexities written respect trace size number schedulable resources maximum deterministic latency constant 
time complexity refers asymptotic time complexity managing resource heap bound costly individual operation 
space complexity refers just resource heap nodes include live 
non deterministic latencies caused scheduling physical memory resources 
latencies known time scheduled check check scheduling mechanism employed 
dependencies case ii 
procedure returns issue level procedure popped barrier stack 
resource scheduling phase scheduling resource scheduling serves constrain ddg due functional unit storage limitations 
generating optimal schedule dag presence resource limitations npcomplete problem employ scheduling heuristic 
tetra implements scheduling heuristics exhibit varying degrees storage space complexity execution return provide equally varying degrees performance respect optimal schedule 
history schedule history list schedule best fit list bf list schedule fit list ff round robin rnd rbn random random 
table summarizes resource constrained scheduling heuristics 
heuristic schedules approximate optimal schedule question naturally arises close optimal heuristic schedule 
question typically answered generating called competitive ratio heuristic 
competitive ratio numeric upper bound showing worse case longer resulting heuristic schedule compared length optimal schedule problem instance 
denote problem instance notation opt represents length optimal schedule heur length heuristic schedule 
competitive ratio heuristic max ae heur opt oe hope examine compute heur non naive lower bound opt upper bound producing result conservative true competitive ratio heuristic 
advancing state art particular heuristic entails finding better lower bounds heur tightening accuracy competitive ratio 
computation non naive competitive ratio heuristics schedule arbitrary dags presence resource constraints unsolved problem 
attempt solve proven appendix relative order performance heuristics 
ordering summarized table appendix summarize results relative ordering spends processor memory resources scheduling heuristic better schedules generated 
underlying assumptions adopted selecting heuristics scheduling limited resources 
operations scheduled order occur trace apply line scheduling heuristic 
assumption required heuristic solution reasonable implement execute 
second schedule particular resource registers physical memory functional units common pool 
example scheduling register resources resource pool associated particular register registers 
implementation heuristics somewhat complicated yields better schedules 
representative compiler dynamic processor scheduling techniques 
history schedule history schedule heuristic powerful expensive retains complete history resource allocation back schedule 
back scheduling allows scheduler allocate resource earlier time previous allocation resource 
may odd scheduling resource past justifiable 
purpose analysis evaluate performance program processor compiler model execution models aggressively exploit fine grain serial program parallelism 
built processors instruction presentation compiler reorganize instruction issue order closer breadth visitation unconstrained scheduled ddg 
history scheduling allows tetra generate best possible schedule independent presentation instructions 
back scheduling requires deterministic latency scheduled operations filling hole schedule allocation request finite known heuristic build schedules limited physical storage 
shows function called history scheduling heuristic 
variable holds complete resource history resource pool 
different variable required pool resources 
entry index array indicates resource instances allocated level scheduled ddg 
entry bounded max resources total number resources resource pool 
level ddg represented array critical path length entries critical path length length program critical path 
algorithm scans slot width inst sufficient resources 
increments array entries levels 
clamping entries max resources ensure resource allocated level 
inst pipeline fill latency operation long issue operation delay pipeline 
inst ddg levels operation inserted resource pipeline 
non pipelined resources memory inst equal total resource allocation latency 
ddg scheduled history schedule heuristic 
example operations back scheduled 
history schedule heuristic performs building constrained ddg schedules high resource utilization 
algorithm processor storage greedy 
worse case proven believe line history scheduler produces schedules comparable line list scheduler 
typically compilers perform instruction scheduling 
var array critical path length integer function inst inst earliest level level integer boolean earliest critical path length true inst max resources false endif endfor inst endfor return endif endfor history scheduling heuristic 
level ddg searched locate slot sufficient resources making time complexity algorithm length trace worse case critical path length 
storage complexity algorithm conceivable operation may scheduled slot resource history array 
keep entire history entire analysis 
possible release resource history base level baselevel advancing 
occur control barriers system calls encountered conservative side effect handling 
tetra take advantage cases worse case time space complexities 
analyzed program executes instruction little program parallelism long critical path resource history requirements soon great reasonable machine particularly respect storage requirements 
event approximate resource history increasing granularity resource history slots 
granularity element resource history array track resources contiguous levels scheduled ddg see 
approximation possible limit average resource demand area represented granularity resource instances available slot contain operations 
exact form ddg slot known 
ddg levels certainly operations level re scheduled granularity require levels execute maximum resource instances 
shows compute maximum error resource history array granularity worse case path length extension single slot arises actual schedule contains operation levels gamma operations contains remaining operations gamma gamma operations 
resulting error due gamma gamma operations rescheduled serviced resource instances 
correct schedule extend gamma gamma ddg levels 
worse case behavior operations dependent operation level operations rescheduled critical path 
actual path length extended gamma gamma converges large values maximum error atypical worse case typical executions smaller path length error 
history list schedule best fit list schedule fit round robin instruction trace topologically sorted unconstrained dynamic dependence graph resulting functional unit constrained schedules scheduling heuristics 
heuristic instructions scheduled trace order operations take unit time functional units capable servicing instruction 
schedule charts show issue time scheduled instruction horizontal axis functional unit vertical axis 
ddg level number operations scheduled number operations scheduled slots grain operations operations total operations max actual path length max error operations maximum error computation history scheduling heuristic 
top graph complete parallelism profile resource constrained schedule program execution 
schedule resources available single ddg level resource history buffer granularity ddg slots buffer slot 
lower graph magnified reconstruction shaded slot top parallelism profile 
hashed region lower graph represents actual form ddg 
list schedule fit best fit list scheduling heuristics reduce time storage complexities allowing back scheduling 
back scheduling need keep complete resource history stored level resource instance available scheduling 
list scheduling heuristics select appropriate resource instance state resource heap earliest time operation schedule execute 
list scheduling heuristics differentiated select resource instances 
list schedule fit heuristic selects earliest available resource instance resource heap 
shows pseudo code list scheduling fit heuristic 
shows example ddg scheduled list schedule fit heuristic 
initially resource heap contains resource instances total number available scheduling available scheduling time 
algorithm employs priority queue manage resource heap 
priority queue support operations delete min insert 
delete min finds earliest available resource instance deletes priority queue 
resource scheduled time resource instance available earliest scheduled input control constraints 
resource instance re inserted insert operation resource heap inst time units scheduled resource instance available operation 
ddg analyzer implement priority queue fibonacci heap ft 
selected structure fastest know priority queue implementations 
asymptotic time complexity access minimum value entry root heap arbitrary value insertions log deletions 
improve fit resource selection strategy 
earliest available resource instance may available earlier operation 
scheduling considerations input availability control constraints force earliest available resource instance wait idle long periods time execute operation 
list scheduling support var priority queue level function inst inst earliest level level schedlevel level schedlevel delete min schedlevel max earliest schedlevel insert schedlevel inst schedlevel list schedule fit heuristic 
var binary tree level function inst inst earliest level level schedlevel level schedlevel delete max prior earliest schedlevel max earliest schedlevel insert schedlevel inst schedlevel list schedule best fit heuristic 
back scheduling idle period results lost scheduling opportunities turn reduces resource utilization 
reduce idle resource time list schedule best fit selects resource instance available closest time time operation execute 
resource instances available time operation execute previous scheduling constraints available resource instance available selected 
shows pseudo code list scheduling best fit heuristic 
shows example ddg scheduled list schedule best fit heuristic 
selecting best fitting resource instance inefficient priority queue number schedulable resource instances employ binary tree 
binary tree nodes sorted time resource instance available scheduling 
delete max prior function selects right node left earliest 
tree nodes left earliest left node returned 
tetra avl binary tree 
balanced binary tree structure asymptotic time complexity log selecting resource instance log arbitrary value insertions log deletions 
list schedule fit best fit storage complexity number schedulable resource instances resource instance records resource heap 
algorithms requires storage 
asymptotic time complexity algorithm log asymptotically dominating operation deleting resource instance heap log 
aggregate time complexity list schedule best fit fit heuristics fit search insert components cheaper 
results significant time savings smaller values find typical case 
total resource latency known case memory resource instances check var queue level function inst inst earliest level level schedlevel level schedlevel delete tail schedlevel max earliest schedlevel insert head schedlevel inst schedlevel round robin scheduling heuristic 
check strategy employed 
check check scheduled resource instance left resource heap know free point checked resource heap resource heap insert function 
resource marked available immediately 
caveats observed generating storage constrained resource schedules 
resource constrained schedules generated single unified resource pool 
separate allocation pools register memory resources certainly give dubious results guarantee compiler allocate values registers memory especially vastly different processor models 
multiple pools desired allocations register memory pools frequency lifetime residing value 
information known release storage resource storage constrained schedules difficult attain 
second extent storage known allocated de allocated re 
point storage re assigned new value typical case actual earlier time re assignment 
consequently resource sit idle period time 
generally storage constrained schedules knowledge conservative 
round robin random schedules round robin random schedules included require storage schedule operations time 
performance studied better computationally expensive scheduling heuristics justified 
round robin scheduler commonly referred fifo scheduler detailed 
simply removes resource tail queue previously resource assigns operation 
resource instance re inserted queue head 
resource scheduled resources scheduled 
queue initially filled resource instances 
random scheduler see simpler 
randomly selects resource existing resources 
tetra tool ddg analysis tetra tool performing dynamic dependence analysis 
implements algorithms described section number run time storage optimizations described 
tetra freely distributable encourage practitioners researchers alike extend see fit 
see epilogue report distribution details 
shows dynamic dependence analysis framework employ 
phases analysis 
phase trace generation qpt quick profiler tracer bl re write var array max resources level function inst inst earliest level level schedlevel level integer random max resources schedlevel schedlevel max earliest schedlevel schedlevel inst schedlevel random scheduling heuristic 
program executable way creates trace file run 
normally need write trace file disk pipe information directly tetra 
multiple analyses run trace file generally faster save trace disk 
second phase analysis trace regeneration program generated qpt recreate original trace number special signals added 
added signals indicate memory accesses loop activity control dependent instruction addresses function calls malloc free calls source line numbers non local gotos 
tetra interfaces qpt linked call back function 
analysis framework supports condor execution 
condor collection software tools libraries allow programs execute remotely homogeneous architectures access program local resource network layer system call interface bll 
condor able save single trace file farm numerous analyses run idle machines 
tetra currently targeted mips architecture running 
written safe extensions 
safe set templates macros performs extensive run time checking pointer array accesses ba 
safe program access bounds lifetime object notification program dumps core 
safe disabled program compile vanilla compiler 
impact program run time minimal instruction analysis gcc compiling stmt option run time bounds checking increased run time 
tetra second ddg analyzer implementation 
implementation paragraph described 
newer implementation nearly orders magnitude faster previous capabilities run time bounds checking code 
speedup comes primarily optimizations hash table storage allocator trace 
primary manipulated data structure tetra live hash table 
hash table implemented array bucket chains 
trace creates values destination addresses hashed xor folding inserted hash lists 
subsequent value hashes address searches bucket chain correct value record 
linear search dominated run time ddg analyzer implementation 
hash table referenced order addresses referenced executing program expect large amount temporal locality stream 
take advantage temporal locality dynamically reordering hash table bucket chains data value referenced moved head bucket chain 
simple line change original program approximately doubled speed analyzer 
simple optimization live removal register value records directly addressed array 
register space small faster store entire space linear array index array register number 
change resulted performance increase adding dynamic hash re ordering 
input data program analyzed qpt regeneration program qpt trace generation code qpt call back interface trace file pipe condor wrapper optional paragraph ii trace generation regeneration analysis framework 
drum shaped objects represent data files rectangular objects represent executable programs 
directed edges programs data files file paths 
dashed line program indicates procedure call linked interface 
solid line program program analyzed qpt trace generation code indicates object level code transformation 
factor slowed original implementation significantly memory thrashing 
analysis programs large data sets working set program quickly larger physical memory 
nearly half allocated memory unusable result internal fragmentation standard unix malloc package 
typical unix malloc package optimized speed space 
packages typically employ power allocation strategy 
program requests block memory say bytes size allocate add header request making needed storage say bytes 
allocator picks byte block node list size nodes 
needed node size available larger block bisected fulfill request 
scheme allocation fast storage overhead high 
overhead need born needed nodes fixed size 
case allocation tetra 
primary structures allocated tetra value records loop descriptors resource instance records individually fixed size 
case employ generic fixed size heap manager reduce storage overhead 
fixed size heap manager allocates fixed size nodes large single allocations chunks near power size reducing internal fragmentation standard unix allocator 
heap manager builds free list fixed size nodes large allocation 
allocation strategy number benefits 
nodes allocated en masse nearly node overhead 
bytes overhead amortized allocated chunks 
effect reducing memory requirements 
typically see analyses memory allocation strategy 
second benefit increased program spatial locality 
measure exactly analyzer cache performance better frequently data resides smaller space overheads incurred moved away individual allocations 
advantage strategy individual heaps deleted entirety need delete individual nodes allocated 
contrast common pool allocator malloc entire class structures deallocated freeing individual allocation malloc fact support resetting heap standard way 
original analyzer implementation pixie generate regenerate traces tetra uses qpt 
pixie uses trace compression minimal witness techniques resulting traces quickly larger store disks 
requires pipe trace generator trace 
initial implementation performance perturbed trace piped output ddg analyzer pipe stages carry information program analyzed analyzer 
unix pipes extremely costly rely heavily system calls 
results large amount user system space copying versa costs associated system calls executed 
qpt produces smaller trace files trace file stored disk easily store instruction traces megabyte disk 
eliminates trace generator trace pipe 
trace tetra pipe eliminated linking regeneration program directly tetra 
run time program improve spends time kernel response machine improved 
optimizations implemented speed control scheduling storage demand profile generation 
basic block control scheduling optimizes basic algorithms section noting instructions basic block control dependent branch independent control scheduling methodology employed 
result determine control barrier instructions basic block basic block 
likewise need determine control barriers created basic block execution instruction second case delay slots create control barrier 
delta distributions provided significant speed generation storage demand profiles 
incremented range representing entire lifetime distribution time value dead 
operation size trace 
longer traces large portion total execution time spent updating storage demand profiles 
delta distribution operation time incrementing slot variable alive decrementing slot variable dead 
slot profile contains derivative slope storage demand profile 
recreate storage demand profile integrate entire range simply requires summing delta values writing current sum slot distribution output file 
close section touching bit testing 
tetra large complex program lines code 
environment analyses billions instructions length nearly impossible apply intuition verify output correct 
testing strategy needs applied 
tested tetra parallel trace printing option 
set option outputs execution graph interval file execution completes 
instructions level execution graph output instructions second level 
generally applicable large traces served testing 
small programs constructed exhibited features exercise certain codes looping code test loop distance implementation parallel traces checked ensure schedule correct specified hardware configuration analysis correct resulting execution graph 
summary tetra tool evaluating serial program performance resource control constraints fine grain parallel processors 
tetra primary advantage architecture designer ability quickly generate performance metrics designed architectures 
user needs specify capabilities architecture number functional units issue model implementation 
tetra employs step process 
extracts canonical form program serial instruction trace generated qpt 
applies control resource constraint scheduling produce execution graph 
global control resource constraint scheduling directed processor model specification supplied program 
resulting execution graph analyzed evaluate serial program performance specified architectural model 
optimization simplified handling delay slot instructions control barriers computed start basic block point conditional branch detected allows tetra easily enforce semantics instruction conditional branch control constraints branch 
control scheduler supports scheduling control flow graph restrictive control dependence graph 
control scheduling models support speculation number static dynamic prediction methods 
resource scheduler supports scheduling heuristics varying cost performance capability limit amount processor functional unit memory resources physical renaming store available building execution graph 
scheduled tetra provides number ways analyze program performance 
include parallelism profiles storage demand profiles data sharing distributions data lifetime analysis control distance branch loop call stack distributions 
feel tetra fills important niche intuitive speculation construction complete architectural simulator compiler 
invite practitioners researchers get tetra apply 
distribution details epilogue report 
gratefully acknowledge contributions people 
alain graciously offering write fibonacci heap code invaluable input 
jim larus writing supporting qpt 
tom ball input support qpt 
scott breach lebeck steve reinhardt vikram adve input 
supported national science foundation ccr 
author todd austin wishes express gratitude wife son support developing tetra working report 
tetra qpt availability readers may get latest tetra source release anonymous ftp electronic mail 
anonymous ftp distribution ftp cs wisc edu file pub tetra tetra tar version number 
electronic mail distribution unix shar format send electronic mail austin cs wisc edu 
tetra distribution provided free charge flexible copyright restrictions 
qpt available jim larus larus cs wisc edu 
licensing agreement ordering instructions qpt available cs wisc edu file pub qpt license ps 
qpt available distribution wisconsin architectural research tool set details concerning distribution available wisc edu file ftp pub license ps information obtained sending mail cs wisc edu 
acm arvind culler maa 
assessing benefits fine grained parallelism dataflow programs 
conference proceedings supercomputing pages november 
allan lee srinivas 
enhanced region scheduling program dependence graph 
conference record th annual international symposium microarchitecture pages portland december 
association computing machinery 
arvind nikhil 
dataflow approach general purpose parallel computing 
computation structures group memo mit july 
todd austin sohi 
dynamic dependency analysis ordinary programs 
conference proceedings th annual international symposium computer architecture pages 
association computing machinery may 
asu aho sethi ullman 
compilers principles techniques tools 
addison wesley reading ma 
ba scott breach todd austin 
safe seat belts available todd austin austin cs wisc edu fall 
bl ball larus 
optimally profiling tracing programs 
conference record th acm symposium principles programming languages pages albuquerque nm january 
association computing machinery 
bll allan michael litzkow miron livny 
condor technical report 
technical report computer sciences department university wisconsin madison january 
bp carl beckmann constantine polychronopoulos 
microarchitecture support dynamic scheduling acyclic task graphs 
conference record th annual international symposium microarchitecture pages portland december 
association computing machinery 
byp butler yeh patt scales 
single instruction stream parallelism greater 
conference proceedings th annual international symposium computer architecture pages 
association computing machinery may 
ca david culler arvind 
resource requirements dataflow programs 
conference proceedings th annual international symposium computer architecture pages 
association computing machinery may 
cfr ron cytron jeanne ferrante barry rosen mark wegman kenneth zadeck 
efficiently computing static single assignment form control dependence graph 
acm transactions programming systems october 
csy chen su yew 
impact synchronization granularity parallel systems 
conference proceedings th annual international symposium computer architecture pages 
association computing machinery may 
fow jeanne ferrante karl ottenstein joe warren 
program dependence graph uses optimization 
acm transactions programming languages systems july 
fs franklin sohi 
expandable split window paradigm exploiting fine grain parallelism 
computer architecture news pages may 
ft fredman robert endre tarjan 
fibonacci heaps uses improved network optimization algorithms 
journal association computing machinery july 
garey graham johnson 
performance guarantees scheduling algorithms 
operations research january 
gj michael garey david johnson 
computers intractability guide theory np completeness 
freeman 
gs rajiv gupta mary lou soffa 
region scheduling approach detecting redistributing parallelism 
ieee transactions software engineering april 
kea kuck measurements parallelism ordinary fortran programs 
computer january 
kum kumar 
measuring parallelism computation intensive scientific engineering applications 
ieee transactions computers september 
lar james larus 
estimating potential parallelism programs 
alexandru nicolau david gelernter thomas gross david padua editors proceedings third workshop languages compilers parallel computing chapter pages 
mit press 
lk lenstra rinnooy kan complexity scheduling precedence constraints 
operations research january 
lt lengauer tarjan 
fast algorithm finding dominators flowgraph 
acm transactions programming languages systems july 
lw monica lam robert wilson 
limits control flow parallelism 
conference proceedings th annual international symposium computer architecture pages 
association computing machinery may 
mahlke warter chen chang hwu 
effect compiler optimizations available parallelism scalar programs 
th annual international conference parallel processing pages 
nf nicolau fisher 
measuring parallelism available long instruction word architectures 
ieee transactions computers november 
nik nikhil 
parallel programming language id compilation parallel machines 
computation structures group memo mit july 
smith johnson horowitz 
limits multiple instruction issue 
conference proceedings third international symposium architectural support programming languages operating systems pages 
association computing machinery may 
tar robert tarjan 
data structures network algorithms chapter 
society industrial applied mathematics 
tf flynn 
detection parallel execution parallel instructions 
ieee transactions computers october 
theobald gao hendren 
limits program parallelism 
conference record th annual international symposium microarchitecture pages portland december 
association computing machinery 
wal wall 
limits instructional level parallelism 
conference proceedings fourth international symposium architectural support programming languages operating systems pages 
association computing machinery april 
appendix relative performance scheduling heuristics history optimal list bf list ff rnd rbn random higher performance potential relative performance scheduling heuristics 
arrows drawn superior schedulers inferior 
solid arrows show performance order proven text 
dashed arrow shows unproven order 
heuristics drawn bold borders sorted selection method 
shows relative performance heuristics studied 
arrows drawn superior heuristics inferior heuristics 
heuristic inferior input produces schedule long longer superior counterpart 
relation transitive descendant heuristic inferior 
root tree heuristic optimal schedule heuristics descend 
scheduling heuristics classified broad categories employ sorted selection methods shown bold boxes shown bold boxes 
sorted selection methods sort resource pools time resource instances available scheduling 
sorting allows schedulers reasonably decisions resource scheduled 
heuristic invests ordering resource heap better decisions 
fact prove showing total order performance heuristics sort resource pools 
claim heur length schedule heuristic heur problem instance resource instances 
problem instance set operations requiring instance execute 
schedules place operations order 
resulting relative performance sorted pool scheduling heuristics history list bf list ff mnemonics history list bf list ff represent schedules history list schedule best fit list schedule fit schedulers respectively 
proof transitive need show history list bf list bf list ff scheduling heuristic define set heur set contains time schedulable resource instance available scheduling scheduling th operation heuristic heur 
jrj length input trace history list bf resources resources resources time resources violating condition jr history tj jr list tj 
length resulting schedule heuristic heur max heur max set function returning maximum member 
words schedule length equal time takes resource instance finish operation scheduled 
jr heur tj number resource instances set heur ready schedule time prove order performance showing heuristics say better worse jr better tj jr worse tj put way schedule progresses better heuristic resource instances lagging inferior heuristic 
follows max better max worse max better greater max worse resource instances worse resource pool maximum resource instance better schedule contradicts assertion 
assertion holds conclude better worse max better max worse 
assertion possible derive invariant lower bound values better worse min better min worse resource instances better min better assertion implies worse 
invariant needed proof employ run time check paragraph ii 
remains prove assertion heuristic pairs mentioned earlier 
jr history tj jr list tj prove induction 
basis jr history tj jr list tj holds true resource instances initially available time 
earliest time operation scheduled executed due previous scheduling constraints 
assuming jr history tj jr list tj assertion jr history tj jr list tj violated event conditions hold jr history tj jr list tj boundary condition assertion fail 
ii 
defines bound assertion fail 
iii 
history schedules resource instance available time list bf schedules resource instance available time time condition violates assertion jr history tj jr list tj essence conditions history resource instances may pass list bf resource instances number history resource instances remaining greater equal number list bf resource instances 
shows condition occur 
shaded boxes resource instances immediately assertion condition ensure resource instance scheduled exist instance history resource pool available instance exist history resource instance list bf resource instances history resource instances 
note moved say condition longer holds history resource instances list bf resource instances performing action condition iii violate assertion 
subtlety examined declare assertion proved 
recurrence exists value placement heuristic 
implication proof comparing heuristics step step fashion assume 
heuristic places operation may force operation scheduled value due input values available time 
fortunately observations show assertion violated 
non decreasing heuristic minimally schedule operation time second jr history tj larger jr list tj list bf scheduler forced place operation time history scheduler 
force list bf scheduler larger corresponding history scheduler possibility smaller values list bf scheduler violate previous arguments violating condition occur induction assertion proved 
back scheduling capability history scheduler allows jr history tj greater jr list tj 
operation back scheduled jr history tj unchanged jr list tj may decrease size 
jr history tj increases size barrier shown created 
jr list tj jr list gammaf tj prove induction 
basis jr list tj jr list gammaf tj holds true resource instances initially available time 
earliest time operation scheduled executed due previous scheduling constraints 
assuming jr list tj jr list gammaf tj assertion jr list tj jr list gammaf tj satisfied 
need examine case jr list tj jr list gammaf tj unaffected 
resource instances list ff scheduler select resource earliest available list bf scheduler may select resource depending better fit exists see 
jr list tj may decrease jr list gammaf tj decrease leaving jr list tj jr list gammaf tj 
consider possibility values list ff scheduler larger list bf scheduler 
event serve increase possibility time list bf rnd rbn possible resource placement scheduling step list bf list ff 
jr list tj grows larger jr list gammaf tj induction assertion proved 
relations presuppositions concerning conclude history list bf list ff examine unsorted selection heuristics 
random easiest handle random selection strategy permits possible schedule constructed 
claim performance order shown random heuristic 
said performs worse optimal scheduler 
proof problem instances constructed random performs worse heuristics case random scheduler selects resource instance step schedule 
hand problem instances constructed heuristics perform correct random sequence random produces optimal schedule 
order shown random heuristics statement proved 
lastly examine performance potential rnd rbn respect scheduling heuristics 
believe examining problem closely rnd rbn perform worse list bf 
numerous attempt generate proof statement counter example failed 
practice rnd rbn generally performs worse list bf 
appendix tetra installation user guide pages installation user guide tetra time writing 
see man page file tetra latest version document 

