high level primitives recursive maximum likelihood estimation bernard levy albert benveniste may proposes high level language constituted small number primitives macros describing recursive maximum likelihood ml estimation algorithms 
language applicable estimation problems involving linear gaussian models processes values finite set 
high level primitives allows development highly modular ml estimation algorithms simple numerical building blocks 
primitives correspond combination different measurements extraction sufficient statistics conversion status variable unknown observed vice versa defined linear gaussian relations specifying mixed deterministic stochastic information system variables 
primitives define macros illustrated deriving new filtering smoothing algorithms linear descriptor systems 
primitives extended finite state processes implement viterbi ml state sequence estimator hidden markov model 
keywords estimation smoothing failure detection descriptor systems belief networks 
research described supported inria national science foundation mip 
performed author visiting researcher institut de recherche en informatique syst emes irisa rennes france 
department electrical computer engineering university california davis ca 
irisa inria campus universitaire de beaulieu rennes cedex france 
institut de recherche en informatique automatique inria domaine de voluceau rocquencourt france 
spite fact kalman filtering relies simple gram schmidt orthogonalization principle years literature devoted kalman filtering smoothing square root fast algorithms implementations relatively complex 
deal numerical conditioning problems possible singularity measurement noise large uncertainties initial state variance number variants basic filtering smoothing algorithms developed 
algorithms superficially similar usually require distinct implementations result duplication programming effort general state confusion user 
need develop modular environment algorithms constructed basic modules underlying simplicity filtering smoothing procedures transparent 
course argue numerical signal processing software packages matlab provide desired environment developing estimation algorithms modular manner 
functions performed matlab low level nature capture statistical aspects operations arising recursive maximum likelihood ml estimation algorithms 
comparison certain real time signal processing languages signal extension random processes called employ small number high level primitives admit simple interpretations terms operations asynchronous data streams 
goal circumscribed signal focus exclusively estimation problems contend timing issues shall retain idea employing just primitives describe recursive estimation algorithms modular format 
objects primitives operate called observations observation describes hard constraints probabilistic relationship existing set measurements unknown variables 
primitives macros construct correspond fact simple statistical operations combination observations extraction sufficient statistics minimal dimension computation marginal conditional densities 
step development estimation primitives taken operations called reduction extraction introduced formulate linear estimation problems graphs 
reduction operation compresses redundant observations extraction effect removing observation certain variables longer interest 
unfortunately construction reduction extraction operations nonprobabilistic focuses exclusively generation ml estimates error variances 
consequence operations accomplish certain tasks generation innovations computation likelihood function observation sequence 
adopt probabilistic point view construction primitives 
statistical concepts employed construct primitives quite general restrict attention gaussian processes 
consider ml estimation problems amenable finite computer programs conditional densities finitely parametrized 
addition gaussian models include processes values finite set finite state markov chains markov random fields 
important feature approach employ formulate recursive ml estimation problems distinction equations describing model dynamics measurements view observations 
motivate viewpoint note step consisting dividing equations describing certain stochastic process subset employed construct process measurement subset totally artificial 
really matters available dynamics measurements estimate variables interest 
raises issues 
concerns fact dynamic measurement equation contains general mixture deterministic probabilistic information system variables 
precisely depending noise enters equation equation specifies deterministic algebraic constraint probability density distribution system variables 
suggests proper probabilistic framework modeling systems consider probability densities distributions defined submanifolds linear subspaces gaussian case underlying domain 
feature appears stochastic processes constructed combining deterministic relations random variable generator 
fact process dynamics measurements progressively constructing process priori engenders second problem 
specifically possible certain variables estimable complete observation set may remain unknown basis current partial observation set 
confronted situation time mixed information system variables known exactly unknown remainder admits probability distribution 
general framework develop combining information contained systems partly random partly deterministic relations presents number similarities dempster schafer theory belief functions artificial intelligence 
particular mechanics employ combine observations propagate information contained systems observations closely related fusion propagation rules developed networks belief functions 
important difference dempster shafer theory relies axiomatic different probability theory rely orthodox probabilistic viewpoint 
viewpoint achieved identifying noise sources models consider assigning probability distribution noise source attempting system variables 
organized follows 
consider case linear gaussian models 
section associate linear gaussian observation pair formed constraint space system parameters measurements gaussian probability density defined space 
criterion determine variable estimable specification pair maximum likelihood estimate obtained 
high level ml estimation primitives introduced section 
primitive performs combination observations 
involves imposing combined observation deterministic constraints associated components 
difficulty arises observations combine contain common parameters measurement vectors 
second third primitive formed reduction operation complement 
statistical point view reduction observation generates minimal sufficient statistic preserving constraint space parameter vector 
extracts observation information concerning measurement vector 
yields likelihood function measurement vector deterministic constraints called parity checks applied measurements determine compatible observation model 
primitive called mutation needed define conditional observations consists switching status parameter measurement unknown known vice versa 
framework extraction operation macro implemented easily terms primitives 
efficient numerical implementations primitives provided primitive corresponds statistical concept numerical module 
primitives macros employed section derive general recursion principle computation ml estimates evaluation likelihood function parity checks coupled observations 
introduce graphical representation displays coupling observations introduced shared variables 
observations vertices graph vertices connected edge share variables 
case graph observations forms tree acyclic graph show recursion principle provides compilation scheme applied observation tree transforms oriented 
resulting oriented tree specifies recursion order observations variables estimated forms complete ml estimation program 
compilation scheme works trees arbitrary graphs systematic procedure transforming graphs trees described 
procedure regrouping observations sparingly tends destroy structure underlying estimation problem 
resulting general ml estimation framework applicable standard kalman filtering smoothing problems large scale systems coupling variables temporal nature spatial subsystems constituting full system 
results illustrated section considering filtering smoothing linear gaussian descriptor systems simple failure detection problem 
high level programs provided kalman filter rauch tung smoother 
programs compact hard believe implement complete kalman filter smoother 
keeping track step step implementation primitive easy relate standard kalman filter smoother algorithms 
case parameter process values finite set considered section 
depending measurements continuous discrete observation modeled density distribution defined constrained set corresponding deterministic constraints imposed parameters measurement 
primitives correspond general statistical operations difficult adapt new model 
difference depending primitives act gaussian finite valued variables rely different numerical implementations 
primitives illustrated considering maximum likelihood state sequence estimation problem hidden markov model 
realizing problem graph dependency structure linear descriptor estimation problem section demonstrate somewhat surprisingly viterbi algorithm admits high level program double sweep rauch tung smoother 
algorithms look quite different primitives sensitive data type allows demonstrate identical precise analogy observed 
estimation constrained spaces observation model consider linear observation ey ax bu zero mean gaussian vector measurement parameter vector respectively known shall say observed unknown 
observation dimension denotes dimension vector ey matrices sizes theta theta theta respectively 
covariance bb noise bu need invertible loss generality assumed full column rank 
observation model nonstandard customary set greater generality afforded model enables combine observations measurement vectors contain common components 
model allows existence algebraic constraints entries context constraint imposed rows worth noting distribution priori distribution noise specification equation 
observation known components may known exactly 
lemma describes precisely information provided equation 
lemma canonical decomposition linear observation form exists transformation invertible orthonormal sa sbv se observation takes form independent 
canonical decomposition provides information belongs subspace defined constraints observation specifies gaussian density exp gamma jj gamma jj jjf yjj constant selected equal require normalized specification observation form equivalent selection gaussian density constraint space pair 
proof need prove direct consequence 
performing qr factorization find orthonormal matrix ta full row rank 
denoting te lb see ey bu sigma orthonormal sigma diagonal positive definite singular value decomposition note block sigma placed position order ensure consistency choice input transformation 
transformation sigma gamma yields block rows orthonormal covariance identity 
implies independent unit variance 
writing words densities differ multiplicative constant viewed equivalent 
enables perform arbitrary linear transformations vectors keeping track jacobians transformations 
account third block row obtain gamma assumption full column rank implies full column rank 
consequently performing qr decomposition find invertible matrix zb multiplying left denoting zf za gives block rows matrix full row rank full row rank invertible 
remarks interesting feature decomposition relies exclusively numerically stable operations qr singular value decompositions 
sense viewed just formalization procedures occur repeatedly different implementation square root kalman filters 
context discussion generalized linear regression models statistics decomposition type appears detailed numerical procedure provided construction 
observation form generation equations requires step procedure employed construct 
specifically indicated matrix rows form basis left null space la equations ley full decomposition procedure lemma needs applied seek compute canonical form 
interesting feature constraints defining second involves 
space identifies observations compatible model test parity check applied observation determine arise model 
context usually desirable compress row space find orthonormal matrix wf full row rank 
parity check test required determine belongs reduces involves independent tests 
furthermore constraint space expressed valid observation vector constrained belong affine space 
space describes deterministic constraints subjected obtained validity established 
affine space parametrized follows 
matrix columns form basis right null space observing gamma particular solution equation vectors admit linear parametrization arbitrary vector 
validity observation established information available presents features density concentrated affine space components known perfectly 
ii components completely unknown reflected fact density uniformly distributed respect variables 
iii remaining components admit gaussian distribution 
confronted situation mixed information various components obviously complete set observations find situation include unknown component 
property described follows 
definition vector said estimable linear parametrization log density gamma ln nonsingular quadratic form vector admits unique maximum integral exists functions grow polynomially jjxjj moments density exist 
setting see estimable marginal density defined space valid observations 
lemma vector estimable observation equation equivalently iff canonical decomposition matrix invertible 
proof substituting parametrization inside density yields gamma ln jjf gamma gamma jj jjf yjj shows gamma ln nonsingular quadratic form iff vector null space belongs null space means invertible note construction 
related transformation equivalent requiring 
expression defines marginal density estimable 
observation form obtain estimable parametrization factoring nm 
mx observation rewritten ey nz bu estimable 
find parametrization allows evaluation marginal density 
particular coordinate system select estimable parameter vector integrating density exp gamma jjf gamma jj jjf yjj affine space gives exp gamma jjf yjj defined pair decomposed pairs specifies marginal density constraint space measurement density called likelihood function observation denoted 
context clear shall denote constraint space associated space parity checks applied determine validity 
ii conditional density constraint space pair xjy coordinate system xjy exp gamma jjf gamma xjj major difference observation models nonsingular noise keep track constraint spaces usual densities conditional densities 
ml estimates observation form estimable ml estimate defined argument maximum xjy equivalently 
maximum likelihood estimation problem linear models constraints examined statisticians estimator proposed aitken see pp 
finding ml estimate measurement hx constraint cx estimator requires estimable full column rank full row rank 
shown lemma arbitrary observation form brought form full rank conditions need satisfied estimator described directly applicable 
construct ml estimate follows 
theorem estimable ml estimate expression equality holds modulo multiplication constant independent likelihood function obtained computing marginal density substituting ml estimate inside joint density 
proof need modify slightly usual derivation gaussian densities unconstrained spaces 
loss generality assumed brought canonical form estimable invertible 
denoting gamma space parametrized yields jy exp gamma jjf gamma jj maximum reached jy holds 
note case gaussian observations nonsingular noise ml squares ls estimates shown 
primitives macros high level language employ specify ml estimation algorithms relies primitives combinator reduction mutation functions described 
primitives generate macros conditioning extraction operations written extend observations usual concepts conditional marginal densities 
motivation introducing macros relying exclusively basic primitives subprograms programming language simplifies writing verification large programs 
note language described richer earlier version contained commands somewhat limited compute 
specifically generate ml estimates provide mechanism computing likelihood functions parity checks conditioning respect set variables 
combinator primitive consider combination observations symbol wedge interpreted logical connector 
definition consider observations noises independent denote respectively measurement parameter vector components common observations 
combined observation conceptually combination just requires juxtaposition equations specifying important feature combinator observations may share common measurements unknown vectors noises remain disjoint independent 
noises private variables observation 
advantage convention observations combined combined noise assigned density 
combination observations require specification information individual observation 
modelling purposes means source randomness dynamical system accounted single observation avoid correlation observations 
combinator clearly commutative associative 
constraint space obtained conjunction constraints defining concern respectively vectors density density associated observation 
note scheme employed combining observations identical called product intersection rule developed dempster combining belief functions 
represents product part rule conjunction deterministic contraints defining forms intersection part 
reduction consider observation form 
typically contains information unknown vector measurement vector basic idea reduction split reduced containing information component information provides useful data validity likelihood measurement vector ability compress observation lower dimensional losing information captured concept sufficient observation extends observation model usual notion sufficient statistic 
definition sufficient observation observation say key kax sufficient observation affine constraint space space xjy xjy xjy xjy denote respectively conditional densities see corresponding densities easy verify condition equivalent requiring independent fact factorization holds affine space takes usual form neyman fisher factorization criterion existence sufficient statistic 
typically situation reduce extracting sufficient observation decomposed appear clearly sufficient observation referring decomposition observation see blocks contain blocks form sufficient observation choice information irrelevant gets thrown away sufficient observations corresponding observation need size 
smallest dimension characterized extending observations concept minimal sufficient statistic 
definition consider observation dimension ey dimension minimal admit sufficient observation lower dimension 
lemma observation minimal matrix full row rank equivalently likelihood function parity space satisfy observation vector unconstrained uniformly distributed 
proof loss generality assumed form representation conclude full row rank iff canonical decomposition include components 
representations equivalent 
conversely suppose full row rank key kax sufficient statistic loss generality assume canonical form requires selecting different coordinate systems represent noise vector case ke ka full row rank canonical decomposition include components 
sufficient observation satisfies conditions 
observing full row rank observations unconstrained condition implies right null spaces gammam gammam identical exists invertible matrix gammam gammam account expression conditional densities condition implies jj gammam jj jj gammam jj vectors belonging right null space matrices denotes additive factor depending consequently exist matrices invertible gammam gammam gammam gammam invertible implies observation dimension greater equal minimal 
identity indicates minimal sufficient observations necessarily related left multiplication invertible matrix due fact minimal canonical decomposition include blocks 
gain intuition concept minimal observation consider minimal observation form assumed estimable 
full row column rank invertible xml gamma ey ml estimate loss information gamma yields xml gu gamma gg error covariance ml estimate 
case minimal observation represents just way coding ml estimate error covariance including extraneous information 
generally estimable minimal observation represents just code ml estimate error variance estimable part ax 
identities show minimal sufficient observation constructed contains useful information constraint space likelihood function original observation reduction primitives described provide mechanism constructing minimal sufficient observations time keeping track constraint space likelihood function definition reduction observation canonical form reduced observation complement combining lemmas obtain result justifies choice reduction primitives 
theorem minimal sufficient observation constraint space likelihood function satisfy reduction operation completely decouples ml estimation problem computing constraint space likelihood function minimal observation contains information necessary ml estimation contains information concerning constraint space likelihood function statistics literature called maximum ancillary statistic statistics contained contain information largest possible dimension 
note primitives rf rf represented different symbols implemented decomposition algorithm lemma 
suggested proof lemma generated implementing phase full decomposition specifically need find basis left null space say performing qr decomposition 

hand compute apply full decomposition procedure lemma 
mutation macros addition primitives rf rf defined need introduce fourth primitive called mutation effect change status vector inside observation known unknown vice versa 
mutation useful formulate concept conditional observation certain vectors need treated temporarily known unknown 
definition observation form ey bu observation fog gammaa bu obtained changing status vector unknown known said mutation inside similarly ax bu observation fo gammae bu generated changing status known unknown constitutes mutation inside mutation just requires checking status observed unknown variable changing accordingly corresponding columns interesting feature operation argument dependent sense fog corresponds different operations depending observed unknown consequence consecutive mutations vector yield original observation 
consider observation form suppose longer interested vector just seek extract information contained 
amounts computing marginal density constraint space pair associated referring property rf see provides mechanism extracting marginal density constraint space associated measurement vector want include vector marginal computation need change status view known vector applying rf primitive 
rf applied course convert back original unknown status 
motivates construction macros expressed entirely terms primitives introduced 
definition conditional observation extraction observation form conditional observation extraction defined respectively fog fog ffi fog ffi denotes composition maps 
account comment preceding see matrix rows form basis left null space fog expressed fog ley la context statistical models containing mixed deterministic stochastic information macros fog perform respectively computation marginal conditional densities constraint spaces 
properties primitives macros subsection describes useful properties primitives macros introduced 
formulas involving single observation 
definitions reduction extraction conditioning operations imply arbitrary observation decomposed fog fo assume form 
extracted observation fog properties 
theorem observation form ml estimates estimable part fog 
extraction operation preserves information contained concerning measurement vector sense fog particular fog fog proof consider decomposition definition minimal observation lemma marginal density respect pair constant impose constraint 
applying mutation operation affect joint density constraint space 
decomposition fog fo observation second component provide information proves statement theorem 
just encodes constraint space likelihood function applying decomposition fo ignore second observation 
proves formula 
just consequence 
move properties involving observations 
note arbitrary vector obvious identity fo fo fo observe presence coupling parameter vector expression effect complicating properties combinator hand presence coupling measurement causes particular trouble seen 
interest characterize case observations behave contain coupling vector turns property captured notion innovation extends observations familiar concept innovations process filtering theory 
definition innovation independence consider observations zero mean independent gaussian vectors unit covariance 
say innovation xc fo rfo furthermore coupling vector said independent 
innovation contain information extracted improve knowledge 
lemma indicates innovation eliminate coupling observations independent 
lemma innovation exists matrix rewritten terms new vector tx independent observation 
proof appear xc fo expression extraction operation indicates basis left null space la left null space consequently column space contained column space find matrix substituting relation inside defining gives 
motivate concept innovation consider state space model dynamics gamma gamma zero mean white gaussian noise unit intensity 
innovation due fact admit coupling vector dimension equal size new state contains just information specify residual information improve knowledge general presence coupling vector reduction rfo likelihood function constraint space expressed purely terms rfo 
innovation vice versa expressions 
lemma innovation combined observation satisfies rfo rfo rfo rfo rfo rfo particular denotes necessarily direct sum vector spaces 
furthermore coupling measurement sum direct 
proof select coordinate system coupling vector observe canonical decomposition observation lemma depends matrix pair 
noises independent coupling vector canonical decomposition obtained reducing separately forming rfo rfo rfo rfo justifies 
representations gives 
property expressed terms parity spaces written equivalently terms constraint spaces furthermore observations contain common measurement component parity check tests concern different measurement vectors sum replaced direct sum 
corollary lemma result 
corollary innovation vice versa unknown vector fo fo fo recursive ml estimation compiler recursion principle disposal tools necessary develop recursive algorithms computing ml estimates evaluate constraint set likelihood function family observations 
approach consists estimating vectors order arise observations discarding vectors processing new observations longer appear 
constraint space obtained juxtaposition old constraints new constraints latest observations 
obtained multiplying old likelihood function term reflecting new information innovation contained measurement 
results rely recursion scheme described 
theorem consider observations structure 
fo xc fo rfo xc fo rfo turn implies xc fo rfo xc fo rfo sum replaced direct sum contain common measurement proof derive 
decomposing observing rfo rfo independent property gives fo ffi rfo appear ffi rfo rfo apply yields fo rfx rfo gg rfo decomposed xc fo xc fo xc fo innovation xc fo xc fo convey information applying reduction operation sides noting xc fo innovation observation xc fo apply 
gives xc fo xc fo jx gg xc fo xc fo jx xc fo combining obtain 
prove 
yields fo ffi rfo xc fo rfo fo rfo observations xc fo rfo independent find rfo xc fo rfo graphical representation recursion scheme admits simple graphical representation displays statistical dependence existing observations presence coupling variables 
consider observations shared unknowns unknowns specific observation denoted respectively 
write innovation mutual innovations innovation 
intuitively graphical notation indicates sufficient estimate means want estimate need full combined observation notation just combination directed edges type going opposite directions 
representing separate edges convenient employ bidirectional edge 
note convention adopt edge orientation notion innovation graphical representation systems observations similar usually employed statistics describe markov random fields generally systems coupled regression models 
specifically graphs branches form duals markov random fields graphs switch notation exchanging edges vertices 
theory belief networks artificial intelligence relies directed graph representations 
important difference model theory belief networks edge orientations represent subjective information cause creates effect probability 
causal models popular medicine econometrics validity questionable need distinguish cause effect 
contrast edge orientation scheme employ completely objective relies examination information content observation pairs 
edge orientations model causality relations represent computational device employed allow recursive evaluation ml estimates likelihood functions parity checks 
general arbitrary observations form fully coupled combined system admits graphical representation 
consider decomposition observation reduced parts 
share variables mutual innovations represented 
similarly decomposition fog observation form fog representations give graphical illustration recursion scheme section start observations form graph 
sequence manipulations transforming elementary nondirected tree directed 
apply decomposition extracting information 
gives xc fo ii combine observations xc fo extract resulting observation obtain fo xc fo gg 
xc fo iii decompose fo xc fo gg reduced components recall fo xc fo gg rfo yields fo xc fo gg 
xc fo estimate estimate xc yx rfo likelihood parity checks estimate steps perform incremental transformations tree just chain case going right left 
phase viewed tree compilation effect orient tree bring form amenable recursive estimation 
indicated recursive estimation phase requires moving tree left right estimating sequentially successive estimate generate 
compilation observation trees compilation procedure described applies elementary trees composed just observations single edge 
graph obtained considering family observations fo ig linking pair observations edge share variables significantly complex 
particular may contain cycles 
particular case graph contain cycles forms tree compilation procedure previous subsection extended follows 
observe interaction graph system observations fo ig forms tree nodes connected unique path 
property induces distance elements index set path linking branches 
select arbitrary node root tree 
partial order defined elements distance write oe node closer root node furthermore find total order elements compatible partial order oe 
achieved hanging tree upside root scanning nodes tree sequentially proceeding top bottom left right nodes located level tree 
compilation changes observations labeling nodes tree changing tree topology refer nodes index label start compilation 
compilation works follows 
process nodes reverse total order 
means start nodes located greatest distance root node move progressively root node 
algorithm terminate root node reached 
suppose current stage algorithm consider node current observation label parent node observation label 
node immediate neighbor unique path linking root node observation pair fo defines elementary tree nodes edge type considered previous subsection 
ci coupling vector 
applying compilation previous subsection elementary tree ci transformed directed tree ci ci fo ci terminates processing node move predecessor list generated 
proceeding manner see compilation transforms nondirected interaction tree observations fo ig directed node needs considered computational complexity compiler proportional cardinality arbitrary graphs compilation procedure previous section applies case interaction graph observations fo ig tree contain cycles 
topology occurs naturally systems evolving time reason believe feedback occur past hierarchical systems agents contact single supervisor subordinates rules exist prevent agent communicating information jumping head supervisor 
arbitrary large scale systems tend complicated structure strict hierarchical models interaction graph describing statistical dependence exists time space components system contains cycles 
order apply compilation procedure previous section graph find mechanism transform tree 
procedure explored consists combining observations larger observation groupings 
aggregations tendency destroy underlying information structure ml estimation problem consider 
combine observations produce interaction graph reduces single node 
transforming interaction graph observations fo ig tree strive keep aggregations minimum 
addition nice structure resulting tree depend order observations 
elegant solution problem proposed lauritzen spiegelhalter see chapter context study graphical models statistics artificial intelligence 
relies observation graph triangulated contain cycle hypergraph formed maximum cliques acyclic 
hypergraph consists set set subsets 
cliques graph sets mutual neighbors clique maximal contained clique 
shown acyclicity hypergraph maximum cliques give tree structure maximum cliques allows combine observations belonging maximum cliques way interaction graph aggregated observations forms tree 
procedure works interaction graph observations fo ig triangulated graph triangulated triangulate adding false edges 
new edges selected judiciously different strategies may lead triangulated graphs different numbers cliques cliques significantly different sizes 
strategy dealing interaction graphs containing cycles proposed fabre 
consists breaking cycles interaction graph identifying variables knowledge convert observation graph tree 
variables estimated separately parametrize recursive estimation algorithms remaining variables 
observe results obtained convert graph observations tree orienting resulting tree applicable wide class decentralized systems provided systems tightly coupled 
aggregation examples 
filtering smoothing descriptor systems application failure detection illustrate recursion method section solve filtering smoothing problems linear descriptor systems apply results failure detection problem 
linear descriptor systems admit dynamics form gammae observations gamma white gaussian noise 
case corresponds usual state space models need invertible general 
fact matrices required square size state change presence deterministic information reflected fact dynamics may contain fixed algebraic constraints entries covariance observation noise may invertible resulting singular estimation problem 
filtering problem descriptor systems examined 
case estimable past observations explicit recursions optimum filter associated riccati equation contains analysis state convergence optimum filter 
square root implementations descriptor kalman filter proposed 
case estimable past observations may estimable past observations combined considered filtering smoothing formulas expressed time terms reduction extraction primitives employed 
probabilistic interpretation primitives available solution filtering problem include generation innovations process parity checks evaluating model validity function received observations 
computation innovations parity checks new aspects descriptor filtering algorithm prove useful study failure detection problem considered subsection double sweep descriptor smoother obtained new 
considering system distinction dynamics measurements view linear relations relating different random variables observations variables 
point view convenient combine dynamics measurement equations single observation form gammae gamma 
information available initial final states assumed separable form gammae gamma gamma xn bn un graph observations illustrate information contain vectors vertices observations successive observations coupled variables graph corresponding descriptor system admits linear tree structure shown fig 

tree compilation method section applicable 
depending consider filtering smoothing problems different observation subsets need considered 
note addition early attempt giving graphical interpretation kalman filter language belief functions 
xn linear tree 
filtering observation obtained combining past observations time filtered observation describing information contained constraint space likelihood function denoted respectively kjk fp sfp applying recursion principle theorem obtain kalman filtering recursions jk fo kjk fo kjk phi fo kjk initial conditions rfo rfo rfo phi denotes direct sum vector spaces 
recursions obviously constitute unusual form kalman filter totally expressed terms primitives macros introduced section 
interpret expressions obtained note definition rf observation fo kjk decomposed 
vectors correspond respectively innovations parity checks model respect vectors recursions take form fy phi exp gamma jj jj nonsingular filtering problems log likelihood function gamma ln obtained summing squared innovations 
interpretation algorithm quite compact constitutes complete implementation kalman filter 
interpretation purposes transcribed form closer usual kalman filter assume estimable gammae full rank case kjk written kjk kjk sigma kjk kjk sigma kjk denote respectively ml estimate error covariance matrix independent model noise arbitary symmetric nonnegative definite matrix sigma sigma denotes square roots 
kalman filtering algorithm broken steps 
time update consider kjk takes form kjk gammae sigma kjk predicted observation jk fo kjk obtained premultiplying gammaa yields jk gammaa kjk gammae gamma sigma kjk extractions change observation constraint spaces likelihood functions jk jk jk jk ii measurement update consider observation jk gammaa kjk gammae gammaa sigma kjk transformation matrix needed bring observation canonical form partitioned notation reduced observation jk rfo jk takes form gammaa kjk 
invertible left multiplication gamma obtain jk jk sigma jk jk gamma gammaa kjk sigma jk similarly rfo jk gammaa kjk innovations parity check vector linear combinations kjk note discussion merely provides step step account operations appearing kalman filtering algorithm 
goal ultimately hide implementation details type focusing larger picture provided primitives macros introduced 
double sweep smoother high level implementation filter smoother descriptor systems 
implementation exclusively reduction extraction operations 
derive high level double sweep rauch tung rts smoother 
smoother entirely new smoothing algorithm descriptor systems high level program underlying smoother 
new ingredient missing involves conditional observations 
denote combination observations system 
smoothed observation fog describes information contained observations 
describe double sweep smoother introduce conditional observation fog representing information contained observations conditioned knowledge consider decomposition combination observations 
noting contribute information appear observation get kjk expression represents forward sweep rts smoother 
implemented concurrently forward kalman recursion 
note effect transforming linear tree fig 
directed shown fig 

forward sweep rts smoother just implements tree compilation procedure described section 
gamma xn directed tree generated forward rts sweep 
get smoothed observation take account decomposition fog 
innovation view definition smoothed observation implies fm 
recursion constitutes backward sweep rts smoother 
interpretation gain better understanding rts recursions derive semi explicit form smoother relies finding closed form implementations primitives appearing recursions 
closed form realizations primitives tendency reliable numerically implicit implementations earlier 
explicit form rts smoother illustrative purposes 
forward sweep restrictive assumption estimable addition assume noises appearing dynamics measurements uncorrelated 
case contribute information kjk kjk 
premultiplying gammaa applying orthonormal transformation vector selected sigma kjk gammaa sigma kjk conditional observation expressed theta kjk theta gamma gammah gamma 
interpret matrices theta appearing note multiplying transpose find sigma kjk sigma jk gamma sigma kjk sigma kjk gamma sigma kjk sigma jk gamma sigma kjk sigma expressions assumed matrix invertible 
square root covariance matrix sigma jk noise gamma sigma kjk predicted observation jk invertible sigma jk positive definite 
substituting inside expressions theta gives theta gamma sigma kjk sigma jk gamma sigma kjk sigma jk gamma see sigma filtered error variance observation past observations pinned value backward sweep mutation inside accomplished moving term right hand side observation 
takes form theta kjk gammar sigma sigma denote respectively smoothed estimate associated error covariance matrix 
extract observation need gives sigma theta kjk sigma sigma sigma application failure detection descriptor kalman filtering recursions section directly applicable proposed failure detection procedure dynamical systems containing nuisance parameters quantities arbitary unknown values 
standard state space model nuisance term appears additively dynamics nuisance removed premultiplying dynamics matrix nulls nuisance space 
yields system descriptor dynamics presence absence failures determined applying innovations statistical failure detection tests algebraic tests relying parity checks 
kalman filtering recursions generate innovations parity checks provide data necessary implement failure detection schemes 
objective stage discuss failure detection issues detail just illustrate applicability descriptor kalman filtering algorithms 
accordingly follow formulation reader referred extensive discussions comprehensive bibliography :10.1.1.22.6332
starting point formulate standard binary hypothesis testing problem linear observation models terms primitives introduced earlier 
measurement vector models ey bu noise ey ax bu signal noise seek decide best fits decision requires checking constraints satisfied satisfied ii computing log likelihood ratio log gamma log comparison selected threshold 
performing decision reduces computing careful inspection canonical decomposition algorithm lemma reveals numerical procedure required compute yields time 
suppose consider problem unknown nuisance vector need decide ey gz bu ey ax gz bu resulting problem choosing hypotheses presence unknown regression vector discussed section 
case need extract observations effect removing nuisance term gz apply previous hypothesis testing procedure extracted observations fh requires computation left null space single algorithm generate fh 
examine failure detection problem isolation studied 
system ax bv gammau fm cx dv state system known input observed output white gaussian noise sequence 
vector represents nuisance modelling unobserved input output disturbances possible alternative failures considered 
term fm represents actuator failure seek detect isolate 
simplicity assumed matrices system dynamics constant appropriate dimensions 
model absence failure corresponds case presence failure known vector 
cast model framework rewrite observation gamma gammau gammai indices appearing correspond respectively time index hypothesis 
absence failure failure case 
onset failure corresponds jumping certain time seek detect jump estimate failure time accomplished different manners 
generalized likelihood ratio glr algorithm prior nuisance rejection see section 
algorithm proceeds follows 
nuisance rejection 
compute fh matrices depend choice hypothesis time index extraction needs performed see previous comment concerning binary hypothesis problem nuisance rejection 
computation generalized likelihood functions different hypothesis kalman filtering 
apply filtering recursions observation stream gamma fh gamma fh models situation failure occured time case corresponds absence failure 
verify measurement vectors belong constraint space sfo compute max arg max properly chosen window length likelihood ratio exceeds properly selected threshold failure occuring time detected 
note system consideration interconnection local subsystems nuisance affects subsystem nuisance rejection step performed locally 
analytical redundancy approach alternatively described section consider state vectors reject 
approach state eliminated estimated kalman filtering techniques 
procedure works follows 
possible failure times compute xm delta delta delta gamma gamma note case states need eliminated appear consecutive observations gamma global extraction implemented separately extracting coupling observations induced variables eliminated local extraction performed recursively equivalent computation parity checks described eqn 

resulting system involves measurements noises unknown failure parameters apply descriptor kalman filter recursions systems obtained step 
yields likelihood ratio failure parameters corresponding estimated failure time side note discussion demonstrates equivalence glr analytical redundancy approaches shows yield result handle state differently 
detailed discussion connection approaches conventional perspective appears section 
finite state processes previous results extended nonlinear nongaussian case parameter vector estimated belongs finite set observation model observation consists pair formed set defining allowable pairs measurements parameter vector depending measurement vector takes discrete continuous values probability distribution density ds dv dv represents element volume set measurements compatible constraint set pi denote respectively domain projection operator associates pair coordinate pi set valid outputs pi measure dv defined left unspecified depends set represents set parameter vectors pair belongs finite set shall focus interest exclusively ml estimates 
say estimable admits unique maximum case ml estimate defined xml arg max corresponding likelihood conditional likelihood functions xml max xjy defined respectively 
gaussian case decomposed pair xjy describing marginal conditional constraint sets likelihood functions 
denoted indicate association gaussian case likelihood conditional likelihood functions xjy coincide marginal conditional probabilities xjy differ fact summing maximizing primitives primitives introduced section linear gaussian observations generalized follows 
consider observations parameter vectors decomposed attributes specific observations component common 
similarly measurements vectors contain parts specific observations common part adopting notation measurement parameter vectors observation combination combined observation formed pair probability distributions densities considering nongaussian finite valued parameter vectors xjy minimal sufficient statistics estimating base reduction operation rf need extract minimal sufficient observation indicated important property reduction operation rf gaussian case retains information output sense likelihood function uniformly distributed 
conditional likelihood function xjy precisely characteristic max xjy implies primitives rf rf defined xjy mutation corresponds change status parameter measurement unknown known vice versa conditional observation extraction macros defined observation likelihood likelihood extracted observation fog max concepts expanded full model hybrid stochastic deterministic systems finite number states 
model called css calculus stochastic systems allows implementation language incremental simulation estimation hybrid systems 
estimation hidden markov models illustrate application primitives consider problem finding state sequence hidden markov model hmm 
models states markov chain observed directly need estimated sequence measurements depending indirectly states 
models type speech processing digital communications decoding convolutional codes deconvolution intersymbol interference 
detailed study hmms control perspective see 
markov chain defined values finite set joint probability distribution chain expressed gamma form slight variant standard expression markov processes employed model markov processes information initial final state available 
situation occurs example convolutional encoder transmit data blocks separated zero bits role ensure encoder starts terminates known state zero state 
case ffi ffi denotes distribution equal zero 
observations gamma may take continuous discrete values 
complete state trajectory modeled conditional density distribution jx gamma oe form indicates may measurement state transition simplicity assume constraints states observations observation consist probability distribution density 
joint distribution density measurements states expressed jx oe gamma maximum likelihood sequence estimation problem hmms consists jointly maximizing respect variables best known solution problem viterbi algorithm 
relate algorithm discussed section descriptor systems note successive observations coupled variables graph corresponding hmm coincides linear tree fig 

words hmms descriptor models graph structure 
structure determines completely order observations processed variables estimated immediately conclude filtering smoothing algorithms section directly applicable hmms 
primitives implemented way interest examine precisely internal mechanics outcome algorithm 
forward pass combination observations past time filtered observation kjk corresponds function measuring likelihood best path terminating state measurements fy gamma 
definition kjk includes reduction likelihood normalized maximum value 
similar normalization performed convolutional decoders subtracting periodically constant value function gamma ln prevent growing excessively 
possible interested relative value different 
consider recursion extracted observation fo kjk specified distribution max oe argument maximum dependence observations time suppressed 
state markov chain time represents state time observations collected time function called back pointer function current state time points state previous instant 
fo kjk max furthermore denotes likelihood function observation satisfies recursion evaluate likelihood function measurement sequence 
forward filter initialized max viterbi double sweep decoder represents combined observation smoothed observation likelihood obtained maximizing respect maximum arg max represents state time trajectory corresponding measurements gamma goes time definition conditional observation macro find observation kjk admits likelihood oe fog distribution oe joint maximum gives states times 
maximizing noting dependent part identical minimized yields trajectory obtained double sweep algorithm forward sweep computes trace back function backward sweep generates optimum state trajectory starting final state xn state computed observing observation jn distribution maximum yields xn employing primitives sensitive data type act differently gaussian finite state processes able show viterbi rauch tung double sweep algorithms identical noted informally 
developed set high level primitives ml estimation rely general statistical concepts applicable estimation problems sufficient statistics admit finite parametrizations linear gaussian models finite state processes 
graphical representation interaction observations develop compilation scheme case interaction graph observations forms tree allows automatic generation recursive ml estimation program tree observations 
high level primitives recursion method illustrated considering ml estimation problems linear descriptor systems hidden markov models 
extended directions 
finite state case assumed observations modeled distribution density constraint set pair 
effect events concerning state observation certain applications networks stochastic petri nets set events restricted 
sophisticated approach set hidden random variables described 
addition restricted attention ml estimation problems simulation hybrid random deterministic systems bayesian framework appropriate procedure transposing ml operations bayesian case 
transformation effect illustrating parallel existing simulation estimation hybrid systems 
duality control estimation come surprise high level primitives applicable control problems 
preliminary results high level formulation linear exponential quadratic gaussian control described 
approach employed consists viewing exponential cost minimized probability density system artificial observations 
solution derived entirely estimation perspective distinction estimation control optimal control just obtained ml estimate system inputs 
difficulty arising context quadratic form appearing exponential cost may indefinite ml estimation problem associated cost needs defined space see study linear filtering spaces 
topic need study concerns flexibility existing choice recursive strategies solving ml estimation problems specified coupled observations 
smoothing problem descriptor systems filter double sweep smoothers possible processing strategies 
choice processing algorithms wider general observation graphs trees 
raises question developing systematic methods determining optimum algorithm terms memory requirements computational cost 
results lines described 
authors drs 
mich ele basseville eric fabre irisa prof alan willsky mit suggestions conversations concerning contents presentation 
sorenson kalman filtering theory application 
new york ieee press 
anderson moore optimal filtering 
englewood cliffs nj prentice hall 
bierman factorization methods discrete sequential estimation 
new york academic press 
kailath lectures wiener kalman filtering 
berlin springer verlag 
le guernic gauthier le le programming real time applications signal proc 
ieee vol 
pp 
sep 
benveniste constructive probability language building handling random processes programming tech 
rep institut national de recherche en informatique automatique rocquencourt france oct 
taylor levy willsky graph structure recursive estimation noisy linear relations tech 
rep institut national de recherche en informatique automatique rocquencourt france may 
appear math 
systems estimation control 
rabiner juang hidden markov models ieee assp magazine vol 
pp 
jan 
elliott moore hidden markov models estimation control 
new york springer verlag 
kindermann snell markov random fields applications 
providence ri american mathematical society 
fort stochastic processes lattice gibbs measure 
boston ma kluwer acad 
publ 
dubes jain random field models image analysis applied statistics vol 
pp 

dempster generalization bayesian inference discussion royal stat 
soc series vol 
pp 

shafer mathematical theory evidence 
princeton nj princeton univ press 
paris uncertain reasoner companion mathematical perspective 
great britain cambridge univ press 
pearl fusion propagation structuring belief networks artificial intelligence vol 
pp 
sep 
peot shachter fusion propagation multiple observations belief networks artificial intelligence vol 
pp 

lauritzen spiegelhalter local computations probabilities graphical structures application expert systems discussion royal stat 
soc series vol 
pp 

remarks linear nonlinear filtering ieee trans 
inform 
theory vol 
pp 
jan 
rao inference linear models fixed effects results problems statistics appraisal david david eds pp 
ames ia iowa state univ press 
linear analysis 
new york wiley 
levy benveniste high level primitives recursive maximum likelihood estimation tech 
rep institut national de recherche en informatique automatique rocquencourt france oct 
lehmann theory point estimation 
pacific grove ca wadsworth brooks cole 
whittaker graphical models applied multivariate statistics 
new york wiley 
harary graph theory 
reading ma addison wesley 
berge graphs hypergraphs 
amsterdam north holland 
fabre multiscale signal processing 
phd thesis universit de rennes france dec 
wang bernhard des syst emes tech 
rep institut national de recherche en informatique automatique rocquencourt france aug 
willsky levy kalman filtering riccati equations descriptor sytems ieee trans 
automat 
contr vol 
pp 
sep 
square root kalman filtering descriptor systems systems control lett vol 
pp 
oct 
taylor willsky maximum likelihood estimation point descriptor systems proc 
conf 
information sciences systems march 
dempster normal belief functions kalman filter may 
dept statistics harvard university 
rauch tung maximum likelihood estimates linear systems aiaa journal vol 
pp 
aug 
benveniste basseville willsky descriptor systems failure detection isolation proc 
ifac world congress sidney australia pp 
july 
innovations generation presence unknown inputs application robust failure detection automatica vol 
pp 
dec 
basseville detection abrupt changes theory applications 
englewood cliffs nj prentice hall 
benveniste levy fabre le guernic calculus stochastic systems specification simulation hidden state estimation tech 
rep institut de recherche en informatique syst emes rennes france july 
appear theoretical computer science 
forney viterbi algorithm proc 
ieee vol 
pp 
march 
forney maximum likelihood sequence estimation digital sequences presence intersymbol interferences ieee trans 
inform 
theory vol 
pp 
may 
viterbi principles digital communication coding 
new york mcgraw hill 
reciprocal processes verw 
gebiete vol 
pp 

viterbi error bounds convolutional codes asymptotic optimum decoding algorithm ieee trans 
inform 
theory vol 
pp 
apr 
levy control formulated recursive estimation problem third ieee mediterranean symposium control automation cyprus july 
hassibi kailath recursive linear estimation spaces 
parts ii proc 
nd ieee conf 
decision control san antonio tx pp 
dec 

