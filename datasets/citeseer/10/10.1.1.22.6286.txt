newsweeder learning filter netnews appear ml ken lang school computer science carnegie mellon university forbes avenue pittsburgh pa akl cs cmu edu significant problem information filtering systems dependence user creation maintenance user profile describes user interests 
newsweeder netnews filtering system addresses problem letting user rate interest level article read learning user profile ratings 
describes newsweeder accomplishes task examines alternative learning methods 
results show learning algorithm minimum description length mdl principle able raise percentage interesting articles shown users average 
performance significantly outperformed successful techniques information retrieval ir inverse document frequency tf idf weighting 
number participants usenet continues multiply spectrum topics covered 
users find increasingly difficult locate useful interesting information diversity expands 
tradeoff user searching fewer articles finding information personal interest 
ideally allow user selectively choose tradeoff increasing scope search cause marginal probability additional articles interesting fall personal threshold 
reality tradeoff usually subscribing small set newsgroups simple keyword matching profiles designed user 
research systems explored extend idea allowing complex rule keyword matching profiles designed user average netnews reader probably willing able build sufficiently complex ones 
consequently efforts profiles automatically learned fischer sheth 
systems focus looking article text determine relevance known content filtering 
systems resnick goldberg focused ratings early readers article predict readers ratings 
known collaborative filtering 
system described newsweeder uses content collaborative filtering 
sufficient data multiple users available focus content part profile learned 
newsweeder currently operates procedure 
user wants read netnews follows link newsweeder world wide web interface 
chooses topic newsgroup netnews articles wishes read typically done 
addition traditional newsgroup hierarchy user newsweeder virtual newsgroups 
example user bob go virtual newsgroup nw top bob see newsweeder personalized list top articles learned preferences bob 
list line article summaries sorted predicted rating 
user selects group articles summaries reads sequentially 
article read user clicks rating replaces need indicate article required 
newsweeder collects user ratings active feedback user interests 
preferable research system passive feedback methods measuring time spent reading performance accurately evaluated 
training signal probably stronger active feedback better performance 
drawback active feedback extra effort user spend decide click rating practice effort appears acceptably minimal 
night system uses collected rating information learn new model user interests 
rating data collected line learning experiments gives evidence line performance comparisons learning methods 
line experiments subject 
approach experiments performed newsweeder cover small set points large space possible filtering methods 
describe space map choices designing text filter 
describe choices newsweeder reasoning 
representation newsweeder raw text parsed generalized words called tokens 
tokens include punctuation specialized symbols engineered structure article headers 
example addition typical words seminar counting tokens punctuation mark symbol newsgroup comp ai tokens 
noun phrases tokens proven evans useful 
kept granularity feature size word level avoid linguistic knowledge noun phrases larger structures usually entail building 
newsweeder creates vector token counts document 
vector size total vocabulary zeros tokens occurring document 
type vector called bag words model basis representation 
bag words model capture order tokens document necessary linguistic syntactic analysis assume captures information needed filtering purposes 
step common ir systems group tokens common linguistic roots procedure called stemming 
newsweeder tokens left unstemmed form 
better performance probably achieved stemming short term due larger statistical samples creates long term research goal waste extra information contained exact tokens 
plan eventually try extra information looking larger size samples unrated text get reliable statistics rare words 
technique attempt large amounts available unrated text unsupervised learning approach bias supervised learning smaller set rated articles 
evidence yang stemming hurt performance approach able strong statistical inferences unstemmed tokens 
learning methods text processed standardized vector choice learning method apply 
vector order tokens long care taken avoid pitfalls learning high dimensional input spaces 
curse dimensionality learning difficult examples needed determine dimensions important space examples look equidistant 
way combat high dimensional space reduce dimensionality applying techniques 
example singular value decomposition svd dumais auto encoding neural nets attempt reduce size space maximally retaining information contained original representation 
initial newsweeder experiments done dimensionality reduction techniques throwing words deemed heuristic described 
earlier experiments lang task predicting newsgroup text body indicated techniques svd outperformed techniques transformation 
result probably caused loss information transformation 
possible learning methods available chosen fit characteristics domain 
tested popular technique ir called term frequency inverse document frequency weighting tf idf salton 
stanford netnews filtering service yan relies technique efforts focused solving delivery indexing problems large numbers users trying develop better learning models 
tf idf provides benchmark learning models compared 
tf idf weighting assumptions tf idf empirical observations regarding text 
times token appears document called term frequency tf relevant topic second times occurs documents called document frequency df poorly discriminates documents 
document terms combined weights multiplying tf inverse df token 
logarithm tf idf taken order de emphasize increases weight larger values 
experiments newsweeder weight token document tf df log entire set documents way tf idf vectors compared takes advantage domain 
documents usually contain small fraction total vocabulary significance word appearing greater appearing 
emphasize stronger information content word appearing cosine angle vectors measure similarity 
effect metric seen example 
suppose documents contain single word words different 
similarity documents zero cosine angle perpendicular vectors zero 
unbiased learning technique take advantage domain feature usually group documents similar elements lengthy vectors agreed zero 
tf idf cosine similarity metric ways classify documents categories 
example family nearest neighbor techniques 
newsweeder documents category converted tf idf vectors normalized unit length average taken get prototype vector category 
advantages doing speed computation compact representation 
classify new document document compared prototype vector predicted rating cosine similarities category rating 
step convert results categorization procedure value linear regression describe detail subsequently 
simply lumped highly rated categories single similarity measure provide rating prediction felt fully take advantage gradations relevance feedback obtained user 
minimum description length mdl alternative machine learning technique compare tf idf weighting mdl principle rissanen 
mdl principle provides information theoretic framework balancing tradeoff model complexity training error 
newsweeder domain tradeoff involves weight token importance decide tokens left model having discriminatory power 
derive mdl principle bayes rule log tf tried decrease performance slightly 
wish find hypothesis maximizes probability observed data bayes rule equivalent maximizing 
doesn depend just maximize equivalently minimize log log information theory shannon know log base equal size bits encoding event optimal binary code 
mdl interpretation expression find probable hypothesis data find hypothesis minimizes total encoding length 
encoding length equal number bits required encode hypothesis plus bits required encode data hypothesis 
mdl principle expresses intuitive notion finding model involves finding right balance simpler models take fewer bits describe complex models models produce smaller error explaining observed data 
essential difference mdl approach popular maximum likelihood ml method ml treats cost encoding models equal 
prior probability hypotheses uniform drop optimization 
mdl applied newsweeder similar tf idf approach perform rating categorization step convert categorization similarities continuous rating prediction 
document token vector containing non zero entries training data train probable category minimizes bits needed encode plus argmax argmin log log train train train simplicity presently focused advantage document length predictive feature treat term constant 
carefully construct model probability distribution probabilistic model token occurrences choosing probabilistic model token distributions want find descriptive function takes measure document length redefined mean number unique tokens document 
advantage know problem space fewest number parameters 
pragmatically want fast large amounts data 
common assumption ir probabilities words occur document independent 
assumption particularly grounded empirically reduces complexity problem space significantly tried 
approach uses assumption allows degree dependence document length word probability range highly dependent completely independent 
treating special tokens just additional words model word probabilities may may depend document length 
example token re abbreviation subject line regarding appear frequently shorter documents longer ones line responses quoting short section longer article 
typically ordinary word appears greater frequency longer document 
formally independence assumption probability data document length category product individual token probabilities train train binary value indicating token occurred document wish derive probability estimate want avoid computationally expensive optimization step parameters model decide 
way compute additional statistics training data parameters model number documents containing token correlation estimate statistic computed category total categories 
objective establish general background distribution token category specific distribution 
subscript designate modeling category specific case equation dependent statistics particular category articles 
equation equally valid modeling background distribution properly standard statistical sample correlation token occurrence document length bounded 
experiments fraction documents containing token proxy estimate concerned inaccurate probabilities frequent tokens 
removing statistics applied articles 
convention token distribution simple binomial independent document length token probability dependent document length approximation distributions combined mixture model weighting choice probability distributions token category specific general distribution 
hypothesize token truly specialized distribution category token unrelated category just exhibits random background fluctuations 
mdl criteria making decision hypotheses choose category specific hypothesis total bits saved hypothesis log log greater complexity cost including extra category specific parameters 
small constant represent complexity cost 
additional pragmatic advantage probabilistic model choice logs taken probabilities get costs bits probability calculation article words simple linear computed longer dictionary due ability precompute sum bits required encode words occurring 
sum bits required actual document quickly computed 
compute similarity document rating category 
similarity inversely proportional number bits required encode theoretically correct complexity costs tried worked simple constant pragmatic advantage multiplicative mixture model linear log typical linear model 
combination probability distributions described previously 
learning algorithm summary explicit entire algorithm tf idf mdl experiments 
divide articles training test unseen sets 

parse training articles throwing tokens occurring times total 

tf idf throw frequent tokens entire training set 

compute token 

tf idf compute term weights normalize weight vector article find average vectors rating category 

mdl decide token category model occurs precompute encoding lengths tokens occurring documents category 

tf idf compute similarity training document rating category prototype cosine similarity metric 

mdl compute similarity training document rating category inverse number bits needed encode category probabilistic model 

similarity measurements computed steps training data compute linear regression rating category similarities continuous rating predictions 
apply model obtained steps similarly test articles 
continuous prediction user ratings 
stage algorithm text categorizing regression motivated considerations 
categories kept distinct standard squares regression mdl tf idf experiments pre processed input variables follows 
similarities normalized range 
second empirically observing normalized similarities needed separation cube normalized similarity inputs regressions 
arbitrary may sound 
token independence assumption adhered expect regression exponential log probabilities 
assumption wildly violated function exponential works linear appears weak 
powers equally 
combined relevant non relevant categories fully take advantage user feedback gradations 
second wanted final prediction matched absolute rating scale user feedback aid user evaluation prediction just obtaining article ranking information 
third needed combine similarity information linear regression simplest ways possible 
evidence fusion results solely content rating predictions 
newsweeder agent combines element collaborative filtering actual line 
new article prefetched system readers chance rate article prediction reader rating components content rating prediction ratings users content rating predictions users components combined final prediction user simple weighted average 
components listed order weight size important 
weights non adaptive arbitrarily chosen subjectively observed provide interesting serendipitous sampling articles scoring highly user filters 
research goals learn model fuse rating evidence components effectively correlating interests multiple users 
results evaluation metrics evaluate performance 
look common evaluation metrics ir precision 
precision defined ratio relevant documents retrieved documents retrieved method choosing subset documents retrieve 
method take top highest predicted rating articles motivated reasoning 
current filter techniques poor people opt read articles 
challenge raise signal noise ratio high small set filtered articles value exceeds costs users 
way analyze results looking confusion matrix errors generated text classifier get better understanding method going wrong 
data labels user meanings ratings provided table 
evaluation purposes define interesting articles rated better define numerical rating skip articles 
table rating labels rating label intended essential articles missed possible interesting articles definite interest borderline articles user uncertain interest commitment boring articles interesting gong articles user wants heavily weight seeing clearly irritating list skip skip articles user want read note category may cover ratings user requests see article users registered newsweeder stuck development process data evaluation 
table summarize data collections recorded year period 
model assumes stable distribution pool temporal dependence users interests lasted year period add amount error performance different users different styles difficult comparisons users ratings 
example skip category means user saw line article summary decided read article optionally depending user style 
typically user pick read rate articles desired list may may choose rate remaining articles skipped 
small important newsgroups user may want clean list articles missed larger newsgroups just letting articles expire fact earlier experiments month period data fewer training examples year period observed approximately better performance smaller data set 
probably due stable set interests 
naturally list may preferable 
user rated articles interesting rating possible considerably smaller percentage interesting articles newsgroups read user table article rating data users rating user user skip total total interesting tf idf performance analysis tf idf ir experts typically call list hand chosen content free words automatically removed articles insignificant 
may arbitrary difficult list accurately create tends improve results significantly worthwhile typical ir text collections 
netnews dynamic environment set content free words constantly shifting assume static list poor choice 
assumption tf idf weighting scheme frequent words tend predictive provides means automating list 
experiments automated list improve results significantly frequent words removed 
looking list words removed words occasionally clearly content free 
effect throwing predictive words useful ones clearly undesirable better keeping 
graph shows effect removing top frequent words precision top highest predicted rating articles 
trials performed user different training test set splits trial results averaged users 
removing words appears range list successful best precision reached words removed 
graph performance removing frequent words number words removed top highest predicted rating avg 
users trials articles indicative real performance means user look forward reading interesting articles rate times higher filtered list looking articles subscribed newsgroups 
course means user interesting articles remaining articles 
user ordinarily wouldn read netnews due low signal noise probably benefit significantly 
mdl performance analysis mdl approach described far affected positively removing frequent words empirically true 
show graph mdl approach performed versus tf idf approach users varying training set size 
performance measured precision percentage interesting articles top highest predicted rating averaged trials randomized training test set splits 
mdl learning algorithm eventually reached precision top highest articles user user compares favorably tf idf approach finding interesting articles user tf idf number selected articles 
user percentage interesting articles compared user surprising see better performance user user far fewer examples 
graph performance tf idf vs mdl test set avg 
trials number training examples user mdl user tf idf user mdl user tf idf table see example confusion matrix mdl categorization articles user single trial 
matrix shows text classifier errors linear regression performed 
numbers reflect mdl filter predict user rating simply chose rating category passing categorization information linear regression 
note tends get large percentage correct main diagonal 
precision obtained just articles predicted rating columns agrees results regression 
general performance regression step tends meet exceed precision obtained method categorization outputs 
table mdl confusion matrix user test articles rating category predicted rating total skip actual rating skip total data gives evidence statements 
machine learning mdl approach significant benefits performance best techniques developed information retrieval tf idf 
second possible gain value learning filtering netnews subset users 
third done reasonable number training examples 
explored potential portion available information leveraged content filtering side explored collaborative filtering potential 
results leave optimistic expanding available information permit automatic learning useful news filtering profiles wide variety users 
multi user data collected hope find content techniques collaborative techniques 
mdl technique presently term frequency fully knowledge term occurred article 
hope adding information mdl tf idf equal footing widen gap performance 
lastly hope take advantage large numbers freely available unrated news articles bias learning stronger statistics global word distributions dependencies 
acknowledgments advisors tom mitchell andy witkin advice guidance research colleagues rich caruana geoff gordon comments discussions 
research supported digital equipment advanced research projects agency 
dumais susan dumais latent semantic analysis improve access textual information 
proceedings chi evans david evans summary clarit project technical report laboratory computational linguistics carnegie mellon university september fischer fischer stevens 
information access complex poorly structured information spaces human factors computing systems chi conference proceedings acm new orleans la april pp 
goldberg goldberg collaborative filtering weave information tapestry 
communications acm pp 
lang ken lang 
internal research report carnegie mellon university www learning cs cmu 
edu res sum ps resnick paul resnick grouplens open architecture filtering netnews internal research report mit center coordination science march anil 
ai research digital service organization 
ai magazine rissanen rissanen 
modelling shortest data description automatica salton gerard salton 
developments automatic text retrieval science august shannon shannon 
mathematical theory communication bell sys 
tech 
journal sheth sheth 
learning approach personalized information filtering master thesis february wallace mosteller wallace 
applied bayesian classical inference case federalist papers pp 
springer verlag new york yan yan garcia molina 
index structures selective dissemination information technical report stanford university yang yiming yang 
example mapping method text categorization retrieval acm transactions information systems vol 
july pp 

