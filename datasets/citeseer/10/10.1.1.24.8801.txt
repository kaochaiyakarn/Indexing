learning pomdp policies internal state gradient ascent douglas aberdeen australian national university douglas aberdeen anu edu au jonathan baxter whizbang 
labs pittsburgh pa august introduced gpomdp algorithm estimating gradient average reward arbitrary partially observable markov decision processes pomdps controlled parameterized stochastic policies :10.1.1.125.2542
gpomdp applies purely reactive memoryless policies policies generate actions function nite histories observations 
fact maintenance belief state sucient optimal control pomdps gpomdp extended cover parameterized stochastic controllers internal state 
investigate performance algorithm load unload pomdp 
analyse previously form internal state nite state machine controller show conditions true gradient controller 
necessary conditions ful lled appears sensible choice initial parameters 
novel variations exhibit problem 
rst modi cation fsm second uses iohmms learn internal state helps maximise reward 
describe novel way performing non stationary signal classi cation preliminary results lvcsr 
gradient approaches reinforcement learning hold promise means solve problems partial observability avoid pitfalls associated non convergence value function methods 
third reason preferring direct policy approaches far easier construct reasonable class parameterized policies construct class value functions know act able compute value acting 
building large body earlier introduced gpomdp algorithm estimating gradient average reward general partially observable markov decision processes pomdps controlled parameterized stochastic policies :10.1.1.125.2542
chief advantages gpomdp requires single sample path underlying markov chain uses free parameter natural interpretation terms bias variance trade requires knowledge underlying state 
showed gpomdp conjugate gradient search nd locally optimal parameters controllers encouraging experimental results variety problems 
generalized gpomdp multi agent problems showed leads natural synaptic update rule networks spiking neurons 
limitation gpomdp applies purely reactive memoryless policies policies function nite observation histories 
general optimal control pomdps policy function entire observation history 
things quite bad knowledge observation history may summarized form belief state current distribution states updated current observation knowledge sucient optimal behaviour 
main contribution extend gpomdp classes parameterized policies actions depend internal state current observation nite history observations 
general possible store entire belief state large potentially nite state spaces encountered practice consider parameterized classes internal states states compute gradient long term average reward respect policy internal state parameters 
approach taken case episodic tasks finite state controllers trained vaps algorithm analysed 
section formally describe task necessary assumptions 
section shows compute arbitrarily close approximation gradient average reward introducing discount parameter section describe extension gpomdp algorithm allows estimate gradient single sample path pomdp augmented observable states 
estimated gradient update parameters policy increasing average reward 
section demonstrates algorithm learns optimal policy section investigate possibility reducing variance value methods learning part controller 
section presents novel application classi cation non stationary signals speech 
section examines technique reducing variance ltering eligibility trace 
implies priori knowledge allows design lter 
section analyse problem nite state pomdp controllers design demonstrated small heaven hell pomdp 
pomdps controlled parameterized policies internal state setting agent actions environment parameterized policy possesses parameterized internal belief state 
agent seeks adjust policy parameters belief state parameters order maximize long term average reward 
formally natural model problem partially observable markov decision processes pomdps 
ease exposition consider nite pomdps consisting states environment 
controls actions available policy 
observations reward state control determines stochastic matrix ij ij probability making transition state state control state observation generated independently probability distribution observations denote probability observation 
denote possible internal belief states policy 
randomized policy simply function mapping belief states probability distributions controls belief state distribution controls denote probability control belief state 
parameterize policies function set real parameters np belief state belief state evolves probabilistically function current observation second set parameters speci cally assume existence function probability making transition belief state belief state ab current observation 
de nitions pomdp evolves follows 
denote initial state environment initial internal belief state policy respectively 

time step generate observation probability 

generate new internal belief state probability 

generate control probability 

generate new environment state probability environment agent evolution finite state pomdp controller 

goto 
illustrates process 
act optimally pomdp environment sucient maintain belief state vector dimension ns probability distribution state occupancy 
computed initial distribution observation history 
number possible belief states grows exponentially time 
learning optimal policy nite horizon pomdp shown pspace hard learning optimal nite horizon pomdp may undecidable 
taken alternative approach controller memory internal states states controller learns remember needed order act optimally 
process viewed automatic quantization belief state space provide optimal policy representable states 
represent optimal policy arbitrarily accurately 
way view process direct learning policy graph 
approach scales computationally large state spaces observation spaces 
problems require nitely internal states large class problems states represent optimal policy load unload problem section 
evolution environment internal belief state pairs markov transition probability matrix entries ns yji bj uj ij assumptions assumption 
unique stationary distribution ns satisfying balance equations denotes transpose 
furthermore assume magnitudes rewards jr uniformly bounded states assumption 
derivatives 
ab exist np ratios 
ab ab uniformly bounded np assumption ensures markov chain generated unique recurrent class mainly just statement theorems compact non unique case easy generalization 
assumption needed gpomdp uses ratios computations 
stochastic controller belief functions uses soft max distribution underlying real parameters satisfy conditions 
goal nd np maximizing long term average reward lim denotes expectation sequences transitions generated 
assumption independent starting state equal ns de ned pursue local approach compute gradient average reward respect parameters adjust parameters gradient direction 
denote gradient parameters respectively 
computing gradient dropping dependence rewrite di erentiating sides yields rp understood equations parameters 
denote dimensional column vector consisting matrix stationary distribution row 

rewrite rp quick induction argument shows converges assumption 
classical matrix theorem exists equal 
write rp course matrix inversion right hand side intractable large state spaces 
replace expression nite series observe rp rewritten rpp discount factor consider expression rp clearly lim take rp back sum write rp vector expected discounted reward environment belief state pair ji expectation sample paths summary proved theorem 
theorem 

lim 
theorem appeared di erent proof 
proof suggestive deeper connection matrix understood terms series expansion perpendicular subspace 
gpomdp internal state point theorem estimated line algorithm needs access reward baselines underlying state environment 
algorithm gives details 
algorithm generalization gpomdp policies internal state called algorithm gpomdp :10.1.1.125.2542
theorem establishes convergence gpomdp policies internal state 
algorithm internal state gpomdp parameterized class randomized policies np parameterized class belief functions pomdp controlled generates parameterized class markov chains satisfying assumption 
discount 
arbitrary intial state internal state observation sequence generated pomdp generated randomly generated randomly 
bounded reward sequence hidden sequence states environment 
set 


np 

observation internal state control reward 






theorem 


estimate produced gpomdp iterations 
assumptions lim 
probability proof 
proof generalization belief states original gpomdp algorithm proof appeared 
denote random process corresponding environment markov chain generated 
denote random process corresponding belief state markov chain generated 
assumption fx asymptotically stationary write rp ab ab ab ab ab ab independent sets gradient values parameters 
remainder proof follows gradient proof 
dropping components rewrite expression right hand side 
ab ab denotes indicator function state random process expectation respect joint distribution fx stationary 
process giving jx stationary ergodic joint process fx stationary ergodic 
fz obtained xed function fx stationary ergodic see proposition 

ab ab bounded assumption ergodic theorem obtain obvious notation pj lim 
ab ab lim 
ab ab lim lim concentrating second term right hand side observe jr br br br 
bounds magnitudes rewards kr assumptions 
pj lim surely 
unrolling equation 
gpomdp algorithm shows equal 

pj required 
procedure followed show true respect parameters 
picture painted theorems pomdps controlled policies internal state gpomdp converges probability approximation gradient average reward respect policy parameters belief state parameters 
turn suciently close approximation 
factor preventing setting variance 
scales proved corollary stateless policies similar result holds setting 
fortunately guaranteed approximation provided exceeds certain mixing time associated pomdp proved stateless policies theorem theorem 
result hold setting appropriate mixing time include mixing time associated policy belief state 
note reason state space nite provided 
exists bounded 
occur example nitely internal states nite subset states candidates state 
practical implication create internal state partially updated deterministically exploiting prior problem knowledge 
example remembering nite number real valued past observations creating nite state space observation history drive transitions nite state machine learning 
load unload problem showed estimates produced gpomdp reactive policies perform conjugate gradient ascent average reward function :10.1.1.125.2542
key trick line search algorithm solves problem noisy value estimates bracketing maximum gradient information 
section technique illustrate application gpomdp toy load unload problem borrowed 
number states states plus loaded unloaded internal states number controls move left right number observations load state unload state intermediate states 
partial observability arises agent determine loaded distinguish internal states 
act optimally agent simply needs remember loaded load unload problem locations 
agent receives reward time passes unload location having rst passed load location 
problem partially observable agent receives distinct observations load state unload state 
state agent choice actions left right 
achieved having internal states bit memory 
parameterize allow arbitrary transitions states current observation parameters write jl jl jl jl ju ju ju ju jn jn jn jn obvious notation 
parameterize allow arbitrary transitions internal states current observation parameters 
far assumed learning independent sets parameters parameterizes policy 
known state policy graph best action associated 
know priori number internal states needed represent optimum policy knowledge avoid learning stochastic policy replacing deterministic mapping internal states actions reducing number parameters learned 
easily shown mapping detrimental underestimate required load unload experiments learn best known 
deterministically choosing roughly halved convergence time 
load unload agent learner load unload problem summarised follows observations set observations indicates loading state unloading state 
states emit null observation 
actions agent choose left move time step 
state updates pomdp state represents road sections agent occupying 
left right null null policy graph learnt gpomdp load unload problem 
node represents state 
arc represent observation 
left state interpreted load move left right state dropped load move right 
belief states problem solved states encode non null observation 
rewards reward picking dropping load 
agent agent lookup table guaranteeing agent represent optimal policy 
gradient policy graph agent policy graph implemented lookup table referencing probability distribution indexed case distribution case 
unfortunately gpomdp requires parameters np probabilities 
parameters left np calculate softmax distribution transform probabilities exp exp similarly 
easily veri ed parameterization controller underlying pomdp satisfy assumptions nite parameter settings require quantity 

exp exp similarly 
reward iterations average reward vs iterations training performance gpomdp load unload problem averaged runs 
iteration step simulation 
results running gpomdp produces policy graph shown 
optimal policy returning long term average reward dashed lines represent transitions learnt experienced optimal policy 
learnt policy converges optimal 
shows training performance system averaged runs 
convergence graph shown algorithm failed converge runs attempts 
due saddle point gradient reward function reached internal state transitions action distributions uniform probability 
internal states may pass saddle point reach optimum parameter values halting gpomdp reports magnitude gradient near 
gpomdp sarsa hybrids section investigate sarsa learn action distribution 
gpomdp learn state fsm 
motivated observations 
firstly fsm described 
considered fully observable mdp 
internal mdp smaller beta bias variance tradeoff discount gpomdp sarsa gpomdp gpomdp gpomdp det bias variance gradient estimates varies 
top line uses gpomdp learn gpomdp det line xes learns gpomdp sarsa line uses sarsa learn gpomdp learn original mdp just states remember information necessary act optimally apply value algorithms sarsa learn globally optimal policy rapidly policy gradient methods 
secondly value approach learns successful policy fsm converged improved actions cause relevant areas environment state space explored meaning gpomdp get sample trajectory highly relevant learning optimal fsm de ned 
improving policy rapidly trajectory actions generated 

improved fsm helps value function method improve estimates value state fsm 
classifying non stationary signals actions produced controller need ect environment directly 
alternatively view action chosen time step label classi cation observations produced environment 
example signal may come radio telescope 
time step observation representation signal instant action estimation steps bias variance reduction time gpomdp sarsa gpomdp gpomdp gpomdp det bias variance gradient estimates estimation time increases 
decision alien intelligence sent signal 
internal state required remember signal history decision 
key advantage reinforcement learning compared sequential data algorithms hidden markov models hmms recurrent neural networks arbitrarily de ne reward signal maximized 
example train system mix signals earth empty space rewarding controller right classi cation 
contrast algorithms goal function chosen restricted family functions maximize likelihood model minimize distance input target output 
gpomdp belief state assumptions signals modeling 
provided implement suciently rich controller suciently belief states achieve classi cation arbitrarily complex non linear discrimination function signal history automatically discovered necessary 
hmms popular choice kind signal analysis interested rest focus compare gpomdp hmm training 
gpomdp inherently discriminatory hmms independent models trained maximize probability match subset training data 
assume independence local stationary sections signal automatically learn cope ects articulation phonemes speech 
single policy graph trained entire data set reducing problems encountered training models instances data model 
disadvantage techniques typically converge quite slowly compared baum welch algorithm error back propagation 
hmms model non stationary signals generate data train test policy graph signal classi cation 
accuracy hmm techniques classifying test data gives baseline compare policy graph hmm classi cation 
need train hmms classi cation optimal hmms generate data rst place 
assume environment contains hmms de ned state transition matrices ij fa mg emission distribution matrices jy generate sequence symbols model time random period switch model 
train agent produce actions produce labels correctly classify generated ann classi cation controller lookup table controller section gradient policy graph agent requires parameters wish belief states high dimension real inputs lookup table controller section falls prey curse dimensionality 
real world problems need approximation lookup table arti cial neural network ann 
careful selection network architecture reduce number parameters ann architecture shown 
belief state input feature augmenting network inputs encode observation input nodes encoding 
step input remainder 
model requires parameters needs modi cation accept multi dimensional real valued observations 
gradient ann agent compute 
ann agent 
rst step lookup table 
evaluate softmax distribution ann outputs chose distribution 
apply chain rule dropping dependencies 

output value belief state amounts error back propagation gradient error outputs rst terms form identical 
hidden nodes output nodes inputs inputs ann architecture hmm discrimination agent 
inputs indicating symbol observed inputs encoding current belief state 
classifying pre segmented signals segments known length generated single model classi ed choosing model corresponding emitted action label correct action chosen average decision correct 
classifying unsegmented signals soon signal starts generated di erent model average action switches correct label 
gpomdp learn mistakes maximize long term label matches need smooth decision 
done ltering action distributions rectangular window arg max window size sensitivity classi cation changes model 
frequently model changes lower 
alternatively view likelihood label current belief state viterbi algorithm 
applied way classi cation procedure closely resembles ann hmm hybrids described training method di erent 
experiments simple tests hand crafted investigate policy graph discrimination signals produced hmms test set test left right left right pr pr pr pr pr pr pr pr hmms generate sequences test hmms classify test sequences generate baseline results 
discriminates models symbols states 
may easy tests iii sets hmms exactly stationary distribution symbols 
learning temporal nonstationary characteristics classi er discriminate signals produced hmms 
shows hmms generating sequences test identical model prefers stay states prefers alternates states 
diculty task illustrated graphical version signals generated models test ii 
details hmms case appendix test cases respectively represent ability discriminate simple signals stationary distribution ii 
harder version test iii 
ability discriminate models iv 
ability discriminate signals di erent stationary distributions 
signals classi ed counting occurrence symbol 
training training data generated running model steps 
gpomdp random segments length taken data model selected uniformly random 
length re ects length speech segments 
training data re estimating hmms just raw sequences generated models test ii 

upper line model lower test table lookup ann ml hmm hmm re est error states error states error states vit vit iii iv table results hmm discrimination tests segmented signals 
table lookup ann agents trained state gpomdp long sequences 
hmm models trained independently models test training performed lookup table representation linear neural network binary encoding 
original experiments multi layer nn output layer changed belief state 
test data generated producing sequences length model 
tests ii iv segments classi ed test iii segments 
gpomdp case order alternating segments model 
true hmm models forward decoding produce best results uniform priors models bayes rule forward decoding produce estimate map probability model ts data 
results belief states trained tests save time second time round just number states initial tests indicated best table shows percentage classi ed segments number belief states gave best performance 
note optimal policy test plug network learnt test reduce error 
fact tests ii iv suciently trained network perform exactly table lookup controllers pick state transitions actions highest probability 
higher errors ann column merely show harder train ann context ann representational power 
test iii states requires stochastic policy clear represent precisely correct probability distribution linear network 
brings achieved better error linear neural net 
net hidden layer squashing converge optimal policy 
linear swapped output neural net just realised exactly equivalent table lookup ensure input set rest 
presumably converged normal multi layer net thrown hidden units time 
model model optimal policy graph tests ii learnt gpomdp 
left states emit label right states emit label model switches state emit di erent symbol policy graph incorrectly emit best decision observations decisions correct 
discussion ml hmms give best results tests 
test ii pretty challenging 
general see gpomdp competitive results re estimated hmms viterbi decoding fair comparison 
amount training data grows hmms barring local maxima problems 
interesting point test iii gpomdp learn stochastic policy perform 
usually policy deterministic testing choosing action 
doing test iii meant chose model error went 
belief states learn optimal nite transient policy quite 
provided sucient belief states represent optimal policy graph increasing number states increase performance 
excess states appear decrease performance training data limited states cause tting 
analysis results tests ii show gpomdp learnt optimal state policy graph illustrated 
belief state emit label corresponding model emit model graph represents policy see observation twice row emit emit 
time performance state model inputs true label segmentation solid lines indicate model generating observations corresponds correct classi cation 
crosses indicate label action output system time step 
dashed line indicates smoothed decision 
automatic segmentation signals fth test de ned test ability system classify signals symbols states 
investigation ability system split continuous stream observations segments belonging model 
test de nes models states discrete output symbols 
appendix shows parameters models 
di erence emission distributions states 
hmm structure previously model speech 
experiment uses ann controller previous experiments 
optimal number belief states experimentally 
early results shown 
crosses indicate raw decisions output system time step 
correct decision time 
plot declaring segment smoothed decision crosses threshold correctly identi es segments deletions 
simple ad hoc segmentation scheme leads reasonable results expected established technique viterbi decoding give better results avenue research 
speech processing demonstrate system works real valued multi dimensional real world signals trained ann agent discriminate spoken digits 
training digits male speakers ti digits database 
digits taken testing 
exactly half digits half 
silence stripped start digit concatenating continuous stream digits random order 
input observations calculated ms intervals ms hamming window 
extracted features typical continuous density hmms mel cepstral parameters log energy delta double delta features added total observation dimension 
inputs normalized presentation neural network 
belief states hidden nodes 
set estimation time training took hour beowulf cluster piii processors 
shows resulting policy graph classi ed segmented test data 
time step labels correct time smoothed decision closely follows correct classi cation 
digit row thresholding smoothed decision section indicate presence multiple digits resulting inability di erentiate digit spoken multiple times 
rectify additional digit boundary class included 
note digits easy discriminate share sounds common 
experiments demonstrated able pick digit continuous stream digits performed digit spotting minimum time step label accuracy maximum 
eligibility trace filtering lines state gpomdp algorithm show update eligibility trace time step 
update viewed rst order nite impulse response iir lter exponential response impulse input 
trace acts memory telling proportion current reward previous action credited 
implicit trace update assumption action considered exponentially impact immediate reward longer ago action occurred 
exponential discounting rewards actions nice analysis properties rl applications may prior knowledge delay execution action rewards ected action 
higher order iir lters lines algorithm encode prior knowledge correct temporal credit assignment reducing variance gradient estimate 
understood variance time performance isolated spoken digits true label segmentation classi cation digits plot 
smoothed decision closely follows true classi cation system able indicate digit spoken twice row adjacent digits force classi cation change 
completely unobservable mdp test iir trace ltering 
experiment set experiment ii set rewards may occur arbitrarily long time action state 
policy gradient techniques high compared value techniques causing slow convergence large problems 
technique reduce variance important practical applications 
line general iir form line jaj jbj jaj gives past remembered jbj past remembered 
line update changes similarly di erent lter 
extensive literature exists design iir lters 
contrived simple example standard gpomdp performs poorly gpomdp trace ltering 
shows completely unobservable mdp optimal policy follow thicker transitions 
actions relevant state 
pomdp harder looks due discounting rewards reward received actions state appears higher upper paths lower optimal path 
gradient point away optimal policy 
optimal lter case nite response impulses corresponding possible reward delays 
note discounting case 
optimal lter case nite response rewards received arbitrarily long time 
expect ltering ective case 
lter experiment impulses allowed decay exponentially setting weight 
true gradient problem computed compared estimates test estimation time bias variance estimates shown table 
experiment produces gradient pointing wrong direction correct direction high variance means convergence times 
simple fir mdp pomdp tests iii iv 
rewards issued delay step time reward received earlier action 
test iii states completely observable test iv depth tree observed 
nodes enter root tree assigned depth bottom tree 
test test ii test iii test iv trace type bias bias bias bias fir iir table results eligibility trace ltering experiments 
impulse response optimal filter optimal fir lter test parameters 
lter wins convergence time order 
experiment ii bias marginally improved variance smaller trace ltering 
note concept ltering trace applied rl algorithm uses trace td 
reinforcement learning lvcsr order demonstrate application policy gradient techniques realworld problem combined state gpomdp trace ltering train phoneme recognizer 
resulting ann hmm hybrid 
rst stage train large ann recognize phones impulse response low bias optimal fir lter test ii parameters 
impulse response iir filter iir lter test ii parameters 
frames mel cepstral features 
time step frames network centered target frame 
th network output estimates jy posterior probability ann input centered phone training set male subset timit continuous speech corpus phones 
network inputs hidden nodes outputs 
case internal state mel cepstral features treated pomdp observations 
controller sets jy 
rewards choosing correct phone gpomdp equivalent supervised training log probability cost function 
testing viterbi decoding procedure single state phone 
scaled node observation likelihoods jm mu jx mu system achieved accuracy test set frames assuming uniform node transition probabilities 
estimated transition probabilities counting transitions training data improved results 
second phase state gpomdp estimate node transition probabilities 
equivalent gpomdp train hmms rst phase estimates observation likelihoods second phase estimates node hmm state transition probabilities 
ideally run phases simultaneously globally optimising performance simplicity training phases independent 
state space state required run viterbi algorithm vector beliefs phone probabilities nite window history tracks best sequence previous phones hypothesized current phone 
general gpomdp state representation provided 
exists bounded 
case updates state deterministic fashion viterbi algorithm 
parameters represent phone transition probabilities 
viterbi algorithm uses max operator view selecting phone transitions boltzmann distribution limit temperature ecient goes 
set pick hypothesized current phone time step 
action composed best phone sequence length computed viterbi history leads chosen phone parameterized contribute gradient 
rewards oldest phone emitted action correctly identi es corresponding input frame rst considered controller time steps ago 
gradient estimate errors corrected decoding likelihood choosing correct phone 
nite viterbi window length limits delay actions relevant rewards steps 
natural ltering section lter assigns credit actions occurred steps ago 
training viterbi nodes phone total nodes hmm terminology tied observation distributions 
initializing transition weights represented system increased accuracy baseline improvement probabilities estimated counting 
fractional improvement demonstrates state gpomdp ability learn complex function internal state perform dicult task reinforcement learning framework 
accuracies frame comparison consider results applying language modelling improve results signi cantly 
heaven hell problem policy action distribution internal state gradient average reward respect policy internal state parameters zero 
policies starting distributions close uniform close point zero gradient respect parameters tend exhibit poor behaviour gradient optimization 
addition internal state transitions close uniform conditional probability internal state trajectories action observation trajectories close uniform 
dicult controller determine internal state transitions adjust order maximize reward 
problem leads small gradients poor performance state gpomdp 
formalise argument theorems 
conditions zero gradient state controller increase require kr 
rpp derived 
setting parameterizes stochastic nite state machine fsm transition probabilities bj 
fsm provides internal state state stochastic policy parameterized 
probabilities individual actions uj 
select initial fsm policy prior training 
sensible choice fsm assumptions task uniform fsm observation equally lead state current state shall prove extra conditions apparently sensible choice initial fsm kr 
de nition 
evolution environment internal belief state pairs markov transition probability matrix entries ns yji bj uj jji de nition 
ns column vector rewards received state case states lemma 
choose bj uj independent current state 
proof 
start re writing account simpli ed distributions combining sums brevity yji bj uj jji similarly write simpli ed gradient yji bj uj jji yji bj uj jji th element dot product row column de nes dot product element computing yji cj uj kji yji bj uj jjk see depend de ne continue moving sum inside yji uj kji cj yji uj kji cj yji uj kji yji uj kji lemma 
conditions lemma column vector rewards 
proof 
th element dot product row de nition independent giving yji cj uj kji move sum inside rest proof lemma 
theorem 
choose bj bj uj uj 
proof 
expand apply lemmas denotes zero matrix pp conditions perpetual zero gradient far shown results 
possible kr conditions theorem 
situations happen 
pomdps possible increase changing stationary distribution actions emitting actions frequently regardless observation 

process function learn optimal reactive policy ignoring 
choose bj bj chosen tells case bj partial observation hiding process way yji partial state hiding process 
useful indicator state may di erent values parameters related di erent states indicating internal state helping maximise 
single iteration gradient ascent cause condition bj bj theorem violated computation gradient results kr 
case interesting allows possibly initialize uj initialize bj bj learn utilize states despite fact theorem tells initializations sucient kr 
theoretical results depend choice parameterization uj 
show uj parameterized real valued table softmax output distribution tighter conditions theorem possible conditions violated kr resulting complete failure utilize states 
theorem 
uj exp exp initialize bj uj uj independent internal state 
proof 
start simplifying noting independence yji uj jji compute show independence easy show uj uj uj 
substitute yji bj uj jji yji uj jji yji jji uj uj yji jji uj uj compute lemma yji jji uj uj yji jji uj uj point observe completely independent internal state 
rewards independent internal state follows independent internal state 
show independent internal state follows independent internal state 
de ned bj probability state time independent factors 
allows state components independent current internal state independent internal state 
signi cance theorem stated conditions impossible system learn internal state advantage 
gradient internal state transition parameters remain 
independent internal state uj results step gradient ascent result state independent policy uj 
bj bj problem optimal policy visit nd move left right top middle state 
theorem kr gradient ascent ect bj 
iteration gradient ascent violate conditions theorem theorem 
induction conditions violated kr 
result lookup tables softmax distribution appear empirically apply anns including linear approximators 
intuitively fair controller initially unbiased internal states subject analysis theorem 
section outlines trick avoid problem demonstrate modi ed heaven hell problem shown 
robot starts position shown 
state completely observable direction revealed bottom right state 
agent remember sign direction top middle state move direction 
receive reward moving correct direction wrong direction theory belief states sucient optimal control 
additional reward received going state problem relatively easy convergence optimal policy achieved 
sparse initial state transitions overcome problem random internal state trajectories modi ed state representation large sparse state fsm states degree assuming consistent initial state trick means trajectory observation action trajectory generate minimally overlapping distributions state trajectories 
allows correlation observation action trajectories set state trajectories 
computationally pay increase analysis anns initial weights fairly simple modi cation theorem leave suciently motivated 
trick resetting internal state receiving reward achieved convergence local maxima maximum achievable reward 
problem achieving average reward indicates system learnt advantage 
setting interesting case implies unique state path observation action trajectory state paths may merge forgetting previous observations 
need set quite large avoid merging state trajectories 
sparse state machine achieve convergence 
ect sparse transition distributions demonstrate problem verify ectiveness sparse state machine performed experiment investigates distribution state trajectories optimal policy 
chose observation sequences priori length 
rst corresponds observations generated acting optimally left side observations left 
second corresponds observations acting optimally right side observations right 
choose state machine choosing degree nodes maximum initial random weight outgoing transitions number states 
feed state machine rst set observations allow generate state transitions internal transition distributions 
de ne set state transitions generated observations rst set observations sample state trajectory trivial compute log probability trajectory construct sample 
sample run state machine second set observations state trajectory sample generated rst set observations 
procedure returns log probability sample trajectory rst set observations jh generated trajectory log probability trajectory sampled second set observations jh 
desire di erent observations induce di erent distributions state trajectories jh jh 
allows rewards consistently associated set trajectories correspond high reward observation action sequences 
generating samples plotting histogram probabilities samples observations axis get feel state trajectories overlap observations sequences length 
optimal trajectories chosen important distinguished 
note di er observations measuring distinction close sets observations 
shows state trajectory probabilities distributed observation sequences state machine fully connected transitions equally 
bars represent exactly probability 
see sampled state trajectories equally independent observation sequence 
little ort convince reader true just possible observation sequences normalised length transitions equal probability 
figures demonstrate result introducing randomness dense state machines 
recall transition probabilities result softmax distribution parameters 
distribution uniform 
choose uniformly random interval 
interval 
soon transitions non uniform observe trajectories take spread 
reduces number trajectories sampled set observations 
observe trajectories slightly probable observations exactly required 
move larger initial weights ect pronounced 
large weights trajectories di erent sets observations 
large weights puts learner local maxima started 
noted ect achieved sparse initialisation 
means state trajectories probability 
demonstrates happens reduce uniform probabilities 
trajectories sampled observations non zero probability equally add random obtain trajectories non zero probability far fewer trajectories sampled move trajectories probability results possible observation sequences selected important trajectories problem 
expect results generalise sequences 
sequences time sequences distributions trajectories generated exactly 
earlier observation sequences diverge longer sequence greater probabilities trajectories note discussion ignored ect distributions actions easily generalise results computing jh 
case problem jh jh observation sequence implies actions chose 
described extension gpomdp handle policies internal state arbitrary eligibility trace ltering demonstrated applicability state gpomdp toy load unload problem learning parameters performing phoneme discrimination lvcsr problem 
illustrated experiments problems state gpomdp state trajectory probabilities uniform fully connected fsm trajectory probability trajectory count state trajectory probabilities random fully connected fsm trajectory log probability trajectories sampled path trajectories path max theta state trajectory probabilities random fully connected fsm trajectory log probability trajectory count trajectories sampled path trajectories path max weight probabilities state trajectories sampled observation sequence sequences gures fully connected state machine 
state trajectory probabilities uniform degree fsm trajectory log probability trajectory count trajectories sampled path trajectories path state trajectory probabilities random degree fsm trajectory log probability trajectory count trajectories sampled path trajectories path max weight non probabilities state trajectories sampled observation sequence sequences gures su er convergence unacceptably poor local maxima 
possible explanation follows policy action distribution internal state gradient average reward respect policy internal state parameters zero 
policies starting distributions close uniform close point zero gradient respect internal state parameters tend exhibit poor behaviour gradient optimization 
addition internal state transitions close uniform conditional probability internal state trajectories action observation trajectories close uniform 
dicult controller determine internal state transitions adjust order maximize reward 
problem leads small gradients poor performance state gpomdp 
problems load unload optimal fsm simple subset random parameters reasonable chance de ning useful fsm 
classi cation problems phoneme recognition task probably assisted lack dependence observations policy 
currently investigating methods initializing internal state parameters avoid local minima problems 
test case details hmm discrimination test sets shown 
consists models states symbols 
common matrices shown 
test test ii test iii test iv test model astr om 
optimal control markov decision processes incomplete state estimation 
journal mathematical analysis applications 
baird andrew moore 
gradient descent general reinforcement learning 
advances neural information processing systems 
mit press 

continuous speech recognition acoustic states 
st meeting acoustical society america april 
peter bartlett jonathan baxter 
hebbian synaptic modi cations spiking neurons learn 
technical report australian national university november 
csl anu edu au jon papers brains ps 
peter bartlett jonathan baxter 
estimation approximation bounds gradient reinforcement learning 
proceedings thirteenth annual conference computational learning theory 
appear 
peter bartlett jonathan baxter 
connection linear ltering gradient reinforcement learning 
preparation 
andrew barto richard sutton charles anderson 
neuronlike adaptive elements solve dicult learning control problems 
ieee transactions systems man cybernetics smc 
jonathan baxter peter bartlett 
direct gradient reinforcement learning gradient estimation algorithms 
technical report research school information sciences engineering australian national university july 
jonathan baxter peter bartlett 
reinforcement learning pomdps direct gradient ascent 
proceedings seventeenth international conference machine learning 
jonathan baxter peter bartlett 
nite horizon policy gradient estimation 
journal arti cial intelligence research 
appear 
jonathan baxter peter bartlett lex weaver 
direct reinforcement learning ii 
gradient descent algorithms experiments 
technical report research school information sciences engineering australian national university september 
bengio renato de mori giovanni ralf 
global optimization neural network hidden markov model hybrid 
ieee transactions neural networks march 
herv bourlard nelson morgan 
hybrid hmm ann systems speech recognition new research directions volume lec lecture notes arti cial intelligence pages 
springerverlag 
leo breiman 
probability 
addison wesley 
xi ren cao han fu chen 
perturbation realization potentials sensitivity analysis markov processes 
ieee transactions automatic control 
xi ren cao yat wah wan 
algorithms sensitivity analysis markov chains potentials perturbation realization 
ieee transactions control systems technology 
anthony cassandra 
exact approximate algorithms partially observable markov decision processes 
phd thesis brown university may 
paul 
speech recognition hidden markov models 
lincoln laboratory journal 
stephen elliott 
signal processing active control 
academic press 
fu hu 
smooth perturbation derivative estimation markov chains 
operations research letters 
ector ge ner bonet 
solving large pomdps real time dynamic programming 
working notes fall aaai symposium pomdps 
www cs ucla edu bonet 
peter glynn 
stochastic approximation monte carlo optimization 
proceedings winter simulation conference pages 
kimura miyazaki kobayashi 
reinforcement learning pomdps function approximation 
fisher editor proceedings fourteenth international conference machine learning icml pages 
kai fu lee 
automatic speech recognition development sphinx system 
kluwer international series engineering computer science 
secs 
kluwer academic publishers 
peter 
simulation methods markov decision processes 
phd thesis laboratory information decision systems mit 
peter john tsitsiklis 
simulation optimization markov reward processes 
technical report mit 
nicolas meuleau leonid peshkin kee kim leslie pack kaelbling 
learning nite state controllers partially observable environments 
proceedings fifteenth international conference uncertainty arti cial intelligence 
tom mitchell 
machine learning 
mcgraw hill new york 
yves normandin re gis 
developments high performance connected digit recognition volume nato asi series pages 
springer verlag berlin 
singh jaakkola jordan 
reinforcement learning soft state aggregation 
tesauro touretzky leen editors advances neural information processing systems volume 
mit press cambridge ma 
smallwood sondik 
optimal control partially observable markov decision processes nite horizon 
operations research 
sondik 
optimal control partially observable markov decision processes nite horizon discounted costs 
operations research 
richard sutton andrew barto 
reinforcement learning 
mit press cambridge ma 
isbn 
sebastian thrun 
monte carlo pomdps 
solla leen kr 
uller editors advances neural information processing systems 
mit press 
citeseer nj nec com thrun monte html 
viterbi 
error bounds convolutional codes asymptotically optimum decoding algorithm 
ieee transactions theory april 
ronald williams 
simple statistical gradient algorithms connectionist reinforcement learning 
machine learning 

