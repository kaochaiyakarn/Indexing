layered quality adaptation internet video streaming reza rejaie mark handley deborah estrin labs research aciri icsi usc isi reza research att com aciri org estrin isi edu streaming audio video applications increasingly popular internet lack effective congestion control applications cause significant concern 
problem adapting compression requiring video servers data fitting resulting stream rapidly varying available bandwidth 
time rapid fluctuations quality disturbing users avoided 
mechanism layered video context unicast congestion control 
quality adaptation mechanism adds drops layers video stream perform long term coarse grain adaptation tcp friendly congestion control mechanism react congestion short timescales 
mismatches timescales absorbed buffering receiver 
efficient scheme distribution available bandwidth active layers 
scheme allows server trade short term improvement long term smoothing quality 
discuss issues involved implementing tuning mechanism simulation results 
keywords quality adaptive video playback unicast layered transmission internet internet experiencing explosive growth audio video streaming 
current applications involve webbased audio video playback stored video streamed server client request 
growth expected continue semi realtime traffic form higher portion internet load 
behavior applications significant impact internet traffic 
support streaming applications internet needs address conflicting requirements 
application requirements streaming applications semi reliable rate 
require isochronous processing quality service qos point view 
mainly stored video intrinsic transmission rate requires relatively constant bandwidth deliver stream certain quality 

network requirements internet shared environment currently micro manage utilization resources 
systems expected cooperative reacting congestion properly promptly 
deploying congestion control results higher utilization network improves inter protocol fairness 
congestion control mechanism determines available bandwidth state network 
available bandwidth vary unpredictable potentially wide fashion 
satisfy requirements simultaneously internet streaming applications quality adaptive 
application adjust quality delivered stream required bandwidth matches congestion controlled 
frequent changes perceived quality resulting earlier version appeared acm sigcomm cambridge massachusetts usa 
rate adjustment disturbing users avoided 
main challenge minimize variations quality obeying congestion controlled rate limit 
currently commercial streaming applications perform congestion control 
rate applications transmit data near constant rate loosely adjust transmission rates long timescales rate adaptation required effective congestion control compatible nature 
large scale deployment applications result severe inter protocol unfairness behaved tcp traffic possibly congestion collapse 
dominant portion today internet traffic tcp crucial realtime streams perform congestion control 
mean realtime traffic share resources tcp traffic fashion 
believe congestion control streaming applications remains critical health internet resource reservation differentiated services widely available 
services provided class basis flow basis 
different users fall class service share reservation interact best effort networks 
furthermore remain significant group users interested realtime applications best effort service due lower cost lack access better services 
presents novel mechanism adjust quality congestion controlled video playback fly 
key feature quality adaptation mechanism ability control level smoothing frequency changes improve quality delivered stream 
design efficient quality adaptation scheme need know properties deployed congestion control mechanism 
primary assumption congestion control mechanism employs additive increase multiplicative decrease aimd algorithm promising rate adaptation algorithm achieve fairness points internet 
previously designed simple tcp friendly congestion control mechanism rate adaptation protocol rap 
rap rate congestion control mechanism employs aimd algorithm manner similar tcp 
shows transmission rate rap source time 
similar tcp fair share bandwidth 
tcp rap ack clocked variations transmission rate regular sawtooth shape 
bandwidth increases linearly period time packet lost exponential backoff occurs cycle repeats 
assume rap underlying congestion control mechanism properties relatively simple predict 
proposed quality adaptation mechanisms applied congestion control scheme deploys aimd algorithm 
transmission rate kb time sec transmission rate rap source fine grain rap transmission rate link bandwidth fig 

transmission rate single rap flow target environment target environment video server simultaneously plays back different video streams demand heterogeneous clients 
current internet video streaming expect length streams range second clips full length movies 
server clients connected internet dominant competing traffic tcp 
clients heterogeneous network capacity processing power 
users expect startup playback latency low especially shorter clips played back part web surfing 
pre fetching entire stream starting playback option 
believe scenario reasonably represents current anticipated internet streaming applications 
motivation video playback stored single lowest encoding server high bandwidth clients receive poor quality despite availability large amount bandwidth 
video stored single higher quality encoding higher data rate server low bandwidth clients play back stream 
past seen streams available kb kb user choose connection speed 
advent adsl cable modems home faster access rates businesses internet heterogeneous 
customers higher speed connections feel frustrated restricted modem speed playback 
network bottleneck may backbone links server 
case user know congestion level congestion control mechanisms streaming video playback critical 
time varying bandwidth channel due congestion control server able maximize perceived quality delivered stream level available network bandwidth permit preventing frequent changes quality 
essence quality adaptation 
quality adaptation mechanisms ways adjust quality pre encoded stored stream including adaptive encoding switching multiple pre encoded versions hierarchical encoding 
may re quantize stored encodings fly network feedback 
encoding servers able large number clients 
furthermore original data compressed stored output rate encoders changed wide range 
alternative approach server keeps versions stream different qualities 
available bandwidth changes server plays back streams higher lower quality appropriate 
hierarchical encoding server maintains layered encoded version stream 
bandwidth available layers encoding delivered 
average bandwidth decreases server may drop layers transmitted 
layered approaches usually decoding constraint particular enhancement layer decoded lower quality layers received 
duality adding dropping layers layered approach switching streams multiply encoded approach 
layered approach suitable caching proxy heterogeneous clients 
addition requires storage server provides opportunity selective repair important information 
design layered approach quality adaptation primarily entails design efficient add drop mechanism maximizes quality minimizing probability base layer buffer underflow 
adopted layered approach quality adaptation 
role quality adaptation hierarchical encoding provides effective way video playback server coarsely adjust quality video stream transcoding stored data 
provide fine grained control bandwidth bandwidth changes granularity layer 
furthermore needs quality adaptation mechanism smoothly adjust quality number layer bandwidth changes 
users tolerate poor stable quality video rapid variations quality disturbing 
hierarchical encoding allows video quality adjustment long periods time congestion control changes transmission rate rapidly short time intervals round trip times 
mismatch timescales buffering data receiver smooth rapid variations available bandwidth allow near constant number layers played 
quality adaptation addressed initial buffering receiver long lived mismatch available bandwidth playback quality results buffer overflow underflow 
main question change bandwidth trigger adjustment quality delivered stream 
tradeoff short term improvement long internet congestion control acker layer layer layer layer bw bw bw bw bw bw quality adaptation layered encoded stored stream bw decoder bw bw bw bw buf buf buf buf display client server bw ack ack available bw fig 

components quality adaptation mechanism time available bandwidth aggressive quality adaptation conservative quality adaptation fig 

aggressive vs conservative quality adaptation term smoothing quality 
illustrates tradeoff 
sawtooth waveform shows available bandwidth specified congestion control mechanism 
quality playback stream aggressive conservative quality adaptation schemes shown solid dashed lines respectively 
aggressive approach new layer added result minor increase available bandwidth 
clear long maintain new layer 
aggressive approach results short term improvement 
contrast conservative alternative adjust quality response minor changes bandwidth 
results longterm smoothing 
effect adding dropping layers perceived quality encoding specific 
addressing problem specific encoding scheme design quality adaptation mechanism ability control level smoothing 
having tuning capability tune quality adaptation mechanism particular encoding scheme minimize effect adding dropping layers perceived quality 
rest organized follows provide overview layered approach quality adaptation explain coarse grain adding dropping mechanisms section ii 
discuss fine grain inter layer bandwidth allocation single backoff scenario 
section iii motivates need smoothing presence real loss patterns discusses possible approaches 
section iv sketch efficient filling draining mechanism achieves smoothing able cope efficiently various patterns losses 
evaluate mechanism simulation section section vi briefly reviews related 
section vii concludes addresses plans 
ii 
layered quality adaptation depicts client server architecture 
streams layered encoded stored server 
congestion control mechanism dictates available bandwidth send amount wish send active layers multiplexed single rap flow server 
client side layers goes corresponding buffer 
decoder drains data buffers feeds display 
assume layers linearly layer bandwidth 
simplifies analysis requirement 
addition assume layer constant consumption rate time 
real codec approximation reasonable 
second assumption relaxed slightly increasing amount receiver buffering layers absorb variations layer consumption rate 
assumptions imply buffers drained constant rate 
congestion control module continuously reports available bandwidth quality adaptation module 
quality adaptation module adjusts number active layers allocated share congestion controlled bandwidth active layer 
draining rate buffer constant known priori server effectively control buffer share layer buf adjusting bandwidth share bw 
fine grain bandwidth allocation performed assigning packet particular layer 
ack packet reports available bandwidth transmission rate inter 
transmission rate limited flow control mechanism due limited buffer space client 
simplicity ignore flow control issues actual implementations 
solutions generally require little receiver buffering issue 
sequence number time time bandwidth consumption rate transmission rate filling phase draining phase filling phase draining phase filling phase packet received packet playout receiver buffer layer layer backoff backoff fig 

layered encoding receiver buffering client playout time server 
having estimate rtt history transmitted packets layer server estimate amount buffered data layer client 
achieve robustness ack loss variations rtt layer buffers rtts worth playback data required quality adaptation 
graphs simple simulation quality adaptation mechanism action 
top graph shows available network bandwidth consumption rate receiver layers consumed startup layer layers 
simulation packets dropped cause congestion control backoffs transmission rate drops consumption rate period time 
lower graph shows playout sequence numbers actual packets time 
horizontal lines show period arrival time playout time packet 
indicates total amount buffering layer 
simulation shows buffered data layer base layer layer enhancement layer 
backoff length lines decreases indicating buffered data layer compensate lack available bandwidth 
time second backoff little data buffered layer addition large amount layer 
data drawn buffers properly compensate lack available bandwidth 
shows single cycle congestion control mechanism 
sawtooth waveform instantaneous transmission rate 
active layers consumption rate left hand side transmission rate higher consumption rate data stored temporarily receiver buffer 
total amount stored data equal area triangle abc 
period time known filling phase 
time packet lost transmit rate reduced multiplicatively 
continue playing layers transmission rate drops consumption rate data drawn receiver buffer transmission rate reaches consumption rate 
total amount data drawn buffer shown triangle cde 
period time known draining phase 
filling phase draining phase time bandwidth deficit supplied receiver buffer available bandwidth network total consumption rate available bandwidth rate increase aggregate filled data aggregate drained data spare data stored receiver buffer fig 

filling draining phase quality adaptation mechanism adjust number active layers bandwidth share 
attempts derive efficient behavior key mechanisms ffl coarse grain mechanism adding dropping layers 
changing number active layers server perform coarse grain adjustment total amount receiver buffered data 
time affects quality delivered stream 
ffl fine grain inter layer bandwidth allocation mechanism active layers 
spare bandwidth available server send data layer rate higher consumption rate increase data buffered layer receiver 
server control distribution total buffered data filling phase fine grain inter layer bandwidth allocation 
receiver buffered data available layer server temporarily allocate bandwidth layer consumption rate layer 
layer buffer buf drained rate equal gamma bw absorb reduction layer bandwidth share 
server control draining rate various layers fine grain allocation bandwidth active layers draining phase 
section coarse grain adding dropping mechanisms relation fine grain bandwidth allocation 
discuss fine grain bandwidth allocation subsequent sections 
adding layer new layer added soon instantaneous available bandwidth exceeds consumption rate decoder existing layers 
excess bandwidth start buffering new layer 
problematic knowing available bandwidth decide possible start decoding layer 
new layer playout determined inter layer timing dependency data base layer 
reasoned decision data new layer send note inter layer timing new layer adjusted maintained long buffer drain completely 
practical approach start sending new layer instantaneous bandwidth exceeds consumption rate existing layers plus new layer 
approach layer start play immediately 
excess bandwidth time available bandwidth exceeds consumption rate existing layers new layer added 
excess bandwidth buffer data existing layers receiver 
bandwidth constraint adding sufficiently conservative may result layers added dropped cycle congestion control sawtooth 
rapid changes quality disconcerting viewer 
way prevent rapid changes quality add buffering condition adding new layer endanger existing layers 
clearly need sufficient buffering receiver smooth variations available bandwidth number active layers change due normal hunting behavior congestion control mechanism 
server may add new layer 
instantaneous available bandwidth greater consumption rate existing layers plus new layer 
sufficient total buffering receiver survive immediate backoff continue playing existing layers plus new layer 
satisfy second condition assume additional backoff occur draining phase slope linear increase properly estimated 
minimal criteria adding new layer 
conditions held new layer kept reasonable period time normal congestion control cycles 
shall show section iii conservative adding mechanisms result smoother changes quality 
expressing adding conditions precisely condition 
na condition na gamma buf na gamma current transmission rate backoff factor na number currently active layers buf amount buffered data layer rate linear increase bandwidth typically packet rtt dropping layer backoff occurs total amount buffering receiver estimated required buffering recovery area triangle cde correct course action immediately drop highest layer 
reduces consumption rate reduces buffer requirement recovery 
buffering insufficient server iteratively drop highest layer amount buffering sufficient 
buffering sufficient maintain base layer session experience interruption playback 
expressing dropping mechanism precisely nac na gamma buf na na gamma mechanism provides coarse grain criteria dropping layer 
may insufficient prevent buffer underflow draining phase reasons ffl may suffer backoff current draining phase completes 
ffl estimate slope linear increase may incorrect network rtt changes substantially 
ffl may sufficient total data buffered may allocated different layers manner precludes aid recovery 
situations due incorrect prediction total amount required buffering recover draining phase term event critical situation 
events appropriate course action drop additional layers soon critical situation discovered 
probability experiencing critical situations effectively reduced deploying conservative adding mechanism address 
third situation problematic relates fine grain bandwidth allocation active layers filling draining phases 
devote rest deriving evaluating near optimal inter layer bandwidth allocation scheme 
tackle problem identify optimal inter layer buffer allocation single backoff draining phase maximize buffering efficiency recovery 
derive fine grain inter layer bandwidth allocation keeps inter layer buffer allocation close possible efficient state 
extend solution scenarios 
inter layer buffer allocation decoding constraint hierarchical coding additional layer depends lower layers correspondingly decreasing value 
buffer allocation mechanism provide higher protection lower layers allocating higher share total buffering 
challenge inter layer buffer allocation ensure total amount buffering sufficient properly distributed active layers effectively absorb shortterm reductions bandwidth occur 
examples illustrate ways improper allocation buffered data fail compensate lack available bandwidth 
ffl dropping layers buffered data simple buffer allocation scheme allocate equal share buffer layer 
highest layer dropped backoff buffered data longer absorbing shortterm reduction bandwidth 
top layer data played providing buffering functionality 
implies beneficial buffer data lower layers 
ffl insufficient distribution buffered data equally sim ple buffer allocation scheme allocate buffering base layer 
consider example layers playing total consumption rate supplied receiver decoder 
transmission rate drops base layer played buffer 
buffering require transmission source 
available bandwidth sufficient feed layer 
dropped total buffering sufficient recovery 
examples total buffering sufficient prevent dropping layers 
inefficient buffering 
general striving distribution buffering efficient sense provides maximal protection dropping layers pattern short term reduction available bandwidth 
examples reveal tradeoffs buffer allocations 
allocating buffering lower layers improves protection increases efficiency buffering 

buffered data layer provide consumption rate reduction available bandwidth layer buffer drained faster consumption rate 
minimum number buffering layers needed successful recovery short term reductions available bandwidth 
minimum directly determined amount reduction bandwidth intend absorb buffering 
expressing precisely na gamma na na min 
number buffering layers transmission rate backoff optimal inter layer buffer allocation draining phase single backoff derive optimal inter layer buffer allocation maximizes buffering efficiency 
illustrates optimal buffer allocation corresponding draining pattern draining phase 
assume total amount buffering receiver time precisely sufficient recovery area triangle afg spare buffering available draining phase 
justify optimality buffer allocation consider consumption rate layer supplied network buffer combination 
supplied entirely buffer layer buffer draining consumption rate area quadrilateral shows maximum amount buffer drained single layer draining phase 
draining phase ends predicted preference buffer distribution active layers long layer worth buffered data 
situation critical due backoffs layers dropped 
allocating area buffering base layer ensure maximum amount buffered data usable recovery maximizes buffering efficiency 
similar reasoning largest amount additional layer buffer contribute quadrilateral bcde portion buffered data allocated enhancement layer 
approach minimizes amount buffered data allocated higher layers dropped critical situation consequently maximizes buffering efficiency 
optimal amount buffering layer buf opt na gamma gamma gamma gamma buf opt nac gamma gamma ic gamma total amount buffering receiver time higher required buffering recovery minimum number highest buffering layers participate recovery 
approach maximizes efficiency lower layers maintain extra buffering draining phase 
note reasoning derive optimal inter layer buffer allocation different layers bandwidth 
case optimal buffer share layer function bandwidth 
fine grain bandwidth allocation server control filling draining pattern receiver buffers proper fine grain bandwidth allocation active layers 
filling phase server gradually fill receiver buffers inter layer buffer allocation remains close optimal 
main challenge optimal inter layer buffer allocation depends transmission rate time backoff known priori backoff may occur random time 
tackle problem filling phase server utilizes extra bandwidth progressively fill receiver buffers optimal state step wise fashion 
step amount buffered data buffering layer raised optimal level sequential fashion starting base layer 
inter layer buffer allocation reaches target optimal state new optimal state calculated sequential filling draining phase total consumption rate time bandwidth available bandwidth network data rate drawn receiver buffer fig 

optimal inter layer buffer distribution new target state performed 
filling phase draining phase layer layer layer layer layer optimal buffer optimal buffer optimal buffer available bandwidth total consumption rate buffer draining time bandwidth buffer filling fig 

optimal buffer sharing illustrates fine grain bandwidth allocation achieve sequential filling pattern filling phase 
server maintains image receiver buffer state continuously updated playout information included ack packets 
filling phase extra bandwidth allocated buffering layers packet basis steps assuming backoff occur immediately keep layer sufficient buffering optimal distribution recover 
sufficient buffering packet assigned condition met second step started 
keep layers sufficient buffering optimal distribution recover 
sufficient buffering packet assigned reaches optimal level 
server starts sending packets layers optimal level buffering survive 
start new step increase number expected surviving layers calculate new optimal buffer distribution sequentially fill buffers new optimal level 
process repeated layers survive single backoff 
fine grain bandwidth allocation strategy filling phase results efficient inter layer buffer allocation point time 
backoff occurs exactly time layers survive backoff 
occurrence backoff earlier results dropping active layers 
buffer state close possible optimal state layers 
backoff occurs adding conditions section ii satisfied new layer added repeat sequential filling mechanism 
illustrates server controls draining pattern proper fine grain bandwidth allocation active layers 
point time draining phase bandwidth share plus draining rate layer equal consumption rate 
maximally efficient buffering results upper layers supplied network draining phase lower layers supplied buffers 
example just backoff layer supplied entirely buffer amount supplied buffer decreases zero data supplied network takes 
layers supplied buffer longer periods 
iii 
smooth add drop strategy previous section derived optimal filling draining scheme assumption buffer survive single backoff layers intact 
examination internet traffic indicates real networks exhibit near random loss patterns frequent additional backoffs draining phase 
aiming survive single backoff aggressive results frequent adding dropping layers 
smoothing achieve reasonable smoothing add drop rate obvious approach refine adding conditions section ii conservative 
considered mechanisms achieve smoothing ffl may add new layer average available bandwidth greater consumption rate existing layers plus new layer 
ffl may add new layer sufficient amount buffered data survive kmax backoffs existing layers kmax smoothing factor value greater 
mechanisms results smoothing allows directly tie adding decision appropriate buffer state adding utilize limited bandwidth links effectively 
example sufficient bandwidth modem link receive layers average bandwidth high add third layer 
contrast mechanism send layers time desirable 
rest assume condition adding new layer availability optimal buffer allocation recovery kmax backoffs 
changing kmax allows tune balance maximizing short term quality minimizing changes quality 
obvious question degree smoothing appropriate 
absence specific layered codec user evaluation kmax analytically derived 
set real world user perception experiments determine appropriate degree smoothing disturbing user 
kmax set average bandwidth rtt determine duration draining phase 
achieve smoothing extend optimal inter layer buffer allocation strategy accommodate efficient recovery multiple backoff scenario 
evolution inter layer buffer allocation determines fine grain bandwidth allocation 
buffering revisited delay adding new layer achieve smoothing affects way fill drain buffers 
demonstrates issue 
time 
second filling phase starts time sufficient buffering survive backoff 
smoothing purposes new layer added point continue buffering data backoff occurs note available bandwidth increases total amount buffering increases required buffering recovery single backoff decreases 
time filling phase filling phase draining phase layer layer layer layer layer layer data added buffers data streamed network sender data draining buffers draining phase fig 

revised draining phase algorithm buffering need survive single backoff insufficient buffering survive second backoff draining phase 
need specify allocate extra buffering time drain buffers maintaining efficiency 
conceptually filling phase server sequentially examines steps step buffer backoff intact 
step buffer backoff intact 
step na buffer backoff gamma intact 
step na buffer backoff gamma intact backoffs intact 
step na buffer backoff gamma intact backoffs gamma intact 
step kmax na buffer backoff gamma intact backoffs gamma intact kmax backoffs gamma intact point filling phase working completion step 
step optimal inter layer buffer allocation calculated current transmission rate number active layers 
buffering layers sequentially filled optimal level described section ii ii 
adding condition met new layer added 
draining phase started due backoffs essentially reverse filling process 
identify steps currently located 
determines layers dropped due lack sufficient buffering 
traverse steps reverse order determine buffering layers drained 
amount pattern draining controlled fine grain inter layer bandwidth allocation server shown 
essence consecutive filling draining phases traverse sequence steps optimal buffer states back forth point time buffer state close optimal possible 
layer added dropped new sequence optimal buffer states calculated process continues 
section describe details calculation set optimal buffer states 
iv 
buffer allocation smoothing design efficient filling draining mechanisms presence smoothing need know optimal inter layer buffer allocation corresponding maximally efficient finegrain inter layer bandwidth allocation multiple backoff scenarios 
optimal buffer allocation scenario multiple backoffs unique depends time additional backoffs occur draining phase 
knowledge loss distribution patterns principle possible calculate optimal buffer allocation 
solution excessively complex problem trying solve rapidly intractable number backoffs increases 
assume additional backoff occurs draining phase 
possible scenarios shown 
illustrates optimal buffer allocation scenario depends time second backoff consumption rate transmission rate backoff 
bandwidth time backoff backoff backoff backoff backoff backoff scenario scenario scenario available bandwidth data consumed buffers consumption rate fig 

possible double backoff scenarios extend idea optimal buffer allocation single backoff section ii individual scenario 
added complexity arises fact different scenarios require different buffer allocations 
equal amount total buffering needed recovery scenarios extreme cases sense need maximum minimum number buffering layers respectively 
addressing extreme scenarios efficiently cover intermediate scenarios scenario 
need decide scenario consider filling phase 
key observation total amount buffering scenarios equal having optimal buffer distribution scenario sufficient recovery scenario maximally efficient 
converse feasible 
higher flexibility scenario comes fact scenario needs larger number buffering layers scenario 
buffer distribution recover scenario able cope scenario requires total buffering vice versa 
suggests filling phase backoff scenarios consider optimal buffer allocation scenario fill buffers step step sequential fashion described section iii 
achieved move consider scenario 
filling phase smoothing extend idea scenarios backoffs need examine optimal buffer allocation scenario successive value illustrates set optimal buffer states including total buffer requirement optimal inter layer allocation scenario different values ideally monotonically increase layer total buffering filling phase traverse optimal buffer states turn 
exceeds kmax smoothing factor add new layer start process new set optimal buffer states 
scenario scenario scenario scenario scenario scenario scenario scenario layer buffer layer buffer layer buffer layer buffer layer buffer fig 

buffer distributions backoffs goal order different buffer states increasing value total amount required buffering 
traversing sequence buffer states optimal state requires buffering 
layer buffer layer buffer layer buffer layer buffer layer buffer fig 

distributions increasing order buffering unfortunately requires occasionally drain existing buffer order reach state examples phenomenon visible ffl moving scenario case scenario case involves draining buffer 
ffl moving scenario case scenario case involves draining buffer 
want drain layer buffer filling phase buffering provides protection previous scenario passed 
seek maximally efficient sequence buffer states consistent existing buffering 
ensures total amount required buffering layer buffer requirement monotonically increasing traverse optimal buffer states 
key observation mentioned earlier allows calculate sequence 
recall having optimal buffer distribution scenario sufficient recovery means order states increasing value total required buffering different order increasing value layer buffering layer 
scenario maximally efficient 
flexibility solution constrain layer buffer allocation scenario state previous scenario state scenario state sequence states 
depicts sequence maximally efficient buffer states applying constraints step filling process numbered 
enforcing constraint traverse buffer states buffer allocation state satisfies buffer requirement previous states 
implies total amount buffering amount layer buffering increase monotonically 
layer buffering aid recovery 
sufficient buffering recovery kmax backoffs scenarios new layer added new set optimal states calculated 
step step step layer buffer layer buffer layer buffer layer buffer layer buffer fig 

step step buffer filling pseudo code function expresses packet algorithm ensure buffer state remains maximally efficient filling phase 
algorithm performs fine grain bandwidth allocation assigning transmitting packet particular layer 
kmax smoothing factor giving number backoffs buffer data adding new layer 
scenario buf total log nac buf total gamma nac gamma delta log nac number backoffs considered scenario buf total log nac buf total gamma nac gamma delta gamma gamma nac delta log nac log nac function returns total amount required buffering layers recover scenario question current sending rate number active layers number backoffs considered 
scenario buf opt log nac buf opt na gamma gamma gamma gamma log nac scenario buf opt log nac buf opt gamma na gamma gamma gamma gamma delta gamma na gamma gamma log nac function returns maximally efficient amount required buffering particular layer scenario state currently working 
input parameters function layer number current sending rate number active layers number backoffs considered 
function backoffs backoffs backoffs kmax increment backoffs scenario backoffs increment backoffs scenario backoffs layer scenario backoffs layer scenario backoffs layer backoffs kmax re considering scenario layer layer return re considering scenario layer backoffs kmax layer layer return worth noting proposed packet bandwidth scheduling inherently adaptive major changes rtt 
server maintains moving average rtt calculate slope linear increase 
major variations rtt affects calculation current set optimal buffer states consequently impact inter layer fine grain bandwidth allocation 
implies buffer state temporarily sub optimal inter layer bandwidth allocation reacts changes 
draining phase smoothing traverse maximally efficient states backoffs eventually move draining phase 
incrementally traverse maximally efficient path buffer states filling phase traverse path reverse direction draining phase 
conceptually point time maximally efficient buffer state drain previous maximally efficient buffer state maximally efficient path 
approach guarantees highest layer buffers drained longer required lowest layer buffers drained early 
achieve draining pattern periodically calculate draining pattern short period time expect drain certain amount total buffering 
amount determined current estimate slope linear increase current total consumption rate current transmission rate length draining period 
calculate algorithm similar pseudo code previous state maximally efficient path called target buffer state reach draining total amount buffering 
comparing target current buffer state determine buffering layers drained 
constraint draining rate layer buffer higher consumption rate amount drained data layer buffer limited maximum amount consumed period 
fine grain inter layer bandwidth allocation performed buffering layer drained specified amount pattern similar 
buffer state reaches target buffer state current period new draining period started move consider new target state maximally efficient path calculate corresponding draining pattern 
draining strategy able adapt variations rtt periodic adjustment fine grain inter layer bandwidth allocation 
process repeated draining phase ended 
simulation evaluated quality adaptation mechanism simulation bandwidth traces obtained rap ns simulator real internet experiments 
provides detailed overview mechanisms action 
shows second trace quality adaptive occurs server estimates slope linear increase total buffering drained faster expected rate 
rap flow exists sack tcp flows additional rap flows kb bottleneck ms rtt 
smoothing factor set provides receiver buffering backoffs adding new layer kmax 
consumption rate layer equal kb shows parameters ffl total transmission rate illustrating saw tooth output rap 
overlaid consumption rate active layers transmission rate demonstrate add drop mechanism 
ffl transmission rate broken bandwidth layer 
shows variation available bandwidth absorbed changing rate lowest layers shown light gray shading 
ffl individual bandwidth share layer 
periods layer streamed consumption rate build receiver buffering visible spikes bandwidth 
ffl buffer drain rate layer 
clearly visible points buffers playout bandwidth share temporarily layer consumption rate 
ffl accumulated buffering receiver active layer 
graphs demonstrate short term variations bandwidth caused congestion control mechanism effectively absorbed receiver buffering 
furthermore playback quality maximized risking complete playback due buffer underflow 
smoothing factor examine impact smoothing factor behavior repeated previous simulation different values kmax shows number active layers buffer allocation active layers kmax kmax kmax 
expected higher values kmax reduce number changes quality expense increasing time takes achieve best short term quality 
manifests ways 
kmax increases total amount buffering increased 
second buffering allocated higher layers cope larger variations available bandwidth result successive backoffs 
responsiveness explored responsiveness quality adaptation mechanism large step changes available bandwidth 
depicts rap trace parameters cbr source rate equal half bottleneck bandwidth started stopped kmax 
rap congestion control mechanism rapidly responds changes adjusting average transmission rate 
quality adaptation mechanism closely follows changes bandwidth 
dropped bandwidth reduces added bandwidth available 
notice layer buffer involved process reception base layer 
satisfied original design goal providing smoothing quality providing protection critical layers 
total transmit consumption rates kb time transmit rate breakdown layer kb time transmit rate layer kb time layer kb time layer kb time layer kb time layer drain rate layer kb time layer kb time layer kb time layer kb time layer data buffered layer bytes layer time bytes layer time bytes layer time bytes layer time fig 

seconds kmax trace efficiency performance algorithms examined efficiency buffer allocation 
inter layer buffer allocation maximally efficient conditions satisfied data buffered layer dropped ii layer dropped total amount buffering insufficient 
quantify efficiency scheme respect condition calculated percentage remaining buffer dropped layer follows buf total drop buf total buf total buf drop denote total buffering buffer share dropped layer 
averaged value drop events simulation evaluation metric efficiency 
table shows efficiency values different values kmax tests 
rap tcp test depicted figures rap tcp test large cbr burst shown 
results show scheme efficient little buffered data available layer dropped 
kmax kmax kmax kmax kmax table efficiency buffer allocation table shows percentage drops due poor buffer distribution tests 
drops happened amount buffered data receiver distributed differently 
mechanism completely efficient respect test performs fairly case 
clearly mechanism efficient kmax increases 
higher value kmax buffering allocated higher layers 
higher probability dropping highest layer buffering particularly sudden drops available bandwidth cbr source appears 
essence conservative buffering higher kmax enables server cope wider variations bandwidth 
sudden drops bandwidth situations results lower efficiency 
kmax kmax kmax kmax kmax table drops due poor buffer distribution vi 
related receiver layered transmission discussed context multicast video accommodate heterogeneity performing coarse grain congestion control 
differs approach allows fine grain congestion control unicast delivery step function changes transmission rate 
merz iterative approach sending high bandwidth video low bandwidth channel 
sug total transmit consumption rates kb time kmax data buffered layer bytes layer time bytes layer time bytes layer time bytes layer time total transmit consumption rates kb time kmax data buffered layer bytes layer time bytes layer time bytes layer time bytes layer time total transmit consumption rates kb time kmax data buffered layer bytes layer time bytes layer time bytes layer time bytes layer time fig 

effect kmax buffering quality total transmit consumption rates kb time transmit rate breakdown layer kb time transmit rate layer kb time layer kb time layer kb time layer kb time layer drain rate layer kb time layer kb time layer kb time layer kb time layer data buffered layer bytes layer time bytes layer time bytes layer time bytes layer time fig 

effect long term changes bandwidth gest segmentation methods provide flexibility playback high quality stream iterations allowing client trade startup latency quality 
discuss congestion control streaming applications focus rate adaptation 
variations transmission rate long lived session result client buffer overflow underflow 
quality adaptation complementary schemes prevents buffer underflow overflow effectively utilizing available bandwidth 
feng propose adaptive smoothing mechanism combining bandwidth smoothing rate adaptation 
send rate shaped dropping low priority frames prior knowledge video stream 
meant limit quality degradation caused dropped frames quality variation predicted 
unfortunately technical information evaluation popular applications unavailable 
vii 
quality adaptation mechanism bridge gap short term changes transmission rate caused congestion control need stable quality streaming applications 
exploit flexibility layered encoding adapt quality long term variations available bandwidth 
key issue appropriate buffer distribution active layers 
described efficient mechanism dynamically adjusts buffer distribution available bandwidth changes carefully allocating bandwidth active layers 
furthermore introduced smoothing parameter allows server trade short term improvement long term smoothing quality 
strength approach comes fact assumptions loss patterns available bandwidth 
server adaptively changes receiver buffer state incrementally improve protection short term drops bandwidth efficient fashion 
simulation experimental results reveal small amount buffering mechanism efficiently cope short term changes bandwidth results aimd congestion control 
mechanism rapidly adjust quality delivered stream utilize available bandwidth preventing buffer overflow underflow 
furthermore increasing smoothing factor frequency quality variation effectively limited 
buffer requirements quality adaptation large believe mechanisms deployed non interactive live sessions client tolerate short delay delivery 
plan extend idea quality adaptation congestion control schemes employ aimd algorithms investigate implications details rate adaption mechanism 
study quality adaptation nonlinear distribution bandwidth layers 
interesting issue measurement approach adjust kmax fly history 
quality adaptation provides perfect opportunity proxy caching multimedia streams 
proxy cache low quality version stream gradually pre fetches higher quality layers demand driven fashion 
preliminary results show proxy effectively improve quality delivered streams high bandwidth clients despite presence bottleneck path server 
viii 
acknowledgments lee breslau thoughtful comments drafts 
microsoft service streaming media business www microsoft com basics 
real networks versus realaudio client server streaming www realaudio com help content vs ra html 
floyd fall promoting congestion control internet ieee acm transaction networking august 
girod aspects image communications signal processing vol 
pp 

rejaie handley estrin rap rate congestion control mechanism realtime streams internet proc 
ieee infocom march 
bolot turletti rate control mechanism packet video internet proc 
ieee infocom pp 
june 
ortega rate control video coding variable bit rate channels applications wireless transmission proc 
ieee international conference image processing october 
tan zakhor error resilient packet video internet proc 
ieee international conference image processing october 
lee kim ko motion prediction temporal layering layered video coding proc 
itc vol 
pp 
july 
mccanne scalable compression transmission internet multicast video ph thesis university california berkeley ucb csd december 
mccanne vetterli joint source channel coding multicast packet video proc 
ieee international conference image processing pp 
october 
chou efficient algorithm hierarchical compression video proc 
ieee international conference image processing november 
rejaie yu handley estrin multimedia proxy caching mechanism quality adaptive streaming applications internet proc 
ieee infocom march 
rejaie architecture quality adaptive streaming applications internet ph dissertation department computer science university southern california september 
bolot characterizing packet delay loss internet journal high speed networks vol 
pp 
september 
bajaj improving simulation network research tech 
rep usc cs 
li ammar paul layered video multicast retransmission evaluation hierarchical rate control proc 
ieee infocom 
mccanne jacobson vetterli receiver driven layered multicast acm sigcomm august 
wu sharma smith thin streams architecture multicasting layered video may 
merz wolf iterative transmission media streams proc 
acm multimedia november 
jacobs eleftheriadis real time dynamic rate shaping control internet video applications workshop multimedia signal processing pp 
june 
padhye kurose towsley tcp friendly rate adjustment protocol continuous media flows best effort networks tech 
rep umass 
schulzrinne loss delay adjustment algorithm tcp friendly adaptation scheme workshop network operating system support digital audio video july 
feng liu priority technique best effort delivery stored video proc 
multimedia computing networking january 
