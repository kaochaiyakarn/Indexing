utility exploiting idle workstations parallel computation anurag acharya guy joel saltz umiacs department computer science university maryland college park cs umd edu examine utility exploiting idle workstations parallel computation 
attempt answer questions 
workstation pool fraction time expect find cluster workstations available 
provides estimate opportunity parallel computation 
second stable cluster free machines stability vary size cluster 
indicates frequently parallel computation adapting changes processor availability 
third distribution workstation idle times 
information useful selecting workstations place computation 
fourth benefit user expect 
state concrete terms pool size big parallel machine expect get free harvesting idle machines 
benefit achieved real machine hard parallel programmer happen 
answer workstation availability questions analyzed day traces workstation pools 
determine equivalent parallel machine simulated execution group known parallel programs workstation pools 
gain understanding practical problems developed system support required adaptive parallel programs adaptive parallel cfd application 
exploiting idle workstations popular research area 
popularity fueled partly studies indicated large fraction workstations unused large fraction time partly rapid growth power workstations :10.1.1.14.7130
batch processing systems utilize idle workstations running sequential jobs production years 
known example condor operation university wisconsin years currently manages workstations 
utility harvesting idle workstations parallel computation clear 
results held promise free cycles assume implicitly progress execution workstation lack thereof effect progress execution workstations :10.1.1.14.7130
assumption hold parallel computation 
particularly data parallel programs written spmd style data parallel programs written spmd style 
workstation running parallel job reclaimed primary user remaining processes job allow computation reconfigured 
reconfiguration may need data repartitioning data process migration updating data location information 
progress parallel job requires group processors continuously available sufficiently long period time 
state large number processors rapidly oscillates available busy parallel computation able little progress processor available large fraction research supported arpa contract syracuse subcontract time 
second parallel programs perfectly parallel 
able run certain configurations example configurations powers processors 
addition deletion single workstation may effect small effect significant effect performance depending application requirements number available machines 
examine utility exploiting idle workstations parallel computation 
attempt answer questions 
workstation pool fraction time expect find cluster workstations available 
provides estimate opportunity parallel computation 
second stable cluster free machines stability vary size cluster 
indicates frequently parallel computation adapting changes processor availability 
third distribution workstation idle times 
probability workstation currently idle idle longer time 
information useful selecting workstations place computation 
fourth benefit user expect 
state concrete terms pool size big parallel machine expect get free harvesting idle machines 
benefit achieved real machine hard parallel programmer happen 
addressed questions different ways 
answer workstation availability questions analyzed day traces workstation pools different sizes workstations different locations college park berkeley madison 
determine equivalent parallel machine simulated execution group known parallel programs workstation pools 
gain understanding practical problems arise trying run parallel programs adaptive fashion developed system support allows programs detect changes environment adapt changes 
developed adaptive version computational fluid dynamics program measured actual performance ibm sp cluster workstations workstation availability traces mentioned sequential workload 
previous research idle workstations parallel computation taken approaches 
leutenegger sun analytic model approach study feasibility running parallel applications non dedicated workstation pool 
study simple synthetic models workstation availability parallel program behavior 
difficult draw behavior real parallel programs real workstation pools 
pruyne livny propose schemes master slave approach 
workstation task executed reclaimed task killed reassigned master different workstation 
problems approach 
parallel programs written master slave style 
second rewriting existing parallel programs master slave programs greatly increase total communication volume require large amounts memory master processor 
arpaci study suitability dedicated non dedicated workstation pools executing parallel programs 
take trace analysis approach base study workstation availability trace job arrival trace node cm partition suite data parallel programs 
results show workstation pool able process workload submitted node cm partition 
approach closest arpaci basic differences 
arpaci focus interactive performance parallel jobs assume time sliced scheduling policy 
deduce need interactive response presence large number short lived parallel jobs cm job trace 
large parallel machines run batch mode 
usually small number processors provided interactive runs 
better understand need interactive performance parallel jobs analyzed long term months year job execution traces supercomputer centers cornell maui san diego 
short lived jobs processors details see section 
take position need interactive response met small dedicated cluster throughput primary goal schemes utilize non dedicated workstations 
doing follow lead miron livny condor group university wisconsin excellent success utilizing idle workstations sequential jobs 
examine workstation availability questions 
describe traces metrics computed estimate opportunity parallel computation 
describe simulation experiments results 
describe experience implementation execution adaptive parallel program 
summary 
workstation availability determine availability free workstation clusters parallel computation analyzed week traces workstation pools 
traces computed metrics 
fraction time expect find cluster free workstations 
refer availability metric 
second long average cluster workstations stable 
long parallel computation running processors expect run 
refer stability metric 
addition computed measures trace 
fraction time workstation available average second number available workstations vary time 
measures comparison previous studies 
computed probability distribution idle times workstations study 
describe traces 
parallel availability metrics measures traces 
traces trace workstation cluster cad group uc berkeley contains data workstations 
trace covers day period 
trace received busy availability periods marked workstation 
trace arpaci 
extracted day segment largest number traced workstations 
refer trace ucb trace 
second trace condor workstation pool university wisconsin contains data workstations 
trace covers day period 
purpose trace workstation considered available condor status monitor marked available 
condor uses criteria including user preferences decide workstation available batch jobs 
collected trace sampling condor status information minutes web interface provided condor project 
refer wisc trace 
third trace public workstation cluster department computer science university maryland 
trace contains data workstations covers day period 
purpose trace workstation considered available load average minutes 
refer umd trace 
number workstations participating pools constant tracing periods 
average number participating workstations ucb trace wisc trace umd trace 
figures measure size corresponding pools 
addition variations size time period location pools vary way 
college park pool consists publicly available machines primarily junior computer science graduate students course assignments personal purposes mail 
berkeley pool consists workstations belonging single research group personal purposes compute intensive research 
madison pool includes compute servers personal workstations 
spans departments 
expect pools representative workstation clusters available university environments 
parallel availability metrics presents availability metric pools 
graph shows metric varies cluster size 
pool fraction time cluster workstations available drops linearly note pool substantial fraction pool available 
umd trace drop relatively slow clusters larger half total size pool available half time 
presents stability metric pools 
graph shows metric varies cluster size 
graphs show clusters half size pool stable fifteen minutes clusters third pool stable minutes 
holds promise parallel applications 
cost reacting reclamation event high minute possible significant progress 
important point note shows large workstation clusters available time clusters stable 
example cluster fraction time available cluster size umd fraction time available cluster size ucb fraction time available cluster size wisc availability pools 
graphs show fraction time expect find cluster free workstations fraction varies cluster size comparison average number participating workstations ucb wisc umd 
average lifetime seconds cluster size umd average lifetime seconds cluster size ucb average lifetime seconds cluster size wisc stability pools 
graphs plot average period cluster stable cluster size 
workstations wisc pool cluster workstations stable half minutes see 
right graphs ucb umd traces correspond small number idle periods weekend nights 
shows fraction workstations idle varies time pools 
weekdays indicated dips nights humps weekends shorter usual dips 
graph horizontal line labeled avg shows average fraction pool available 
results indicate average workstations pool available 
agrees previous results :10.1.1.14.7130
distribution workstation idle time section try answer question probability idle workstation idle longer time 
question previously looked researchers 
common experience machines idle short time reclaimed machines idle relative long period 
douglis ousterhout mention cluster machines idle seconds idle average minutes arpaci mention study recruitment threshold minutes provided best throughput 
relative plenty terms workstation availability focus issue recruitment 
looked distribution relatively long idle periods tens minutes hours 
goal help select multiple available workstations placement computation 
fraction machines available time seconds umd avg fraction machines available time seconds ucb avg fraction machines available time seconds wisc avg fraction workstations available pools 
fraction workstations length idle period umd fraction workstations length idle period ucb fraction workstations length idle period wisc cumulative distribution idleness cutoff pools 
workstation occurring workstation availability traces computed probability idle period longer time considered idle periods minutes long 
probability distribution varied widely 
summarize information characterized workstation time 
refer measure idleness cutoff 
idle periods shorter probability greater half idle periods longer probability half 
minimum value idleness cutoff minutes maximum value hours 
shows cumulative distribution idleness cutoff 
average value idleness cutoff minutes ucb trace minutes umd trace minutes wisc trace 
large value idleness cutoff simple strategies lifo fifo random selecting available workstations suffice 
note values significantly higher minutes reported douglis sprite workstations 
benefit user expect 
estimate benefit parallel programs achieve shared workstation environments simulated execution group known parallel programs pools 
selected suite programs includes nas parallel benchmarks programs studied research groups working parallel processing 
simulated scenarios repeated execution individual applications gaps repeated execution entire set applications gaps 
scenarios keep pool busy times provide approximate upper bound throughput 
equivalent parallel machine metric 
describe programs benchmarks 
describe simulations information drive 
results 
benchmarks programs suite programmed spmd model 
shows speedups benchmarks running dedicated parallel machines 
numbers obtained publications 
programs described 
class datasets nas benchmarks 
ffl nas bt program uses approach block tridiagonal matrices solve navier stokes equations 
running time processor ibm sp seconds total memory requirement gb 
program runs configurations square number processors 
ffl nas sp program uses matrix algorithm navier stokes equations 
running time processor ibm sp seconds total memory requirement mb 
program runs configurations square number processors 
ffl nas lu program uses block lower triangular block upper triangular approximate factorization solve navier stokes equations 
running time processor ibm sp seconds total memory requirement mb 
program runs configurations powers oftwo processors 
ffl nas mg implements multigrid algorithm solve scalar discrete poisson equation 
running time processor ibm sp seconds total memory requirement mb 
program runs configurations powers processors 
ffl nas program solves poisson partial differential equation fft algorithm 
running time sixteen processors ibm sp seconds total memory requirement gb 
program runs configurations powers processors 
ffl dsmc monte carlo simulation study flow gas molecules dimensions 
running time processor ipsc seconds total memory requirement mb 
ffl unstructured flow solver capable solving navier stokes equations complex geometries unstructured grids 
running time processor intel paragon seconds total memory required mb 
data program running processors 
ffl hydro parallel implementation dimensional relativistic hydrodynamics 
running time processor intel delta seconds total memory required mb 
data program running processors 
simulations compute equivalent parallel machine scenarios mentioned performed high level simulation execution spmd parallel programs non dedicated workstation pools 
simulator takes input workstation availability traces description parallel programs simulated computes total execution time 
equivalent parallel machine measure computed determining size parallel machine required complete execution time 
programs study run fixed set configurations powers 
execution time falls configuration linear interpolation compute equivalent parallel machine 
programs study iterative 
purpose study characterize speed execution time taken execute iteration 
obtained time iteration publications cited 
characterize size process size partition program speedup number processors nas bt nas sp nas lu unstructured dsmc hydro nas mg ideal speedups benchmark programs dedicated parallel machines 
data 
obtained size program data code inspection benchmarks 
programs obtained size information publications 
benchmarks large data sets reasonably fit memory single workstation 
assumed workstations mb perform experiments required amount machines 
ways spmd program adapt change number available processors 
example checkpoint evicted process disk restart condor process copy memory memory locus copy stack small number pages fault rest lazily accent sprite 
schemes involve moving executing process 
spmd applications run multiple copies program usually loose synchrony possibly cheaper alternative 
just program data process moved scratch data text need moved 
sufficient workstations available data moved processes running program pay cost starting new process new location cost specific scheme expanding number processors requires new processes 
points note 
adaptation happen data clean state part code processor reach 
usually means outside parallel loops 
second process startup cost includes cost recomputing communication schedules 
study assumed adaptation technique 
simulator assumes point point mb link interconnect 
models eviction cost parts fixed eviction cost consists process startup cost variable part includes memory copy cost ends time wire point congestion data motion required eviction 
process startup cost paid account initialization time 
paid time application adapts change processor availability 
ms mb memory copy cost obtained empirically dec alpha server running digital unix 
simulator models settling period completion program start 
settling period seconds 
idle workstations relatively plentiful goal simple scheduling strategy possible 
study arpaci focus interactive performance parallel jobs assume time sliced scheduling policy 
deduce need interactive response presence large number short lived parallel jobs cm job arrival trace 
parallel machines run batch mode 
better understand need interactive response parallel jobs analyzed long cumulative fraction jobs number processors cornell cumulative fraction jobs number processors maui cumulative fraction jobs number processors san processor usage distribution short lived jobs 
cornell results jobs executed jun dec maui results jobs executed jan aug san diego results jobs executed jan dec 
total number short lived jobs san diego maui cornell 
average number short lived jobs day respectively 
term months year job execution traces supercomputer centers cornell maui san diego 
shows processor usage distribution short lived jobs jobs run minutes traces 
cases short jobs run sixteen processors 
experience parallel machines speculate interactive performance usually desired debugging testing purposes production runs batch jobs 
take position need interactive response met small dedicated cluster throughput primary goal schemes utilize non dedicated workstations 
doing follow lead miron livny condor group university wisconsin excellent success utilizing idle workstations sequential jobs 
study assume simple come served batch scheduling policy 
ran experiments week simulated time 
allowed study long term throughput understand effect time day day week variations workstation usage 
results table presents equivalent parallel machine implied performance different applications week long runs 
computed aggregate measures average equivalent machine median equivalent machine 
median measure computed avoid possible bias due outliers 
results conclude harvesting idle workstations pools provide equivalent college park berkeley madison dedicated processors 
measures berkeley pool match rule thumb arpaci suggest parallel machine equivalent non dedicated workstation pool 
rule match results clusters 
rule difference scheduling strategies primary cause difference large quantum eliminate effects time slicing 
believe difference due limited configuration effect difference load characteristics 
limited configuration effect refers fact parallel programs run certain configurations 
addition deletion single workstation may effect small effect significant effect performance depending application requirements number available machines 
effect particularly important number available workstations hovers magic numbers powers squares 
shows temporal variation performance period experiment 
benchmark programs run widely varying periods possible compute aggregate number 
selected nas bt exemplar program 
obvious diurnal variations graphs show impact limited configuration effect 
sharp changes performance workstation availability crosses certain thresholds 
note benchmarks nas bt run maximum number configurations runs square number processors 
point note difference nature graphs umd ucb hand graph wisc hand 
graphs umd ucb jagged graph wisc consists thick college park berkeley madison average proc average proc avail applications dsmc hydro nas bt nas nas lu nas mg nas sp unstructured roundrobin average par mc median par mc table average application equivalent parallel machine week 
process startup time assumed seconds 
fraction parentheses ratio equivalent parallel machine size pool 
equivalent parallel machine time seconds umd equivalent parallel machine time seconds ucb equivalent parallel machine time seconds wisc variation equivalent parallel machine week 
nas bt exemplar 
band 
indicates workstation availability magic numbers forced switches different configurations 
thick band indicates workstations plentiful program change configurations 
workstation taken away replacement available 
deep dip middle graph ucb corresponds period intensive see corresponding availability graph 
impact change eviction cost experiments described assumed process startup time seconds 
recall process startup time fixed portion eviction cost 
includes cost initiating adaptation cost starting new process need cost recomputing communication schedules 
cost depends particular adaptation mechanism 
determine impact eviction cost performance repeated experiments wide range process startup costs 
shows equivalent parallel machine varies process startup cost 
graph plot performance achieved applications dsmc nas bt nas lu nas mg 
performance applications lies approximately curves dsmc nas bt nas lu 
observations equivalent parallel machine cost process startup seconds dsmc nas bt nas lu nas mg equivalent parallel machine cost process startup seconds dsmc nas bt nas lu nas mg equivalent parallel machine cost process startup seconds dsmc nas bt nas lu nas mg college park berkeley madison variation equivalent parallel machine process startup cost 
equivalent parallel machine time seconds nas bt equivalent parallel machine time seconds nas lu equivalent parallel machine time seconds nas impact configuration flexibility 
performance nas mg drops sharply pools relative drop performance applications largest wisc followed ucb umd drops umd quite small 
primary cause sharp drop performance nas mg runs short time 
total execution time seconds single processor seconds processors 
result performance nas mg swamped startup costs 
gradation performance difference pools attributed differences frequency reclamation events 
impact configuration flexibility examine effect configuration flexibility compared performance single pool programs nas bt nas lu nas different levels configurability 
selected berkeley pool comparison configuration flexibility maximum impact situations relatively small number processors relatively frequent 
programs nas bt runs square number processors run powers processors 
dataset nas large run configurations smaller processors 
effect configuration flexibility seen parts graph apparent central dip 
programs able salvage computation time period nas bt successful run processors 
hand nas virtually progress period 
point period question order days 
num processors dataset ms ms ms ms ms dataset ms ms ms ms ms table time iteration datasets 
evaluation real machine gain understanding practical problems arise trying run parallel programs adaptive fashion developed system support allows programs detect changes environment adapt changes 
developed adaptive version computational fluid dynamics program measured actual performance ibm sp cluster workstations workstation availability traces mentioned sequential workload 
system called finch uses central coordinator keep track workstation availability application manager process keeps track progress application 
central coordinator resembles condor central manager runs central machine 
application manager created job submitted lives duration job 
runs submitting machine 
global resource allocation decisions central coordinator coordination application processes purpose adaptation done application manager 
currently assume cooperative user environment provide pair programs primary user workstation workstation available reclaim personal 
user requests reclamation sent central coordinator selects application respond event 
informs corresponding application manager coordinates response 
finch portable unix environments 
currently runs alphas rs 
study template extracted computational fluid dynamics application solves thin layer navier stokes equations surface 
iterative spmd program iteration corresponds different timestep 
chose top time step loop safe point eviction 
reclamation request received program point eviction delayed till processes reach point 
described section additional delay introduced program quite small 
adaptive parti library university maryland parallelizing application 
library performs data partitioning normal execution repartitioning adaptation 
manages normal data communication data motion needed eviction 
achieve efficient communication library pre computes communication schedules 
changing number identity processors requires recomputation schedule 
adaptive parti unique providing services 
drms system ibm research provides similar functionality 
point support implemented parallel programmer 
needed changes program allow run adaptive fashion 
added call initialization code includes contacting central coordinator resources 
second added code top time step loop check adaptation events call adaptation routine check succeeds 
third wrote adaptation routine data arrays moves destination nodes 
added call finalization routine things informs central coordinator completion program 
evaluated performance finch application processor ibm sp workstation pool workstation availability traces college park pool sequential workload 
ran program powers configurations sixteen processors 
input datasets experiments different meshes 
table shows time iteration different configurations 
designed experiments allow compute measures 
cost running adaptive version adaptation required 
second time eviction 
time user wait workstation reclamation request 
divided time num processors dataset dataset table slowdown relative non adaptive version 
workstation pool assumed unused period experiment 
num src proc num dest proc remap time ms ms ms ms ms ms ms ms ms ms table application level cost adaptation dataset 
parts 
part consists time spent application time repartition move data compute new communication schedules second part consists time spent central coordinator application manager 
computed equivalent parallel machine 
table shows slowdown adaptive version code compared original non adaptive version 
period experiment workstation pool assumed quiescent adaptation required 
note overhead adaptive version negligible 
understandable check adaptation event checking pending message socket 
rest adaptation code adaptations 
table presents application level cost adapting different configurations 
cost roughly proportional magnitude change number processors size data partition owned processor 
shows equivalent parallel machine copies program running 
experiments copy allowed start follow sequence 
copy assigned nodes wants start time copies compete remaining nodes nodes available computation 
result copy achieves better performance 
largest equivalent parallel machine processors dataset processors second data set 
corresponds size pool 
comparison equivalent parallel machine entire set umd traces computed see section 
average time user wait guest process leave depended number processors size data job guest process part 
single program running pool average wait time eviction seconds 
multiple programs running average wait time eviction seconds 
number adaptation events period experiment 
related considered idle workstations compute servers 
current growth number size data intensive tasks exploiting idle workstations memory attractive option 
dahlin study feasibility idle memory increase effective file cache size 
describe low level global memory management system uses idle memory back just file pages virtual memory 
show scheme able idle memory improve performance suite sequential data intensive tasks factor 
franklin describe unified memory management scheme servers clients client server database system 
goal avoid replication pages buffer pools clients buffer pools servers 
explicit memory servers proposed number processors number parallel programs parallel program parallel program parallel program parallel program number parallel programs parallel program parallel program parallel program parallel program equivalent parallel machine programs 
graph left dataset graph right second dataset 
iftode 
describe memory server similar spirit condor central manager 
keeps track idle memory available ships memory objects corresponding machines needed 
iftode propose extending memory hierarchy multicomputers introducing remote memory server layer 
harvesting idle workstations memory imposes fewer requirements system support harvesting computation 
done properly memory shared long periods significant impact interactive performance particularly today machines large primary memories 
eviction guest memory pages urgency eviction guest processes 
summary primary study 
significant utility harvesting idle workstations parallel computation 
considerable variance performance achieved 
non dedicated pools studied achieve performance equal dedicated parallel machine third size pool 
supporting evidence provided experience finch adaptive navier stokes template 
second parallel throughput achieved non dedicated pool depends characteristics sequential load flexibility parallel jobs run 
jobs run small number configurations able take advantage dynamic changes availability jobs run large set configurations achieve better throughput 
effect particularly important number workstations available hovers magic numbers powers squares 
study ffl average workstations pool available 
fraction time cluster workstations available drops linearly clusters larger half total size pool available half time 
substantial fraction workstations available 
ffl large clusters available time clusters stable 
clusters half size pool stable fifteen minutes clusters third pool stable minutes 
ffl wide variance distribution length idle periods different workstations 
expected length idle period varied minimum minutes maximum hours 
average workstation idle minutes expected idle minutes 
ffl difficult convert spmd programs run adaptive environment 
conversion benign 
modifications adverse impact performance programs 
useful gains possible real machines 
ffl eviction delay seen user unacceptably large 
caution reader scheme checkpointing unable recover failures 
acknowledgments arpaci uc berkeley workstation availability traces 
condor group university wisconsin providing web interface condor status monitor 
steven cornell theory center trace jobs submitted ibm sp regan moore george san diego supercomputing center trace jobs submitted intel paragon peter young maui high performance computing center trace jobs submitted ibm sp 
agarwal 
efficient algorithm fft nas parallel benchmark 
proceedings scalable high performance computing conference pages may 
arpaci dusseau vahdat liu anderson patterson 
interaction parallel sequential workloads network workstations 
proceedings acm sigmetrics joint international conference measurement modeling computer systems pages may 
smith 
scalable high performance environment fluid flow analysis unstructured grids 
proceedings supercomputing pages november 
carriero gelernter kaminsky 
piranha scheduling strategies implementation 
international journal parallel programming feb 
condor status monitor 
www cs wisc edu cgi bin condor status server 
condor summary status monitor 
www cs wisc edu cgi bin condor status server tot 
dahlin wang anderson patterson 
cooperative caching remote memory improve file system performance 
proceedings symposium operating system design implementation pages nov 
douglis 
transparent process migration sprite operating system 
phd thesis computer science division department electrical engineering computer sciences university california berkeley sep 
fred douglis john ousterhout 
transparent process migration design alternatives sprite implementation 
software practice experience august 
agrawal sussman saltz 
data parallel programming adaptive environment 
proceedings ninth international parallel processing symposium pages april 
morgan karlin levy thekkath 
implementing global memory management workstation cluster 
proceedings fifteenth acm symposium operating system principles pages dec 
franklin carey livny 
global memory management client server dbms architectures 
proceedings eighteenth international conference large data bases pages aug 
iftode li petersen 
memory servers multicomputers 
compcon spring digest papers pages feb 
leutenegger 
sun 
distributed computing feasibility non dedicated homogeneous distributed system 
proceedings supercomputing pages november 
litzkow livny 
experiences condor distributed batch system 
proceedings ieee workshop experimental distributed systems pages oct 
moreira 
programming environment dynamic resource allocation data distribution 
technical report rc ibm research may 
matt mutka miron livny 
available capacity privately owned workstation environment 
performance evaluation july 

remote memory resource distributed systems 
proceedings third workshop workstation operating systems pages april 
david nichols 
idle workstations shared computing environment 
proceedings eleventh acm symposium operating systems pages november 
popek walker 
locus distributed system architecture 
mit press 
pruyne livny 
parallel processing dynamic resources 
proceedings workshop job scheduling strategies parallel processing pages april 
woo 
nas parallel benchmarks results 
technical report nas nasa ames research center august 
sharma moon hwang das saltz 
runtime compile time support adaptive irregular problems 
proceedings supercomputing pages november 
tannenbaum litzkow 
condor distributed processing system 
dr journal feb 
marvin theimer keith 
finding idle machines workstation distributed system 
ieee transactions software engineering november 
dean strayer 
spline methods hydrodynamic equations parallel implementation 
proceedings sixth siam conference parallel processing scientific computing pages march 

development flexible efficient multigrid flow solver aiaa 
proceedings st aerospace sciences meeting exhibit january 

copy process migration system 
phd thesis department computer science carnegie mellon university pittsburgh pa april 

