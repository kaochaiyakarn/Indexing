bi directional conversion graphemes phonemes joint gram model statistical model languageindependent bi directional conversion spelling pronunciation joint grapheme phoneme units extracted automatically aligned data 
model evaluated spelling pronunciation pronunciation conversion nettalk database cmu dictionary 
study effect including lexical stress pronunciation 
direct comparison difficult model performance appears better data driven approaches applied tasks 

problem spelling pronunciation conversion studied large extent context text systems usually require intermediate phonemic representation 
significant amount research conversion stemmed psycho linguistic models reading aloud 
reverse problem pronunciation spelling conversion received far attention 
new impetus research automatic techniques arisen connection advances large vocabulary continuous speech recognition especially spontaneous speech recognition important find way dealing words outside recognizer vocabulary pronunciation variants differ known baseforms 
technology transferred new languages increasingly important find automatic techniques avoid long cycles development maintenance pronunciation dictionaries expert lexicographers 
conversion techniques fall major categories rule systems pronunciation analogy pba systems 
derive pronunciations dictionary lookup fails successively applying carefully crafted pronunciation rules input word 
pba systems measure similarity words retrieve partial pronunciations fragments input word concatenated obtain final pronunciation 
excellent review critique approaches knowledge gives extension term grapheme sense functional spelling unit corresponding single phoneme cf 

principle allow possibility grapheme may correspond multiple phonemes order handle insertions pronunciations single letters 
james allen department computer science university rochester james cs rochester edu pba framework applied conversion 
contains extensive review different techniques comparative evaluation 
comparatively fewer studies problem 
interesting approach hierarchical framework incorporating levels linguistic description bi directional conversion 
bi directional models include evaluations 
describes hmm approach problem 
statistical finite state transduction methods datadriven techniques attempt infer probabilistic transformation rules large databases spelling pronunciation pairs 
continues tradition 
specifically developed probabilistic techniques align spellings pronunciations automatically 
resulting alignments set grapheme phoneme gp correspondences terminology induced 
gram model trained data aligned set correspondences applied bidirectional conversion spelling pronunciation 
section theoretical formulation model 
devote describing model design evaluation methodology results evaluation 
conclude discussion relate model performance approaches 

bi directional model 
inferring correspondences alignment step building pronunciation model spelling pronunciation word training data need aligned 
current approaches problem assumption alignment letters phonemes available 
trick phoneme set augmented null pseudo phoneme special double phonemes standing groups phonemes correspond letter 
arbitrariness data manipulations see example arguments correspondences problematic technical point view data aligned strict basis difficult obtain doing automatically introduces level imprecision 
seen section possible phonemes corresponding letter english course language dependent issue 
decided avoid problems automatically aligning phonemic strings input 
resulting grapheme phoneme correspondences constrained contain letter phoneme 
set correspondences associated probability distribution inferred version em algorithm 
due space limitations refer reader detailed exposition learning process 
examples correspondences induced data sets section 
alignment procedure fully automatic purely statistical linguistic information phonetic categories require seeding manually designed correspondences readily applied languages 
time applied english 

joint gram model gram models successfully speech recognition applications statistical modelling sequences data 
flexible compact fast decoding algorithms exist 
gram model described predicts words syntactic semantic tags simultaneously 
analogously define pronunciation model joint grapheme phoneme pairs 
idea adopted stochastic grammar approach particular instantiation fact bigram model multigram model 
formally spelling word pronunciation letters phonemes associate joint probability 
task formulated finding input transcription arg max similarly task formulated finding input transcription arg max joint gp correspondence model gp alignment word unit correspondence grapheme phoneme unit define probability conditional probability distributions represents joint grapheme phoneme probability model 
express terms summing possible alignments allowed set gp correspondences denote set allowable alignments 
combining obtain equations tasks respectively joint grapheme phoneme probability model arg max arg max bi directional joint gram model obtained restricting size contexts conditional probability joint gram probabilities estimated aligned data standard maximum likelihood approach 
decoding space possible alignments match input searched done efficiently ngram models summation equations may expensive 
common approach replace summation maximization 
viterbi algorithm accomplish search efficiently 
common problem associated gram models number probabilities estimated large large amount data needed training 
isn sufficient data training model smoothed lower order models classbased models :10.1.1.13.9919
deficiency gram models context span fixed quite short 
linguistic phenomena modelled grams may involve long range dependencies 
typical solution problem augment model vocabulary multi unit tokens may designed manually automatically 

evaluation technique just described languageindependent far conducted evaluation english 
sets data nettalk manually aligned dictionary cmu pronunciation dictionary 
chosen techniques tested data evaluation conditions may differ approach may serve indication stand compared state art 
second data set larger contains abbreviations proper names loan words 
dictionary primarily designed continuous speech recognition incorporates deviations standard baseforms allophonic variations deletions insertions 
part dictionary manually 
dictionary provides opportunity comprehensive realistic testing especially conversion 
interested predicting accented phonemes 
dictionaries include stress markers pronunciation nettalk transcriptions stress markers assigned null phonemes 
difficult correct deficiency results comparable literature 
tested conversion graphemes accented phonemes cmu dictionary doesn problem 
ultimate measure performance conversion tasks word accuracy wacc transcription counted correct coincides dictionary 
decision may possible correct outputs case homographs 
model encode syntactic semantic information words required making decision 
decided common ad hoc approach removing homographs data 
statistical model output hypotheses close scores best list left modules natural language system decide best fits linguistic constraints part speech 
scoring strategy word pronunciations test set result conversion matched yielding minimal phoneme error rate 
similar strategy applied conversion pronunciation correspond different spellings choosing word minimal letter error rate ler 
symbol error rate defined number total errors deletions insertions substitutions number symbols 
authors prefer report symbol accuracy proportion symbols correct number symbols provide phoneme accuracy letter accuracy results 

experiments nettalk dictionary nettalk dictionary publicly available originally developed manually aligned sejnowski rosenberg training nettalk neural network 
contains words multiple pronunciations total number entries 
phone set uses phonemes including null phoneme double phonemes insure correspondence letter string phoneme string 
stress markers markers provide syllabic information 
detailed description symbols 
evaluation split vocabulary randomly selected disjoint test sets words included word pronunciations nettalk dictionary 
corresponding test set training set formed including unique entries dictionary rest words 
null phoneme eliminated pronunciations 
gp correspondences obtained database 
units inferred training sets units typically 
examples correspondences table 
upper part table contains straightforward obviously correct correspondences lower part contains obviously wrong correspondences resulting incorrect alignments 
noted notion correct correspondence ill defined rely performance correspondence model conversion tasks quality measure 
correspondence model aligning data adapted version viterbi algorithm 
back gram model gp model witten bell discounting trained aligned data cmu cambridge toolkit 
cross evaluation results conversion tasks shown table 
closed test performance included model estimated data set 
high word accuracy shows grams able describe quite information encoded dictionary having potential lexicon compression 
studies vowel quality confusion leading source errors conversion 
substitution vowel responsible full absolute word errors 
nettalk cmu ah che ah xi ow ist ah eb eh iy eh ic iy table examples gp correspondences inferred nettalk cmu dictionaries 
wacc closed gp wacc ler closed gp table closed open test results nettalk task 

class smoothing classes correspondences obtained greedy algorithm automatic classification described 
implementation srilm toolkit 
gram class model built interpolated baseline gram 
assigned weight class model 
training testing done pairs evaluation data 
table shows results smoothing classes correspondences training set 
gp model baseline slightly lower word accuracy compared cross evaluation average reported table indicates particular test set larger proportion difficult words 
class interpolated models indicated number classes 
wacc gp wacc ler gp table results class smoothed models 
small improvement seen measures task significant improvement task 
appears beneficial classes small number units smoothing may occur 

chunk models speech recognition including number phrases multiword units vocabulary gram model shown lead improvement 
useful phrases automatically iteratively gluing promising sequences units 
various techniques measure promising sequence called stickiness measures bigram frequency mutual information change bigram log likelihood 
pilot experiments tested usefulness augmenting vocabulary basic model units chunks 
experiment bigram frequency stickiness measure second mutual information mi 
case training data processed replace appropriate correspondence sequences chunks 
estimated chunk gram models interpolated equal weights baseline model 
note passing interpolated chunk model theoretically multigram model estimated differently 
unfortunately results chunk models disappointing 
frequency models lower performance mi ones 
task mi chunk models slightly worse baseline 
task adding mi chunks showed increase performance small 
experiments compelling believe careful admittedly timeconsuming joint optimization number chunks interpolation parameters technique eventually prove useful 
encouraged fact chunks included number morphologically meaningful units affixes ize ing ment inter ism words appear compounds man hand form port frequently occurring letter units special pronunciation qu ng 
frequent sequences predicted baseline model preliminary analysis conclude augmenting vocabulary chunks low frequency sequences high mutual information fact beneficial 

models correspondences results situate approach best applied task 
due fact model meaningful gp correspondences fact model represents jointly phonemic dependencies 
final set experiments compared model joint gram model correspondences model 
estimation procedure exactly model gp 
correspondences thoroughly checked hand gives opportunity check automatically learned gp correspondences really 
order asses benefits extra information built gram models included training data stress markers model second stress markers syllable markers model ss 
trained tested models task pairs evaluation data 
order compare performance models gp model markers null phonemes removed output ss models scoring 
results shown table 
compared gp model models slightly consistently lower word accuracy phoneme accuracy error rate practically 
results allow conclude gp correspondences inferred reliable occasional errors probably don big impact results quite erroneous correspondences occur data 
models performed quite context gives confidence power approach resides modeling joint dependencies 
wacc gp ss table comparison model inferred correspondences models manually designed correspondences 
experiments cmu dictionary cmu pronunciation dictionary consists words total pronunciations 
phone set composed phones 
lexical stress indicated stress markers stress primary stress secondary stress 
reserved words testing different pronunciations stress included rest dictionary training 
dictionary contains critical words uncommon pronunciations reason approaches evaluated showed poor performance relative results english dictionaries 
chose remove words corpus 
training data inferred sets gp correspondences non accented phonemes accented phonemes 
correspondence sets included units respectively 
examples table 
seen units nettalk dataset 
due large number proper names foreign origin abbreviations non standard pronunciations include various phonological deviations baseforms 
give examples uncommon correspondences dictionary entries occur hh ill iy aa er dr uw graduate table contains example correspondence containing 
non alphabetic character allowed removed 
constrained correspondences allow sole component part attach neighboring graphemes 
built back gram models witten bell discounting viterbi aligned training data 
open test results models tables 
gp models units non accented phonemes gp models units accented phonemes 
class smoothed gp model classes showed improvement basic model task absolute wacc improvement task 
cmu task difficult nettalk task availability large amount data training significantly higher results conversion 
true conversion 
fact phoneme appears correspondences task difficult 
remarkable including stress information training data improved word accuracy conversion table 
availability stress information input conversion help little 
caused large number insertion errors conversion responsible close absolute word errors 
wacc gp wacc ler gp table open test results cmu task non accented phonemes 
wacc gp table open test results cmu task accented phonemes training data non accented phonemes testing 
wacc gp wacc ler gp table open test results cmu task accented phonemes 

discussion lack standardized data sets evaluation methodology difficult compare performance joint gram model approaches 
attempt relate model state art 
provide opportunity comments differences evaluation methodology various approaches indicate strengths model 
word accuracy conversion obtained nettalk comparable best results obtained database 
word accuracy reported model decision trees id 
authors acknowledge test set development may lead overfitting test set 
word testing rest training 
pba approach overlapping chunks achieves wacc fold cross evaluation words 
model backing guarantees word receive pronunciation 
similar pba model described wacc obtained combined scores different scoring strategies 
optimal combination chosen post hoc 
best single strategy fared 
study homographs letter words removed purpose evaluation 
wacc reported decision tree model cart model benefited smoothing techniques rescoring trigram phonotactic model 
noticed output errors stemming inappropriate vowel stress patterns phonotactic model help correct errors substitution errors 
similarly reverse conversion interesting see model may 
cmu dictionary reports wacc task including accented phonemes decision tree id model 
model performance task 
authors hand designed set correspondences failed provide alignments relatively large part dictionary entries removed dictionary prior evaluation 
mentioned cart model achieved wacc non accented phonemes top frequent words dictionary nab news corpus 
approach know tested data earlier smaller release removing critical words statistical model described 
model factored phonotactic model matching model mixture models encoding different context dependencies 
wacc obtained model low compared performance joint ngram considering fact model trained data 
may indication joint model better alternative knowledge technique applied nettalk data cmu dictionary conversion 
includes experiments direction nettalk technique described unrealistic assumption null phonemes input 
ways continued way 
avenue explore word frequency training joint grams 
pilot study nettalk data word frequency information may lead absolute increase word accuracy 
test approach languages order better asses language independence feature compare systems tested nettalk cmu dictionaries 
possibilities mentioned 
intend apply context speech recognition system recognizing vocabulary words generating pronunciations specialized domains 
ample room optimizing structure parameters models 
appears joint ngram model reached level performance best data driven approaches reported literature 

described joint grapheme phoneme gram model statistical model bi directional spelling pronunciation conversion automatically inferred minimal correspondences graphemes phonemes 
method fully automatic language independent builds successes achieved gram models areas language processing 
initial evaluation showed approach compares favorably data driven techniques 
expect see improvements results looking better models joint ngram framework 

reported supported onr research 
darpa research 

acknowledge active role jeff adams paul preliminary phase leading author worked summer intern 

coltheart writing systems reading disorders 
henderson ed reading pp 

london uk lawrence erlbaum 
glushko organization activation orthographic knowledge reading aloud journal experimental psychology human perception performance 
marchand damper multi strategy approach improving pronunciation analogy 
computational linguistics 
damper marchand adamson gustafson evaluating pronunciation component text speech systems english performance comparison different approaches 
computer speech language 
meng hierarchical lexical representation pronunciation pronunciation generation speech communication 
luk damper stochastic transduction english 
computer speech language 
sharman bi directional model english 
proc 
eurospeech pp 

kokkinakis language independent probabilistic model automatic conversion phonemic transcription words proc 
eurospeech pp 

deligne bimbot variable length sequence matching phonetic transcription joint multigrams 
proc 
eurospeech 
kokkinakis efficient multilingual phoneme grapheme conversion hmm 
computational linguistics 
damper pronunciation analogy impact implementational choices performance 
language speech 
pagel black letter sound rules accented lexicon compression 
proc 
icslp sydney australia 
dempster laird rubin maximum likelihood incomplete data em algorithm roy 
stat 
soc ser 
vol 
pp 

adams allen sub word language modeling technical report university rochester forthcoming 
augmenting words linguistic information gram language models 
proc 
eurospeech 
viterbi error bounds convolutional codes asymptotically optimum decoding algorithm ieee transactions information theory vol 
pp 

jelinek self organized language modeling speech recognition 
waibel lee editors readings speech recognition pp 

morgan kaufmann publishers 
brown della pietra desouza lai mercer class gram models natural language computational linguistics 
heeman deriving phrase language models 
proc 
ieee workshop 
sejnowski nettalk corpus phonetic transcription english words 
weide cmu pronunciation dictionary release 
carnegie mellon university 
bakiri dietterich 
achieving high accuracy text speech machine learning 
damper ed data mining speech synthesis chapman hall 
clarkson rosenfeld statistical language modelling cmu cambridge toolkit 
proc 
eurospeech pp 

stolcke srilm sri language modeling toolkit sri international 
ries waibel class phrase models language modeling 
proc 
icslp 
grapheme phoneme conversion multiple unbounded overlapping chunks 
proc 
pp 
jiang 
hon huang improvements trainable letter sound converter 
proc 
eurospeech pp 

statistical approach multilingual phonetic transcription 
philips journal research 
damper personal communication 
