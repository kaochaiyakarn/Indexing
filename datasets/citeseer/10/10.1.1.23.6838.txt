maximizing parallelism minimizing synchronization affine transforms amy lim monica lam computer systems laboratory stanford university stanford ca cs stanford edu presents algorithm find optimal affine transform maximizes degree parallelism minimizing degree synchronization program arbitrary loop nestings affine data accesses 
problem formulated imprecise data dependence abstractions data dependence vectors 
algorithm subsumes previously proposed program transformation algorithms unimodular transformations loop fusion fission scaling reindexing statement reordering 
multiprocessors popular important develop compilers automatically translate sequential programs efficient parallel code 
getting high performance multiprocessor requires finding parallelism program minimizing synchronization overhead 
synchronization expensive multiprocessor 
cost synchronization goes far just operations manipulate locks equivalent data structures 
processor may wait processors reach synchronization point 
modern multiprocessor processors execute number operations completion times vary greatly due memory subsystem 
furthermore high synchronization frequency generally accompanied high levels data communication processors reduce efficiency machine 
fact experience parallel applications multiprocessors indicates uncommon programs fine grain synchronization run slower serial counterpart 
important compiler find parallelism requires minimal synchronization 
words wish identify coarsest granularity parallelism program finding largest units independent computations research supported part air force materiel command arpa contract nsf young investigator award 
appears th annual acm sigplan sigact symposium principles programming languages paris france jan 
permission digital hard copy personal classroom granted fee provided copies distributed profit commercial advantage 
carried different processors synchronization 
lot research program transformations enhance program parallelism data locality 
domain techniques generally limited loops loop bounds array accesses affine functions loop indices 
forms program transformations proposed catalog survey bacon graham sharp 
key challenge remains combine transformations optimally achieve particular goals 
presents algorithm finds maximum degree parallelism general program domain arbitrarily nested loops array accesses affine index expressions 
parallelism coarsest granularity 
informally degree parallelism refers number nested parallel loops granularity parallelism measured length computation parallelized loop 
algorithm concept affine partitioning 
instances instruction identified loop index values surrounding loops affine expressions map loop index values partition number 
partition numbers different purposes ffl space partition 
operations belonging space partition mapped processor 
possible mapping assign operations partition processor ffl time partition 
operations belonging time partition execute partition 
execution order expressed loop ith iteration executes operations belonging partition algorithm finds combination affine space time partition mappings maximizes degree parallelism successively greater degrees synchronization 
problem formulated imprecise abstractions direction vectors commonly existing parallelizing compilers 
algorithm find parallelism coarsest granularity just degree needed exploit particular parallel hardware configuration 
affine partition mappings straightforward algorithm fourier motzkin elimination generate desired spmd single program multiple data code 
technique affine partitioning find loop level parallelism exposed combination previously defined transformations including ffl loop fission loop distribution ffl loop fusion ffl unimodular transforms interchange reversal skewing ffl loop scaling ffl loop reindexing index set shifting ffl statement reordering 
rest organized follows 
explain different forms parallelism problem statement section 
section give overview algorithm 
formally define model program define affine partition mappings section 
algorithms solving subproblems combined find maximum degree coarse grain parallelism 
discuss related conclude 
forms parallelism motivate problem addresses explain different forms parallelism available program simple examples 
granularity parallelism say program degrees parallelism units computations executed parallel number iterations loop 
example loop nest degrees parallelism transformed contain nested parallel loops 
loops degree parallelism may differentiated granularity parallelism illustrated examples contain degree parallelism 
example gamma example gamma gamma gamma example gamma gamma shows iteration space programs 
axis represents loop operation iteration represented point coordinates 
arrows represent data dependences operations program 
gray box groups computation assigned processor parallelization scheme 
thick lines represent barriers synchronizations 
iteration space data dependences different parallelization schemes outer sequential example fine grain coarse grain example example wavefront pipeline iteration space data dependences parallelization schemes examples 
example illustrates difference fine grain coarse grain parallelism 
possible finegrain parallelization schemes example execute row iterations parallel 
parallelization scheme hand specify different columns iterations execute parallel 
schemes expose degree parallelism allows processors run pace having execute rows lock step 
synchronizations needed enforce fine grain scheme synchronization needed coarse grain version 
example illustrates possible find synchronization free parallelism parallelism requires synchronization 
form loop level parallelism execute row time 
barrier needed ensure processors finished assigned iteration proceed row 
example example synchronization free parallelism 
example choice parallelization schemes 
wavefront computation example processors execute iterations diagonal parallel shown 
better choice pipeline computation 
example assigning row processor processor proceed soon neighbor assigned row finishes executing column 
pipelining benefits barriers reduced point point synchronizations processors need wavefront time spmd code implement pipelining simpler processors tend better data locality 
degrees nesting parallelism say different degrees parallelism loop nest exist nesting level require amount synchronization 
example deep loop nest iterations completely independent degrees parallelism 
number iterations loop processors simultaneously execute iterations participate barrier synchronization 
nesting structure original loop immaterial consider degrees parallelism case level 
hand degree parallelism loop nest requires synchronization say nested 
example nested levels parallelism follows example gamma gamma gamma iterations outermost loop performs computation example disjoint set data 
outermost loop trivially parallelizable allowing processors execute parallel synchronization 
wish keep processors busy assign iteration outermost loop set processors 
set processors participate barrier synchronizations iteration middle loop discussed 
example illustrates degrees parallelism may require different amounts synchronization prefer exploit outer levels parallelism minimize synchronization 
problem statement algorithm described locates degrees parallelism program finds maximum degree parallelism level granularity starting coarsest finest finds opportunities pipelined parallelism 
spmd code show assumes processor independent thread computation program 
generate code specific number processors simply combine multiple parallel threads assign processor 
processor assignment algorithm uses fine grain parallelism insufficient coarser grain parallelism keep processors occupied 
additional considerations data locality guide assignment threads belonging level granularity 
maximize flexibility processor assignment important locate degrees parallelism level granularity 
example blocking processor assignment scheme demonstrated improve locality applied loop nests degree parallelism 
overview algorithm decompose problem finding maximum degree parallelism subproblems maximize degree parallelism requires amounts synchronization respectively number iterations loop 
solving problems turn algorithm finds successively degrees parallelism higher synchronization costs 
steps repeated find parallelism requiring synchronization sufficient parallelism occupy available hardware 
subproblem maximizing synchronization free parallelism formulated dividing dynamic operations largest number independent partitions 
specifically algorithm finds affine partition mapping instruction maximizes degree parallelism instruction 
affine mappings subject set space partition constraints ensures processors executing operations different partitions need synchronize 
subproblem find parallelism synchronizations number synchronizations performed depend number iterations loop 
parallelization scheme may introduce synchronizations instructions program constant program 
algorithm divides instructions sequence stages 
algorithm locate synchronization free parallelism stage introduce barrier synchronizations parallelized stage 
find parallelism synchronizations wish find affine time partition mapping instruction 
affine mappings subject time partition constraints ensure data dependences satisfied executing partitions sequentially 
objective find affine mappings yield maximum parallelism operations time partitions 
space partition time partition constraints similar ways amenable kind techniques 
affine form farkas lemma transform constraints systems linear inequalities 
problem finding partition mapping gives maximum degree loop level pipelined parallelism satisfying space partition time partition constraints reduces finding null space system equations 
desired affine partition mapping easily set simple algorithms 
definitions introduce notations definitions rest 
define model program dependences affine partition mappings 
programs notation represent ith element vector represent subvector ith jth element vector 
empty vector 
domain algorithm set sequential programs consisting arbitrary nestings sequences loops array accesses affine functions outer loop indices loop invariant variables 
constructs conditional statements non affine array accesses handled treating conservatively 
definition program 
ffi ffl set instructions 
instruction indivisible unit simple arithmetic operation program variables 
instruction appears lexically instruction iff ffl ffi depth number surrounding loops instruction ffl ds ds ds derived loop bounds affine expression valid loop index instruction iff ds 
constant loop bounds represented additional symbolic variables 
ffl affine expression maps iteration array index computed rth access function instruction array ffl true iff rth access function instruction array write operation 
ffl ss number common loops shared instructions data dependences access patterns program define constraints program transformations 
notion data dependences understood definitions included completeness 
informally data dependence access function access function iff instance uses location subsequently accesses write operation 
data dependence set program contains pairs data dependent access functions program 
definition define oe lexicographically operator program 
ffi oe ss iff iteration instruction executed iteration ffi ffi ds oe ss ss ss ss ae ss ss ss ss literature dependences pairs instances satisfying constraint ss known carried loop definition data dependence set program 
ffi zs fi fi fi fi fi fi fi zs ffi ffi oe ss gamma zs ds better accuracy substitute symbolic variables representing constant loop bounds actual values 
affine partition mappings operations program identified loop index values 
algorithm uses affine expressions map vector original loop index values processor identification number iteration number sequential loop 
define affine partition mappings formally introduce properties algorithms 
definition dimensional affine partition mapping instruction program dimensional affine expression phi cs maps instance instruction indexed element vector 
dimensional affine partition mapping program phi phi phi phi number instructions definition rank affine partition mapping instruction phi cs cs rank matrix cs rank affine partition mapping phi program maximum ranks affine partition mappings instructions 
definition dimensional affine partition mappings instruction phi cs cs phi linearly dependent iff cs linearly dependent 
dimensional affine partition mappings phi phi program linearly dependent iff phi phi linearly dependent instructions synchronization free parallelism consider problem finding parallelism requires synchronization 
formulated dividing operations program partitions dependent operations placed partition 
assigning partition different processor synchronization needed processors 
seek partitioning described affine mapping instruction 
define necessary sufficient constraints affine partition mapping synchronization free 
describe algorithm solves constraints finds partition mapping highest rank possible solutions 
show rank partition mapping corresponds degree synchronization free parallelism 
algorithm finds maximum degree synchronization free parallelism 
space partition constraints symbolic constants incorporated algorithm treating loop variables 
coefficients affine partition mapping rational numbers 
dimension row partition mapping program may scaled integral values code generation phase 
definition space partition constraints data dependence set program 
ffi 
affine space partition mapping phi iff zs ffi ffi ds gamma zs phi gamma phi ds ds ds phi cs cs data dependence zs imposes constraints row synchronization free mapping instructions theta gammac gamma cs ds ds theta gammaf zs gamma zs linearizing constraints convert space partition constraints set linear equations algorithm algorithm convert constraints form system equations 
step simplify constraints successively removing variables equations 
variant gaussian elimination algorithm 
select equation express variable element terms variables equation 
substitute occurrences expression 
result step form xe vector variables step reduce system set linear equations affine form farkas lemma 
lemma affine form farkas lemma synchronization free parallelism necessary distinguish direction data dependence redundancy included constraints emphasize similarity definition section 
matter simple optimization eliminate redundant constraints 
pairs inequalities reduced equation eliminated 
affine expression define non empty polyhedron 
iff theta delta delta delta delta delta delta theta delta delta delta number rows positive constants existence asserted farkas lemma called farkas multipliers 
rewrite constraint xe gammax applying farkas lemma xe theta delta delta delta delta delta delta theta delta delta delta number rows holds xe theta delta delta delta delta delta delta theta delta delta delta unknowns new constraints delta delta delta interested apply elimination eliminate farkas multipliers obtain set constraints form xa renaming gammax applying farkas lemma fourier motzkin elimination get xa combining get set linear constraints xa solving space partition constraints constraints expressed equations techniques linear algebra find desirable partition mappings 
algorithm find highest ranked synchronization free affine partition mapping program 
ffi 
step construct data dependence space partition constraints dimensional partition mapping form 
step apply algorithm rewrite constraints system linear equations form vector variables representing unknown coefficients constant terms affine partition mapping 
step rank affine partition mapping dependent values constant terms eliminate unknowns elimination technique step algorithm 
simplified system represents unknown coefficients step find solutions expressed set basis vectors spanning null space step derive row desired affine partition mapping basis vector coefficients specified directly basis vector constant terms derived coefficients 
note basis vectors representing coefficients affine mapping instructions linearly independent 
rows affine mappings instruction necessarily linearly independent 
mapping may rows rank dimensionality processor space may larger degree parallelism 
algorithm finds parallelism step taken map partitions computations physical processors 
parallelism synchronization algorithm find degrees parallelism program step apply algorithm get highest ranked synchronization free affine partition mapping phi 
step generate spmd code processor executes partition phi instructions partition executed sequential order theorem algorithm finds maximum degree synchronization free parallelism 
proof synchronization free partition mapping rank creates independent partitions mapped different processors produce degrees synchronization free parallelism 
synchronization free affine partition mapping highest rank algorithm highest degree parallelism 
parallel program correct computations different processors independent operations executed processor follow original sequential order 
straightforward generate spmd code maximum degree synchronization free parallelism algorithm 
code generation algorithm irigoin polyhedron scanning code generation technique details examples 
ensure algorithm step finding maximum degrees parallelism synchronizations lemma lemma step approach finding synchronization free partitions finding maximum degree parallelism partitions maximizes degree parallelism program 
proof mapping operations different partitions processor decrease degree parallelism program 
example illustrate algorithm introduce slightly complicated example 
example gamma gamma subset dynamic operations example 
show iterations loops 
iteration innermost loop represented pair nodes white node representing operation instruction black representing operation instruction 
arrows represent dependences operations 
easy see best way parallelize code assign chain alternating black white nodes processor 
show step step algorithm systematically derives affine partition mappings describe optimal parallelization scheme 
step construct space partition constraints 
iteration space instructions loop body ae fi fi fi fi oe gamma gamma gamma gamma data dependence set contains pairs access functions fb fb fa fa fb fb fb gamma fb fb fb fa fa fa fa fa fa gamma theta derive data dependent pair fb fb constraint theta fb gammaf fb gamma fb theta gammac gamma similarly derive fa fa constraint theta fa gammaf fa gamma fa theta gammac gamma step apply algorithm rewrite constraints system linear equations 
constraints eliminate variables theta fb gammaf fb gamma fb gamma gamma gamma algorithm substitutes simplified constraint gamma gamma gamma gamma gamma gamma gamma apply farkas lemma eliminate farkas multipliers fourier motzkin elimination 
farkas lemma rewrites theta delta delta delta gammax theta delta delta delta final system equations farkas multipliers eliminated constraints applying similar steps yields step eliminate constant terms constraints 
column substitute gamma theta gamma gamma gamma gamma theta gammac resulting constraints coefficients gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma theta gammac step find solution space 
solution space null space matrix spanned vector theta gamma gamma step construct synchronization free partition mapping coefficients corresponding gamma 
setting get gamma space partition mapping phi phi gamma gamma phi gamma spmd code generated space partition mapping phi follows denotes processor id gamma gamma equation normalized gcd coefficients 
max min gamma gamma gamma gamma gamma gamma gamma gamma parallelism synchronizations find parallelism requires constant amount synchronization program dependence graph dynamic instances instruction represented single node 
definition data dependence set program 
ffi 
dependence graph node vs represents instruction vs iff zs lemma nodes strongly connected component scc program dependence graph share common surrounding loop nest 
proof instructions share common surrounding loop nest instances instruction executed instruction original program 
dependences instructions unidirectional instructions belong scc program dependence graph 
algorithm find degrees parallelism requiring synchronizations program step construct program dependence graph partition instructions sccs 
step apply algorithm scc find synchronization free parallelism 
step generate code execute scc topological order introduce barrier parallelized scc 
lemma step approach partitioning program sccs finding maximum degree parallelism scc maximizes degree parallelism program 
proof executing sccs parallel increase parallelism constant factor affect degree parallelism 
dependences individual sccs subset entire program sccs degrees parallelism entire program 
theorem algorithm finds maximum degree coarse grain parallelism uses synchronizations 
proof executing sccs topological order clearly honors dependences operations different sccs 
second algorithm introduces program constant number synchronizations bounded number instructions program dependent number iterations loop 
third lemma considering strongly connected components independently reduce degree parallelism program 
parallelization scheme assigns data dependent operations scc different processors requires synchronizations number iterations loop 
finding maximum degree synchronization free parallelism scc maximize parallelism requires synchronizations 
parallelism sequential loops consider programs synchronization free parallelism program dependence graph consists strongly connected component 
lemma exist outermost loop surrounds instructions program theorem available parallelism require synchronizations number iterations loop 
express parallelized version computation outermost loop multiple processors cooperate execute iteration parallel synchronize iteration ensure iterations executed sequence 
refer loops outer sequential loops 
goal partition operations operations iteration outer sequential loop admit largest degree parallelism 
specifically wish find affine mapping original loop index values operation iteration number new loop legal execute loop sequentially operations iteration largest degree parallelism 
define legal affine partition mapping executing partitions sequentially violate data dependence constraints 
calculate maximally independent solutions constraints 
show linear combination solutions yields affine mapping exposes degrees parallelism degree synchronization 
furthermore making maximally independent solutions rows affine mapping exploit possible degrees pipeline parallelism reducing degree parallelism 
time partition constraints wish partition operations iterations executing iterations sequentially violate data dependences 
words operation depends operation execute iteration executed iteration comes iteration executes time partition constraints simply relaxed version space partition constraints 
definition time partition constraints data dependence set program 
ffi 
dimensional affine partition mapping phi legal iff zs ffi ffi oe ss ds gamma zs phi gamma phi definition rewrite set linear time partition constraints zs ffi ffi ss ss ds gamma zs phi gamma phi exists legal affine partition mapping outer sequential loop time partition constraints trivially satisfied making iteration number outermost loop partition number 
furthermore clearly possible order operations partition executing partitions sequentially honors data dependences loop 
solving time partition constraints concept maximally independent partition mappings describe space solutions constraints show find possible legal partitions 
definition set legal dimensional affine partition mappings phi phi phi program 
ffi said maximally independent iff set minimal legal dimensional affine partition mapping phi expressed affine combination mappings legal phi kn phi phi phi delta delta delta kn phi algorithm find set legal maximally independent affine partition mappings outer sequential loop 
step steps algorithm rewrite system constraints 
affine partition mappings linearly independent depends coefficients eliminate constant terms fourier motzkin elimination 
simplified system represents coefficients step find maximal set linearly independent solutions algorithm brief algorithm introduces set new variables row theta gammai thetam size matrix theta obvious solution solution theta gammai thetam conversely solutions solutions values non negative 
basis solutions consists rows matrix theta thetan algorithm applies elementary row operations maximize number rows non negative values furthermore set rows non negative guaranteed linearly independent 
step solution step derive affine partition mapping 
coefficients directly constant terms derived 
parallelism synchronizations objective create affine partitions contains parallelism possible 
parallelism available partition determined data dependences operations partition 
placing pair data dependent operations different time partitions partition mapping dependence operations consideration parallelism partition 
generally desirable affine time partition mappings dismiss dependences possible maximize parallelism partition 
domain affine mappings objective maximizing degree parallelism sufficient seek affine mappings dismiss maximal set linear time partition constraints 
define notion constraint formally propose algorithm show algorithm optimal 
definition legal affine partition mapping linear time partition constraint imposed zs carried level iff ffi ffi ss ds gamma zs phi gamma phi algorithm find degrees parallelism requiring synchronizations outer sequential loop step apply algorithm program get set maximally independent partition mappings partition operations time phi phi phi generate code execute time partitions sequential order introduce barrier partition 
step apply algorithm time partition phi find degrees parallelism requiring synchronizations 
show algorithm finds parallelism requiring degree synchronization introduce lemmas 
lemma positive linear combination legal affine partition mappings legal affine partition mapping 
elementary row operations include interchanging rows scaling row adding scalar multiple row 
proof combinations affine partition mappings satisfy time partition constraints satisfy constraints 
lemma exists legal dimensional affine mapping linear time partition constraints dismissed legal dimensional mapping 
proof legal dimensional affine mappings phi phi dismiss different linear time partition constraints oe oe respectively 
construct legal mapping phi phi phi oe oe lemma legal dimensional affine partition mappings outer sequential loop linearly dependent yield degree parallelism level granularity 
proof phi phi legal dimensional linearly dependent affine partition mappings outer sequential loop instructions 
show phi yields degree parallelism phi showing converse symmetric argument prove phi phi degree parallelism 
partition phi consists instances different instructions fg set instances instruction phi phi linearly dependent set exists partition phi contains operations set 
phi maps operations different partitions 
subdivide operations partition numbers phi phi legal mapping legal execute sequentially order partition numbers 
exploit parallelism introducing gamma barriers 
outer sequential loop needs synchronizations exploit degree parallelism phi yields degree parallelism level granularity 
lemma phi phi phi set maximally independent legal affine partition mappings outer sequential loop dimensional affine mapping phi phi legal partition mapping partitions phi create largest degree parallelism 
proof phi legal dimensional affine partition mapping maximal set linear constraints lemma 
phi yield maximum degree parallelism dimensional legal mappings 
set legal maximally independent partition mappings phi phi phi kn phi phi phi delta delta delta kn phi max 
phi phi gamma phi phi phi gamma non negative gamma phi legal partition mapping lemma 
phi combination phi legal mappings phi legal maximal set linear constraints 
lemma phi phi linearly dependent maximal degree parallelism 
theorem algorithm finds maximum degree coarse grain parallelism degree synchronization outer sequential loop proof obvious definition executing time partitions sequentially honor dependences operations different partitions 
second steps algorithm introduce synchronizations 
lemma step algorithm finds legal mapping creates partitions largest degree parallelism theorem step finds parallelism uses synchronizations partition 
algorithm finds maximum degree parallelism requires degree synchronization 
exploiting pipeline parallelism linearly independent solutions time partition constraints partition mapping chosen algorithm equivalent ways exposing maximum degree parallelism program 
preferred approach fact mapping algorithm pipeline computation 
show pipelining achieve degrees parallelism advantage replacing barriers point point synchronizations regular code better data locality amenable blocking increase granularity parallelism constant factor 
definition multi dimensional affine partition mapping phi program iff row mapping legal mapping data dependence set zs ffi ffi oe ss ds gamma zs phi gamma phi algorithm find degrees parallelism requiring synchronizations outer sequential loop exploit pipeline parallelism possible 
step apply algorithm program get set maximally independent partition mappings phi phi phi dimensional partition mapping rows mappings step phi gamma rows phi phi row 
map operations gamma processors phi number iterations loop partition computation assigned processor time phi processor executes time partitions sequence operations time partition executed original sequential order 
processor pm gamma synchronizes gamma neighbors gamma pm gamma pm gamma gamma time partition gamma neighbors pm gamma pm gamma time partition 
step apply algorithm partition phi find degrees parallelism requiring synchronizations 
lemma algorithm yields gamma degrees pipeline parallelism rank affine partition rows legal maximally independent affine mappings outer sequential loop 
proof synchronizations program ensure partition pm executes definition fact instructions partition execute original sequential order program correct 
rank phi mapping divides computation time partitions 
synchronization pattern see partition needs wait time step start 
gamma degrees parallelism 
theorem algorithm finds maximum degree parallelism degree synchronization outer sequential loop proof phi phi set legal maximally independent partition mappings step algorithm phi rank affine mapping 
prove theorem introducing algorithm prove new algorithm property desired establish equivalence algorithm 
phi phi phi phi gamma rows phi 
steps algorithm partition program time phi partition time partition space phi find parallelism requiring synchronizations space partitions 
phi synchronization free space partition mapping partition phi phi linear time partition constraints different partitions phi 
rank phi rank dimensional mapping rows phi phi partitioning time space creates partitions requiring synchronizations yielding gamma degrees parallelism 
lemmas algorithm finds parallelism synchronizations 
lemma pipelining yields gamma degrees parallelism degree synchronization 
phi sum rows phi easy show partition phi exists partition phi operations vice versa 
algorithm find maximum degree parallelism synchronizations 
example consider example outer sequential loop example gamma gamma gamma gamma gamma gamma gamma gamma instruction modifies th row matrix lower triangular elements matrix instruction updates upper triangular portion consider previous loop transformation algorithms parallelize code 
able handle perfectly nested loops algorithms unimodular transforms find parallelism innermost loops example 
loop fusion technique combines sequence loops integrated unimodular framework 
clear fuse innermost loops number iterations loops different 
fusion applied result perfectly nested loop amenable unimodular transforms 
summary previous approaches find degree parallelism program degrees synchronization 
algorithm finds degree parallelism parallelism find coarser grained requiring degree synchronization 
theta theta applied example step algorithm finds legal independent affine time partition mappings phi phi phi phi psi phi phi phi phi psi step algorithm pipelines computation mapping operations processors phi partitioning space partitions time phi desired spmd code derived affine partitions easily 
processor id denoted ith wait executed processor stalls execution processor executes ith signal 
gamma wait gamma gamma gamma signal gamma wait gamma gamma gamma gamma gamma gamma gamma gamma signal execution order processor example 
computation assigned processor simply pth iteration outermost loop 
execution order processor illustrated 
shows original iteration space instructions assigned processor iteration labeled new indices spmd code 
note column iteration space instruction executed outside innermost loop 
labels operations indicate relative order respect operations 
code generated algorithm requires processor synchronize processors operations previous solutions parallelizing innermost loops require processor execute barrier operation 
maximum degrees parallelism final algorithm uses components find parallelism coarsest granularity program 
practice need find coarse grain parallelism available parallel hardware 
steps algorithm find successively finer granularities parallelism algorithm halt soon sufficient parallelism 
algorithm find degrees parallelism program parallelism coarse grain possible 
step find maximum degree synchronization free parallelism apply algorithm program 
step find maximum degree parallelism requires synchronizations apply algorithm space partitions step 
step find maximum degree parallelism requires synchronizations apply algorithm algorithm partitions step 
step find maximum degree parallelism successively greater degrees synchronization recursively apply step computation belonging space partitions generated previous step parallelism 
theorem algorithm finds degrees parallelism program parallelism possible 
proof lemmas theorem program maximizes degree parallelism program 
theorems steps find degrees parallelism synchronization respectively recursive application step finds coarsest granularity parallelism 
related lot research loop transformations limit comparison closely related 
loop interchanges reversals skewing combinations thereof modeled unimodular transformations various algorithms framework developed 
framework desired combination loop transformations achieved finding suitable unimodular matrix generate desired spmd code straightforward manner 
major limitations unimodular loop transformation approach imprecise dependence abstractions distance direction vectors operations iteration treated indivisible unimodular transformations apply nested loops 
unimodular transformations achieve effects obtained loop fission fusion scaling reindexing statement reordering 
feautrier finds piece wise affine schedule minimizes execution time program tries minimize communication parallelization 
instruction finds piece wise affine mapping maps loop indices multi dimensional time index 
uses heuristic parametric integer programming minimize dimensionality time 
minimizing dimensionality time schedule corresponds maximizing degree parallelism 
contrast solving coarsest granularity parallelism algorithm minimizes synchronization finds parallelism 
focusing degree parallelism shortest free schedule feautrier case find optimal mappings simple algorithms find null spaces set linear constraints 
addition generate pipeline parallelism better locality synchronization overhead 
code generation framework straightforward schedules find regular 
kelly pugh framework unifying iteration reordering transformations 
described way find affine mapping instruction performance estimation 
described algorithm minimize communication preserving parallelism 
algorithm finds dimension parallelism possible degrees parallelism 
estimates parallelism communication cost legal loop permutations reversals instruction 
builds conflict graph searches best mapping minimum cost 
restricting search loop permutations reversals instruction algorithm fails find parallelism requires skewing loops 
algorithms proposed solving subproblem finding communication free parallelism 
huang sadayappan find degree parallelism sequence perfectly nested loops 
anderson lam apply unimodular transforms loop nests increase granularity parallelism find maximum degree communication free parallelism loops heuristically introduce communication necessary 
bau propose improved algorithm find communication free parallelism 
algorithms treat operations loop iteration indivisible previously communication free parallelization algorithm transforms instructions individually 
synchronization free algorithm simpler direct possible application farkas lemma 
presents algorithm find optimal affine mapping maximizes degree parallelism minimizing degree synchronization 
affine framework encompasses previously proposed program transformations including unimodular transforms loop fusion loop fission loop scaling loop reindexing statement reordering algorithm powerful existing techniques 
formulate problem maximizing parallelism minimizing synchronization principles imprecise abstractions data dependence vectors 
define fundamental affine program transformation constraints space partition constraints constraints relaxation 
show problem finding degrees parallelism coarsest granularity parallelism reduced finding solutions constraints alternately 
algorithm problem simple ideas 
farkas lemma reduce constraints set linear inequalities 
second degrees parallelism metric optimal mappings obtained simple algorithms find solutions linear inequalities 
algorithm finding synchronization free space partitions different literature 
surprising similarly simple technique minimize synchronization find time partitions maximum degrees parallelism 
key insight set maximally independent legal partition mappings combined linearly form mapping yields maximum parallelism 
legal mappings combined rows multidimensional mapping yield pipeline parallelism 
allen callahan kennedy 
automatic decomposition scientific programs parallel execution 
conference record fourteenth annual acm symposium principles programming languages pages january 
irigoin 
scanning polyhedra loops 
proceedings third acm sigplan symposium principles practice parallel programming pages april 
anderson lam 
global optimizations parallelism locality scalable parallel machines 
proceedings acm sigplan conference programming language design implementation pages june 
torres 
partitioning statement iteration space non singular matrices 
proceedings acm international conference supercomputing pages july 
bacon graham sharp 
compiler transformations high performance computing 
computing surveys december 
banerjee 
unimodular transformations double loops 
proceedings third workshop languages compilers parallel computing pages august 
banerjee 
loop transformations restructuring compilers 
kluwer academic 
bau pingali 
solving alignment elementary linear algebra 
proceedings seventh workshop languages compilers parallel computing pages 
springer verlag august 
feautrier 
efficient solutions affine scheduling problem part dimensional time 
int 
parallel programming october 
feautrier 
efficient solutions affine scheduling problem part ii multidimensional time 
int 
parallel programming december 
feautrier 
automatic distribution 
technical report institut blaise pascal laboratoire december 
huang sadayappan 
communication free hyperplane partitioning nested loops 
journal parallel distributed computing october 
kelly pugh 
determining schedules performance estimation 
technical report cs tr university maryland 
kelly pugh 
framework unifying reordering transformations 
technical report cs tr 
university maryland april 
kelly pugh 
minimizing communication preserving parallelism 
proceedings acm international conference supercomputing pages may 
kennedy mckinley 
optimizing parallelism data locality 
proceedings acm international conference supercomputing pages july 
kennedy mckinley 
maximizing loop parallelism improving data locality loop fusion distribution 
proceedings sixth workshop languages compilers parallel computing pages 
springer verlag august 
lim lam 
communication free parallelization affine transformations 
proceedings seventh workshop languages compilers parallel computing pages 
springer verlag august 
sarkar thekkath 
general framework iteration reordering loop transformations 
proceedings acm sigplan conference programming language design implementation pages june 
schrijver 
theory linear integer programming 
wiley chichester 
wolf 
improving locality parallelism nested loops 
phd thesis stanford university august 
published csl tr 
wolf lam 
loop transformation theory algorithm maximize parallelism 
transactions parallel distributed systems october 
wolfe 
optimizing supercompilers supercomputers 
mit press cambridge ma 
appendix algorithm input matrix output matrix description algorithm finds maximal set linearly independent solutions expresses rows matrix size matrix denotes bth component matrix thetan identity matrix true 
gamma gamma diagonal matrix positive diagonal entries 
solutions 
gamma gamma move pivot row column interchange 
interchange row row gamma gamma row row row gamma row row row row row 
find solution 
non negative combination gamma 
find kr gamma kr delta delta delta gamma gamma non trivial solution say kr kr delta delta delta gamma gamma alse solutions 
true 
rn gamma 
move solutions rn gamma 
interchange rows gamma rn gamma row addition find solutions 
rn col row col row col row rn gamma row col gammam row col col row row row row row rn gamma step row col rn rn gamma interchange row rn interchange row rn 
rn gamma gamma row rn gamma col gamma row col pick col gammam row col col row row row row 
necessary repeat rows rn rn rn remove row rn row return cn col step col rn cn cn gamma interchange column col cn rn cn 
