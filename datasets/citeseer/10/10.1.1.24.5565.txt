msri workshop nonlinear estimation classification 
boosting approach machine learning overview robert schapire labs gamma research shannon laboratory park avenue room florham park nj usa www research att com schapire december boosting general method improving accuracy learning algorithm 
focusing primarily adaboost algorithm chapter overviews boosting including analyses adaboost training error generalization error boosting connection game theory linear programming relationship boosting logistic regression extensions adaboost multiclass classification problems methods incorporating human knowledge boosting experimental applied boosting 
machine learning studies automatic techniques learning accurate predictions past observations 
example suppose build email filter distinguish spam junk email non spam 
machine learning approach problem start gathering examples spam non spam emails 
feed examples labels indicating spam favorite machine learning algorithm automatically produce classification prediction rule 
new unlabeled email rule attempts predict spam 
goal course generate rule accurate predictions possible new test examples 
building highly accurate prediction rule certainly difficult task 
hand hard come rough rules thumb moderately accurate 
example rule phrase buy occurs email predict spam 
rule come close covering spam messages instance really says predict buy occur message 
hand rule predictions significantly better random guessing 
boosting machine learning method subject chapter observation finding rough rules thumb lot easier finding single highly accurate prediction rule 
apply boosting approach start method algorithm finding rough rules thumb 
boosting algorithm calls weak base learning algorithm repeatedly time feeding different subset training examples precise different distribution weighting training examples 
time called base learning algorithm generates new weak prediction rule rounds boosting algorithm combine weak rules single prediction rule hopefully accurate weak rules 
approach fundamental questions answered distribution chosen round second weak rules combined single rule 
regarding choice distribution technique advocate place weight examples misclassified preceding weak rules effect forcing base learner focus attention hardest examples 
combining weak rules simply weighted majority vote predictions natural effective 
question base learning algorithm question purposely leave unanswered general boosting procedure combined base learning algorithm 
boosting refers general provably effective method producing accurate prediction rule combining rough moderately inaccurate rules thumb manner similar suggested 
chapter presents overview boosting focusing especially adaboost algorithm undergone intense theoretical study empirical testing 
distribution training examples generate subset training examples simply sampling repeatedly distribution 
gamma initialize ffl train base learner distribution ffl get base classifier ffl choose ff ffl update exp gammaff normalization factor chosen distribution output final classifier sign ff boosting algorithm adaboost 
adaboost working valiant pac probably approximately correct learning model kearns valiant pose question weak learning algorithm performs just slightly better random guessing boosted arbitrarily accurate strong learning algorithm 
schapire came provable polynomial time boosting algorithm 
year freund developed efficient boosting algorithm optimal certain sense suffered schapire algorithm certain practical drawbacks 
experiments early boosting algorithms carried drucker schapire simard ocr task 
adaboost algorithm introduced freund schapire solved practical difficulties earlier boosting algorithms focus 
pseudocode adaboost fig 
slightly generalized form schapire singer 
algorithm takes input training set belongs domain instance space label label set assume gamma section discuss extensions multiclass case 
adaboost calls weak base learning algorithm repeatedly series rounds main ideas algorithm maintain distribution set weights training set 
weight distribution training example round denoted 
initially weights set equally round weights incorrectly classified examples increased base learner forced focus hard examples training set 
base learner job find base classifier appropriate distribution 
base classifiers called rules thumb weak prediction rules section 
simplest case range binary restricted gamma base learner job minimize error ffl pr id base classifier received adaboost chooses parameter ff intuitively measures importance assigns deliberately left choice ff unspecified 
binary typically set ff ln gamma ffl ffl original description adaboost freund schapire 
choosing ff follows section 
distribution updated rule shown 
final combined classifier weighted majority vote base classifiers ff weight assigned analyzing training error basic theoretical property adaboost concerns ability reduce training error fraction mistakes training set 
specifically schapire singer generalizing theorem freund schapire show training error final classifier bounded follows jfi gj exp gammay henceforth define ff sign 
simplicity notation write shorthand respectively 
inequality follows fact gammay 
equality proved straightforwardly unraveling recursive definition eq 
suggests training error reduced rapidly greedy way choosing ff round minimize exp gammaff case binary classifiers leads choice ff eq 
gives bound training error ffl gamma ffl gamma fl exp gamma fl define fl gamma ffl bound proved freund schapire 
base classifier slightly better random fl fl fl training error drops exponentially fast bound eq 
gamma fl bound combined bounds generalization error prove adaboost boosting algorithm sense efficiently convert true weak learning algorithm generate classifier weak edge distribution strong learning algorithm generate classifier arbitrarily low error rate sufficient data 
eq 
points fact heart adaboost procedure finding linear combination base classifiers attempts minimize exp gammay exp gammay ff essentially round adaboost chooses calling base learner sets ff add term accumulating weighted sum base classifiers way sum exponentials maximally reduced 
words adaboost doing kind steepest descent search minimize eq 
search constrained step follow coordinate directions identify coordinates weights assigned base classifiers 
view boosting generalization examined considerable detail duffy helmbold mason friedman 
see section 
schapire singer discuss choice ff case real valued binary 
case interpreted confidence rated prediction sign predicted label magnitude jh gives measure confidence 
schapire singer advocate choosing ff minimize eq 
round 
generalization error studying designing learning algorithms course interested performance examples seen training generalization error topic section 
section training examples arbitrary assume examples train test generated unknown distribution theta generalization error probability misclassifying new example test error fraction mistakes newly sampled test set generalization error expected test error 
simplicity restrict attention binary base classifiers 
freund schapire showed bound generalization error final classifier terms training error size sample base classifier space number rounds boosting 
specifically techniques baum haussler show generalization error high probability pr td pr delta denotes empirical probability training sample 
bound suggests boosting overfit run rounds large 
fact happen 
early experiments authors observed empirically boosting overfit run thousands rounds 
observed adaboost continue drive generalization error long training error reached zero clearly contradicting spirit bound 
instance left side fig 
shows training test curves running boosting top quinlan decision tree learning algorithm letter dataset 
response empirical findings schapire bartlett gave alternative analysis terms margins training examples 
margin example defined margin yf jff ff jff vapnik chervonenkis vc dimension standard measure complexity space binary functions 
see instance refs 
definition relation learning theory 
soft oh notation delta informally meant hide logarithmic constant factors way standard big oh notation hides constant factors 
error rounds cumulative distribution margin error curves margin distribution graph boosting letter dataset reported schapire 
left training test error curves lower upper curves respectively combined classifier function number rounds boosting 
horizontal lines indicate test error rate base classifier test error final combined classifier 
right cumulative distribution margins training examples iterations indicated short dashed long dashed hidden solid curves respectively 
number gamma positive correctly classifies example 
magnitude margin interpreted measure confidence prediction 
schapire proved larger margins training set translate superior upper bound generalization error 
specifically generalization error pr margin high probability 
note bound entirely independent number rounds boosting 
addition schapire proved boosting particularly aggressive reducing margin quantifiable sense concentrates examples smallest margins positive negative 
boosting effect margins seen empirically instance right side fig 
shows cumulative distribution margins training examples letter dataset 
case training error reaches zero boosting continues increase margins training examples effecting corresponding drop test error 
margins theory gives qualitative explanation effectiveness boosting quantitatively bounds weak 
breiman instance shows empirically classifier margin distribution uniformly better classifier inferior test accuracy 
hand lozano proved new margin theoretic bounds tight give useful quantitative predictions 
attempts successful insights gleaned theory margins authors 
addition margin theory points strong connection boosting support vector machines vapnik explicitly attempt maximize minimum margin 
connection game theory linear programming behavior adaboost understood game theoretic setting explored freund schapire see grove schuurmans breiman 
classical game theory possible put person zero sum game form matrix play game player chooses row player chooses column loss row player payoff column player ij generally sides may play randomly choosing distributions rows columns respectively 
expected loss mq 
boosting viewed repeated play particular game matrix 
assume base classifiers binary fh entire base classifier space assume finite 
fixed training set game matrix rows columns ij 
row player boosting algorithm column player base learner 
boosting algorithm choice distribution training examples distribution rows base learner choice base classifier choice column example connection boosting game theory consider von neumann famous minmax theorem states max min mq min max mq matrix applied matrix just defined reinterpreted boosting setting shown meaning distribution examples exists base classifier error gamma fl exists convex combination base classifiers margin fl training examples 
adaboost seeks find final classifier high margin examples combining base classifiers sense minmax theorem tells adaboost potential success base learner exist combination base classifiers 
going adaboost shown special case general algorithm playing repeated games approximately solving matrix games 
shows asymptotically distribution training examples weights base classifiers final classifier game theoretic approximate minmax maxmin strategies 
problem solving finding optimal strategies zero sum game known solvable linear programming 
formulation boosting problem game connects boosting linear generally convex programming 
connection led new algorithms insights explored ratsch grove schuurmans demiriz bennett shawe taylor 
direction schapire describes analyzes generalization adaboost freund earlier boost majority algorithm broader family repeated games called drifting games 
boosting logistic regression classification generally problem predicting label example intention minimizing probability incorrect prediction 
useful estimate probability particular label 
friedman hastie tibshirani suggested method output adaboost reasonable estimates probabilities 
specifically suggested logistic function estimating pr gammaf usual weighted average base classifiers produced adaboost eq 

rationale choice close connection log loss negative log likelihood model ln gamma function noted adaboost attempts minimize gammay specifically verified eq 
upper bounded eq 

addition add constant gamma ln eq 
affect minimization verified resulting function eq 
identical taylor expansions zero second order behavior near zero similar 
shown distribution pairs expectations ln gamma yf ji gammayf minimized unconstrained function ln pr pr gamma reasons minimizing eq 
done adaboost viewed method approximately minimizing negative log likelihood eq 

may expect eq 
give reasonable probability estimate 
course friedman hastie tibshirani point minimizing exponential loss eq 
attempt directly minimize logistic loss eq 

propose logitboost algorithm 
different direct modification adaboost logistic loss proposed collins schapire singer 
kivinen warmuth lafferty derive algorithm unification logistic regression boosting bregman distances 
connects boosting maximum entropy literature particularly iterative scaling family algorithms 
give unified proofs convergence optimality family new old algorithms including adaboost exponential loss adaboost logistic loss logistic regression 
see lebanon lafferty showed logistic regression boosting fact solving constrained optimization problem boosting certain normalization constraints dropped 
logistic regression attempt minimize loss function ln gammay eq 
inconsequential change constants exponent 
modification adaboost proposed collins schapire singer handle loss function particularly simple 
adaboost unraveling definition fig 
shows proportional equal normalization exp gammay gamma define ff minimize loss function eq 
necessary modification redefine proportional exp gamma similar algorithm described duffy helmbold 
note case weight examples viewed vector proportional negative gradient respective loss function 
algorithms doing kind functional gradient descent observation spelled exploited breiman duffy helmbold mason friedman 
logistic regression number approaches taken apply boosting general regression problems labels real numbers goal produce real valued predictions close labels 
freund schapire attempt reduce regression problem classification problem 
friedman duffy helmbold functional gradient descent view boosting derive algorithms directly minimize loss function appropriate regression 
boosting approach regression proposed drucker 
multiclass classification methods extending adaboost multiclass case 
straightforward generalization called adaboost adequate base learner strong achieve reasonably high accuracy hard distributions created adaboost 
method fails base learner achieve accuracy run hard distributions 
case sophisticated methods developed 
generally reducing multiclass problem larger binary problem 
schapire singer algorithm adaboost mh works creating set binary problems example possible label form example correct label labels 
freund schapire algorithm adaboost special case schapire singer adaboost algorithm creates binary problems example correct label incorrect label form example correct label methods require additional effort design base learning algorithm 
different technique incorporates dietterich bakiri method error correcting output codes achieves similar provable bounds adaboost mh adaboost base learner handle simple binary labeled data 
schapire singer allwein schapire singer give method combining boosting errorcorrecting output codes 
incorporating human knowledge boosting machine learning methods entirely data driven sense classifier generates derived exclusively evidence training data 
data abundant approach sense 
applications data may severely limited may human knowledge principle compensate lack data 
standard form boosting allow direct incorporation prior knowledge 
describe modification boosting combines balances human expertise available training data 
aim approach allow human rough judgments refined reinforced adjusted statistics training data manner permit data entirely overwhelm human judgments 
step approach human expert construct hand rule mapping instance estimated probability interpreted guessed probability instance appear label 
various methods constructing function hope difficult build function need highly accurate approach effective 
basic idea replace logistic loss function eq 
incorporates prior knowledge ln gammay re gammaf re ln gamma ln gamma gamma binary relative entropy 
term eq 

second term gives measure distance model built boosting human model 
balance conditional likelihood data distance model human model 
relative importance terms controlled parameter experiments applications practically adaboost advantages 
fast simple easy program 
parameters tune number round 
requires prior knowledge base learner flexibly combined method finding base classifiers 
comes set theoretical guarantees sufficient data base learner reliably provide moderately accurate base classifiers 
shift mind set designer trying design learning algorithm accurate entire space focus finding base learning algorithms need better random 
hand caveats certainly order 
actual performance boosting particular problem clearly dependent data base learner 
consistent theory boosting fail perform insufficient data overly complex base classifiers base classifiers weak 
boosting especially susceptible noise exps 
adaboost tested empirically researchers including 
instance freund schapire tested adaboost set uci benchmark datasets base learning algorithm algorithm finds best decision stump single test decision tree 
results experiments shown fig 

seen boosting weak decision stumps usually give results boosting generally gives decision tree algorithm significant improvement performance 
set experiments schapire singer boosting text categorization tasks 
base classifiers test presence absence word phrase 
results experiments comparing boosting stumps boosting boosting stumps boosting comparison versus boosting stumps boosting set benchmark problems reported freund schapire 
point scatterplot shows test error rate competing algorithms single benchmark 
coordinate point gives test error rate percent benchmark coordinate gives error rate boosting stumps left plot boosting right plot 
error rates averaged multiple runs 
adaboost methods shown fig 

nearly experiments performance measures tested boosting performed significantly better methods tested 
shown fig 
experiments demonstrated effectiveness confidence rated predictions mentioned section means speeding boosting 
boosting applied text filtering routing ranking problems learning problems arising natural language processing image retrieval medical diagnosis customer monitoring segmentation 
method incorporating human knowledge boosting described section applied speech categorization tasks 
case prior knowledge took form set hand built rules mapping keywords predicted categories 
results shown fig 

final classifier produced adaboost instance decision tree base learning algorithm extremely complex difficult comprehend 
greater care human understandable final classifier obtained boosting 
cohen singer showed design base number classes adaboost sleeping experts rocchio naive bayes prtfidf error number classes adaboost sleeping experts rocchio naive bayes prtfidf comparison error rates adaboost text categorization methods naive bayes probabilistic tf idf rocchio sleeping experts reported schapire singer 
algorithms tested text corpora reuters newswire articles left ap newswire headlines right varying numbers class labels indicated axis 
learning algorithm combined adaboost results final classifier consisting relatively small set rules similar generated systems ripper irep rules 
cohen singer system called slipper fast accurate produces quite compact rule sets 
freund mason showed apply boosting learn generalization decision trees called alternating trees 
algorithm produces single alternating tree ensemble trees obtained running adaboost top decision tree learning algorithm 
hand learning algorithm achieves error rates comparable ensemble trees 
nice property adaboost ability identify outliers examples mislabeled training data inherently ambiguous hard categorize 
adaboost focuses weight hardest examples examples highest weight turn outliers 
example phenomenon seen fig 
taken ocr experiment conducted freund schapire 
number outliers large emphasis placed hard examples detrimental performance adaboost 
demonstrated convincingly dietterich 
friedman hastie tibshirani suggested variant adaboost called gentle adaboost puts emphasis outliers 
ratsch onoda muller show regularize adaboost handle noisy data 
freund suggested algorithm called brownboost takes radical approach de emphasizes outliers clear hard classify correctly 
algorithm adaptive number rounds discrete adaboost discrete adaboost mh real adaboost mh number rounds discrete adaboost discrete adaboost mh real adaboost mh comparison training left test right error boosting methods class text classification problem trec ap collection reported schapire singer 
discrete adaboost mh discrete adaboost multiclass versions adaboost require binary gamma valued base classifiers real adaboost mh multiclass version uses confidence rated real valued base classifiers 
version freund boost majority algorithm demonstrates intriguing connection boosting brownian motion 
overview seen emerged great views interpretations adaboost 
foremost adaboost genuine boosting algorithm access true weak learning algorithm performs little bit better random guessing distribution training set prove arbitrarily bounds training error generalization error adaboost 
original view adaboost interpreted procedure functional gradient descent approximation logistic regression repeated game playing algorithm 
adaboost shown related topics game theory linear programming bregman distances support vector machines brownian motion logistic regression maximum entropy methods iterative scaling 
connections interpretations greatly enhanced understanding boosting contributed extension practical directions logistic regression loss minimization problems multiclass problems incorporate regularization allow integration prior background knowledge 
training sentences data knowledge knowledge data training examples classification accuracy data knowledge knowledge data comparison percent classification accuracy spoken language tasks may help left help desk right function number training examples data knowledge separately reported 
discussed growing number applications adaboost practical machine learning problems text speech categorization 
steven abney robert schapire yoram singer 
boosting applied tagging pp attachment 
proceedings joint sigdat conference empirical methods natural language processing large corpora 
erin allwein robert schapire yoram singer 
reducing multiclass binary unifying approach margin classifiers 
journal machine learning research 
peter bartlett 
sample complexity pattern classification neural networks size weights important size network 
ieee transactions information theory march 
eric bauer ron kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
eric baum david haussler 
size net gives valid generalization 
neural computation 
blumer andrzej ehrenfeucht david haussler manfred warmuth 
learnability vapnik chervonenkis dimension 
journal association computing machinery october 
sample examples largest weight ocr task reported freund schapire 
examples chosen rounds boosting top line rounds middle rounds bottom 
underneath image line form label example labels get highest second highest vote combined classifier point run algorithm corresponding normalized scores 
bernhard boser isabelle guyon vladimir vapnik 
training algorithm optimal margin classifiers 
proceedings fifth annual acm workshop computational learning theory pages 
leo breiman 
arcing classifiers 
annals statistics 
leo breiman 
prediction games arcing classifiers 
neural computation 
william cohen 
fast effective rule induction 
proceedings twelfth international conference machine learning pages 
william cohen yoram singer 
simple fast effective rule learner 
proceedings sixteenth national conference artificial intelligence 
michael collins 
discriminative reranking natural language parsing 
proceedings seventeenth international conference machine learning 
michael collins robert schapire yoram singer 
logistic regression adaboost bregman distances 
machine learning appear 
corinna cortes vladimir vapnik 
support vector networks 
machine learning september 
darroch ratcliff 
generalized iterative scaling log linear models 
annals mathematical statistics 
stephen della pietra vincent della pietra john lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence april 
demiriz kristin bennett john shawe taylor 
linear programming boosting column generation 
machine learning 
thomas dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 
thomas dietterich bakiri 
solving multiclass learning problems error correcting output codes 
journal artificial intelligence research january 
harris drucker 
improving regressors boosting techniques 
machine learning proceedings fourteenth international conference pages 
harris drucker corinna cortes 
boosting decision trees 
advances neural information processing systems pages 
harris drucker robert schapire patrice simard 
boosting performance neural networks 
international journal pattern recognition artificial intelligence 
nigel duffy david helmbold 
potential 
advances neural information processing systems 
nigel duffy david helmbold 
boosting methods regression 
machine learning 
gerard german rigau 
boosting applied word sense disambiguation 
proceedings th european conference machine learning pages 
yoav freund 
boosting weak learning algorithm majority 
information computation 
yoav freund 
adaptive version boost majority algorithm 
machine learning june 
yoav freund raj iyer robert schapire yoram singer 
efficient boosting algorithm combining preferences 
machine learning proceedings fifteenth international conference 
yoav freund mason 
alternating decision tree learning algorithm 
machine learning proceedings sixteenth international conference pages 
yoav freund robert schapire 
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 
yoav freund robert schapire 
game theory line prediction boosting 
proceedings ninth annual conference computational learning theory pages 
yoav freund robert schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences august 
yoav freund robert schapire 
adaptive game playing multiplicative weights 
games economic behavior 
jerome friedman trevor hastie robert tibshirani 
additive logistic regression statistical view boosting 
annals statistics april 
jerome friedman 
greedy function approximation gradient boosting machine 
annals statistics october 
johannes furnkranz gerhard widmer 
incremental reduced error pruning 
machine learning proceedings eleventh international conference pages 
adam grove dale schuurmans 
boosting limit maximizing margin learned ensembles 
proceedings fifteenth national conference artificial intelligence 
satoshi shirai 
decision trees construct practical parser 
machine learning 
raj iyer david lewis robert schapire yoram singer amit singhal 
boosting document routing 
proceedings ninth international conference information knowledge management 
jeffrey jackson mark craven 
learning sparse perceptrons 
advances neural information processing systems pages 
michael kearns leslie valiant 
learning boolean formulae finite automata hard factoring 
technical report tr harvard university aiken computation laboratory august 
michael kearns leslie valiant 
cryptographic limitations learning boolean formulae finite automata 
journal association computing machinery january 
kivinen manfred warmuth 
boosting entropy projection 
proceedings twelfth annual conference computational learning theory pages 

empirical margin distributions bounding generalization error combined classifiers 
annals statistics february 
vladimir fernando lozano 
explanation effectiveness voting methods game margins weights 
proceedings th annual conference computational learning theory th european conference computational learning theory pages 
vladimir fernando lozano 
new bounds generalization error combined classifiers 
advances neural information processing systems 
john lafferty 
additive models boosting inference generalized divergences 
proceedings twelfth annual conference computational learning theory pages 
guy lebanon john lafferty 
boosting maximum likelihood exponential models 
advances neural information processing systems 
richard maclin david opitz 
empirical evaluation bagging boosting 
proceedings fourteenth national conference artificial intelligence pages 
mason peter bartlett jonathan baxter 
direct optimization margins improves generalization combined classifiers 
advances neural information processing systems 
mason jonathan baxter peter bartlett marcus frean 
functional gradient techniques combining hypotheses 
alexander smola peter bartlett bernhard scholkopf dale schuurmans editors advances large margin classifiers 
mit press 
mason jonathan baxter peter bartlett marcus frean 
boosting algorithms gradient descent 
advances neural information processing systems 
stefano barbara andrea 
tuning boosting application melanoma diagnosis 
multiple classifier systems proceedings nd international workshop pages 
merz murphy 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
pedro moreno beth logan raj 
boosting approach confidence scoring 
proceedings th european conference speech communication technology 
michael mozer richard david grimes eric johnson howard 
predicting subscriber dissatisfaction improving retention wireless telecommunications industry 
ieee transactions neural networks 
takashi onoda gunnar ratsch klaus robert muller 
applying support vector machines boosting non intrusive monitoring system household electric appliances inverters 
proceedings second icsc symposium neural computation 

new zero error bounds voting algorithms 
unpublished manuscript 
quinlan 
bagging boosting 
proceedings thirteenth national conference artificial intelligence pages 
ross quinlan 
programs machine learning 
morgan kaufmann 
ratsch onoda 
muller 
soft margins adaboost 
machine learning 
gunnar ratsch manfred warmuth sebastian mika takashi onoda steven klaus robert muller 
barrier boosting 
proceedings thirteenth annual conference computational learning theory pages 
greg david madigan thomas richardson 
boosting methodology regression problems 
proceedings international workshop ai statistics pages 
schapire gupta riccardi bangalore alshawi douglas 
combining prior knowledge boosting call classification spoken language dialogue 
unpublished manuscript 
marie robert schapire narendra gupta 
boostexter text categorization spoken language dialogue 
unpublished manuscript 
robert schapire 
strength weak learnability 
machine learning 
robert schapire 
output codes boost multiclass learning problems 
machine learning proceedings fourteenth international conference pages 
robert schapire 
drifting games 
machine learning june 
robert schapire yoav freund peter bartlett wee sun lee 
boosting margin new explanation effectiveness voting methods 
annals statistics october 
robert schapire yoram singer 
improved boosting algorithms confidence rated predictions 
machine learning december 
robert schapire yoram singer 
boostexter boosting system text categorization 
machine learning may june 
robert schapire yoram singer amit singhal 
boosting rocchio applied text filtering 
proceedings st annual international conference research development information retrieval 
holger schwenk bengio 
training methods adaptive boosting neural networks 
advances neural information processing systems pages 
paul viola 
boosting image retrieval 
proceedings ieee conference computer vision pattern recognition 
valiant 
theory learnable 
communications acm november 
vapnik ya 
chervonenkis 
uniform convergence relative frequencies events probabilities 
theory probability applications xvi 
vladimir vapnik 
nature statistical learning theory 
springer 
marilyn walker owen rambow monica 
spot trainable sentence planner 
proceedings nd annual meeting north american chapter computational linguistics 

