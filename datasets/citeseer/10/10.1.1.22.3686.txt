efficient crawling url ordering junghoo cho hector garcia molina lawrence page department computer science stanford ca cho hector page cs stanford edu study order crawler visit urls seen order obtain important pages 
obtaining important pages rapidly useful crawler visit entire web reasonable amount time 
define importance metrics ordering schemes performance evaluation measures problem 
experimentally evaluate ordering schemes stanford university web 
results show crawler ordering scheme obtain important pages significantly faster 
crawler program retrieves web pages commonly search engine pin web cache 
roughly crawler starts url initial page 
retrieves extracts urls adds queue urls scanned 
crawler gets urls queue order repeats process 
page scanned client saves pages creates index pages summarizes analyzes content pages 
crawlers widely today 
crawlers major search engines altavista infoseek excite lycos attempt visit text web pages order build content indexes 
crawlers may visit pages may look certain types information email addresses 
spectrum personal crawlers scan pages interest particular user order build fast access cache 
design crawler presents challenges 
externally crawler avoid overloading web sites network links goes business kos 
internally crawler deal huge volumes data 
unlimited computing resources unlimited time carefully decide urls scan order 
crawler decide frequently revisit pages seen order keep client informed changes web 
challenges importance crawlers internet little research done crawlers 
address important challenges crawler select urls scan queue known urls 
crawler intends perform single scan entire web load placed target sites issue url order suffice 
eventually single known url visited order critical 
crawlers able visit possible page main reasons client may limited storage capacity may unable index analyze pages 
currently web contains tb growing rapidly reasonable expect clients want able cope data kah 
crawling takes time point crawler may need start revisiting previously scanned pages check changes 
means may get pages 
currently estimated gb web changes month kah 
case important crawler visit important pages fraction web visited kept date meaningful 
sections different useful definitions importance develop crawling priorities important pages higher probability visited 
experimental results crawling stanford university web pages show effective different crawling strategies 
importance metrics pages necessarily equal interest crawler client 
instance client building specialized database particular topic pages refer topic important visited early possible 
similarly search engine may number web urls point page called backlink count rank user query results 
crawler visit pages better visit high backlink count give user higher ranking results 
web page define importance page ways 
metrics combined discussed 

similarity driving query query drives crawling process defined textual similarity similarity studied information retrieval ir community sal applied web environment 
refer importance metric case 
wish query explicit 
compute similarities view document dimensional vector wn term wi vector represents ith word vocabulary 
wi appear document wi zero 
appear wi set represent significance word 
common way compute significance wi multiply number times ith word appears document inverse document frequency idf word 
idf factor divided number times word appears entire collection case entire web 
idf factor corresponds content discriminating power word term appears rarely documents queue high idf term occurs documents low idf 
wi terms take account page word appears 
instance words appearing title html page may higher weight words body 
similarity defined inner product vectors 
option cosine similarity measure inner product normalized vectors 
note idf terms similarity computation importance page computed local information just idf terms need global information 
crawling process seen entire collection estimate idf factors pages crawled idf terms computed time 
refer estimated importance page different actual importance computable entire web crawled 
idf factors 

backlink count value number links appear entire web 
ib refer importance metric 
intuitively page linked pages important seldom referenced 
type citation count extensively evaluate impact published papers 
web ib useful ranking query results giving users pages general interest 
note evaluating ib requires counting backlinks entire web 
crawler may estimate value ib number links seen far 

pagerank ib metric treats links equally 
link yahoo home page counts link individual home page 
yahoo home page important higher ib count sense value link highly 
pagerank backlink metric ir recursively defines importance page weighted sum importance pages backlinks metric useful ranking results user queries pb goo 
ir estimated value ir subset pages available 
formally page outgoing link assume outgoing links single web page 
consider page pointed pages tn 
ci number links going page ti 
damping factor intuition 
weighted backlink count page ir ir ir tn cn leads equation web page equal number unknowns 
equations solved ir values 
solved iteratively starting ir values equal 
step new ir value computed old ir ti values equation values converge 
calculation corresponds computing principal eigenvector link matrices 
intuitive model pagerank think user surfing web starting page randomly selecting page link follow 
user reaches page outlinks jumps random page 
user page probability visited page completely random 
damping factor sense users continue clicking task finite amount time go unrelated 
ir values computed give probability random surfer time 

forward link count completeness may want consider metric counts number links emanate metric page outgoing links valuable may web directory 
metric computed directly 
kind metric conjunction factors reasonably identify index pages ppr 
define weighted forward link metric analogous ir consider 

location metric il importance page function location contents 
url leads il function example urls com may deemed useful urls endings urls containing string home may interest urls 
location metric considers urls fewer slashes useful slashes 
examples local metrics evaluated simply looking url stated earlier importance metrics combined various ways 
example may define metric ic ib constants 
combines similarity metric query backlink metric 
pages relevant content backlinks highest ranked 
note similar approach improve effectiveness search engine mar 
problem definition goal design crawler possible visits high pages lower ranked ones definition 
course crawler available values guess high pages fetch 
general goal stated precisely ways depending expect crawler operate 
evaluations section second model cases compare model experiment 
believe useful discuss models understand options 
crawl model crawler starts initial page stops visiting pages 
point perfect crawler visited pages rk page highest importance value highest 
call pages rk hot pages 
pages visited real crawler contain pages rank higher equal rk 
define performance crawler pcs performance ideal crawler course 
crawler manages visit pages entirely random may revisit pages performance total number pages web 
page visited hot page probability expected number desired pages crawler stops crawl threshold assume crawler visits pages 
importance target page considered hot 
assume total number hot pages performance crawler pst fraction hot pages visited crawler stops 
ideal crawler performance ideal crawler perfect performance 
purely random crawler revisits pages expected visit hot pages stops 
performance random crawler visits pages performance expected 
limited buffer crawl model consider impact limited storage crawling process 
assume crawler keep pages buffer 
buffer fills crawler decide pages flush room new pages 
ideal crawler simply drop pages lowest value real crawler guess pages buffer eventually low values 
allow crawler visit total pages equal total number web pages 
process fraction buffer pages hot gives performance pbc 
define hot pages target importance rb rb page bth highest importance value 
performance ideal random crawler analogous previous cases 
note evaluate crawler metrics need compute actual values pages involves crawling entire web 
keep experiments section manageable imagines stanford university pages form entire web evaluate performance context 
assume pages outside stanford links pages outside stanford links pages outside stanford count computations 
section study implications assumption analyzing smaller web stanford domain seeing web size impacts performance 
ordering metrics crawler keeps queue urls seen crawl select queue url visit 
ordering metric crawler selection selects url highest value urls queue 
metric information seen remembered space limited crawler 
metric designed importance metric mind 
instance searching high ib pages sense ib page points 
sense ir importance metric weighted 
experiments explore types ordering metrics best suited ib 
location importance metric il metric directly ordering url directly gives il value 
forward link similarity metrics harder devise ordering metric seen 
see similarity may able text anchors url predictor text contain 
possible ordering metric isis anchor text url driving query 
experiments avoid network congestion heavy loads servers experimental evaluation steps 
step physically crawled stanford web pages built local repository pages 
done stanford webbase bp system designed create maintain large web repositories 
built repository ran virtual crawlers evaluate different crawling schemes 
note complete image stanford domain repository virtual crawler crawling decisions pages saw 
section briefly discuss particular database obtained experiments 
description dataset download image stanford web pages started webbase initial list stanford edu urls 
urls obtained earlier crawl 
crawl non stanford urls ignored 
limited actual data collected reasons 
heuristics needed avoid automatically generated potentially infinite sets pages 
example urls containing cgi bin crawled contain programs generate infinite sets pages produce undesirable side effects unintended vote online election 
heuristics similar aspects discussed location metric described earlier eliminate urls look undesirable 
way data set reduced robots exclusion protocol rob allows webmasters define pages want crawled automatic systems 
process known urls stanford pages 
noted known urls server www slac stanford edu program generate unlimited number web pages 
crawl stopped complete urls servers believe dataset reasonable representation stanford edu web 
dataset consisted crawled valid html pages consumed roughly gb disk space 
pages pages unreachable starting point crawl total number pages experiments 
stress virtual crawlers discussed webbase directly 
stated earlier dataset collected webbase crawler crawling 
virtual crawlers simpler webbase crawler 
instance detect url invalid simply seeing dataset 
similarly need distribute load visited sites 
simplifications fine virtual crawlers evaluate ordering schemes real crawling 
backlink crawlers section study effectiveness various ordering metrics scenario importance measured backlinks ib metrics 
start describing structure virtual crawler consider different ordering metrics 
noted stanford dataset described section crawls considered page valid web server responded header ok algorithm crawling algorithm backlink input starting url seed url procedure enqueue url queue starting url empty url queue url dequeue url queue page crawl page url enqueue crawled pages url page url list extract urls page foreach url list enqueue links url url queue crawled pages enqueue url queue reorder queue url queue function description enqueue queue element append element queue dequeue queue remove element queue return reorder queue queue reorder queue information links refer basic crawling algorithm started stanford homepage 
pagerank metric damping factor ir ir experiments 
shows basic virtual crawler 
crawler manages main data structures 
queue url queue contains urls seen need visited 
page visited stored url crawled pages 
links holds pairs form url seen visited page url 
crawler ordering metric implemented function reorder queue shown 
ordering metrics breadth backlink count ib pagerank ir 
breadth metric places urls queue order discovered policy crawler visit pages breadth order 
start showing crawler performance backlink ordering metric 
scenario importance metric number backlinks page ib consider crawl threshold model section 
recall page backlinks considered important hot 
hot page definitions pages total web pages considered hot respectively 
horizontal axis fraction stanford web pages crawled time 
right horizontal axis pages visited 
vertical axis represents pst fraction total hot pages crawled point 
st breadth null operation backlink count ib foreach url queue backlink count number terms links sort url queue backlink count pagerank ir solve set equations ir ir vi ci vi links ci number links page vi sort url queue ir description reorder queue ordering metric fraction stanford web crawled ideal experiment fraction stanford web crawled vs pst 
ib ib 
st fraction stanford web crawled ordering metric pagerank backlink breadth ideal random fraction stanford web crawled vs pst 
ib 
solid lines show results experiments 
example crawler experiment visited stanford pages crawled total hot pages 
dashed lines graph show expected performance ideal crawlers 
ideal crawler reaches performance pages crawled 
dotted line represents performance random crawler increases linearly time 
graph shows definition hot page stringent larger faster crawler locate hot pages 
result expected pages backlinks seen quickly crawl starts 
shows large finding group hot pages difficult 
right point horizontal axis crawler finds hot pages roughly rate random crawler 
experiment compare different ordering metrics breadth pagerank corresponding functions 
continue crawl threshold model ib importance metric 
shows results experiment 
results counterintuitive 
intuitively expect crawler backlink ordering metric ib matches importance metric ib perform best 
case pagerank metric ir outperforms ib 
understand manually traced crawler operation 
noticed ib crawler behaved depth frequently visiting pages cluster moving 
hand ir crawler combined breadth depth better way 
illustrate consider web fragment 
ib ordering crawler visits page labeled quickly finds cluster pages point 
pages temporarily backlinks page visit page delayed page backlinks pages cluster hand ir ordering page may higher rank link comes high ranking page pages cluster pointers low ranking pages cluster 
page reached faster 
summary early stages crawl backlink information biased starting point 
crawler bases decisions skewed information tries getting locally hot pages globally hot pages bias gets worse crawl proceeds 
hand cs cluster cluster crawling order fraction stanford web crawled ordering metric pagerank backlink breadth ideal random fraction stanford web crawled vs pcs 
ib 
ir pagerank crawler biased locally hot pages gives better results regardless starting point 
shows limited crawl threshold model 
show performance crawlers crawl model section 
remember crawl model definition hot pages changes time 
crawler predefined notion hot pages crawler visited say entire web considers top pages hot pages 
ideal crawler performance times download pages order importance 
compares breadth backlink pagerank ordering metrics ib importance metric model 
vertical axis represents pcs crawled fraction hot pages point varying definition hot pages 
shows results crawl model analogous crawl threshold model pagerank ordering metric shows best performance 
returning crawl threshold model shows results ir pagerank importance metric 
pagerank ordering metric shows ir metric number hot pages close st fraction stanford web crawled ordering metric pagerank backlink breadth ideal random fraction stanford web crawled vs pst 
ir 
st fraction database group web crawled ordering metric pagerank backlink breadth ideal random fraction db group web crawled vs pst 
ib 
best performance 
backlink breadth metrics show similar performance 
results recommend pagerank ordering metric ib ir importance metrics 
small scale crawl cases crawler client may interested small portions web 
instance client may interested single site create mirror say 
subsection evaluate ordering metrics scenario 
study impact scale performance crawler ran experiments similar section pages stanford database group server www db stanford edu 
subset stanford pages consists html pages smaller entire stanford domain 
experiments database group domain crawling performance stanford domain 
shows 
number pages number backlinks histogram backlink counts db group web results 
case crawl threshold model importance metric ib 
graph shows performance worse random crawler times ordering metrics 
poor performance mainly importance metric backlinks measure importance small domain 
small domain pages small number backlinks number backlinks sensitive page creator style 
example shows histogram number backlinks database group domain 
vertical axis shows number pages backlink count 
see pages fewer backlinks 
range rank page varies greatly style creator page 
creator generates cross links pages pages high ib rank 
rank sensitive measure importance pages 
see impact locally dense clusters locally popular globally unpopular pages 
performance backlink ib crawler initially quite flat initially depth crawl cluster 
visiting pages crawler suddenly discovers large cluster accounts jump graph 
hand pagerank ir crawler large cluster earlier performance better initially 
show results database group web importance metric ir pagerank metric 
ordering metrics show better performance ir metric ib metric performance larger stanford domain 
ir ordering metric shows best performance 
similarity crawlers experiments previous subsections compared different backlink crawlers 
subsection results experiments similarity crawlers 
similarity importance metric measures relevance page topic query user mind 
clearly possible metrics consider experiments intended comprehensive 
goal briefly explore number hot pages similar 
st fraction database group web crawled ordering metric pagerank backlink breadth ideal random percentage db group web crawled vs pst 
ir 
potential various ordering schemes sample scenarios 
particular experiments consider definition page considered hot contains word computer title occurrences computer body 
similarity crawling crawler appropriate take content page account 
give priority pages related topic interest modified crawler shown 
crawler keeps queues urls visit hot queue stores urls topic word computer anchors urls 
second queue url queue keeps rest urls 
crawler prefers take urls visit hot queue 
shows pst results crawler importance metric defined 
horizontal axis represents fraction stanford web pages crawled vertical axis shows crawled fraction total hot pages 
results show pagerank crawler behaved better random crawler 
breadth crawler gave reasonable result 
result unexpected crawlers differ ordering metrics neutral page content 
crawlers visited computer related urls immediately discovery 
schemes theoretically equivalent give comparable results 
observed unexpected performance difference arises breadth crawler fifo nature 
breadth crawler fetches pages order 
computer related page crawled earlier crawler discovers visits child pages earlier 
pages tendency computer related performance better 
observed property page high value children high value 
take advantage property modified crawler shown 
crawler places hot queue urls target keyword anchor url links hot page 
illustrates result crawling strategy 
crawlers showed significant improvement difference breadth crawler decreased 
breadth crawler superior believe difference mainly third experiment consider different topic admission show results 
algorithm crawling algorithm modified similarity input starting url seed url procedure enqueue url queue starting url empty hot queue empty url queue url dequeue hot queue url queue page crawl page url enqueue crawled pages url page url list extract urls page foreach url list enqueue links url url queue hot queue crawled pages contains computer anchor url enqueue hot queue enqueue url queue reorder queue url queue reorder queue hot queue function description dequeue queue queue empty queue dequeue queue dequeue queue st similarity crawling algorithm ordering metric pagerank backlink breadth ideal random basic similarity crawler 
topic computer 
algorithm crawling algorithm similarity input starting url seed url procedure enqueue url queue starting url empty hot queue empty url queue url dequeue hot queue url queue page crawl page url page contains computer body computer title hot url true enqueue crawled pages url page url list extract urls page foreach url list enqueue links url url queue hot queue crawled pages contains computer anchor url enqueue hot queue distance enqueue hot queue enqueue url queue reorder queue url queue reorder queue hot queue function description distance return hot true return hot true links 
modified similarity crawling algorithm st fraction stanford web crawled ordering metric pagerank backlink breadth ideal random modified similarity crawler 
topic computer 
st fraction stanford web crawled ordering metric pagerank backlink breadth ideal random modified similarity crawler 
topic admission 
due statistical variation 
experiments including pagerank crawler shows similar better performance breadth crawler 
final experiment results shown repeat scenario reported different query topic 
case word admission considered interest 
details identical previous case similarity important effective ordering metric considers content anchors urls distance hot pages discovered 
addressed problem ordering urls crawling 
defined different kinds importance metrics built models evaluate crawlers 
experimentally evaluated combinations importance ordering metrics stanford web pages 
general results show pagerank ir excellent ordering metric pages backlinks high pagerank sought 
addition similarity driving query important useful visit earlier urls anchor text similar driving query query terms url short link distance page known hot 
ordering strategy build crawlers obtain significant portion hot pages relatively early 
property extremely useful trying crawl fraction web resources limited need revisit pages detect changes 
limitation experiments run stanford web pages 
believe stanford pages reasonable sample example managed different people structure pages variety ways 
include individual home pages clusters carefully managed organizations 
interesting investigate non stanford web pages analyze structural differences implication crawling 
material supported national science foundation cooperative agreement iri 
funding cooperative agreement provided darpa nasa industrial partners stanford digital libraries project 
bp sergey brin lawrence page 
anatomy large scale hypertextual web search engine 
proceedings seventh international world wide web conference brisbane australia april 
goo google www google com 
kah brewster kahle 
archiving internet 
scientific american march 
kos martijn koster 
robots web threat treat 
connexions april 
mar massimo marchiori 
quest correct information web hyper search engines 
proceedings sixth international world wide web conference pages santa clara california april 
pb lawrence page sergey brin 
anatomy large scale hypertextual web search engine 
proceedings seventh international world wide web conference brisbane australia april 
pin brian pinkerton 
finding people want experiences web crawler 
proceedings second world wide web conference chicago illinois october 
ppr peter pirolli james pitkow ramana rao 
silk sow ear extracting usable structures web 
proceedings conference human factors computing systems chi pages vancouver british columbia canada april 
rob robots exclusion protocol 
info webcrawler com mak projects robots exclusion html 
sal gerard salton 
modern information retrieval 
mcgraw hill edition 
lam jerry ying dik lee 
world wide web resource discovery system 
proceedings fourth international world wide web conference darmstadt germany april 

