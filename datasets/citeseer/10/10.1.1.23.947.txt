cs tr august fast sequential parallel algorithms association rule mining comparison andreas mueller department computer science university maryland college park college park md field knowledge discovery databases data mining received increasing attention years large organizations begun realize potential value information stored implicitly databases 
specific data mining task mining association rules particularly retail data 
task determine patterns rules characterize shopping behavior customers large database previous consumer transactions 
rules focus marketing efforts product placement sales promotions 
early algorithms required unpredictably large number io operations reducing io cost primary target algorithms literature 
proposed algorithms called partition uses new tid list data representation new partitioning technique 
partitioning technique reduces io cost constant amount processing database portion time memory 
implemented algorithm called sptid incorporates tid lists partitioning study benefits 
comparison non partitioning algorithm called sear new prefix tree data structure 
experiments sptid sear indicate tid lists inherent inefficiencies furthermore algorithms tested tend cpu bound trading cpu overhead operations partitioning lead better performance 
order scale mining algorithms huge databases multiple terabytes large organizations manage near implemented parallel versions sear spear partitioned counterpart 
performance results show algorithms parallelize easily obtain speedup scale results parallel sear version performs better parallel spear despite fact uses communication 
research funded part air force syracuse university subcontract caltech subcontract table contents section page overview kdd research types knowledge database specifics general issues kdd system architecture structure report association rule mining association rules definition frequent sets definition association rules basic algorithmic scheme mining association rules previous algorithms ais setm apriori aprioritid apriorihybrid partition sequential algorithms sear modifying apriori prefix trees storage sets candidates prefix trees sear sptid partitioning algorithm spear algorithm spinc incremental algorithm experiments sequential algorithms synthetic data generation sear pass bundling varying minimum support increasing number transactions comparing tid lists item lists varying minimum support explanation performance results varying number items increasing transaction size data sets summary partitioning summary parallel algorithms programming environment data distribution pear parallel sear algorithm algorithm description implementation ppar partitioned parallel algorithm algorithm description computing union locally frequent sets parallel experiments cost combine operation speed experiments scale experiments size experiments extensions related discovering episodes sequences episodes sequences algorithm concept hierarchies new definitions interestingness algorithms parallel discovery classification rules classification rule mining parallel mining engine meta learning multi strategy learning bibliography chapter data analysis long object statistics fields computer science including machine learning expert systems knowledge acquisition expert systems particular techniques years applied large databases wealth knowledge buried 
growing interest businesses data mining knowledge discovery data bases kdd field called appearance data mining tools marketplace show need means handle today large growing databases 
frawley defined data mining problem nontrivial extraction implicit previously unknown potentially useful information data 
unfortunately characteristics databases sheer size impossible simply apply tools developed problems areas mentioned 
new algorithms designed take peculiarities account provide users effective convenient ways discover knowledge need 
association rule mining arm problem kdd received considerable attention past year demonstrated large number new publications :10.1.1.50.1686:10.1.1.144.4956
problems kdd certain type database special application lead specialized algorithms problem 
case retail data database consumer transactions large retail stores searched rules characterize common consumer behavior 
discovered rules may reveal example percent chance people buy spaghetti sauce buy pasta ground meat 
example intuitive rules may obvious means find required 
furthermore mining tools enable users quantify assumptions refute confirm data mining queries 
relevance rules applications marketing product planning store layout advertisement sales promotion easily conceived 
arm algorithms applied broader scope problems failure correlation complex systems analysis medical data 
association rules introduced formally chapter 
unfortunately number possible association rules grows exponentially number items considered 
items example rules considered naive approach 
algorithms proposed literature search effective apriori partition 
algorithms differ mainly respect internal data representations intermediate results io cost cpu overhead incur 
algorithms apriori needs scan entire database times 
reduce number io operations partition implements new divide conquer partitioning strategy 
reads processes database partition second scan necessary join partial results 
second novelty partition new tid list structure store partition memory opposed item list representation apriori 
report partition superior apriori needs io cpu referred basket data overhead due new features 
report investigate effect data structures partitioning concept performance algorithms 
particular sequential efficient association rules algorithm sear employs new prefix tree data structure includes optimization call pass bundling investigated literature 
sear item list representation compared sptid short sequential partitioning tid lists uses tid lists 
examine effects partitioning non partitioning sear algorithm contrasted partitioning algorithms sptid uses tid lists spear partitioned version sear spinc new incremental partitioning algorithm allows scans data 
shown table sear spear spinc item lists prefix trees ensure comparability sptid included completeness 
non partitioning partitioning incremental tid lists sptid partition sptid item lists prefix trees sear spear spinc table overview sequential algorithms experiments show 
pass bundling reduces cpu io overhead significantly 
partitioning introduces constant cpu overhead partition exceeds far benefits due reduced io cost disk io extremely expensive 
tid lists fast phases highly inefficient early phases algorithm performance avoided bypassing initial phases special optimizations tid lists 
contend partitioning contrary common opinion reduce execution time arm algorithms leads worse performance number partitions grows 
efficient tid lists requires partitioning large intermediate results swapped disk databases larger available buffer space algorithms tid lists slowed partitioning overhead cpu cost greatly reduced means bypass optimization 
contrast item lists pass bundling achieve low io cpu cost partitioning overhead 
unfortunately bypass optimization mentioned partition add sptid compare concepts directly determine performs better research required provide definitive answer question 
sequential algorithms described detail chapter results sequential experiments chapter 
spite continued improvements sequential algorithms data mining queries general arm algorithms particular remain expensive time consuming operations costly convenient interactive mining tools 
rapid growth database sizes calls higher mining speeds currently available 
sampling solution problem looking portion database produce exact results 
parallel data mining second option focus second part report parallel spmd implementations sear spear ibm sp messagepassing multiprocessor results speedup scale experiments 
assume short single program multiple data shared paradigm commonly parallel databases nature problem suits assumption :10.1.1.59.3161
findings confirm initial expectations algorithms parallelize require comparatively small amount communication achieve near linear speed diminished sequential portions algorithms 
overview kdd research give overview field kdd introduce major types knowledge desire extract data 
proceed list additional difficulties arise databases data source pattern discovery algorithms 
section ends general issues proposed architecture kdd systems 
detailed introductions kdd see 
types knowledge recalling aforementioned definition kdd frawley discovered information implicit previously unknown potentially useful state knowledge means context 
implicit knowledge data mining sense goes mere factual knowledge stored managed successfully dbms years 
fact ms works earns annually desired output data mining algorithm matter revealing surprising fact may user 
shows previously unknown understood system perspective respect user current level knowledge 
kind knowledge called meta knowledge patterns characterize data hidden laws structures need strict functional dependencies may hold certain probability 
frawley demands statement discovered pattern somewhat simpler subset data objects describes 
simpler means left vague intentionally 
length encoding information theoretic measures reasonable widely 
short interested facts data term knowledge shall meaning 
requirement potentially useful highly dependent application special focus current mining task 
issue revisited sections background knowledge related discussed section section 
agrawal identifies types knowledge discovered databases classification association sequences 
classification tries divide data set disjoint classes supervised unsupervised learning referred clustering 
goal find set predicates characterize supervised case predefined class data objects applied unknown objects predict class membership 
bank example want classify credit customers determine give loans 
frawley shapiro subdivide task summarization searches common characteristic features class discrimination goal find features help distinguish different classes alternatively class 
discovering sequences time additional attribute questions concentrate dynamic patterns 
examples stock market data consumer behavior 
special algorithms sequence discovery discrete fourier transform speed pattern search refer reader 
interesting problem involving sequences discovery episodes frequent partially ordered sets events occur time window 
algorithms finding episodes similar arm described comparatively detailed way section 
machine learning terminology concept descriptions third type knowledge association rules 
associations arbitrary rules form conjunctions attribute value restrictions 
introduced area treat depth section 
proposes unified framework areas arguing variations basic rule discovery task reduced set standard operations 
fact classification viewed discovering antecedent rules class label consequent 
rule salary 
average balance age class low credit risk may suffice example 
performance reasons advisable exploit application characteristics specialized algorithms different tasks 
observation supported large number highly specialized algorithms proposed arm 
database specifics argue aspects data mining different standard machine learning problems 
special situation database data source causes additional difficulties 
size data sets training sets machine learning rarely exceed items databases hundreds thousands millions items common today trend rising 
introduces problem handling data potentially large amounts intermediate results 
furthermore current data mining algorithms view database global relation 
universal relation assumption aggravates size problem databases split relations normal form save disk space joined large relation 
noise missing contradicting data databases typically generated maintained purpose data mining data intended serve interest application facilitate mining job 
worst case attributes indispensable discovery task hand may missing altogether bad wrong results produced 
smaller scale task harder mining algorithms null values occur frequently relational data 
basic choice dealing null values replace default values determined conditional probabilities available fields ignore data object altogether avoid presumably dangerous wrong choice 
alternatives inherently flawed may produce wrong results 
resort system try obtain missing values user external sources 
usually feasible 
real world data noisy contains contradictory information due errors simply due nature data 
best possible input standard learning algorithms 
probabilistic methods needed deal difficulties 
redundant information discovery algorithms supposed detect patterns database undesirable previously known dependencies 
identifies cases trivial patterns wrongly reported knowledge due redundant information stored database 
case strong functional dependencies field function fields example profit sales expenses 
case occurs field values merely constrained fields 
additional unfortunately frequent situation related concept hierarchies 
higher concept stored data object considered pattern algorithm knowledge interest user 
examples city state department division large aids 
difficulties indicate mining process directed supported background knowledge domain knowledge issue talk section 
knowledge representation knowledge discovered readable humans allow easy retrieval items database match discovered rule 
pretty rules neural nets learned information usually hidden internal layers net 
fine learning robot motions sales manager wants know products arrange 
difference take example arm symbolic learning produces actual rules neural net decide rule significant 
reason neural nets prohibit database query language optimized retrieval items match rule 
fast indexed selection scan entire database necessary 
argued symbolic learning neural nets multiple passes training set necessary learning process 
favors symbolic representations predicates called decision trees predominantly classification 
general issues users mine mining tools able able determine knowledge discover interest user 
mining tool easily flood user trivial domain knowledge pointed previous sections 
comes surprise example pregnant patients female 
interactively directing discovery process desired 
requires new means formulate highly complex queries way acceptable domain expert sophisticated means result representation short high system user bandwidth vice versa 
example association rule discovery investigates rule templates visualization results 
clear just user interaction desired necessary answer question may dependent mining domain 
systems various degrees independence exist tradeoff versatility independence discovery system typify kdd systems 
human loop requires fast response times execution times minutes tolerated 
demands users discovery tools create additional challenges kdd algorithms deserve attention 
users want fast accurate results quick look data focus points interest demanding accuracy reliability go 
typical usage patterns raise issues result reuse result refinement may efficient rerunning algorithms scratch user query 
storing discovered knowledge consistency growing data knowledge bases dynamic environments examples problems addressed interactive mining systems 
including domain knowledge point strongly related previous tries contain amount insignificant information generated mining algorithms 
domain knowledge elaborate systems common sense reasoning filters designed prevent user 
providing discovery tool domain knowledge speed discovery process algorithm take previous knowledge identify certain paths useless abandon 
obvious example name patient usually decisive factor diagnosis illness 
attribute removed entirely mining algorithm invoked 
problem selecting exploiting knowledge investigated 
examples domain knowledge concept hierarchies user defined predicates automated selection relevant attributes reduce data sizes running time 
kdd system architecture internal external miners mining tool viewed application uses dbms server data management requirements 
case called external discovery tool internal miners read data dbms convert format working independently point 
external tools necessarily tailored database interface sql dbms usually optimized data mining performance may suffer approach 
mentioned universal relation assumption lead huge amounts data repeated expensive joins 
relations inverted convert format suitable mining 
consider example grouping type history relation account number 
internal tools hand need storage data access efficient especially large parts data need considered passes algorithms internal miners prune data set way 
hybrid approaches mining algorithms rely heavily database functionality possible course 
argue mining capabilities provided applications separate actual database integral part dbms capabilities 
vision dbms able provide user insight data just selection aggregation 
interesting questions arise context concerned tradeoffs support oltp mining algorithm needs fast access entire database 
words question couple knowledge base database model kdd conclude chapter presenting model reflects observations previous sections 
section follows roughly idealized model kdd systems proposed 
actual systems may map scheme major components represented 
depicted input system comes user runs mining queries database form domain knowledge database 
knowledge base store pre supplied domain specifics possibly knowledge acquired previous mining sessions 
background knowledge focus component selects part database pertinent current task db interface creates actual database query retrieve data 
note model precludes internal external mining 
pattern extraction denotes set actual mining algorithms evaluation component filters discovered patterns interestingness 
model proposed added user interface display components emphasize need non triviality convenient effective user interaction 
current research investigating primarily areas pattern extraction core component kdd tool inter working domain knowledge focus extraction evaluation process user interface aspects 
subsequent chapters focus pattern extraction problem association rules 
issues general way surface narrow perspective 
dbms domain knowledge input user focus display controller user interface database pattern discovery knowledge base evaluation discoveries general model kdd systems structure report remainder report structured follows 
chapter introduces association rule mining formally provides overview previous algorithms 
sequential algorithms described chapter covering algorithms sear sptid spear spinc order 
subsequent chapter reports results experiments sequential algorithms focusing effects different data structures benefits partitioning 
second part report dedicated parallel implementations sear spear algorithms 
major concern investigate algorithms parallelize contrasting algorithms compares non partitioning partitioning algorithm providing insight parallel partitioning 
chapter describes algorithms pear parallel sear ppar parallel spear results parallel experiments algorithms chapter 
short overview interesting extensions related chapter conclude report summary results list research topics chapter 
chapter association rule mining chapter introduces association rule mining detail starting formal problem statement properties association rules frequent sets play central role arm 
second part chapter dedicated describing evaluating previous algorithms problem 
related treatment concept hierarchies parallel kdd projects episode discovery sequences briefly covered chapter 
association rules section give formal definition association rule mining problem 
introduce frequent sets form basis generating actual association rules 
stated follow initial terminology 
definition frequent sets set fi items set items sold retail store transaction defined subset items 
sets transactions contain duplicates extend pure notion set assume items transactions itemsets may consider sorted 
database set transactions transaction labeled unique transaction identifier tid 
transaction said support set contains items need refer set transactions support denote set tids transactions 
define support abbreviated supp fraction transactions support supp min minimum support value min set called frequent motivation minimum support want concern itemsets occur interesting 
infrequent itemsets minimum support considered interesting 
call itemset cardinality jx itemset 
properties frequent sets helpful properties particular form foundation arm algorithms 
note special term transaction arm context 
different usual context dbms term applies data consumer transactions 
early refer frequent sets large sets lead confusion cardinality set chose adopt change terminology proposed :10.1.1.144.4956
property support subsets itemsets supp supp transactions support necessarily support 
property supersets infrequent sets infrequent itemset lacks minimum support supp min superset frequent supp supp min property 
property subsets frequent sets frequent itemset frequent supp min subset frequent supp supp min property 
particular fi frequent subsets frequent 
note converse hold 
definition association rules association rule implication form disjoint itemsets 
furthermore required 
rule understood prediction transaction supports support certain probability called confidence denoted conf rule 
confidence defined conditional probability supports support formally conf supp supp support rule defined supp 
confidence rule reveals expected apply support indicates trustworthy entire rule rule relevant needs support sufficient confidence 
say rule holds respect fixed minimum confidence level min fixed minimum support min conf min supp min note necessary condition rule hold antecedent consequent rule frequent 
illustrate importance requirements minimum support minimum confidence assume base rule just data object 
rule maximum confidence describe common pattern database discarded lacks minimum support 
assume rule support low confidence example says customers buy soap purchase 
fact supported data database relevant express strong correlation 
properties association rules requirement antecedent consequent disjoint absolutely necessary lead nonsense rules redundant insignificant ones 
example trivially true equivalent interesting 
antecedent rule may empty transaction considered support empty itemset entire database satisfies antecedent 
confidence rule equal relative frequency consequent itemset 
consequent required non empty reason demand antecedent consequent disjoint 
recall confidence rule interested conditional probability transaction supp supp 
shows fact rules equivalent 
opposed functional dependencies require strict satisfaction association rules generally transitive compose 
cases rule holds inferred confidence rule 
observations stated formally list property composition rules hold necessarily true consider case transactions support support set support confidence 
similar argument applies composition rules antecedent property decomposition rules holds may hold 
case example transaction supp supp 
support sufficiently greater supp individual rules required confidence 
situation depicted 
circles denote set transactions support itemset labelled 
counter example composition rules converse holds supp xy supp xy supp xz supp xy 
support confidence smaller rules increase compared original rule 
unfortunately help rule construction process build larger rules smaller ones way 
property transitivity hold infer assume instance ae ae minimum confidence level min conf conf min relative support values get conf min min min confidence rule hold 
note computation set inclusion 
general confidence values multiplied manner 
property inferring rules hold show rule 
gamma minimum confidence 
gamma itemsets supp supp property definition confidence obtain conf 
gamma supp supp supp supp likewise rule gamma holds gamma consequent required non empty 
property speed generation rules frequent sets support determined 
see section details algorithms 
possible definitions preceeding definition restricted version association rules general form may contain negations disjunctions 
disjunctions express different variations product sold difference relevant butter 
approach successful additional expressivity adds complexity discovery algorithms 
furthermore problem deciding interestingness rules harder large disjunctions arbitrary items easily reach minimum support database 
hierarchical solutions describe section better suited treating groups items 
rules contain negation suffer similar drawbacks 
rule space searched grows immensely discarding vast amount irrelevant rules difficult 
intuitively obvious large number possible itemsets number sets items bought exceeds purchased orders magnitude 
note limiting rules conjuncts antecedent consequent restricts patterns possibly find 
may interesting piece information people buy coke chips display strong dislike dissociation expressed rules 
basic algorithmic scheme mining association rules definitions proceed describe structure basic rule discovery algorithm 
algorithms different basic scheme components may arranged differently entire scheme applied repeatedly 
construct association rules support frequent itemset database computed 
algorithms proceed stages frequent sets generated second phase charged generating actual rules confidence frequent sets 
finding frequent sets noted number potential frequent sets equal size power set items grows exponentially number items considered 
straightforward algorithm perform exhaustive search test set power set frequent 
basic method algorithm follows create set itemsets called candidates believes frequent 
candidates created sequence depends individual algorithm 
find candidate itemsets frequent exact support support candidate set counted pass database 
counting occurrences candidate set involves considerable amount processing time memory obvious goal reduce number candidates generated algorithm 
rule construction soon support values available possible rules created confidence determined 
frequent set proper subsets chosen antecedent rule remaining items constitute consequent 
frequent subsets frequent property support known 
confidence rule computed rule accepted discarded depending minimum confidence level 
improvements gained property rule fails subsets antecedent considered 
compared task finding frequent sets step straightforward 
previous algorithms arm algorithms proposed literature separate rule construction finding frequent sets 
interesting part solved differently individual algorithms 
focus finding frequent sets section 
algorithms major data representations store database item lists candidate lists tid lists 
describe advantages disadvantages introduced argue choice representation influences performance algorithms 
ais problem association rules introduced algorithm called ais authors 
tid tid transaction frontier set extensions extensions illustration extensions ais find frequent sets ais creates candidates fly reads database 
passes necessary pass entire database read transaction 
candidate created adding items sets frequent previous passes 
sets called frontier sets 
candidate created adding item frontier set called extension item added avoid duplicate candidates items larger largest item considered extensions 
avoid generating candidates occur database ais build extensions blind faith encountered reading database 
illustrates extensions created transaction read supports frontier set illustrates concept extended extensions formed adding items frontier set 
associated candidate counter maintained keep track frequency candidate database 
candidate created counter set subsequently transactions counter incremented 
complete pass transactions counts examined candidates meet minimum support requirement new frontier sets ideally frontier sets discarded possible extensions considered keeping pass create candidates evaluated 
cases storage requirements candidate sets frontier sets exceed main memory frontier sets swapped disk 
extensions generated point abandoned produced pass cheaper swapping disk 
new frontier sets determined candidates minimum support extension counting phase begins frontier sets left means previous candidates frequent 
initially frontier set extended itemsets pass turn extended itemsets second pass 
unfortunately candidate generation strategy creates large number candidates sophisticated pruning techniques necessary decide extension included candidate set 
methods include technique called pruning function optimization estimating support prospective candidate relative frequencies subsets 
pruning functions fact sum carefully chosen weights item rule certain sets candidates counting 
example total transaction price 
fewer transactions minimum support fraction exceed price threshold sets expensive possibly frequent 
decisions fairly costly repeatedly subsets transaction 
candidate set rejected decision transaction set appears 
initial rules restricted item consequent allowed union items antecedent 
limitation justified algorithm finds frequent sets information produce rules limitation 
setm setm designed standard database operations find frequent sets 
reason uses data representation stores itemset supported transaction transaction tid 
shows part example run setm tiny database illustrates database stored table tid records 
setm repeatedly modifies entire database perform candidate generation support counting removal infrequent sets 
representation algorithm database contains candidate itemsets frequent itemsets depends current stage 
explain algorithm 
assume infrequent items deleted 
part stage contains frequent itemsets 
form joined equal tids shown 
transaction contains candidates supports accompanied tid 
task delete infrequent candidate itemsets produce table contains frequent itemsets 
done sorting itemsets simplification determining expansions include candidates tricky presence extensions support estimation 
extensions example maximal frequent sets frontier sets 
details refer reader tid tid items items tid tid items database tid items join items sort infrequent itemsets delete sort tids join minimum support occurrences tid items items items tid part sample run setm counting support means aggregation deleting infrequent sets 
compute candidates supported transaction accomplished join insert select tid item item item item lk lk tid tid item item item gamma item gamma item item unfortunately currently sorted itemsets tids order required carry join efficiently 
sorted tids self join 
created sorted itemsets infrequent sets removed lk empty 
problem algorithm candidates replicated transaction occur results huge sizes intermediate results 
itemsets stored explicitly listing items ascending order 
candidate ids save space join carried sql operation 
worse huge relations sorted twice generate larger frequent sets 
advantage novelty approach fact setm creates fewer candidates ais 
assume ais reads small example database third pass itemsets frontier sets 
looking transaction ais notice supports frontier sets contains item 
stops ais generating candidates algorithm apriori gen insert select item item item item item item item gamma item gamma item item prune rules subsets missing forall itemset forall subsets delete item frequent 
candidates considered setm 
spite advantage reported inefficiencies setm outweigh far ais 
apriori aprioritid apriorihybrid candidate generation apriori gen vast number candidates ais creates caused authors develop new candidate generation strategy called apriori gen part algorithms apriori aprioritid 
apriori gen successful reducing number candidates algorithm proposed published :10.1.1.50.1686:10.1.1.144.4956
underlying principle property generate candidates subsets previously determined frequent 
particular candidate accepted subsets frequent 
assume candidates size created 
shown sql code algorithm apriori gen takes set frequent itemsets input searches pairs sets smallest items common 
common items different items sets joined form prospective candidate 
duplicates avoided demanding largest item second set greater largest item 
far frequency subsets asserted mere presence input set allowed candidate created place 
note point apriori gen similar candidate generation strategy setm 
novelty apriori gen existence remaining subsets candidate tested second part algorithm 
example frequent joined create candidate set 
subsets remain checked infrequent candidate discarded 
note pair frequent sets differ largest set 
consider frequent sets differ third item 
candidate created pair 
set candidate subsets generate 
candidates simply sets contain item procedure necessary generate 
apriori gen generate idea suggested independently called line candidate determination ocd second novelty apriori gen leading name candidate generation done prior separate counting step algorithm called create candidates size 
improvements achieved apriori gen candidate generation strategy employed ais fold fewer candidates created secondly created repeatedly transaction 
setm creates candidates algorithms apriori gen creates transaction 
apriori apriori algorithm apriori gen candidate generation 
mentioned previous paragraph apriori gen separate counting step determines frequency current candidate 
means pass apriori consists call apriori gen generate candidates size size pass counting phase determines support candidates 
counting phase scans entire database 
reading transaction counting phase pass apriori determine candidates supported increment support counters associated candidates 
order perform operation efficiently apriori stores candidate sets tree 
illustrates structure candidates possible items 
actual sets stored leaves tree edges labeled items 
find proper location candidate starting root traverse edge item set 
reaching internal node choose edge labeled item set leaf reached 
path locate set marked arrows 
note virtue ordering items set unique place tree 
smallest items set path leaf need stored 
inserting sets tree cause leaf node overflow case split tree grows 
set apriori tree structure candidates count candidates transaction leaves contain candidate searched reach leaves apriori tries possible combinations items paths leaf 
leaf set candidates located fashion remains checked supported transaction 
far implementation concerned test set inclusion optimized storing sets bitmaps bit item 
observed bitmaps quite large items bytes items cause considerable overhead 
internal nodes implemented hash tables allow fast selection node 
reach leaf set start root hash item set 
reaching internal node hash second item leaf 
implemented different hash tree structure algorithms resembles apriori avoids overhead searching candidates leaves 
data structure explained section 
item lists major problem apriori ais read entire database pass items transactions longer needed passes algorithm 
particular items frequent transactions contain items current candidates necessary 
removing obviate expensive effort try count sets possibly candidates 
apriori include optimizations hard add apriori ais 
reason stems item list data representation algorithms 
depicted transactions stored sequence sorted representation 
item lists common representation usually assumed input format difficult remove unnecessary parts data 
assume want remove items part frequent set 
unfortunately knowledge items keep discard available applicable scanning database count support candidates 
eliminate items subsequent pass data read really necessary 
see representations remove items instantly leads smaller data sizes passes unfortunately case early passes volume intermediate data representations exceed original data size 
advantage item lists size data grow course algorithms 
tid items item list data representation aprioritid shortcoming apriori remove unwanted parts database passes prompted authors develop aprioritid uses different data representation item lists apriori 
aprioritid considered optimized version setm rely standard database operations uses apriori gen faster candidate generation 
furthermore aprioritid reads data tries buffer data passes 
pass consists apriori gen call followed counting phase determine support current candidates 
setm aprioritid stores transaction list current candidates supports 
representation call candidate lists depicted 
note database transactions name conflicts terminology data representations suggests tid lists true 
fact authors tids illustration aprioritid need 
stored fashion sets candidates frequent itemsets lists items ascending order usual 
initial conversions necessary create candidate lists common input structure item associated corresponding candidate set pass candidate sets size difference representation item lists 
pass data changes transaction candidate sets supports stored second pass replaced candidates size 
data previous passes needed 
counting support candidates easy candidates stored explicitly transaction 
database tid items minimum support occurrences tid candidates tid candidates tid candidates candidate list data representation interesting part aprioritid list candidates supported transaction derived set candidates supported auxiliary data structures useful accomplish task running apriori gen create candidates store candidate itemsets generation 
furthermore itemsets candidate extensions kept 
computed follows frequent candidate set consider candidate extensions computed apriori gen check generating itemsets add compared setm aprioritid advantage candidate identifiers explicitly listing items candidate set effectively reduces size intermediate results furthermore auxiliary data structures save wasted setm sorting joining create candidate sets 
importance aprioritid stems fact algorithm read database copy memory point 
idea limited algorithm limits size database limits overcome partition algorithm section 
candidate lists disadvantage candidate lists size intermediate results smaller handled setm times original data size 
reason transaction frequent items supports gamma delta candidates 
gamma candidate identifiers stored pass compared candidates item original database 
experience synthetic datasets items typically frequent means extreme cases 
example transaction items frequent candidates opposed candidates 
matters worse size intermediate results known means sampling estimate characteristics data set 
note behavior feature candidate lists independent specifics aprioritid 
handle cases intermediate results exceed main memory aprioritid uses buffer manager swaps part data disk temporarily 
constitutes inherent limit size data processed efficiently aprioritid 
advantage candidate lists useless parts data discarded automatically process 
transaction support current candidates removed 
course current candidates supported transaction stored number expected small candidates items 
data large pass size decreases rapidly subsequent passes 
apriorihybrid combining apriori aprioritid performance tests conducted agrawal show apriori aprioritid outperform ais setm 
comparing apriori aprioritid interesting generate number candidates differ mainly underlying data representation 
apriori avoids swapping data disk weed useless items passes wastes time futile attempts count support sets involving items 
aprioritid hand prunes data set described previous section result apriori passes 
unfortunately slowed mainly second pass data fit memory consequence candidate list representation swapping necessary 
case apriori beats aprioritid 
reason algorithm apriorihybrid proposed uses apriori initial passes switches aprioritid soon data expected fit memory 
switch takes extra effort transform representation balanced savings passes 
hybrid version led improvements apriori aprioritid long switch offset extra effort performed slightly worse 
furthermore size database mined hybrid version limited aprioritid 
partition algorithms far variations scheme partition algorithm takes somewhat different approach 
doing partition tries address major shortcomings previous algorithms 
see section description commonly accepted generation procedure exact corresponding itemsets frequent 
section 
problem previous algorithms number passes database known regardless representation 
number io operations known large 
aprioritid tries circumvent problem buffering database database size limited size main memory 
second problem lies pruning database passes removing unnecessary parts data 
item lists ais apriori suited optimization said earlier section 
candidate lists permit pruning database cause problems unpredictably large intermediate results early passes 
describe detail partition addresses shortcomings paragraphs 
examine chapter severe problems really counter measures perform algorithm sptid uses features partition 
implementation sptid described chapter 
partitioning database approach taken partition algorithm solve problem unpredictably large io cost divide database equally sized horizontal partitions 
algorithm determine frequent sets run subset transactions independently producing set local frequent itemsets partition 
denote local frequent itemsets partition note minimum support level percentage number transactions database partition respectively 
minimum support partition transactions equal transactions regardless large actual database partition size chosen entire partition reside memory 
read necessary step passes access buffered data 
principles apriori gen applied candidate generation 
data partition different different effect support sets available partitions 
counts necessary decide set globally frequent 
obtain called global support itemsets scan database required called counting phase context partition algorithm 
know sets count phase global candidate set formed union local frequent sets 
sets counted ones frequent partition counts available 
counting phase global support values available sets minimum support discarded leaving frequent sets 
total io requirement scans regardless database characteristics 
procedure correct contains possible frequent itemsets 
set frequent entire database frequent partition expressed property 
needless say converse hold counting phase necessary 
property support partition horizontal partitioning fp itemset frequent exists partition frequent proof contradiction assume frequent frequent 
min fixed minimum support level jdj jp number transactions respectively 
algorithm partition compute partition ig forall partitions read partition gamma gamma forall candidates size generate tid list jt min fcg drop candidate iig forall partitions read partition forall generate tid list count jt return fc jc count min frequent follows supp min frequent supp min supp supp min min supp min contradicts assumption frequent tid list data representation address second problem failure reduce database size passes partition uses new tid list data representation determine frequent itemsets partition count global support counting phase 
tid lists invert candidate list representation associating itemset list tids transactions support set words database transformed set pairs 
illustrates representation initial database intermediate results extended version sample database 
minimum support equivalent transactions marked vertical line 
tids required preserve association items transaction 
tid lists candidate computed easily intersecting tid lists subsets 
tid lists sorted intersection computed efficiently merge join requires traversing lists 
candidate lists tid lists change pass may swapped disk memory available store 
size intermediate results larger original database tid items minimum support occurrences tid representation itemset tids itemsets tids join candidates tid list data representation data size known 
reason candidate lists section difference ins partition tids replicated candidate set replicating candidate identifiers transaction 
size problem severe tid lists candidate generation counting done merge join operation 
candidate infrequent dropped immediately 
example shown lists crossed 
shows example size intermediate data structures changes depending minimum support 
data originate runs implementation partition mbytes database different items transactions average size 
seen data size grows lowest minimum support level data sizes decrease continuously minimum support values 
increases data size severe tid lists problem remains amount increase easily estimated moment 
second important disadvantage experiments significantly costly early passes algorithm comparable steps representations 
explain problem detail section 
tid lists hand permit removal useless data tid lists infrequent items dropped easily tids transactions support itemset omitted automatically merge join 
seen data sizes drop quickly passes minimum support levels 
paragraphs comment individual aspects partition problem choosing size partition details candidate generation apriori gen slightly different way counting phase carried 
choice partition size partition negatively impacted data skew causes local frequent sets different 
event global candidate set large renders counting phase expensive 
smaller partitions negative effects due data skew itemsets frequent partitions partitions 
size bytes pass number data size size intermediate data various minimum support values cases adversely effect performance increase number sets counted phase ii hand partition size chosen large risk increases intermediate results exceed available buffer space necessitate swapping data disk slowing algorithm 
unfortunately size intermediate data structures known heuristics applied choosing partition size 
heuristics detailed 
heuristic take account memory size average transaction size current minimum support threshold number items number items minimum support necessary estimate number frequent sets 
understand dependency consider expected frequency itemsets total number transactions 
assuming uniform distribution usually actual item distribution serves illustrate minimum support results larger frequent sets database different items 
estimates inaccurate actual decisive factors size number frequent sets known result algorithm 
possible improvement adaptive strategies exploit knowledge gathered preceeding partitions adjust partition size dynamically 
pursue issue leave research 
candidate generation counting local support candidates generated way apriori gen counting support candidate done immediately creation shown algorithm 
candidate created union frequent itemsets support current partition jt jt obtained merge join sorted tid lists 
frequent stored building larger candidates 
note partition sizes counting phase different sizes phase partitions equal size 
counting global support phase ii soon global candidate set available global supports counted 
candidates frequent partition need counted removed phase uses tid lists merge joins obtain support values 
general counting phase expensive phase sets known frequent counted 
phase tid lists generated candidate mentioned repeatedly number greater number frequent sets 
concludes list previous algorithms continue describe chapter 
chapter sequential algorithms order investigate effects different data representations benefits partitioning implemented algorithms listed table 
study tradeoffs item list tid list data representation implemented sear algorithm uses item lists conjunction new prefix tree structure storage frequent sets candidates 
sear implements optimization call pass bundling benefits examined literature previously 
sear compared sptid tid lists 
fact sptid uses partitioning number partitions set experiments exclude influence 
study effects partitioning implemented spear partitioned version sear new incremental partitioning algorithm call spinc reduces io regular partitioning 
algorithms item list representation prefix tree structure comparable non partitioning sear algorithm 
included comparison 
chapter describes sear prefix tree structure section followed sptid section spear section 
explanation spinc section concludes chapter 
performance results reported chapter 
sear modifying apriori apriori pass sear consists candidate generation phase followed counting phase 
apriori gen procedure create candidates 
fact pseudo code sear algorithm apriori 
sear uses prefix tree data structure itemsets developed improve data structure apriori pass bundling optimization mentioned briefly literature investigated closely 
prefix trees pass bundling described 
prefix trees storage sets candidates recall apriori stores candidates tree data structure depicted 
candidate sets stored leaf nodes accommodates candidates 
purpose internal nodes direct search candidate proper leaf 
contrast sear employs prefix tree structure distinction internal leaf nodes 
structure nodes contain sets information sets counters 
edge tree labeled item node contains information set items labeling optimization mentioned specific name 
algorithm sear ig fall forall transactions forall subsets count fc jc count generate candidates return edges path root 
example shown set marked path 
items set understood path descriptor reach node sorting items required order avoid ambiguity nodes correspond set 
frequent sets candidates prefix tree storage sets candidates difference structures prefix trees store frequent sets candidate sets tree 
candidates counted determined frequent simply remain proper position tree frequent sets 
node tree contains frequency counts corresponding set 
root represents empty set counter equal number transactions database transactions support empty set 
candidate sets count equal zero processed 
shows complete tree containing subsets normal conditions frequent sets candidate sets stored tree infrequent sets created place deleted immediately 
normally level candidates expanded actual run sear 
candidates shown order illustrate unbalanced shape tree immediate result traversing edges sorted order items 
properties stated storage structure store frequent sets property reformulation property counts nodes path non increasing 
property reformulation property set frequent tree subsets proper place tree 
property consider parent node count node subsets parent path missing edge path node set occur frequently database subsets 
property computing confidence rule 
note property restricted sets path root naturally exist 
example set tree clearly path property tree 
illustrate differences prefix trees apriori structure example shows support candidates counted 
consider transaction assume candidates candidates supported assume apriori stored leaf candidates prefix supported node reached root traversing edge labeled item item 
apriori tests items reach leaf check candidates supported 
items considered larger items candidate set check sets stored means comparisons candidate example 
sets stored bit masks apriori needs integer comparisons candidate case items bytes integer sear reaches node apriori selects edge item directly item increments count attempt find edge leaving node fails candidate 
sear returns node selects edge edge increments count amounts total edge selection operations implemented efficiently hash table look ups 
operations necessary larger transactions apriori need additional comparisons find candidate items transaction 
note algorithms overhead reduced removing items part candidate transaction 
summarize apriori subset method uses tree reduce number candidates tested transaction method uses tree reach exactly candidates supported transaction 
mentioned prefix trees contain frequent itemsets current candidates 
shows tree minimum support equivalent items 
sets removed fewer candidates created candidate generation phase 
consider set instance 
frequent set created candidate 
prefix trees sear dead branches tree storing candidate sets frequent sets tree cause severe performance degradation support candidates counted 
consider 
set candidate supersets 
keeping tree means transaction contains items try modify counts non existent candidates branch 
clearly waste effort 
set frequent want keep tree order generate rules 
frequent sets candidates candidates generated practice nodes produce candidates call dead branches pruned tree stored separately way revived easily needed point time 
frequent sets candidates dead branches tree dead branches candidate generation candidate generation performed apriori gen tree structure provides fast test subsets prospective candidates frequent 
furthermore sets needed create prospective candidates readily available exactly direct siblings tree 
generate candidate extensions set need consider sibling items 
sorting frequent itemsets necessary algorithms 
actual implementation optimizes candidate generation combining tests candidates possibly produced node immediate siblings testing single prospective candidate set time 
motivation optimization candidate similar subsets accessing tree separately repetition effort 
candidates example existence subsets checked 
combining tests saves having descend branches repeatedly 
pass bundling candidates frequent counting pass new candidate checked 
fact candidate left entire pass database necessary count just set 
avoid waste effort sear allows expansion levels tree counting candidates 
candidates example created counted 
technique call pass bundling mentioned effects examined 
tradeoff involved number io operations save additional computation necessary count additional candidates 
clearly pass bundling levels desirable passes number candidates small 
expanding level early results pruning candidate set 
pass bundling implemented setting lower limit number candidates created counting step allowed 
choosing pass bundling factor low multiple number items ensure early passes effected number candidates exceeds limit far passes bundled 
implementation prefix trees provide fast selection edge traversing prefix tree hash table associated node children 
illustrated entries hash buckets records count item number pointer child hash table 
hash table dead pointer store linked list dead items roots dead branches 
crucial terms resource usage speed size hash tables 
root table fairly large tables deeper tree smaller 
candidates leaf generated simultaneously number known compute size hash table 
size hash table power allow fast computation hash function simple operation 
hashtable dead joint representation edge node item count sibling children hash table fast edge selection implementation prefix trees internal hash nodes addition prefix tree list items items members candidate sets maintained candidate generation 
list possible screen items transaction longer relevant reducing number failed hash operations counting 
experiments typically items part candidate passes 
sptid partitioning algorithm short section describes implementation details sptid implemented description partition 
experiments unable obtain results comparable reported 
reason algorithm obtain results uses important optimization mentioned 
optimization basically skips second pass counting support candidates directly 
suspected confirm early include report 
return issue performance section tid lists section 
distinguish algorithms refer implementation conforming description sptid name partition optimized version algorithm 
similar partition sptid works tid list representation partitions data ensure database scanned times 
support candidate determined immediately generated frequent sets 
compute support tid lists frequent sets joined merge join 
minor difference sptid description original sptid uses prefix tree structure store frequent sets candidate generation technique sear section 
prefix tree important sear fast access candidates indispensable counting support database 
case sptid support candidates generated means tid lists fast access frequent sets required 
prefix trees vital sear merely implementational detail sptid 
candidate generation code simplifies comparison tid lists item lists results obscured different candidate generation procedures 
implementation stored tid lists buffer pages database buffer manager handle swapping pages disk necessary 
providing robust storage interface easily cope unexpectedly large intermediate results additional layer caused intolerable overhead 
reason switched specialized buffer manager stores tid lists dynamic arrays writes disk lru strategy memory tid lists exceeds permitted buffer size 
improved performance significantly results reported chapter obtained optimized version 
just tree store frequent sets matter partition frequent subset relevant current partition 
advantage approach additional structures maintained getting unused sets way accomplished easily declaring branches dead 
nodes sptid prefix tree slightly different sear tree contain address corresponding tid list addition frequency counters 
algorithm spear compute partition ig forall partitions read partition ig fall forall transactions forall subsets count fc jc count min generate candidates iig forall transactions forall count return fc jc count min ng spear algorithm section describes spear algorithm uses item lists prefix trees sear partitions database sptid 
algorithm realization partitioning principle require tid list representation combination chosen avoid poor performance tid lists section reduce io cost due partitioning 
partition loaded memory processed completely sear entire database 
sptid scans database required 
algorithm shows pseudo code spear 
advantage item lists data size grow course algorithm buffer management required 
major disadvantage mentioned section pruning transactions possible extent sptid leading higher cpu costs passes 
counting phase spear basically single counting pass sear active sets tree considered candidates 
sets frequent partitions hidden dead branches avoid counting 
data structure little different sear nodes store counters local global support number partitions set frequent 
furthermore just tree store candidates frequent sets sets frequent current partition avoiding confusion tree tricky implementation details significant explained 
spinc incremental algorithm fourth algorithm study spinc version spear uses new incremental partitioning technique 
incremental method process partitions independently spear uses partial results preceeding partitions 
basic principle count set partition frequent 
recall set frequent partition part global candidate set counted phase ii spear algorithm 
may mistake remove set consideration current partition frequent 
set happened frequent previous partition support assessed current partition anyway counting phase 
specific consider case assume database divided partitions set frequent frequent partitions 
sptid spear support partition counting phase starts 
counted partitions 
frequent count support phase succeeding partitions phase accumulated support available partitions 
phase ii support need counted missing partition 
spinc produces immediate savings set counted twice partitions frequent 
directly reduces amount computation 
furthermore partition counted phase ii saves io operations partition allows algorithm passes data 
reason candidate element frequent time support available frequent previous partition case support partition counted 
generating frequent itemsets phase spinc outlined algorithm differ greatly counterpart spear 
sets previous frequent itemsets grow partition partition new frequent sets added pass partition 
candidates newly generated partition labeled partition number stored partition known phase ii set frequent 
check candidate simple due fact sets kept tree structure inserting new candidate tree simply test 
basic idea algorithm simple complicated need distinguish candidates current partition sets counted frequent earlier partition 
separating sets necessary candidates pass generated actual candidates possibly larger union completing global support counts candidate sets removed prior counting partition frequent 
number sets counted decreases partition partition reaches just partition possibly earlier 
algorithm shows formally accomplished 
note contrary partition sptid partitions counting phase exactly phase avoid missing duplicate counts candidates removed right time 
algorithm spinc phase compute partition forall forall partitions read partition ig fall forall partition forall transactions forall subsets count previously frequent sets count fc jc count min fadd local frequent sets previously frequent added sets generate candidates forall partition return data distribution expected large randomized data sets hope new sets partitions 
partitions considered counting phase 
partitions read 
lowering minimum support possible extension spinc increase savings effect counting phase lower minimum support level slightly phase cause sets counted phase possibly save io phase ii 
illustrate reason example 
partitions assume set set frequent partition 
single set entire partition read counting phase sets counted partition phase chances lower minimum support phase frequent partition reading partitions gamma necessary phase ii 
note lowering minimum support increases computational cost phase locally frequent sets 
min chosen low savings io easily offset increased cost counting 
lowering minimum support level expected delicate trade io savings increased cpu cost 
spinc expected reduce io cpu overhead compared spear 
results performance tests verify expectation section 
algorithm spinc phase ii input true global candidate fc jc partition ig break read partition forall transactions forall count return fc jc count min chapter experiments sequential algorithms chapter reports results sequential experiments 
brief description data generation technique create data sets show performance results sear particular effects pass bundling significantly reduces io cpu overhead 
tid lists item lists compared section focussing algorithms sear sptid sptid partition 
show tid lists sptid inefficient early passes algorithm sear outperforms sptid parameter settings reason uses multiple scans database 
section investigates usefulness partitioning comparing algorithms sptid sear spear incremental spinc 
main comparison covers item list algorithms sear spear spinc sptid included completeness shown tid lists remains handicap sptid number partitions increases 
experiments run sp thin node equipped mhz rs processor mb main memory gb scsi disks 
experiments shown mb sec raw input performance obtained disk 
tasks system 
timing results show system time obtained program gettimeofday system call 
keep operating system buffering entire database flush system buffers scan writing mb file 
course clock turned writes 
synthetic data generation experiments performed synthetic data generated procedure outlined designed model buying behavior consumers retail environment 
note notation varies slightly 
main characteristics retail data items sold frequently sets items purchased 
achieve grouping items put transactions randomly taken set maximal potentially frequent sets just maximal frequent sets basic idea create transactions unions maximal frequent sets 
arm algorithms rediscover maximal frequent sets database combinations subsets 
generate transaction size computed poisson distribution mean jt maximal frequent sets chosen fixed probability assigned set 
probability chosen exp distribution 
maximal frequent set selected information sp section maximal potentially frequent sets jdj number transactions jt average size transactions jij number items jf number maximal potentially frequent itemsets jf average size set corruption level correlation level table parameters synthetic data generation name transaction size max 
transactions data size size jt freq sets jf jdj mb mb mb mb mb mb mb likewise transactions data size triples table synthetic data sets items added transaction discarded introduce noise data set 
probability item dropped determined corruption level corruption level fixed set chosen normal distribution mean variance 
maximal frequent sets selected items added transaction manner required transaction size reached 
number jf sets provided parameter generation algorithm 
size set determined poisson distribution mean jf take account observation frequent sets overlap sets created incrementally retaining certain number items previous set adding rest randomly 
portion items migrate set controlled exp distributed random variable 
distribution determines similar frequent sets parameter called correlation level 
effects different corruption levels correlation levels reported insignificant conduct experiments investigate issues values proposed 
table lists different parameters 
naming convention data sets customary easier 
notation describes dataset items average transaction size average size maximal frequent itemsets transactions 
table summarizes parameters data sets 
sear pass bundling comparing sear algorithms examine behavior closely section 
importantly benefits pass bundling 
sear pass bundling pass bundling factor call algorithm sear plain 
experiment varying minimum support chosen illustrates best pass bundling effects mainly passes 
confirm results size experiment increasing number transactions included second part section 
experiments varying average transaction size number items provided section compare tid lists item lists 
varying minimum support described section bundling factor sear allow additional limited number candidates created database scanned count support 
value database items example effect sear adds level candidates prefix tree theta candidates created pass 
correct choice bundling factor important small values little negative effects large values cause candidates created leading high cpu overhead counting step 
shows choice bundling factor effects running time minimum support levels 
range minimum support experiments extends range values literature include harder mining problems frequent sets upper limit candidates frequent allow rules created exploring point meaningless 
means pass bundling turned chosen upper limit value minimum passes reached minimum support values 
higher bundling factors lead performance degradation useless candidates created effort count support exceeds cost additional passes 
reason bundling passes useful number candidates large 
passes constitutes inherent barrier savings achieved pass bundling implemented sear low bundling factors lead little improvement worse performance sear plain corresponds bundling factor higher values reduce running time half lower minimum support levels 
highest bundling factor showed greatest improvement possible remaining candidates generated pass counted third scan database 
table lists number passes required number candidates generated pass bundling factor 
note number passes reduced significantly expense comparatively additional candidates lower minimum support level 
saves io computation time 
shows nicely pass bundling effects mainly passes required small minimum support values 
fact minor differences various bundling factors passes required 
note reflected high pass bundling factors lead worse performance higher minimum support values candidates created 
experiment case min 
number frequent itemsets small candidates created pass bundling 
third scan database necessary 
combining passes possible implemented sear special optimizations required count large number candidates case 
explained section partition follows approach avoid expensive second pass 
time sec minimum support sear running times various bundling factors high pass bundling factor large number candidates added tree pass knowledge candidates frequent available resulting unnecessary waste counting time 
table shows exactly number candidates effected case 
small number candidates high minimum support levels scenario highly probable 
stated pass bundling factor experiments performs best low minimum support values 
bundling factor min min passes candidates passes candidates table number passes number candidates different pass bundling factors increasing number transactions database minimum support values ran sear sear plain different database sizes ranging transactions 
running times depicted 
expect linear growth running times done transaction remains constant 
results confirm expectation previous observation sear sear plain differ min lower minimum support level min requires passes sear plain 
sear runs twice fast plain counterpart 
comparing tid lists item lists section investigates tid lists compare item lists conjunction prefix trees pass bundling 
algorithms sptid sptid partition sear sear plain time sec number transactions sear sear sear plain sear plain sear transaction scale included help distinguish effects caused item lists form caused pass bundling optimization 
report experiments varying minimum support section 
results explained subsequent section 
possible understand results experiments vary number items average transaction size follow sections 
section results experiments conducted various data sets confirm results described briefly results section summarized 
number partitions set sptid section interested effects partitioning comparing data structures 
just partition avoids overhead larger global candidate set obviates need counting phase 
stated database created items maximal frequent sets 
data size mbytes data intermediate results fit memory easily distortions results operating system page swapping precluded 
algorithms generate approximately number candidates sear creates slightly due pass bundling 
sear plain sptid exactly number candidates 
varying minimum support experiment examines performance different minimum support levels results depicted 
running time sear sear plain determined number passes accounts difference running time algorithms minimum support 
decreasing minimum support size largest frequent set grows number passes sear plain perform 
sear number passes increases slower combines passes shown section 
low minimum support levels sptid slower sear versions sear times faster sptid 
sear plain retains edge sptid 
sptid comparable sear large minimum supports values itemsets frequent results trivial rules generated 
reason number frequent itemsets extremely small number candidates runs 
minimum supports items respectively frequent 
reduces number tid lists sptid join sear profit small number candidates time sec minimum support sear sear plain sptid total running times depending minimum support minimum support sptid comparisons candidates minimum support candidates sptid sptid number comparisons depending minimum support sptid candidates depending minimum support look transaction database 
timing experiments shown sptid spends running time joining tid lists 
number elementary integer comparisons tid comparisons shown performed sptid join tid lists course experiment 
number candidates created minimum support level shown comparison 
strong similarity graphs suggests number candidates decisive factor running time sptid 
reason sptid join lists tids candidate 
explain support hypothesis 
explanation performance results shed light causes differences sptid sear algorithms observed section figures show measurements pass basis logarithmic scale 
sear versions pass means database scan denotes combined candidate generation counting phase sptid 
chose low minimum support level min differences algorithms apparent difficult mining problems 
note graphs logarithmic scale 
database parameters chosen previous section 
total sear sear plain sptid hash operations comparisons time sec hash operations comparisons time sec hash operations comparisons time sec table total statistics minimum support experiment consider shows number candidates created pass 
expected candidates generated second pass 
note pass bundling effect sear creates candidates sear plain third pass require passes 
shows time algorithm spends pass 
timing graphs pass sptid time takes load database transform tid list representation turns vary significantly time taken sear pass 
important observation graphs sptid spends time second pass algorithm extremely fast subsequent passes 
contrast amount time needed sear sear plain pass vary greatly 
decreasing number candidates passes lead expect decreasing times sear algorithms case smaller number candidates offset larger size causes hash operations reach candidate leaf prefix tree 
sear slower pass due pass bundling perform subsequent passes 
shows number integer comparisons sptid depicts number hash operations performed sear pass 
sear uses powers size hash tables computing hash function reduced operation roughly comparable integer comparisons performed sptid 
pass sptid counting itemsets combined loading database 
graphs confirm observations previous paragraphs amount done sptid highly concentrated second pass sear spreads evenly passes 
table contains total statistics different minimum support levels 
note difference number hash operations sear number comparisons sptid 
sptid find correlation number candidates counted number integer operations time spent pass 
note correlation candidates amount observed sear 
effect explained follows 
consider database items assume frequent means combinations reasonable assumption items frequent quite small minimum support values example 
pass number sear plain sptid sear candidates pass min time sec pass number sear sear plain sptid running time pass min pass number sptid comparisons hash operations pass number sear hash operations sear plain hash operations hash operations pass number sear hash operations sear plain hash operations pass min integer comparisons sptid hash operations sear items gamma candidates evaluated sptid pass 
assume transactions average items 
average length tid list itemset theta tids 
merge join count candidate requires comparisons items longer list theta comparisons necessary pass 
usually larger lists longer average cause comparisons assumed 
estimate number hash operations performed sear regardless pass bundling pass 
sear tries locate subset transaction prefix tree 
assume items frequent transaction size follows poisson distribution average 
random variable transaction size 
average number subsets transaction dx gamma 
dx hash operations subset total theta theta hash operations necessary 
actual distribution required calculation sptid error introduced mean comparatively small 
number items sear sear plain sptid candidates number items candidates sptid total running times depending number items sptid candidates depending number items poor performance tid lists confined second pass tid list algorithms optimized counting support candidates directly tid lists generating candidates size larger 
fact undocumented optimization partition 
undocumented optimization know partition compares sear 
detailed analysis left research rough estimates possible state expectations chapter effects partitioning discussed 
summarize note sptid number comparisons strongly dependent number candidates 
explains increase running time sptid decreasing minimum support 
done sear hand dependent number candidates sensitive minimum support level 
varying number items able explain effects occur total number items jij increased 
results second experiment shown 
minimum confidence database size stayed constant number items transaction remained unchanged 
previous experiment show number candidates generated runs 
correlation observed number candidates execution time sptid 
effect increasing number items fewer frequent sets larger gets lead expect algorithms run faster items 
effect outweighed increased number candidates created second pass 
explained section sptid highly sensitive number candidates causes bad performance larger numbers items 
fact sptid better sear plain items outperformed sear 
larger sizes sptid times slower sear plain sear version considerably faster values jij 
furthermore seas seas plain significantly effected number items slight rise running time greater numbers items attributed increased effort candidate generation 
increasing transaction size third experiment compares sptid sear algorithms size transaction varied data size kept constant 
shows results experiment different time sec average transaction size sear sear plain sptid total running times depending average transaction size minimum support transactions minimum support levels 
product transaction size number transactions kept constant 
transactions average size transactions size 
adjustment ensures itemset approximately frequency database regardless number transactions 
definition minimum support relative frequency maintaining fixed minimum support experiment produce distorted results 
reason smaller number transactions relative minimum support corresponds fewer occurrences set databases 
number frequent sets increase effecting results 
consequently absolute minimum support value expressed number transactions support set ensure number candidates number frequent sets remains constant experiment 
low level transactions equivalent chosen parameters staying 
higher value transactions equivalent leads similar behavior 
sear roughly linear rise running time observed expected calculation 
jdj theta jt done transaction approximately jt described jdj theta jt theta jt linear transaction size 
sear plain slope steeper additional cost items transaction paid additional passes necessary pass bundling 
done sptid depend average transaction size number candidates average length tid lists 
values change experiment observe constant running time regardless transaction size 
running times sear stay times sptid minimum support levels transactions sear plain performs worse sptid large transaction sizes 
clearly sptid eventually perform better sear average transaction size grows databases occur practice 
data sets complete comparison sptid sear conducted series experiments data sets 
minimum support level varied form previous minimum support experiment 
general sptid performs transaction sizes small large minimum support high clearly outperformed settings sear sear plain 
small size maximal frequent sets puts sptid disadvantage having perform expensive second pass opportunity gain ground passes sear slower 
large maximal frequent sets sear algorithms disadvantage prefix tree deep levels traversed count candidates leaves 
sear plain expensive operations repeated time tree expanded level pass bundling avoids repeated waste effort extent sear 
spite disadvantage sear twice fast sptid minimum support databases listed 
summary main experiments conducted section varying minimum support number items average transaction size showed performance algorithms tid lists mainly determined number candidates leads inefficiencies second pass candidates created 
algorithms sensitive number items minimum support level 
contrast item list algorithms greatly effected number candidates second pass number passes determined maximal size frequent sets number items transaction 
poor performance tid lists second pass avoided counting candidates directly 
fact optimization necessary tid lists efficiently 
optimized tid algorithm partition compares item list algorithms determined research optimizations implemented 
partitioning investigating item lists tid lists section section focuses question partitioning algorithms compare non partitioning io intensive algorithms 
contrasting sear spear partitioned counterpart spinc uses incremental partitioning sptid see partitioning fulfill hopes improved performance causes constant cpu overhead additional partition 
secondary objective compare performance partitioning algorithms 
benefits spinc spear smaller expected 
sptid remains handicapped inefficiency tid lists second pass section included completeness 
find partitioning leads improvement non partitioning sear algorithm set experiments conducted data base different numbers partitions 
varying number partitions corresponds different buffer sizes available partitioning algorithms store partition 
results compared running time sear 
ran algorithms larger database transactions avoid extreme cases partitions local minimum support partition drops just transactions 
hand size processed memory conduct tests just partition 
databases minimum support varied previous experiments items maximal frequent sets parameters 
partitioning algorithms comparable memory requirements buffering partitions include various buffer sizes comparison focus differences appear number partitions increased 
means particular sptid swap intermediate results disk relevance io reducing io cost important objective partitioning algorithms remarks issue order performance results 
experiments show sear disk io accounts running time high minimum support levels little computation little low minimum support 
percentages partitioned algorithms smaller minimum compute intensive runs 
recall io operations sequential scans database faster random accesses 
algorithms cpu bound io comparatively little influence performance 
reason saving io expense additional computation lead improvements see shortly comparing partitioning algorithms 
amount computation required transaction constant io cpu cost increase rate database size grows fraction time due io change 
sptid bypassing second pass clearly reduces amount computation suspect io important 
infer timing results pass sptid shown 
assuming optimization reduces cost second pass seconds lower estimate total running time seconds 
database read io rate mb sec measured initial experiments means mb read second words running time 
partition database read twice additional counting phase partitioning overhead increase amount computation 
expect sptid cpu bound environment bypass optimization 
performance comparisons number partitions set assess results reducing number io operations buffering entire database increase number partitions examine partitioning overhead 
shows typical behavior partition entire database fits memory 
partitioning algorithms read database memory passes sear deliberately put disadvantage having read entire database disk pass 
object experiment quantify effect disadvantage 
furthermore database easily fit memory sear expect larger databases 
observation common data sets tested sptid falls far low minimum support values seen previously prefix tree algorithms perform equally 
line expectations io minor relevance execution time 
sear requires passes due pass bundling terms computation basic differences sear spear spinc partition case 
results partitions database shown 
database chosen differences algorithms greater due high cpu cost large maximal frequent sets experiments databases produced similar dramatic results 
partitioning algorithms perform worse non partitioning sear indicating additional overhead associated partitioning 
particular overhead offset possible savings resulting fewer io operations 
difficult sure accesses involve disk irregular access patterns want complicate issue 
time sec minimum support sear spear spinc sptid running times algorithms partition minimum support sear spear spinc sptid time sec minimum support sear spear spinc sptid running times sear partitioning algorithms partitions partitions number partitions grows gap sear partitioning algorithms widens result additional overhead partitioning 
additional time part due additional counting phase necessary partition part due candidate generation repeated partition 
table shows time spent candidate generation sear spear number partitions grows 
times spinc comparable ones spear 
partitions incremental version displays noticeable improvements non incremental counterpart spear 
reason spear processes entire database twice generate possible frequent sets second time count global support sets 
contrast spinc avoids counting second partition second time processing database times 
puts spinc middle spear sear processes database involves scans data 
performance spinc partitions disappointing inbetween sear spear higher minimum support levels improvement spear significant partitions partition size gets smaller partitions 
savings achieved having process partition important 
noted described spinc min min sear spear spear spear spear table time candidate generation sec lowering minimum support phase improve performance algorithm pursue idea 
furthermore spinc outperformed demanding problems 
effect dramatically partitions caused increased overhead required manage old frequent sets candidate sets tree 
slowing effect increased number partitions partitioning algorithms shown 
graphs show algorithms adversely effected 
minimum support sptid sptid sptid sptid sptid time sec minimum support spear spear spear spear time sec minimum support spear spear spear spear spear time sec minimum support spinc spinc spinc spinc spinc times different numbers partitions summary sptid comparable prefix tree algorithms high minimum support parameters regardless number partitions 
furthermore partitioning show expected improvements due reduced io overhead due partitions outweighs io savings 
incremental spinc achieves savings simple partitioned version high minimum support values gains noticeable partitions processed 
summary comparing tid lists item lists results indicate sear superior sptid exception marginal cases rarely occur practice 
sear plain pass bundling optimization better cases 
explained influence parameters performance algorithms importantly sptid sensitivity large numbers candidates causes twice slow sear algorithm average 
weaknesses tid list representation second pass identified major cause poor performance sptid 
sear hand depends average transaction size sptid 
partitioning algorithms performed worse sear particular spear spinc comparable sear prefix tree structure show expected improvements due reduced io 
keeping observation io accounts running time algorithms cpu bound 
overhead due additional partitions slowed partitioned algorithms 
partitioning algorithms justified environments extremely high io cost high priority keep io loads low find useful admittedly high environment 
showed pass bundling reduces io cpu overhead significantly item list algorithms 
pass bundling alternative partitioning 
question remains item lists prefix trees pass bundling compare tid lists expensive second pass avoided counting support candidates directly 
tid lists superior item list algorithms passes sear outperformed tid list algorithm 
hand pass bundling greatly reduces number passes tid lists require partitioning large databases incur overhead associated 
furthermore short cutting second pass causes additional overhead 
item lists may outperform tid lists definitive answers question require research 
performance incremental spinc disappointing savings achieved spear significant greater numbers partitions 
difficult problems spear outperform spinc slowed overhead managing different sets prefix tree 
chapter parallel algorithms programming environment data distribution assume shared parallel database architecture processor equipped main memory private disks 
remote data accessed processing node owns 
sp distributed memory message passing architecture suits paradigm clear distinction local remote data 
communication included explicitly programmer occur automatically result memory access page happens located processor case shared memory architectures 
message passing allows control nature extent communication beneficial case results chapter demonstrate 
spmd programming model provided sp requires program run processor nodes contrast simd different instructions executed concurrently 
fact programs run independently communication desired may act barrier processors participate 
shows node sp switch 
sp high performance switch transaction database assumed partitioned horizontally set processing nodes scalable system trademark ibm 
algorithm pear ig fall foreach processor parallel local support forall transactions forall subsets count global support foreach count count parallel fc jc count min generate candidates return partitions approximately number transactions avoid load imbalances 
data assigned processors randomly possible load imbalances example hashing tid 
apart restrictions data processed parallel stored disk sequential case 
special data structures 
assume processor knows number distinct items database number transactions assigned average transaction size needed determine partition size partitioning algorithms 
pear parallel sear algorithm algorithm description recall sear alternates candidate generation support counting steps pair call pass 
counting support candidates easily done parallel processor evaluating private data 
contrast decision candidates accept frequent discard done correctly knowledge global support candidates computed adding local support counts processor 
consider code pear parallel version sear algorithm shows sear augmented communication run parallel 
statements parallel parallel block sequential parts algorithm means processors perform exactly operations identical data 
data node referred confused partitions sequential case 
algorithm starts set candidates easily created locally communication number items database 
processor counts support candidates individually portion database 
superscript count variable count refers processor number indicates variable contains different values processor 
counting step global combine operation adds local counts determine global support candidates 
result distributed processors global count candidate available processor 
node continue compute set frequent itemsets removing infrequent candidate sets completes pass 
note processor eliminates exactly candidates comes set nodes create candidate set pass 
procedure continues empty set indicates frequent sets 
processor pass pass pass 
combine combine combine pass pass pass processor 
sear pear algorithm scheme essentially program running individual processors sear exception pass candidate generation local support counting counts processor added able proceed correctly pass 
scheme depicted 
combine communication necessary compute frequent sets 
effect global barrier processor continue pass counts generate set candidates 
amount node perform pass roughly balanced sure excessive idle times avoided 
actual association rules computed parallel determined sequentially frequent sets nodes 
implementation message passing library mpl combine operation compute global counts accepts vectors addition case corresponding elements added sum returned processor shown 
efficient facility keep local counts array store index array candidate node prefix tree 
combine performed integer array 
candidate frequent combine index tree replaced actual support count memory array freed 
important release memory large number candidates lead huge buffer requirements 
quite reasonable number candidates bytes integer array size mb 
furthermore combine place requires separate input output buffers 
huge combine split smaller combine operations allow smaller output buffer 
processor processor processor processor processor processor example add combine operation array integers ppar partitioned parallel algorithm potential problem pear needs large combine operations count variables 
combines function global barriers reduce efficiency processors smaller load wait idle 
partitioning approach allows reduction amount communication number synchronizations describe section ppar parallel version spear 
version partitioned algorithm briefly outlined partition algorithm 
algorithm description spear processes partition independently order relevant results partition needed 
straightforward approach choose consider data processor partition 
local frequent sets determined phase local means local processor case 
set processor communication required phase 
form global candidate set union computed processors second phase 
result broadcast processors 
third phase corresponds phase ii sequential algorithm processor determine local support sets spear sets counted global counts available phase 
phase communication needed phase 
phase iv global support counts sets coalesced combine operation just candidate counts combined pear 
discarding globally infrequent sets final result available 
algorithm outlined explanation far assumed data local processor fits memory processed partition 
usually case data processor partitioned call local partitioning opposed global partitioning amon processors 
local partitions processed sequence phase parallel case fact identical phase spear difference runs processor independently 
see algorithm note clarity code include removal candidates phase iii don counted phase generate local frequent sets phase generate local frequent sets phase iii count local support candidates phase iii count local support candidates discard infrequent sets discard infrequent sets processor processor generate global candidate set phase ii phase iv combine local support counts 
spear ppar algorithm scheme pseudo code general case 
ppar phases involve communication barriers may cause performance degradation 
furthermore size messages sent phases smaller combines pear 
reason pear needs exchange counts candidates ppar exchanges counts locally frequent sets number fraction number candidates 
phase phase iii implemented manner similar corresponding phases spear don details implementation 
combine phase iv straightforward identical combines pear union locally frequent sets phase iii interesting section explain solution 
computing union locally frequent sets combine operations long processors know array position belongs set 
adding corresponding array elements adds correct local support values 
case pear processors create number candidates sequence 
case phase iv ppar processor knows determine rank set case ppar phase ii 
processor different set locally large sets determined directly share data frequencies itemsets 
fact purpose phase establish common understanding processors sets counted eventually 

algorithm ppar generate locally frequent foreach processor parallel compute partition forall partitions find frequent sets parallel ii compute global candidate broadcast processor iii local support global candidates foreach processor parallel foreach count count support parallel iv global support global foreach count count return fc jc count min recall sets sets stored prefix trees represented trees processor 
solution linearize tree scheme uses exactly integers frequent set pear needs integer candidate 
form union sets sets processor linearizes tree sends node compares input set updates accordingly 
nodes involved recipient creates linearization updated tree fly sends third node 
node receives union sets predecessors node complete union 
procedure linear number nodes involved inefficient node start predecessor finished 
adjust message size processing overlapped pipeline fashion 
illustrates technique nodes 
communicating result nodes done easily series broadcasts shown 
processor processor processor processor broadcasting result pipeline build union sets union sets sets processors chapter parallel experiments chapter report results experiments parallel algorithms pear ppar 
show algorithms display near optimal speed scale size behavior 
results due largely fact communication overhead small algorithms constant respect number transactions varies number processors involved 
surprising partitioned ppar algorithm performs worse pear needs communication 
results detail show results initial experiments combine operation important communication operation pear 
experiments conducted node sp multiprocessor equipped mhz rs processor mb main memory gb scsi disks 
data came disks 
cost combine operation shows results tests conducted asses efficiency mpl combine operation integer vector addition 
recall operation performed arrays arbitrary size long sufficient buffer space provided 
number processor processors varied number integers array ranged 
shows timing results depending array size shows numbers depending number processors 
times obtained averages runs variance individual times significant 
array size int node nodes nodes nodes nodes nodes nodes nodes nodes time sec number processors times parallel combine operation depending array size depending number processing nodes time sec number processors pear pear ppar ppar running times various numbers processors observations important context 
operations extremely fast considering values processor added result needs distributed 
secondly shows nicely combine linear number array elements rate elements processed second processing nodes twice rate processors 
note time processor basically time needed copy data input buffer output buffer 
shown time required combine vary significantly number processors long number 
importance results algorithms expect low communication times vary number processors involved 
speed experiments ran algorithms data set increasing number processors 
minimum support set size data set mb 
experiments chapter buffer size ppar set constant amount mb node number processors varies 
shows resulting running times indicating pear superior ppar demanding low minimum support values 
ppar slowed local partitioning overhead offsets benefits reduced communication due global partitioning 
course times lower minimum support level algorithms 
change computation distributed increasing number nodes indicating algorithms scale larger numbers processors 
speed ups obtained algorithms experiment shown 
algorithms show linear speed 
exact values table 
min nodes nodes nodes pear pear ppar ppar table exact speed numbers speed number processors pear pear ppar ppar speed pear ppar deviation optimal linear speed curve due sequential portions algorithm communication overhead 
sequential parts algorithm performed processor exactly data candidate generation step pear 
parts algorithm done parallel relative portion running time grows number processors increases 
time wasted due communication 
actual communication slows algorithm time spent waiting communication acts barrier 
observation ppar obtains better speed ups pear 
reason pear spends time communication average seconds opposed ppar seconds minimum support seconds 
different times communication part caused reduced amount communication listed table recall ppar needs communicate support frequent sets pear candidates 
difference communication time mainly caused fact ppar communication operations opposed pear spends time waiting barriers 
min pear ppar table communication sizes pear ppar bytes reason better speed ups achieved ppar lies fact repeats candidate generation local partition 
candidate generation dependent number local partitions size local data causes time task decrease number processors grows contributing higher speed ups 
contrast time spent candidate generation pear remains constant experiments 
ironically repeated effort candidate generation due local partitioning reason ppar outperformed pear low minimum support 
second observation higher minimum support values achieve worse speed ups 
speed ups table lower amount computation smaller communication overhead stays approximately regardless minimum support level 
reason comparatively low speed pear nodes 
note pear communication overhead sequential parts constant respect time sec number processors pear pear ppar ppar scale pear ppar transactions node number transactions 
running larger problems raise speed algorithm 
summarize note algorithms obtain speed ups improve size data base problem difficulty increases 
ppar obtains better speed ups performs worse pear 
scale experiments experiment examines behavior algorithms data size increased number processing nodes 
minimum support levels database transactions mb node 
shows running times processors 
ppar mb buffer space causes database processed local partitions leads slightly worse performance pear 
algorithms display near optimal scale behavior number processors increases 
running times slightly greater numbers nodes attribute characteristics particular data set 
size experiments experiment shown investigates effects increasing number transactions keeping constant number processors 
experiment conducted processors minimum support levels 
mentioned earlier section growing problem sizes reduce portion running time due communication leads expectation algorithms size running times linear problem size 
confirms expectation range transactions 
parameters chosen 
addition speed scale term third criteria evaluate parallel database performance case number processors remains constant problem size increased 
time sec number transactions pear pear ppar ppar size pear ppar transactions nodes chapter extensions related sections outline current concept hierarchies parallel classification directly related subject report belongs context 
especially section episode discovery interesting demonstrates arm algorithms modifications applied completely different problem 
discovering episodes sequences episodes sequences surprisingly discovering events sequences frequently occur time interval easily mapped arm problem 
set event types transaction contain events occurred time window fixed length 
transaction corresponding window starting fa cg 
window slide sequence create set transactions 
simple preprocessing step arm algorithms solve problem modification 
events windows events windows episode discovery unfortunately simple solution may take account important characteristic sequences partial ordering events events may occur parallel 
occurrences event window considered example event example window 
reasons define episode partially ordered multi set events capture time relation events 
events ordered respect considered parallel 
episodes example window fag fbg fcg fc fc fc fc ag fc bg fc cg fa fa bg fa cg fb cg numerous sequential parallel compositions fa 
kc fa cg 
model recursive nature episodes episodes consist sub episodes partially ordered respect 
episode frequent predefined threshold 
similar properties hold frequent episodes frequent itemsets example episode frequent sub episodes frequent 
similar association rules prediction rules constructed predict occurrence episode took place 
confidence rules defined manner similar association rules 
algorithm arm algorithm algorithm alternates candidate generation counting steps 
candidate episodes initially built elementary events support counted frequent episodes create new candidate episodes 
candidate episodes created way candidate sets built apriori gen episodes size share gamma sub episodes atomic compound joined form candidate 
frequency sub episodes tested mandatory prerequisite 
counting support episodes tricky episodes window detection sequential episodes requires finite state automata recursive structure episodes 
furthermore task complicated optimizations fact adjacent windows differ greatly episodes previous window part 
results show sufficiently small number candidates generated algorithm effective savings factor obtained incremental episode counting 
concept hierarchies concept hierarchies taxonomies mentioned way domain knowledge speed mining process improve quality results 
usually taxonomy provided domain experts may determined straightforward manner case geographical data 
feasible methods available create concept hierarchies data automatically modify existing hierarchies suit current mining task example part data examined 
necessary current structure general specific unbalanced causes distorted results 
concept hierarchies classification mining prominent example attribute oriented induction realized system 
aggregate relations built successively replacing values ancestors hierarchy 
context association mining concept hierarchies viewed directed acyclic graphs nodes labeled literals compared definition section augmented literals higher level concepts example addition basic items milk beverage part universe items 
edge nodes graph represents relationship 
notion ancestors descendants items defined terms edges transitive closure taxonomy graph 
sets rules items replaced ancestors called original set rule respectively 
presence multiple levels concept hierarchy association rules may contain literals levels hierarchy may restricted level 
generalized association rules defined level rules require additionally literal consequent ancestor literal antecedent avoid trivial rules 
insight gained multiple taxonomies may price store brand producer 
reasons taxonomies include ffl rules leaf level may minimum support 
may different brands confusing variety cereal milk association milk minimum support 
ffl pruning redundant rules possible 
continuing example expect find rules link individual cereal brands milk mining restricted leaf level 
brands need especially popular case rules express fact cereal bought milk 
clearly easily subsumed rule cereal milk 
ffl fast aggregate analysis possible 
discovery process conducted different levels independently users start discovery higher level focus confined areas interest lower levels 
looking certain group items database reduces amount significantly speeds mining process especially interactive mining sessions 
ffl items possible removing items entire branches taxonomy consideration higher level concept frequent reduce number literals lower levels 
larger numbers items tractable 
content chapter different algorithms include concept hierarchies general remarks provide short sketch algorithms section new definitions interestingness concept hierarchies new criteria interestingness rule applied 
general idea rule interesting reveals information expected ancestors rule 
ancestor rule general preferred 
recalling cereal example rules involving individual cereal brands considered redundant confidence support deviate significantly ancestor rule cereal milk 
details :10.1.1.144.4956
introduce notion strong rules focus search hierarchy levels 
rule strong ancestor item antecedent consequent frequent level taxonomy 
strengthened require antecedent consequent sets equivalent replacing item antecedent frequent previous level 
possible different minimum support confidence levels permitted various levels 
algorithms basic approach add ancestor literals transaction run algorithms mining single level rules :10.1.1.144.4956
dramatically increase size transactions number possible refer treatment taxonomies generalized association rules term multiple level association rules proposed :10.1.1.144.4956
synonymously hierarchical association rules 
frequent sets 
methods including sampling proposed prune candidate set avoid unnecessary passes database due large number candidates 
algorithms encode hierarchy representation leaf literal encoded transaction table initial translation step 
original version basic algorithm scheme section applied repeatedly mine rules level hierarchy starting general level 
modifications algorithms provided allow rules items different levels multiple concept hierarchies 
major drawback approach large number scans required due separation hierarchy levels 
advantage different minimum support confidence values possible level 
parallel discovery classification rules contents section directly applicable problem parallel arm belong general context parallel kdd 
classification rule mining section attempts brief done classification machine learning kdd 
reader referred information 
basic familiarity decision trees constructed classification algorithms general ai search techniques assumed 
supervised learning case presupposed approaches solve problem 
data object labeled class identifier usually additional attribute relation 
age car price safe driver safe driver dangerous driver dangerous driver safe driver truck type car example decision trees classification dominant operation building decision trees gathering histograms attribute values 
conjunction attribute restrictions path leaf selects portion database paths partition relation horizontally disjoint subsets 
histograms built subset attribute class individually 
information information theoretic measures nodes added leaves tree effect splitting partition associated parent node 
histograms built anew new sub partitions 
process continues classification performance tree depth reach predefined thresholds 
open question methods optimal deciding leaves expand expanding tree altogether avoid poor performance due fitting hand incomplete trees 
approaches parallel system features sequential external mining tool uses parallel databases server approach called meta learning allows actual mining done parallel 
parallel mining engine classification tool developed cwi merely directs discovery process expensive histogram building procedures incorporated extendable parallel dbms monet designed exploit large amounts main memory shared memory multi processors 
experiments run node sgi machine mbytes main memory 
database completely partitioned vertically binary association tables accordance assumed column oriented nature access patterns 
selection create partitions construction histograms done parallel 
successive subdivision horizontal partitions suggests assigning individual processors different partitions 
results show considerable linear speed factor nodes database authors brief issue 
particular issues scalability skew load balancing impact communication overhead addressed 
assume shared architecture arm algorithms row vertical partitioning database yield satisfactory results association mining 
meta learning multi strategy learning previous approach meta learning parallelize mining algorithm involves horizontally dividing database subsets applying sequential learning algorithms subset 
result set different decision trees depending data trained 
avoid losing classification reliability due smaller size subsets arbiter combines votes classifier single result 
actual meta learning part begins arbiter trained correctly decide conflicting votes individual classifiers 
concept extended just arbiter binary tree arbiters shown resolve conflicting results 
adjusting arbiter tree set classifiers involves learning data passes various parts database 
building individual classifiers parallel done efficiently considerable amount time spent build suitable arbitrator 
theoretical speedups lgp obtained processors results reported reach boundary 
apart moderate speedups see shortcomings approach classifier unseen data slow classifiers applied data item results processed arbiter tree 
done parallel certainly efficient 
secondly discovered knowledge hidden inside classifier arbitrator set way hidden neural nets 
database retrieval class members inefficient indexes precluded 
strength approach lies possibility incorporate different classification methods leaf classifiers improve classification performance means diversity 
version meta learning referred multi strategy learning 
sequence analysis refers discovery dna sequences classification algorithms 
classifiers training database arbiters example arbiter tree meta learning knowledge attempts apply meta learning principle arm partitioning principle partition see section intermediate final results coalesced constitutes fundamental difference principles 
chapter studied association rule mining problem contributions related sequential algorithms ffl shown partitioning reduces io cost lead performance improvements introduces constant cpu overhead partition 
algorithms slowed considerably io cost extremely high 
algorithms cpu bound environment large databases trying save io operations expense additional computation bound success 
ffl report provides performance analysis item list pass bundling technique shows reduces cpu io cost significantly overhead caused partitioning 
ffl shown tid lists perform phases algorithms highly inefficient early phases resulting poor performance avoided special optimizations tid lists early phases 
ffl describe new incremental partitioning algorithm produced slight improvements standard partitioning technique incurs overhead due partitioning 
incremental partitioning standard counterpart performed worse algorithms pass bundling 
studies partitioning currently considered best solution arm problem reduce execution time arm algorithms fact lead worse performance number partitions grows 
partitioning may beneficial conjunction tid lists greatly reduce cpu cost provided optimization bypassing early phases benefits lessened partitioning overhead 
contrast item lists pass bundling achieve low io cpu cost partitioning overhead 
bypassing optimization mentioned partition techniques compared directly find definitive answer question performs better 
plan conduct research issue requires implementing bypass optimization sptid sear 
second part report described parallel versions sear spear algorithms differ amount communication require 
ppar spear parallel counterpart uses communication necessary parallel sear due global partitioning scheme performs worse due local partitioning 
show algorithms display near optimal scale size behavior speedups linear speed factor obtained node sp multi processor problems tested 
speed ups improve database size increased mining problem difficult 
unfortunately limited experiments synthetic data sets previous studies task evaluate performance algorithms real life data set 
integrating algorithms existing dbms lead additional problems related generating proper input format mining algorithms query optimization queries involving data mining operations mention theses difficulties 
issues investigated closely re results subsequent runs algorithms database different minimum support values situation occur interactive mining 
report version master thesis want dr mike franklin guidance encouragement support study possible 
bibliography rakesh agrawal ghosh tomasz imielinski arun swami 
interval classifier database mining applications 
th int conf 
large databases vldb vancouver canada pages 
rakesh agrawal tomasz imielinski arun swami 
database mining performance perspective 
ieee transactions knowledge data engineering vol 
december 
rakesh agrawal tomasz imielinski arun swami 
mining association rules sets items large databases 
sigmod washington pages may 
rakesh agrawal ramakrishnan srikant 
fast algorithms mining association rules large databases 
th int conf 
large databases vldb santiago chile sept 
expanded version available ibm research report rj june 
marko ivan bratko 
trading accuracy simplicity decision trees 
machine learning 
boral alexander clay copeland franklin hart smith valduriez 
prototyping bubba highly parallel database system 
ieee transactions knowledge data engineering march 
cai nick cercone han 
attribute oriented induction relational databases 
gregory piatetsky shapiro william frawley editors knowledge discovery databases pages 
aaai mit 
philip chan salvatore stolfo 
multi strategy parallel distributed learning sequence analysis 
proc 
intl 
conf 
intel 
sys 
molecular biology pages 
philip chan salvatore stolfo 
parallel distributed learning meta learning 
working notes aaai 
knowledge discovery databases pages 
philip chan salvatore stolfo 
scalable parallel learning case study splice junction prediction 
technical report cucs columbia university 
david chiu andrew wong benny cheung 
information discovery hierarchical maximum discretization synthesis 
gregory piatetsky shapiro william frawley editors knowledge discovery databases pages 
aaai mit 
david dewitt gray 
parallel database systems database processing passing fad 
sigmod december 
david dewitt ghandeharizadeh donovan schneider allan hui hsiao rick rasmusen 
gamma database machine project 
ieee transactions knowledge data engineering march 
elomaa 
defense notes learning level decision trees 
cohen hirsh editors machine learning proc 
th int conference pages 
morgan kaufmann july 
william frawley gregory piatetsky shapiro christopher matheus 
knowledge discovery databases overview 
gregory piatetsky shapiro william frawley editors knowledge discovery databases pages 
aaai mit 
jiawei han cai nick cercone 
knowledge discovery databases attribute oriented approach 
th int conf 
large databases vldb vancouver canada 
jiawei han cai nick cercone 
data driven discovery quantitative rules relational databases 
ieee transactions knowledge data engineering feb 
jiawei han fu 
dynamic generation refinement concept hierarchies knowledge discovery databases 
aaai workshop knowledge discovery databases kdd seattle wa july 
jiawei han fu 
discovery multiple level association rules large databases 
st int conf 
large databases vldb zurich switzerland sept 
appear 
marcel arno siebes 
architectural support data mining 
technical report cs cwi 
marcel arno siebes 
data mining search knowledge databases 
technical report cs cwi january 
arun swami 
set oriented mining association rules 
technical report rj ibm research report oct 
ibm kingston nj 
ibm aix parallel environment release 
ibm thomas watson research center box yorktown heights ny 
sp communication subsystem aug 
available ibm tc cornell edu bm pps doc css css ps 
kenneth kaufman michalski larry kerschberg 
mining knowledge databases goals general description system 
gregory piatetsky shapiro william frawley editors knowledge discovery databases pages 
aaai mit 
mika klemettinen heikki mannila ronkainen hannu toivonen verkamo 
finding interesting rules large sets discovered association rules 
rd internat 
conf 
information knowledge management maryland 
acm press nov 
wei zhong liu allan white 
importance attribute selection measures decision tree induction 
machine learning 
heikki mannila hannu toivonen verkamo 
improved methods finding association rules 
aaai knowledge discovery seattle washington pages july 
heikki mannila hannu toivonen verkamo 
discovering frequent episodes sequences 
proc 
knowledge discovery data mining kdd appear 
technical report university helsinki department computer science finland march 
christopher matheus philip chan gregory piatetsky shapiro 
systems knowledge discovery databases 
ieee transactions knowledge data engineering dec 
gur ali william wallace 
induction rules subject quality constraint probabilistic inductive learning 
ieee transactions knowledge data engineering dec 
jong soo park mink chen philip yu 
effective hash algorithm mining association rules 
sigmod san jose ca pages 
acm 
arun swami rakesh agrawal christos faloutsos 
efficient similarity search sequence databases 
th int conf 
foundations data organization algorithms chicago oct 
lecture notes computer science springer verlag 
ashok edward navathe 
efficient algorithm mining association rules large databases 
st int conf 
large databases vldb zurich switzerland sept 
gatech technical report 
git cc 
cullen schaffer 
overfitting avoidance bias 
machine learning 
padhraic smyth rodney goodman 
rule induction information theory 
gregory piatetsky shapiro william frawley editors knowledge discovery databases pages 
aaai mit 
ramakrishnan srikant rakesh agrawal :10.1.1.144.4956
mining generalized association rules 
st int conf 
large databases vldb zurich switzerland sept 
appear 
craig denis shea don grice peter hochschild michael tsao 
sp highperformance switch 
proc 
scalable high performance computing conference pages may 
allan white wei zhong liu 
bias information measures decision tree induction 
machine learning 
beat 
knowledge discovery databases 
draft course manuscript hong kong university science technology may 

