bit progress language modeling extended version joshua goodman machine learning applied statistics group microsoft research microsoft way redmond wa microsoft com august technical report msr tr microsoft research microsoft microsoft way redmond wa www research microsoft com overview language modeling art determining probability sequence words 
useful large variety areas including speech recognition optical character recognition handwriting recognition machine translation spelling correction church brown hull kernighan srihari 
commonly language models simple katz smoothed trigram model 
improvements simple model including caching clustering higherorder grams skipping models sentence mixture models describe 
unfortunately complicated techniques rarely examined combination 
entirely possible techniques separately show possible techniques better 
examine aforementioned techniques separately looking variations technique limits 
examine techniques various combinations compare katz smoothed trigram count cuto small training data set words get perplexity reduction bit entropy 
larger data sets improvement declines going largest data set words 
similar large set punctuation reduction 
data set achieve word error rate reduction 
largest reported perplexity reductions language model versus fair baseline 
organized follows 
section describe terminology briefly introduce various techniques examined describe evaluation methodology 
sections describe technique detail give experimental results variations technique determining best variation limits 
particular caching show trigram caches nearly twice potential unigram caches 
clustering find variations slightly better traditional clustering examine limits 
gram models examine grams show largest models performance grams 
skipping models give detailed comparison di erent skipping techniques know gram level 
sentence mixture models show mixtures sentence types lead improvements 
give experiments comparing techniques combining techniques various ways 
experiments done data sizes showing techniques improve data get worse 
concluding section discuss results 
appendices give proof helping justify kneser ney smoothing describe implementation tricks details handling large data sizes optimizing parameters clustering smoothing 
technique introductions goal language model determine probability word sequence 
probability typically broken component probabilities 
may di cult compute probability form large typically assume probability word depends previous words trigram assumption shown practice 
trigram probabilities estimated counts training corpus 
represent number occurrences training corpus similarly 
approximate unfortunately general approximation noisy word sequences occur 
consider instance sequence party tuesday 
tuesday party 
training corpus contain instances phrase party tuesday instances phrase party 
predict tuesday party clearly underestimate 
kind probability problematic applications language models 
instance speech recognizer words assigned probability recognized matter unambiguous acoustics 
smoothing techniques take probability away occurrences 
imagine training data single example phrase party stan chen birthday 
typically occurs time greatly overestimated 
particular stan party party stan party probability away words stan redistributing words tuesday zero probabilities avoided 
smoothed trigram model extra probability typically distributed smoothed bigram model commonly smoothing techniques katz smoothing katz jelinek mercer smoothing jelinek mercer called deleted interpolation fine better smoothing techniques exist 
particular previously shown chen goodman versions kneser ney smoothing feb 
stan provide cake 
invited 
ney outperform smoothing techniques 
appendix give proof partially explaining optimality 
kneser ney smoothing backo distribution modified normal bigram distribution special distribution 
kneser ney smoothing traditional techniques improvement 
obvious extension trigram models simply move higherorder grams grams grams 
show fact significant improvements gotten moving grams 
furthermore past shown significant interaction smoothing gram order chen goodman higher order grams better kneser ney smoothing methods especially katz smoothing 
look improvement gotten higher order grams examining grams 
simple extension gram models skipping models rosenfeld huang ney condition di erent context previous words 
instance computing compute 
model probably combined standard model yield improvements 
clustering called models attempt similarities words 
instance seen occurrences phrases party monday party wednesday imagine word tuesday similar monday wednesday follow phrase party 
majority previous research word clustering focused get best clusters 
concentrated research best way clusters report results showing novel techniques bit better previous methods 
show significant interactions clustering smoothing 
caching models kuhn kuhn de mori kuhn de mori observation word 
tend easy implement lead relatively large perplexity improvements relatively small word error rate improvements 
show trigram cache get twice improvement unigram cache 
sentence mixture models iyer ostendorf iyer observation di erent sentence types making models type sentence may better global model 
traditionally types sentences show improvements gotten going mixtures 
evaluation section describe justify perplexity entropy evaluation technique 
describe data experimental techniques experiments sections 
commonly method measuring language model performance perplexity 
language model assigned equal probability words perplexity 
general perplexity language model equal geometric average inverse probability words measured test data perplexity properties attractive measure language model performance true model data source lowest possible perplexity source 
lower perplexity model closer sense true model 
alternatives perplexity shown correlate better speech recognition performance typically free variables need optimized particular speech recognizer significantly di cult compute framework 
alternative equivalent measure perplexity entropy simply log perplexity 
entropy nice property average number bits word necessary encode test data optimal coder 
familiar information theory measure cross entropy test data model 
far common type entropy measured abuse term simply saying entropy really mean particular cross entropy 
entropy perplexity measurements entropy reductions nice properties including additive graphing perplexity reductions common literature 
table may helpful 
notice relationship entropy perplexity reductions roughly linear bits 
reduction entropy perplexity experiments performed nab north american business news corpus stern 
performed experiments di erent training data sizes words words words corpus wsj data approximately words 
cases performed parameter optimization separate set heldout data performed testing set test data 
data sets overlapped 
heldout test sets sentence non overlapping sets words taken section 
appendix describe implementation tricks tricks possible train complex models large amounts training data probably sounds better get perplexity reduction get bit entropy reduction 
hard test large test sets 
reason words total testing heldout data 
hand simply want say word contiguous test heldout set constitute articles risk problems homogeneity chose th sentence non overlapping word sets 
experiments done word vocabulary 
sentence paragraph article symbols included perplexity computations vocabulary words 
interesting try experiments corpora data sizes 
previous chen goodman compared corpora data sizes 
di erent corpora qualitatively similar important di erences training data sizes 
decided concentrate experiments di erent training data sizes di erent corpora 
toolkit unusual allows parameters jointly optimized 
particular combining techniques interpolation smoothing parameters need optimized 
powell algorithm press heldout data jointly optimize parameters 
smoothing section recap chen goodman 
read skip 
di erent smoothing techniques subject surprisingly subtle complicated 
interested smoothing consult previous chen goodman detailed descriptions detailed comparisons commonly smoothing algorithms done 
limit discussion main techniques simple interpolation katz smoothing backo kneser ney smoothing interpolated kneser ney smoothing 
section describe techniques recap previous results including important result interpolated kneser ney smoothing minor variations outperforms smoothing techniques 
simplest way combine techniques language modeling simply interpolate 
instance trigram model bigram model unigram model interpolate trigram bigram unigram constants 
practice interpolate uniform distribution uniform size vocabulary ensures word assigned probability 
need deal case instance trigram context seen 
case interpolated bigram model simplicity simple interpolation works surprisingly techniques katz smoothing better 
katz smoothing katz turing formula 
notice particular word sequence party stan occurs words probably significantly overestimated probably just showed chance true probability 
turns thing true lesser degree sequences occurred twice 
represent number grams occur times proved weak assumptions gram occurs times discount pretending occurs disc times disc disc typically written 
language modeling estimate disc leave certain amount probability left 
fact letting represent total size training set left probability equal represents amount probability allocated events seen 
really quite amazing generally useful fact predict expect happen happened looking proportion things occurred 
context katz smoothing uses formulae 
word sequence seen katz smoothing uses discounted count sequence divided counts context hand sequence seen back lower distribution basically formula katz disc katz normalization constant chosen probabilities sum 
katz smoothing commonly smoothing techniques turns techniques better 
chen goodman performed detailed comparison smoothing techniques modified interpolated form kneser ney smoothing ney consistently outperformed smoothing techniques 
basic insight chen goodman appendix give details implementation katz smoothing 
briefly smooth unigram distribution additive smoothing discount counts determine large possible giving reasonable discounts turing formula add pseudo counts context discounted counts 
tricks estimate nr kneser ney smoothing 
consider conventional bigram model phrase katz francisco 
phrase san francisco fairly common conventional unigram probability katz smoothing techniques deleted interpolation francisco fairly high 
means instance model katz smoothing probability katz francisco disc francisco francisco katz francisco katz francisco fairly high 
word francisco occurs exceedingly contexts probability occuring new low 
kneser ney smoothing uses modified backo distribution number contexts word occurs number occurrences word 
probability kn francisco fairly low word tuesday occurs contexts kn tuesday relatively high phrase tuesday occur training data 
smoothing uses simpler discounting scheme katz smoothing computing discounts turing single discount optimized held data 
particular backo kneser ney smoothing uses formula bigram vw number words occur context 
vw vw normalization constant probabilities sum 
formula easily extended higher order grams general 
instance trigrams unigram bigram distributions modified 
chen goodman showed methods katz smoothing backo kneser ney smoothing backo lower order distributions higher order count missing low counts counts counts 
estimates low counts fairly poor estimates ignore useful information lower order distribution 
interpolated models combine higher order lower order distribution typically better 
particular basic formula interpolated kneser ney smoothing vw vw normalization constant probabilities sum 
chen goodman proposed additional modification smoothing multiple discounts counts counts counts 
formulation modified kneser ney smoothing typically works slightly better interpolated kneser ney 
experiments combining techniques nearly number parameters system needed search pilot study techniques combined better interpolated kneser ney 
rest interpolated kneser ney modified kneser ney 
appendix give details implementation smoothing techniques including standard refinements katz smoothing 
give arguments justifying kneser ney smoothing example code showing interpolated kneser ney smoothing easy implement 
completeness show exact formula interpolated smoothed trigram 
practice avoid zero probabilities smooth unigram distribution uniform distribution unigram smoothing formulas simplicity include completeness 
represent size vocabulary 
mod bigram mod bigram vw vw mod unigram mod unigram vw vw repeat results chen goodman 
results run exactly sections corpus heldout training test rest expect comparable 
baseline experiments simple version jelinek mercer smoothing single bucket version identical smoothing technique described simple interpolation 
kneser ney smoothing interpolated version kneser ney smoothing kneser ney mod version discounts single discount 
katz smoothing essentially version 
short jelinek mercer smoothing called deleted interpolation abs disc interp interpolated version absolute discounting 
training set size measured sentences words words sentence 
notice jelinek mercer smoothing katz smoothing cross better lower data sizes higher sizes 
part motivation running experiments multiple data sizes 
hand experiments done multiple corpora find techniques technique worked better corpus worked better 
feel reasonably confident decision run multiple corpora 
chen goodman give complete comparison techniques depth analysis 
chen goodman gives superset serves tutorial 
diff test cross entropy baseline bits token training set size sentences relative performance algorithms wsj nab corpus gram jelinek mercer baseline katz kneser ney kneser ney mod abs disc interp witten bell backoff smoothing results data sizes gram order vs entropy higher order grams trigram assumption proven practice reasonable cases longer contexts helpful 
natural relax trigram assumption computing longer context gram model 
cases sequence form seen training data system need backo interpolate grams trigrams bigrams unigrams cases long sequence seen may predictor earlier experiments longer contexts showed little benefit 
turns partially due smoothing 
shown chen goodman smoothing methods significantly better higher order grams 
particular advantage interpolated kneser ney smoothing larger higher order grams lower order ones 
performed variety experiments relationship gram order perplexity 
particular tried katz smoothing lated kneser ney smoothing gram orders standard data sizes 
results shown 
seen previously observed chen goodman behavior katz smoothing di erent behavior kneser ney smoothing 
chen goodman determined main cause di erence backo smoothing techniques katz smoothing backo version kneser ney smoothing interpolated kneser ney smoothing poorly low counts especially counts gram order increases number counts increases 
particular katz smoothing best performance trigram level gets worse level exceeded 
kneser ney smoothing hand essentially monotonic grams 
plateau point kneser ney smoothing depends amount training data available 
small amounts words plateau point trigram level full training data words small improvements occur gram bits better gram gram bits better gram 
di erences size interesting practical importance 
di erence grams grams bits important rest experiments models built gram data appears give tradeo computational resources performance 
note practice going trigrams impractical 
tradeo memory performance typically requires heavy pruning grams grams reducing potential improvement 
ignore memory performance tradeo overly complicate di cult comparisons 
seek build single best system possible ignoring memory issues leaving practical interesting complicated issue finding best system memory size research bit past research goodman gao 
note experiments done section done special tool described briefly detail appendix 
skipping moves larger larger grams chance having seen exact context chance having seen similar context words increases 
skipping models rosenfeld huang ney martin siu ostendorf observation 
variations technique techniques lattices saul pereira dupont rosenfeld models combining classes words 
considering gram context subsets consider 
seen phrase show john time seen phrase show stan time 
normal gram predicting time show john back time john time relatively low probability 
hand skipping model form assign high probability time show 
skipping grams interpolated normal gram forming models usual 
traditional skipping sort poor man higher order gram 
instance create model form model form component probability depends previous words trigram probability gram depends extend idea combining pairs contexts gram gram way component probability depending previous words 
performed sets experiments grams trigrams 
gram skipping experiments contexts depended previous words words variety ways 
tried models interpolated baseline gram model 
readability conciseness define new notation letting allowing avoid numerous subscripts follows 
results shown 
model interpolated dependencies vw xy 
simple model smallest training data sizes competitive larger ones 
tried simple variation model interpolated making simple addition leads sized improvement levels roughly bits simpler skipping model 
variation analogous adding back dependencies missing words 
particular interpolated models depended variables interpolation order modified 
instance refer model form interpolated vw interpolated interpolated interpolated interpolated 
experiments done interpolated kneser ney smoothing probability uses modified backo distribution 
model just previous component starts interpolation full gram 
hoped case full gram occurred training data skipping model accurate help 
fact hurt tiny bit bits word training level 
turned training data sta vw xy sk vw xy sk gram skipping techniques versus gram baseline wanted try radical approaches 
instance tried interpolating baseline 
model puts preceding words important position component 
model previous leading conclude word far important 
tried model puts word possible position backo model 
worst model intuition word critical 
saw adding vw xy having component position final important 
case trigrams 
wanted get sort upper bound gram models 
interpolated 
model chosen include pairs triples combinations words possible 
result marginal gain bits best previous model 
find results particularly encouraging 
particular compared sentence mixture results potential gained skipping models 
sentence mixture models appear lead larger gains data skipping models appear get maximal gain words 
presumably largest data sizes gram model trained fewer instances skipping model useful gram 
examined trigram models 
results shown 
baseline comparison trigram model 
comparison show relative improvement gram model trigram relative improvement skipping gram vw xy trigram skipping models component depended previous words 
tried experiments form 
intuition pairs back word useful interpolated xy wy vy uy ty models 
particularly largest sizes 
presumably sizes appropriate instances back word seen 
tried pairs words gram level xy wy wx 
considering simplicity worked 
tried similar models gram pairs gram pairs gram pairs model contained di erent pairs 
improvement gram pairs marginal especially considering large number increased parameters 
trigram skipping results relative baseline better gram skipping results 
appear data comparable sentence mixture models terms improvement get 
furthermore lead due technical smoothing issues 
particular experimentation turned due interpolated kneser ney smoothing single discount know multiple discounts better 
multiple discounts problem goes away 
trigram skipping techniques versus trigram baseline improvement gram small amounts data course best gram skipping model better best trigram skipping model 
reasonable technique small intermediate amounts training data especially grams 
clustering clusters describe clustering techniques bit di erent show slightly ective traditional clustering brown ney 
consider probability tuesday party 
training data contains instances phrase party tuesday phrases party wednesday party friday appear 
put words classes word tuesday class weekday 
consider probability word tuesday phrase party word weekday 
denote probability tuesday party weekday 
decompose probability tuesday party weekday party tuesday party weekday word belongs class called hard clustering strict equality fact trivially proven 
represent cluster word word belongs single cluster substituting equation equation get equation strict equality smoothing taken consideration clustered probability accurate non clustered probability 
instance seen example party tuesday seen examples phrases party wednesday probability weekday party relatively high 
may seen example party weekday tuesday backo interpolate lower order model may able accurately estimate tuesday weekday 
smoothed clustered estimate may 
call particular kind clustering predictive clustering 
hand show clusters poor predictive clustering lead degradation 
note predictive clustering uses improving perplexity 
predictive clustering significantly speed maximum entropy training goodman factor compress language models goodman gao 
type clustering cluster words contexts 
instance party class event class preposition write tuesday party tuesday event preposition generally combining equation equation get equation take account exact values previous words interpolate normal trigram model 
call interpolation equation trigram clustering 
call generalization technique invented ibm brown uses approximation get interpolated normal trigram refer ibm clustering 
clustering uses information regular ibm clustering assumed lead improvements 
shown works interpolated normal trigram model 
alternatively discarding information simply change backo order called index clustering index tuesday party tuesday party event preposition abuse notation slightly order words right side indicate backo interpolation order 
equation fact originally name assumption better 
implies go tuesday party event preposition tuesday event preposition tuesday preposition tuesday preposition tuesday 
notice word belongs single cluster variables redundant 
instance notation party event preposition party event preposition event generally write index clustered model 
especially noteworthy technique 
best performing technique combination techniques 
technique intuition predictive clustering factoring problem prediction cluster followed prediction word cluster 
addition level smooths prediction combining word cluster estimate 
interpolated normal trigram model 
form variations themes 
happens works better ibm clustering describe briefly 
combining index predictive clustering interpolating normal trigram predictive clustered trigram wanted get sort upper bound gained clustering tried combining clustering techniques get call interpolation normal trigram model index model predictive model true model model 
boring details comparison di erent clustering techniques kneser ney smoothing di erent way combining interpolates models cluster level word level models 
boring details show comparison di erent clustering techniques kneser ney smoothing 
clusters built separately training size 
keep comparable chart katz smoothing baseline relative entropy comparisons 
notice value comparison kneser ney smoothed clustering katz smoothed clustering decreases training data small data sizes bits best clustered model largest sizes bits 
clustering technique dealing data sparseness unsurprising 
notice ibm clustering consistently works 
techniques tried worked better clustering simple variation interpolate 
works bits better ibm clustering 
problem smallest training size case worse 
believe clusters smallest training size poor predictive style clustering gets trouble happens smooths words may unrelated ibm clustering interpolates normal trigram model making robust poor clusters 
models predict clustering interpolate unclustered trigram worse baseline smallest training size 
note particular experiments fixed vocabulary severe test clustering smallest levels 
words word vocabulary occur words training data 
attempted partially deal adding pseudo count word occurrence fake word 
property making unseen words words low counts similar hopefully putting cluster 
data words training interpreted system bad clusters realistic system vocabulary match training data 
show comparison techniques katz smoothing techniques kneser ney smoothing 
results similar interesting exceptions particular works kneser ney smoothed model poorly katz smoothed model 
shows smoothing significant ect techniques clustering 
result clustering techniques size kneser ney version outperforms katz smoothed version 
fact kneser ney smoothed version outperformed interpolated backo absolute discounting versions technique size 
ways perform clustering explore 
cluster groups words complete contexts individual words 
change notation moment computing word cluster word cluster compute context cluster instance trigram model cluster contexts new york los angeles city wednesday late tomorrow time 
di cult issues solve kind clustering 
kind conditional clustering empirically determine context best combination clusters words approach 
finding clusters large amount previous research focused best find clusters brown kneser ney yamamoto sagisaka pereira bellegarda 
previous research small di erences di erent techniques finding clusters 
result automatically derived clusters outperform part speech tags niesler training data ney 
explore di erent techniques finding clusters simply picked thought previous research 
need clusters di erent positions 
particular model ibm clustering call cluster predictive cluster clusters conditional clusters 
predictive conditional clusters different yamamoto sagisaka 
instance consider pair words 
general follow words predictive clustering belong cluster 
words follow conditional clustering belong di erent clusters 
pilot experiments optimal number clusters predictive conditional clustering di erent optimize number conditional predictive clusters separately technique training data size 
particularly time consuming experiment time number clusters changed models rebuilt scratch 
try numbers clusters powers allows try wide range numbers clusters factor away optimal number 
examining charts performance heldout data produce numbers clusters close optimal 
clusters automatically tool attempts minimize perplexity 
particular conditional clusters try minimize perplexity training data bigram form equivalent maximizing predictive clusters try minimize perplexity training data 
minimize boring details doing minimization unsmoothed training data formula equal clustering 
method leaving kneser ney formula approach di cult 
independent clustering su cient try maximize 
convenient exactly opposite done conditional clustering 
means clustering tool simply switch order program get raw counts clustering 
give details boring details clustering algorithm section 
caching speaker uses word word near 
observation basis caching kuhn kuhn de mori kuhn de mori kupiec jelinek 
particular unigram cache form unigram model spoken words article article markers available fixed number previous words 
unigram cache linearly interpolated conventional gram 
type cache model depends context 
instance form smoothed bigram trigram previous words interpolate standard trigram 
particular trigram cache smooth simple interpolated trigram model counts preceding words document 
technique conditional caching 
technique weight trigram cache di erently depending previously seen context 
boring details digress moment mention trick 
interpolating probabilities lillian lee allows simplify constraints search believe aids parameter search routine adding useful dimension search 
particularly useful components 
particular conditional caching formula smooth cache smooth tried additional improvement 
assume data useful cache linear functions amount data cache number words far current document 
multiplier min usual multiplier parameters estimated heldout data 
parameter search engine nearly set near maximum value allowed assigning multiplier small value typically meaning variable weighting essentially ignored 
try conditionally unigram bigram trigram caches 
smooth cache smooth cache smooth di erent cache models interpolated trigram compared trigram baseline boring details gives results running cache models 
interpolated kneser ney smoothed trigram 
gram cache models smoothed simple interpolation technical reasons 
seen caching potentially powerful techniques apply leading performance improvements bits small data 
large data improvement substantial bits 
data sizes gram caches perform substantially better unigram cache version gram cache appears small di erence 
noted results assume previous words known exactly 
speech recognition system product scenarios include user correction 
possible cache errors 
instance user says recognize speech system hears nice beach user says speech recognition system may hear beach ignition probability beach significantly raised 
getting improvements caching real product potentially harder problem 
sentence mixture models iyer ostendorf iyer 
observed corpus may di erent sentence types sentence types grouped topic style criterion 
matter grouped modeling sentence type separately improved performance achieved 
instance wall street journal data assume di erent sentence types financial market sentences great deal numbers stock names business sentences promotions mergers general news stories 
compute probability sentence sentence type take weighted sum probabilities sentence types 
long distance correlations sentence lots numbers lots promotions captured model model better 
course general know sentence type heard sentence 
treat sentence type hidden variable 
denote condition sentence consideration sentence type probability sentence type written global model sentence types better individual sentence type 
special context true di erent sentence types typical sentence interpolation parameters optimized held data subject constraint 
probability sentence equal equation read saying hidden variable sentence type prior probability sentence type compute probability test sentence sentence type sum probabilities prior probability sentence type 
probabilities may su er data sparsity linearly interpolated global model interpolation weights optimized held data 
number ofs types number sentence types versus entropy sentence types training data clustering program clustering words case tried minimize sentence cluster unigram perplexities 
represent sentence type assigned sentence word part 
words sentence assigned sentence type 
tried put sentences clusters way maximized 
simpler technique iyer ostendorf 
stage process stage unigram similarity agglomerative clustering method second stage em gram reestimation 
technique soft clusters sentence belong multiple clusters 
assume technique results better models 
performed fairly large number experiments sentence mixture models 
sought study relationship training data size gram order number sentence types 
ran number experiments trigrams grams standard data sizes varying number sentence types normal model sentence mixtures 




number ofs types number sentence types versus entropy relative baseline experiments done kneser ney smoothing 
results shown 
give results graphed relative respective gram model baselines 
note trust results mixtures experiments words heldout data experiments 
sentence types parameters system may heldout data accurately estimate parameters 
particular plateau shown training data show heldout data 
ideally run experiment larger heldout set required days words impractical 
results interesting number reasons 
suspected sentence mixture models useful larger training data sizes words improvement sentence mixture model bits words nearly bits 
sentence mixture models computers get faster larger training data sizes increase 
second suspected grams sentence mixture models attempt model long distance dependencies improvement combination sum individual improvements 
seen words training data di erence trigrams grams small anyway question important 
words training data negative interaction 
instance sentence types training data improvement bits trigram bits gram 
similarly mixtures improvement trigram gram 
approximately third improvement correlated 
iyer ostendorf reported experiments mixture components components significant di erence words training data 
thorough investigation shows substantial room improvement larger numbers mixtures especially training data potential extends sentence types largest size 
important result leading twice potential improvement small number components 
think new result interesting research 
particular techniques relatively simple extensions techniques lead larger improvements 
instance simply smoothing sentence type global model create sentence types supertypes smooth sentence type supertype global model combined 
alleviate data sparsity ects seen largest numbers mixtures 
boring details sentence mixture model results encouraging disappointing compared previous results 
iyer ostendorf achieve perplexity reduction word error rate reduction mixtures similar data achieve show reductions mixtures 
largest di erence aware system di erence clustering technique fairly simple technique fairly complex 
possibilities include di erent smoothing witten bell smoothing versus katz interpolated kneser ney smoothing fact clusters 
katz interpolated kneser ney different techniques report sentence mixture models produce improvement think di erence smoothing explains di erent results 
iyer ostendorf significant di erence clusters di erence similar data 
noteworthy iyer ostendorf baseline perplexity versus baseline perplexity training set di erent vocabularies test sets 
unknown factor accounts di erence baseline perplexities gives room improvement sentence mixture models 
worth noting improvement caching iyer ostendorf versus 
cache implementations similar main di erence exclusion words 
adds support di erent baseline test condition explanation 
di erence perplexity reduction due di erence mixture model implementation test conditions additional perplexity reduction achieved amount merits additional exploration 
boring details sentence mixture models useful combining di erent language model types 
instance jurafsky 
uses sentence mixture model combine stochastic context free grammar scfg model bigram model resulting marginally better results model separately 
model jurafsky form scfg scfg bigram bigram turns equivalent model form equation 
version equations advantage stack decoder allows sentence mixture models relatively little overhead compared equation charniak discussed section uses sentence level mixture model combine linguistic model trigram model achieving significant perplexity reduction 
combining techniques section additional results combining techniques 
techniques works separately show synergistically partially redundant 
instance shown improvement modeling gram models larger improvement 
similarly shown improvement sentence mixture models combined grams sentence mixture models techniques increase data sparsity 
section systematically study issues ect smoothing technique technique help combined technique ect word error rate separately 
di erent ways combine techniques 
obvious way combine techniques simply linearly interpolate lead largest possible improvement 
try combine concepts 
give simple example recall clustered trigram form simply interpolate clustered trigram normal gram course sense combine concept gram concept clustered gram follow idea combining concepts simply interpolating section 
tends result performance complex models 
boring details combine sentence mixture models caches need answer additional questions 
iyer ostendorf separate caches sentence type putting word cache sentence type 
required great deal additional system pilot experiments combining sentence mixture models caches single global cache improvement combination nearly equal sum individual improvements 
iyer ostendorf get improvement nearly equal sum concluded separate caches reasonable combination technique 
boring details combination technique somewhat complicated 
highest level sentence mixture model sum sentence specific models sentence type 
particular sentence mixture model combine di erent techniques predictive clustering 
combine sentence specific global cache global skipping models predict cluster word predict word cluster 
sentence type wish linearly interpolate sentence specific gram model global gram model skipping models cache models 
clustering wish words clusters 

interpolation parameters 
define similar functions 
define analogous function predicting words clusters ws ws write probability model clearly combining techniques easy show ects combination roughly additive ort worthwhile 
performed sets experiments 
experiments perform caching unigram cache conditional trigram cache sentence mixture models mixtures trigram skipping wx gram skipping vw interpolated xy word error rate experiments done punctuation aid comparisons perform additional entropy experiments section punc set punctuation 
formula oversimplification values depend amount training data linear fashion context occur cache trigram cache 
case values renormalized context sum 
relative entropy technique versus katz trigram baseline relative entropy technique versus kneser ney trigram baseline relative entropy removing technique versus techniques combined baseline techniques versus katz trigram baseline set experiments technique separately katz smoothing 
results shown 
performed experiments techniques kneser ney smoothing results shown 
results similar techniques independent smoothing grams kneser ney smoothing clearly large gain fact kneser ney smoothing grams hurt small medium data sizes 
wonderful example synergy techniques help separately 
caching largest gain small medium data sizes combined kneser ney smoothing grams largest gain large data sizes 
caching key data sizes advantages kneser ney smoothing clustering clearer combined techniques 
set experiments shown tried removing technique combination techniques equation 
baseline techniques combined show performance instance kneser ney gram models show techniques versus katz smoothed trigram 
add additional point graph 
words model bits normal katz model excellent result knew word model hurt poor performance clustering smallest data size 
interpolated normal gram word level technique indicated normal gram 
led entropy reduction bit 
gain clearly real world value entropy gains small medium sizes come caching caching lead substantial word error rate reductions 
allow nice title 
interpolating normal gram larger sizes led essentially improvement 
performed word error rate experiments rescoring best lists wsj dev eval utterances 
best error rate best lists recognizer models slightly worse baseline rescoring best error rate minimum possible rescoring 
able get word error rate improvements caching cache consisted output recognizer hurt caching interpolation parameters estimated correct histories recognized histories 
shows word error rate improvement technique katz smoothing kneser ney smoothing removed caching 
important single factor word error rate kneser ney smoothing leads small gain skipping grams ective 
clustering leads significant gains 
case clustering kneser ney smoothed model lower word error rate corresponding katz smoothed model 
strange clustering result katz entropy higher due noise due fact optimized number clusters separately systems optimizing perplexity leading number clusters word error rate versus entropy optimal word error rate reduction 
get word error rate reduction katz smoothed baseline model 
expect perplexity reductions 
probably due rescoring best lists integrating language model directly search rescoring large lattices 
implementation notes implementing model described straightforward 
give notes significant implementation tricks reasonably novel appendix give details 
describe parameter search technique 
discuss techniques deal large size models constructed 
consider architectural strategies research possible 
give hints implementing clustering methods 
size models required research large 
particular techniques roughly multiplicative ect data sizes moving grams trigrams results factor increase clustering results nearly factor increase combination sentence mixture models skipping leads factor 
model size roughly times size standard trigram model 
building complex model impractical 
simple trick 
pass test data text best lists heldout data parameter optimization determine complete set values need experiments 
go training data record values 
drastically reduces amount memory required run experiments reducing gigabytes roughly 
trick divide test data pieces test data fewer values need store 
appendix describes ways verified cheating resulted results non cheating model gotten 
careful design system necessary 
particular concept model object set parameters return probability word class history 
created models compose models interpolating word level class level sentence level multiplying done predictive clustering 
allowed compose primitive models implemented caching various smoothing techniques large variety ways 
smoothing techniques interpolation require optimization free parameters 
cases free parameters estimated training data leaving techniques better results obtained powell search parameters optimizing perplexity held data chen goodman technique 
allowed optimize parameters jointly say optimizing model interpolation parameters typically done 
relatively easy essentially experiment find optimal parameter settings model suboptimal guesses results related models 
smoothing algorithms reimplemented research reusing small amount code details closely follow chen goodman 
includes additive smoothing unigram distribution katz smoothing kneser ney smoothing 
constant added unigram counts leads better performance small training data situations allowed compare perplexities di erent training sizes unigram received counts meaning probabilities returned 
shortage techniques generating clusters appears little evidence di erent techniques optimize criterion result significantly di erent quality clusters 
note different algorithms may require significantly di erent amounts run time 
particular agglomerative clustering algorithms may require significantly time top splitting algorithms 
top splitting algorithms additional tricks including techniques buckshot cutting 
computational tricks adapted brown 

details clustering techniques appendix 
techniques section briefly discuss techniques received interest language modeling done experiments techniques 
techniques include maximum entropy models sentence maximum entropy models latent semantic analysis parsing models neural network models 
rosenfeld gives broader di erent perspective field additional techniques discussed 
maximum entropy models maximum entropy models darroch ratcli received fair amount attention language modeling rosenfeld 
maximum entropy models models form maxent exp exception katz smoothed models estimated number clusters simple katz clustered models large katz smoothed models extremely time consuming need find potential parameter change 
normalization function simply set equal exp real numbers learned learning algorithm generalized iterative scaling gis darroch ratcli 
arbitrary functions input typically returning 
capture grams caches clusters skipping models 
sophisticated techniques triggers described 
generality power major attractions maximum entropy models 
attractions maximum entropy models 
training data possible find maxent words number times expect occur model number times observe training data 
define arbitrary predicates words histories build model predicates true observed 
addition model maximizes entropy sense smooth possible close uniform distribution meeting constraints 
quite capability 
furthermore pieces research especially optimistic maximum entropy models 
smoothing technique chen rosenfeld maximum entropy smoothing technique works better smoothing 
second speedup techniques goodman lead factor speedup traditional maximum entropy training techniques slow 
reasons optimism pessimism respect maximum entropy models 
hand maximum entropy models lead largest reported perplexity reductions 
rosenfeld reports perplexity reduction maximum entropy models combined caching conventional trigram 
hand pilot experiments maximum entropy negative compared comparable interpolated gram models aware research maximum entropy models yield significant improvement comparable gram models 
furthermore maximum entropy models extremely time consuming train speedup techniques slow test situations 
maximum entropy techniques may impractical real world 
special aspect maximum entropy models worth mentioning word triggers rosenfeld 
word triggers source substantial gain maximum entropy models 
trigger models word school increases probability probability similar words teacher 
rosenfeld gets approximately perplexity reduction word triggers gain reduced combining model contains cache 
tillmann ney achieves perplexity reduction combined model cache zhang 
reports reduction 
sentence maximum entropy models variation maximum entropy approach sentence maximum entropy approach rosenfeld 
variation probability sentences predicted probabilities individual words 
allows features entire sentence coherence cai word level features 
represents entire sentence sentence maximum entropy model form exp starting model gram model 
notice normalization factor case constant eliminating need compute normalization factor separately context 
fact necessary compute uses 
main benefits models proponents techniques goodman reduce burden normalization 
problems sentence approach 
training sentence maximum entropy models particularly complicated chen rosenfeld requiring practice sampling methods monte carlo markov chain techniques 
second benefits sentence model may small divided words 
consider feature sentence parsable grammar 
imagine feature contributes entire bit information sentence 
note practice grammar broad coverage parse grammatical sentences broad coverage parse ungrammatical sentences reducing information provides 
average word sentence bit information reduces entropy bits word relatively small gain complex feature 
problem see models features captured ways 
instance commonly advocated feature sentence maximum entropy models coherence notion words sentence similar topics 
techniques caching triggering sentence mixture models ways improve coherence sentence require expense sentence approach 
pessimistic long term potential approach 
course pessimistic 
latent semantic analysis bellegarda shows techniques latent semantic analysis lsa promising 
lsa similar principle components analysis pca dimensionality reduction techniques way reduce data sparsity plagues language modeling 
technique leads significant perplexity reductions word error rate reductions relative compared katz trigram words 
interesting formally compare results conventional caching results exploit similar long term information 
bellegarda gets additional improvement results clustering techniques lsa perplexity reductions appear similar perplexity reductions conventional ibm style clustering techniques 
neural networks interesting neural networks language modeling bengio 

order deal data sparsity map word vector continuous features probability various outputs learned function continuous features 
mapping learned backpropagation way weights network 
best result perplexity reduction baseline deleted interpolation style trigram 
boring details wanted see neural network model compared standard clustering models 
performed experiments corpus vocabulary splits training test heldout data bengio supplying relevant data 
verified baseline correct got perplexity simple interpolation versus deleted interpolation style baseline reasonable 
hand kneser ney smoothed trigram perplexity 
remaining experiments kneser ney smoothing 
tried ibm clustered gram model ways similar neural network model words context 
clustered model perplexity compared neural network perplexity 
number interpolate trigram clustered model 
ran gram ibm clustered model 
model perplexity worse best neural network model score model interpolated trigram fair comparison 
tried build pull stops model 
best model build include caching instance best model build inputs best neural network previous words 
previous words getting benefit th word back 
model form stops perplexity meaningless better best neural network 
best neural network model relied interpolation trigram model trigram model exclusively low frequency events 
trigram model relatively high perplexity compared kneser ney smoothed trigram assume significant perplexity reduction gotten simply kneser ney smoothed trigram 
means neural network model results really 
interesting see neural network model common traditional clustered models 
simple experiment interpolate neural network model clustered model see gains additive 
relative simplicity neural network results promising area research 
boring details structured language models interesting exciting new areas language modeling research structured language models 
successful structured language model chelba jelinek models successful charniak 
basic idea structured language models properly phrased statistical parser thought generative model language 
furthermore statistical parsers take account longer distance dependencies subject direct indirect objects 
dependencies useful previous words captured trigram model 
chelba able achieve perplexity reduction baseline trigram model charniak achieves impressive reduction 
boring details hypothesized benefit structured language model redundant techniques skipping clustering grams 
question information model model captures turns di cult 
formulas measuring conditional entropy word di erent models known rely computing joint probabilities 
models sparsely estimated conventional trigram structured language model computing joint probabilities hopeless 
decided approximate measures 
simple practical technique simply try interpolating models 
glance interpolation leads gain models capturing information gains additive common baseline information independent 
unfortunately assumption incorrect 
particular comparing structured language model kneser ney smoothed trigram interpolated trigram cache assume overlap information minimal getting gain interpolation versus cache model kneser ney trigram 
simply interpolation crude way combine information don know better 
cache model better structured language model interpolation weight goes cache model structured language model help learn cache model lower perplexity knew 
strategy bit complex bit harder interpret 
versions systems simple interpolation smoothing 
smoothing technique implemented slm program toolkit 
removed smoothing factor 
tried comparing slm trigram various cache sizes course cached document boundaries interpolating slm 
assumed slm cache size capturing roughly orthogonal information 
amount perplexity reduction expect models uncorrelated 
instance baseline slm interpolated trigram perplexity 
clustered model perplexity similarly trigram trigram cache words context perplexity clustered model 
combination slm word context cache perplexity reduction cache 
combined clustered model perplexity reduction clustered model 
improvements uncorrelated assumed reduction saw reduction 
reduction expected 
say overlap clustered model roughly 
table shows results various experiments 
model column describes model interpolated slm 
column shows perplexity model reduction column shows reduction interpolating model slm 
second column shows perplexity similar cache model second reduction column shows perplexity reduction interpolating cache model slm 
final column overlap shows ration reduction second presumed overlap model slm 
model closest cache model reduction reduction overlap kneser ney gram trigram skipping special cluster special cluster trigram skipping model model pairs gram level 
special cluster special cluster models clustered skipping models designed capture contexts similar assumed structured language model doing 
liked looked combinations kneser ney smoothing clustering best cache model tested perplexity kneser ney clustered model lower perplexity 
di cult draw strong results 
odd result large overlap kneser ney smoothing 
suspect breakdown language model individual components structured lm smoothing ect 
entire evaluation flawed 
looked correlation individual word probabilities 
examined model word di erence probability baseline trigram model 
measured correlation di erences 
results similar results 
interesting perform kind experiments structured language models 
unfortunately current best charniak predict words left right 
charniak interpolate sentence level 
sentence level interpolation experiments harder interpret 
reasonably confident large portion increase slm order comes information similar clustering 
surprising slm explicit models part speech tags non terminals objects behave similarly word clusters 
performed experiments chelba reached opposite concluded glass half empty chelba concluded glass half full 
boring details previous combinations relatively little previous research attempted combine techniques previous research combining techniques particularly systematic 
furthermore techniques typically combined cache language model 
cache model simply linearly interpolated model room interaction 
previous papers merit mentioning 
martin 

combined interpolated kneser ney smoothing classes word phrases skipping 
unfortunately compare baseline compare call interpolated linear discounting poor baseline 
improvement interpolated kneser ney achieve perplexity reduction baseline versus baseline 
improvement clustering comparable improvement skipping models improvement word phrases small di erence results due mainly implementation additional techniques caching grams sentence mixture models 
word error rate reduction interpolated kneser ney 
assume reason word error rate reduction proportional perplexity reduction fold 
perplexity reduction came caching word error rate results 
second able integrate simpler model directly recognizer needed rescore best lists reducing number errors correct 
piece worth mentioning rosenfeld 
large number techniques combined maximum entropy framework interpolation 
techniques tested multiple training data sizes 
best system interpolates katz smoothed trigram cache maximum entropy system 
maximum entropy system incorporates simple skipping techniques triggering 
best system perplexity reductions data similar 
rosenfeld gets approximately reduction word triggers technique 
rosenfeld results excellent quite possibly exceed modern techniques kneser ney smoothing trigram model interpolated smaller cuto possible faster machines newer training techniques smoothing maximum entropy model newer techniques 
rosenfeld experiments simple class techniques success assume modern technology 
rosenfeld achieves word error rate unsupervised adaptation way adapted 
achieves assuming users correct mistakes immediately 
surprisingly little combining techniques 
noteworthy research aware weng 
performed experiments combining multiple corpora fourgrams classbased approach similar sentence mixture models 
combining techniques leads perplexity reduction hub language model 
model trained tested di erent text genre models comparison 
hope abandon ye enter long results particularly novel ideas 
grad students thinking research language modeling read section section argue meaningful practical reductions word error rate hopeless 
point trigrams remain de facto standard don know beat improvements justify cost 
claim little evidence entropy meaningful measure progress perplexity entropy improvements small 
conclude language modeling research including comparing straw man baseline ignoring cost implementations presents illusion progress substance 
go describe language modeling research worth ort 
practical word error rate reductions hopeless language modeling improvements require significantly space trigram baseline compared typically require significantly time 
language modeling papers contain paragraph trigram models state art 
trigram models obviously 
slightly 
model lower perplexity somewhat lower word error rates trigram 
papers don say universally applies resulting model requires space time simple trigram model 
trigram models state art sense commercial speech recognizers pass research recognizers 
reasons fairly space cient sorts tricks integrate trigram storage search 
instance recognizers microsoft storage method trigrams stored phonetic trees 
complex language modeling techniques especially clustering techniques incompatible type storage cient practical optimized recognizer 
consider practice techniques implemented 
sentence mixture models require substantially storage corresponding global models di erent models stored mixture global model needs stored interpolate mixture specific models 
furthermore integrated pass recognizer di erent path explored mixture type significantly slowing recognizer 
caching problems discussed significantly increase storage recognizer requires users correct mistakes sentence mistakes locked 
clustered models sort described substantially increase storage interact poorly kinds prefix trees search 
grams require substantially extended version technical report full rave fear editorial review willy sections innocent graduate students 
express frustration vent unsuspecting readers 
natural skip section risk consequences 
space trigrams slow search 
kneser ney smoothing leads improvements theory practice language models built high count cuto conserve space speed search high count cuto smoothing doesn matter 
skipping models require complex search space lead marginal improvements 
short improvements studied di erence real system exception caching typically available turned default users don trained consistently correct recognizer errors 
results surveyed reimplemented majority promising language modeling techniques introduced katz 
improvement perplexity largest reported improvement word error rate relatively small 
partially reflects emphasis perplexity word error rate flaws experiments 
rescoring lattices best lists lead larger improvements 
integrating language modeling search helped 
trying optimize parameters techniques word error rate reduction perplexity reduction worked better 
despite flaws think results reasonably representative 
tried best get language modeling improvements mixed results 
instance martin 
gets relative word error rate reduction fair baseline larger easier baseline slightly better 
cost improvements high size language model times cpu time 
larger improvements probably gotten easily cheaply simply increasing number mixture components loosening search thresholds acoustic model 
fact simply language model training data comparable improvements probably 
perplexity wrong measure claim little evidence word error rate correlates better entropy perplexity 
relative entropy reductions smaller relative perplexity reductions di cult get useful word error rate reductions 
considered doing experiment incrementally add portions test data training set compute perplexity entropy word error rates 
noticed conducting experiment slightly idealized world predict results 
add test data perfect information 
su ciently powerful gram model say gram search errors speech recognizer get portion exactly correct leading criticisms apply just martin 
martin system example quirks tool see appendix hard measure size models acoustic modeling done ine best rescoring integrated search 
word error rate test set cross entropy test set abs disc interp katz kneser ney fix kneser ney mod word error rate versus entropy relative reduction word error rate 
similarly require essentially bits predict portion training data seen leading reduction entropy 
theoretical analysis leads hypothesize linear relationship entropy word error rate 
course perform realistic experiments see perplexity entropy better word error rate 
experiments shed light question correlation word error rate entropy correlation word error rate perplexity relatively narrow range perplexities tested 
consider previous chen goodman 
show experiments done correlation word error rate entropy 
obviously relationship strong appears linear large range including roughly intercept just analysis predicted 
consider domains text compression 
trivially relationship entropy objective function number bits required prefer predictions seeing data vastly increases accuracy 
represent data linear 
strong arguments especially light researchers gotten results entropy word error rate correlate 

true imply get large reductions perplexity get meaningful word error rate reductions 
instance perplexity reduction say corresponds reduction bits bits entropy reduction 
appears meaningful perplexity reduction trivial entropy reduction 
fear means commercially useful reductions perplexity 
progress illusory common related pitfall language modeling research straw man baseline 
language modeling papers point problem trigram models typically capture long distance dependencies proceed give way model longer contexts 
papers compare smoothed gram simple obvious way trigram capture longer contexts mind comparing trigram cache technique known years models sentence mixture models 
result way beat trigrams assuming don care speed space 
new techniques better previous ones combined larger improvements er better speed space tradeo techniques rarely explored 
cache comparison typically context combining cache model baseline trigram 
perplexity results usually compare technique cache versus trigram 
cache look cache 
instance trigger model maximum entropy done 
useful triggers just self triggers basically cache features just cache 
poor experimental technique misleading papers means unique language modeling 
prevalence trigram baseline practice allowing researchers call state art straight face particularly easy give illusion progress 

previous sections language modeling research 

areas see hope language model compression language model new applications language modeling basic research 
wait minute doesn bad things 
doesn claim trigrams state art misleadingly call fair baseline fairly compares techniques 
sounds 
language model compression pruning area received relatively little research done kneser seymore rosenfeld stolcke siu ostendorf goodman gao 
care needed 
instance techniques seymore stolcke recognizer techniques clustering interact poorly search speech recognizers 
area progress 
similarly comprehensive showing space perplexity tradeo techniques examined pruning interpolated models skipping models sentence mixture models 
area language modeling research language model adaptation 
common product scenario involves small amount domain training data lots domain data 
moderate amount research area 
research compare simply interpolating domain domain language models experiments works quite 
best research area probably iyer ostendorf 
suspect better techniques attempts failed 
language models understood applied domains church brown hull kernighan srihari 
trying improve trying new ways fruitful 
machine learning problem potentially language modeling techniques solution identifying new areas language models useful trying basic research 
unfinished areas language modeling basic research 
huge practical impact help advance field 
area continuous version kneser ney smoothing 
interpolated smoothing wonderful 
matter kind model worked better kneser ney smoothing smoothing technique tried 
kneser ney smoothing limited discrete distributions handle fractional counts 
fractional counts common instance expectation maximization em algorithms 
means currently know smoothing distributions learned em instances hidden markov models probabilistic context free grammars 
continuous version kneser ney areas 
related area needs research soft clustering versus hard clustering 
hard clustering assigns word single cluster soft clustering allows word placed di erent clusters 
essentially comparing hard clustering soft clustering soft style techniques including bengio 
bellegarda success hinting techniques may ective 
reason tried soft clustering know properly smooth soft clustered models fractional counts best continuous version kneser ney smoothing 
hope left summarize language modeling di cult area completely hopeless 
basic research possible continue new practical certainly interesting language modeling results 
appear areas useful language modeling research promising 
language modeling research area faint heart easily depressed 
long discussion believe results perplexity reduction small data set reduction large data punctuation best reported language modeling measured improvement fair baseline katz smoothed trigram model count cuto systematically explored smoothing higher order grams skipping sentence mixture models caching clustering 
important result superiority interpolated smoothing situation examined 
previously showed chen goodman kneser ney smoothing best technique training data sizes corpora types gram order :10.1.1.131.5458
shown best clustering techniques important factors building high performance language model especially grams 
carefully examined higher order grams showing performance improvements plateau gram level results gram level showing improvement gained past grams 
systematically examined skipping techniques 
examined models pairs gram level captures benefit 
performed experiments gram skipping models finding combination contexts captures benefit 
carefully explored sentence mixture models showing improvement previously expected increasing number mixtures 
experiments increasing number sentence types allows nearly twice improvement small number types 
caching results show caching far useful technique perplexity reduction small medium training data sizes 
show trigram cache lead twice entropy reduction unigram cache 
systematically explored clustering trying di erent techniques finding new clustering technique bit better standard ibm clustering examining limits improvements clustering 
showed clustering performance may depend smoothing cases 
word error rate reduction combining techniques caching 
put techniques leading reduction perplexity depending training data size 
results compare favorably reported combination results martin essentially subset techniques comparable baseline absolute discounting perplexity reduction half 
results show smoothing important factor language modeling interaction techniques ignored 
ways results bit discouraging 
model built complex slow large completely impractical product system 
despite size complexity word error rate improvements modest 
implies potential practical benefit speech recognizers language model research limited 
hand language modeling useful fields speech recognition interesting test bed machine learning techniques general 
furthermore parts results encouraging 
show progress language modeling continues 
instance important technique system sentence mixture models years old showed potential partially tapped 
similarly combination techniques novel 
furthermore results show improvements di erent techniques roughly additive expect improvement bits largest training size simply adding results total bits similar 
means incremental improvements may lead improvements best models simply overlapping redundant 
noted section promising language modeling techniques currently pursued maximum entropy models neural networks latent semantic analysis structured language models 
figuring combine techniques ones implemented lead larger gains complex models 
entire microsoft speech net research team help especially milind mahajan huang alex chelba gao 
anonymous reviewers sarah roland kuhn eric brill suzuki wang comments drafts 
especially stanley chen useful discussions addition small amounts text code implementation irrespectively originally coauthored stanley chen 
fil huang mei hwang 

improvements pronunciation prefix tree search organization 
icassp volume pages 
bellegarda yen lu chow 

novel word clustering algorithm latent semantic analysis 
icassp volume pages 
bellegarda 

exploiting latent semantic information statistical language modeling 
proceedings ieee august 
bengio pascal vincent 

neural probabilistic language model 
technical report departement informatique recherche universite de montreal 
reinhard 

combination words word categories histories 
icassp volume pages 
peter brown john cocke stephen dellapietra vincent dellapietra frederick jelinek john la erty robert mercer paul 

statistical approach machine translation 
computational linguistics june 
peter brown vincent dellapietra peter desouza jennifer lai robert mercer 

class gram models natural language 
computational linguistics december 
cai ronald rosenfeld larry wasserman 

exponential language models logistic regression semantic coherence 
proc 
nist darpa speech transcription workshop may eugene charniak 

immediate head parsing language models 
acl pages july 
chelba frederick jelinek 

exploiting syntactic structure language modeling 
proceedings th annual meeting acl august 
stanley chen joshua goodman 

empirical study smoothing techniques language modeling 
technical report tr harvard university 
available www research microsoft com 
stanley chen joshua goodman 

empirical study smoothing techniques language modeling 
computer speech language october 
stanley chen ronald rosenfeld 

cient sampling feature selection sentence maximum entropy language models 
proc 
icassp march 
stanley chen ronald rosenfeld 

gaussian prior smoothing maximum entropy models 
technical report cmu cs computer science department carnegie mellon university 
kenneth church 

stochastic parts program noun phrase parser unrestricted text 
proceedings second conference applied natural language processing pages 
douglas cutting david karger jan pedersen john tukey 

scatter gather cluster approach browsing large document collections 
sigir 
darroch ratcli 

generalized iterative scaling log linear models 
annals mathematical statistics 
pierre dupont ronald rosenfeld 

lattice language models 
technical report cmu cs school computer science carnegie mellon university pittsburgh pa september 


population frequencies species estimation population parameters 
biometrika 
joshua goodman gao 

language model size reduction pruning clustering 
icslp 
joshua goodman 

classes fast maximum entropy training 
icassp 
huang 
hon 
hwang 
lee rosenfeld 

sphinx ii speech recognition system overview 
computer speech language 
jonathon hull 

combining syntactic knowledge visual text recognition hidden markov model part speech tagging word recognition algorithm 
aaai symposium probabilistic approaches natural language pages 
iyer mari ostendorf 

transforming domain estimates improve domain language models 
eurospeech volume pages 
iyer mari ostendorf 

modeling long distance dependence language topic mixtures versus dynamic cache models 
ieee transactions acoustics speech audio processing january 
iyer mari ostendorf robin rohlicek 

language modeling sentence level mixtures 
darpa hlt pages 
frederick jelinek robert mercer 

interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice amsterdam netherlands northholland may jelinek merialdo roukos strauss 

dynamic lm speech recognition 
proc 
arpa workshop speech natural language pages 
daniel jurafsky chuck wooters jonathan segal eric gary nelson morgan 

stochastic context free grammar language model speech recognition 
icassp pages 
slava katz 

estimation probabilities sparse data langauge model component speech recognizer 
ieee transactions acoustics speech signal processing assp march 
kernighan church gale 

spelling correction program noisy channel model 
proceedings thirteenth international conference computational linguistics pages 
reinhard kneser hermann ney 

improved clustering techniques class statistical language modeling 
eurospeech volume pages 
reinhard kneser hermann ney 

improved backing gram language modeling 
proceedings ieee international conference acoustics speech signal processing volume pages 
reinhard kneser 

statistical language modeling variable context length 
icslp volume pages 
kuhn de mori 

cache natural language model speech reproduction 
ieee transactions pattern analysis machine intelligence 
kuhn de mori 

correction cache natural language model speech reproduction 
ieee transactions pattern analysis machine intelligence 
kuhn 

speech recognition frequency words modified markov model natural language 
th international conference computational linguistics pages budapest august 
kupiec 

probabilistic models short long distance word dependencies 
proc 
arpa workshop speech natural language pages 
david mackay linda peto 

hierarchical dirichlet language model 
natural language engineering 
sven martin christoph jorg frank wessel hermann ney 

assessment smoothing methods complex stochastic language modeling 
th european conference speech communication technology volume pages budapest hungary september 
hermann ney ute essen reinhard kneser 

structuring probabilistic dependences stochastic language modeling 
computer speech language 
niesler whittaker woodland 

comparison part speech automatically derived category language models speech recognition 
icassp volume pages 
fernando pereira naftali tishby lillian lee 

distributional clustering english words 
proceedings st annual meeting acl 
appear 
press flannery teukolsky vetterling 

numerical recipes cambridge university press cambridge 
ronald rosenfeld stanley chen zhu 

sentence exponential language models vehicle linguistic statistical integration 
computer speech language 
appear 
ronald rosenfeld 

adaptive statistical language modeling maximum entropy approach 
ph thesis carnegie mellon university april 
ronald rosenfeld 

decades statistical language modeling go 
proc 
ieee august 
lawrence saul fernando pereira 

aggregate mixed order markov models statistical language processing 
proceedings second conference empirical methods natural language processing pages 
seymore ronald rosenfeld 

scalable backo language models 
icslp 
siu mari ostendorf 

variable grams extensions conversational speech language modeling 
ieee transactions speech audio processing 
rohini srihari charlotte 

combining statistical syntactic methods recognizing handwritten sentences 
aaai symposium probabilistic approaches natural language pages 
richard stern 

specification arpa hub evaluation unlimited vocabulary nab news baseline 
proceedings darpa speech recognition workshop pages 
andreas stolcke 

entropy pruning backo language models 
proc 
darpa broadcast news transcription understanding workshop pages 
see online version corrections 
christoph tillmann hermann ney 

statistical language modeling word triggers 
proceedings int 
workshop speech computer pages october 


cient clustering grams statistical language modeling 
eurospeech pages 
weng stolcke sankar 

hub language modeling domain interpolation data clustering 
darpa speech recognition workshop pages february 
yamamoto sagisaka 

multi class composite ngram connection direction 
proceedings ieee international conference acoustics speech signal processing phoenix arizona may zhang black andrew finch sagisaka 

integrating detailed information language model 
icassp pages 
kneser ney smoothing section briefly justify interpolated kneser ney smoothing giving proof helps explain preserving marginals useful 
give implementation tips kneser ney smoothing 
proof give short theoretical argument kneser ney smoothing 
argument chen goodman derived done kneser ney 
prove additional fact previously assumption somewhat strengthening argument 
particular show optimal smoothing algorithm preserve known marginal distributions 
mention impossible prove smoothing technique optimal making assumptions 
instance mackay peto proves optimality particular simple interpolated model subject assumption dirichlet prior 
prove optimality kneser ney smoothing assumptions 
note assumption dirichlet prior probably empirically bad 
dirichlet prior bit odd language modeling correspond reality 
assumptions empirically mathematical convenience 
note aware case smoothing method outperforms kneser ney smoothing excellent piece evidence 
argument kneser ney smoothing follows 
absolute discounting empirically works reasonably approximates true discounts measured real data 
second interpolated techniques fairly consistently outperform backo techniques especially absolute discounting style smoothing 
empirical facts experiments chen goodman 
chen goodman kneser ney assumption preserving marginal distributions 
assumption prove 
chen goodman prove argument kneser ney smoothing technique uses absolute discounting interpolation preserves marginals interpolated kneser ney smoothing 
contribution section prove previously assumption strengthening argument interpolated kneser ney smoothing relies empirical facts proofs derived facts small amount hand waving 
previously assumed smoothing techniques preserve marginal distributions smoothing bigram unigram distribution preserved thing 
section prove modeling technique preserve known marginal distributions improved preserving distributions 
admittedly smooth kneser ney bigram preserving observed unigram distribution observed unigram dis tribution true unigram distribution simply approximation true unigram 
proof follows 
represent estimated distribution represent true probability 
consider estimated bigram distribution form 
show entropy respect reduced new distribution multiplier 
simply take old distribution multiply probability renormalize turn optimal value 
proof follows consider entropy respect convenient measure entropy nats bits logarithm 
nat ln bits 
entropy nats ln consider derivative entropy respect 
ln ln ln fact practice smooth unigram distribution uniform distribution divergence true distribution 
small amount hand waving 
agrees proof certainly improvement previous argument simply preserving marginal distribution 
reduces zero true marginal equal marginal derivative meaning decrease entropy decreasing derivative meaning decrease entropy increasing 
value leads lower entropy 
means smoothing algorithm preserve known marginal distributions modify resulting distribution multiplying way marginal preserved 
smoothing algorithm preserve known marginals suboptimal 
notice argument really smoothing just preserving known marginal distributions general 
argument justify maximum entropy models best model preserves known marginals 
proof really new novel part noticing fact applies smoothing algorithms distributions 
implementing kneser ney smoothing formula kneser ney smoothing bit complex looking 
algorithm finding counts kneser ney smoothing quite simple requiring trivial changes interpolated absolute discounting 
give code training interpolated absolute discounting model give corresponding code interpolated kneser ney model 
lines change marked asterisk notice change placement right curly braces 
completeness gives code testing interpolated absolute discounting interpolated kneser ney smoothing code identical 
implementation notes number contributions originality ideas completeness research size combination optimality 
research areas thorough previous research trying variations ideas able exceed previous size boundaries examining grams able combine ideas believable nearly parameters optimized jointly heldout data 
contributions possible designed tool interesting implementation details 
implementation details truly novel part 
section sketch useful implementation tricks needed perform 
store need single important trick part previously chen goodman store parameters needed models 
particular action program takes read heldout test data determine counts needed calculate heldout test probabilities 
training phase needed counts recorded 
note number necessary counts may smaller total number counts test data 
instance test instance xy typically need store training counts xy 
depends particular model type 
instance baseline smoothing simple interpolation necessary know xy xyz lower order counts 
fact useful implementation caching baseline smoothing ciency 
kneser ney smoothing things complex trigram need temporarily store xy compute 
need store seen 
need store actual count save fair amount space 
furthermore know discard table ones occurred 
partially reason calculate parameters models steps 
instance skipping model interpolates di erent kinds trigrams find needed parameters code implementing interpolated absolute discounting usage discount training test training data test data word line discount shift train shift test shift open train train line training data td increment trigram denominator tn increment trigram numerator tz needed increment trigram non zero counts curly brace move kneser ney bd increment bigram denominator bn increment bigram numerator bz needed increment bigram non zero counts curly brace move kneser ney ud increment unigram denominator un increment unigram numerator close train interpolated absolute discounting perl code code implementing interpolated kneser ney usage discount training test training data test data word line discount shift train shift test shift open train train line training data td increment trigram denominator tn increment trigram numerator tz needed increment trigram non zero counts bd increment bigram denominator bn increment bigram numerator bz needed increment bigram non zero counts ud increment unigram denominator un increment unigram numerator curly brace moved kneser ney curly brace moved kneser ney close train interpolated kneser ney perl code open test test line test data unigram un ud compute unigram probability defined bd non zero bigram denominator defined bn non zero bigram numerator bigram bn discount bd bigram bigram bigram bz discount bd unigram add trigram probability exists defined td non zero trigram denominator defined tn non zero trigram numerator trigram tn discount td trigram trigram trigram tz discount td bigram probability trigram probability bigram probability unigram print probability probability close test testing code interpolated absolute discounting kneser ney model discard parts model longer need find parameters second model 
results significant savings storage 
problem store need technique interacts poorly katz smoothing 
particular katz smoothing needs know discount parameters 
recall represents number grams occur times turing formula states gram occurs times pretend occurs disc times disc face computing requires knowing counts 
typically discounting done counts occur fewer times 
case need know fact need know ratio 
means sample counts estimate relative frequencies 
sampling counts mean sampling data mean sampling counts 
particular define random function sequence words simple hash function store count sequence hash mod picked way get counts 
save time need consider count saves huge amount space 
pilot experiments ect sampling accuracy negligible 
problem store need technique requires reading test data 
bug excellent perfect performance achieved 
important check bugs introduced 
done various ways 
instance added option program compute probability words just words test data 
verified sum probabilities words error 
verified probability correct word special score mode probability normal mode 
individual results report extraordinarily range typically reported techniques 
extraordinarily result comes combining technique total improvement total individual improvements implying reasonable range 
furthermore code rescore best lists reads lists figures counts needed score 
best lists indication correct output 
separate standard program scores output program correct transcription 
get improvement best rescoring improvement consistent perplexity reductions model model model ole oe ole ee eee class structure evidence implementation store need lead cheating 
system architecture turns language modeling ideal place object oriented methods inheritance 
particular language modeling tool virtual base class called model 
single important member function model provides probability word cluster context 
models sorts useful functions instance return set parameters optimized 
go test data determine counts need 
go training data accumulate counts 
base class construct sorts subclasses 
particular construct models store probabilities models contain models 
instance kneser ney smoothed models katz smoothed models simple interpolated models absolute discounting models 
models substructure instance absolute discounting backo models kneser ney backo models katz models share code interpolated kneser ney models interpolated absolute discounting models share code 
type model model containing models 
instance contains list models probability interpolates 
clustered models form xy model contains list models multiplies probabilities 
contains list models interpolates sentence level word level 
container type models similar behavior ways code shared 
part architecture providing set basic building blocks tools combining allowed di erent experiments performed 
parameter search key tool parameter search engine 
cases possible find em algorithms estimate various parameters models 
cases possible find theoretical approximate closed form solutions leaving discounting parameters 
large number di erent models approach lead endless coding complex model types suboptimal parameter settings 
powell method press search parameters models 
advantages 
parameters katz smoothing hard find general search procedure 
second meant optimize parameters jointly separately 
instance consider skipping model interpolates models 
traditionally optimize smoothing parameters model optimize smoothing parameters model find optimal interpolation interpolation models smoothing parameters 
find jointly optimal settings 
course gradient descent techniques parameters find probably locally optimal improvement traditional techniques 
certain tricks speed improve parameter search 
theory powell search find local minima run certain problems 
consider interpolating distributions 
imagine parameters optimized 
powell search routine optimizes 
awful set nearly 
eventually routine searches parameters finding essentially ect changing parameters quickly gives searching 
search interpolation parameters start distributions interpolated evenly 
ensures distribution parameters optimized 
furthermore searching parameters interpolated slow distributions evaluated 
typically search parameters separately potentially halving computation 
optimized optimize jointly nearly optimal search goes faster 
case distributions savings may small case may order magnitude 
technique care optimal settings distribution di er wildly settings combined 
particular sentence mixture models individual mixture component significantly trained jointly 
clustering shortage techniques generating clusters appears little evidence di erent techniques optimize criterion result significantly di erent quality clusters 
note di erent algorithms may require significantly di erent amounts run time 
techniques speed clustering significantly 
basic criterion followed minimize entropy 
particular assume model form want find placement words clusters minimizes entropy model 
typically done swapping words clusters swap reduces entropy 
important approach took speeding clustering top approach 
note agglomerative clustering algorithms merge words bottom may require significantly time topdown splitting algorithms 
basic algorithm top 
perform iterations swapping words clusters 
final swapping typically time consuming part algorithm 
technique buckshot cutting 
basic idea small number words estimate parameters cluster 
proceed top splitting clusters 
ready split cluster randomly pick words put random clusters swap way entropy decreased convergence decrease 
add words typically put best bucket swap convergence 
repeated words current cluster added split 
haven tested particularly thoroughly intuition lead large speedups 
important technique speeds computations adapted earlier brown 

attempt minimize entropy clusters 
represent words vocabulary represent potential cluster 
minimize wv log inner loop minimization considers adding removing word cluster new entropy 
face appear require computation proportional vocabulary size recompute sum 
letting new cluster called xv log xv xv log xv xv log summation equation computed relatively ciently time proportional number di erent words follow second summation needs transformed xv xv log xv wv log xv wv log log xv wv notice xv wv log wv log xv wv log xv wv xv wv substituting equations equation get xv xv log wv log xv wv log log xv wv notice wv log just old entropy adding assuming precomputed recorded value summations sum words xv cases smaller vocabulary size 
clustering techniques brown attempt maximize zp log clusters 
original speedup formula uses version complex minimize 
di erent clusters di erent positions leads marginally lower entropy leads simpler clustering 
smoothing smoothing algorithms reimplemented research details closely follow chen goodman 
includes additive smoothing unigram distribution katz smoothing kneser ney smoothing 
constant added unigram counts leads better performance small training data situations allowed compare perplexities di erent training sizes unigram received counts meaning probabilities returned 
katz smoothing maximum count discount data sparsity prevented estimates discount 
typically done corrected lower discounts total probability mass counts unchanged cuto included parameter added distribution counts discounted 
novel technique getting counts katz smoothing 
described appendix record counts needed experiments 
problematic katz smoothing need counts counts order compute discounts 
really need ratio counts 
statistical sampling technique randomly sample counts word histories actual counts allows accurately estimate needed ratios storage 

