variable length markov chains peter abraham wyner eth zurich switzerland university pennsylvania usa february study estimation class stationary variable length markov chains vlmc finite space 
processes class markovian higher order memory variable length yielding bigger structurally richer class models ordinary higher order markov chains 
algorithmic view vlmc model class attracted interest information theory machine learning statistical properties explored 
provided estimation available additional structural richness model class enhances predictive power finding better trade model bias variance allows better structural description specific interest 
exemplified dna data 
version tree structured context algorithm proposed rissanen information theoretical set shown new asymptotic properties estimation class vlmc underlying model increases dimensionality consistent estimation minimal state spaces mixing properties fitted models 
propose new bootstrap scheme fitted vlmc 
show validity quite general stationary categorical time series broad range statistical procedures 
ams subject classifications 
primary secondary key words phrases 
bootstrap categorical time series central limit theorem context algorithm data compression finite memory sources model kullback leibler distance model selection tree model 
short title variable length markov chain research supported part swiss national science foundation 
part done visiting university heidelberg germany 
research supported nsf dms 
general models stationary process assuming particular underlying mechanistic system full markov chain high finite order 
implicit assumption aside stationarity finite memory process 
consider exclusively case takes values finite categorical space refer stationary full markov chain order transition mechanism carries specific structure state space entire probabilistically nice model full markov chains aren appropriate estimation point view 
illustrate main problems specific momentarily take illustrative purposes cardinality jx fa tg letters dna string 
discussed problems apply finite space problem class finite order valued full markov chains structurally rich implying aren members class 
structural particularly implies kind parsimonious representation state space possible 
table additionally demonstrates structural terms dimension full markov chain models number free parameters function orders dim 
jx gamma jx cardinality jx 
dim 
delta models impossible fit model say parameters 
discontinuous increase dimensionality model allow trade bias low parameters variance low parameters predictor 
problem seen table curse dimensionality heavily fitting higher order models dimensionality increases exponentially order leading highly variable estimates 
practical example table problems apply full markov chain modeling dna sequences cf 

braun muller 
class models estimator study lead alternative purposes better statistical description dna sequences 
give section real data example field applications 
examples applications modeling potentially attractive precipitation analysis guttorp flood analysis brillinger analysis discrete directional data repeated patterns behavioral events raftery 
problems cured simple idea memory stationary markov chain allowed variable length function values past 
precisely time homogeneous transition probabilities ip jx gamma gamma gamma gamma functions depending variable number lagged values ip jx gamma gamma gamma gamma gamma gamma function past 
gamma gamma gamma gamma obtain full markov chain model order variable supf gamma gamma gamma gamma embedding markov chain order additional interpretable structure variable length memory structure implies transition probabil ities embedding markov chain lumped 
call process variable length markov chain vlmc 
closely related models information theory tree models models finite memory sources cf 
rissanen weinberger 
feder 

able choose data driven way appropriate member class vlmc lose gain comparison class ordinary full markov chains higher order 
give new results vlmc particular addressing problem select data driven way asymptotically correct member extremely large class vlmc 
merits results different areas 
theoretical level offer better understanding problems 
goal mind study vlmc estimation se interesting task 
practical merits results offer new insights vlmc apply generally problems involving categorical binary time series see examples mentioned 
offer advances statistical methodology specifically new bootstrap scheme vlmc categorical time series attractive alternative general blockwise bootstrap 
fitted vlmc excellent exploratory tool dynamics categorical time series 
accomplished representing structural dependencies graphically compactly demonstrate dna data section 
explorative information build specific parametric model second stage 
offer new insights information theory findings sharpen extend existing results vlmc compression rates see rissanen weinberger 

notion variable length memory markov chain particularly attractive long memory certain directions 
cases minimal state space drastically smaller embedding state space full markov chain having equivalent states lumped vlmc vlmc yields parsimonious parameterization state space 
difficulty attractive notion estimation minimal state space 
seen model selection problem 
due extremely large number vlmc sub models higher order markov chain global model selection techniques aic bic mdl 
estimation minimal state space probability distribution vlmc done tree structured scheme called context algorithm rissanen acts hierarchically local pairwise decisions 
weinberger 
proved consistency optimal compression rates context algorithm assumption true underlying process finite dimensional vlmc 
give entirely new consistency result true underlying model allowed grow dimensionality sample size increases 
growth rate probabilistic terms large log arbitrary 
describes better performance context algorithm increasing dimensionality consistency obvious 
important consequence result implies non trivial balance estimation true model 
loosely translated fact bias variance tradeoff possibly high dimensional problem handled context algorithm appropriate way 
allowing asymptotically infinite dimensional models new results contribute explore approximation general sufficiently nice stationary process esti mated vlmc 
consistency context algorithm estimating stationary processes minimal state spaces require pre specified model structure 
estimation context algorithm robust model 
general consistency result described propose novel resampling scheme vlmc bootstrap 
prove asymptotic validity vlmc bootstrap class estimators argue proving mixing property estimated vlmc scheme works general conditions 
vlmc bootstrap tailored categorical time series nice probabilistic interpretation enjoys advantage applicable simple plug rule efron original proposal independent case user friendly blockwise bootstrap 
results theory small simulation study conclude vlmc bootstrap new universal resampling tool categorical time series expected better blockwise bootstrap 
organized follows 
section give definition vlmc section describes process fitting models gives new asymptotic properties thereof 
section discuss vlmc bootstrap asymptotic validity results simulation study including comparison blockwise bootstrap 
proofs section 
variable length markov chains starting point consider stationary full markov chain finite order values finite categorical space sequel denote gamma gamma string components written reverse order wu jwj juj concatenation strings usually denote capital letters random variables small letters fixed deterministic values 
ip jx gamma gamma ip jx gammak gammak gamma note stationarity time indices gamma irrelevant replaced indices gamma gamma introduce idea variable length memory seen lumping irrelevant states history gammak formula 
values infinite history gamma variable relevant thought context achieve flexible model class ranging type sparse full markov chains length context depend actual values gamma formalize defining vlmc 
related models introduced information theory tree models models finite memory sources cf 
rissanen weinberger 

definition stationary process values jx 
denote variable projection function maps gamma 
gamma defined gamma minfk ip jx gamma gamma ip jx gammak gammak corresponds independence called context function gamma gamma called context variable name context refers portion past influences outcome 
definition implicitly reflects fact context length variable gamma gamma jc gamma gamma depending history gamma gamma gamma gamma projection structure context function context length jc determines vice versa 
definition stationary process values jx corresponding context function definition 
smallest integer jc gamma gamma gamma called context function order called stationary variable length markov chain vlmc order identify vlmc probability distribution sequel write probability measure ip xw 
transition probabilities denoted jc gamma coincide conditional probabilities jc gamma 
clearly vlmc order markov chain order having memory variable length 
context function order full projection gamma 
gammak gamma vlmc full markov chain order range space context function full space empty space 
class context functions length rich obtain broad class markov chains including special sparse types 
particular context functions yield substantial reduction number states compared full markov chain order context function 
phrases relate solutions problems mentioned section 
tree representation minimal state space requiring stationarity vlmc completely specified transition probabilities ip jx gamma gamma jc gamma gamma states determining transition probabilities values context function 
convenient represent states minimal state space vlmc tree 
consider trees root node top branches growing downwards internal node jx offsprings 
value context function represented branch terminal node tree 
context gamma represented branch sub branch top determined sub branch gamma terminal sub branch gamma gamma exemplify context trees complete internal node need exactly jx offsprings 
example jx 
function gamma gamma gamma arbitrary gamma gamma gamma gamma arbitrary gamma gamma gamma gamma arbitrary gamma gamma gamma arbitrary represented tree left hand side 
growing left sub branch represents symbol vice versa symbol 
definition context function stationary vlmc 
jx ary context tree terminal node context tree defined fw gamma gamma fw wu xg notion terminal context tree convenient formulating estimation procedure context tree minimal state space see section 
definition says terminal nodes tree representation considered elements terminal node context tree states need terminal nodes reconstruct context function context tree minimal state space vlmc refer elements branches nodes tree 
internal node jx offsprings implicitly adds complementary offspring lumping gamma non offsprings single new terminal node new represents single state example jx 
function gamma gamma gamma arbitrary gamma gamma arbitrary gamma gamma arbitrary gamma gamma gamma arbitrary gamma gamma gamma arbitrary represented tree right hand side 
rectangle usually don draw symbolizes absent nodes depth thought completion tree nodes lumped terms transition probabilities means xj 
terminal node context tree context tree 
state represented internal node tree element alternative representation state final complementary node indicated rectangle lumping non nodes new terminal node 
tree representations context functions examples 
semiparametric vlmc model sequences vlmc finite order vlmc consider semiparametric model spirit ritov bickel 
semiparametric vlmc model set stationary vlmc order fp stationary vlmc order kg member belongs nice parametric vlmc model order arbitrarily large 
study section consistent estimator semiparametric vlmc model additional structural information underlying context function 
specify particular model structure estimator robust model set vlmc dense set stationary processes respect finite dimensional weak convergence 
asymptotic analysis robust estimator framework underlying process changes sample size called moving truth model see 
reasons twofold 
interesting see estimation technique consistent situation non moving case considered weinberger 
asymptotic point view interesting problem finite high dimensional 
secondly moving truth model limiting elements boundary semiparametric model infinite dimensional non vlmc models 
sense moving truth model yields interesting approximation general stationary valued processes 
moving truth sequence vlmc data finite realizations triangular scheme finite realization context function transition probabilities corresponding denoted 
sequel writing data just usually think generating model 
context algorithm consistency data aim find underlying context function minimal state space estimate version context algorithm rissanen solve problem 
obvious uses numerical estimate probability distribution including resampling scheme section estimated context tree excellent exploratory tool dynamic structure underlying process see section 
context algorithm describe context algorithm aim mentioned 
main strategy follows 
large context tree grown represents overfitted vlmc model 
value space finite aren sophisticated problems finding accurate splits predictor space construction large tree turns simple computationally fast 
secondly algorithm employs backward tree pruning procedure considering local decision criterion 
basic level context algorithm similar architecture tree fitting methods 
sequel convention quantities involving time indices ng equal zero irrelevant 
jwj gamma denote number occurrences string sequence uw algorithm constructs estimated context tree biggest context tree respect order defined step delta wu log wu wu log jx cut chosen user 
step valued data fit maximal context tree search context function max terminal node context tree representation max see definition max biggest tree element terminal node max observed twice data 
formalized follows max max implies implies holds max means wu 
set max step examine element terminal node follows order examining irrelevant see 
corresponding context function wu gamma gamma gamma gamma wu element terminal node compare pruned version gamma pruned version empty branch root node 
prune wu gamma gamma delta wu log wu log jx defined 
decision pruning terminal node yields possibly smaller tree construct terminal node context tree step repeat step gamma gamma pruning possible 
denote maximal pruned context tree necessarily terminal node type corresponding context function 
step interested probability distributions estimate transition probabilities jc gamma jc gamma defined 

pruning decision step related kullback leibler distance likelihood ratio test 
definition delta wu log wu jj jw wu wu defined jjq log distance probability measures denote estimated likelihood function conditioned state context function jc gamma gamma order jc gamma gamma defined 
denote context function non pruned context tree context function sub tree pruned terminal node wu gamma parent node gamma multiplicative structure terms cancel likelihood ratio statistic remaining term node considered pruning 
gets delta wu log lower order gamma minor edge effects due conditioning variables arise 
formula says pruning criterion likelihood ratio test large acceptance region log pruned sub tree 
algorithm viewed doing likelihood ratio tests 

cut value log step pruning decision chosen asymptotic consideration 
clearly interpretation likelihood ratio tests small cut values result larger context trees overfitting occurs 
estimation cut value constant formulation aims optimality respect loss function specification allows tailoring procedure particular aims prediction error loss 
cut value interpreted stepwise gamma ff quantile multiple testing problem ff ff 
necessity ff converging zero explained rissanen 

matter terminal node wu step examined second tree order testing terminal nodes irrelevant 

pruning criterion delta wu need distance 
quantity jj jw replaced squared distance gamma jw definition see assumption 
case cut step context algorithm needs satisfy log jx 
see proof theorem mainly terms theorem 

maximal tree max step constructed basis occurrences terminal node sequence 
number low value practice guarantees sufficiently large initial tree observations estimate transition probabilities associated terminal nodes states max easy show assumptions section ip cn asymptotic properties algorithm remain unchanged replacing number finite number 

algorithm priori length restriction long contexts deep nodes tree log log jx employed weinberger 
severe restriction practical applications 
generally pruning context algorithm viewed hierarchical backward selection 
dependence values back history weaker considering deep nodes tree hierarchical way relevant 
hierarchic structure clear distinction cart algorithm breiman tree architecture binary built time structure 
consistency increasing dimensionality give results dealing consistency dimension underlying model allowed increase 
shows consistency finding minimal state spaces second describes properties estimated probability distributions 
consider sequence vlmc described context tree induced context function 
assumptions 
satisfies sup sup jp zn gamma zn gamma zn ip denotes step transition kernel state process gamma 
minw ffl kp gamma jw distance kfk jf 
gamma log gamma fi fi ffl gamma log gamma ffi nb ffi minimal transition probabilities satisfy min 
assumption transition kernels related ergodicity coefficient stationary markov processes cf 

implies state processes vlmc geometrically oe mixing mixing coefficients bounded sup oe gamma 
assumption minimum stationary probability bounds size context tree gamma log fi 
note number transition probability parameters process 
bound probabilistic terms weak condition number parameters explicit restriction order depth context tree 

distinguishing context wu parent node context tree assumption guarantees minimal distance relevant conditional distributions 
special case fixed vlmc context tree sufficient assume transition probabilities min implies assumptions ffl ffl 
theorem consider data denotes context function context tree process satisfying 
defined estimate step context algorithm 
lim ip equivalently lim ip ii sup gamma jc gamma gamma jc gamma 
proof theorem section 
explicit bounds events choosing large small minimal state spaces 
theorem explains context algorithm powerful tool 
dimensionality increases estimator chooses large small model asymptotically robust model respect sequences broad semiparametric class increase dimensionality underlying model restricted probabilistic terms allows growth fast log 
problem highly non trivial possible failure simple estimation rules consider models fixed increase dimensionality independent underlying process quite general order 
relates bias variance trade high dimensional vlmc general stationary processes boundary theorem describes solution model selection problem impossible deal global selection criterion due extremely large number possible models 
example number vlmc sub model full valued mc order jx delta selection criterion hierarchical local criterion context algorithm interestingly works case model dimension increases 
theory practically feasible minimum description length estimator yield consistent state estimation cf 
weinberger feder general class finite state models 
result describes construction properties estimator underlying probability measure define metric probability measures gammam dm ffi gamma ffi gamma dm ffi gamma ffi gamma sup jp gamma 
xm coordinate function 
theorem consider data satisfying 
estimate step context algorithm lim ip set jc gamma gamma generates unique stationary probability measure ii 
iii process satisfies ip oe mixing mixing coefficients satisfying oe pn gamma particular bound mixing coefficients oe pn non random proof theorem section 
statement theorem tells constructive way simulate estimated underlying process convergence ii minimal requirement reasonable estimator iii important simulation tasks bootstrapping complicated statistics 
dna example interesting instructive application vlmc estimation 
particular demonstrate usefulness estimated context tree excellent graphical tool detecting structure time series 
data consists distinct sequences dna drosophila genome 
point genetic data natural candidate modeling vlmc finite alphabet time index memory vanishes increasing lag time 
furthermore known data far independent higher order markov models easily fitted explosion number parameters cf 
braun muller compare problems section 
compounded observed degree non stationarity prohibits estimation long sequences 
environment paramount importance model parameters great economy order capture significant dependent structure 
data began single base contiguous stretch dna drosophila genome genbank number ds 
base possible dna residues adenine abbreviations respectively 
variety tools biologists gerry rubin lab university california berkeley segmented sequence genes code amino acid sequences non genes called junk dna ignored cell chemistry 
physically genes spaced apart separated junk dna term 
genes segmented coding regions called exons non coding regions called introns 
cell engine transcribing dna copies gene intron exon splices intron sections 
gene turn subdivided alternating stretches exon intron 
form single sequence exons concatenating exons order 
similarly form sequences introns inter genes 
ggg ttg tgt tgt ttt ttt ttt ttg ttt exon tree ttg ttt triplet tree representation estimated minimal state space exon sequence 
triplets denoted reverse order terminal node concatenation describes context gamma gamma gamma gamma gamma variable goal application vlmc estimation algorithm learn dependence structure estimated minimal state space graphically tree branches contexts 
application algorithm datasets suggests complicated structures exist exons introns 
hand inter genes showed complex structure order markov model fit 
exons exhibit structure surprising due constraints imposed coding function 
introns understood function evidence structure suggests intron constrained way unable freely mutate 
consider sequences reduction ary alphabet possible binary alphabets identifying equivalences genetic meaning third reducing data random bits 
expected final equivalence produces sequences dependence structure 
dramatic finding produced exon sequence reduced binary alphabet identifying base bonding pair identified 
resulting context tree branches lengths 
interestingly represent terms triplets shown 
amino acids known coded triplets dna letters structure beautiful biological interpretation 
finding suggests triplet coding structure strongly despite dramatic processing data actual code recoverable binary reduction 
point emphasis vlmc estimation algorithm learned triplet structure discovery reduced binary alphabet coding sequences 
vlmc bootstrap theorem indicates estimate resampling 
proposal bootstrap stationary categorical time series 
semiparametric model dense respect metric set stationary valued processes bootstrap general 
offers attractive accurate alternative model free blockwise bootstrap proposed 
proceed follows 
step valued data fit vlmc described section yielding stationary probability measure see theorem 
step draw finite realization ffi gamma variables called vlmc bootstrap sample random sample fitted vlmc 
practice choose starting values generate longer random sample estimated transition probabilities jc gamma elements bootstrap sample 
device tries avoid nonstationarity effects due starting values simulated markov chain 
course draw bootstrap samples size cf 
bickel 

estimator measurable function bootstrapped estimator defined plug rule 
plugin rule basis efron bootstrap independent case convenient practice 
bootstrap sample constructed bootstrapping done exactly computing tools programs original estimator case blockwise bootstrap estimator non symmetric observations estimators section 
quantities induced resampling step denoted asterisk 
consistency vlmc bootstrap increasing dimensionality asymptotic result justifies defined vlmc bootstrap estimators smooth functions means 
discuss informally vlmc bootstrap general framework empirical processes giving exact arguments 
assume observations sequence vlmc 
consider class estimators smooth functions means gf gamma gamma gammam gamma smooth function bounded jx 
examples include estimators transition probabilities finite state markov chains order gamma functions frequencies tuples size scores genetics cf 


usually assumption 
having continuous partial derivatives neighborhood 
exists gamma gamman cov gamma gamma positive definite 
assumption positive definiteness covariance matrices simplifies assuming limiting model lim metric defined 
generally vlmc anymore 
sufficient assume gamma cov gamma gamma vg gamma cov gamma gamma positive definite theorem justifies vlmc bootstrap smooth functions means 
theorem satisfying 
assume holds 
vlmc bootstrap defined section denote 
sup gamma gamma ip gamma proof theorem section 
results generalized consistency vlmc bootstrap general empirical processes due fact vlmc bootstrap categorical time series satisfies oe mixing property exponentially decaying mixing coefficients see theorem iii 
extensions useful studying bootstrap consistency estimators smooth functional general empirical measure class estimators considerably larger class 
includes examples maximum likelihood estimators generalized linear models autoregressive type quite general link functions cf 
fahrmeir 
simulations study vlmc bootstrap variance estimation various cases simulation 
represent vlmc models context trees equip terminal nodes tuples describing transition probabilities 
tuple jx gamma corresponds jjw jx gamma jx gamma loss generality jx gamma 
consider models 
full binary markov chain order full ary markov chain order semi sparse binary vlmc order ary vlmc order sparse binary vlmc order sparse ary vlmc order 
precise specifications trees numbers consider stationary nonlinear process exp gamma gamma gamma gamma gamma exp gamma gamma gamma sequence independent process known exponential ar 
quantized binary process non markovian valued markov order 
interesting see vlmc bootstrap provides finite sample approximation model vlmc 
interesting test case fair comparison vlmc blockwise bootstrap 
sample sizes choose 
consider statistics 
binary models 
frequency word defined ary models 
gamma relative frequency symbol binary model 
variance estimates oe nv ar oe nv ar oe ar oe ar oe oe gamma oe ar oe oe table vlmc bootstrap variance estimates sample size 
oe oe gamma oe ar oe oe table vlmc bootstrap variance estimates sample size 
oe oe gamma oe ar oe oe table blockwise bootstrap variance estimates sample size 
boxplots bootstrap variance estimates case top case bottom 
vlmc bootstrap estimates denoted ff quantiles cut values blockwise bootstrap estimates denoted line denotes true variance 
vlmc bootstrap resamples note different 
results table 
moments bootstrap variances oe estimated simulations different models true value oe estimated simulations 
relative mean square error oe gamma oe oe estimated standard errors bias parentheses 
tried different cut values act tuning parameter see 
report jx gamma ff quantiles different levels ff corresponding asymptotic distribution log likelihood ratio statistic intuitive numerical value cut log theory 
hand point danger direct interpretation cut jx gamma ff quantile ff fixed depending sample size contradicts essence context algorithm 
binary models cut offs ff ary models cut offs ff denoted short ff 
results promising relative mean square error smaller 
exceptions performance better sparse models 
indicates algorithm adapts sparseness exactly cases methods fail 
comparison tried blockwise bootstrap cases sample size different see table 
graphical representation 
comparison fair model vlmc structure quantized series doesn allow sparse approximation 
case bootstraps similar performance best tuned vlmc bootstrap better terms relative means error best tuned blockwise bootstrap 
case blockwise bootstrap exhibits serious bias large variability 
vlmc bootstrap far better sparse vlmc 
conclude vlmc bootstrap blockwise bootstrap enjoys important practical advantage defined plug rule see 
role cut tuning parameter vlmc bootstrap follows absolute value bias bootstrap variance estimator increases variance decreases increasing cut parameter 
expected larger cut parameter leads lower dimensional fitted vlmc model design context algorithm 
note simulation examples behavior significantly visible 
blocklength blockwise bootstrap turn general asymptotic behavior observed bias decreases variance increases growing proofs recall notation 
usually denote sequences written reverse time jwj concatenation wu jwj juj 
transition probabilities context tree indexed abbreviated pw jw estimated transition probabilities denoted pw xw defined 
abbreviate pw xw general necessarily context stationary probability measure ip 
recall defined delta jj 
looking sequence vlmc drop index proof theorem 
define events overestimation sample size exists wu wu exists tau wu wu note formula characterize terms pruning criterion delta wu log 
error event theorem assume fi ffi hold 
ip gamma log fi gamma log ffi constants 
proof partition underestimation event event fn ae ae constant chosen 
ip ip ip 
pursue bound ip bounding ip ip 
ip wu ip delta wu log wu ae wu aen ip jj pw log wu known cf 
cover thomas divergence lower bounded distance jj pw jj gamma pw jj jj gamma pw jj gamma pw fx pw ip jj pw log wu ip gamma pw log wu assumption pw far pw respectively 
formalize letting fl log pw pw goal establish ja gamma bj small jr gamma aj large js gamma bj large 
assume loss generality gamma ffl ja gamma bj fl implies ja gamma rj ffl gamma fl furthermore js gamma bj ffl gammas case jr gamma aj ffl gamma fl gamma gammas case js gamma bj ffl taken proved gamma pw fl gamma ffl gamma fl pw gamma pw ffl gamma fl 
applied proved ffl gamma fl ip jj pw log wu ip gamma wu ip pw gamma pw jx max ip gamma wu jx max ip pw gamma pw choose ae log fi max fl log follows assumption ffl min min ffl gamma fl const log ffi nb min ka const log ffi treat cases rhs simultaneously denoting wu respectively 

find upper bound probability event fjp gamma pj kg 
random number terms denominator apply large deviations bound directly 
consider extension infinite sequence define fthe time th occurrence symbol occurs th occurrence sequence stationary oe mixing sequence mixing coefficients bounded bound original sequence marginal probability distribution equal observe fj gamma pj kg fj gamma pj established upper bound ip jp gamma pj ip gamma pj point readily able apply exponential inequality 
lemma defined 
assume conditions fi ffi 
ae sup ip gamma pj gamma log ffi constant 
proof assumption process mixing coefficients oe gamma bound applies mixing coefficients process apply theorem ch 
log ffi note ae const log fi ka const log ffi ae completes proof 
denote exp gammaf log ffi 
straightforward application lemma equation proves ae max ip gamma wu max ip pw gamma pw jg ip jx wu aen assumption ip jn gamma exp log ffi log ffi complete proof theorem need bound ip 
union bound get ip ip ae ip gamma np ae gamma np ip gamma np gammab ip jn gamma bound quantity exponential inequality 
lemma assume fi ffi hold 
max ip jn gamma gamma log constant 
proof write gamma assumption oe mixing mixing coefficients bounded sup oe gamma cf 

apply theorem ch 
log fi nb const log fi completes proof 
lemma ip gamma gamma log const fi gamma log const fi estimate follows 
complete proof theorem 
consider overestimation event exists sequence element necessary delta log 
weinberger 
establish ip delta log jx gammac algorithm overestimation event occur string jwj log log establish ip jwj log log jx gammac jx gammac jx inequality follows jx distinct sequences length jwj possible prove stronger result eliminating need length restriction jwj 
just give outline proof 
lemma swv possible string swv swv delta swv log swv 
denote min min pw max maximal context tree step context algorithm 
assumptions ip swv min ip sw max gammac jx proof 
theorem assumptions ip log proof apply lemma swv ip swv ip swv gammac jx swv ip sw max estimate follows 
number sequences occur twice data swv ip sw max jx jie sw sw occurs twice jx jie jx jn jx complete proof 
defining pruning criterion step context algorithm terms distance sharpen theorem 
delta wu pw gamma jj define exists delta log theorem assumptions cut step context algorithm satisfying log jx ip log proof proof theorem jjq kp gamma qk delta delta proof lemma 
context su swv swv aim bound probability overestimation su 
recalling inequalities definitions weinberger 

fix sequence realization determine probability law su jx set sequences length defined follows log su jx sw js log log sw js defined formally weinberger 
sum log probability symbols occur context sw 
important observation sequence sw su probability probability 
define oe set sequences delta swv log follows weinberger 
oe su oe jx gammac point need introduce new probability distribution set sequences length closely related su sequence symbol occurs occurrence sw symbol immediately preceding occurrence sw 
occurs extended context swb define log jx log su jx log jsw gamma log jsw define log jx log su jx log jsw gamma log sw jx 
follows definition su jx min jx bound oe oe min gammac construction oe fact sw implies oe jx sw jx sw max furthermore jx distinct classes oe follows ip swv delta swv log min sw max gammac jx theorem imply assertion theorem 
assertion theorem ii follows theorem lines proof theorem partition set lemma 
proof theorem 
statements iii follow general formula statement ii immediate consequence theorem ii 
give assumption estimated process step transition kernel zn ip state process gamma vlmc characterized transition probabilities context function gamma zn gamma gamma cn wx gamma gamma gammai jc gammai gamma wx gamma note component 
process vlmc 
consider step transition kernel states ip pn vj gamma state process transition characterized vjw gamma obtain jt vjw gamma gamma vjw gamma jt vjw gamma gamma vjw gamma jt vjw gamma gamma vjw gamma jt vjw gamma gamma vjw gamma invoke true underlying process 
terms finiteness theorem 
obtain sup gamma sup jt vjw gamma gamma vjw gamma gamma consider sets sup gamma 
gamma constructed theorem uniquely determined stationary oe mixing mixing coefficients bounded oe pn gamma set cf 
lem 

theorem ip completes proof theorem iii 
proof theorem 
usually suppress index writing consider gamma gamma gammam gamma denote sigma cov covariance matrix lemma assume satisfying 
exists sigma positive definite 
ii sup sigma gamma gamma gamma ip proof process oe mixing mixing coefficients bounded sup oe gamma cf 
lem 

bounding covariances terms mixing coefficients cf 
bound implies vg gamma sigma gamma gamman cov gamma gamma gamma assertion follows assumption 
assertion assumption allow write sigma gamma gamma sup max jv gamma write sigma gamma gamma gamma gammam gamma gamma gamma gamma gamma 
construction cov gamma gammam gamma sup ek apply theorem gamma gammam gamma 
conditions version note corrigendum vol 
easily verified invoking mixing bound 
gamma gammam gamma assertion ii follows polya theorem 

smoothness assumption order taylor expansion gamma dg gamma dg gamma ku gamma lemma ii gamma dg gamma dg boundedness sigma lemma ii implies sup gamma gamma ip sigma dg 
going show bootstrap analog 
theorem iii bootstrap process high probability stationary geometrically oe mixing mixing coefficients denoted oe oe pn theorem iii 
note distribution depends sample size denote gamma gamma gammam gamma sigma cov covariance matrix respect bootstrap distribution 
lemma assume conditions theorem 
sigma gamma sigma 
ii lim ip sigma positive definite 
iii sup sigma gamma gamma gamma ip proof vg sigma gammam gamman cov gamma gamma gamma jkj gamma gammam cov gamma gamma gamma jkj gamma delta finite constant 
known bounds covariances terms mixing coefficients cf 
delta const oe theorem iii ip lim delta theorem ii max gamma ip boundedness finiteness imply gammam cov gamma gamma gamma gammam cov gamma gamma geometric oe mixing property see boundedness gammam cov gamma gamma gamma gamma cov gamma gamma shown assertion 
assertion ii follows lemma 
assertion iii proved lemma ii invoke mixing bound theorem iii 
finiteness jx gamma ip gamma ip continuous differentiability dg gamma dg gamma ku gamma order taylor expansion lemma iii boundedness sigma imply sup gamma gamma ip sigma dg 
complete proof theorem 
acknowledgments 
itai zukerman carrying computations 
acknowledge interesting general comments bickel 
olshen speed specific suggestions ferrari editor referees 
bickel van 

resampling fewer observations gains losses remedies losses 
statistica sinica 
breiman friedman olshen stone 


classification regression trees 
wadsworth 
brillinger 

trend analysis binary valued point cases 
stochastic hydrology 
braun muller 

statistical methods dna sequence 
statist 
sci 



model selection variable length markov chains tuning context algorithm 
appear ann 
inst 
statist 
math 
cover thomas 

elements information theory 
wiley 


mixing 
properties examples 
lect 
notes stat 

springer 
efron 

bootstrap methods look jackknife 
ann 
statist 

fahrmeir 

multivariate statistical modelling generalized linear models 
springer 
feder merhav gutman 

universal prediction individual sequences 
ieee trans 
inform 
theory 
guttorp 

stochastic modeling scientific data 
chapman hall 


random processes learning 
springer 


jackknife bootstrap general stationary observations 
ann 
statist 



finding words unexpected frequencies acid sequences 
roy 
statist 
soc 

raftery 

estimation modelling repeated patterns high order markov chains mixture transition distribution model 
appl 
statist 



bootstrap markov sequences estimates transition density 
ann 
inst 
statist 
math 

rissanen 

universal data compression system 
ieee trans 
inform 
theory 
rissanen 

complexity strings class markov sources 
ieee trans 
inform 
theory 
rissanen 

stochastic complexity statistical inquiry 
world scientific 
ritov bickel 

achieving information bounds non semiparametric models 
ann 
statist 

weinberger feder 

predictive stochastic complexity model estimation finite state processes 
statist 

infer 

weinberger lempel ziv 

sequential algorithm universal coding finite memory sources 
ieee trans 
inform 
theory 
weinberger rissanen feder 

universal finite memory source 
ieee trans 
inform 
theory 


central limit theorems dependent variables 
verw 
gebiete corr 
seminar fur statistik department statistics eth zurich university pennsylvania ch zurich philadelphia switzerland usa mail stat math ethz ch mail wharton upenn edu 
