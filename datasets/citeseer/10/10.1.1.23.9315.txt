kernel methods computer vision theory applications francesco dipartimento di informatica scienze dell informazione universita di genova italy di di xvi thesis supervisor prof alessandro verri dipartimento di informatica scienze dell informazione universita di genova italy contents general goals thesis kernel methods support vector machines 
optimal hyperplane algorithm 
kernel trick 
support vector machines construction 
gaussian processes 
regression gaussian processes 
justification thesis general aims 
motivation kernels computer vision 
motivation unsupervised kernel clustering 
motivation classification gaussian processes operative goals thesis kernel design image retrieval face recognition 
mercer kernel means 
kernel formulation trn 
kernel formulation lvq 
kernel mds 
kernel pca 
gaussian processes classification 
efficient implementation svm training 
preliminary results theoretical foundations kernel trick distances 
mercer kernel means 
plan design implementation mercer means 
design implementation mercer trn 
design implementation mercer lvq 
design implementation kernel mds 
design implementation kernel pca 
design implementation gaussian processes classification design implementation kernels image retrieval face recognition 
thesis drafting 
aim thesis study theoretically experimentally kernels methods emerging branch machine learning 
particular investigate algorithms useful resolution computer vision problems face recognition image retrieval 
framework goals thesis 
formulate unsupervised kernel algorithms 
particular develop algorithms clustering 

design kernel functions computer vision problems 

investigate gaussian processes kernel algorithm classification task 
chapter artificial intelligence relevant branch computer science 
artificial intelligence techniques neural networks important increasing number industrial applications 
particular neural networks massively optical character recognizers computer vision analysis financial markets bioinformatics 
mc pitts works neural networks 
rosenblatt proposed perceptron network units organized layers feed forward connections layer 
minsky papert showed perceptron solve elementary problems xor 
minsky papert criticism neural networks neglected rumelhart hinton williams proposed new neural network multi layer perceptron mlp neurons organized layers 
order mlp submitted learning process 
learning mlp performed means back propagation algorithm maximum likelihood principle 
mlp trained back propagation able solve complex problems including xor 
mlp success led big development research neural networks 
neural network models developed years 
currently neural networks solve different classes problems ffl classification ffl regression ffl probability density estimation probability density estimation consists estimating unknown probability density function pdf input knowing input samples methods try estimate explicitly function try reproduce pdf estimating explicitly 
case belong neural models aim cluster categorize input data 
classes network net self organize correlations input data 
neural models self organize called unsupervised 
addition clustering algorithms unsupervised models algorithms aim represent high dimensionality data low dimension manifolds trying preserve original information data 
techniques called mds algorithms useful deal high dimensionality problems computer vision ones 
classification problems task assign new inputs number discrete classes categories 
typical classification problem assign character bitmap correct letter alphabet 
tasks shall refer regression problems outputs represent values continuous variables prediction stock exchange index time 
classification regression problems particular cases function approximation 
neural networks order solve classification regression problem trained necessary give neural net set samples function approximate training set 
neural networks kind called supervised 
says nets supervised learning learning teacher 
order verify performance classifier regressor set sample previously seen provided net performance expressed terms error expressed loss function mean square error measures classifier far desired behaviour 
vladimir vapnik statistic theory learning put connection error expected risk classifier test set error empirical risk training set 
expected ex empirical em risk connected relation ex em es es estimation error 
vapnik proved es log cardinality training set vapnik chervonenkis vc dimension classifier 
roughly speaking vc dimension samples classifier classify correctly rate negligible expected risk differs notably empirical risk 
neural networks trained maximum likelihood principle tries minimize empirical risk care vc dimension 
happens classifier small empirical risk training set performs poorly test set 
corinna cortes vladimir vapnik proposed new neural network called support vector machines neural networks maximum likelihood principle try minimize expected risk 
possible vc dimension controlled training 
learning consists solving problem quadratic programming 
distinctive feature support vector machines kernel trick perform inner product 
inner product performed particular functions called mercer kernels 
support vector machines belongs wide class neural models called kernel methods algorithms kernels order perform computations 
investigate kernels methods phd thesis 
particular consider algorithms kernel methods relevant solution computer vision problems supervised algorithms classification unsupervised algorithms clustering multidimensional scaling consider computer vision problems relevant applicative interest intelligent image retrieval human face recognition 
problems undoubtedly kernels methods characterized high dimensionality sparseness data input space 
computer vision problems represent severe test show kernel methods robustness abovementioned topics 
thesis proposal organized follows chapter general goals thesis declared chapter kernel methods reviewed relevance justification thesis goals chapter described operative goals thesis chapter preliminary results chapter plan thesis shown 
chapter general goals thesis thesis investigate theoretically experimentally kernel methods resolution computer vision problems 
kernel methods particular support vectors machines applied successfully computer vision 
big efforts thesis spent making robust kernel methods robust critical characteristics computer vision high dimensionality input space 
general goals thesis summed points ffl studying theoretically experimentally kernels computer vision applications intelligent image retrieval human face recognition ffl developing unsupervised kernel methods 
particular study methods kernels clustering multidimensional scaling mds ffl investigating theoretically experimentally gaussian processes classification problems 
chapter kernel methods kernel methods relevant ones support vector machines gaussian processes 
support vector machines section describe brevity sake svm classification 
treatment svm regression quite similar 
problem learning classify patterns consists estimating function sigma input output training data theta sigma correctly classify unseen examples examples generated unknown probability distribution training data 
usually assumption couples identically independent distributed 
order get reliable classifiers necessary anticipated capacity expressed terms vc dimension controlled 
underlying idea svm optimal hyperplane algorithm optimal hyperplane algorithm vapnik lerner vapnik chervonenkis considered class hyperplanes delta corresponding decision functions sgn delta proposed learning algorithm generalized portrait separable problems computed empirical data 
shown abovementioned concept 
vapnik observed hyperplanes separating data exists unique yielding maximum margin separation classes max min kx gamma delta vc dimension classifier decreases increasing margin 
order construct optimal hyperplane solves optimization problem minimize kwk delta delta constrained optimization problem dealt introducing lagrange multipliers ff lagrangian ff kwk gamma ff delta gamma lagrangian minimized respect primal variables maximized respect dual variables ff saddle point 
optimization problem solved means kuhn tucker theorem 
kuhn tucker theorem generalization lagrange multiplier methods allows optimize problems constraints inequalities 
condition saddle point derivatives respect primal variables vanish ff ff leads ff ff solution vector expansion terms subset training patterns patterns ff equal 
patterns called support vectors sv 
kuhn tucker theorem implies ff satisfy called karush kuhn tucker complementarity conditions ff delta delta gamma condition implies support vectors lie margin 
remaining samples training set irrelevant optimization ff null 
implies hyperplane completely determined patterns closest solution depend pattern training set 
written ff sv ff plugging eliminates primal variables optimization problem maximize ff ff gamma ff ff delta ff ff hyperplane decision function written sgn ff delta delta optimal hyperplane algorithm described solve linear problem 
fit solve simple problems xor underlined minsky papert 
order build classifier cope nonlinear problems necessary find method compute dot products feature spaces nonlinearly related input space 
method consists kernels kernel trick perform scalar products kernel trick idea kernel trick map data scalar product space feature space means nonlinear map phi perform linear algorithm cases mapping phi computed due high dimensionality obstacle decision function require evaluation scalar products phi delta phi pattern phi explicit form 
scalar products evaluated simple kernel phi delta phi example polynomial kernel delta shown corresponds map phi space spanned products exactly dimensions 
delta take account products order generally result allows discover kernels give rise maps phi holds theorem mercer continuous symmetric kernel positive integral operator 
tf dx thetac dxdy compact subsets expanded uniformly convergent series theta terms eigenfunctions positive eigenvalues nf nf number positive eigenvalues 
mercer proved originally theorem schwartz extended theorem general compact spaces 
equivalent way characterize mercer kernel finite case 
theta kernel function simply kernel 
definition nonempty set 
function theta called positive definite kernel mercer kernel xn easy show previous definition mercer kernels give rise positive definite matrices ij 
recall positive definite matrice property eigenvalues nonnegative 
positive definite functions shared inner products properties support vector machines construction order construct support vector machines computes optimal hyperplane feature space 
necessary substitute phi training examples training examples weight vector expansion feature space correspond image single vector input space 
patterns occur scalar products substitute mercer kernels scalar products leading decision function general form sgn ff delta phi delta phi sgn ff delta quadratic problem optimize maximize ff ff gamma ff ff ff ff practical problems separating may exist 
due presence noise happens exists overlapping classes 
necessary allow possibility examples violate 
order get necessary slack variables relaxing constraints delta delta gamma constructed classifier support vector machine control time capacity kwk number training errors minimizing objective function kwk subject constraints value constant determining trade plugging constraints rewriting terms lagrange multipliers obtain problem maximize maximize ff ff gamma ff ff ff ff difference separable case upper bound lagrange multipliers ff separable case decision assumes form threshold computed exploiting fact svs ff slack variable zero ff delta gaussian processes gaussian processes emerging branch kernel methods 
svm designed solve mainly classification problems gaussian processes designed solve essentially regression problems 
gaussian processes novelty 
field proposed framework regression optimal linear estimator called kriging honour south african mining engineer 
framework identical gaussian processes currently machine learning 
kriging developed considerably years gaussian process model concentrated mainly low dimensional problems problems machine learning community ignored completely gaussian processes neal proposed 
neal argued reason believe real problems neural networks limited nets containing small number hidden nodes 
neural network model huge number nodes trained backpropagation algorithm maximum likelihood algorithm neural net huge number node tends overfit data 
neal investigated net behavior number hidden nodes goes infinity showed get performances bayesian learning maximum likelihood strategy 
bayesian approach neural networks prior distribution weights induces prior distribution functions 
prior combined noise model specifies probability observing targets function values yield posterior functions predictions 
neal proved large class neural network models multi layer perceptron converge gaussian process prior functions limit infinite number hidden units 
infinite networks method creating gaussian process possible specify directly parametric forms mean covariance functions 
advantage gaussian process formulation comparison infinite networks integrations approximated neural nets carried exactly matrix computations 
section described regression means gaussian processes 
regression gaussian processes stochastic process collection random variables fy jx xg indexed set stochastic process specified giving probability distribution finite subsets variables consistent manner 
gaussian process stochastic process fully specified mean function covariance function gamma gamma joint multivariate gaussian distribution 
section considered gaussian processes 
case neural networks priors assumes known offset trend data removed 
non zero incorporated framework leads extra notational complexity 
prior covariance function noise process cn cn data gamma prediction distribution corresponding test point obtained simply marginalizing dimensional joint distribution obtain mean variance kp kn gamma oe cp cn gamma kp kn gamma kp kp ij cp kn ij cn kp cp cp xn gamma oe gives measure error prediction yields 
assume noise process variance oe depend sample kn oe substituting previous equations kp oe gamma oe cp cn gamma kp oe gamma kp prediction value possible obtain svm regression optimization performed norm 
big difference gaussian processes gp svm gp provides svm estimate error prediction 
peculiarity gp appealing problems necessary get prediction measure prediction 
examples portfolio management 
attempts gaussian processes classifications problem opened 
justification thesis general aims section provide justification relevance general aims thesis 
abovementioned developing specific kernels computer vision investigate unsupervised kernel models evaluate application gaussian process classification 
provide justification single activity subsections 
motivation kernels computer vision years massive different applications support vector machines shown performance strongly influenced choice kernel 
choice kernel fundamental svm general kernel method 
kernel method application necessary activity study design kernels kernel engineering 
authors proposed kernels resolution specific problems protein homology detection text categorisation image classification 
specific kernels hausdorff distance images computer vision problems 
research activity kernel design proposed thesis focusing intelligent image retrieval face classification 
relevant activity kernel engineering prove designed kernel satisfies mercer theorem assures kernel kernels machines 
mathematical methods allow decide kernel fulfills mercer theorem 
say kernel fulfills mercer theorem means show example show delta ff ff mercer kernel 
delta ff delta ff delta ff cases prove kernel positive definite trivial 
propositions useful criteria allows establish kernel positive definite 
theorem gamma continuous positive definite function exists bounded nondecreasing function gamma fourier transform gamma gamma gammay dv function gamma satisfies condition positive definite 
theorem schoenberg call function completely monotonic provided satisfies condition gamma function kx gamma yk positive definite kx gamma yk continuous completely monotonic 
theorem polya real continuous function convex satisfies ffu gamma ff fff gamma ff ff positive definite 
basis theorems construct different positive definitions type gamma 
notwithstanding class mercer kernels quite restrictive limits possibility kernel engineering 
necessary enlarge class functions kernels methods 
define positive definite kernels 
definition call kernel positive definite kernel order symmetric kx gamma xn polynomials degree lower positive definite kernels considered positive kernels order 
examples positive kernels gamma kx gamma yk hardy multiquadric kx gamma yk ln kx gamma yk thin late spline pointed girosi conditionally positive definite kernels admissible svm general kernel methods 
underlined result 
theorem define space polynomials degree lower generates admissible kernel sv expansions space functions orthogonal setting kx gamma yk 
proof shown positive definite kernels generate semi norms kfk kx gamma yk dxdy provided projection space polynomials degree lower zero 
kfk second side mercer condition kx gamma yk obviously 
mercer condition fulfilled kx gamma yk defines scalar product feature space 
svm kernels 
results enlarges remarkably class functions kernel machines 
easiest way yield new mercer kernel existing mercer kernels 
result shows mercer kernels satisfy quite number closure properties 
theorem mercer kernels theta real valued function oe mercer kernel theta functions mercer kernels 

ak 


oe oe proof theorem 
previous results allows complicated kernels starting mercer kernels 
useful result corollary derived theorem 
corollary mercer kernel theta polynomial positive coefficients 
functions kernels 

exp 
exp gamma zk previous proposition allows kernels existing mercer kernel building blocks way obtain mercer kernel start features obtain inner product 
case need verify mercer condition constructed kernel inner product mercer kernel definition 
kernels devoted resolution specific problem constructed way 
kernels constructed features text categorization image classification 
motivation unsupervised kernel clustering support vector machines kernel trick inner product performed mercer kernels euclidean spaces immediate inner product define distance scholkopf smola muller proposed compute distances mercer kernels 
motivation unsupervised kernels methods fact unsupervised algorithms particular clustering techniques distances 
generalized terms mercer kernels 
authors proposed mercer kernels clustering possibility generalize clustering algorithms terms mercer kernels investigated 
chapter show generalized terms mercer kernels famous clustering algorithm kmeans 
opens possibility formulate clustering algorithms terms mercer kernels 
algorithms learning vector quantization topology representing networks quite fit formulated terms mercer kernels 
applied abovementioned algorithms cursive character recognition 
opinion applied successfully computer vision problems 
unsupervised method class reformulated terms kernel methods mds algorithms 
particular sammon mapping local version algorithm generalized terms mercer kernels computations euclidean distances 
mds algorithms particular relevant computer vision problems 
computer vision problems characterized input high dimensionality mds algorithms reduce dimensionality retaining data information useful preprocessing stage 
fact classifiers svm robust curse dimensionality classifier reliability hard input dimension high 
motivation classification gaussian processes comparison kernel methods svm gaussian processes regression problem appealing main reasons 
reason gaussian processes exact computations matrix calculus svm minimization problem solved approximating problems 
case regression provides value prediction variance prediction providing way reliability degree prediction 
gaussian processes classification quite interesting classifier provide analogously regression problems measure classification reliability 
peculiarity important industrial requires decrease low possible errors 
notwithstanding gaussian processes classification remain open problem interesting topic kernel methods 
thesis investigate gaussian processes classification comparing support vector machines 
binary classification problem separate circles disks 
optimal hyperplane orthogonal shortest line connecting convex hulls classes intersects half way classes chapter operative goals thesis set operative goals thesis represents intermediate specification general aims thesis implementation means specific projects 
list operative goals considered maximal years relevant results fields quoted reached 
minimal subsets operative goals formed points kernel design image retrieval face recognition performances kernel methods influenced choice kernels 
goal thesis develop specific kernels face recognition image retrieval proposed kernel computer vision problem kernel hadamard distance images 
weak point kernel strong dependence outliers 
kernel fit cope noisy images 
goal develop robust kernels applicative computer vision abovementioned 
mercer kernel means means popular algorithm clustering 
computation euclidean distances 
generalized terms mercer kernels chapter reported model called mercer kernel means 
notwithstanding theoretical problems opened 
necessary investigate theoretical properties kmeans hold mercer kmeans 
means convergent algorithm optimal case data gaussian distribution 
interesting investigate properties hold mercer kernel means kernel formulation trn topology preserving network trn unsupervised algorithm proposed martinetz 
appeal trn theoretical result 
martinetz proved trn preserves optimally preserves topology data 
trn give rough estimate intrinsics dimensionality data 
trn generalized terms mercer kernels uses computation euclidean distances 
far know trn generalization terms mercer kernels represent novelty neural network literature 
generalization trn terms mercer kernels behaves interesting theoretical problem show trn generalized mercer kernels preserves topology data 
kernel formulation lvq learning vector quantization lvq supervised version vector quantization 
recall vector quantization scope represent data set vector dimensionality data cardinality lower cardinality data set 
unsupervised vector algorithms clustering compression lvq generally classification tasks 
applicative tasks character recognition compared successfully neural models 
lvq generates codevector produce near optimal decision boundaries 
lvq rule deducted bayes theorem 
lvq algorithm uses distances generalized terms mercer kernels 
interesting investigate lvq rule deducted bayes rule lvq generalized terms mercer kernels kernel mds multi dimensional scaling mds technique dimensionality reduction consists projecting data set submanifold trying preserve distance data 
example technique sammon mapping 
sammon mapping defines function stress couple points difference distances points computed original data set projected manifold 
sammon mapping tries minimize cumulative stress computed couple points data set generally gradient descent techniques 
contrary isomap algorithm proposed performs locally mds 
fact consider point nearest neighbours 
course cardinality data set 
sammon mapping isomap euclidean distances generalized terms mercer kernels 
provide information dimension submanifold project data 
techniques estimate intrinsics dimension data described useful order set dimension 
kernel pca principal component analysis pca statistics technique input dimensionality reduction 
consists projecting data directions maximal variance data eigenvectors covariance matrix 
input dimensionality consider directions number generally set basis empirical considerations magnitude eigenvalues covariance matrix 
pca linear algorithm tends overestimate dimensionality data 
example data arranged curve pca provides input dimension equal 
scholkopf proposed new algorithm kernel pca 
kernel pca implies created square matrix rank equal cardinality dta set generic element ij samples data set mercer kernel 
pca kernel pca considered algorithm dimensionality reduction 
fact cardinality th training set higher input dimensionality possible get dimensionality reduction 
contrary kernel pca considered algorithm denoising currently kernel pca applied toy problem interesting test algorithm computer vision problems 
application kernel pca data set samples creates problem numerical analysis fact compute eigenvectors covariance matrix rank greater unfeasible classical algorithms gauss jordan 
problem substituting gauss jordan efficient algorithms remains open 
possible evolution kernel pca kernel ica 
independent component analysis ica technique represents data independent components independent probabilistic sense 
pca represents data decorrelated components 
important decorrelated weak version independent 
independent component decorrelated viceversa hold 
fact possible example component decorrelated independent probabilistic sense 
ica successfully applied solve cocktail problem separate unknown sources information unknown way knowing final result combination cocktail 
idea investigate kernel transformation ica pca proposed 
opinion kernel ica efficient denoising kernel pca gaussian processes classification gaussian processes kernel models originally designed regression 
attempts gaussian processes classification 
issues gaussian processes classification described 
assume data set consists inputs xn corresponding targets indicator variables 
goal model data bayesian conditional classifier predicts conditional assume existence function models logit log jx jx function jx exp gammaa complete model place distribution unknown function 
gaussian process approach model directly gaussian process 
involves modelling joint distribution xn gaussian distribution 
theta gp exp gamma gamma gp normalization constant cn appropriate positive definite covariance matrix function inputs xn set hyperparameters theta xn interesting line research compare gaussian processes svm 
svm gaussian processes margin control classifier complexity 
interesting test gaussian processes classification task show generalization capabilities svm ones 
aim computer vision problems represent valid test bench 
efficient implementation svm training svm involves solution quadratic programming problem implies squared matrices dimension cardinality training set 
set 
quite problematic svm solution require training set thousands samples 
research svm years devoted solve efficiently quadratic problem particular large training sets 
years developed method chunking decomposition approach 
method optimization performed smaller subset training set working set optimization stage retained part working set support vectors 
part part training set processed forms new working set optimization repeated 
process lasts samples training set examined 
procedure completely satisfactory widely svm community svm simulators developed 
procedure greedy strategy assured optimal 
decomposition training set subsets carried means heuristic considerations 
alternative strategy making decompositions clustering strategies 
quite interesting test decomposition strategies influences svm performances terms classification rate 
chapter preliminary results preliminary result obtained concern unsupervised kernel models 
investigated theoretical foundations kernel trick distances showing distance computation mercer kernels theory negative definite kernels 
theoretical foundations shown section formulated algorithm means terms mercer kernels 
described section theoretical foundations kernel trick distances order justify possible compute distance mercer kernel necessary introduce concept negative definite kernel 
definition call kernel negative definite kernel symmetric xn important property negative definite kernel illustrated result 
lemma theta negative definite satisfies ff ff 
proof lemma reported 
positive negative definite kernels strictly connected 
positive definite gammak negative definite 
contrary negative definite gammak positive definite kernel order positive negative definite functions related lemma 
lemma nonempty set theta symmetric kernel put gamma gamma positive definite negative definite 
gamma positive definite negative definite 
proof lemma omitted brevity sake reported 
proposition relation negative definite kernels distances 
lemma distance ae kernel negative definite 
proof choose triangular inequality ae ae ae ae ae ae ae ae ae ae ae ae definition terms right side inequality null 
get ae far know previous result reported literature 
distance negative definite contrary negative kernels distances 
example consider kernel gammag gaussian kernel 
negative definite distance axiom violated 
summing set distances subset set negative definite kernels 
kernel dependent possible express kernel terms inner product 
axioms valid equation hold 
possible associate dependent kernels distance 
order show consider associated dependent kernel kernel gamma definition motivated result 
corollary definite positive definite negative 
negative definite 
proof gamma gamma definition gamma definite positive 
order show negative definite observe fact cauchy schwarz inequality gamma gamma gamma lemma negative definite 
introduce result due schoenberg 
theorem nonempty set theta negative definite 
hilbert space mapping 
gamma function nonnegative metric proof fix define gamma gamma positive definite lemma 
associated hilbert space put 
gamma gamma gamma setting gamma statements derived immediately 
immediate consequence previous theorem corollary result 
corollary definite positive kernel 
kernel ae ae gamma distance 
proof corollary kernel gamma negative definite 
applying theorem get ae distance 
possible compute distance mercer kernel 
section shows clustering algorithms reformulated terms mercer kernels 
mercer kernel means section shown clustering algorithm represented terms mercer kernels 
order show consider means version line data set goal means optimal vector quantization data set vector quantization means represent set set called codebook elements called codevectors call optimal codebook codebook minimizes quantization error kx gamma minimize gradient respect get gamma gamma equation motivates means algorithm steps 
initialize codebook vectors chosen randomly input probability distribution 
take random input 
determine element winner closer argmin kx gamma 
adapt input vector deltaw gamma 
elements unchanged 
go step maximum number steps reached 
parameter called learning rate usually decreases time order algorithm convergent 
pass describe means generalized kernel trick compute distances 
kernel trick equation gamma mercer kernel 
particular case equation usual inner products vectors 
analogous way equation generalized gamma gamma formulate means algorithm terms mercer kernel mercer kernel means 
initialize codebook vectors chosen randomly input probability distribution 
take random input 
determine element winner closer argmin gamma 
adapt input vector deltaw gamma 
elements unchanged 
go step maximum number steps reached 
case particular class kernel independent kernel learning rule assumes simpler form say kernel independent represented form case independent kernel learning rule deltaw theta gamma easy verify kernel scalar product vectors equation turns 
immediate test mercer kernel means experimental data 
chapter plan chapter shows projects thesis goals 
projects describes briefly required activities get operative goals described chapter 
activity provided estimate effort expressed man month expected dates kick kick activity 
splitting plan collection projects tries decompose research order get intermediate results milestones plan determinate times develop alternative research paths 
project indicates operative goal project related 
presentation order different project reflects fulfilment priority 
phases thesis overlapping different activities 
expected fulfillment times indicated projects include time necessary dissemination writing papers conferences journals related results 
design implementation mercer kmeans operative goals kernel formulations means project phases 
design algorithm 
theoretical study algorithm property 
software implementation model 
experimentation algorithm public databases face database cbcl mit 
performance comparison algorithms mlp svm starting date january closing date april effort man months design implementation mercer trn operative goals kernel formulations trn project phases 
design algorithm 
theoretical study algorithm property 
software implementation model 
experimentation algorithm public databases face database cbcl mit 
performance comparison algorithms mlp svm starting date february closing date may effort man months design implementation mercer lvq operative goals kernel formulations lvq project phases 
design algorithm 
theoretical study algorithm property 
software implementation model 
experimentation algorithm public databases face database cbcl mit 
performance comparison algorithms mlp svm starting date may closing date june effort man months design implementation kernel mds operative goals kernel mds project phases 
design algorithm 
theoretical study algorithm property 
software implementation model 
comparison algorithms pca sammon mapping starting date july closing date october effort man months design implementation kernel pca operative goals kernel pca project phases 
software implementation model 
experimentation public databases starting date september closing date november effort man months design implementation gaussian processes classification operative goals gaussian processes classification project phases 
design algorithm 
theoretical study algorithm property 
software implementation model 
experimentation public databases 
comparison algorithms svm mlp starting date november closing date march effort man months design implementation kernels image retrieval face recognition operative goals kernel design image retrieval face recognition efficient implementation svm training project phases 
theoretical study kernels 
software implementation kernels 
efficient implementation svm training 
experimentation public databases phase considered databases human face images databases web images produced esprit project 
comparison algorithms svm mlp starting date march closing date august effort man months thesis drafting phase concludes thesis 
activity thesis drafting start expect concentrate months 
starting date august closing date december effort man months shown gantt plan 
jan apr aug dec apr aug dec gantt plan 
stands activity 
bibliography aizerman braverman theoretical foundations potential function method pattern recognition learning automation remote control pp 
bellman adaptive control processes guided tour princeton university press princeton nj berg christensen harmonic analysis semigroups theory positive definite related functions springer bishop neural networks pattern recognition oxford university press burges tutorial support vector machines pattern recognition networks data mining knowledge discovery pp 
cursive recognition learning vector quantization pattern recognition letters may pp 
elsevier intrinsic dimension estimation data approach grassberger procaccia algorithm neural processing letters august pp 
kluwer neural cursive character recognizer eds 
proc 
th workshop italiano press comon independent component analysis new concept signal processing elsevier cristianini taylor support vector machines cambridge academic press cortes vapnik support vector networks machine learning pp cox cox multidimensional scaling chapman hall london cressie statistics spatial data wiley devroye lugosi probabilistic theory pattern recognition springer dyn interpolation approximation radial related functions chui schumaker ward eds 
approximation theory vi pp 
academic press new york 
schwarz linear operators part ii spectral theory self adjoint operators hilbert spaces pure applied mathematics john wiley sons new york firenze morasso application topological representing networks estimation intrinsic dimensionality data proc 
icann october paris france fritzke competitive learning methods tech 
report bochum girosi jones poggio priors stabilizers basis functions regularization radial tensor additive splines memo mit haussler convolution kernels discrete structures tech 
report ucsc crl hertz krogh palmer theory neural computation addison wesley huttenlocher rucklidge comparing images distances ieee trans 
pattern analysis machine intelligence pp 
jain dubes algorithms clustering data prentice hall haussler exploiting generative models discriminative classifiers kearns solla cohn eds advances neural information processing systems mit press joachim making large scale support vector machine learning practical scholkopf burges smola eds 
advances kernel methods pp mit press principal component analysis springer verlag new york kohonen self organizing maps springer berlin kuhn tucker nonlinear programming proc 
nd berkeley symposium mathematical statistics pages berkeley university california press linde gray algorithm vector quantization design ieee trans 
communication com squares quantization pcm technical note bell laboratories published ieee transaction information theory mackay practical bayesian framework backpropagation networks neural computation pp 
gibbs mackay variational gaussian process classifiers tech 
report 
cavendish laboratory university cambridge mac queen methods classifications analysis multivariate observations proc 
fifth berkeley symposium mathematical statistics probability vol 
pp 
berkeley university california press nelson multivariate interpolation conditionally positive definite functions mathematics computation pp 

martinetz schulten topology representing networks neural networks pp 

principles economic geology pp 

mcculloch pitts logical calculus ideas nervous activity bulletin biophysics pp 
mercer functions positive negative type connection theory integral equations philos 
trans 
royal soc 
london pp 
micchelli interpolation scattered data distance matrices conditionally positive definite constructive approximation pp 
minsky papert perceptrons mit press neal bayesian learning neural networks springer verlag trucco verri general purpose matching matching gray level images di eds 
proc 
th workshop visual forms lecture notes computer science lncs pp 
springer verlag osuna freund girosi improved training algorithm support vector machines principe giles morgan wilson eds 
neural networks signal processing vii proceedings ieee workshop pp 
new york ieee press poggio trainable pedestrian detection mit research report platt fast training support vector machines sequential minimal optimization scholkopf burges smola eds 
advances kernel methods pp mit press pontil verri object recognition support vector machines ieee trans 
pami pp 
rosenblatt principles neurodynamics perceptrons theory brain mechanism spartan rumelhart hinton williams learning representations back propagating errors nature pp 
rumelhart hinton williams learning internal representations error propagation parallel distributed processing vol 
chap mit press schoenberg metric spaces completely monotone functions ann 
math 
schoenberg metric spaces positive definite functions trans 
amer 
math 
soc 
scholkopf smola muller nonlinear component analysis kernel eigenvalue problem neural computation pp 
scholkopf kernel trick distances microsoft research report de silva langford global geometric framework nonlinear dimensionality reduction science pp vapnik lerner pattern recognition generalized portrait method automation remote control vapnik ya 
chervonenkis note class perceptron automation remote control vapnik ya 
chervonenkis uniform convergence relative frequencies events probabilities theory probab 
appl 
pp 
vapnik estimation dependencies empirical data nauka vapnik nature statistical learning theory springer verlag vapnik statistical learning theory john wiley sons williams barber bayesian classification gaussian processes ieee trans 
pattern analysis machine intelligence pp 
williams gaussian processes regression touretzky mozer hasselmo eds 
advances neural information processing systems mit press 
