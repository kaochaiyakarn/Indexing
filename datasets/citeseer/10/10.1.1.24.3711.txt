spectral kernel methods clustering nello cristianini technologies nello support vector net john shawe taylor kandola royal holloway university london cs rhul ac uk introduce new algorithms unsupervised learning kernel matrix 
information required algorithms contained eigenvectors matrix closely related matrices 
di erent related cost functions alignment cut cost 
rst discussed companion second graph theoretic concepts :10.1.1.23.6757
functions measure level clustering labeled dataset correlation data clusters labels 
state problem unsupervised learning assigning labels optimize cost functions 
show optimal solution approximated slightly relaxing corresponding optimization problem corresponds eigenvector information 
resulting simple algorithms tested real world data positive results 
kernel learning provides modular approach learning system design 
general algorithm selected appropriate task mapped particular application choice problem speci kernel function 
kernel method works mapping data high dimensional feature space implicitly de ned choice kernel function 
kernel function computes inner product images inputs feature space 
practitioners viewpoint function regarded similarity measure provides natural way incorporating domain knowledge problem bias system 
important learning problem dividing data classes cost function relative positions feature space 
think clustering kernel de ned feature space non linear clustering input space 
introduce novel kernel methods clustering 
assume kernel chosen kernel matrix constructed 
methods matrix eigenvectors eigenvectors closely related laplacian matrix order infer label assignment approximately optimizes cost functions 
see spectral decompositions kernel matrix 
includes analysis algorithms tests methods real world data encouraging results 
partition cost measures information needed specify clustering set data contained matrix ij cluster cluster 
clustering speci ed measure cost ways 
propose cost functions easy compute lead ecient algorithms 
learning possible collusion input distribution target exists predict target input 
typically expect points similar labels clustered clusters separated 
detected ways measuring amount label clustering measuring correlation variables 
rst case need measure points class close distant points di erent classes 
second case kernels regarded oracles predicting points class 
true oracle knows true matrix measure quality obtained measuring pearson correlation coecient kernel matrix true approaches lead quantity known alignment :10.1.1.23.6757
de nition inner product matrices hk 
index refers frobenius norm corresponds inner product 
de nition alignment empirical alignment kernel kernel respect sample quantity hk hk hk kernel matrix sample kernel viewed cosine angle bi dimensional vectors representing gram matrices 
consider yy vector labels sample slight abuse notation hk yy hk kif hyy yy hk yy hyy yy measure separation classes average separation points di erent classes normalised matrix norm 
de nition cut cost 
cut cost clustering de ned ij quantity motivated graph theoretic concept 
consider kernel matrix adjacency matrix fully connected weighted graph nodes data points cost partitioning graph total weight edges needs cut remove exactly numerator cut cost 
notice relation alignment ij hk kif ones vector 
appealing properties alignment quantity sharply concentrated mean proven companion :10.1.1.23.6757
shows expected alignment reliably estimated empirical estimate 
cut cost expressed di erence alignments similarly concentrated expected value 
optimising cost spectral techniques section introduce test related methods clustering extensions transduction 
general problem want solve assign class labels datapoints maximize cost functions 
equation optimal solution problems identical xed data set kernel 
di erence approaches approximation algorithms developed di erent cost functions 
approximation algorithms obtained relaxing discrete problems optimising possible labellings dataset closely related continuous problems solved eigenvalue decompositions 
see eigenvectors partitioning sparse matrices 
optimising alignment optimise alignment problem nd maximally aligned set labels max max hk yy hk kif setting kernel xed maximising alignment reduces choosing maximise hk yy ky allow chosen larger set subject constraint kyk obtain approximate maximum alignment problem solved eciently 
solving relaxed problem obtain approximate discrete solution choosing suitable threshold entries vector applying sign function 
bounds quality approximations 
solution approximate problem follows theorem provides variational characterization spectrum symmetric matrices 
theorem courant fischer minimax theorem symmetric max dim min mv min dim max mv consider rst eigenvector rst min apply obtain approximate alignment problem solved rst eigenvector maximal alignment upper bounded multiple rst eigenvalue max max kv transform vector vector choosing threshold gives maximum alignment sign max 
de nition value alignment obtained lower bound optimal alignment max estimate quality dichotomy comparing value upper bound 
absolute alignment tells specialized kernel dataset higher quantity committed speci dichotomy 
rst eigenvector calculated ways example lanczos procedure ective large datasets 
search engines google estimating rst eigenvector matrix dimensionality large datasets approximation techniques 
applied procedure outlined datasets uci repository 
preprocessed data normalising input vectors kernel de ned feature space centering shifting origin feature space centre gravity 
achieved transformation kernel matrix jg gj kjj ones vector ones matrix vector row sums eigenvalue number threshold number plot alignment di erent eigenvectors labels ordered increasing eigenvalue 
plot breast cancer data linear kernel max straight line sign max bottom curve accuracy middle curve threshold number rst experiment applied unsupervised technique breast cancer data linear kernel 
shows di erent eigenvectors labels 
highest alignment shown eigenvector corresponding largest eigenvalue 
value threshold shows upper bound max straight line alignment sign max bottom curve accuracy middle curve 
notice actual alignment upper bound alignment get closest con dence partitioned data fact accuracy maximized 
notice choice threshold corresponds maintaining correct proportion positives negatives 
suggests possible threshold selection strategy availability labeled points give estimate proportion positive points dataset 
way label information choose threshold 
experiments describe transduction method 
measure naturally data separates procedure able optimise split accuracy approximately choosing threshold maximises alignment threshold number making labels 
results gaussian kernel 
case accuracy obtained optimising alignment threshold number resulting dichotomy impressive 
shows results ionosphere dataset 
accuracy split optimises alignment threshold number approximately threshold number threshold number plot breast cancer data gaussian kernel ionosphere data linear kernel max straight line sign max bottom curve accuracy middle curve threshold number 
approach adapt kernel data 
example choose kernel parameters optimize max nd rst eigenvector choose threshold maximise alignment output corresponding cost alignment changing label point isolated equally close di erent classes changing label small ect 
hand labels strongly clustered points clearly contribute cost changing label alter alignment signi cantly 
method described viewed projecting data dimensional space nding threshold 
projection implicitly sorts data points class nearby ordering 
discuss problem class case 
consider embedding set real line satisfy clustering criterion 
resulting kernel matrix appear block diagonal matrix 
problem addressed case information retrieval applied assembling sequences dna 
cases eigenvectors laplacian approach called fiedler ordering 
fiedler ordering variation simple kernel matrix 
coordinate point real line 
consider cost function ij 
maximized points high similarity sign high absolute value points di erent sign low similarity 
choice coordinates optimizes cost rst eigenvector sorting data value entry eigenvector hope nd permutation renders kernel matrix block diagonal 
shows results heuristic applied breast cancer dataset 
grey level indicates size kernel entry 
gure left unsorted data right shows plot sorting 
sorted gure clearly shows method 
optimising cut cost xed kernel matrix minimising cut cost corresponds minimising sum kernel entries points dif gram matrix cancer data permutation data sorting order rst eigenvector ferent classes 
dealing normalized kernels controls expected distance 
express quantity ij ij ky ly laplacian matrix de ned diag dm 
nd minimize cut cost subject division problem np hard 
strategy alignment impose slightly looser constraint 
gives problem min ly subject zero eigenvalue eigenvector ones vector problem equivalent nding eigenvector smallest non zero eigenvalue min ly eigenvalue provides lower bound cut cost min eigenvector corresponding eigenvalue laplacian obtain approximate split gives lower bound cut cost 
threshold entries eigenvector order obtain vector entries 
plot lower bound cut cost error rate function threshold 
applied procedure breast cancer data linear gaussian kernels 
results shown 
cut cost select best threshold linear kernel sets accuracy signi cantly worse results obtained optimising alignment 
gaussian kernel hand method selects threshold accuracy slight improvement results obtained kernel optimising alignment 
far algorithms unsupervised data 
consider situation partially labelled dataset 
leads simple algorithm transduction semi supervised learning 
idea labelled data improve performance comes observing selection cut cost clearly suboptimal 
incorporating label information hoped obtain improved threshold selection 
threshold number cut cost error rate threshold number plot breast cancer data linear kernel gaussian kernel dashed curves sign max error solid curve threshold number vector containing known labels 
set kp zz positive constant parameter 
original matrix generate eigenvector matrix kp measuring cut cost classi cations generated di erent thresholds 
performed random selections data obtained mean success rate standard deviation breast cancer data gaussian kernel marked improvement achieved label information 
considered partition costs rst derived called alignment kernel label vector second cut cost label vector kernel matrix 
quantities optimised labelling give rise di erent approximation algorithms discrete constraint removed labelling vector 
shown relaxed problems solved exactly spectral techniques leading distinct approximation algorithms post processing phase re vector create labelling chosen optimise criterion 
experiments showing performance clustering techniques striking results 
second algorithm gave preliminary experiment transductive version enables labelled data re ne clustering 
berry hendrickson raghavan 
sparse matrix reordering schemes browsing hypertext 
numerical analysis pages 
ams 
cristianini shawe taylor 
support vector machines 
cambridge university press 
see web site www support vector net 
nello cristianini andr john shawe taylor kandola :10.1.1.23.6757
alignment 
submitted proceedings neural information processing systems nips 
nello cristianini lodhi john shawe taylor 
latent semantic kernels feature selection 
technical report nc tr neurocolt working group www neurocolt org 
pothen simon liou 
partitioning sparse matrices eigenvectors graphs 
siam matrix anal 
