decoding neuronal firing modeling neural networks abbott center complex systems brandeis university waltham ma published quart 
rev biophys 


biological neural networks large systems complex elements interacting complex array connections 
individual neurons express large number active conductances connors adams gavin mccormick exhibit wide variety dynamic behaviors time scales ranging milliseconds minutes harris churchland sejnowski turrigiano 
neurons cortical circuits typically coupled thousands neurons stevens little known strengths synapses see nelson 
complex firing patterns large neuronal populations difficult describe understand 
little point accurately modeling membrane potential large neural circuit effective method interpreting visualizing enormous numerical output model 
major issues modeling biological circuits describe interpret activity large population neurons model neural circuits ii individual neurons complex elements iii knowledge synaptic connections incomplete 
review covers approaches techniques developed try deal problems 
considered solved progress combining number different techniques put fairly comprehensive description aspects neural systems function 
review concentrate mathematical computational methods applications provided primarily illustrate methods 
reader consult applications specific neural systems 
general techniques proven particularly valuable analyzing complex neural systems dealing problems mention 
patterns activity neural networks gain functional behavioral significance interpreted correctly 
step process 
spike trains individual neurons considered activity entire population neurons interpreted collectively 
second step described 
spike train produced single neuron extremely complex reflecting part complexity underlying neuronal dynamics problem ii 
method analyzing neuronal spike trains linear filter bialek bialek developed applied problem decoding information contained spike trains single neurons bialek bialek 
approach reviewed section 
vast majority cases neuronal spike trains characterized firing rate 
characterization thought eliminate information arising spike timing quite distinct linear filter approach case 
shown section precisely defined firing rate variables special case decoding schemes linear filters 
firing rates generated linear filters see section provide concise description output single neuron contains information individually conjunction population decoding methods discussed sections describe interpret neuronal activity 

cases information represented activity ensemble neurons collective distributed manner knudsen konishi sejnowski konishi eichenbaum discussed section 
various methods decoding information represented neural ensembles developed applied number systems wilson mcnaughton abbott 
reviewed sections 
ability decode activity neural network tremendous advantage modelers 
addition providing functional interpretation neuronal output decoding allows complex activity neural circuit described concisely loss functionally relevant information 
impact various neuronal synaptic properties activity large network typically complex subtle 
effect properties value coded quantity understood interpreted quite easily clearly abbott blum 
examples section 
cases applied decoding provides effective solution problem 
combining optimal linear filter efficient methods decoding populations neurons possible extract maximum amount information neural circuit 
stressed decoding considered tool analyzing neural network doing 
methods nervous system interpret respond patterns neuronal firing necessarily relationship decoding methods 
assume result decoding procedure provides accurate measure information represented neural network cautionary note see newsome 

problem iii lack knowledge synaptic strengths severe problems addressed 
difficulty measuring strength single synapse enormous number synapses severely limit amount information obtained connections large neural networks 
great deal information available synaptic strengths change due activity long term potentiation ltp bliss depression singer 
sense study effect synaptic changes synaptic values 
synaptic learning rules see section inspired ltp data compute changes synaptic strengths arise specific training experiences 
effect changes studied looking resulting modifications network activity decoding 
allows examine impact training experience learning network function shown section 
changes small added advantage impact determined linear computation resolves problems associated intrinsic neuronal complexity problem ii 
changes larger calculating effects ambiguous linear approximation serve basic guideline 
approach taken sections 

describing decoding neuronal activity aspect problem modeling neural network 
challenge build model accounts activity describes changes time 
cases firing rates describe activity tempting try build dynamic model purely firing rates frequently done wilson cowan hopfield abbott amit tsodyks ermentrout 
firing neuron affected large number variables state activation inactivation numerous ion channels concentration calcium various second messenger molecules inside cell described purely terms firing rate 
firing rate model compute rates principles basis ion channels fundamental neuronal characteristics 
mathematical fits measured rates incorporated models augmented various approximations unmeasured elements model 
approach sections 
dynamics describing firing rate changes time greatly simplified integration time define firing rate longer intrinsic neuronal time scale affecting firing discussed section 
case measured calculated static properties construct dynamic model 
firing rate models highly simplified essential feature 
possible calculate include nonlinear effects considered section 
difficult interpret results modeling studies neural networks patterns firing produce decoded behaviorally relevant way 
reason consider cases firing single neuron population neurons correlated external variable may sensory input coded value input output network motor output 
quantity single number vector array numbers 
mainly consider case single coded quantity leaving discussion coded vectors section 
classic experimental procedure neuroscience find neuron firing pattern related sensory stimulus study response properties neuron varying stimulus 
paradigm carried replacing sensory stimulus motor output 
experiments sort built large catalog neuronal responses external correlates discussed section 
clarity refer quantity coded stimulus stimulus value 
kept mind equally represent motor output 
review concentrate methods results illustrate methods 
section considers effect training coded value stimulus 
phenomenon applications memory learning 
section provides simple example activity dependent development neuronal response tuning curve 
sections resonance phenomenon may amplification mechanism recognition input analyzed 

spike coding fundamental unit information signaling nervous system action potential 
action potential stereotyped response shape amplitude convey information 
information carried action potential timing 
response single neuron specified listing times fires action potentials 
spiking output neuron characterized series delta functions times action potentials occur ffi gamma representation neuronal firing complete awkward behaved function complete listing individual spike times may information handled practically computationally 
problems resolved spike decoding 

spike decoding general terms spike decoding method generating time dependent variable describes useful way spike train produced neuron 
way define linear filter bialek bialek 
expresses variable characterizing spike train convolution spike train signal specific kernel dt gamma best kernel equation depends information suppose convey 
suppose neuron firing response stimulus 
common modeling studies address question stimulus produce observed spike train 
decoding allows ask question information stimulus encoded series spikes produced 
address question decode spike train constructing estimate est stimulus value 
method decoding direct estimate value stimulus bialek bialek est key find kernel generates est providing best possible direct estimate value stimulus 
demand est close stimulus value possible minimizing square difference averaged time dt est gamma minimum approach success reconstruct coded stimuli spike trains movement sensitive neurons bialek neurons systems bialek 
kernel accomplishes task computed method fourier transforms bialek bialek constructed applying modification rule gradually changes kernel improve match true stimulus predicted value sen abbott unpublished 
describe kernel easiest divide time discrete intervals deltat integer define kernel terms discrete array deltat practical reasons kernel defined nonzero finite time range fixed value 
spike train similarly defined variable deltat gamma deltat dt equal number action potentials occurring interval gamma deltat deltat 
deltat small zero 
terms variables response function provide estimate written est deltat kms gammam kernel modification rule minimization condition 
gradient decent algorithm find modification km km ffl est delta gamma deltat gammam ffl small parameter reduce squared difference est 
procedure generating kernel start random elements compute resulting estimate est corresponding stimulus apply modification rule repeatedly difference est decreased desired level accuracy 
shows results process 
quasi periodic signal generated current input integrate fire model neuron 
decoded signal est computed spike train produced model neuron equation 
kernel initially random shown resulting estimate match input data 
shows comparison est modification process constructed kernel seen 
kernel provides estimate signal large positive values produce spikes 
smaller subthreshold inputs reconstruction accurate 
small delay reconstructed value caused latency spike response input 
modification rule develops kernel extended period final form kernel reflects firing properties neuron statistics input data stream 
generating direct estimate stimulus signal est thing done linear filter 
measures spike train may useful 
example method stimulus signal unknown 
cases chosen information content entropy maximized abbott unpublished 
done making distribution values time constant possible 
alternately kernel chosen generate considered measure firing rate neuron 
cases rate action potential firing desired measure neuronal response stimulus additional information may encoded characteristics firing patterns richmond miller 
despite fact firing rate way experimental theoretical papers little care taken defining meant firing rate 
neurons fire perfect periodicity indefinitely care taken defining mean firing rate neuron 
method define firing rate choose time interval count number spikes appearing period time define rate number spikes divided length interval 
measure firing rate undesirable property defined discrete values time 
neuron models definition update firing rate discontinuously discrete time intervals pitts hopfield amit hertz 
useful generate firing rate variable defined times 
instantaneous firing rate denote variables 
linear filter define firing rate variable appropriate constraint place kernel 
suppose neuron firing constant rate period require considered firing rate variable value averaged period equal firing rate definition means dt dt gamma switching order integration eliminating common factor rewritten condition dt dt gamma second integral equal neuron firing period find qualify firing rate variable provided dt firing rate variable just special case linear filter kernel required satisfy condition 
conversely decoded signal produced linear filter proportional quantity considered firing rate 
condition obviously leaves infinite variety kernels 
possibilities exp gammat exp gammat gamma exp gammat gamma exp gammat obvious interpretations terms stream processing spike train forms characterize postsynaptic responses 
exponential kernel defines firing rate low pass filtered version spike train 
resulting firing rate jumps time spike decays exponentially zero time constant spikes 
discontinuous jump firing rate produced kernel eliminated kernels alpha function difference exponentials respond instantaneously 
second advantage rise time fall time adjusted independently 

population coding extremely widespread form neural coding involves large arrays neurons represent information collectively ensemble knudsen konishi sejnowski konishi eichenbaum 
arrays individual neurons stimulus value 
individual firing rates depend reflect value coded information accurate decoding action potentials coming neurons 
examples information coded firing rates arrays tuned neurons 
partial list neuronal arrays corresponding coded quantities includes neurons primary visual cortex cat coding orientation bar light review see direction arm movements monkey coded neurons motor cortex neurons superior colliculus encoding saccade direction van lee interneurons cercal system cricket coding direction wind stimulus bacon miller theunissen miller theunissen similar system levy direction sound stimulus coded neural arrays barn owl knudsen konishi konishi position rat environment coded hippocampal place cells keefe keefe nadel keefe wilson mcnaughton mt neurons coding visual motion direction maunsell newsome neurons coding echo delay bat neill 
population coding subject number theoretical analyses paradiso vogels koenderink seung sompolinsky touretzky abbott related issue hyperacuity baldi sejnowski zhang miller connected network models van kappen 
experiments involving coding arrays tuned neurons average responses tuning curves individual neurons measured functions value stimulus variable index runs total number neurons array label individual neurons 
average firing response neuron stimulus takes value denoted 
purposes review consider idealized array tuned neurons gaussian tuning curves identical shape varied optimal stimulus value max exp gamma gamma oe shown 
parameter stimulus value neuron responds maximum average firing rate max width tuning curves oe tuning curves cover range densely overlap approximate sums neurons integrals positions neurons fire maximum rates follows ae dy ae neuronal tuning curve density number neurons covering unit range ignore edges array coded value reaches limits range making range infinite 
useful results approximations aeoe max exp gamma gamma oe find aer max oe gamma exp gamma gamma oe theta aer max oe prime denotes differentiation 
useful formula derivations follow dz exp gamma gamma oe gamma gamma oe oe oe oe oe exp gamma gamma oe oe equations derive results sections 

population decoding information stimulus motor output coded entire ensemble neurons decoding full population response requires procedures combining firing rates neurons population ensemble estimate 
georgopoulos collaborators developed approach defining population vector relating activity motor cortex monkey direction arm movements georgopoulos 
population vector method developed system similar arm movement studies premotor cortex parietal area cerebellum 
addition vector method applied systems primary visual cortex gilbert wiesel parietal visual neurons steinmetz inferotemporal neurons monkey responding human faces young 
application involved multidimensional scaling relate responses face cells encoding dimensional vector 
wilson mcnaughton different method decode output position selective place cells keefe keefe nadel keefe hippocampus rat 
number different methods decode neural population activity variety systems review see abbott 
decoding activity neural ensemble means computing estimate coded quantity est firing rates neurons ensemble 
similar spirit procedure discussed section time estimate firing neurons 
typically calculation tuning curves individual neurons responding stimulus decoding schemes works est typically close true value stimulus need case 
decoding way 
neural network changes strengths synapses neurons may shift value est constructed decoding scheme 
case shift est may tell meaning significance synaptic changes occurred 
discussed section est different may functional significance reflecting inaccuracy decoding scheme 
methods construct population estimate coded signal 
denote collection firing rates notation likewise collection tuning curves sophisticated methods require knowledge probability stimulus value population firing response denoted xjr 
commonly maximum likelihood method consists finding value maximum probability set firing rates estimate 
est jr maximized 
rjx probability stimulus producing set firing rates obtained experimental measurements est jr determined bayes theorem xjr rjx maximum likelihood procedure maximizes probability respect value factor relevant express maximum likelihood condition rjx est est maximum population neurons large necessary probabilities know maximum likelihood method probably optimal decoding procedure 
population neurons decoding large rigorous theoretical justification maximum likelihood method 
may better bayesian decoding scheme guaranteed minimize average squared difference estimated value true stimulus value 
estimate just average value corresponding probability distribution xjr est dx xp xjr cases probability rjx may known 
decoding method case churchland sejnowski wilson mcnaughton chooses estimated value est gamma est gamma maximum considered vectors dimensional space neural responses 
equation minimizes angle true response vector mean response vector method particularly useful population wide variations neurons decoded 
review covers theoretical applications decoding type variability problem simpler squares approach discussed 
general method population decoding choose est mean firing rates est provide best possible match observed rates done performing squares fit average rates est observed rates gamma est minimum functions represent average firing responses neurons array coded quantity takes value due noise neuronal variability variations mean firing rates 
variability decoded value est identical true value average quantities est fluctuate average due variability firing rates 
suppose fluctuations uncorrelated zero mean standard deviation oe variance estimate est squares method oe oe oe gamma prime denotes differentiation 
case gaussian tuning curves reduces oe oe oe aer max higher density coverage lower decoding error dependence oe ae indicates typical square root effect 
error depends directly variability individual neuronal rates square root tuning curve width 

decoding direction coding decoding direction interesting special case population coding 
case coded quantity vector multiple components dimensional vector coded 
dimension typically 
direction magnitude coded convenient unit vector xj 
cartesian components vector projections unit vectors form orthogonal coordinate axes delta cos angle inverse procedure reconstruction vector components accomplished summing component values times appropriate unit vectors vectors typically represented mathematically equation components projected orthogonal cartesian coordinate axes 
remarkable neuronal circuits construct essentially representation touretzky abbott 
indication happening tuning curves form cosine functions just equation firing rates proportional projections coded vector preferred axes defined neurons 
cosine tuning curves systems 
interneurons cricket cercal system code wind direction low velocity firing rates proportional cartesian coordinates wind direction vector theunissen miller theunissen 
systems vector method applied monkey arm movements motor cortex schwartz premotor cortex parietal area cerebellum exhibit cosine tuning curves 
cosine responses neurons coding body suzuki head position cats shor head direction rats chen taube 
parietal visual neurons fire rates proportional cosine angle movement direction visual stimulus steinmetz 
course cosine functions positive negative firing rates negative 
systems means half cosine function represented neuron 
example cercal system cricket neuron represents positive part cosine function second neuron negative part theunissen miller theunissen 
systems monkey motor cortex schwartz background rate firing 
subtracted definition firing rate negative 
result larger portion cosine curve represented single neuron 
population vector decoding method georgopoulos standard formula reconstructing vector components equation 
neuron responds stimulus firing rate proportional delta vector method estimate est just 
shown uniform distribution preferred direction vectors direction est converge provided neurons included sum georgopoulos 
complication neural coding direction usual cartesian system 
number neurons typically exceeds number coordinate axes ae axes defined neurons form orthogonal system 
correct method similar spirit bayesian method decoding linear firing rates 
express estimated vector linear sum similar equations est basis vectors set equal projection vectors determined requiring linear estimate minimizes average squared error est gamma resulting basis vectors abbott gamma ij xf ij oe ffi ij vectors represent center mass tuning curves 
symmetric tuning curves point directions preferred direction vectors correlation matrix firing rates 
method provides optimal linear estimate coded signal 

network models powerful experimental tool decoding useful technique studying model neural networks provides concise description population activity direct functional significance 
consider model network architecture shown 
information stimulus value carried network set input firing rates 
firing rate input stimulus takes value synaptic coupling input neuron strength ia result inputs neuron fires rate response stimulus 
firing neuron affected recurrent synapses network neurons 
strength synaptic connection neuron neuron weight ij architecture address issues concerning network function plasticity 
response tuning curves arise afferent activity synaptic couplings input lines coding neurons 
modifications strengths input synapses affect response properties network neurons ensemble coded value stimulus 
role recurrent synapses network represented matrix 
modifications strengths recurrent synapses affect response properties network neurons ensemble coded value stimulus 
sections discuss general techniques addressing questions results obtained applying 
applications require know firing rates network neurons depend input firing rates synaptic weights connecting network neurons inputs 
simplest assume linear formula ia ij term represents effect input firing second term recurrent network connections 
discuss corrections linear approximation section 
equation refers static quasi static rates 
case dynamically varying rates considered section 

synaptic modification learning rules widely believed modification synaptic strength basic phenomenon underlying learning memory morris byrne berry gluck rumelhart davis hawkins 
addition activity dependent synaptic plasticity plays important role development nervous system miller 
large body experimental evidence supports notion neuronal activity affect synaptic strength specific features ltp bliss singer synapses characterized 
addition evidence ideas molecular basis phenomenon accumulating lynch gustafsson stevens madison 
reveal functional behavioral significance ltp understand experience training modify synapses resulting modifications change patterns neuronal firing affect behavior 
issues led large number neural network learning rules proposing activity training experience change synaptic weights hebb sejnowski bienenstock reviewed abbott 
proposals concerning second questions include ideas associative memory pattern recognition marr hopfield grossberg kohonen rolls amit hertz storage statistics sejnowski levy storage recall paths sequences motor actions blum abbott abbott blum 
form synaptic modification attracted attention models learning memory hebbian ltp hebb 
potentiation synapse occur mechanism presynaptic postsynaptic neurons 
suggests learning rule strength synapse neuron neuron changes rate proportional product firing rates dw ij dt ffl ffl parameter determines rate synaptic change 
similarly strength synapse input neuron modified rate dm ia dt ffl ffl corresponding rate constant course equations assume synapse modified 
physical synapse exists corresponding element ij ia kept value zero 
particular synapses neuron equation included 
practice common assume coupling ignore constraints 
valid approximation large highly coupled networks 
equations allow synaptic strengths grow bound pre postsynaptic firing rates nonzero long period time 
clearly unreasonable form constraint imposed prevent synaptic weights growing large oja linsker miller mackay 
discuss constraints assuming synaptic potentiation stops constraint reached 
requires mechanism capable turning synaptic potentiation 
justify ignoring constraints assuming initial pattern changes synaptic strengths characterize final outcome changes mackay miller 
relate changes synaptic strength characterized equations experience animal training development 
divide problem parts 
development interested large changes give rise tuning curves training development changes apt subtle consider small deviations basic firing rates induced training experience 
computation training induced changes requires fewer assumptions case discuss section 
equations require knowledge firing rates tuning curves developed previously changes large knowledge tuning curves predict firing rates 
approximation equations 
suppose training period stimulus described 
firing rates network neurons dw ij dt ffl dm ia dt ffl equation integrated determine weight changes resulting entire training period see section 
equations ignore feature ltp may important consequences 
efficacy ltp depends critically relative timing preand postsynaptic activity gustafsson 
simultaneous ltp occurs discussed 
strict simultaneity activity required 
postsynaptic activity follow presynaptic activity ms ltp occur 
hand postsynaptic activity precedes presynaptic activity ltp occur 
may result sequence stimulation 
represent relative efficacy ltp postsynaptic activity follows presynaptic activity time rate change synaptic strengths dw ij dt ffl dt dm ia dt ffl dt replace equations 
temporal asymmetry dramatic effect synaptic weight changes produced training causes depend temporal order activity generated training 
allows synaptic weights store information temporal sequences proposed mechanism learning paths sequential motor actions blum abbott abbott blum 
cases considered temporal ordering play important role equations lead results equations 
studying development tuning curves rely approximation assumes functions established 
linear approximation firing rates equation 
simplify discussion tuning curve development ignore effect recurrent synapses setting 
basic synaptic modification rule obtained substituting firing rates dm ia dt ffl ib set stimulus values development period 
equation development tuning curves analyzed section 

effects synaptic modification coding responses neurons population coding variable characterized set firing rate tuning curves 
tuning curves shaped input connections recurrent synapses network neurons 
recurrent synaptic connections network neurons cause firing rates neurons depend input firing rates 
effect firing rates considered section 
analyze effect changes strengths synapses 
suppose mapped average firing responses modifications occur due experience learning change synaptic weights amounts deltaw deltam 
changes computed integrating equations time 
small taylor expand write deltam ia deltaw ij result valid order synaptic changes assuming corresponding dependence firing rates linear equation 
equation useful result allows compute effect training experience response properties neurons 
suppose changes represented deltam deltaw decode original response functions section 
result get longer est shift neuronal responses implied second term right side equation 
est deltam ia deltaw ij result obtained minimizing squared difference respect est equation firing rates computing lowest order weight changes 
equation allows evaluate effect synaptic modifications value coded quantity 
effect shift second term right side equation coded value away stimulus value 
decode firing rates derive result original tuning curves match actual firing rates despite fact assumes downstream neural networks changed training experience induced changes synaptic weights represented deltam deltaw 
time networks modified appropriate tuning curves decoding equation original shift equation eliminated find est 
effective way erasing effects learning having reset synaptic weights modified training process 
results previous section give information predict exposure stimulus training period affect synaptic strengths changes synaptic strengths modify patterns firing modified firing patterns affect value quantity encoded neural ensemble 
training period stimulus takes sequence values subsequently expose animal stimulus neuron fire rate equation deltaw deltam determined integrating equations 
basis equations determine network firing value est obtained firing affected exposure time dependent stimulus training 
relates value coded quantity behavioral relevance stimulus experience animal training allows direct computation effects training coding 
results calculations figures see abbott blum 
examples consider effects changes recurrent synapses network neurons set deltam 
case considered varies randomly uniformly entire range 
corresponds exposing animal random sequence stimuli training period 
results section find exposure changes recurrent synaptic strengths amount deltaw ij max oe exp gamma gamma oe constant related ffl duration training period 
resulting effect sort training neuronal response tuning curves determined inserting expression equation results section 
find random training gw aer max oe exp gamma gamma oe second term wider gaussian curve original tuning curve 
exposure random stimuli training tends broaden tuning curves 
effect value coded quantity tuning curves shifted asymmetrically distorted 
result decoding equation produces result est consider case value coded quantity changed experience training 
training period consists exposure single stimulus value 
training changes recurrent synapses deltaw ij max exp gamma oe constant defined somewhat different appearing depends rate synaptic change length training period 
rate firing response input training computed techniques equation max aeoe exp gamma oe causes increase strength response tuning curves centered asymmetric distortion tuning curves near point 
impact changes decoded value est function true stimulus value computed equation est gamma aer max oe exp gammax oe jj plotted 
far coded actual stimulus values agree 
stimuli near zero produce coded value different stimulus value 
exposure stimulus output network merely represent stimulus combination value training value 
significance shift causes est differ region training point see 
information stimuli experienced training affects perception new stimuli 
network interprets inputs near located nearer 
relates ideas pattern completion associative memory neural networks hopfield grossberg kohonen amit hertz 
point recovered initial points near zero feed value est coded output network back acts new input value 
result iterating feedback loop shown 
iteration leads coded output value est initial input near zero 
feedback network coded output input achieved internally network level arise response animal coded output leads new stimulus value equal coded value 
network associative memories final fixed point value example functional significance 
value deviation coded value true value contains information 
difference identifies direction training point nearby position 
shown networks type store recall information paths sequences motor actions blum abbott abbott blum 
suppose example input signal corresponds sensory information position limb 
initially network activity codes value est identical true position suppose training session motion repeated synaptic potentiation takes place 
temporal inputs lead asymmetric sequence dependent potentiation synapses coding neurons ltp processes described equation 
potentiation result decoding network activity est shifted away input value amount equation 
shift causes coded value est lead true value provided information limb go order execute learned motion 
coded value fed motor networks controlling limb motion learned sequence automatically recalled generated 
idea applied recall paths spatial environment hippocampus blum abbott 
hippocampus rat place cells collectively code information spatial position animal keefe keefe nadel keefe wilson mcnaughton 
ltp occurs rat travels particular path path dependent pattern synaptic potentiation established 
ltp shown shift location coded place cell ensemble activity away location rat direction learned path 
position coded point guide navigation previously learned path 

development tuning curves previous section studied existing tuning curves modified synaptic changes impact coded quantities 
consider larger synaptic modifications lead development tuning curves 
issue analyzed great depth studies development ocular dominance directional selectivity primary visual cortex malsburg linsker miller bienenstock 
discuss simpler generic example illustrate general mechanism 
examine development recurrent collateral input 
tuning curves case just firing rates equation ia rate parameter ffl controlling speed synaptic changes small synaptic changes slower changes stimulus value average stimulus history rewrite dm ia dt ffl ib brackets 
signify time averaging 
quantity correlation matrix input firing rates 
matrix positive eigenvalues synaptic weights grow exponentially 
combination synaptic weights grows rapidly ultimately dominate approximate final weights considering mode equation hertz mackay miller 
means ultimately proportional eigenvector matrix maximum eigenvalue 
result interesting interpretations 
means weights automatically select combination inputs maximum input correlation input variance mean input signal zero 
resulting firing rate proportional maximal principal component input data set oja hertz 
certain conditions apply shown assures neuronal response carries maximum amount information input signal linsker 
remarkable simple developmental rule leads optimal neuronal response 
shows development neuronal response tuning curve single neuron basis equation 
inputs case set gaussian firing rate curves shown equation 
initially input weights set equal resulting firing rate function equation flat edges 
development stimulus consisted single value max exp gamma oe leads development gaussian firing response curve centered point figures 
case final tuning curve property neuron responds optimally value stimulus seen development 

effects recurrent synapses firing rates section saw neuronal responses tuned specific stimulus value established input synaptic weights ignoring recurrent synapses network neurons 
section noted effect training induced changes strengths recurrent synapses consider impact 
role recurrent network synapses tuning refining neuronal network responses inputs 
suppose input synaptic weights developed produce tuning curves equation 
add recurrent synapses firing rates network neurons depend stimulus value ij determine neuronal responses presence recurrent synapses solve equation firing rates ffi ij gamma ij gamma assume diagonalized define complete set eigenvectors eigenvalues labeled index satisfying ij terms write solution firing rates gamma equation displays interesting phenomenon resonance 
suppose eigenvalues near slightly 
stimulus causes value corresponding eigenvalues appreciably greater zero elicit strong firing response 
firing response amplified factor gamma denominator eigenvalues near 
strong response proportional network exhibiting pattern simple case value recognition 
special values stimulus excite modes eigenvalues near elicit extremely strong responses system 
addition strong bias coded value stimulus values corresponding eigenvectors eigenvalues near 
eigenvalues larger resonance phenomenon dramatic 
case inhibition mechanism limit firing rates dynamic model section shows network unstable firing rates diverge 
limiting mechanisms incorporated model strong resonance phenomenon observed 
shown section implications resonant responses discussed 

network models nonlinear effects results discussed previous sections assumption firing rate individual neuron depends firing rates neurons synapse 
assumed equation dependence linear assumption compared experimental results corrected accurate 
unfortunately easily done 
neurophysiology experiments firing rate usually measured function amount current injected soma recorded neuron gustafsson 
natural setting neurons receive current synapses electrodes 
parts computation determines neuronal firing rates depend 
need know firing rate neuron depends amount current injected soma 
second determine presynaptic firing changes conductance postsynaptic terminal leading synaptic current postsynaptic neuron 
relate amount current entering soma induced conductance changes 
evidence circuits situations neurons may operate fairly linear range 
potential sources nonlinearities pathway presynaptic postsynaptic firing 
firing rates neurons strictly linear dependence amount current injected 
threshold current firing occurs 
threshold firing rate may suddenly jump nonzero value may rise continuously zero rinzel ermentrout 
bifurcation theory indicates case wide class models firing rate depend square root difference injected current threshold current near transition firing rinzel ermentrout 
high current firing rates rise indefinitely saturate effects refractory period 
constant current injection firing rate may remain steady may fall due spike rate adaptation 
simplest model neurons fire model firing rate nonlinear logarithmic function injected current see example tuckwell 
presynaptic firing results release transmitter presynaptic neuron induces change membrane conductance postsynaptic neuron site synapse 
amount current unit area enters neuron due synaptic conductance written syn syn syn gamma syn synaptic reversal potential 
current depends presynaptic voltage produces potential nonlinear effects discussed 
postsynaptic conductance unit area written form syn syn constant dynamic variable range 
variable probability synaptic receptor channel open conducting state 
step analyzing dependence synaptic conductance presynaptic firing rate determine amount transmitter released presynaptic terminal depends rate 
simplicity take dependence linear need 
suppose transmitter interacts receptor simple binding reaction closed receptor transmitter molecules open receptor forward rate constant backward rate constant gamma terms open probability receptor reactions imply dm dt gamma gamma gamma transmitter concentration 
steady state open probability equation gamma linear small 
synaptic conductances induce currents surface large elaborate dendritic trees 
total current soma feeding soma trunk dendritic tree shown analog current injected electrode experimental setting 
currents entering dendritic synapses travel dendritic cables reaching soma 
subject losses longitudinal resistance dendritic cable due leakage dendritic membrane 
synaptic conductances dendritic cable affect amount membrane leakage potential nonlinear impact amount current entering soma rall koch segev parnas abbott rapp 
assume neuron modeled passive dendritic tree 
particularly valid assumption provides lower limit nonlinear effects expected 
active dendritic trees introduce additional sources nonlinearity 
passive case current soma entering soma dendritic tree computed methods cable theory jack rall tuckwell butz cowan koch poggio abbott 
examples compute current flows trunk dendritic tree soma synapses dendrite introduce conductance change unit area syn synaptic current density syn computation greatly simplified convenient simplification due rall 
idea replace complete dendritic tree single cylindrical cable having total length original tree total surface area 
simplification shown figures 
give results total dendritic current entering soma soma equivalent cable radius length different synaptic placements 
suppose synapses uniformly spread entire dendritic cable 
case abbott soma syn tanh arm rl syn mrm rm conductance unit area membrane rl longitudinal intracellular fluid 
short cable small synaptic conductance formula fact linear synaptic current soma ali syn just area cable times synaptic current density 
cable length longer synaptic conductance larger somatic current grows square root synaptic conductance 
nonlinear dependence soma firing rates dramatic synaptic input located equivalent cable 
case abbott soma syn cosh syn mrl sinh arm rl short cable soma syn syn linear syn small synaptic conductances approach finite limiting values large synaptic conductances 
summary possible sources nonlinearity relation preand postsynaptic firing rates 
transmitter release nonlinear function presynaptic firing rate synaptic conductance nonlinear function transmitter concentration somatic current nonlinear function synaptic conductance postsynaptic firing rate nonlinear function somatic current 
luck combine produce fairly linear dependence course small range nonlinear functions approximated linear expressions 
situations nonlinearities may important seen section 

network dynamics single feature biological neural systems responsible making difficult model tremendous range time scales dynamics churchland sejnowski 
fastest currents typical neuron responsible initiating action potential turn millisecond 
slowest currents synaptic currents time constants hundreds milliseconds 
neurons shown activity dependent changes periods hour turrigiano 
dynamic range modeled extends orders magnitude 
drastic approximations produce idealized neural network models collapsing huge range single time scale 
approximation rigorous cases synaptic couplings slower time constants affecting neuronal dynamics ermentrout 
take approach assume time scale kernel equation define spike rate variable significantly longer time scale affecting neuronal network dynamics 
time scale kernel slower intrinsic synaptic time scales problems associated complexity neuronal dynamics disappear 
consider single neuron responding stimulus 
suppose stimulus described slowly varying function average quasi steady state firing rate response stimulus 
neuronal dynamics faster time scale kernel actual firing rate settle close steady state value time shorter scale 
addition slow kernel spike integrated kernel changes appreciably 
result replace actual spike response function equation average firing rate particular value write dt gamma relates dynamics firing rate variable directly stimulus dynamics steady state response function 
slowly varying kernel define firing rate avoid complications neuronal network dynamics build dynamic model purely basis steady state properties 
take exponential kernel equation express form differential equation dr dt gammar equation discussed derived times wilson cowan hopfield abbott amit tsodyks ermentrout 
stress viewpoint time constant appearing equation directly related time constant characterizing neuron membrane time constant particular 
parameter related properties neuronal spike train characterization terms integrated variable 
differential equations written corresponding kernels 
example kernel exponential kernel pair differential equations dr dt gammar da dt gammaa double exponential kernel firing rate described gamma gamma da dt gammaa db dt gammab effects recurrent synapses network neurons included type firing rate model 
example equation describe network firing rates depend exponential kernel firing rates network obey equations dr dt gammar ij nonlinear effects discussed section incorporated complex expressions term equation abbott 
dynamic model neuronal response allows study things resonance phenomenon discussed section 
pointed eigenvalues synaptic weight matrix greater linear network unstable uncontrolled growth firing rates 
seen performing linear stability analysis equation 
various ways controlling explosive unstable rise firing proposed 
basic mechanism inhibition involves nonlinear firing rate inhibitory neurons buhmann nonlinear effects fast inhibition abbott relative timing excitatory inhibitory responses douglas douglas martin 
model shown figures unbounded growth firing rates linear networks resonance controlled nonlinear effects inhibition computed section abbott 
typical response population cortical hippocampal neurons stimulation induced firing followed kinds inhibition connors 
inhibitory responses fast component terminate firing followed slower long lasting 
result activating types receptors labeled respectively 
fast inhibition essential normal operation excitatory neurons blocked tend fire epileptic bursts miles wong 
existence forms inhibition time scales associated suggest resonances occur controlled inhibition terminated inhibition 
shows non resonant response network neurons model includes excitatory synapses slow fast inhibition see abbott details model 
stimulation induces firing principal neurons network turn causes rise fast slow inhibition 
case stimulus produce strong response particular response duration stimulus 
shows resonant response achieved increasing strength excitatory synapses cause instability network firing stimulation 
instability seen rapid rise firing rate stimulation 
response stabilized rapid activation fast inhibition stops exponential rise firing rate 
firing temporarily self sustaining ultimately slower inhibition terminates firing 
result stronger response lasts significantly longer stimulus 
shows resonance effect achieve pattern recognition 
model network exposed random patterns neuronal stimulation lasting ms occurring ms produced modest patterns firing shown 
ms special pattern stimulation introduced led activation strong excitatory coupling network neurons resulted resonance 
seen greatly enhanced activity corresponding pattern stimulation 
interpreted concept decoding resonance phenomenon means stimulus values close values experienced training period elicit stronger response network values previously experienced 
resonance phenomenon amplification process caused recurrent discussed section model shown controlled nonlinear process reviewed section 

possibility decoding spike trains single neurons firing entire ensemble neurons important implications experiments theoretical studies 
theoretical side allows interpret results models way behavioral experimental significance 
reporting results modeling studied terms patterns neural firing provided values coded stimuli motor outputs 
knowledge synaptic plasticity growing continually results translated synaptic weight learning rules provide possibility avoiding dilemma caused ignorance synaptic strengths real neuronal circuits 
cases detailed knowledge neural network affected training experience coupled information initial state system training provided example neuronal tuning curves may sufficient allow predict analyze training changes behavior 
neural network models extremely simplified essential feature modeling 
nonlinear dynamic effects included models needed account particular phenomenon studied 
example nonlinear resonance effect may related recognition previously learned stimuli 
emphasized simplicity great virtues network models allows phenomena studied detail provide new insights 
people eager complexity study biological system 
am extremely grateful collaborators kenneth blum kamal sen carl van vreeswijk helped develop ideas 
research supported nsf dms 
abbott 
learning neural network memories 
network comp 
neural sys 

abbott 
firing rate models neural populations 
del eds neural networks biology high energy physics 
ets pisa 
pp 

abbott 
realistic synaptic inputs network models 
network comp 
neural sys 

abbott 
simple diagrammatic rules solving dendritic cable problems 
physica 
abbott blum 
functional significance long term potentiation sequence learning prediction 
cerebral cortex 
adams gavin 
voltage dependent currents vertebrate neurons role membrane 
adv 



ubiquity hyperacuity 
acoust 
soc 
am 

amit 
modelling brain function 
cambridge university press ny 
amit tsodyks 
quantitative study attractor neural network retrieving low spike rates ii 
network effective neurons attractor neural networks cortical environment 
network 
amit tsodyks 
effective neurons attractor neural networks cortical environment 
network 
singer 
long term depression excitatory synaptic transmission relationship long term potentiation 
trends neurosci 


inhibitory potentials neurons deep layers vitro neocortical slice 
brain res 

bacon 
receptive fields cricket interneurons related dendritic structure 
physiol 
lond 

baldi 
sensory maps enhance resolution ordered arrangements broadly tuned receivers 
biol 
cybern 

davis eds 
long term potentiation 
mit press cambridge ma 
bienenstock cooper munro 
theory development neuron selectivity orientation specificity binocular interaction visual cortex 
neurosci 

douglas martin koch 
synaptic background activity determines spatio temporal integration single pyramidal cells 
proc 
natl 
acad 
sci 
usa 
bialek 
theoretical physics meets experimental neurobiology 
jen ed lectures complex systems sfi studies science complexity vol 

addisonwesley redwood city ca 
pp 

bialek rieke de van 
reading neural code 
science 
bliss 
synaptic model memory long term potentiation hippocampus 
nature 
blum abbott 
model spatial map formation hippocampus rat 
neural comp 

davies 
molecular switch activated receptors regulates induction long term potentiation 
nature 
buhmann 
oscillations low firing rates associative memory neural networks 
phys 
rev 
butz cowan 
transient potentials dendritic systems arbitrary geometry 
biophys 

otto johnson 
visuomotor transformation underlying arm movements visual targets neural network model cerebral cortical operations 
neurosci 

byrne berry 
neural models plasticity 
academic press san diego 
levy 
code stimulus direction cell assembly 
comp 
physiol 

johnson 
making arm movements different parts space premotor motor cortical representations coordinate system reaching visual targets 
neurosci 

chen mcnaughton barnes ortiz 
head directional behavioral correlates posterior medial cortex neurons rats 
soc 
neurosci 
abst 

churchland sejnowski 
computational brain 
mit press cambridge ma 
connors prince 
electrophysiological properties neocortical neurons vitro 
neurophysiol 

connors silva 
inhibitory postsynaptic potentials receptor mediated responses neocortex rat cat 
physiol 

thompson 
asynchronous pre postsynaptic activity induces associative long term depression area ca rat hippocampus vitro 
proc 
natl 
acad 
sci 
usa 

cellular mechanisms epilepsy status report 
science 
douglas martin 
canonical neocortex 
neural comp 

douglas martin 
cat visual cortex 
physiol 

eichenbaum 
thinking brain cell assemblies 
science 
ermentrout 
reduction conductance models slow synapses neural nets 
neural comp 


models sensory coding 
cambridge univ phd thesis 

ideal statistical inference neural population responses 
bower eds computation neural systems 
kluwer academic publishers norwell ma 
pp 
smith 
cerebellar neuronal activity related arm reaching movements monkey 
neurophysiol 


point approximation describing total electrical activity brain simulation model 
biophysics 
georgopoulos kettner schwartz 
primate motor cortex free arm movements visual targets dimensional space 
ii 
coding direction movement neuronal population 
neurosci 

georgopoulos schwartz kettner 
neuronal population coding movement direction 
science 
georgopoulos taira 
cognitive neurophysiology motor cortex 
science 
gilbert wiesel 
influence contextual stimuli orientation selectivity cells primary visual cortex cat 
vision res 

gluck rumelhart 
neuroscience connectionist theory 
lawrence erlbaum hillsdale nj 
grossberg 
nonlinear neural networks principles mechanisms architectures 
neural networks 
miller 
ensemble coding information primary sensory interneurons cricket cercal system ii 
submitted 
gustafsson 
shape frequency current curves ca pyramidal cells hippocampus 
brain res 

gustafsson 
physiological mechanisms underlying long term potentiation 
trends neurosci 

gustafsson abraham huang 
long term potentiation hippocampus current pulses conditioning stimulus single synaptic potentials 
neurosci 

harris 
modulation neural networks behavior 
annu 
rev neurosci 

hawkins kandel 
learning modulate transmitter release themes variations synaptic plasticity 
annu 
rev neurosci 

hebb 
organization behavior neuropsychological theory 
wiley ny 
hertz palmer 
theory neural computation 
addison wesley ny 

probability transmitter release mammalian central synapse 
nature 

channels excitable membranes 
assoc sunderland ma 
hopfield 
neural networks systems emergent selective computational abilities 
proc 
natl 
acad 
sci 
usa 
hopfield 
neurons graded response collective computational properties state neurons 
proc 
natl 
acad 
sci 
usa 
jack noble 
electrical current flow excitable cells 
oxford univ press oxford 
wheat ferster 
linearity summation synaptic potentials underlying direction selectivity simple cells cat visual cortex 
science 
stevens 
quantitative description receptor channel kinetic behavior 
neurosci 

georgopoulos 
cortical mechanisms related direction dimensional arm movements relations parietal area comparison motor cortex 
exp brain 
res 

knudsen konishi 
neural map auditory space owl 
science 
knudsen 
computational maps brain 
annu 
rev neurosci 

koch poggio 
simple algorithm solving cable equation dendritic trees arbitrary geometry 
neurosci 
meth 

koch poggio torre 
nonlinear interactions dendritic tree localization timing role information processing 
proc 
natl 
acad 
sci 
usa 
kohonen 
self organization associative memory 
springer verlag berlin 
kohonen 
neural computing 
neural networks 
konishi 
centrally synthesized maps sensory space 
trends neurosci 

konishi 
deciphering brain codes 
neural comp 

storm andersen 
current frequency transduction ca hippocampal pyramidal cells slow dominate primary firing range 
exp brain res 

lee rohrer sparks 
population coding saccadic eye movements neurons superior colliculus 
nature 
sejnowski 
neural model depth interpolation distributed representation stereo disparity 
neurosci 

levy 
elemental adaptive processes neurons synapses statistical computational perspective 
gluck rumelhart eds neuroscience connectionist theory 
lawrence erlbaum pp 


mechanism hebb anti hebb processes underlying learning memory 
proc 
natl 
acad 
sci 
usa 

feasibility long term storage graded information ca dependent protein kinase molecules postsynaptic density 
proc 
natl 
acad 
sci 
usa 

basic network principles neural architecture 
proc 
natl 
acad 
sci 
usa 
linsker 
self organization perceptual network 
computer 
linsker 
local synaptic learning rules suffice maximize mutual information linear network 
neural comp 

intrinsic electrophysiological properties mammalian neurons insights central nervous system function 
science 

learned neural network simulates properties neural population vector 
biol 
cybern 

lynch larson kelso 
intracellular injections block induction hippocampal long term potentiation 
nature 
madison 
mechanisms underlying long term potentiation synaptic transmission 
annu 
rev neurosci 


receptor dependent synaptic plasticity multiple forms mechanisms 
trends neurosci 

malsburg 
self organization orientation sensitive cells striate cortex 
kybernetik 
malsburg 
development domains growth behavior axon terminals 
biol 
cybern 

marr 
simple memory theory 
phil 
trans 
royal soc 
london 
maunsell newsome 
visual processing monkey cortex 
annu 
rev neurosci 

mccormick 
membrane properties actions 
shepherd ed synaptic organization brain 
oxford univ press ny 
pp 

mcculloch pitts 
logical calculus ideas nervous activity 
bull 
math 
biophys 

mackay miller 
analysis linsker application rules linear networks 
network 
clock xu green 
panoramic code sound location neurons 
science 
miles wong 
inhibitory control local excitatory circuits guinea pig hippocampus 
physiol 

miller jacobs theunissen 
representation sensory information cricket cercal sensory system 
response properties primary interneurons 
neurophysiol 

miller 
correlation models neural development 
gluck rumelhart eds neuroscience connectionist theory 
lawrence erlbaum hillsdale nj 
pp 

miller 
models activity dependent neural development 
seminars neurosci 

miller 
model development simple cell receptive fields ordered arrangement orientation columns activity dependent competition center inputs 
neurosci 

miller mackay 
role constraints hebbian learning 
neural comp 

morris anderson lynch 
selective impairment learning long term potentiation methyl receptor antagonist ap 
nature 
oja 
simplified neuron model principal component analyzer 
math 
biol 

keefe 
review hippocampal place cells 
prog 


keefe 
hippocampus spatial map 
preliminary evidence unit activity freely moving rat 
brain res 

keefe nadel 
hippocampus cognitive map 
clarendon oxford 
neill 
encoding target range information representation auditory cortex bat 
neurosci 

richmond 
temporal encoding dimensional patterns single units primate inferior temporal cortex iii 
information theoretic analysis 
neurophysiol 


neuronal operations visual cortex 
springer berlin 
paradiso theory visual orientation information exploits columnar structure striate cortex 
biol 
cybern 

rall 
theory physiological properties dendrites 
ann 
acad 
sci 

rall 
theoretical significance dendritic trees neuronal input output relations 
reiss ed neural theory modeling 
stanford univ press stanford ca 
pp 

rall 
core conductor theory cable properties neurons 
kandel ed handbook physiology vol 

amer 
physiol 
soc bethesda 
pp 

rapp segev 
impact parallel background activity cable properties cerebellar cells 
neural 
comp 

physical principles underlying sensory processing computation 
univ california berkeley phd thesis 
rinzel ermentrout analysis neural oscillations koch segev eds methods neuronal modeling 
mit press cambridge ma 
rolls 
parallel distributed processing brain implications functional architecture neuronal networks hippocampus 
morris ed parallel distributed processing implications neuroscience 
oxford univ press oxford 
pp 

clements westbrook 
nonuniform probability release hippocampal synapse 
science 
abbott 
vector reconstruction firing rates 
comp 
neurosci 

newsome 
neural mechanisms forming perceptual decision 
science 
schwartz kettner georgopoulos 
primate motor cortex free arm movements visual targets dimensional space 
relations single cell discharge direction movement 
neurosci 

segev parnas 
synaptic integration mechanisms theoretical experimental investigation temporal postsynaptic interaction excitatory inhibitory input 
biophys 

sejnowski 
storing covariance nonlinearly interacting neurons 
math 
biol 

sejnowski 
neural populations revealed 
nature 
seung sompolinsky 
simple models reading neuronal population codes 
proc 
natl 
acad 
sci 
usa 
impulse activity patterning connections cns development 
neuron 
shor miller 
responses head tilt cat central vestibular neurons 
direction maximum sensitivity 
neurophysiol 

nelson 
estimates functional synaptic convergence rat cat visual cortical neurons 
neurosci 
abstr 

koenderink 
discrimination thresholds channel coded systems 
biol 
cybern 

steinmetz duffy 
functional properties parietal visual neurons radial organization visual field 
neurosci 

stevens 
cortical varies network size 
neural comp 

suzuki wilson 
body position respect head body position space coded interneurons 
neurophysiol 


model formation ocular dominance stripes 
proc 
roy 
soc 
london 
taube muller 
head direction cells recorded freely moving rats 
description quantitative analysis 
neurosci 

theunissen 
investigation sensory coding principles advanced statistical techniques 
univ california berkeley phd thesis 
theunissen miller 
representation sensory information cricket cercal sensory system 
ii 
information theoretic calculation system accuracy optimal tuning curve widths primary interneurons 
neurophysiol 

touretzky wan 
neural representation space sinusoidal arrays 
neural comp 

tuckwell 
theoretical neurobiology 
cambridge univ press cambridge 
turrigiano abbott 
activity dependent changes intrinsic properties neurons 
science 
van van tax 
ensemble coding saccades vector summation 
neuroscience 
van kappen 
dimensional ensemble coding model spatial temporal transformation saccades monkey superior colliculus 
network 
vogels 
population coding stimulus orientation cortical cells 
neurosci 
miller bialek 
reading spikes cercal hair receptors cricket 

bower 
eds analysis modeling neural systems 
kluwer academic publishers norwell ma 
wilson cowan 
excitatory inhibitory interactions localized populations model neurons 
biophys 

wilson cowan 
mathematical theory functional dynamics cortical thalamic nervous tissue 


wilson mcnaughton 
dynamics hippocampal ensemble code space 
science 
young 
population coding faces inferotemporal cortex 
science 
zhang miller 
model resolution enhancement layered sensory systems 
biol 
cybern 


population coding visual stimuli cortical neurons tuned dimension 
biol 
cybern 

time ms time ms time ms time ms fig 
spike decoding stimulus linear filter 
stimulus shown solid lines 
spike trains generated integrate fire model neuron stimulus acting input current 
decoded linear filter estimate stimulus 
estimate indicated dashed lines figures 
spike train shown 
random kernel plotted estimate match signal 
match estimate stimulus value seen produced kernel shown resulted repeated application modification rule discussed section 
max fig 
array gaussian tuning curves 
tuning curves identical shape overlapping 
tuning curve centered darkened clarity 
ij ia fig 
network model 
neurons label index shown circles respond stimulus value firing rate 
information carried network inputs shown small striped circles synapses depicted arrow heads 
firing rates input lines label index 
strength synapse input neuron ai synapses network neurons denoted filled circles 
strength synapse neuron neuron ij est est fig 
effect training single stimulus value coded variable 
coded value est plotted function true stimulus value training period consisted exposure stimulus 
results distortion coded value near seen magnified 
est fig 
iteration trained network 
coded value est fed back new stimulus value results steady progression initial stimulus value final coded value est equal training stimulus 
fig 
development gaussian tuning curve 
correlation synaptic modification input synapses developmental period stimulus value zero results appearance gaussian tuning curve centered point 
initial tuning curve flat near edges 
gaussian curve begins appear 
final gaussian tuning curve 
plots firing rates divided maximum rates case 
absolute terms tuning curve represents larger firing rate curve shown 
soma fig 
model neuron synaptic placements considered section 
soma typical passive dendritic tree 
current soma computed current flowing dendritic tree soma 
show soma single passive dendritic cable equivalent full tree 
synapses uniformly distributed equivalent cable synapses located cable 
fig 
resonance firing rate model forms inhibition 
network excitatory neurons spikes firing rate plotted 
levels fast slow inhibition function time indicated 
brief ms stimulation current system initially 
stimulation results transient weak firing response modest increases inhibition 
stronger excitatory couplings network neurons cause firing rate climb sharply fast inhibition controls rate resulting sustained firing 
ultimately slow inhibition increases point terminates firing 
typical resonance response stimulus 
available web 
fig 
network resonance pattern recognition 
excitatory couplings network neurons chosen particular pattern activity resulted particularly strong network excitation 
trace represents firing unit tick marks indicate action potentials 
ms brief current injected random collection neurons 
elicits small number spikes stimulated neurons 
time ms specific pattern leading strong excitatory network coupling stimulated 
result system responds firing resonance 
available web 
