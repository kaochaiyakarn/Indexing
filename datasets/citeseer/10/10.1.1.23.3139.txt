bundle relaxation methods multicommodity capacitated fixed charge network design gabriel epartement des sciences universit du qu ebec montr eal centre de recherche sur les transports universit de montr eal antonio dipartimento di informatica universit di pisa bernard epartement informatique de recherche op centre de recherche sur les transports universit de montr eal february eciently derive bounds large scale instances capacitated xed charge network design problem lagrangian relaxations appear promising 
presents results comprehensive experiments aimed calibrating comparing bundle subgradient methods applied optimization lagrangian duals arising lagrangian relaxations 
study fact bundle methods appear superior subgradient approaches converge faster robust relative di erent relaxations problem characteristics selection initial parameter values 
demonstrates ective lower bounds may computed eciently large scale instances capacitated xed charge network design problem 
fraction time required standard simplex approach solve linear programming relaxation methods attain high quality solutions 
key words multicommodity capacitated xed charge network design lagrangian relaxation subgradient methods bundle methods 
esum la relaxation comme une technique pour en des de pour des de grande taille du probl eme de conception de eseau avec ut xe cet article pr les esultats exp les ethodes de sous gradients de appliqu ees optimisation des de deux relaxations 
cette etude que les ethodes de sup aux ethodes de sous gradients convergent plus sont plus 
cet article que des inf de pour des de grande taille du probl eme de conception de eseau avec ut xe peuvent etre calcul ees de mani ere 
en les de relaxation des de tr es grande en une fraction du temps par des ethodes de type 
mots cl es probl eme de conception de eseau avec ut xe relaxation ethodes de sous gradients ethodes de 
ii network design models arise various applications telecommunications transportation logistics production planning 
applications models characterized follows network arc capacities required send ows fractional order satisfy known demands pairs 
doing pays price routing ows form xed costs arcs 
deceptively simple state capacitated xed charge network design problems notoriously dicult 
np hard attempts formulate solve mixed integer programs complications emerge 
simple linear programming relaxations generally provide approximations mixed integer programs derive formulations large numbers variables constraints order obtain tight bounds 
traditional simplex branch bound methods incorporate cutting plane column generation procedures solve simplest instances 
lagrangian relaxation approaches er interesting alternative 
lagrangian relaxations possible researchers shown yield theoretical lower bound 
furthermore initial studies clear implementation calibration lagrangian methods signi cant impact behavior performance 
initial studies authors traditional subgradient methods optimize lagrangian duals derive lower bounds 
general techniques eld erentiable optimization bundle methods appear especially promising 
contrast subgradient methods ecient variants subgradients previous iterations aggregated form avoid bundle approaches keep subgradients generated far form called bundle compute tentative direction ascent 
wide choice alternatives appears available design implement lagrangian bounding procedures capacitated xed charge network design problem experimental results knowledge guide choice 
aim address issue 
main objective results comprehensive experiments aimed calibrating comparing bundle subgradient methods applied optimization lagrangian duals arising promising relaxations 
lagrangian relaxations compared rst called shortest path relaxation relaxes called forcing constraints yields lagrangian subproblem decomposes collection shortest path problems second called knapsack relaxation ow conservation constraints allows solve lagrangian subproblem collection continuous knapsack problems 
signi cant contribution detailed analysis resulting relaxation methods implementations experiments large set test problems various characteristics 
study ers insights behavior particular relaxation method proposes strategies design implementation 
experiments demonstrate bundle methods show main advantages compared subgradient approaches bundle methods converge faster 
increased complexity bundle methods theory implementation viewpoint compensated faster convergence optimal value lagrangian dual 
experiments show computational ort measured cpu time bundle methods generally provide better values best subgradient methods 
bundle methods robust 
ecient portable software tools developed bundle methods require parameters adjust 
initial bad parameter settings allowed bundle methods derive relatively precise estimate optimal value lagrangian dual 
contrast subgradient methods usually require parameter adjustments sensitive 
unusual parameter setting particular relaxation performs instances poorly 
study shows ective lower bounds computed eciently large scale instances capacitated xed charge network design problem 
lagrangian duals corresponding relaxations optimal value optimal value called strong linear programming relaxation 
instances tested resulting strong lower bound optimal value mixed integer program reported optimality average instances feasible necessarily optimal solution obtained standard simplex branch bound code 
average optimality gap reasonable large scale complex problems exempli ed literature solution methods related capacitated network design models see example survey 
furthermore lagrangian bounding procedures approximate strong lower bound high accuracy fraction time required competitive commercial simplex code compute 
organized follows 
section recall formulation capacitated xed charge network design lagrangian relaxations 
special case shortest path relaxation shown ective experiments best knowledge described rst time 
section gives overview erentiable optimization methods implementation 
section analyze results experiments large set randomly generated problems various characteristics 
summarize propose extensions 
problem formulation relaxations directed graph set commodities represent demands satisfy origin destination pairs objective minimize sum arc transportation design costs charged arc 
arc transportation cost unit commodity denoted ij design cost denoted ij costs assumed nonnegative 
denote demand satisfy origin destination commodity arc capacity ij upper bound ij minfd ij may imposed amount ow commodity formulate problem introduce continuous ow variables ij re ect transportation decisions arc commodity design variables ij de ne sets outward inward neighbors node fj ag fj ag respectively 
model min ij ij ij ij ij ji ij ij ij ij ij ij ij ij ij ij ij integer ow conservation constraints ensure demands satis ed origin destination pair total incoming ow equals total outgoing ow transshipment node 
ow constraints associate lagrangian multipliers unrestricted sign 
constraints ensure arc capacities respected force ow commodity arc chosen design 
constraints achieve objective completely redundant 
signi cantly improve lower bounds obtained relaxations demonstrated computational experiments reported section 
constraints called weak strong forcing constraints respectively nonnegative multipliers associated 
rst lagrangian relaxation called shortest path ow relaxation obtained dualizing forcing constraints 
resulting lagrangian subproblem min ij ij ij ij ij ij ij ij ij ij subject constraints 
decomposes jkj shortest path problems problem variables solvable simple inspection cost signs 
lagrangian subproblem integrality property optimal value relax integrality constraints lagrangian dual max optimal value strong linear programming relaxation obtained dropping integrality requirements 
observation suggests interesting alternative approach compute lower bound seen particular case shortest path relaxation 
idea follows fact assuming design variables continuous may eliminated projecting space ow variables 
precisely assume integrality constraints dropped strong forcing constraints relaxed lagrangian way add capacity constraints ij ij ij resulting formulation easy see variables dropped optimal solution subproblem satisfy ij ij ij ij ij ij ij ij projection variables variables dualize capacity constraints nonnegative multipliers resulting subproblem min ij ij ij ij ij ij ij ij ij subject ij maxf ij ij minf ij decomposes jkj shortest path problems 
projected shortest path relaxation easily seen special case shortest path relaxation obtained setting ij ij ij ij computational results see section suggest projected variant shortest path relaxation ective usually converges faster optimal value lagrangian dual ordinary shortest path relaxation 
intuitively due fact projected variant produces better better subgradients drive search see section 
interesting special case projected shortest path relaxation arises xes equivalent remove valid inequalities formulation 
case lagrangian subproblem interpreted relaxation capacity constraints multicommodity minimum cost network ow problem transportation costs de ned ij ij ij easy see resulting lagrangian dual optimal value weak linear programming relaxation obtained problem formulation dropping integrality requirements strong forcing constraints 
weak bound computed exactly eciently bundle method relaxation capacity constraints suggested computational experiments reported section results probably weak serve basis branch bound methods 
second lagrangian relaxation study called knapsack relaxation dualizing ow conservation constraints 
lagrangian subproblem may written min ij ij ij ij subject constraints 
subproblem easily solved inspection signs costs problem min jaj ij ij ij ij optimal value continuous knapsack problem ij min ij ij ij ij ij ij lagrangian subproblem integrality property lagrangian dual max optimal value strong linear programming relaxation 
gives bound lagrangian dual shortest path relaxation provided solve lagrangian duals optimality 
relaxations theoretically equivalent provide lower bound relative performances terms computational eciency speed convergence fully characterized 
performances clearly dependent method chosen optimize lagrangian duals 
description methods implementation topic section 
erentiable optimization methods lagrangian duals related relaxations previous section may cast form erentiable optimization problems schematically described max concave erentiable function nite set represents real space dimension corresponding number multipliers nonnegative orthant 
value 
obtained solving lagrangian subproblem subgradient 
easily retrieved optimal primal solution subproblem 
shortest path relaxation subgradient takes form ij ij ij ij ij ij knapsack relaxation expression subgradient ij ji iterative methods designed solve problem sole help kind information 
tested bundle algorithm variants subgradient method 
algorithms start initial estimate solution iteratively repeat basic steps select tentative ascent direction select stepsize evaluate 
td corresponding subgradient eventually move current point td check stopping criteria 
despite sharing algorithmic structure bundle subgradient methods di erent viewpoints highlighted computational experiments reported section bundle approaches ascent methods move current point suciently better point subgradient methods update current point iteration newly obtained point worse smaller value function 
subgradient methods oblivious search history sense iteration current subgradient previous direction compute new bundle methods principle retain previously obtained subgradients form 
variants subgradient methods necessarily converge practical choice stepsize usually give guarantee reaching optimal solution bundle methods nitely converge maximizing polyhedral function give proof optimality obtained solution 
optimize lagrangian dual variants subgradient methods convey dual information bundle methods provide satisfactory primal information 
subgradient methods unstable performances highly dependent setting parameters best setting class instances may poor class point method diverge performances bundle methods stable depend parameters may signi cant impact 
subgradient methods easy implement computational cost dominated cost evaluating bundle methods require sophisticated implementation computational bottleneck practice computation direction substantiate points describing detail basic operations accomplished class methods 
subgradient methods mentioned subgradient methods move current point iteration denote current point direction th iteration respectively calculated tentative point current point moved 
guaranteed better terms value best value far corresponding retained denote value 
early versions subgradient algorithm subgradient compute direction 

quickly realized account direction previous iteration lead performance improvements 
general formula previous special case usually reported ective clever choices simplest choice called crowder rule xed value 
approach sophisticated fratta rule jj parameter selected computational experiments authors indicate usually constitutes choice 
rationale proper choice interval guarantees direction need hand tuned parameter may rule jjg jj geometrical arguments 
yields modi ed fratta rule jjg jj jj feasibility issues may arise subgradient algorithm tackled means projection 
actual direction projection active constraints direction obtained previous formulae costless case nonnegativity constraints 
tentative point projected ensure feasibility stepsize usually selected constraints consideration 
variant standard stepsize formula implementation iteration dependent scaling factor estimate maximum 
selection rule computing critical issues subgradient methods come adjustment parameters stepsize formula 
fundamental issues updated estimated 
typically divided constant factor time improved consecutive iterations 
note exponential decrease may theoretically cause subgradient converge non optimal point fact usual convergence theorems require stepsize converge zero slowly series diverges 
practice subgradient converge near optimal solution converges 
may provided typically computation lagrangian bound associated heuristic enumerative approaches original problem produce feasible solutions 
case 
usual way providing estimate multiply constant 
arguably coarse method sophisticated ones call parameters order dynamically adjust furthermore experience shown providing tight bound available may deteriorate performances method 
choice stepsize uenced parameters initial value practice parameters may impact critical followed shown computational experiments setting usually large part di erence obtaining performances having method diverge able improve initial estimate 
unfortunately settings appear provide reasonable performances problem classes depending particular instance parameters numerical values may vary 
extensive experimentation appears way guess reasonable value 
possible choice appears multipliers nonnegative 
calculating stepsize formula scalar product denominator may original projected direction sound theoretical arguments notion conditional subgradients computational results ered supporting choice projected direction 
results generally con rmed validity choice computational results necessary order establish 
especially true original development limited basic subgradient method extended general case 
suggested possibility projected versions computing direction results generally worse non projected variants 
far stopping criteria concerned theoretical stopping criterion subgradient jjg jj small 
sound criterion nding zero subgradient suces prove optimal 
constrained case nonnegative multipliers jjg jj replaced norm projected subgradient 
unfortunately stopping criterion applies practice require optimal solution lagrangian subproblem feasible original problem alternative criterion improved consecutive iterations 
order avoid stopping early chosen small 
result easy instances subgradient methods having performed maximum number iterations 
bundle methods main idea bundle methods information generated previously build model function maximized model drive search better point 
information transport property subgradient 


subgradient 
follows property iteration associate previously generated subgradient linearization error subgradient current point polyhedral concave function 
min called cutting plane model upper approximation 
set bundle 
maximizer trial point results known cutting plane algorithm su ers drawbacks particular maximizer unde ned subgradients known algorithm locality property tends generate iterates far current point near optimum 
bundle methods viewed stabilized versions cutting plane algorithm maximized subject stabilizing device enforces locality properties iterates usually addresses unboundedness problem 
implementation chosen standard stabilizing device direction selected quadratic problem qp 
max called trust region parameter 
problem attains unique nite solution direction constant stepsize 
may interpreted stepsize 
quadratic dual min jj jj optimal solution 
new trial point direction convex combination previously obtained subgradients prede nite stepsize 
bundle method requires solution iteration order compute trial point 
easily computational bottleneck approach especially number variables grows specialized qp codes instrumental order obtain ecient implementation 
especially true constrained case fact order ensure feasibility current point constraints added possibly making harder solve 
important feature qp solver possess extensively support reoptimization particular allowing line creation destruction variables 
called variable generation strategy shown critical order keep low cost solving qp applied lagrangian relaxation approach problems 
importance technique con rmed setting results section clearly show 
possibility reducing cost solving qp keep small size bundle 
usually subgradients discarded having inactive xed number consecutive iterations 
quite conservative rules best inactive iterations old subgradient turn useful selecting smaller values algorithm potentially useful information deteriorating speed convergence 
possible bundle maximum size done impairing convergence provided aggregated subgradient aggregated linearization error added bundle time active subgradients eliminated 
aggressive aggregation corresponding limited bundle size usually leads poorer performances terms convergence may signi cantly reduce cost algorithm time memory making comparable subgradient method 
illustrated computational results described section 
choice trust region parameter may regarded choice stepsize potentially critical performances bundle algorithm 
intimately tied choice moving current point trial point called serious step ss leaving unchanged called null step ns 
typically ss performed 
suciently improves 

xed parameter 
usual value rarely happens slightly better 
note increase predicted cutting plane model movement ss performed predicts behavior actual function 
hand ns performed poor model 
case new information gathered re ne model current point hopefully leading better bundle algorithm xed nite value execution 
small perform short steps yielding little improvement large perform ns consecutive ss 
increasing ss decreasing ns appears sensible choice 
case heuristics developed approximate restriction quadratic function choose maximizer provided larger turns case slope positive 
case typical check involves size aggregated linearization error rationale sort measure accuracy rst order information believed carry accurate information xed parameter 
decrease preferred hope better model nearer current point 
actual value selected heuristic rule similar increasing case 
value usually suggested 
appears sensible choice just shown inaccurate better information required 
experience value invariably leads sequence reductions long run seriously slows convergence phenomenon commonly referred ect 
higher settings prevent changing run proved quite ective preventing ects 
correct choice initial value important heuristics usually capable correcting wrong choices value depends scaling usually guessed observing run just instance class choosing order magnitude produced average heuristics 
furthermore kind scaling information useful appropriate setting stopping criterion discussed shortly 
counter ect sophisticated self adjusting rules choosing called long term strategies developed 
detailed description outside scope brie recall basic ingredients 
long term strategies idea quanti es maximum improvement expected step quantity increasing guessing reasonable improvement possible increase hard long term strategy inhibit decrease soft long term strategy turns produce small values 
obviously main issue reasonable improvement estimate may obtained 
implementation de ne initial target allow decrease stopping criterion algorithm satis ed target 
required relative accuracy stopping criterion bundle algorithm jjz jj user provided parameter 
easy see stopping criterion bundle algorithm require jjz jj small 
literature suggested test separate thresholds 
guring numerical value threshold jjz jj easy 
parameter convenient units measure fact choice usually just order magnitude larger chosen reduces critical parameters sought provides solutions required precision long bundle time converge 
furthermore form stopping criterion critical implementing long term strategies 
important characteristic bundle subgradient stopping criterion ective easy instances bundle usually reaches convergence maximum iteration limit exceeded 
mean bundle stops soon solution certifying optimality nding small norm costly 
cases bundle method may require signi cantly running time subgradient method simply able earlier 
computational results combination particular lagrangian dual problem resulting shortest path knapsack relaxations section erentiable optimization method subgradient bundle choice implementation criteria section yields relaxation methods possibly signi cantly di erent behavior performance 
characteristics rst quali ed prior rigorous comparison 
computational experiments performed guided objectives determine promising set parameters rules relaxation method di erent classes instances calibration phase compare relaxation methods number known bounding procedures linear programming relaxations analyzing performances respect various problem characteristics 
order achieve objectives run tests problem instances test tabu search procedure problem obtained network generator similar described 
provided target values jn jaj jkj generator creates arcs connecting randomly selected nodes parallel arcs allowed 
proceeds similarly create commodities 
costs capacities demands generated uniformly distributed intervals 
capacities costs scaled obtain networks various degrees capacity tightness relative importance xed costs 
ratios purpose capacity ratio ij xed cost ratio jkj ij ij capacities xed costs adjusted ratios come close user provided values 
general approaches network lightly capacitated congested increases 
close xed costs low compared transportation costs relative importance increases class class ii class iii class iii table classi cation instances problem dimension instances divided classes 
class consists instances commodities signi cantly number nodes class ii instances commodities usually number nodes 
class iii instances speci cally generated problem characteristic versus performance analyses easier 
divided subclasses iii iii corresponding node node problem instances respectively 
networks generated subclass combining arc densities roughly commodity densities roughly density ratio respect jn 
networks problem instances created combining values values 
problem instances created network classes ii represent various ratio values 
table summarizes characteristics problem instances classes problem dimension represented triplet jn jaj jkj 
number instances displayed parentheses infeasible problem instances discarded 
test problems generator class iii obtained authors 
subsection presents calibration results relaxation method classes instances second dedicated comparative analyses 
performance measures gap lower bound obtained method best available lower bound corresponds optimal value strong linear programming relaxation dicult problem instances technological limitations indicated section permit computation 
cpu time sun ultra workstation specint specfp mb ram memory 
code programmed compiled cc compiler option 
calibration methods ne tuned class independently analysis large number strategies parameter sets 
available space allow detailed presentation results 
summarize main ndings number aggregated performance measures support 
larger set analyses aggregated measures complete experimental results obtained authors 
aggregated measures obtained averaging performance measures instances class 
note averaging large sets instances varied characteristics appear coarse proves remarkably reliable detailed instance instance analyses revealed similar results tendencies method 
note maximum number iterations subgradient bundle methods xed 
number allows methods come reasonably close optimal value instances important mention signi cantly diminished bundle usually converges rapidly subgradient see section 
shortest path bundle method development bundle method shortest path relaxation requires careful management bundle size due large number multipliers jaj generated relaxation 
course experiments rapidly clear discarding inactive subgradients ecient resolution required maximum size bundle relatively small value 
conservative value maximum bundle size ram memory quickly consumed large scale problem instances 
resulted need access secondary memory devices yielding prohibitive computation times 
interesting question arises tradeo numerical accuracy computation time maximum bundle size varies 
interesting issues aim address experiments concern impact solution quality computation performance resulting utilization projected shortest path relaxation ordinary variable generation strategy line creation destruction multipliers long term strategies 
variants shortest path bundle strategy tested parameters bundle method especially calibrated extensively class instances 
typically longterm strategy setting instances leads slightly better results problems class noteworthy reasonable settings lead similar results clear demonstration robustness bundle method 
interesting emerged calibration experiments 
note commodity instances class solved bundle size xed ram memory limitation quickly exceeded occurs straightforward simplex approach solve strong linear programming relaxation explained section 
commodity instances require secondary memory devices 
instances treated maximum bundle size set signi cant impact convergence gaps slightly increase average remain order magnitude problems classes ii class iii instances 
cpu times reduced signi cantly average 
important concerns projected version shortest path relaxation signi cantly consistently improves ordinary version 
slightly smaller computational ort projected version decreases gap order magnitude problems classes ii class iii instances 
behavior constantly con rmed parameter settings selected bundle subgradient methods alike 
comparisons show computational bene ts variable generation strategy basically accuracy cpu times reduced average problems classes ii iii respectively 
may observe hard long term strategy slightly superior soft variant generally convergence computational evidence 
promising variant shortest path bundle method selected comparisons projected version maximum bundle size plus variable generation strategy hard long term strategy 
shortest path subgradient method indicated projected shortest path relaxation ective ordinary 
main issues remain designing methods shortest path relaxation selection rule computing direction crowder adjustment corresponding parameter 
adjustment stepsize particular 
setting 
adjustment testing settings promising halve consecutive iterations improvement kept larger minimum xed setting adopted remaining tests 
determination estimate tried upper bound provided tabu search heuristic rough estimate preferable cases 
choice projecting direction nonnegative orthant 
biggest surprises encountered running experiments performance modi ed fratta rule 
tested mentioned promising alternative formula 
best knowledge seldom 
experiments completely outperformed fratta rule suggested setting 
competitive crowder rule displays similar gap cpu results slight edge modi ed fratta rule 
note true direction projected 
important calibration phase concerns projection direction 
appears crowder modi ed fratta rules projecting direction improves gap order magnitude essentially computational ort order problems classes ii class iii instances 
interesting note options projecting direction require di erent optimal settings parameters 
expected run known jamming phenomenon method unable improve value objective function consequently parameter continually decreased 
result series short steps method jam far away optimal solution 
variants tested su er degree problem display similar tendency diverge 
fact experiments illustrated happen subgradient method suggested literature run carefully adjusting parameters 
example fratta rule suggested projecting direction calibrating determined promising classes produced average gaps orders magnitude larger obtained best variants 
clear demonstration lack robustness subgradient method settings reasonable settings try bad 
experiments selected variant shortest path subgradient method comparisons modi ed rule projecting direction 
knapsack bundle method compared shortest path relaxation knapsack relaxation relatively small number multipliers jn 
consequently exibility allowed devel opment bundle method optimization lagrangian dual 
particular variable generation strategy generally slightly inferior case slows convergence usual increases computation time 
keeping low maximum bundle size necessary 
interesting qualify tradeo accuracy computation time maximum bundle size varies small maximum bundle size reduces burden solving qp iteration signi cantly improve eciency cost losing ectiveness 
interesting issue ect long term strategies shown marginally superior shortest path bundle method 
light remarks experimented bundle sizes implementing hard long term strategy shown superior soft 
parameters bundle method calibrated class problem instances 
turned values selected promising case shortest path bundle method 
furthermore reasonable settings displayed similar results 
indication robustness bundle method 
size bundle appear uence signi cantly quality solution problems classes iii average gaps order respectively identical variants 
di erence little bit signi cant class ii problems average gaps vary approximately maximum bundle size decreased 
results show method capable generating high quality solutions maximum bundle size signi cantly reduced 
hand computation times cut factor problems classes iii class ii instances 
similarly shortest path bundle method long term strategy slightly improves performances bundle method 
represents fair balance numerical precision computation time selected comparisons strategy low maximum bundle size hard long term strategy 
knapsack subgradient method light previous experience making subgradient method converge applied knapsack relaxation appeared dicult task 
authors disappointing results knapsack subgradient method instances method diverging point bound computed worse weak linear programming bound 
better results obtained similar procedure uses variant crowder rule updating direction 
computational experiments claim fundamental di erence procedures described lies way multipliers initialized set assume values node potentials obtained solving jkj shortest path problems costs ij convex combination vector node potentials 
fact prior publication similar procedure tested improvement bound observed 
latest computational experiments attribute negative result bad adjustment parameters especially tests procedure multipliers initialized values node potentials resulting solution jkj shortest path problems costs ij ij ij choice experimentally shown better 
carefully adjusting parameters able obtain accurate results 
procedure unable method converge irrespective parameter setting con rming bad results reported 
note procedure tested knapsack bundle method shown inferior straightforward initialization multipliers 
similar method solving continuous knapsack problems tested shortest path relaxation improve bundle subgradient 
results calibration phase support previous subgradient method fratta rule consistently outperformed rules modi ed fratta crowder rules competitive slight edge 
variant selected comparisons modi ed fratta rule adjusted class class class ii class iii 
remaining issues adjustment determination estimate settled way shortest path subgradient method 
important point relatively results obtained knapsack subgradient method average gaps order class due careful adjustment strategy class problems 
values di erent ones mentioned lead disastrous results 
example modi ed fratta rule class iii average gap order obtained 
illustrates lack robustness subgradient method 
comparison section compare promising variants relaxation methods identi ed section bounding procedures 
characterize performances methods respect problem dimension relative importance capacities xed costs measured ratios 
bounding procedures solves weak linear programming relaxation formulated problem costs ij ij ij known competitive simplex solver cplex version 
uses option provides starting basis solving minimum cost network ow problem resulting relaxation capacity constraints switches dual simplex method 
wb solves lagrangian dual projected weak shortest path relaxation bundle method relaxations 
explained section optimal value lagrangian dual gives bound weak linear programming relaxation 
solves strong linear programming relaxation cplex option 
undoubtedly straightforward approach procedure relaxing forcing constraints introducing gradually formulation certainly ecient 
illustrates diculty solving large scale formulations standard methods highlights degeneracy issue faced simplex methods 
ss promising variant shortest path subgradient method identi ed calibration phase section 
sb promising variant shortest path bundle method identi ed calibration phase section 
ks promising variant knapsack subgradient method identi ed calibration phase section 
kb promising variant knapsack bundle method identi ed calibration phase section 
tables display results obtained methods applied problem instances classes 
problems grouped dimension number instances group written parentheses 
rst gure indicates average gap signals corresponding method unable solve instances group respect best value obtained gure shows average cpu time 
results demonstrate weak bound really weak certainly viable basis branch bound methods displays average gap roughly respect strong bound 
interesting note method wb converges exactly tolerance weak bound instances order magnitude faster standard simplex method large scale problems computational evidence 
bundle method capable extracting optimal primal solution converged especially appealing context designing heuristics problem nding feasible solution amounts solving problem 
number commodities increases resources time memory required method prohibitive point commodity instances solved due lack memory 
comparison implementations relaxation methods ecient especially large scale problem instances run fraction time taken provided size bundle controlled mentioned maximum bundle size shortest path bundle method fall memory problems 
knapsack methods due simplicity lagrangian subproblem approximately times faster shortest path methods 
note method sb remarkably ective displays best average gaps classes class iii 
performances procedure ks came surprise fastest average gaps comparable obtained method kb 
pointed strategy kb er better performance iterations say allowed maximum bundle size increased say 
fact instances bundle method achieves progress rst iterations iterations bring minor contribution terms bound quality 
subgradient method signi cantly di erent behavior converges slowly point substantiated 
relative gaps displayed tables point properly calibrated method problem class subgradient methods er similar performances terms solution quality range problem dimensions 
observation con rms reported minoux relative implementations settings proposed 
results show apparent problem dimension characterizes bundle implementations theoretical experimental investigations matter required fall outside scope 
analyzed performances methods respect importance xed costs capacities 
results detailed show gap weak strong bounds increases xed costs get higher capacities tight 
weak bound general easily computed easier compute instances large small true wb methods wb signi cantly faster 
procedure cpu time increases importance xed costs tightness capacities 
relaxation methods generally exhibit larger gaps xed costs important 
clear tendency emerges respect capacity ratio 
results demonstrate performances implementations lagrangian relaxation methods dependent relative importance problems wb ss sb ks kb average table method comparisons respect problem dimension class problems wb ss sb ks kb average table method comparisons respect problem dimension class ii problems wb ss sb ks kb average table method comparisons respect problem dimension class iii problems wb ss sb ks kb average table method comparisons respect problem dimension class iii xed costs capacities simplex methods 
previous analyses allow qualify performances various methods respect di erent problem characteristics discriminate relaxation simplex approaches 
may misleading suggesting relaxation methods equivalent instances relative accuracy similar edge method sb computation times improve signi cantly straightforward simplex implementation method ks fastest 
results obtained xed maximum number iterations methods maximum attained give indication speed convergence 
illustrate performance methods respect characteristic representative instances class dimension class ii dimension 
problem instance figures show evolution bound time cpu seconds relaxation strategies method dot gure corresponds iteration method 
rst instance clear bundle methods converge faster subgradient approaches fastest sb kb close second ss far slowest 
second instance hierarchy time knapsack relaxations closer 
gures highlight degeneracy problem faced method instances objective remains consecutive iterations 
consequently method converges slowly rst instance took seconds reach optimal value second seconds 
time sb ss kb ks comparison speed convergence problem class time sb ss kb ks comparison speed convergence problem class ii analyzed results comprehensive study bounding methods capacitated xed charge network design problems 
relaxation methods di ered dual formulation solved relaxing ow forcing constraints erentiable optimization approach subgradient bundle choice number important rules parameters 
linear programming relaxations solved competitive commercial simplex code included study order correctly characterize performances various methods 
experimentation performed large set test problems various characteristics 
calibration phase allowed gain precious insights behavior method class instances studied single critical parameters identify recommended strategies parameter settings 
experiments allowed substantiate fact complex implement bundle methods appear superior subgradient approaches converge faster robust relative di erent relaxations problem characteristics selection initial parameter values 
note codes generic bundle subgradient classes projects di erent research groups 
public domain obtained second author controlled experiments 
mentioned section bundle implementation uses standard quadratic stabilizing device 
choices possible 
instance generalization standard bundle methods proposed allows stabilizing device chosen large class functions 
methods eld erentiable optimization context lagrangian relaxation notable proximal level method analytic center method 
particular successfully applications closely related linear nonlinear capacitated lot sizing 
study demonstrated ective lower bounds may computed eciently large scale instances capacitated xed charge network design problem 
fraction time required standard simplex approach solve linear programming relaxation may solved methods attain high quality solutions 
prove prime importance optimal solutions sought branch bound methods 
fascinating research avenues inviting 
integration strong valid inequalities relaxations 
preserve structure lagrangian subproblems valid inequalities relaxed lagrangian way introduce large quantities multipliers subgradients related constraints 
implementation bundle method features allow ecient treatment relaxations size bundle subgradients limited user possible restrict direction nding subproblem set multipliers corresponding violated constraints revised dynamically ecient way 
experiments show features essential obtain results shortest path relaxation 
issue concerns determination tight feasible solutions particular bundle lagrangian heuristics appear promising 
combination relaxation methods heuristics reoptimization variable xing techniques branch bound procedures constitutes goal major research trusts 
acknowledgments want acknowledge orts helped testing code 
want anonymous referees comments helped improve presentation 
financial support project provided 
canada fonds 
qu ebec 
particular antonio sojourn centre research transportation montr eal supported network computing mathematical modeling partnership 
allen shetty 
generalization convergence result subgradient optimization mathematical programming 
balakrishnan magnanti 
network design chapter annotated bibliographies combinatorial optimization dell martello eds john wiley sons 
balakrishnan magnanti wong 
models planning capacity expansion local access telecommunication networks annals operations research 
unl uk 
capacitated network design polyhedral structure computation informs journal computing 
fratta 
improving relaxation methods modi ed gradient techniques mathematical programming study 
chang 
lower bounding procedures multiperiod telecommunications network expansion problems operations research 

bundle relaxation methods multicommodity capacitated fixed charge network design problems publication crt centre de recherche sur les transports universit de montr eal 
gendreau 
simplex tabu search multicommodity capacitated fixed charge network design problem publication crt centre de recherche sur les transports universit de montr eal forthcoming informs journal computing 
crowder 
computational improvements subgradient optimization symposia mathematica vol 
xix academic press london 
du gon vial 
lagrangian relaxation capacitated multi item lot sizing problem solved interior point cutting plane algorithm research report university geneva 
du gon vial 
improvements analytic center cutting plane method computational optimization applications 

solving semide nite quadratic problems nonsmooth optimization algorithms computers operations research 

dual ascent methods multicommodity flow problems ph thesis dipartimento di informatica universit di pisa 

generalized bundle methods technical report tr dipartimento di informatica universit di pisa 
gallo 
bundle type dual ascent approach linear multicommodity min cost flow problems technical report tr dipartimento di informatica universit di pisa forthcoming informs journal computing 

topological design telecommunications networks local access design methods annals operations research 

relaxations multicommodity capacitated network design problems publication crt centre de recherche sur les transports universit de montr eal 

bounding procedures multicommodity capacitated fixed charge network design problems publication crt centre de recherche sur les transports universit de montr eal 

multicommodity capacitated network design telecommunications network planning sans eds kluwer academics publishers 
geo 
lagrangean relaxation integer programming mathematical programming study 
gon vial 
solving nonlinear multicommodity flow problems analytic center cutting plane method mathematical programming 
gon vial 
decomposition erentiable optimization projective algorithm management science 
minoux 
graphes algorithmes paris 
held wolfe crowder 
validation subgradient optimization mathematical programming 

convex analysis minimization algorithms ii series comprehensive studies mathematics springerverlag 
yuan 
lagrangean heuristic branch approach capacitated network design problem research report mat department mathematics link oping institute technology 
kelley 
cutting plane method solving convex programs journal siam 
larsson str 
conditional subgradient optimization theory applications european journal operational research 

erentiable optimization chapter optimization nemhauser kan todd 
eds handbooks operations research management science volume nemhauser kan 
eds 
magnanti 
modeling solving facility capacitated network loading problem operations research 
magnanti wong 
network design transportation planning models algorithms transportation science 
minoux 
network synthesis optimum network design problems models solution methods applications networks 

minimization nonsmooth functionals ussr computational mathematics mathematical physics 

version bundle idea minimizing nonsmooth function conceptual idea convergence analysis numerical results siam journal optimization 

