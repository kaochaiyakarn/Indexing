machine learning 
kluwer academic publishers boston 
manufactured netherlands 
efficient extension mixture techniques prediction decision trees fernando pereira pereira research att com labs park avenue florham park nj yoram singer singer research att com labs park avenue florham park nj 
efficient method maintaining mixtures prunings prediction decision tree extends previous methods node prunings buntine willems shtarkov helmbold schapire singer larger class edge prunings :10.1.1.32.8918
method includes online weight allocation algorithm prediction compression classification 
set edge prunings tree larger node prunings algorithm similar space time complexity previous mixture algorithms trees 
general online framework freund schapire prove algorithm maintains correctly mixture weights edge prunings bounded loss function 
give similar algorithm logarithmic loss function corresponding weight allocation algorithm 
describe experiments comparing node edge mixture models estimating probability word english text show advantages edge models 
keywords mixture models decision prediction trees line learning statistical language modeling 
information theory willems machine learning helmbold schapire shows possible maintain efficiently mixture weights certain sets prunings decision prediction trees :10.1.1.32.8918
solution employs recursive algorithm updates weights possible pruning computes weighted decision prediction entire mixture 
results apply class node prunings obtained template tree removing entire subtrees rooted selected nodes 
node template tree associated predictor applications consider estimated empirical observations 
prediction pruning appropriate combination predictions leaf nodes pruning 
applications statistical language modeling degree nodes template tree may large nodes seldom visited 
empirical estimates prediction functions nodes may poor 
predictions nodes contribute prunings 
propose alleviate problem larger class edge prunings obtained deleting internal edges template tree descendants 
node pruning deletes edges leaving node subtrees edge pruning may pereira singer delete edges subtrees 
edge pruning eliminate poor performing descendants node keeping performing ones possible node prunings 
online weight allocation algorithm maintains efficiently correct weight possible edge pruning 
core algorithm new recurrence calculating mixture prunings rooted node template tree 
set possible edge prunings larger set node prunings show space time requirements new algorithm similar previous algorithms 
describe statistical language modeling experiments edge mixture models estimate probability word english news text showing edge models perform better node models kinds mixture models competitive back models katz standard tool statistical language modeling 
conclude discuss possible extensions 

preliminaries tasks examine online classification prediction 
time step 
learning algorithm receives instance xt outputs prediction example context sequence prediction compression algorithm instance xt xt suffix underlying input sequence xt fixed finite input alphabet 
prediction outcome yt observed results loss lt yt 
derive weight update rule bounded loss predictors framework introduced freund schapire generalizes line weight allocation algorithms markowsky wegman vovk littlestone warmuth cesa bianchi applied wide variety learning problems 
derivation depend precise form loss function requiring appropriate loss value provided learning algorithm prediction 
precisely generic learning algorithm bounded loss case maintains weights pool predictors 
time step uses normalized weight vector pt pt weight predictor pti 
predictor suffers loss lt described mixture loss suffered pt lt pti lt goal minimize cumulative mixture loss relative cumulative loss best predictor la mini li la pt lt li set predictors consider rest lt special form prunings fixed prediction tree 
applications compression language modeling speech recognition output algorithm output input probability distribution possible symbols actual symbol yt xt algorithm suffers loss lt log xt 
loss function fit requirements analysis generic algorithm just described turn efficient extension mixture techniques 
template tree edge extension 
boxes denote internal incomplete nodes prediction 
weight update method works case 
start extending generic algorithm analysis bounded loss case edge prunings show weight update applies probabilistic predictions logarithmic loss 
finite alphabet symbols 
template tree rooted tree edges leaving node labeled distinct elements maximum node degree 
clearly template tree identified prefix closed subset root node identified empty string identified edge labeled identified 
depth node just length corresponding string 
convenient represent edge labeled string 
clear context string represents node edge 
write indicate string prefix string identification nodes edges strings possibly improper ancestor template trees associate paths strings instances follows 
node template tree associated test instances possible outcomes 
instance tests performed sequentially starting test root node 
test node outcome test performed 
maximal path denoted classified belonging node convenient abbreviate formula asserting node maximal path noted definitions template tree maximal path allows incomplete template trees instance may map path prefix path instance 
generalizes definitions willems 
helmbold schapire allow complete ary trees 
sequence prediction compression algorithms test node depth returns xt decision trees sequence tests defined algorithm yielded breiman friedman olshen stone quinlan 
instance decision tree test may check numerical feature greater smaller threshold 
outcome pereira singer test edge labeled false labeled true followed 
simplicity follows concentrate sequence prediction case identify instance reversal suffix input sequence corresponding maximal path describe analyze edge prunings need associate certain quantities edge leave node edge occurs particular pruning 
simple way represent potential edge leaving node represented string appropriate symbol may correspond node template tree 
formally template tree define corresponding edge extension superset defined follows leaf node fixed new symbol 
leaf call internal edges inherits terminal additional edges terminal edges serve placeholders quantities associated missing edges just argued 
instance perform tests determine maximal path construction test outcome performed node edge belongs tree outgoing edge case maximal path show template tree extended version 
extended template trees analysis actual algorithm need represent explicitly additional terminal edges extended tree 
pruning extended tree extended tree obtained replacing zero internal edges terminal edges label deleting subtrees reached affected edges 
write indicate pruning clearly prunings correspondence prunings set edges denoted edges set terminal edges term 
instance evaluated follows path followed reaches terminal edge 
conventions introduced earlier denote maximal path final terminal edge 
size pruning written sum number internal edges number terminal edges terminal edges size pruning number edges option include exclude edge descendants pruning 
definition analogous willems 
helmbold schapire node prunings count number nodes option prune tree 
cases prior probability pruning simple function size assume fixed probability pruning edge node 
appendix gives details relationship edge prunings node prunings 
classification prediction node original template tree predictor instances terminating node 
terms extended template terminal edge pruning associated predictor source efficient extension mixture techniques 
sample edge prunings template tree 
node 
predictions observations matching nonterminal edges node come descendants node predictions observations matching terminal edges come node 
intuitively subtree rooted edge pruned observation suffixes represented subtree significantly informative shorter suffix represented source node edge 
clearly prunings share terminal edge agreeing predictions instance reaches terminal edge common subtree 
example shows prunings template tree 
terminal edges pruning drawn gray 
leftmost middle tree contain terminal edge reached instance predict instance 
foregoing discussion prediction time step pruning prediction function terminal edge reached predictions nodes depend instance different counts symbol context sequence predictions label associated instance decision trees breiman quinlan 
prediction pruning suffers loss 
loss time weight allocation algorithm possible edge prunings seek weight allocation algorithm cumulative loss la close possible loss lp best pruning 
efficient weight allocation algorithm describe efficient weight allocation algorithm competing best edge pruning prediction tree hedge algorithm freund schapire maintains non negative weight vector pereira singer predictors 
case predictors prunings 
denote unnormalized weight pruning time wt initial weight initial weights viewed priors prunings 
reasonable choice prior size pruning defined section 
prior monotonically decreasing size pruning 
generally define prior prunings assignment pruning probabilities template tree edges 
prior probability pruning probability pruning generated recursive process 
starting root node edge leaving node randomly decide prune subtree reached edge tossing coin bias edge pruning probability 
subtree kept process recursively applied root subtree 
recursive prior enables efficient evaluation mixture weights results worst case loss bounds relatively simple compute 
earlier experiments node prunings pereira singer tishby suggest exact value pruning prior important language modeling large training sets detailed analysis role prior worthwhile 
time step pruning suffers loss lt weight updated multiplicative rule wt lt ls ls learning rate 
hedge uses normalized weight vector wt combine predictions predictors 
algorithm maintains efficiently weight vector wt normalization wt freund schapire proved algorithms form suffer loss min ln ln lp min ln lp bounded loss functions 
loss weight allocation algorithm scales linearly loss best submodel 
na implementation weight vector contribution normalization computed pruning separately 
approach clearly infeasible huge number possible prunings especially large 
adopt techniques willems 
helmbold schapire case 
techniques require time update weights number edges path associated main idea algorithm extend technique helmbold schapire computing certain sums possible node prunings template tree similar sums ranging edge prunings template efficient extension mixture techniques tree 
fixed template tree function edges interested computing term term set terminal edges defined earlier 
show sums form main tool forming predictions new instances 
fix template tree assume complete node leaf children 
show extend analysis incomplete template tree 
node tu subtree rooted convention representing nodes edges strings node edge tu identified string tu string function edges define function tu term 
lemma gives efficient method computing particular method computing 
lemma generalizes lemma helmbold schapire turn generalizes specialized results buntine lemma willems 
appendices iii iv 
recall identify edge string path leading edge turn identified target node edge 
lemma 
node complete template tree 
leaf 
internal node proof case follows definition leaf single node edge extension 
case internal node consider special case 
pruning tu falls cases outgoing edges terminal edges edge terminal edge terminal edges internal denote subtree rooted child may empty corresponding edge terminal definition easy see general binary alphabet 
decomposing sum possible prunings cases binary alphabet written term term pereira singer ranges prunings tu 
case general alphabet define internal edge ip ip ip 
ip possible pruning corresponds unique binary vector 
decompose terms corresponding possible values yielding general case ip ip second equality derived applying times identity aibj 
template tree incomplete internal nodes outgoing terminal edge 
incomplete node terminal edge ip 
single pruning pu tu containing single edge 
equation case takes form straightforward albeit tedious modify lemma general case incomplete template trees 
briefly follow line inductive proof applying recursion node subtrees contain edge 
lemma adaptation derivation helmbold schapire compute losses prunings weights weight normalization 
weight step edge defined li 
efficient extension mixture techniques assume terminal edge pruning condition equivalent assumption conclude li terminal edge pruning product loss factors time steps predictor associated source node 
recall li allocation algorithm weight update factor pruning time step li term term wt 
weight normalization prunings term form computed lemma 
compute weight update edge need maintain terminal edges lemma case 
loss incurred initially edge straight forward induction lemma shows priors prunings add correctly required 
normalized weights computed directly wt node 
shows weight update pseudo code complete template trees summarizing results analysis 
basic algorithm improved ways 
practical applications compression statistical language modeling shape template tree defined advance instance complete tree depth actual tree data structure built fly new instances looked template 
internal node tree data structure lack outgoing edge instance reaching edge encountered 
case loss incurred loss associated parts template built terms associated losses ignored recursive calculation weights second calculation iterates internal node reality children affected update satisfying xt wt wt allowing update nodes reached time step performed time independent follows li pereira singer input complete template tree access losses node predictors parameter initialize edges 
update edge weights path path update subtree weights xt path lt xt terminal edge wt wt internal edge 
pseudo code weight allocation algorithm 
wt wt wt wt 
time required update entire tree instance depth template tree template tree infinite length input instance 
bounded loss functions loss bounds hedge algorithm apply giving theorem template tree 
xt sequence instances losses lt associated pruning form 
loss hedge line weight allocation algorithm ln ln lp ln ln lp pruning furthermore running time algorithm time step linear 
weight allocation algorithm prediction 
prediction node time step wp weighted mixture predictions prunings rooted time step assuming complete template tree straightforward adaptation argument lemma gives recur sion wp wp pu xt terminal edge wt internal edge efficient extension mixture techniques prediction entire mixture denoted computed normalizing wp total mixture weight applying appropriate function normalized prediction wp 
function depends learning rate need bounded discussed detail cesa bianchi 

compression applications appropriate loss function pruning negative log probability logloss input sequence determines code length sequence compressed optimal coder instance arithmetic coder governed input probabilities pruning bell cleary witten 
loss incurred time log xt 
weight update algorithm prediction mixing formula set lt xt easy see wp 
prediction mixture simply 
time space complexity edge algorithm logloss remain 
lemma get bound predictions mixture 
theorem template tree 
sequence instances lp lt loss pruning sequence lt 
logloss weight allocation algorithm ln lp ln lp proof proof direct application proof technique technique 

get log loss mixture sequence 
ln ln ln ln ln ln equality fact initial weights set 
lemma holds pruning wt 
get bound logloss combining fact pereira singer 
experiments algorithm just discussed build statistical language models estimate probability word follows sequence words english text 
models essential large vocabulary speech recognition jelinek 
contrast character models text compression word models speech recognition large alphabets words dictionary 
addition typical applications model built batch mode large collection training text applied test task online modification 
experiments show online algorithm competitive performance widely batch language modeling method katz large vocabularies 
experiments suggest edge pruning considerable advantages node pruning large alphabets required speech recognition 
experiments compare edge node mixture algorithms baseline back method katz training test material derived nist supplied north american business news nab corpus english news text 
text tokenized words 
alphabet algorithm consists frequent words plus unknown word symbol words mapped experiments memory limits forced remaining experiments 
model performance evaluated log likelihood assigned model test material 
training text consisted words words training material size training sets determined memory limits 
cases training data created random drawing repetition sentences nab corpus 
log conditional probability word prediction loss 
store working memory subtree template containing prediction suffixes training data 
suffix corresponding new branch template tree observed allocate corresponding nodes initialize counts forming predictions zero 
word probabilities estimated online counts describe 
node predictor assign conditional probability word word observed context represented node 
possible methods assign word probabilities 
modified laplace rule character text compression turned inadequate large alphabets 
rule estimates conditional probability observing symbol tree node number times symbol observed node number times node visited 
regret additional loss estimator scales linearly size alphabet performance poor small fraction entire alphabet observed node 
common method estimating probabilities unseen words due turing 
turing estimator batch uses number words appeared estimating conditional probability unseen words 
efficient extension mixture techniques estimation techniques shown perform natural data sets easily modified line prediction problems witten bell 
estimates probability symbol node number different symbols observed node 
second method approximation turing formula estimates probability symbol number different words observed node 
scheme requires fall back estimate described detail witten bell 
stress goal experiments compare different mixture techniques possible online estimation techniques node predictors 
confidence advantage edge mixtures node mixtures increased fact demonstrated unseen event estimators see 
template tree depth larger training set larger template tree depth smaller training set 
xt pnode xt probability xt induced edge node mixture model 
shows differences average logloss models words ti ti log xt log pnode xt ti algorithms operate online mode training data plot shows results estimation methods 
plot gives results depth template tree larger data set second plot gives results depth tree 
cases initial predictions node mixture better edge mixture eventually edge mixture achieves better performance starting words depth tree words depth tree 
results show similar trends estimation methods consistent theoretical analysis 
set edge prunings strictly contains set node prunings initial weight pruning belongs sets smaller member edge pool prunings 
run number observations relatively small initial weights prediction trees large influence logloss 
data processed influence initial weights crucial 
addition larger collection edge prunings may achieve lower loss node pruning takes lot data adjust weights favor better prunings 
note time space needed build edge node language model practically see appendix 
specifically corpora described section takes cpu minutes mhz mips processor build model faster standard statistical language modeling algorithms 
major drawback online algorithms memory requirements gb experiments described 
reason limit training set vocabulary size experiments 
describe possible extension overcomes memory requirements section 
pereira singer log loss difference estimation method estimation method log loss difference estimation method estimation method 
difference log loss edge node mixtures nab corpus depth left depth right template trees 
table 
perplexity results data nab corpus 
model perplexity back trigram node mixture edge mixture node mixture adaptation edge mixture adaptation compared performance mixture models standard back model katz 
model estimates probability word ngram seen training empirical frequency training discounted turing formula 
discounting probability mass available unseen grams 
mass distributed unseen grams model probabilities grams obtained grams dropping word 
fair comparison took unseen test set words measured loss fixed mixture model built training set test set 
mixture model allowed adapt test set 
table summarizes test set perplexity measure evaluating statistical language models simply exponentiation average logloss test data 
second estimation method node predictors 
calculated perplexity models allowed adapt weights test data 
mixture models created online algorithm achieve performance close back model uses turing estimator online approximation 
furthermore edge mixture model achieves small significant improvement perplexity node mixture model 
large sizes training test sets perplexity differences statistically significant 
efficient extension mixture techniques 
illustration equivalence edge node prunings prunings tree shown left encoded node prunings tree right 

efficient method maintaining mixtures prunings prediction decision tree extends node prunings earlier larger class edge prunings time space complexity previous mixture algorithms trees 
method extended ways 
particular sophisticated data structures may possible maintain efficiently edge prunings unbounded depth trees maximal paths determined input sequence 
generally method applicable maintain mixtures probabilistic models product distributions represented bayesian networks 
acknowledgments dana ron rob schapire anonymous reviewers useful comments 
appendix equivalence edge node prunings claimed edge prunings generalize node prunings 
node pruning complete ary tree simulated edge pruning removes edges internal node leaf node pruning 
clearly edge prunings node prunings template tree 
clarify connections types prunings calculate precisely numbers possible prunings 
denote nn number distinct node prunings complete ary tree depth ne number distinct edge prunings 
complete ary tree depth complete subtrees rooted pereira singer children nodes root nn distinct node prunings 
node pruning root node root node child subtrees selected nn possible ones 
total number prunings nn nn edge prunings possible subsets edges deleted root node subtrees point 
zi indicate ith outgoing edge root deleted 
ne zi 
zk ne zi ne zi ne ne nn simple induction recurrences nn nd shows nn ne encode edge prunings template tree node prunings need increase depth template 
original template edge prunings enlarged template node prunings 
define prediction associated node reached instance instance xn xn equal prediction node reached xn augment instance symbol set possible edge prunings equivalent set possible prunings excluding root node 
construction illustrated 
bell cleary witten 

text compression 
englewood cliffs new jersey prentice hall 
breiman friedman olshen stone 

classification regression trees 
wadsworth international group 
buntine 

theory learning classification rules 
unpublished doctoral dissertation university technology sydney 
cesa bianchi freund helmbold haussler schapire warmuth 

expert advice 
journal association computing machinery 
markowsky wegman 

learning probabilistic prediction functions 
proceedings workshop computational learning theory pp 

san francisco california morgan kaufmann 
freund schapire 

decision theoretic generalization line learning application boosting 
journal computer system sciences 
efficient extension mixture techniques 

population frequencies species estimation population parameters 
biometrika 
helmbold schapire 

predicting nearly best pruning decision tree 
machine learning 
jelinek 

statistical methods speech recognition 
cambridge massachusetts mit press 
katz 

estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics signal processing 


performance universal coding 
ieee transactions information theory 
littlestone warmuth 

weighted majority algorithm 
information computation 
pereira singer tishby 

word grams 
yarowsky church eds proceedings third workshop large corpora 
somerset new jersey association computational linguistics 
quinlan 

programs machine learning 
san francisco california morgan kaufmann 
rissanen 

complexity strings class markov sources 
ieee transactions information theory 
rissanen langdon 

universal modeling coding 
ieee transactions information theory 
ron singer tishby 

power amnesia learning probabilistic automata variable memory length 
machine learning 
singer 

adaptive mixtures probabilistic transducers 
neural computation 
vovk 

aggregating strategies 
proceedings third annual workshop computational learning theory pp 

san francisco california morgan kaufmann 
weinberger lempel ziv 

universal coding finite memory sources 
ieee transactions information theory 
weinberger merhav feder 

optimal sequential probability assignment individual sequence 
ieee transactions information theory 
weinberger rissanen feder 

universal finite memory source 
ieee transactions information theory 
willems shtarkov 

context tree weighting method basic properties 
ieee transactions information theory 
witten bell 

zero frequency problem estimating probabilities novel events adaptive text compression 
ieee transactions information theory 
