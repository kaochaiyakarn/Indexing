extended version kernel text classi cation 
proceedings ijcnn 
kernel text categorization olivier eric universit lumi ere lyon avenue pierre es france cedex isc umr cnrs bd cedex eric univ lyon fr presents techniques text categorization 
new algorithms particular new svm kernel text categorization developed compared usual techniques 
kernel leads natural space elaborating separations euclidian space frequencies inverse frequencies distance space usual pseudo distance distributions 
give application recognition author text put relief kernel classi cation distributions 
experimentally discuss eciency algorithms depending precision estimation frequencies possibility building statistical bounds error 
experiments underconstrained problems 
classes texts call text categorization task determining class text learning labeled training set 
include language recognition topic recognition 
restricted study algorithms grams generality kind sequences discrete alphabet see example applications biology generalization dimension robustness noise approaches dictionaries :10.1.1.11.6124
usual methods nn dissimilarity measures conclude roughly ecient method svms :10.1.1.11.6124:10.1.1.11.6124:10.1.1.11.6124
con rm comparisons introduce new techniques new kernel 
de nitions alphabet gram sequence elements gram letter grams called bigrams grams called trigrams 
set words set maximal inclusion grams punctuation space 
calls pro le texts sequence grams text decreasing order frequency frequencies :10.1.1.11.6124
text categorization parts areas text categorization rst uses dissimilarity measures second encoding classical learning algorithms 
text categorization dis similarity measures algorithms text categorization distances generally similarities dissimilarities 
methods rely nearest neighbor algorithms 
diculty nearest neighbors approach de nition distance 
simplest oldest consists building pro les class text dissimilarity measure ct 
pro les ct distance de ned ct rp min rp dmax jxj absolute value rp gram pro le rank pro le belongs dmax nmax dmax 
possible distance leibler kl dissimilarity measure kl ng log sum taken grams texts frequency gram text avoid strong penalization unseen grams half frequency gram occur added frequency done close laplace smoothing solution suggested anonymous referee jensen shannon divergence leibler dissimilarity mean :10.1.1.11.6124
information 
possibility cosine dissimilarity measure uses centered space mean frequency vectors translation :10.1.1.11.6124
cos ng ng ng ng ng ng ng chose dissimilarity measure dissimilarity 
ng distance ng ng ng ng practical experiments 
replace ng ng ng ng continuous extension 
classi cation methods encoding approach consists encoding documents vectors order classify points allows classical methods backpropagation neural networks sparseness vectors see experiments backpropagation lists support vector machines svms nearest neighbors directly previous dissimilarity measures decision trees nite set words subwords essential parts words example de ne number frequency :10.1.1.11.6124
vector associated nite set words set words included considered texts set grams number replaced di erent functions lists di erent possibilities :10.1.1.11.6124
possible consider signi cant variables ones 
di erent solutions possible kind data famous information gain criterion see :10.1.1.11.6124
experimental results show possible keep variables done sequel :10.1.1.11.6124:10.1.1.11.6124:10.1.1.11.6124
new positive de nite kernel svm encoding allows lots training algorithms particular svms see :10.1.1.11.6124
svms way de ne exp dissimilarity measures suggested 
experimented exp 
conjecture function exp positive de nite kernel 
attempt proving conjecture corollary prove :10.1.1.11.6124
positive de nite deduce 
negative de nite 
theorem conjunction theorem prove squared kernel positive de nite 
unfortunately interested reader verify mistake argument 
intuitively sounds reasonable mercer condition veri ed prove completely 
new kernel disposal advantages pseudo distance natural linear svms distance euclidian distance space frequencies inverse frequencies look rbf radial basis function separations space classical distance distributions 
learn compact representation data kernel matrix number texts training set svm rbf 
hyperparameter chosen results showing fat shattering dimension bounded function lipschitz coecients lipschitz coecients depending weights :10.1.1.11.6124
experiments rst benchmark results lots di erent values 
rbf networks text categorization case svm rbf network encoding texts dissimilarity example 
explained corresponds linear separation feature space 
method successfully tested 
algorithm summarized family labeled texts training family texts classi ed 
matrix belongs class 

matrix exp matrix resulting adjunction column right 

matrices exp matrix resulting adjunction column right 

weight matrix non unique choose multiplication pseudo inverse chosen minimal norm 
problem solved decomposition algorithm numerical stability 

classify class argmax 
explains partly behavior svms text categorization capacity treat dimensions having select relevant variables :10.1.1.11.6124:10.1.1.11.6124:10.1.1.11.6124
notice rbf particular kernel veri es property training set translated kernel matrix size account information number texts learning set 
di erences similarities previous svm algorithm rbf reason sparse 
favor svm 
rbf minimize geometrical margin svm pseudo inverse algorithm looks minimum norm solution coecients supposed small backpropagation problems local minima resulting classi er expected small empirical error 
expect low fat shattering dimension low empirical error bounds practical ones svm see de nitions detailed bounds :10.1.1.11.6124
writer recognition working large samples success rate evaluated leave case author recognition training set small classes authors texts 
set french books written known writers engels fourier france gautier hugo pascal rousseau sand zola 
writers translated languages 
complete list titles long listed les asked email authors 
fact texts formatted way hasn corrected considered supplementary diculty algorithm notice formatting correlated author 
texts come abu site cedric cnam fr abu nationale de france www bnf fr 
experimental results grams table 
table result rst line values algorithm success rate rbf kernel rbf kernel multiclass svm kernel multiclass linear svm svm kernel nn dissimilarity linear svm nn kl dissimilarity tests implementations octave see www che wisc edu octave description interesting free clone matlab 
source codes asked email authors 
call multiclass svm svm designed multiclass categorization de ned 
worth putting relief case high dimensionality classes svm signi cantly better usual method consisting combining svms suggested :10.1.1.11.6124
svm multiclass signi cantly better svm linear svm multiclass signi cantly better linear svm 
experiments gives results denoting di erence con dence di erence con dence rbf svm multiclass svm multiclass svm svm nn notice experiments ones concern texts large nice approximation frequencies :10.1.1.11.6124
experiments done case 
language recognition working small samples case success rate evaluated validation disjoint part data set 
previous benchmark conclude quickly rbf kernel ultimate algorithm text categorization 
multiclass version svms looks powerful rbf faster simple implement 
experiments focus algorithms rbf eciency shown previous benchmark nn simplicity eciency case see experiments widely practical applications 
experiments java implementations jama matrix package 
java source codes asked email authors 
task consists recognizing language written text 
languages french arabic english spanish german 
known easy task complicate small parts texts 
detail comparison particular set samples bytes samples bytes samples bytes bytes average big texts kb de ne pro les come van noord page rug 
nl list html short samples languages arabic ones built html pages german ones stochastic language identi er www com french ones book www org english spanish ones corpus 
datasets asked email authors 
testing set samples bytes sr meaning success rate get results table 
table result parenthesis got pro les class computed subparts training set pro le computed class training set 
leads better results rbf 
trick doesn experiments shorter samples 
algo 
sr 
sr 
sr 
nn kl nn rbf rbf samples bytes learning set study precisely uence gathering learning texts rbf nn 
results reported table 
table means training texts gathered sets texts gathered texts class training set gathered gathered maximal 
keeping small preserves variability training set larger leads precise pro les 
algo 

sr sr sr rbf rbf rbf rbf rbf rbf nn nn kl nn nn nn nn kl nn nn nn kl case small testing samples kl remains better kl unable short learning samples illustrated case non gathered learning samples 
hyperparameter rbf learning easily chosen previous benchmark classi cation authors success rate constant wide range empirical success closely related generalization success case bytes strings ciency depending gathering leads dicult hyperparameters 
datasets frequencies precise doesn mean depend class depend author language topic time writing nally sum previous results results rbf svm mc svm mc svm svm nn llsf nb svm mc multiclass svm svm classical svm llsf described neural nets svm famous algorithm induction trees see text categorization nb naive bayes algorithm see :10.1.1.11.6124:10.1.1.11.6124:10.1.1.11.6124
notice rbf svm mc signi cant terms performance keep comparison rbf advantage faster learning easier implement 
part result restricted case relatively small learning samples 
results resulting linear separations reproducing kernel hilbert space associated symmetrized distance assuming kernel positive de nite 
suggests space natural place study separations classes distributions 
notice experimental results case overconstrained problems put relief fact euclidean distance nearly ecient distance 
experiments histogram image classi cation suggest true underconstrained problems :10.1.1.11.6124
course benchmarking generalization experiments benchmarks 
case precise frequencies small parts text testing set nn better rbf kl provided learning set large compute precise frequencies 
results markov models languages compared results suggest markov models trained kb language languages nearly error rate nn kb language languages dicult ones error rate random classi er languages languages nn adapted task markov models 
tested version nn uses grams markov models order ecient nn require computations bigger pro les markov models 
nn eciently keeping pro le class true rbf nn advantage robustness gathering pro les hyperparameter assumption nn generally nn ecient solution classify small samples texts 
choice distance interesting question dissimilarity ct isn mathematically justi ed kl measure diculties small learning samples implies particular cases unseen grams experimental bad behavior small samples 
prefer dissimilarity didn give signi cantly worst results distances kl ct cosine precise frequencies better ones recall small testing sets kl gave best results 
experiments con rm point :10.1.1.11.6124
underline detailed study shows algorithms errors come unbalanced classi ers class invading suggests algorithms helping handicapped classes typically boosting give results 
object 
put relief fact time complexity case text mining especially experiments underconstrained problems behavior problems 
example dimension large 
suggests remove inputs order get optimal results decision trees example slow :10.1.1.11.6124:10.1.1.11.6124:10.1.1.11.6124
hand inputs sparse experiments show implementations backpropagation neural networks computational time :10.1.1.11.6124
svm recognition time complexity linear product number support vectors dimension rbf recognition time complexity linear product number support vectors dimension polynomial kernels 
svm rbf probably suitable learning sample large 
experiments learning samples small medium second part 
advantage algorithms decreasing case time complexity 
suggests simple algorithms remain better large samples 
applications web mining learning samples usually large 
fast recognition time important dimension reduction solution 
hash tables linear kernels fast 
computational considerations learning tasks large learning samples short recognition time rbf svm linear kernels probably outperform rbf svm gaussian kernel 
large learning samples rbf svm probably anyway articles reported results large training samples report results better simple algorithms 
indexed documents com sake intuitive understanding resulting classi er reduction dimension 
consider point view error rate 
pletely di erent techniques 
puts relief fact study relevant large learning samples involved thousands examples 
small recognition time necessary indexation possible rbf svm linear kernels backpropagation implementation lists outperform decision trees fast results decision trees remain fastest solution 
fast implementations support vector machines place svm rbf point view time complexity 
results support vector machines sensitive precision implementation quadratic problem 
results interior points sequential minimization cost function small derivatives near limit point algorithms rely heuristics implementations di erent 
hand algorithms pseudo inverse old known kind bias introduced comparison due fact rbf eciently implemented svm applied experimental implementation sequential optimization algorithm 
better implementations support vector machines modify lines 
andr multiclass svm fruitful discussions 
grateful sch olkopf 
:10.1.1.11.6124
bartlett sample complexity pattern classi cation neural networks size weights important size network ieee transactions information theory 
berg 
christensen harmonic analysis semigroups theory positive de nite related functions springer 

gram text categorization 
symposium document analysis information retrieval las vegas cover thomas elements information theory 
wiley series telecommunications 
new york chapelle haffner 
vapnik support vector machines histogram image classi cation ieee transactions neural networks vol 
crawford 
fung 

tong classi cation trees information retrieval machine learning proceedings eighth international workshop morgan kaufmann pp dunning statistical identi cation languages computing research laboratory technical memo new mexico state university las cruces new mexico 
estimation probabilities essay modern bayesian methods mit press elisseeff new multiclass svm uniform convergence result 
ijcnn huffman acquaintance languageindependent document categorization grams trec proceedings joachims text categorization support vector machines learning relevant features machine learning ecml tenth european conference machine learning pp :10.1.1.11.6124:10.1.1.11.6124:10.1.1.11.6124
miller shen liu nicholas performance scalability large scale gram information retrieval system journal digital information mehran sahami thesis machine learning improve information access ph computer science stanford university sibun reynar language identi cation examining issues :10.1.1.11.6124
symposium document analysis information retrieval pp 
las vegas kernel image classi cation accepted icann vapnik nature statistical learning springer verbeek information theoretic approach nding word groups text classi cation institute logic language computation :10.1.1.11.6124
yang liu re examination text methods sigir proceedings nd annual international acm sigir conference research development information retrieval august berkeley ca usa :10.1.1.11.6124
acm yang pedersen comparative study feature selection text categorization international conference machine learning icml :10.1.1.11.6124
