gradient flow recurrent nets difficulty learning long term dependencies sepp hochreiter fakultat fur informatik technische universitat munchen munchen germany informatik tu muenchen de bengio dept informatique recherche op universit de montr eal cp succ 
centre ville montr eal qu ebec canada iro umontreal ca paolo frasconi dept systems computer science university florence di santa marta firenze italy paolo dsi jurgen schmidhuber idsia corso lugano switzerland juergen idsia ch kremer kolen eds field guide dynamical recurrent neural networks 
ieee press recurrent networks chapter principle feedback connections store representations input events form activations 
widely algorithms learning put short term memory take time feasible especially minimal time lags inputs corresponding teacher signals long 
theoretically fascinating provide clear practical advantages say backprop feedforward networks limited time windows see chapters 
conventional algorithms computation complete gradient back propagation time bptt real time recurrent learning rtrl error signals flowing backwards time tend blow vanish temporal evolution backpropagated error exponentially depends size weights :10.1.1.41.7128
case may lead oscillating weights case learning bridge long time lags takes prohibitive amount time 
follows give theoretical analysis problem studying asymptotic behavior error gradients function time lags 
section consider case standard rnns derive main result approach proposed 
section consider general case adaptive dynamical systems include standard rnns recurrent architectures different connectivities choices activation function rbf second order connections 
analysis reported show undesirable situations necessarily arise system unable robustly store past information inputs gradients vanish exponentially 
section shortly review alternative optimization methods architectures suggested improve learning presence long term dependencies 
exponential error decay gradients error function results going prove hold regardless particular kind cost function long continuous output regardless particular algorithm employed compute gradient 
shortly explain gradients computed standard bptt algorithm see chapter details analytical form better suited forthcoming analyses 
error time denoted 
considering error time output unit error signal ffi net non output unit backpropagated error signal time ffi net ij ffi net ij gamma unit current net input net activation non input unit differentiable transfer function ij weight connection unit corresponding contribution jl total weight update ffi gamma learning rate stands arbitrary unit connected unit error path integral suppose fully connected net non input unit indices range focus local error flow output unit arbitrary unit see analysis immediately extends global error flow 
error occurring time step propagated back time gamma time steps arbitrary unit time scales error factor ffi ffi net gamma kv gamma net ffi ffi lv gamma order solve equation expand unrolling time done example deriving bptt 
particular denote index generic non input unit replica network time obtain ffi ffi gamma gamma gamma net gamma net proof induction 
immediately shown local error vanishes global error vanishes 
see compute ffi ffi denotes set output units 
intuitive explanation equation fi fi fif net gamma fi fi fi largest product increases exponentially gamma gamma 
error blows conflicting error signals arriving unit lead oscillating weights unstable learning error blow ups bifurcations see 
hand fi fi fif net gamma fi fi fi largest product decreases exponentially gamma gamma 
error vanishes learned acceptable time 
logistic sigmoid function maximal value 
gamma constant equal zero size gradient fi fi fif net gamma fi fi fi takes maximal values gamma gamma net size derivative goes zero fi fi fiw gamma fi fi fi fi fi fiw gamma fi fi fi absolute maximal weight value max smaller 
conventional logistic sigmoid transfer functions error flow tends vanish long weights absolute values especially training phase 
general larger initial weights help seen fi fi fiw gamma fi fi fi relevant derivative goes zero faster absolute weight grow weights may change signs crossing zero 
likewise increasing learning rate help change ratio long range error flow short range error flow 
bptt sensitive distractions 
note summation terms equation may different signs increasing number units necessarily increase error flow 
weak upper bound scaling factor slightly extended vanishing error analysis takes number units account 
gamma formula rewritten gamma gamma net weight matrix defined ij ij outgoing weight vector defined iv iv incoming weight vector defined ki ki diagonal matrix order derivatives defined ij ij net 
transposition operator ij element th column th row matrix th component vector matrix norm compatible vector norm define max max gamma max fjx jg kxk get fi fi fix fi fi fi kxk kyk jf net kf max obtain inequality fi fi fi fi fi ffi ffi fi fi fi fi fi max gammas kw kw kwk gammas gamma max gammas inequality results kw kw ke kw ke wk ke unit vector components th component 
note weak extreme case upper bound reached kf take maximal values contributions paths error flows back unit unit sign 
large typically result small values kf confirmed experiments see 
example norms max jw rs kxk max jx max logistic sigmoid 
observe jw ij max max result exponential decay setting obtain fi fi fi fi fi ffi ffi fi fi fi fi fi gammas refer hochreiter thesis details 
dilemma avoiding gradient decay prevents long term latching bengio analysis problem gradient decays generalized parameterized dynamical systems including second order recurrent architectures 
main theorem shows sufficient condition obtain gradient decay necessary condition system robustly store discrete state information long term 
words weights state trajectory network latch information hidden units represent longterm dependencies problem gradient decay obtained 
long term gradients decay exponentially difficult learn longterm dependencies total gradient sum long term short term influences short term influences completely dominate gradient 
result decomposition state space hidden units types regions gradients decay possible robustly latch information 
denote dimensional state vector time example vector net net considering standard order recurrent network gamma map state time gamma autonomous inputs dynamical system 
decomposition expressed terms condition jm robust latching possible jm gradient decay jm norm jacobian matrix partial derivatives map analysis focuses basins attraction attractors domain manifolds domain 
particular analysis concerned called hyperbolic attractors locally stable need fixed points eigenvalues absolute value 
state function remains certain region space versus region presence perturbations noise inputs possible store bit information arbitrary durations 
regions jm shown arbitrarily small perturbations example due inputs eventually kick state basin attraction see sample trajectory right 
regions jm level perturbation depending state remain basin attraction gradually get closer certain volume attractor see left 
reason call condition information latching allows store discrete information arbitrary duration state variable 
unfortunately regions jm latch information show gradients decay 
argument similar developed previous section 
partial derivative jm jm jm jm robust latching 
simplicity fixed point attractor shown 
shadow region basin attraction 
dark shadow region subset jm robust latch occurs 
see text details 
respect simply product map derivatives gamma gamma gamma norm factors right hand side left hand side converges exponentially fast zero gamma increases 
effect decay gradients explicit follows term sum fi fi fi fi fi fi fi fi fi fi term tends small comparison terms close means exist change allow jump better basin attraction gradient cost respect clearly reflect possibility 
explanation effect small change felt near past close 
remedies theoretical investigations indicate basic limitation gradient descent search procedure finding optimal weights rnn 
proposals cope problem long term dependencies attempting solve optimization problem alternative search algorithms trying devise alternative architectures 
give brief accounts proposals 
time constants deal long time lags mozer uses time constants influencing changes unit activations principe related approach may viewed mixture time delay neural networks tdnn time constants 
long time lags time constants need external fine tuning 
sun alternative approach updates activation recurrent unit adding old activation scaled current net input 
net input tends perturb stored information long term storage impractical 
lin propose variants time delay networks called networks see chapter 
gradient flow architecture improved embedded memories effectively introduce shortcuts error propagation path time 
idea applied architectures inserting multiple delays connections hidden state units output units 
architectures solve general problem increase constant multiplicative factor duration temporal dependencies learned 
el bengio looked hierarchically organized recurrent neural networks different levels time constants time delays 
ring approach ring proposes method bridging long time lags 
unit network receives conflicting error signals certain error signals suggest increase unit activity suggest adds higher order unit influencing appropriate connections 
approach extremely fast bridge time lag involving steps may require addition units 
ring net generalize unseen lag durations 
searching gradients difficulty learning long term dependencies strictly related continuous optimization approach guides search weight solution 
possibility avoiding problem resort kinds search weight space operators generating candidate weight solution continuous gradients 
bengio investigate methods simulated annealing multi grid random search discrete error propagation 
angeline see chapter propose genetic approach avoids gradient computation 
simplest kind search gradient simply randomly initializes network weights resulting net happens classify training sequences correctly 
fact discussed chapter book simple weight guessing solves popular benchmarks described previous faster recurrent net algorithms proposed compare 
mean weight guessing algorithm 
just means problems simple 
realistic tasks require free parameters input weights high weight precision continuous valued parameters guessing completely infeasible 
currently unclear extent complex gradient methods improve guessing case realistic tasks 
probabilistic target propagation bengio frasconi propose probabilistic approach propagating targets 
called state networks time system different discrete states 
parameters adjusted expectation maximization algorithm 
solve problems require significant amount memory store contextual information systems require unacceptable number states state networks 
adaptive sequence schmidhuber hierarchical chunker systems principle bridge arbitrary time lags local predictability sub sequences causing time lags see 
instance postdoctoral thesis schmidhuber uses hierarchical recurrent networks self organizing time scales rapidly solve certain grammar learning tasks involving minimal time lags excess steps 
performance chunker systems deteriorates noise level increases input sequences compressible 
long short term memory novel efficient gradient method called long short term memory lstm 
lstm designed get rid vanishing error problem 
truncating gradient harm lstm learn bridge minimal time lags excess discrete time steps enforcing constant error flow constant error special units 
multiplicative gate units learn open close access constant error flow 
lstm local space time computational complexity time step weight 
far experiments artificial data involved local distributed real valued noisy pattern representations 
comparisons rtrl bptt recurrent cascade correlation elman networks neural sequence chunking lstm led successful runs learned faster 
lstm solved complex artificial long time lag tasks solved previous recurrent network algorithms 
interesting examine extent lstm applicable real world problems speech recognition 
principle rnns general powerful current sequence learning method 
instance hidden markov models hmms successful technique sequence processing applications see review limited discrete internal states allow continuous distributed sequence representations 
solve tasks current method solve 
problem vanishing gradients conventional rnns hard train 
suspect feedforward neural networks outnumber rnns terms successful real world applications 
remedies outlined chapter may lead effective learning systems 
long lime lag research early stage commercial applications methods reported far 
long time lags pose problems soft computing method just rnns 
instance dealing long sequences speech biological data hmms rely localized representation time means highly constrained non ergodic transition diagrams different states designed different portions sequence 
belief propagation long time lags effectively occur phenomenon called diffusion credit closely resembles vanishing gradients problem rnns 
angeline saunders pollack 
evolutionary algorithm constructs recurrent neural networks 
ieee transactions neural networks 
baldi pineda 
contrastive learning neural oscillator 
neural computation 
bengio 
markovian models sequential data 
neural computing surveys 
bengio frasconi 
credit assignment time alternatives backpropagation 
cowan tesauro alspector editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
bengio frasconi 
diffusion context credit information markovian models 
journal artificial intelligence research 
bengio simard frasconi 
learning long term dependencies gradient descent difficult 
ieee transactions neural networks 
de vries principe 
theory neural networks time delays 
lippmann moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
doya 
bifurcations learning recurrent neural networks 
proceedings ieee international symposium circuits systems pages 
el bengio 
hierarchical recurrent neural networks long term dependencies 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press cambridge ma 
gers schmidhuber cummins 
learning forget continual prediction lstm 
proc 
icann int 
conf 
artificial neural networks pages edinburgh scotland 
iee london 
hochreiter 
untersuchungen zu 
diploma thesis institut fur informatik lehrstuhl prof brauer technische universitat munchen 
see www informatik 
de 
hochreiter schmidhuber 
long short term memory 
neural computation 
hochreiter schmidhuber 
lstm solve hard long time lag problems 
mozer jordan petsche editors advances neural information processing systems pages 
mit press cambridge ma 
lang waibel hinton 
time delay neural network architecture isolated word recognition 
neural networks 
lin horne giles 
embedded memory recurrent neural network architectures helps learning long term temporal dependencies 
neural networks 
lin horne ti giles 
learning long term dependencies recurrent neural networks 
ieee transactions neural networks november 
mozer 
induction multiscale temporal structure 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
ortega 
iterative solution non linear equations variables systems 
academic press new york 
pineda 
dynamics architecture neural computation 
journal complexity 
ring 
learning sequential tasks incrementally adding higher orders 
cowan hanson giles editors advances neural information processing systems pages 
morgan kaufmann 
robinson fallside 
utility driven dynamic error propagation network 
technical report cued infeng tr cambridge university engineering department 
rumelhart hinton williams 
learning internal representations error propagation 
parallel distributed processing volume pages 
mit press 
schmidhuber 
learning complex extended sequences principle history compression 
neural computation 
schmidhuber 
und 
institut fur informatik technische universit munchen 
sun chen lee 
time warping invariant neural networks 
cowan hanson giles editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
werbos 
generalization backpropagation application recurrent gas market model 
neural networks 
williams zipser 
gradient learning algorithms recurrent networks computational complexity 
backpropagation theory architectures applications 
hillsdale nj erlbaum 

