ieee transactions circuits systems video technology vol 
august hardware software cache prefetching techniques mpeg benchmarks daniel zucker member ieee ruby lee member ieee michael flynn member ieee popularity multimedia acceleration instructions mmx mpeg decompression increasingly executed general purpose processors dedicated mpeg hardware 
gap processor speed memory access means significant amount time spent memory system 
processors get faster terms higher clock speeds increased instruction level parallelism time spent memory system significant 
data prefetching known technique improving cache performance 
studies examined prefetch strategies scientific commercial applications focuses video applications 
data types hardware prefetching schemes stream buffer stride prediction table spt stream cache new software directed prefetching technique emulation hardware spt 
misses occur prefetching eliminated 
stream cache cut execution time half addition relatively small amount additional hardware 
software prefetching achieves nearly equal performance minimal additional hardware 
techniques improve performance general purpose cpu embedded mpeg processor 
performance gains achieved mpeg benchmarks apply equally effectively similar multimedia applications 
index terms cache mpeg multimedia prefetching software prefetching stride prefetching 
studying memory performance scientific general purpose applications research focuses needs multimedia applications 
relatively simple prefetching techniques significantly improve memory hit rates mpeg applications 
processors faster utilize increasing instruction level parallelism memory performance dominating effect processor performance 
improvements memory performance eventually result performance increases relatively little additional hardware 
manuscript received june revised april 
recommended associate editor 
zucker computer systems laboratory department electrical engineering stanford university stanford ca usa 
box palo alto ca usa mail zucker org 
lee computer systems laboratory department electrical engineering stanford university stanford ca usa 
department electrical engineering princeton university princeton nj usa mail ee princeton edu 
flynn computer systems laboratory department electrical engineering stanford university stanford ca usa mail flynn arith stanford edu 
publisher item identifier 
ieee number techniques exist cache prefetching 
idea prefetching predict data access needs advance specific piece data loaded main memory needed application 
number papers written studying hardware software prefetching techniques relatively little looked specifically memory behavior mpeg applications 
earliest hardware prefetching reported smith proposed block lookahead obl scheme prefetching cache lines 
demand brings block cache block prefetched 
jouppi expanded idea proposal stream buffers 
scheme causes block brought cache causes prefetching blocks separate stream buffer 
jouppi recognized need multi way stream buffers multiple active streams maintained cache 
reported significant rate reduction 
palacharla kessler proposed enhancements stream buffer 
developed filtering scheme limit number unnecessary prefetches method allowing variable length strides prefetching stream data 
hardware approach prefetching differs stream buffer data prefetched directly main cache 
addition form external table keep track past memory operations predict requirements prefetching 
advantage efficiently handling variable length data accesses linearly traverse data set steps 
fu patel proposed utilizing stride information available vector processor instructions prefetch relevant data 
expanded application scalar processors cache look table called stride prediction table spt 
chen baer proposed similar structure called prediction table 
scheme additionally includes state bits state information maintained concerning character memory operation 
limit unnecessary prefetching 
analysis scheme investigate timing issues prefetching cycle cycle processor simulation 
presents third variation theme external table predict memory 
number techniques exist software prefetching 
porterfield proposed technique prefetching certain types array data 
mowry proposed early practical software prefetch scheme information obtained compile time 
software prefetching clearly cost zucker hardware software cache prefetching techniques mpeg benchmarks advantage introduce additional overhead application 
extra cycles spent execute prefetch instruction code expansion required may result negative side effects increased register usage 
compares mowry software prefetching scheme hardware prefetching scheme 
determine software approach compile time information perform complex analysis hardware prefetching advantage dynamic information 
furthermore determine methods improve rate overhead adding software prefetch instructions significant 
sophisticated compile time algorithm insert software prefetch instructions hp pa 
hp compiler furthermore capability add prefetch instructions execution profile data 
improvement mowry algorithm 
speedups reported specfp 
software prefetch scheme emulating hardware spt adding software prefetch instructions 
data prefetch usefulness obtained profiling simulated spt execution 
technique unique relies completely profile information prefetching 
low complexity means technique easily add prefetch instructions existing executable optimization existing compiler 
ii 
methodology simulation methods cache simulator linked memory address traces generated instruction instrumentation tool hewlett packard pa risc architecture 
data modeled instruction accesses ignored 
simulator provides data wide range data cache sizes 
line size bytes chosen simulations 
line size chosen better expose potential benefits prefetching 
single process simulated cache configuration expected performance cache sizes reported corresponds larger cache size real system 
rates applications run baseline cache enhancements shown fig 

characteristics movies benchmark executions summarized table complete rate data range cache line sizes 
performance metrics fraction misses eliminated primary performance metric reported 
metric judges performance prefetch scheme independent particular cache implementation 
perfect prefetching scheme eliminate memory misses 
fraction misses eliminated value misses eliminated 
similarly architecture eliminates half misses cache similar size associativity fraction misses eliminated value 
case second level cache fraction misses eliminated metric identical hit rate second level fig 

baseline rates 
data direct mapped cache 
data way associative cache 
cache 
misses occur level cache fraction hit second level cache definition equal fraction misses eliminated 
reason fraction misses eliminated second level hit rate configurations spt parallel stream cache discrete second level cache exists 
way comparisons common metric cache configurations study 
metric desirable number reasons 
way performance improvement judged independently cache design considerations main cache size associativity 
size main cache dominating effect rate results simply compared terms absolute rates variation due cache size tend mask variation due prefetching scheme 
furthermore performance judged independently memory implementation parameters time access main memory 
case ieee transactions circuits systems video technology vol 
august varying memory parameters cycles fill main cache line significant impact results 
deriving fraction misses eliminated memory access counted hit long prefetch address issued 
means data process returning memory counted hit 
done compare prefetching performance schemes best conditions 
counting incomplete prefetches misses furthermore little effect resulting data 
results reported execution time numbers cycles 
results aggressive memory limited processor model assumed 
way superscalar processor assumed sufficient resources perform operation single cycle 
limit computation time number memory operations 
relative execution time reports execution time compared identical cache configuration prefetching 
relative execution time indicates performance improvement prefetching 
constant memory access time cycles misses prefetches assumed 
furthermore fully interleaved memory assumed multiple outstanding prefetch requests allowed 
software prefetching execution time incorporates cost additional prefetch instructions executed 
assumptions show effect prefetching processor unlimited resource constraints 
section ix effect modifying assumptions investigated 
execution time calculated considering cost partially completed prefetches 
furthermore instruction mix memory operations fraction total instructions considered 
different bus models fixed number simultaneous outstanding memory requests allowed considered 
memory bandwidth purposes study memory bandwidth assumed large limiting factor performance 
assumption study effects differing prefetch strategies independent memory bus architectures 
recognized assumption may valid terms today architectures 
trend wider bandwidth memory indicates may problem 
architectures studied rely significantly increasing accesses main memory form increased prefetching 
techniques filtering exist ad table benchmark image characteristics fig 

way stream buffer architecture 
dress issue 
software prefetching proposed eliminates significant number unneeded prefetches 
describe additional analytic techniques limit number prefetches issued 
iii 
stream buffers stream buffer architecture simulated shown fig 

proposed jouppi stream buffer fifo type queue sits refill path main cache 
new stream allocated successive data cache stream buffers allocated data cache replaces stream accessed lru replacement 
memory access misses main cache hits stride buffer counted hit 
refill time stream buffer order magnitude faster refill main memory assumption significantly affect reported results 
simulations assume parallel stream buffers 
number selected large number stream buffers limiting factor performance 
simulate stream buffer depth entries 
proposed enhancement stream buffer filter unnecessary excess prefetches 
memory bandwidth limiting factor model potentially hurt performance included 
performance data range cache sizes direct mapped way shown fig 

applications stream buffer tends peak eliminating misses 
mpeg play playing easter mpg single exception eliminate misses large caches 
outlying data point 
zucker hardware software cache prefetching techniques mpeg benchmarks fig 

fraction misses eliminated way stream buffers 
way stream buffer direct mapped main cache 
way stream buffer way associative main cache 
cases misses eliminated stream buffer 
relatively complicated algorithms involved tend access data strides stream buffer designed aid cases unit strides 
way stream buffer approximately misses eliminated stream buffer totally effective prefetching technique 
iv 
spt structure spt simulated shown fig 

table indexed instruction address maintained memory operations executed holds address memory address accessed 
memory instruction executed address compared instruction addresses stored spt 
instruction match instruction stored spt spt occurs 
spt new entry composed instruction address data memory address fig 

spt architecture 
added spt replacing lru entry 
memory access instruction contained spt spt hit occurs 
current memory access address subtracted previously stored memory address calculate data stride value 
value nonzero prefetch issued 
prefetch address calculated adding stride value current memory address 
data prefetched main cache 
spt requires access program counter pc may slightly disadvantageous compared stream buffer 
stream buffer relies external data requests may added easily spt existing commercial processor 
data obtained simulations entry stride table shown fig 

applications perform stride cache entries 
large main cache sizes misses eliminated relative cache size associativity stride prediction mechanism 
knee curve appears cache size approximately kb spt rapidly effective 
surprisingly major factor mpeg encode performance appreciably decay small cache sizes 
due memory intensive motion estimation done mpeg encoding 
fairly large search space required motion vector encoding repeated macroblock image 
requires large total number memory memory locality quite traditional cache structure performs small caches 
smaller number remaining misses captured traditional main cache handled easily stride prediction mechanism 
interesting effect observed spt greater entries 
cases stride prediction harms memory performance relatively small cache sizes 
large number prefetches begins remove useful data cache 
problem potentially solved filtering techniques 
spt works middle large cache sizes 
difficult better eliminating misses 
range spt effective means prefetching 
ieee transactions circuits systems video technology vol 
august fig 

fraction misses eliminated spt 
entry spt direct mapped main cache 
entry spt way associative main cache 
entry spt direct mapped main cache 
entry spt way associative main cache 
stream cache series stream cache stream cache overcomes problems spt improving performance small cache sizes 
stream cache independent cache data prefetched spt hit 
spt data prefetched stream cache instruction missed spt added spt described section iv 
spt job predicting data prefetch fails smaller cache sizes prefetches large amount unnecessary data 
stream cache data prefetched main cache independent stream cache 
data prefetched directly main cache polluting main cache problem 
series stream cache architecture shown fig 

term series stream cache connected series main cache 
series stream cache queried main cache fill main cache fig 

series stream cache architecture 
desired data 
data missed main cache stream cache brought main memory directly zucker hardware software cache prefetching techniques mpeg benchmarks fig 

fraction misses eliminated entry series stream cache entry spt 
entry series stream cache direct mapped main cache 
entry series stream cache way associative main cache 
entry series stream cache direct mapped main cache 
entry series stream cache way associative main cache 
main cache 
series stream cache simulated fully associative 
data series stream cache copied stream cache main cache mru replacement policy fetching new data stream cache 
lru replacement policy cause data copied main cache linger stream cache 
data main cache keeping copy stream cache inefficient stream cache storage 
new data fetched stream cache spt hit 
prefetch completed data prefetch address contained main cache 
stream buffer works unit strides inherently configured fixed number streams 
way stream buffer separate streams application data cache memory efficiently utilized 
stream cache solves problems stream buffer uniting separate fifo multiple stream buffers relatively small fully associative stream cache 
spt predict data prefetch data prefetched stream cache main cache 
stream cache unified specific number streams application irrelevant 
performance data entry stream cache shown fig 

entry stream cache appears large give fairly uniform performance improvement cache sizes 
performance main cache sizes approximately kb significantly improved cache configurations spt 
region left part graph significant smaller main caches performing efficiently memory performance higher percentage execution time 
ieee transactions circuits systems video technology vol 
august fig 

parallel stream cache architecture 
parallel stream cache parallel stream cache similar series stream cache location stream cache moved refill path main cache position parallel main cache 
shown fig 
hypothesis multimedia applications tend operate relatively small workspace data movie 
data workspace operated short time frequently reused 
goal modified stream cache isolate local workspace stream cache 
prefetched data brought stream cache copied main cache 
cache access search main cache stream cache parallel 
cache satisfied main cache stream cache data fetched main memory directly main cache 
data stream cache accessed multiple times stale lru replacement scheme employed 
mru replacement scheme prematurely discard accessed data stale 
series stream cache data prefetched stream cache spt hit 
prefetch completed data contained main cache 
spt data prefetched stream cache instruction missed spt added spt described section iv 
rate data entry parallel stream cache entry spt shown fig 

smaller caches show greater enhancement mid sized caches greater benefit keeping frequently data main cache 
small cache sizes performance better entry series stream cache described previously section 
furthermore region main cache suffering high rates improvement particularly beneficial 
vi 
time area tradeoffs previous sections significant improvements rates reported 
increase come free 
cost additional die area required spt stream cache 
section performance comparisons considering additional area 
fig 

fraction misses eliminated entry parallel stream cache entry spt 
direct mapped main cache 
way associative main cache 
additional area requirement actual die area highly implementation dependent difficult model accurately 
spt entry hold complete instruction address data address valid bit entry 
spt area modeled assuming bit words bytes entry 
additional area required associated logic adders comparators 
considered model 
additional area stream cache calculated assuming bytes entry 
considers byte data line size entry neglects tag bits 
may possible reduce size spt storing lower bits instruction address 
cause aliasing instructions hypothesis effect significant 
furthermore may possible store lower bits address assumption data strides kb 
zucker hardware software cache prefetching techniques mpeg benchmarks fig 

execution times mpeg play entry parallel stream cache entry stride table adjusted extra area required 
direct mapped main cache 
way associative main cache 
cause address aliasing 
techniques considered area model employed 
execution time absolute execution times single application shown fig 

execution time calculated assuming main memory latency main stream caches cycles 
data needed process loaded cache balance cycles remaining counted total execution time 
memory latency charged constant latency cycles conflicts requests simulated 
horizontal axis adjusted total area including main cache stream cache shown enhanced cache 
small cache sizes stream cache cut execution time half 
cache sizes kb original time required execution 
large cache sizes traditional cache design fig 

histogram useful prefetches movie flower kb direct mapped cache parallel stream cache 
fairly job capturing working set stream cache proportionately beneficial detrimental cases 
case large caches spt effective means prefetching 
image sizes larger break point shifts right stream cache useful larger range caches 
data suggests stream cache effective improving execution time small chip cache low cost multimedia system small cache 
entry spt entry stream cache adds kb extra area cause kb main cache perform baseline kb cache kb cache perform baseline kb cache application shown 
vii 
motivation software directed prefetching technique advantage hardware stride prediction stride value change dynamically 
single instruction prefetch different stride values duration program 
spt described section iv accessed load store executed order determine prefetch 
necessary keep track small subset load store instructions 
fig 
shows histogram total useful prefetches kb direct mapped main cache entry parallel stream cache movie flower 
total useful prefetches shown axis instruction axis 
define useful prefetch prefetched data subsequently application 
prefetches counted instruction predicted stride 
graph sorts instructions causing prefetches causing number prefetches 
histogram observed relatively small number instructions cause useful prefetches 
fig 
shows data cumulatively cache sizes entry parallel stream cache 
line indicating total prefetches level instructions ieee transactions circuits systems video technology vol 
august fig 

effect main cache size 
graphs image direct mapped main cache parallel stream cache 
kb main cache 
kb main cache 
kb main cache 
mb main cache 
point causing significant prefetches 
relatively small number instructions order needed cause prefetches 
graphs furthermore divide prefetches separate categories indicated separate lines graphs 
top line total number useful prefetches 
bottom line counts prefetches constant strides 
spt unnecessarily complex strides change dynamically 
capability change strides dynamically key features spt 
middle line shows prefetches result instruction static stride associated 
single stride value selected common stride value run 
static prediction performs slightly worse dynamic prediction 
figures indicate hardware mechanism allows fully exploit dynamically changing stride value best guess static stride value works equally effectively 
furthermore individual instructions thousands executable cause useful prefetches 
software directed stride prefetching technique replaces hardware spt proposed 
viii 
trace software prefetching technique software prefetching technique works gathering execution profile information simulation hardware spt 
prefetch hint file generated tracing instructions caused useful prefetches hardware spt simulation 
hint file insert instructions 
automatically compiler possible profiling inserting prefetch instructions code separate steps 
zucker hardware software cache prefetching techniques mpeg benchmarks fig 

effect varying cache type generating prediction file 
graphs compare effects predictions generated parallel stream cache series stream cache hardware directed prefetching movie kb direct mapped main cache 
effective instructions issue prefetches software case 
fraction misses eliminated reported execution performed direct mapped main cache stream cache parallel stream cache series stream cache 
profile step simulates hardware spt 
tracking instructions caused cache lines prefetched keeping track prefetch data application determine instructions useful prefetch data subsequently application 
furthermore keeping track stride value prefetch data determine best value static stride prediction 
obtaining data describing prefetches useful selectively insert software prefetch instructions executable code static stride prediction 
results generated simulating discrete software prefetch instruction 
particular prefetch instruction implemented variety ways 
simulations atomic prefetch stride instruction assumed 
instruction prefetches special purpose stream cache prefetches directly main cache depending cache configuration simulated 
prefetch stride struction invoked immediate stride value 
executed load store address added stride value prefetch new address initiated 
stride value available compile time derived hint file generated profiling step 
fig 
shows fraction misses eliminated different cache configurations software prefetch technique described 
useful prefetch instructions added executable code 
fig 
shows data prefetched directly main cache fig 
shows data prefetched parallel stream cache fig 
shows data prefetched series stream cache 
lines graph indicate different methods inserting prefetch instructions 
hardware spt compared software prefetch trials 
case useful prefetch data collected simulation trace parallel stream cache collected series stream cache 
ieee transactions circuits systems video technology vol 
august fig 

effect varying movie prediction file 
prediction file generated kb direct mapped main cache series stream cache 
effective instructions issue prefetches 
fraction misses eliminated reported execution performed direct mapped main cache parallel stream cache 
general shapes curves stream cache series parallel stream caches matches shape curve hardware stride prediction 
software prediction results similar performance hardware directed prefetching cost hardware spt 
software directed prefetching better number cases 
recall small caches hardware prefetch excessive amount data useful data degrading performance 
software directed prefetching eliminates problem effective prefetches inserted 
software prefetching effective replacement hardware spt 
effect frame size fig 
illustrates effect varying movie displayed holding cache size associativity configuration cache collect prefetch data constant 
image series stream cache direct mapped main cache size kb collect prefetch statistics 
useful instructions execute prefetches 
data shown range cache sizes direct mapped main cache parallel stream cache images flower easter 
data terms fraction misses eliminated 
movies flower perform approximately easter appears significantly worse 
due frame size movies 
flowers share frame size easter frame size indicates frame dimensions important components stride information 
important movie displayed collecting trace information frame size movie displayed 
effect sorting shown software prefetching generally effective technique replace spt 
section methodology developed automatic insertion software prefetch instructions 
focus case stream cache 
way prefetching methodology applied automatically 
task determine software prefetch instructions insert 
section look different possibilities sorting useful prefetches 
hint table section generated sorting useful prefetches performed static prefetch 
data prefetched spt dynamic mechanism included 
main cache size associativity movie type possible orderings instructions 
order accesses parallel stream cache simulation order accesses series stream cache simulation 
accesses series stream cache record indicated instruction prefetches data accessed give measure data accessed 
accesses parallel stream cache give indication useful data prefetched 
relative execution time range cache sizes constant useful prefetches inserted shown fig 

series ordering wins cases 
parallel ordering prioritizes total number accesses just access series ordering 
execution time access important data demand fetched cache automatically 
percent prefetch instructions insert section determine best number prefetch instructions insert 
fig 
show effect inserting different percentages total possible number prefetch instructions single movie 
fig 
shows fraction misses eliminated range direct mapped caches 
aside smaller caches fraction misses eliminated gets continuously better prefetch instructions inserted 
smaller caches benefit inserting fewer instructions useful data replaced speculative prefetches 
relative execution time shown direct mapped kb cache fig 

data shown movies easter bicycle 
percentage useful prefetch instructions inserted varied axis 
illustrates cost inserting prefetch instructions shows worst relative execution time 
pattern relative execution time decreasing optimal point prefetching 
fraction misses eliminated tends continuously improve instructions inserted 
cache size tangent minimum point relative execution time shows optimal relative execution time achievable 
indicates optimal percentage prefetch instructions insert 
inserting instructions capture available prefetches optimal choice cache sizes 
ix 
software prefetching performance fig 
show relative execution time range cache sizes optimal number prefetches inserted 
parameters chosen model achieve zucker hardware software cache prefetching techniques mpeg benchmarks fig 

relative execution time available prefetches inserted comparing prediction file generated series parallel stream caches 
direct mapped cache 
way associative cache 
improvement execution time range caches kb adding software prefetch instructions 
section investigate adjusting memory access model affects performance 
effect execution time parameters section investigate altering certain parameters model effects relative execution time 
point assuming accesses memory demand prefetch misses cost cycles instructions consist load stores 
consider impact altering memory access cost instruction mix parameters 
memory access cost cycles instruction mix loads stores comprise instructions investigated 
measured instruction mix mpeg play determined 
full range graphs memory access costs cycles direct mapped way fig 

performance data different numbers prefetch instructions inserted movie direct mapped cache 
fraction misses eliminated 
relative execution time kb main cache 
associative number instruction mix parameters available 
assume instructions cache misses execute cycle 
show data movies fig 

general maximum benefit added prefetching increased memory access time increased 
memory access time increased fraction execution time spent memory system increased 
time spent memory system potential prefetching improve performance 
indicated graphs way curves tend dip lower lower higher memory access costs 
memory access cost cycles improvement performance possible 
changing instruction mix similarly affects fraction execution time spent memory system 
time spent memory system potential improvement 
illustrated downward dip graphs smaller increase instruction mix 
additional effect decreasing ieee transactions circuits systems video technology vol 
august fig 

relative execution time prefetches inserted executed direct mapped cache 
executed way associative cache 
overhead cost additional prefetch instructions 
additional instructions executed mix execution added prefetch instructions noticeable 
effect memory access parameters section data account different memory access models 
previously simply assumed full partial cache hits completed cycle 
long memory request issued counted hit regardless cycles remained data return memory 
calculate total execution time simply multiplied total number misses constant penalty 
section fully account case partial hits 
penalty cycles memory access occurs data cycles prefetch balance cycles charged execution time 
furthermore investigate different possibilities simultaneous number outstanding prefetches 
previously assumed memory system fully pipelined limit fig 

relative execution time way associative cache prefetches inserted 
memory access cost cycles instruction mix loads stores 
memory access cost cycles loads stores 
outstanding prefetches allowed time 
limit accesses finite number accesses allowed 
limit prefetches allowed constant memory access penalty assumed 
look different techniques prioritizing memory accesses prefetches different possibilities simultaneous numbers outstanding prefetches 
configurations simulated summarized table ii 
configuration assume demand loads stores priority prefetches 
words memory system services requests order received 
advantage implementation simplicity 
memory accesses equivalent special considerations prefetches 
significant disadvantage performance prefetches stall processor data requested necessary 
assume maximum pending prefetches queued 
queue full additional prefetches zucker hardware software cache prefetching techniques mpeg benchmarks fig 

relative execution time different memory models way associative cache 
configuration 
configuration 
configuration 
configuration table ii summary memory access models simulated discarded 
allowed infinite number prefetches queue performance prefetching greatly degraded base case large numbers waiting prefetches hopelessly stall servicing demand misses 
outstanding memory access occur time 
performance configuration shown fig 

configuration continue assume memory access occur time assume loads stores priority prefetches 
queue prefetches waiting executed load store jump front queue executed 
outstanding memory operation balance memory penalty incurred prefetch returns load store executed immediately 
demand loads stores priority prefetches need limit pending queue entries 
performance data case shown fig 

configuration assumption assumes memory way interleaved memory accesses outstanding 
configuration allows maximum interleaving memory access return constant time cycles 
performance configurations shown figs 
respectively 
figures indicate long demand loads stores priority prefetches number simultaneous outstanding prefetches significantly effect performance 
furthermore figures confirm perfor ieee transactions circuits systems video technology vol 
august mance benefit prefetching enhanced way associativity 
summary investigated number prefetching techniques mpeg benchmarks 
regular memory access pattern applications form data prefetching attractive strategy improving memory performance 
stream buffers eliminate data misses small moderately sized caches 
small cache sizes large number misses contribute significantly total execution time large reduction misses desirable 
series stream cache added improvement spt smaller sized caches left performance improvements intact large caches 
parallel stream cache resulted extremely performance enhancements small cache sizes small amount additional hardware cases slightly worse spt large cache sizes 
spt shown perform extremely large caches series stream cache parallel stream cache perform small cache sizes 
cases performance improvements result small increase hardware 
extremely cost area sensitive applications small cache required benefit significantly employing technique 
performance data analysis hardware prefetching software prefetching scheme proposed replace hardware spt 
performance similar reduced cost hardware 
methodology developed add software prefetch instructions existing compiled code 
small number prefetch instructions generate useful prefetches 
machine special purpose hardware prefetch instruction way associative cache simulated improvement execution time observed 
peleg weiser mmx technology extension intel architecture ieee micro vol 
pp 
aug 
smith cache memories acm comput 
surveys vol 
pp 
sept 
jouppi improving direct mapped cache performance addition small fully associative cache prefetch buffers proc 
th annu 
int 

computer architecture may pp 

palacharla kessler evaluating stream buffers secondary cache replacement proc 
st annual international symposium computer architecture apr pp 

fu patel data prefetching multiprocessor vector cache memories proc 
th annu 
int 
symp 
computer architecture may pp 

fu patel janssens stride directed prefetching scalar processors proc 
th int 
symp 
microarchitecture dec pp 


baer 
chen effective chip preloading scheme reduce data access penalty proc 
supercomputing nov pp 


chen 
baer effective hardware data prefetching high performance processors ieee trans 
comput vol 
pp 
may 
prefetch unit vector operations scalar computers acm comput 
architec 
news vol 
pp 
sept 
porterfield software methods improvement cache performance supercomputer applications rice univ houston tx tech 
rep comp tr may 
mowry lam gupta design evaluation compiler algorithm prefetching sigplan notices sept pp 


chen 
baer performance study software hardware data prefetching schemes proc 
st int 
symp 
computer architecture apr pp 


hsu data prefetching hp pa th annu 
int 
symp 
computer architecture june 
zucker karp versatile instruction instrumentation tool pa risc comput 
syst 
lab stanford univ stanford ca tech 
rep csl tr jan 
zucker architecture arithmetic multimedia enhanced processors ph dissertation stanford univ stanford ca june 
fu zucker flynn memory hierarchy synthesis multimedia embedded processor proc 
int 
conf 
computer design oct pp 

daniel zucker received degree distinction ph degrees electrical engineering stanford university stanford ca national science fellow hitachi fellow 
currently chief technical officer san carlos ca provider networked software handheld computers 
founder llc developed popular palm computing consumer application 
worked advanced micro devices sun microsystems fujitsu research laboratories led secure network applications group published academic popular articles multimedia computer architecture handheld computing 
dr zucker recipient frederick award stanford university 
member phi beta kappa tau beta pi acm 
ruby lee received degree distinction cornell university degree computer science computer engineering ph degree electrical engineering stanford university stanford ca 
september forrest professor engineering princeton university princeton nj affiliated appointment computer science 
consulting professor electrical engineering stanford university chief architect hewlett packard hp leading interdisciplinary security architecture team enterprise commerce systems 
prior key contributions hp include pa risc architecture initial design generations server workstation products pa risc cmos microprocessor max max multimedia acceleration instructions microprocessors intel hp ia epic architecture emerging bit intel microprocessors 
granted foreign patents 
research interests computer architecture multimedia architecture security architecture 
dr lee member phi beta kappa alpha lambda delta acm 
michael flynn received degree manhattan college ny degree syracuse university syracuse ny ph degree purdue university west lafayette 
designer mainframe computers ibm professor electrical engineering stanford university stanford ca formed stanford architecture arithmetic group 
authored authored books professional papers 
dr flynn founded specialist organizations computer architecture ieee computer society technical committee computer architecture acm sigarch 
recipient acm ieee eckert award ieee computer society harry memorial award 
