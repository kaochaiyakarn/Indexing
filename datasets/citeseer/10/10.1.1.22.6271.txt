likelihood data squashing modeling approach instance construction 
david madigan raghavan william dumouchel labs research raghavan research att com martha nason christian com september greg university greg stat washington edu squashing lossy data compression technique preserves statistical information 
speci cally squashing compresses massive dataset smaller outputs statistical analyses carried smaller squashed dataset reproduce outputs statistical analyses carried original dataset 
likelihood data squashing lds di ers previously published squashing algorithm insofar uses statistical model squash data 
results show lds provides excellent squashing performance target statistical analysis departs model squash data 
massive datasets containing millions billions observations increasingly common 
data arise instance large scale telecommunications astronomy computational biology internet logging 
statistical analyses data scale new computational statistical challenges 
computational challenges derive large part multiple passes data required statistical algorithms 
data large memory especially pressing 
atypical disk drive factor times slower performing random access main memory computer system gibson 
furthermore costs associated transmitting data may prohibitive 
statistical challenges constitutes statistical signi cance observations 
dowe deal dynamic nature massive datasets 
best visualize data scale 
current research massive datasets concerns scaling existing algorithms see example bradley 
provost kolluri 
focus alternative approach data 
previous direction focused sampling methods random sampling strati ed sampling duplicate compaction catlett boundary sampling aha 
dumouchel 
proposed approach constructs reduced dataset 
speci cally data squashing algorithm seeks compress squash data away statistical analysis carried squashed data provides outputs resulted analyzing entire dataset 
success respect goal deal ectively computational challenges mentioned entire statistical tools massive datasets routine fashion commonplace hardware 
approach squashing model free relies moment matching 
squashed dataset consists set pseudo data points chosen replicate moments mother data subsets partition mother data 
explore various approaches partitioning experiment der moments 
logistic regression example mother data contains observations squashed dataset points outperformed simple ran dom sample points factor terms mean square error respect regression coe cients mother data 
provide theoretical justi cation method considering taylor series expansion arbitrary likelihood function 
depends moments data method application likelihood approximated rst terms taylor series subsets partitioned data 
empirical evidence provided date limited logistic regression 
consider variant squashing idea suppose declare statistical model advance 
suppose particular statistical model squash data 
improve squashing performance 
improvement extend models squashing 
refer approach likelihood data squashing lds 
lds similar original algorithm ds insofar rst partitions dataset chooses pseudo data points corresponding subset partition 
algorithms di er create partition create pseudo data points 
instance context logistic regression continuous predictors shows partitions dimensional predictor space generated algorithms single value dichotomous response variable 
ds algorithm partitions data certain marginal quantiles matches moments 
lds algorithm partitions data likelihood clustering selects pseudo data points mimic target sampling posterior distribution 
section describes algorithm detail 
follows explore application lds logistic regression variable selection logistic regression neural networks 
note ds lds algorithms produce pseudo data points associated weights 
squashed data requires software weights appropriately 
lds algorithm motivate lds algorithm bayesian perspective 
suppose com puting distribution parameter posterior data points lds mother data 
ds data partitions created lds ds pr pr pr pr pr suppose pr pr values non trivial posterior mass construct pseudo data point pr pr pr squashed dataset comprising weight weight see table approximate analysis posterior entire mother data 
practice mother data point di lds rst evaluates pr di set values kg generate likelihood pro le pr di pr di di 
lds clusters mother data points likelihood pro les 
lds constructs pseudo data points cluster assigns weights pseudo data points functions cluster sizes 
note lds clusters mother data points likelihood pro les resultant clusters typically bear relationship kinds clusters table simple example squashing pr pr 
lds constructs pseudo data point pr pr pr pr pr 
mother data squashed data instance weight instance weight result traditional clustering data points 
ex ample shows lds constructing clusters containing data points disparate coordinates 
shows lds clusters context simple linear regression origin model single parameter 
case likelihood pro les data point di represent likelihoods di variety lines de ned set slopes kg 
left hand panel shows mother data generated bivariate normal distribution zero correlation noise right hand panel shows mother data generated model true slope 
plots demonstrate substantial symmetries origin likelihood point plots cluster centered origin 
lines pass origin points near origin similar likelihoods lines 
right hand panel exhibits distinctive radial clusters likelihood context function distance data point line 
detailed description observations yn realized values random variables yn 
suppose functional form probability density function ofy speci ed nite number unknown parameters 
denote log likelihood log denote value maximizes 
lds noise lds signal data partitions created lds ds base version lds base lds proceeds follows select select values select set values central composite design centered estimate generally pass mother data 
central composite design box chooses values central point star points axes factorial points corners cube centered illustrates design 
design basic standard response surface mapping box draper 
section addresses exact locations star factorial points 
profile evaluate likelihood pro les 
evaluate yi single pass mother data creates likelihood pro le observation 
cluster cluster mother data single pass 
select sample datapoints mother data form initial cluster centers 
remaining datapoints assign datapoint yi cluster minimizes kx yi lc lc denotes average log likelihoods data points cluster construct construct pseudo data 
clusters construct sin gle pseudo datapoint 
consider cluster containing datapoints yi yim 
yi denote corresponding pseudo datapoint 
algorithm initializes yi optionally re nes yi numerically minimizing kx mx results reported include optional step 
central composite design variables described algorithm requires passes mother data estimate evaluate likelihood pro les perform clustering 
rst pass omitted favor estimate random sample adversely ect squashing performance see section 
exist variety elaborations base algorithm discuss follows 
large central composite design choose unnecessarily large set values select phase 
literature experi mental design see example box provides rich array fractional factorial designs ciently scale clustering algorithm base lds improved zhang 
describe alternative read ily provide replacement cluster phase 
elaborations include alternative clustering metrics cluster phase varying number pseudo points construction algorithm construct phase ing entire lds algorithm 
elaborations require extra passes mother data 
evaluation logistic regression evaluate performance lds conducted variety experiments datasets various sizes 
case primary goal compare parameter estimates mother data corresponding estimates squashed data 
provide baseline computed estimates simple random sample 
provide results simulated data data 
report results form residuals mother data parameter estimates reduced data parameter estimate mother data parameter estimate 
residuals standardized standard errors estimated mother data averaged parameters pertinent model 
note reproducing parameter estimates represents challenging target reproducing predictions requires obtain high quality estimates parameters 
section shows accurate parameter estimate replication result high quality prediction replication 
small scale simulations implementation base lds requires initial estimate choice locations values central composite design 
carried extensive experimentation small scale simulated mother data order understand ects various possible choices squashing performance 
initial estimate considered possibilities srs srs maximum likelihood estimator random sample approximate maximum likelihood estimator single step standard logistic regression newton raphson algorithm requires single pass mother data maximum likelihood estimator mother data 
central composite design df denote distance factorial points ds denote distance star points distances standard error units 
considered df ds 
case mother data consisted observations generated logistic regression model log pr pr 
simulated mother datasets model lds generated squashed datasets corresponding design settings 
parameter estimates srs sample computed 
lds srs datasets size 
shows boxplots standardized residuals parameter estimates 
residuals respect parameter estimates mother data standardized standard errors estimates mother data 
features immediately apparent appropriate choices df lds outperforms random sampling settings note results shown log scale instance lds mle ds df lds outperforms srs factor squashing performance improves improves srs dependence size df srs df optimal setting choices 
choices df yield equivalent performance 
df optimal setting choices 
choice ds relatively small ect squashing performance 
df df df df df df df df df df df df lds mle ds lds ds lds srs ds lds mle ds lds ds lds srs ds lds mle ds lds ds lds srs ds log mse lds mse srs lds mle ds lds ds lds srs ds small scale simulation results 
boxplot shows particular setting df ds 
horizontal axes show log ratio mean square error random sampling mean square error lds 
de nes center design matrix lds evaluates likelihood pro les hardly surprising performance degrades departs itis evidently important cluster datapoints similar likelihoods region maximum likelihood estimator large datasets close posterior mean cluster datapoints similar likelihoods regions negligible posterior mass somewhat surprising extent design points need depart case best evaluate likelihood pro les di set values far tails posterior distribution 
fact choosing ds df large gives acceptable performance implies lds doesn avery estimate needs ensure broad coverage likelihood surface 
medium scale simulations consider performance lds somewhat larger scale setting 
particular simulated mother datasets size logistic regression model speci ed 
shows results di erent choices clearly setting srs yields substantially poorer squashing performance section describes ated lds achieves squashing performance comparable starting srs 
note observations parameters model speci ed signi cantly di erent zero 
experiments models parameters indistinguishable zero pa rameters signi cantly di erent zero yielded lds performance results similar reported 
simplicity report results model 
log mse srs lds srs lds lds mle performance base lds repetitions medium scale simulated data 
srs refers performance ofa random sample 
lds srs refers base lds srs maximum likelihood estimator ona random sample lds refers base lds maximum likelihood estimator single pass mother data lds mle refers base lds maximum likelihood estimator mother data 
lds srs lds set df ds lds mle set df ds 
note vertical axis log scale 
mse table performance base lds data 
number evalu ations likelihood data point 
srs lds average mse simple random sampling case divided mse lds improvement factor simple random sampling 
shows comparable results note uses observations compared observations rows 
df ds mse srs lds ds srs replications larger scale application data describe dataset customer records 
binary response variable identi es customers switched long distance carrier 
predictor variables 
continuous level categorical variables 
logistic regression parameters 
consider random squashed samples 
parameters central composite design requires factorial points star points central point total points 
incur signi cant computational ort 
place fully factorial component central composite design evaluated fractional factorial designs resolution design requiring factorial points resolution iv design requiring points box 
brief resolution design confound main ects factor interactions confound factor interactions factor interaction 
resolution iv design confound main ects factor interactions confound factor interactions factor interactions 
table describes results 
lds outperforms srs wide margin provides better squashing formance ds case 
table comparison predictions data logistic regression main ects 
reduced dataset predictive residuals de ned probability dataset probability mother data 
row table describes distribution corresponding residuals reduction method 
method mean stdev min max random sample lds actual parameter estimates mother data rst step algorithm setting possible reduce mse 
extreme setting srs increases mse 
prediction primary goal far emulate mother data parameter estimates 
coarser goal see squashing emulates mother data predictions 
consider data observation dataset assigned probability 
parameter estimates random sample squashed dataset assign probability compared true probability mother data model 
observation mother data compute probability reduced dataset probability mother data multiplied descriptive purposes 
table describes results 
lds performs orders magnitude better simple random sampling outperforms comparable model free method 
evaluation variable selection preceding results demonstrate particular logistic regression model squash dataset allows accurately retrieve parameter estimates model squashed sample 
utility algorithm enhanced ability facilitate analyses analyst formed mother data 
variable selection widely modeling step regression analysis consider question variable selection algorithm applied squashed data select model algorithm select applied mother data 
follows examine possi ble subsets predictor variables subsets score competing models bayesian information criterion bic schwarz 
bic penalized log likelihood evaluated mle bic number datapoints dimensionality data subsets applied mother data random sample squashed dataset select full model 
rank correlation bic scores mother data bic scores squashed data opposed mother data srs comparison 
simulated medium scale mother data datapoints pre see section lds squashed sample selected correct model replications 
comparison srs selected correct model replications 
table shows results 
results suggest possible achieve fold reduction compu tational ort variable selection certain model classes 
facilitate application expensive variable selection algorithms subsets bayesian model averaging massive data 
furthermore costs associated transmitting dataset network greatly reduced variable selection target activity 
note linear certain non linear regression models wilson singhal describe highly cient approach variable selection require maximum likelihood estimation individual model 
table lds logistic regression variable selection 
lds correct shows percentage replications lds selected correct model model selected mother data 
srs correct shows percentage replications simple random sample selected correct model 
model lds srs logit ixi correct correct unif unif evaluation neural networks evaluations far focused logistic regression 
consider application lds logistic regression model perform squashing neural networks 
simulated data feed forward neural network input units hidden layer units single dichotomous output unit ripley 
left hand panel compares test data misclassi cation rate neural network model mother data points test data misclassi cation rate simple random sam ple size black dots lds squashed dataset size red dots 
case predictions holdout sample generated neural network model generated mother data 
results replications 
apparent lds consistently reproduces misclassi cation rate mother data 
right hand panel compares predictive residuals probability reduced dataset probability mother data methods 
table shows results format ble table 
predictive results logistic regression analysis data table application di er ent model class squashing lds substantially outperforms simple random sampling 
reduced data misclassification rate mother data misclassification rate predicted probability mother predicted probability reduced srs lds comparison neural network predictions random sampling lds 
left hand panel shows misclassi cation rates mother data predictions versus reduced data predictions 
right hand panel shows predictive resid 
panels re ect performance hold datapoints generated neural network model generated mother data 
gure replications 
mother data predictions reduced data predictions mother data predictions reduced data predictions mother data predictions reduced data predictions mother data predictions reduced data predictions mother data predictions reduced data predictions mother data predictions reduced data predictions mother data predictions reduced data predictions mother data predictions reduced data predictions mother data predictions reduced data predictions comparison neural network predictions random sampling lds 
scatterplot red dots represent lds predictions black dots represent predictions sampling 
horizontal axis shows pre probabilities neural network tted mother data 
vertical axis shows equivalent predicted probabilities neural network model tted reduced datasets 
points diagonal line predictions agree 
gure shows replications 
table comparison neural network predictions random sampling lds 
reduced dataset residuals hold data de ned probability dataset probability mother data 
row table describes distribution corresponding residuals reduction method 
results averaged replications 
method mean stdev min max random sample lds shows individual predictions replications lds predictions red dots superimposed srs predictions black dots 
points diagonal line represent predictions reduced data prediction mother data prediction agree 
variability prediction random sampling apparent 
note lds srs back propagation algorithm neural network source variability convergence local log likelihood maxima frequently occurs 
iterative lds noted evaluations reported far utilize single pass mother data compute case logistic regression output rst step standard newton raphson algorithm estimating fact provides remarkably accurate estimate results squashing performance close provided setting cases exist high quality pass estimate furthermore passes data required exact estimate iterative lds ilds provides alternative approach 
ilds works follows 
set srs estimate simple random sample mother data 

squash mother data lds requires pass 
table cooling schedule ilds iteration df ds 
squashed data estimate lds 

set lds go 
practice procedure requires iterations achieve squashing performance similar performance achievable iteration requiring pass mother data 
shows mse reduction achievable iterations 
squashed sample mother data generated model repetitions 
experiments reported section reduced df ds iterations proceeded 
table shows schedule results 
generally performance sensitive particular schedule important reduce df ds quickly 
discussion possible re nements lds clustering algorithm base lds assigns datapoint yi cluster minimizes kx yi lc lc denotes average log likelihoods data points cluster note approach independent method log mse iteration squashing performance ilds 
rst iteration sets equal max imum likelihood estimator ona random sample 
subsequent iterations set maximum likelihood estimator squashed sample previous iteration 
mse subsequently select pseudo data points 
obvious alternative assign datapoint yi cluster minimizes kx lc current pseudo point cluster similar optional step cluster phase base lds initial results suggest impact squashing performance negligible 
lds selects single pseudo data point cluster 
contrast ap proach constructs multiple points cluster choosing points match mo ments mother data 
possible combine approaches 
moment matching approach construct points lds derived clusters 
approaches include sampling multiple points cluster selecting multiple points minimize criterion described previous point 
breiman friedman proposed squashing methodology called delegate sampling 
basic idea construct tree datapoints leaves tree approximately uniformly distributed 
delegate sampling samples datapoints leaves inverse proportion density leaf assigns weights sampled points propor tional leaf density 
principle combined lds ds 
evaluations lds assume response variable squashing subsequent analysis 
case wewould expect ds outperform lds 
statistical methods depend strongly local data characteristics trees non parametric regression may particularly challenging squashing algorithms 
concern minor deviations location squashed data points may result substantial changes tted model 
case con approach squashing may promising methods partitioning 
evaluate lds large number input variables large 
neural network context preliminary experiments suggest squashing performance base lds neural networks degrade number units input layer increases 
including interaction terms logistic regression model squashing alleviates problem somewhat 
lds software available madigan research att com 
robert bell simon byers pregibon werner stuetzle chris helpful discussions 
aha albert 

instance learning algorithms 
machine learning 
box hunter hunter 

statistics experimenters design data analysis model building 
john wiley sons new york ny usa box draper 

empirical model building response sur faces 
john wiley sons new york ny usa bradley fayyad reina 

scaling clustering algorithms large databases 
proceedings fourth international conference knowl edge discovery data mining 
breiman friedman 

tool large data set analysis 
statistical signal processing edward wegman james smith eds new york dekker 
catlett 

test ight 
proceedings eighth inter national workshop machine learning 
dumouchel johnson cortes pregibon 

squashing le 
proceedings fifth acm conference knowl edge discovery data mining 
wilson 

regression leaps bounds 
techno metrics gibson vitter wilkes 

report working group storage issues large scale computing 
acm computing surveys 
singhal 

cient screening nonnormal regression mod els 
biometrics 
provost kolluri 

survey methods scaling inductive algorithms 
journal data mining knowledge discovery 
schwarz 

estimating dimension model 
annals statistics 
liu sung 

study support vectors model independent example selection 
proceedings fifth acm conference knowledge discovery data mining 
ripley 

modern applied statistics plus 
springer verlag new york 
zhang ramakrishnan livny 

birch cient data clus tering method large databases 
sigmod 

