building topic dependent maximum entropy model large corpora jun wu sanjeev khudanpur center language speech processing johns hopkins university baltimore md usa jhu edu maximum entropy techniques successfully combine di erent sources linguistically meaningful constraints language models 
current models small corpora computational load training models large corpora 
problem especially severe non local dependencies considered 
show train topic dependent models ciently large corpus broadcast news bn 
training time greatly reduced hierarchical training divide conquer approaches 
computation model simpli ed pre normalizing denominators model 
report new speech recognition results showing improvement topic model relative standard gram model broadcast news task 

non local dependencies topic sentence improve performance statistical language models tasks 
maximum entropy techniques provide natural way combine topic dependencies conventional gram constraints uni ed model consecutive words sentence topic sentence binary feature functions parameters normalization factor 
model successfully implemented reduce speech recognition errors switchboard task :10.1.1.52.4206
fundamental diculty exists implementing model large corpora due heavy computational cost training procedure 
solve problem approaches hierarchical training method divide conquer strategy 
brie show exploit topic dependencies broadcast news corpus section 
section introduce background language modeling describe ecient training algorithms topic dependent models 
section describe ecient way computing probabilities models 
section shows experimental results broadcast news bn 

exploiting topic dependencies broadcast news corpus contains documents amounting words text 
document processed obtain tf idf vector vocabulary terms excluding words 
modi ed cosine similarity measure employed measure distance vectors 
vectors clustered means procedure initial cluster assignments derived bottom classi er 
words unigram frequency topic cluster varies considerably frequency corpus chosen topic related words 
select words log bn words related topic roughly words topic cluster amounting topic word dependencies totally 
model trained topic constraints corpus size words addition standard grams 
assign fresh topic test utterance allow changes topic story progresses 
yields better recognition accuracy topic assignment entire story :10.1.1.52.4206

training model language modeling interested conditional probability yjx word vocabulary history 
language model form yjx number features 
generalized iterative scaling gis algorithm variant improved iterative scaling iis provide iterative procedure estimate model 
term frequency inverse document frequency 
word list words low semantic content ignored topic classi er 
desired expectation 
yjx 
current expectation starting arbitrary algorithms obtain model marginal distribution approximated empirical distribution sake simpli cation 
computation complexity jxj 
jv jxj number seen histories training text 
number enormous 
instance number simple trigram model bn corpus 
ecient model estimation methods shown sections 

computing denominator della partition feature set subsets unigram marginal feature subset features independent history subset features history dependent 
cache computational results related unigram features reduce computational time jxj 
jv average number words conditional features activated history 
typically orders magnitude smaller vocabulary size jv approach unigram caching regarded classical technique training model treated baseline method compare running time 
hierarchical training method extends idea simpli es computation gram models 
basic idea share computation related gram calculations grams ends 
complexity linear number di erent grams 
example complexity jv trigram model numbers active bigrams trigrams respectively 
see section details 
eciency hierarchical training method mainly comes nested structure features 
ecient models overlapping features 
simpli cation methods 
guarantee running time size training set 
know algorithm running time models overlapping features train model certain kinds non nested features topic models eciently gram models 
essential idea divide conquer 
topic model normalization factor 


notice value topic feature independent topic take advantage simplify computation algorithm step partition training text topics 
step training data topic denoted collect grams 
step compute topic de ne 

gk nested features 
step hierarchical training method section compute xed number multiplication step exactly number topic features 
complexity step jd topic strictly bounded 

feature expectation simpli cation computing applied estimation feature expectations 
approaches previous section simplify estimation feature expectations 
show estimate time consuming parts expectations topic features unigram features 
estimation feature expectations similar 
topic model expectation topic feature note rst summation xed computed jx jx number histories topic reused topic features ii second summation byproduct needs extra computation 
number terms summed number di erent iii third summation number terms summed number combinations complexity feature expectation 
unigram features 


complexity strictly bounded 

computation testing addition training time problem may prevent models real speech recognition system time calculate probabilities 
recall back trigram model threshold bow relative frequency smoothing bow remaining probability mass unseen bigram probability backed unigram probability estimate 
computation probability model involves operations table lookup 
contrary model needs summation hundreds thousands terms obtain denominator practical necessary compute normalization factor jv histories advance 
models gram models exactly way backo models 
show method example trigram model 
map parameters model back 




bow 
bow obtain model arpa back format 
extend method model gram features 
models overlapping features topic model corresponding back models map 
approximate normalization factor history independent constant plus terms pre computed need compute line 
instance normalization factor rewritten yw 
de ned yw sets words gram features topic features activated respectively rst terms history independent depending partial histories pre computed 
observe term relatively small topic model omit 
course approximation may result errors 
question error 
approximation topic model simple back model 
compared word error rate wer approximate model exact model 
results shown section 
experimental results experiments broadcast news corpus 
test set dev test set containing utterances amounting words 
vocabulary words contains words acoustic training data words occur times language model training text 

speed training time roughly proportion total number terms summed 
count number estimate running time training method number approaches baseline unigram caching hierarchical training hierarchical training plus divide conquer dc 
nominal speed ups de ned speed terms baseline method terms new method shown table 
hierarchical training achieves nominal speed folds compared 
divide conquer reduces computational load folds hierarchical training 
total speed orders magnitude 
train model methods 
baseline method computational load computers process 
hierarchical training divide conquer reduces training time cpu hour tremendously compared hierarchical training table 
method terms speed unigram caching 
hierarchical 
hierarchical dc 

table nominal speed method training hours speed hierarchical hierarchical dc table training time iteration real speed pre normalized models rescoring best hypotheses running time percent needed original models 

performance topic model rst rows table show performance standard back trigram model corresponding model constraints 
minimum count bigram included model trigram 
model just replicates performance model perplexity wer back gram gram topic topic approx na table perplexity wer back trigram model maximum entropy models 
back gram model gram constraints 
improvement degradation adding constraints attributable features method 
perplexity topic dependent model row reduces compared trigram model wer reduces absolute approximated model tested 
obtain perplexity model output real probability 
language model scores generates re score hypotheses obtain wer result 
see approximation degrade speech recognition accuracy wer identical original model row vs row 

interpolated topic grams alternative approach build interpolation topic models individual topic 
choose interpolation weight minimize perplexity test set interpolated model 
topic assignment topic model 
best interpolation models row table obtains similar perplexity wer model params ppl wer back gram bo topic gram bo topic gram bo topic gram topic table comparison interpolated topic gram models 
model 
approach integrates topic dependent topic independent constraints compact way interpolation method increases model size interpolation models increase size fold 
method practical interpolation method large corpora adaptation 

acknowledgments substantial assistance topic classi cation provided radu florian david yarowsky 
particular radu florian provided initial topic tags story bn training text 
shankar kumar helped generate best lists rescoring 

darroch ratcli generalized iterative scaling log linear models annals math 
stats vol 
pp 
della pietra della pietra la erty inducing features random fields ieee trans 
pami pp april 
florian yarowsky dynamic nonlocal language modeling hierarchical topic adaptation proceedings acl pp june 
khudanpur wu maximum entropy language model integrate grams topic dependencies conversational speech recognition proceedings icassp pp :10.1.1.52.4206
la erty suhm 
cluster expansions iterative scaling maximum entropy language models maximum entropy bayesian methods kluwer academic publishers 
iyer ostendorf modeling long range dependence language proceedings icslp pp oct 
wu khudanpur ecient training methods maximum entropy language modeling proceedings icslp vol 
pp 
oct beijing china 
