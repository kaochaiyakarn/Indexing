webbase repository web pages jun hirai sriram raghavan hector garcia molina andreas paepcke system integration technology center toshiba tokyo japan computer science department stanford university stanford ca usa jun hirai toshiba jp hector paepcke db stanford edu study problem constructing maintaining large shared repository web pages 
discuss unique characteristics repository propose architecture identify functional modules 
focus storage manager module illustrate traditional techniques storage indexing tailored meet requirements web repository 
evaluate design alternatives experimental results prototype repository called webbase currently developed stanford university 
keywords repository webbase architecture storage management number important applications require local access substantial portions web 
examples include traditional text search engines related page services topic search categorization services 
applications typically access mine index local cache repository web pages performing analyses directly web slow 
example google search engine computes pagerank web page recursively analyzing web link structure :10.1.1.109.4049
repository receives web pages crawler component responsible mechanically finding new modified pages web 
time repository offers applications access interface api may efficiently access large numbers date web pages 
study design large shared repository web pages 
architecture repository consider evaluate various implementation alternatives describe prototype repository developed part webbase project stanford university 
prototype collection web pages testbed study different storage indexing data mining techniques 
earlier version prototype back storage system google search engine 
new prototype intended offer parallelism multiple storage computers support wider variety applications opposed just text search engines 
prototype currently implement features components important functions services place 
web repository stores manages large collection data objects case web pages 
conceptually different systems store data objects file systems database management systems information retrieval systems 
web repository need provide lot functionality systems provide transactions general directory naming structure 
web repository optimized provide just essential services provide scalable efficient way 
particular web repository needs tuned targeted provide scalability size growth web paramount repository scale large numbers objects 
ability seamlessly distribute repository cluster computers disks essential 
particular interest network disks hold repository 
network disk disk containing processor network interface allows connected directly network 
network disks provide simple inexpensive way construct large data storage arrays may appropriate web repositories 
streams repository needs provide access individual stored web pages demanding access bulk large collections pages indexing data mining 
repository support stream access instance entire collection scanned fed client analysis 
eventually repository may need support ordered streams pages returned high speed order 
instance data mining application may wish examine pages increasing modified date decreasing page rank 
large updates web changes rapidly 
repository needs handle high rate modifications 
new versions web pages arrive space occupied old versions reclaimed history maintained consider 
means substantially space compaction reorganization file data systems 
repository strategy avoid excessive conflicts update process applications accessing pages 
pages file data systems objects explicitly deleted longer needed 
web page removed web site repository notified 
repository mechanism detecting obsolete pages removing 
akin garbage collection counting 
study build web repository meet requirements 
particular propose repository architecture supports required functionality high performance 
architecture amenable require network disks 
alternatives distributing web pages computers disks 
consider different mechanisms staging new pages provided crawler applied repository 
consider ways crawler repository interact including batch updates incremental updates 
study strategies organizing web pages node computer system 
consider space compaction reorganization performed scheme 
experimental results prototype simulated comparisons approaches 
sheds light available design options illustrates nature workload terms crawling speed streaming rate determines appropriate design choices 
goal cover wide variety techniques keep space limitations forced restrictions scope 
particular assumptions operations crawler repository 
alternatives interesting important simply covered 
assume crawler incremental visit entire web time runs 
crawler merely visits pages believes changed created run 
crawlers scale better web grows 
repository maintain temporal history multiple versions web 
words latest version web page retained repository 
repository stores standard html pages 
media document types ignored crawler 
indexes constructed snapshot view contents repository 
words indexes represent state repository successive crawler runs 
updated crawler run incrementally 
rest organized follows 
section architectural description various components repository section concentrate fundamental components architecture storage manager 
section results experiments conducted evaluate various options design storage manager section survey related 
conclude section 
architecture webbase architecture depicts architecture webbase system terms main functional modules interactions 
shows main modules crawler storage manager metadata indexing module multicast module query engine 
connections modules represent exchange information indicated directions 
crawler module responsible retrieving pages web handing storage management module 
crawler periodically goes web retrieve fresh copies pages existing repository pages crawled 
storage module performs various critical functions include assignment pages storage devices handling updates crawler fresh crawl scheduling servicing various types requests pages 
focus storage module section provide overview components 
metadata indexing module responsible extracting metadata collected pages indexing pages metadata 
metadata represents information extracted web pages example title creation date set outgoing urls 
may include information obtained analyzing entire collection 
instance number incoming links page coming pages citation count computed included metadata 
module generates indexes metadata web pages 
indexes may include traditional full text indexes indexes metadata attributes 
example index citation count quickly locate web pages having say incoming links 
metadata indexes stored separate devices main web page collection order minimize access conflict page retrieval query processing 
prototype implementation simple metadata attributes stored indexed relational database 
query engine multicast module provide access content stored repository 
roles described subsection 
access modes repository supports major access modes retrieving pages random access query access streaming access random access mode specific page retrieved repository specifying url unique identifier associated page 
query access mode requests set pages specified queries characterize pages retrieved 
queries may refer metadata attributes textual content web pages 
example suppose indexing module maintains index words title web page index hypertext links pointing page 
indexes respond query retrieve urls pages contain word stanford title point www db stanford edu 
query engine shown responsible handling query accesses repository 
streaming access streaming access mode substantial portion pages repository retrieved delivered form data stream directed requesting client application 
access mode unique web repository important applications need deal large set pages 
example search applications mentioned section require access millions pages build indexes perform analysis 
particular webbase system streaming interface indexing module retrieve pages repository build necessary indexes 
multicast module responsible handling external requests streaming mode access 
particular multiple clients may concurrent stream requests 
streams organized properly clients may share transmitted pages 
goal webbase repository streams available just locally remote applications institutions 
unnecessary sites crawl store web 
believe efficient multicast streams single repository internet opposed having multiple applications crawling hitting web sites occasions requesting copies pages 
initially webbase supports stream requests entire collection web pages arbitrary order best suits repository 
webbase supports restartable streams give client ability pause resume stream 
requires state information stream continuously maintained stored repository client pages missed delivered multiple times 
stream requests extended include requests subsets pages get pages edu domain arbitrary order 
eventually plan introduce order control applications may request particular delivery orders pages increasing page rank 
currently investigating strategies combining stream requests different granularities orders order improve data sharing clients 
page identifier web page fundamental logical unit managed repository important defined mechanism modules uniquely refer specific page 
webbase system page identifier constructed computing signature checksum cyclic redundancy check url associated page 
url multiple text string representations 
example www stanford edu www stanford edu represent web page give rise different signatures 
avoid problem normalize url string derive canonical representation 
compute page identifier signature canonical representation 
details follows normalization url string normalized executing steps removal protocol prefix removal port number specification non standard port number specifications retained conversion server name lower case removal trailing slashes resulting text string hashed signature computation yield bit page identifier 
hashing function implies non zero collision probability 
hash function large space hashes occurrence 
example bit identifiers pages repository probability collision 
repositories collision 
bit identifiers page collection probability collision 
see discussion derivation general formula estimating collisions 
storage manager section discuss design storage manager 
module stores web pages local disks provides facilities accessing updating stored pages 
storage manager stores latest version page retrieved crawler 
goal store latest version history page 
issues require consideration consistency indexes page referenced indexes removed repository version page retrieved crawler 
pages versions need temporarily exist indexes modified 
requirement impacts functioning various update schemes defer discussion issue section 
pages storage manager free pages longer exist web 
crawler explicitly indicate pages removed web sites responsibility storage manager ensure old copies non existent pages periodically 
traditional garbage collection algorithms reclaim space discarding objects longer referenced 
inherent assumption data objects available testing non referenced objects identified 
differs situation aim detect soon possible object deleted remote location web site similarly deleted local copy repository 
cleanup storage manager associates numerical values page repository allowed lifetime lifetime count 
allowed lifetime represents time page remain repository refreshed replaced 
page crawled time new version page received crawler page lifetime count set allowed lifetime 
lifetime count pages regularly decremented reflect amount time repository 
periodically storage manager runs background process constructs list urls corresponding pages lifetime count reach 
forwards list crawler attempts visit urls crawling cycle 
urls list pages received crawler update cycle removed repository 
crawler indicates unable verify existence certain page possibly network problems page 
lifetime count set small value ensure included list time 
note crawler parameters deciding periodicity individual pages crawled 
list provided storage manager addition pages crawler intends visit 
scalability storage manager distributed collection storage nodes equipped processor disks ability connect high speed communication network 
webbase node network disk regular computer 
coordinate nodes storage manager employs central node management server server maintains table parameters describing current state storage node 
parameters include total storage capacity occupied space free space node extent fragmentation storage device current state node possible states include idling streaming storing significance states clear update operations number outstanding requests page retrieval types random access query streaming mode information node management server allocates storage nodes service requests stream accesses 
schedules controls integration freshly crawled pages repository 
remainder section discuss design issues storage manager distribution pages storage nodes section organization pages storage device maximum efficiency streaming random access section update mechanism integrate freshly crawled pages system section page distribution nodes consider policies distributing pages multiple storage nodes 
uniform distribution storage nodes treated identically page assigned nodes system 
hash distribution page identifier computed signature url described section decide allocation pages storage nodes 
storage node associated range identifiers contains pages identifiers fall range 
hash distribution policy requires sparse global index locate node page identifier located 
global index fact implicit interpret portion say high order bits page identifier denoting number storage node page belongs 
comparison uniform distribution policy requires dense global index maps page identifier node containing page 
hand imposing fixed relationships page identifiers nodes uniform distribution policy simplifies addition new storage nodes system 
hash distribution require form extensible hashing 
reason uniform distribution policy robust failures 
failure nodes crawler providing new pages handled allocating new incoming pages remaining nodes 
hashing incoming page falls failed node special recovery measures called 
organization pages disk storage node capable efficiently supporting operations page addition high speed streaming random page access 
subsection describe ways organize data node support operations hash organization log structured organization hashed log organization 
defer analysis pros cons methods section describe experiments aid comparison 
hash organization hash organization treats disk collection hash buckets 
bucket small read memory processed written back disk 
bucket stores pages allocated node identifiers fall bucket range 
note range different range page identifiers allocated storage node hash distribution policy section 
buckets associated successive ranges identifiers assumed physically continuous disk excluding overflow buckets 
assume bucket pages stored increasing order identifiers 
note time portion space allocated hash bucket occupied pages rest free space 
clearly organization efficient random page access need local index map page identifier physical location disk 
streaming supported efficiently sequentially reading buckets disk order physical locations 
effective streaming rate fraction maximum disk transfer rate fraction average space utilization hash buckets 
performance hash organization page addition depends order pages received 
new pages received purely random order page addition require read relevant bucket followed memory update disk write flush modified bucket back disk 
space old unwanted pages reclaimed part process 
note buffering useful probability buffered pages hash bucket low size number hash buckets typical disk 
hand pages received order page identifiers efficient method possible 
particular bucket read disk batch new pages added written disk 
memory addition simple incoming stored pages order 
result buffering pages guaranteed effective unsorted case 
main memory available bucket read memory merged incoming pages allowing disk operation amortized pages cf 
scenario section 
log structured organization log structured page organization principles log structured file system lfs described 
new pages received node simply appended log making process efficient 
specific storage node maintains objects disk large log occupies space available disk includes pages allocated disk single continuous chunk catalog contains entry corresponding page log 
typical catalog entry includes information identifier page question pointer physical location page log size page status page valid deleted semantics states clear update strategies discussed timestamp denoting time page added repository random access page required local tree index maps page identifier corresponding location page maintained 
typical network pc disk sizes average web page sizes number pages log small leaves tree need reside disk 
assume disk access required retrieve entry tree index 
new pages appended log 
assume catalog tree necessarily kept continuously date disk batch mode page addition extremely efficient involves successively writing contiguous portions disk 
required modifications catalog buffered memory periodically flushed disk 
log space eventually compacted remove old unwanted pages 
page addition completes tree index updated 
random page access requires disk accesses read appropriate tree index block retrieve physical position page retrieve actual page 
streaming restrictions stream order efficient merely involves sequential read log 
note assumes pages log time streaming valid 
batch update systems perform disk compaction cf 
section assumption guaranteed true 
systems guarantee additional disk accesses needed examine catalog discard pages status flag set valid 
hashed log organization name suggests hashed log organization hybrid organization methods 
scheme disk contains number large logs associated catalogs similar structure smaller single log pure log structured organization 
individual logs typically mb size 
hash buckets log file associated range hash values stores pages assigned node page identifier falls range 
major differences hash buckets logs hashed log organization 
logs bigger hash buckets expensive read memory random page access 
second pages stored sorted order log 
result efficient random access hashed log node requires tree index 
page addition hashed log node involves buffering pages memory extent possible appending appropriate logs identifiers 
important feature organization purposes ability stream pages sorted order 
accomplish logs read memory order associated hash range values sorted memory transmitted 
note assume logs bigger hash buckets small fit completely memory 
motivation organization discussed section 
update schemes assume updates proceed cycles 
crawler collects set new pages incorporated repository metadata indexes built new snapshot new cycle begins 
model period time crawl completion index rebuilding older versions pages referenced existing index need retained 
ensure ongoing page retrieval requests query access streaming mode access disrupted 
classify pages repository class old versions pages referenced current active indexes newer versions exist repository 
class unchanged pages pages version exists crawled time index built time latest crawl executed 
class include pages seen new versions pages older versions exist class pages 
words pages received crawler crawling cycle class pages 
update process consists steps receive class pages crawler add repository 
rebuild indexes class class pages 
delete class pages 
system accept random query page access requests entire update operation complete possible exchange order execution steps 
case class pages need retained indexes rebuilt 
batch update method described section operates manner 
batch update scheme briefly describe incremental update scheme section 
additional options 
example full copies repository alternate copy updated 
discuss strategies 
batch update scheme update scheme storage nodes system partitioned sets update nodes read nodes 
freshly crawled class pages stored update nodes class class pages stored read nodes 
definition active index set rebuilding class class pages requests page retrieval involve read nodes 
analogously update nodes involved receiving storing pages retrieved crawler 
illustrates flow data crawler sets nodes batch update process 
steps batch update follows batch update strategy 
system isolation 
multicast module stops accepting new stream requests 

crawler finishes adding class pages update disks 

queries suspended system waits ongoing stream transfers complete 

page transfer class pages transferred update nodes read nodes class pages removed read nodes 
details operations depend page organization scheme page distribution policy 
discuss examples page transfer section 

system restart 
class pages stored update nodes removed 
needed crawler restarted start populating update nodes 

pages read nodes streamed indexing module enable index reconstruction 
external requests streaming access accepted provided involve access indexes 

indexes rebuilt read nodes start accepting random query requests 
exact mechanism transfer pages update read nodes depends page organization distribution policy set nodes organization policy different sets 
follows illustrate possible scenarios transfer 
scenario log structured organization hash distribution policy sets illustration assume update nodes read nodes 
crawler computes identifier new page obtains 
pages quarter identifier range stored update node pages second quarter go node 
crawl cycle ends update node sub partition allocated identifier range subranges send pages read disks 
similarly update disk partition pages read disks 
read disk receives pages update disk 
sequence steps transferring pages read nodes follows 
update node constructs lists li list li contains identifiers class pages currently destined read node 


suppose read node receives list pages update node computes intersection denotes list identifiers corresponding pages currently stored note directly available scan catalog 
definition represents set class pages read node catalog entries pages located status flags modified indicate deleted 
read nodes go compaction mode reclaim space created deletion class pages 
update node begins transmitting streams class pages corresponding read nodes 
stream contains exactly pages destined receiving read node 
pages received read node necessary builds local tree index support random access 
scenario hash organization hash distribution policy sets hash node organization allows certain optimizations transferring pages read nodes 
corresponding class class pages guaranteed hash bucket deletion class pages occupy separate step performed conjunction addition class pages 
steps follows 
update node reads hash buckets memory order physical locations disk 
result section pages read increasing order identifiers 

page retrieved disk update nodes determine read node page forwarded transmit page accordingly 

read node begins receive sorted stream pages update nodes 

read nodes read hash buckets memory order physical locations disk 
execute merge sort involves incoming sorted stream pages read disk 
part merge sort page arriving stream identifier pages retrieved disk preferred discarded 
corresponds replacing class page corresponding class page 
resulting merged output written disk modified hash buckets 
illustrates merge sort executed 
addition new pages merge sort advantages batch update scheme attractive characteristic batch update scheme lack conflict various operations executed repository 
single storage node deal page addition page retrieval concurrently 
useful property physical locations pages read nodes change updates 
compaction operation potentially change physical location part update 
helps greatly simplify state information required support restartable streams described section 
restartable stream state information merely consists pair values node id page id node id represents unique identifier storage node contained page transmitted client interruption page id represents identifier page 
incremental update scheme incremental update scheme division read update nodes 
nodes equally responsible supporting update access simultaneously 
crawler allowed place freshly crawled pages nodes system long conforms page distribution policy 
analogously requests pages access modes may involve nodes system 
extraction metadata construction indexes undertaken periodically independent new pages repository 
result incremental update scheme capable providing continuous service 
batch update scheme need isolate system update 
addition new pages repository continuous process takes place conjunction streaming random access 
scheme possible provide crawled pages clients streaming random access modes metadata indexes associated pages may available 
continuous service drawbacks performance penalty performance may suffer conflicts multiple simultaneous read update operations 
performance penalty alleviated extent employing uniform distribution policy having node management server try balance loads 
example node management server detects set nodes busy responding number high speed streaming requests ensure addition new pages crawler take place nodes 
page addition requests redirected lightly loaded nodes 
requires dynamically maintained local index consider case incremental update scheme employed conjunction log structured page organization 
holes created removal class pages reclaimed compaction effect altering physical location stored pages 
necessary dynamically maintain local index map page identifier current physical address 
necessary batch update scheme physical location pages read nodes unaltered successive updates 
restartable streams complicated incremental update conjunction hash page organization state information required support restartable streams gets complicated physical locations pages preserved bucket overflows 
turns case log structured organization possible execute compaction way despite incremental update simple state information batch update system suffices 
extended version discusses issues detail 

experiments conducted experiments compare performance design choices 
section describe selected results experiments discuss various system parameters influence best configuration terms update strategy page distribution policy page organization method storage manager 
experimental setup webbase prototype includes implementation storage manager configuration update strategy batch update page distribution policy update read nodes hash distribution page organization method sets nodes log structured organization storage manager fed pages incremental crawler retrieves pages rate approximately pages second 
webbase storage manager run network disks conventional pcs 
experiments report cluster pc connected mbps ethernet lan debugging data collection easier pcs 
addition repository implemented client module crawler emulator 
client module sends requests random streaming mode accesses repository receives pages read nodes response 
crawler emulator retrieves stored web pages earlier version repository transmits update nodes controllable rate 
emulator actual crawler provided necessary flexibility control crawling speed interacting actual web sites 
ease implementation storage manager implemented top standard linux file system 
order conform operating system limit maximum file size log structured organization approximated creating collection individual log files approximately mb size node 
compare different page distribution node organization alternatives conducted extensive simulation experiments 
simulation hardware parameters selected match prototype hardware 
allowed verify simulation results prototype scenario prototype implements batch updates hash page distribution log structured nodes 
scenarios simulator allows predict prototype performed chosen strategies 
performance results section simulator table report performance actual prototype 
performance metrics comparing different system configurations expressed terms number pages second node page addition rate maximum rate system able receive new pages add repository 
random page access rate random page access refers retrieval certain page repository specifying identifier associated page 
random page access rate maximum rate requests serviced system 
streaming rate refers rate pages system retrieved transmitted imposing specific transmission order 
note performance metrics node basis 
enables results independent number nodes actual system 
systems batch update inherent parallelism operations implies page addition rate system random page access rate simply node value multiplied number update nodes number read nodes assume network bottleneck 
incremental update systems scale perfectly linear conflict operations 
batch update systems additional performance metric batch update time 
time repository isolated provide page access services 
selected experimental results 
set results include comparison different page organization methods described section comparison different system configurations 
second set results measure performance implemented prototype 
additional experiments discussions illustrate optimization process carried tune nodes handle web data included extended version 
comparing different systems section selected results simulator 
simplicity consider network performance assume network capable handling traffic 
performance solely determined disk characteristics disk access pattern associated system configuration 
architecture simulator details simulation process described extended version 
comparing page organization methods section table analyze page organization methods section 
note table deals performance characteristics single node 
system performance dependent performance individual node influenced factors page distribution policy update strategy 
discussed succeeding sections 
performance characteristics table compares organizations performance characteristics 
streaming pages tightly packed log log structured node stream pages faster hash node 
hashed log node matches high streaming rate log structured organization generates sorted stream 
random read hash node local index random reads able read pages higher rate higher log structured node 
page addition random log structured node clearly append new pages higher rate hash node 
increasing available memory mb improves add rate hash node pages sec way performance log structured node 
observed improvement merely buffering allows single disk sweep elevator algorithm update hash buckets 
hashed log node distribute pages multiple logs page addition rate log structured organization 
times better page addition rate possible hash organization 
page addition sorted hand pages received sorted order identifier merging technique section improve page addition performance hash organization orders magnitude 
performance metric log structured hash hashed log streaming rate ordering pages sec unsorted pages sec sorted pages sec sorted random page access rate page addition rate random order buffering page addition rate random order mb buffer page addition rate sorted order mb buffer pages sec pages sec pages sec pages sec pages sec pages sec pages sec pages sec pages sec pages sec pages sec pages sec table comparing page organization methods single node simulation hash uses kb buckets average occupancy space usage log structured organization requires space maintain catalog information 
experience catalog typically contributes additional space overhead implies effective space utilization 
hash organization disk space utilization function empty full hash buckets 
analysis sample data repository indicates average space utilization organization typically 
motivation hashed log results table suggest batch system update nodes log structured support large throughput crawler read nodes hash support high random read traffic 
achieve performance read nodes update pages transferred update nodes arrive sorted order 
log structured organization capable providing sorted stream 
hashed log organization provides exactly facility conjunction reasonably high page addition rate 
comparing different configurations compare different system configurations require notation easily refer specific configuration 
adopt convention incr denotes system uses incremental update scheme policy distributing pages nodes organization method organize pages node 
hash uniform hash log 
batch denotes system uses batch update scheme uses policy organization method update nodes policy organization method read nodes 
example system configuration prototype discussed section represented batch hash log hash log 
table presents sample performance results different system configurations batch update method employ hash distribution policy hash page organization update nodes 
experiment assume pages read nodes replaced newer versions update process 
call update ratio 
configurations differ organization method employ update nodes 
center column gives page addition rate supported single update node derived earlier 
multiply entries number update nodes get total rate crawler deliver pages 
third column gives total time perform batch update read disks represents time repository unavailable clients 
configuration uses hashed log organization update nodes provides best balance page addition rate reasonable batch update time 
note parallelism available batch update systems update time depend number nodes purely determined update ratio 
system configuration page addition rate pages sec node batch update time update ratio batch hash log hash hash secs batch hash hash hash hash secs batch hash hashed log hash hash secs table sample performance results different configurations simulation experiments system performance table summarizes results experiments conducted directly prototype 
prototype employs log structured organization sets nodes exhibits impressive performance streaming page addition 
note results table include network delays numbers lower predicted table 
particular streaming rate measured client module page addition rate emulated crawler sees 
performance metric observed value streaming rate pages sec read node page addition rate pages sec update node batch update time seconds update ratio random page access rate pages sec read node table performance prototype actual measurements plots variation batch update time update ratio prototype 
update ratio refers fraction pages read nodes replaced newer versions 
prototype system uses batch update process stages corresponding scenario section 
shows stage contributes batch update time 
note axis cumulative curve includes sum contributions stages represented 
example update ratio see catalog update page identifier transfer tree construction require seconds respectively 
compaction requires secs page transfer requires seconds 
domination compaction page transfer holds update ratios 
addition shows time page transfer remains constant independent update ratio 
increase update ratio requires corresponding increase number update nodes accommodate larger number pages received crawler 
page transfer operation update node executes independently simultaneously able achieve perfect parallelism keep page transfer time constant 
compaction hand exhibits marked decrease increase update ratio 
reasonable higher update ratios class pages deleted leaving smaller set class pages moved read nodes compaction 
summary batch update time prototype wide spectrum system configurations web repository different strengths weaknesses 
choice appropriate configuration influenced deployment environment anticipated workload functional requirements 
factors influence choice crawling speed required random page access performance required streaming performance node computing power storage space importance continuous service 
example environment includes high speed crawler configurations perform poorly page addition incr hash hash batch hash hash suitable 
similarly continuous service essential certain environment batch update schemes applicable 
table presents summary relative performance useful system configurations 
table symbols represent order spectrum values favorable favorable performance metric 
system configuration streaming random page access page addition update time incr hash log inapplicable incr uniform log inapplicable incr hash hash inapplicable batch hash log hash log batch hash log hash hash batch hash hash hash hash batch hashed log hash hash hash table relative performance different system configurations 
related nature services infer web search engines construct access web repository 
proprietary specific search application 
attempted discuss application independent manner functions features useful web repository proposed architecture provides functions efficiently 
number web services web repositories part system architecture 
repositories constructed smaller scale restricted purpose 
example system allows users explore changes world wide web web structure supporting recursive document comparison 
tracks changes user specified set web pages difference engine aide provides graphical visualization tool top aide 
aide version repository retrieves stores pages explicitly requested users 
size repository typically smaller sizes targeted webbase 
similarly called webglimpse provides text indexing neighborhood search facilities existing repositories web pages 
emphasis actual indexing facility construction maintenance repository 
internet archive project aims build digital library long term preservation web published information 
focus project addressing issues relevant archiving preservation 
target client population consists scientists sociologists journalists historians want information research purposes 
hand focus webbase architecting web repository way kept relatively fresh able act immediate current source web information large number existing applications 
log structured organization storage manager log structured file systems described 
essential differences systems 
multiple disks webbase enables separate read update operations disjoint sets nodes 
second difference need webbase support high speed streaming operation part anticipated workload design lfs 

proposed architecture structuring large shared repository web pages 
argued construction repository calls judicious application new existing techniques 
discussed design storage manager detail qualitative experimental analysis evaluate various options stage design 
webbase prototype currently developed architecture section 
currently working implementations incremental crawler storage manager indexing module query engine available 
low level networking file system operations implemented query interface implemented java 
plan implement experiment advanced system configurations section 
plan develop advanced streaming facilities discussed section provide client control streams 
eventually plan enhance webbase maintain history web pages provide temporal information 
wish members stanford webbase project contributions design implementation webbase prototype 
wish quantum donating network disks build prototype 
alexa incorporated www alexa com altavista incorporated www altavista com sergey brin larry page anatomy large scale hypertextual web search engine proc :10.1.1.109.4049
th intl 
www conference april 
junghoo cho hector garcia molina incremental crawler evolution web technical report department computer science stanford university 
available www db stanford edu cho papers cho incre ps 
arturo crespo hector garcia molina archival storage digital libraries third acm conference digital libraries june 
fred douglis thomas ball yih chen koutsofios internet difference engine tracking viewing changes web world wide web volume january 
fred douglis thomas ball yih chen koutsofios querying navigating changes web repositories proc 
th intl 
www conference may 
fred douglis anja feldmann balachander krishnamurthy jeffrey mogul rate change metrics live study world wide web usenix symposium internet technology systems dec 
google incorporated www google com jun hirai sriram raghavan hector garcia molina andreas paepcke webbase repository pages stanford digital libraries project technical report wp computer science dept stanford university nov 
available www stanford edu wp public doc html internet archive www archive org steve lawrence lee giles accessibility information web nature vol 
july 
udi manber mike smith gopal webglimpse combining browsing searching proc 
usenix technical conference jan 
national storage industry consortium nasd project www org nasd mendel rosenblum john design implementation log structured file system proc 
th acm symposium operating systems principles oct pages 
craig wills mikhail better understanding web resources server responses improved caching proc 
th intl 
www conference may 
paul wilson uniprocessor garbage collection techniques proc 
intl 
workshop memory management sept pages 
yahoo incorporated www yahoo com jun hirai jun hirai received electrical electronic engineering university tokyo japan respectively 
joined toshiba engaged research development area network management telecommunication technology 
visiting scholar computer science department stanford university 
current interests include information management issues various scales ranging personal level world wide web internet related technologies 
sriram raghavan sriram raghavan currently ph student computer science department stanford university stanford california 
received bachelor technology degree computer science engg 
indian institute technology india 
research interests include information management web large scale searching indexing database ir systems integration query processing 
hector garcia molina hector garcia molina leonard sandra lerner professor departments computer science electrical engineering stanford university stanford california 
august december director computer systems laboratory stanford 
faculty computer science department princeton university princeton new jersey 
research interests include distributed computing systems database systems 
received bs electrical engineering instituto de mexico 
stanford university stanford california received ms electrical engineering phd computer science 
garcia molina fellow acm received acm sigmod innovations award member president information technology advisory committee 
andreas paepcke dr andreas paepcke senior research scientist director digital library project stanford university 
years object oriented technology address interoperability problems context distributed digital library services 
second interest exploration user interface systems technologies accessing digital libraries small handheld devices pdas 
dr paepcke received bs ms degrees applied mathematics harvard university ph computer science university karlsruhe germany 
previously worked researcher hewlett packard laboratory research consultant xerox parc 
footnotes performed jun hirai visiting scholar computer science department stanford university 
