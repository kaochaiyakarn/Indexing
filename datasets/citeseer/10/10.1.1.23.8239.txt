object adaptive load balancing mpi programs milind kal eric de jay center simulation advanced rockets university illinois urbana champaign milind cs uiuc edu october parallel computational science engineering cse applications exhibit irregular structure dynamic load patterns 
applications developed procedural languages fortran message passing parallel programming paradigm mpi distributed memory machines 
incorporating dynamic load balancing techniques application level involves significant changes design structure applications 
hand traditional run time systems mpi support dynamic load balancing 
object parallel programming languages charm support efficient dynamic load balancing object migration irregular dynamic applications deal external factors cause load imbalance 
converting legacy mpi applications object paradigms cumbersome 
describes implementation mpi called adaptive mpi supports dynamic load balancing multithreading mpi applications 
approach implementation user level migrating threads load balancing capabilities provided charm framework 
conversion legacy codes platform straightforward large legacy codes 
converted component codes rocket simulation application 
experience shows minimal overhead effort incorporate dynamic load balancing capabilities legacy fortran mpi codes 
computational science engineering cse applications development today exhibit dynamic behavior 
computational domains irregular making research funded department energy university california tract number 
difficult subdivide problem partition equal computational load optimizing communication 
addition computational load requirements partition may vary computation simulation complex system progresses 
example applications adaptive mesh refinement amr techniques increase resolution spatial discretization partitions interesting physical phenomena occur 
increases computational load partitions drastically 
example applications simulation pressure driven crack propagation finite element method fem extra elements inserted near crack dynamically propagates structures casing solid fuel rockets leading severe load imbalance 
type dynamic load variance seen heterogeneous computational platforms clusters workstations carry regular applications bk 
cases availability individual workstations changes dynamically 
load imbalance reduced decomposing problem smaller partitions available physical processors mapping remapping partitions physical processors response variation load conditions 
expect application programmer pay attention dynamic variations computational load communication patterns due internal external factors described addition programming complex cse application 
parallel programming environment needs provide dynamic load balancing hood 
traditional runtime systems message passing paradigms mpi allow efficient migration tasks order provide dynamic load balancing capabilities 
parallel programming environment effectively load balance application needs know precise load conditions runtime 
needs supported runtime system parallel language 
needs predict load patterns current past runtime conditions provide appropriate re mapping partitions 
fortunately empirical observation cse applications suggests changes occur slowly life running application leading principle persistent computation communication structure 
load changes dramatic case adaptive refinement infrequent 
measuring variations load communication patterns runtime system accurately forecast load conditions effectively load balance application 
charm kk object oriented parallel programming language provides dynamic load balancing capabilities runtime measurements computational loads communication patterns employs object migration achieve load balance 
cse applications written languages fortran mpi gls message passing interface communication 
cumbersome convert legacy applications newer paradigms charm machine models paradigms different 
essentially attempts result complete rewrite applications 
frameworks computational steering automatic resource management autopilot provide ways instrument parallel programs collecting load information runtime fuzzy logic decision engine advises parallel program regarding resource management 
left parallel program implement advice 
load balancing transparent parallel program runtime system parallel language actively participate carrying resource management decisions 
similarly systems pl simply inform parallel program load imbalance leave application processes explicitly move new processor 
multithreaded systems nm require thread store state specially allocated memory system migrate thread automatically 
load balancing strategies instrumentation performance monitoring integrated 
frameworks automatic load balancing fem framework bk framework adaptive mesh refinement codes bn specific certain application domains apply general programming paradigm message passing general purpose messaging library mpi 
describe path taken solve problem load imbalance existing fortran mpi applications dynamic load balancing capabilities charm minimal effort 
section describes load balancing framework charm 
describe multi partitioning approach basis 
section describe implementation adaptive mpi uses user level migrating threads message driven objects 
show simple convert existing mpi code 
discuss methods efforts needed convert actual application codes performance implications section 
charm load balancing framework charm parallel object oriented language 
charm program consists set medium grained objects called 
mapped available processors message driven runtime system charm called converse 
communication asynchronous object method invocations 
methods called entry methods invoked execute atomically block waiting messages 
powerful abstractions created making members arbitrarily indexed collections multi dimensional arrays 
core charm message driven scheduler picks messages prioritized queue pending messages schedules object execute entry method object indicated message 
runtime system charm knows object executing methods 
track execution times computational loads individual objects 
entry method execution directed objects having know processor resides 
runtime system gather communication patterns objects 
charm provides dynamic load balancing lb framework gathers load communication data presents distributed load database load balancing strategies 
strategies provided charm plugged runtime charm application 
load database viewed weighted communication graph connected vertices represent communicating objects 
load balancing strategies produce re mapping objects order balance load 
performance crack propagation application charm load balancing framework 
experiment performed processors sgi origin national center supercomputing applications ncsa 
np hard multidimensional optimization problem producing optimal solution feasible 
experimented heuristic strategies shown achieve load balance 
new mapping produced lb strategy communicated runtime system invokes special serialization methods objects migrated carries migration 
molecular dynamics application routinely developed charm 
combines spatial functional decomposition 
simulation complex molecule progresses atoms may move neighboring partitions lead load imbalance 
charm lb framework shown effective allowed scale thousands processors achieving unprecedented speedups molecular dynamics applications processors 
application simulates pressure driven crack propagation structures implemented charm lb framework bk shown effectively deal dynamically varying load conditions 
crack develops structure discretized finite element mesh extra elements added near crack resulting severe load imbalance 
charm lb framework responds load imbalance migrating objects improving load balance seen increased throughput measured terms number iterations second 
order lb framework needs redesign application object entry methods objects execute atomically 
converting parallel cse applications written mpi blocking receives blocking collective operations load balancing framework cumbersome 
section describe basic methodology adapt existing message passing applications charm lb framework 
multi partitioning approach key effectively charm load balancing framework split computational domain small partitions available physical processors 
smaller partitions called virtual processors chunks mapped re mapped load balancing framework order balance load physical processors 
message driven object parallel programming paradigm charm chunks implemented objects 
communication objects accomplished asynchronous remote method invocation 
methods execute atomically block waiting messages 
charm runtime system schedules objects execute entry methods routes remote method invocations processors objects reside chunks need aware physical location parallel system 
runtime system free migrate chunks available physical processors 
having chunks map re map results better load balance 
large number chunks result better cache behavior resultant smaller partitions may utilize cache better 
having independent pieces computation processor results better latency tolerance computation communication overlap 
mapping chunks single physical processor reduces granularity parallel tasks computation communication ratio 
chunks tradeoff overhead virtualization effective load balance 
order study tradeoff carried experiment finite element method application structural simulation fem mesh elements 
ran application processor origin mhz mips different number partitions mesh mapped processor 
results 
shows increasing number chunks beneficial chunks physical processor 
increase performance caused better cache behavior smaller partitions overlap computation communication latency tolerance 
overhead introduced chunks physical processor small 
numbers may vary depending application expect similar behavior applications deal large data sets near neighbor communication 
challenges existing mpi applications converting existing message passing applications multi partitioned approach tricky 
applications assume distributed memory architecture process contains data accessed process 
process communicates process point point messages collective operations barriers reductions broadcasts 
process issues receive request blocks process requested message arrives process 
process processor mapping blocking receives typically waste processor cycles dedicated machine 
time seconds iteration number chunks processor effects multi partitioning fem application 
mapping operating system schedule process run blocked process inter process context switch costly 
application programmers non blocking sends receives overlap communication useful computation requires careful tuning applications maximize overlap 
case techniques perform computational load dynamically varies amount overlap typically determined useful process may vary lifetime running process 
order convert legacy mpi codes blocking operations need find locations program performs blocking operations receives barriers collective operations 
locations need break flow execution ensure atomicity entry methods chunks 
achieve splitting program subroutine blocking call separate subroutines 
subroutine replace blocking receive call non blocking receive specify second subroutine continuation runtime system non blocking receive complete 
approach allows port legacy mpi codes take advantage charm lb framework alters structure program considerably 
especially true blocking operations requiring split subroutines 
blocking operation performed subroutine subroutine callers split 
done recursively outermost scope 
lead rise complexity resulting code especially communication performed deep subroutine call hierarchy calls conditionally 
adaptive mpi implementation uses user level threads allows blocking receives slight overhead mpi codes charm lb framework efforts original multi partitioning approach 
adaptive mpi main disadvantage message driven multi partitioning approach described earlier significantly alters original program structure forcing non blocking sends receives 
subroutines split local variables original subroutines retained new program structure part dynamically allocated chunk data 
may disable compiler optimizations 
adaptive mpi avoids modification program structure cost potential overhead context switching migration 
implements chunks user level threads enable issue blocking receives 
alternatives processes kernel level threads 
process context switching costly means switching page table flushing cache lines process migration thread migration serialize entire core image process useful part 
kernel threads typically preemptive accessing shared variable mean locks mutexes increasing overhead 
needs os support migrating kernel threads process 
user level threads complete control scheduling maintain information communication pattern chunks computational memory requirements 
entire collection chunks implemented charm array objects object user level thread associated 
communication primitives mpi blocking non blocking implemented 
multiple chunks mapped processor data originally shared chunks 
convert mpi program need localize data 
processor private data user defined type making variable type local variable main subroutine resides thread stack 
processor private data large accommodated thread stack dynamically allocate data chunk 
data registered runtime system available chunk subroutine 
global variables changed indirect registered chunk data pointer 
write serialization subroutines called runtime system decides migrate chunk processor load balancing 
tricky migrate user level threads making sure stack valid migration different processors 
note chunk may migrate anytime blocking messages 
time thread local variables refer local variables may valid processor stack may located different location memory 
need mechanism making sure remain valid processors 
absence compiler support means thread stacks span range virtual addresses processor may migrate 
preliminary implementation threads stack copy mechanism contents thread stack copied context switch threads 
threads execute stack refer valid addresses migration assuming main process stack processors begins virtual address 
drawbacks 
inefficient copy overhead context switch second thread access thread local variables making mandatory copy shared location 
user program written process access process variables directly second restriction impact application programmer 
order ensure efficiency mechanism recommended keep stack size low possible time context switch 
current implementation threads uses new scalable variant functionality abn 
implementation thread stack allocated spans reserved virtual addresses processors 
achieved splitting unused virtual address space physical processors 
thread created stack allocated portion virtual address space assigned creating processor 
ensures thread addresses spanned stacks processor 
allocation deallocation assigned portion virtual address space done mmap functionality unix 
fixed size thread stacks eliminate overheads associated implementation 
results context switching overheads low non migrating threads irrespective stack size allowing migration threads 
efficient keep stack size time migration reduce thread migration overhead 
conversion order convert existing mpi programs sure variables global scope common blocks global variables defined blocking call mpi recv 
reason restriction straightforward 
global variable defined blocking call may modified userlevel thread defining thread blocks thread scheduled 
returning blocked mpi call original thread see different value variable 
careful inspection program may reveal variables 
order variables localized copied variables local subroutines stack 
careful inspection may possible 
case devised method systematically put global variables private area allocated dynamically thread stack 
idea user defined type global variables members type 
main program allocate variable type pass pointer variable subroutine global variables 
access previously global variables subroutines pointer 
simple source source translator recognize global variables automatically modifications program 
currently working comparison context switching times stack copying migrating threads non migrating threads 
experiment performed ncsa origin mhz mips processor 
modifying front parallelizing compiler bef incorporate translation 
currently done hand 
converted large mpi applications approach 
techniques efforts involved preliminary performance data section 
case studies compared original message driven multi partitioning approach evaluate overheads associated typical computational fluid dynamics cfd kernel performs jacobi relaxation large grids partition contains grid points 
ran application single mhz mips processor different number chunks keeping chunk size constant 
different decompositions 
decompositions vary number blocking receives chunk 
chunks blocking receive calls chunk iteration chunks blocking receive calls chunk iteration 
cases half calls block waiting data resulting context switches chunk iteration respectively 
seen optimization due availability local variables blocking calls larger subroutines version thread context switching overheads reasonable number chunks processor 
load balancing framework effectively user level threads incurring significant overheads 
encouraged results converted large mpi applications throughput number iterations second jacobi relaxation application 
left decomposition 
right decomposition 
processors mpi sec 
sec 
table comparison mpi versions 
part center simulation advanced rockets university illinois 
goal produce detailed multi physics rocket simulation virtual prototyping tool hd 
gen generation integrated simulation code composed coupled modules explicit fluid dynamics code implicit structural simulation code parallel interface pan 
written fortran lines respectively mpi parallel environment 
converted codes 
conversion techniques described section resulted changes original code fact changed codes run original mpi preprocessor directives take time authors unfamiliar codes week person codes 
addition overhead mpi shown tables minimal original decomposition partition processor 
expect performance better multiple partitions mapped processor depicted 
ability respond dynamic load variations outweighs overheads 

processors mpi sec 
sec 
table comparison mpi versions 
note scaled problem 
efficient implementations increasing number dynamic irregular computational science engineering applications require dynamic load balancing 
applications written procedural languages fortran message passing parallel programming paradigm 
traditional implementation message passing libraries mpi support dynamic load balancing 
charm parallel programming environment supports dynamic load balancing object migration 
applications developed charm shown adaptively balance load presence dynamically changing load conditions caused factors external application clusters workstations 
converting existing procedural message passing codes object charm cumbersome 
developed adaptive mpi implementation mpi message driven object runtime system user level threads run existing mpi applications minimal change insignificant overhead 
conversion legacy mpi programs adaptive mpi need significant changes original code structure changes needed mechanical fully automated 
converted large scientific applications adaptive mpi dynamic load balancing framework shown applications overhead small 
currently working reducing messaging overhead automating code conversion methods 
abn gabriel luc raymond 
efficient transparent thread migration scheme pm runtime system 
proc 
rd workshop runtime systems parallel programming san juan puerto rico held conjunction th intl parallel processing symp 
ipps spdp ieee acm 
lecture notes computer science pages 
springer verlag april 
bef william blume rudolf eigenmann keith john jay david padua paul petersen bill pottenger lawrence rauchwerger peng tu stephen 
polaris improving effectiveness parallelizing compilers 
proceedings th international workshop languages com parallel computing number lecture notes computer science pages ithaca ny usa august 
springer verlag 
bk robert brunner kal 
adapting load workstation clusters 
seventh symposium frontiers massively parallel computation pages 
ieee computer society press february 
bk milind kal 
parallel framework explicit fem 
technical report parallel programming laboratory department computer science university illinois urbana champaign submitted may 
bn norton 
innovative language object oriented structured amr fortran openmp 
deng editors new trends high performance computing conference stony brook ny august 
gls gropp lusk skjellum 
mpi portable parallel programming message passing interface 
mit press 
hd heath dick 
virtual rocket science meets computer science 
ieee science engineering 
milind robert brunner 
run time support adaptive load balancing 
proceedings th workshop runtime systems parallel programming cancun mexico march 
milind sanjeev krishnan joshua 
converse interoperable framework parallel programming 
proceedings th international parallel processing symposium pages april 
kk sanjeev krishnan 
charm parallel programming message driven objects 
gregory wilson paul lu editors parallel programming pages 
mit press 
kal robert milind robert brunner neal james phillips krishnan varadarajan klaus schulten 
greater scalability parallel molecular dynamics 
journal computational physics 
nm 

parallel multithreaded machine 
computing environment distributed architectures 
hollander peters editors parallel computing state art perspectives proceedings conference september ghent belgium volume advances parallel computing pages amsterdam february 
elsevier north holland 
pan parsons acharya 
loosely coupled simulation solid rocket 
fifth national congress computational mechanics boulder colorado august 
pl pruyne livny 
parallel processing dynamic resources 
lecture notes computer science 
randy jeffrey vetter daniel reed 
autopilot adaptive control distributed applications 
proc 
th ieee symp 
high performance distributed computing chicago il july 

