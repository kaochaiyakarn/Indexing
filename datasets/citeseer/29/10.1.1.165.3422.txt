properties support vector machines massimiliano pontil alessandro verri dipartimento di dell universita di genova genova support vector machines svms perform pattern recognition point classes nding decision surface determined certain points training set termed support vectors sv 
surface feature space possibly nite dimension regarded hyperplane obtained solution problem quadratic programming depends regularization parameter 
study mathematical properties support vectors show decision surface written sum orthogonal terms rst depending margin vectors svs lying margin second proportional regularization parameter 
values parameter enables predict decision surface varies small parameter changes 
special important case feature space nite dimension margin vectors observe svs usually su cient fully determine decision surface 
relatively small result leads consistent reduction sv number 
support vector machines svms introduced new technique solving pattern recognition problems cortes vapnik blanz scholkopf osuna freund girosi 
theory svms vapnik traditional techniques pattern recognition minimization empirical risk attempt optimize performance training set svms minimize risk probability misclassifying seen patterns xed unknown probability distribution data 
new induction principle equivalent minimize upper bound generalization error relies theory uniform convergence probability vapnik 
svms attractive ability condense information contained training set families decision surfaces relatively low vc dimension vapnik chervonenkis 
linear separable case key idea svm explained plain words 
training set points classes svm separates classes hyperplane determined certain points termed support vectors 
separable case hyperplane maximizes margin minimum distance class hyperplane support vectors lie minimum distance hyperplane termed margin vectors 
real cases classes may separable hyperplane support vectors obtained solution problem constrained optimization 
solution trade largest margin lowest number errors trade controlled regularization parameter 
aim gain better understanding nature support vectors regularization parameter determines decision surface linear nonlinear case 
investigate mathematical properties support vectors characterize dependence decision surface changes regularization parameter 
analysis rst carried simpler linear case extended include nonlinear decision surfaces 
organized follows 
rst review theory svms section analysis section 
summarize section 
theoretical overview section recall basics theory svm vapnik cortes vapnik linear nonlinear case 
start simple case linearly separable sets 
optimal separating hyperplane follows assume set points xi ir point xi belongs classes label yi 
goal establish equation hyperplane divides leaving points class side maximizing minimum distance classes hyperplane 
purpose need preliminary de nitions 
de nition 
set linearly separable exist ir ir xi yi xi yi compact notation inequalities rewritten yi xi pair de nes hyperplane equation named separating hyperplane see gure 
denote norm signed distance di point xi separating hyperplane isgiven di xi combining inequality equation xi separating hyperplane optimal separating hyperplane 
solid lines separate sets open circles triangles solid line leaves closest points lled circles triangle maximum distance 
dashed lines identify margin 
lower bound distance points xi separating hyperplane 
ask simply rewrite inequality yi xi purpose right hand side inequality establish toone correspondence separating hyperplanes parametric representation 
done notion canonical representation separating hyperplane de nition 
separating hyperplane linearly separable set canonical representation separating hyperplane obtained rescaling pair pair away distance closest point equals de nition xi consequently separating hyperplane canonical representation bound inequality tight 
follows assume separating hyperplane canonical representation write 
position de ne notion optimal separating hyperplane 
intermediate step derivation optimal separating hyperplanes slightly different derivation originally developed cortes vapnik 
de nition 
linearly separable set separating hyperplane osh separating hyperplane maximizes distance closest point distance closest point equals osh regarded solution problem maximizing subject constraint problem minimize subject yi xi comments order 
pair solves xi xi 
particular implies solution separating hyperplane canonical representation 
second parameter enters constraints function minimized 
quantity distance classes direction named margin 
osh seen separating hyperplane maximizes margin see gure 
properties solution problem 
support vectors problem solved means classical method lagrange multipliers shetty 
denote nonnegative lagrange multipliers associated constraints solution problem equivalent determining saddle point function nx xi 
saddle point minimum maximum write nx nx yi wn substituting equations right hand side see problem reduces maximization function nx nx xj subject constraint formulated problem maximize subject yi sums andd pair equation follows new problem called dual problem matrix dij xj determined kuhn tucker conditions nx yi xi note nonzero equation constraints satis ed equality sign 
corresponding points xi termed support vectors points closest osh see gure 
support vector xj parameter obtained corresponding kuhn tucker condition yj xj problem classifying new data point simply solved computing sign support vectors condense information contained training set needed classify new data points 
linearly nonseparable case set linearly separable simply ignores set linearly separable problem searching osh meaningless may separating hyperplane start 
fortunately previous analysis generalized introducing nonnegative variables suchthat yi xi point xi satis es inequality null reduces 
point xi satisfy inequality term added right hand side obtain inequality 
generalized osh regarded solution follows means component vector problem minimize subject yi xi 
term sum thought measure amount misclassi cation 
note term leads robust solution statistical sense intuitively appealing term words term osh sensitive presence outliers training set 
parameter regarded regularization parameter 
osh tends maximize minimum distance small minimize number misclassi ed points large intermediate values solution problem trades errors larger margin 
behavior osh function studied detail section 
analogy done separable case problem transformed dual problem maximize subject yi matrix separable case 
note dimension size training set dimension input space gives rank constraints problem follows su ciently large set linearly separable problem reduces 
pair easy nd nx determined new kuhn tucker conditions solution dual problem yi xi values saddle point 
similarly separable case points xi termed support vectors 
main di erence distinguish support vectors cand rst case condition follows condition support vectors lie distance osh 
support vectors termed margin vectors 
support vectors misclassi ed points points correctly classi ed closer osh degenerate cases points lying margin 
event refer support vectors errors 
example generalized osh relative margin vectors errors shown gure 
points support vectors correctly classi ed lie outside margin strip 
generalized optimal separating hyperplane 
sets circles triangles linearly separable 
solid line optimal separating hyperplane lled circles triangles support vectors margin vectors shown black errors gray 
conclude section discussing extension theory nonlinear case 
nonlinear kernels cases linear separation input space restrictive hypothesis practical 
fortunately theory extended nonlinear separating surfaces mapping input points feature points looking osh corresponding feature space cortes vapnik 
ir input point corresponding feature point mapping ir certain space typically hilbert space nite nite dimension 
cases denote components 
clearly osh corresponds nonlinear separating surface input space 
rst sight nonlinear surface determined mapping completely known 
formulation problem classi cation stage equation follows enters dot product feature points dij xi xj iyi xi consequently ifwe nd expression dot product feature space uses points input space xi xj xi xj full knowledge necessary 
symmetric function equation called kernel 
nonlinear separating surface solution problem dij xi xj classi cation stage reduces computing sign xi extension theory nonlinear case reduced nding kernels identify certain families decision surfaces written equation 
useful criterion deciding kernel written equation mercer theorem courant hilbert cortes vapnik kernel ir dot product feature space zz dxdy possible set functions satisfying equation determined eigenfunctions solution eigenvalue problem dx 
set eigenfunctions nite rewritten nite kernel said sum ranges set eigenfunctions 
general case set nite kernel said nite sum equation series integral 
simple examples kernels 
rst polynomial kernel easily veri ed polynomial kernel satis es mercer theorem nite 
separating surface input space polynomial surface degree case mapping determined directly de nition particular case andd example write second example gaussian kernel exp kx yk ir 
gaussian kernel clearly satis es mercer theorem nite equation continuum eigenvalues 
easy verify case eigenvalues normalized fourier transform gaussian exp ksk exp ix corresponding eigenfunctions 
separating surface input space weighted sum gaussians centered support vectors 
fully equipped discuss mathematical properties solution problem 
mathematical properties goal study dependence osh parameter linear case extend analysis nonlinear kernels 
lagrange multiplier margin vector start establishing simple important result lagrange multipliers margin vectors 
want show thatthe lagrange multiplier associated margin vector step wise linear function regularization parameter prove need preliminary de nitions 
risk confusion write introduce sets support vector indexes fi cg fi cg ande number indexes respectively 
set identi es margin vectors errors 
equal suppose margin vectors 
hypothesis may satis ed highly degenerate con gurations points small values appear restrictive cases interest 
loss generality support vectors start sorting support vectors fng fm mg labeling points 
kuhn tucker conditions tell yi xi equation means rewritten nx equality constraint yi wehave time equation get nx iyi follows fact points discarded problem solution 
plugging equations obtain yi matrix hij xi xj notice written submatrix margin vectors submatrix errors hme submatrix margin vectors errors 
separating sum margin vectors errors equation nd yi vector notation equation rewrites ch ym ym vectors components equal unit 
assuming matrix hm invertible see appendix proof fact ym ch equation infer lagrange multiplier associated margin vector written sum terms 
clear subscript rst term depends margin vectors second proportional depends margin vectors errors 
important consequence existence vectors xi linearly independent 
corollary number margin vectors exceed notice mean number points lying margin exceed 
degenerate cases may points lying margin support vectors lying margin dependence regularization parameter position study dependence osh parameter rst show normal osh written sum orthogonal vectors 
orthogonal decomposition components equation rewritten ri notice ri gi necessarily positive negative 
de ne rn gn yi equation true margin vector index rn yi yi equality due constraint fact plugging equation separating constant linear term obtain cw easily seen orthogonal 
substituting equations respectively obtains xi xn yi xi xn xi xn de nition mh mh mg plugging equation follows immediately 
changing regularization parameter ect small changes regularization parameter osh 
free parameter svms study relevant theoretical practical viewpoint 
follows take positive real axis ir notice possible choices support vectors possible values distinguishing margin vectors errors nite 
neglect degenerate con gurations support vectors implies ir partitioned nite number disjoint interval characterized support vectors 
notice rightmost interval necessarily unbounded 
preliminary observation conclude exception values corresponding interval ends set support vectors vary small changes previous analysis study dependence normal vector parameter equation follows changes margin vectors errors remain normal vector changes cw direction wecan statement precise distinguishing cases 
rst case reach maximum value hm maximum rank haven independent kuhn tucker conditions equation osh completely determined margin vectors 
consequently set support vectors remains small changes vanish equation tells osh xed unambiguously identi ed margin vectors 
fact osh xed possible determine maximum interval say osh equation 
purpose su cient compute ri gi equations nd minimum maximum associated margin vector xi satisfy constraint second case osh equation 
small change new osh written cw equation tells osh changes amount cw 
exists maximum interval osh equation 
similarly previous case determine minimum maximum associated margin vectors satisfy constraint changing osh correspond new set support vectors minimum maximum values lower upper bound respectively 
observe osh written linear combination support vectors example errors 
example cases means numerical example shown gure 
gure shows osh displayed training set 
support vectors denoted lled circles triangles margin vectors black errors grey 
accordance equation margin vectors osh xed 
straightforward computations predict osh remain 
prediction veri ed numerically 
optimal separating hyperplane respectively 
legend gure 
shows new osh obtained just outside interval 
notice errors gure margin vectors 
discussed osh change small variations predicted equation 
veri ed numerically gure displays obtained equation direct solution problem 
osh coincide numerical precision 
larger variation see gure number margin vectors goes back solution xed 
notice transition errors margin vector error upper part margin strip gure margin vector gure 
mentioned previous section worthwhile noticing solutions smaller see gure larger margin solutions larger see gure smaller number errors 
extension nonlinear kernels extend analysis case nonlinear kernels 
lagrange multiplier margin vector start observing decomposition lagrange multiplier margin vector derived linear case holds true nonlinear kernels 
note matrix equation rewrites hij xi xj xj xi equations remain unchanged 
orthogonal decomposition care needed extension orthogonal decomposition study behavior separating surface function nonlinear case may possible recover explicit expression pose major problems expressions involving ectively dot products feature points computed means kernel take dot product obtain written nx nx xi xi xi yjk xj xi terms equation counterparts equations de ning respectively 
note explicit expression orthogonality relation remains true 
seen fact equation depends matrix nonlinear case rewritten equation 
respect terms equation regarded orthogonal 
changing regularization parameter far results derived linear case carried case nonlinear kernels 
dependence separating surface parameter convenient distinguish nite nite kernels 
nite kernels results obtained linear case valid simply replacing dimension input space dimension feature space 
example osh feature space change small changes second term equation vanishes furthermore interval osh xed determined exactly linear case 
kernels nite dimension nite number margin vectors su cient fully determine osh 
consequently di erently nite case osh xed second term equation vanish 
small change dot product changes amount yjk xj xi summary results derived linear case extended major changes nonlinear case exception properties depending niteness dimension linear case upper bound number margin vectors properties true nite kernels 
case pattern recognition svms depend free parameter regularization parameter mathematical properties support vectors useful characterize behavior decision surface respect wehave identi ed special subset support vectors margin vectors lagrange multiplier strictly smaller regularization parameter wehave shown margin vectors linearly independent decision surface written sum orthogonal terms rst depending margin vectors second proportional regularization parameter 
values parameter enabled predict decision surface varies small parameter changes 
general solution usually stable respect small changes obtained results conveniently summarized distinguishing nite kernels 
kernels nite dimension turned upper bound number margin vectors behavior osh function depends margin vectors su cient fully determine equation osh feature space values osh vary small changes osh varies amount proportional change direction identi ed margin vectors errors 
cases worthwhile observing number support vectors ectively needed identify decision surface greater 
result may useful reduce number support vectors ectively needed perform recognition 
nite kernels margin vectors linearly independent upper bound number 
small changes osh xed varies case mof nite kernels 

edgar osuna read manuscript useful remarks 
partially supported 
appendix appendix sketch proof existence transform original dual problem linear complementary problem lcp derive explicit expression matrix de nes polyhedral set solution lcp lies 
de ne remind yi sum ranges 
number points positive negative labels respectively 
start rewriting problem equality constraint problem minimize subject yi yi set indexes corresponding yi 
bethe lagrange multipliers associated constraints problem respectively 
lcp associated problem obtained 
setting equal gradient lagrangian associated problem yi yi ui vi 
introducing slack variables ands sn satisfying si iyi iyi associated complementary conditions constrained optimization jargon slack variable nonnegative variable turns inequality equality constraint 

ivi solution problem obtained solution lcp problem solve mz subject yn yn similarly case linear programming solution problem polyhedral set 
addition solution satisfy complementarity conditions 
case problem solution vector polyhedral set fp gp pb pn andb matrix de ned columns corresponding active variables 
simple lengthy calculations seen matrix hm submatrix submatrix 
existence ensured existence 
shetty nonlinear programming 
john wiley new york 
blanz scholkopf burges vapnik vetter 
comparison view object recognition algorithms realistic models 
proc icann 


cortes vapnik 
support vector network 
machine learning 
courant hilbert 
methods mathematical physics 
john wiley new york 
osuna freund girosi 
training support vector machines applications face detection 
cvpr 
scholkopf sung burges girosi niyogi poggio vapnik 

comparing support vector machines gaussian kernels radial basis function classi ers 
ai memo massachusetts institute technology cambridge 
vapnik 
estimation dependencies empirical data 
springer verlag new york 
vapnik 
nature statistical learning theory 
springer verlag new york 
vapnik chervonenkis ja 

uniform convergence relative frequencies events probabilities 
theory probab appl 


