comparative evaluation meta learning strategies large distributed data sets andreas prodromidis stolfo columbia university computer science dept new york ny usa considerable interest various approaches scaling machine learning systems large distributed data sets 
studying approaches parallel application multiple learning programs distributed sites followed meta learning stage combine multiple models principled fashion 
empirically determine best data partitioning scheme selected data set compose subsets evaluate compare di erent strategies voting stacking stacking correspondence analysis scann combining classification models trained subsets 
seek find ways ciently scale large data sets maintaining improving predictive performance measured error rate cost model tp fp spread 
keywords classification multiple models meta learning stacking voting correspondence analysis data partitioning email address contact author andreas cs columbia edu phone number contact author electronic version postscript icml ijs si research supported intrusion detection program darpa nsf iri cda supported part ibm fellowship inductive learning classification techniques applied problems diverse areas success 
field machine learning substantial progress past years empirically theoretically lasting challenges development inductive learning techniques ectively scale large possibly physically distributed data sets 
