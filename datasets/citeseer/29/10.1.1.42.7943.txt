weight adjustment schemes centroid classifier shankar george karypis university minnesota department computer science minneapolis mn technical report tr shankar karypis cs umn edu years seen tremendous growth volume text documents available internet digital libraries news sources wide intra nets 
automatic text categorization task assigning text documents pre specified classes topics themes documents important task help organizing finding information huge resources 
similarity categorization algorithms nearest neighbor generalized instance set centroid classification shown effective document categorization 
major drawback algorithms features computing similarities 
document data sets small number total vocabulary may useful categorizing documents 
possible approach overcome problem learn weights different features words document data sets 
supported nsf ccr army research office contract da doe program army high performance computing research center contract number daah 
access computing facilities provided minnesota supercomputer institute 
related papers available www url www cs umn edu karypis seen tremendous growth volume online text documents available internet digital libraries news sources wide intra nets 
documents unstructured data predominant data type stored online 
automatic text categorization task assigning text documents pre specified classes documents important task help people find information huge resources :10.1.1.11.6124
text categorization presents huge challenges due large number attributes attribute dependency multi modality large training set 
various document categorization algorithms developed years fall general categories :10.1.1.11.9519:10.1.1.11.6124:10.1.1.43.9670
category contains traditional machine learning algorithms decision trees rule sets instance classifiers probabilistic classifiers support vector machines directly adapted context document data sets 
second category contains specialized categorization algorithms developed information retrieval community 
related papers available www url www cs umn edu karypis seen tremendous growth volume online text documents available internet digital libraries news sources wide intra nets 
documents unstructured data predominant data type stored online 
automatic text categorization task assigning text documents pre specified classes documents important task help people find information huge resources :10.1.1.11.6124
text categorization presents huge challenges due large number attributes attribute dependency multi modality large training set 
various document categorization algorithms developed years fall general categories :10.1.1.11.9519:10.1.1.11.6124:10.1.1.43.9670
category contains traditional machine learning algorithms decision trees rule sets instance classifiers probabilistic classifiers support vector machines directly adapted context document data sets 
second category contains specialized categorization algorithms developed information retrieval community 
examples algorithms include relevance feedback linear classifiers generalized instance set classifiers general class algorithms shown produce document categorization performance similarity 
class contains algorithms nearest neighbor generalized instance set centroid classifiers 
refer algorithms feature weight adjustment just weight adjustment techniques 
report presents fast iterative feature weight adjustment algorithms linear complexity centroid classification algorithm 
algorithms measure discriminating power term gradually adjust weights features concurrently 
analysis shows approach gradually eliminates discriminating features document improving classification accuracy 
experimentally evaluate algorithms reuters ohsumed document collection compare performance terms precision recall rocchio widrow hoff support vector machines :10.1.1.11.6124:10.1.1.92.8815
compared performance terms classification accuracy data sets multiple classes 
data sets described section 
data sets compared performance traditional classifiers nn naive bayesian 
experiments show feature weight adjustment improves performance classifier substantially outperforms rocchio widrow hoff competitive svm 
section describes classification schemes text data section gives brief overview centroid classifier 
section describes weight adjustment schemes discusses computational complexity 
section presents analysis schemes 
section documents results schemes various data sets performance classifiers data sets 
previous linear classifiers linear classifiers family text categorization learning algorithms learn feature weight vector category :10.1.1.43.9670
weight learning techniques rocchio widrow hoff algorithm learn feature weight vector training samples 
weight learning algorithms adjust feature weight vector features words contribute significantly categorization large values 
rocchio widrow hoff weight vector classification follows 
test document classified pre defined threshold 
weight learning techniques rocchio widrow hoff algorithm learn feature weight vector training samples 
weight learning algorithms adjust feature weight vector features words contribute significantly categorization large values 
rocchio widrow hoff weight vector classification follows 
test document classified pre defined threshold 
assigned positive class note concept weight vector different rocchio rocchio batch algorithm learn weight vector existing weight vector set training examples :10.1.1.43.9670:10.1.1.43.9670
th component new vector nc nc number training instances set positive training instances nc number positive training instances 
usually rocchio uses positive weights negative weights reset 
widrow hoff widrow hoff algorithm online algorithm runs training examples time updating weight vector :10.1.1.43.9670:10.1.1.43.9670
new weight vector computed follows 
test document classified pre defined threshold 
assigned positive class note concept weight vector different rocchio rocchio batch algorithm learn weight vector existing weight vector set training examples :10.1.1.43.9670:10.1.1.43.9670
th component new vector nc nc number training instances set positive training instances nc number positive training instances 
usually rocchio uses positive weights negative weights reset 
widrow hoff widrow hoff algorithm online algorithm runs training examples time updating weight vector :10.1.1.43.9670:10.1.1.43.9670
new weight vector computed follows 
label row negative class positive class 
parameter controls quickly weight vector change influence new example 
may final vector theoretical results suggest better final weight vector average weight vectors computed way :10.1.1.43.9670
widrow hoff widrow hoff algorithm online algorithm runs training examples time updating weight vector :10.1.1.43.9670:10.1.1.43.9670
new weight vector computed follows 
label row negative class positive class 
parameter controls quickly weight vector change influence new example 
may final vector theoretical results suggest better final weight vector average weight vectors computed way :10.1.1.43.9670
support vector machines support vector machines svm new learning algorithm proposed vapnik 
algorithm introduced solve class pattern recognition problem structural risk minimization principle 
training set vector space method finds best decision hyper plane separates classes 
quality decision hyper plane determined distance referred margin hyper planes parallel decision hyper plane touch closest data points class 
quality decision hyper plane determined distance referred margin hyper planes parallel decision hyper plane touch closest data points class 
best decision hyper plane maximum margin 
svm problem solved quadratic programming techniques 
svm extends applicability linearly non separable data sets soft margin hyper planes mapping original data vectors higher dimensional space data points linearly separable 
efficient implementation svm application text categorization reuters corpus reported :10.1.1.11.6124:10.1.1.11.6124
implementation comparison purposes 
nearest neighbor nearest neighbor nn classification instance learning algorithm applied text categorization early days research shown produce better results compared machine learning algorithms ripper :10.1.1.33.4944
classification paradigm nearest neighbors test document computed 
similarities document nearest neighbors aggregated class neighbors test document assigned similar class measured aggregate similarity 
svm problem solved quadratic programming techniques 
svm extends applicability linearly non separable data sets soft margin hyper planes mapping original data vectors higher dimensional space data points linearly separable 
efficient implementation svm application text categorization reuters corpus reported :10.1.1.11.6124:10.1.1.11.6124
implementation comparison purposes 
nearest neighbor nearest neighbor nn classification instance learning algorithm applied text categorization early days research shown produce better results compared machine learning algorithms ripper :10.1.1.33.4944
classification paradigm nearest neighbors test document computed 
similarities document nearest neighbors aggregated class neighbors test document assigned similar class measured aggregate similarity 
major drawback similarity measure nn uses features equally computing similarities 
lead poor similarity measures classification errors small subset words useful classification 
experiments focus evaluating accuracy classifier data sets multiple labels document considering binary classification class 
third evaluates classifier way classifier 
experiments compared performance weight adjustment schemes performance achieved classifiers 
obtained results linear classifiers rocchio respectively learn weight vectors 
case rocchio wh initial vector :10.1.1.43.9670

ran svm lite polynomial kernel rbf kernel data sets :10.1.1.11.6124
feature weight adjustment schemes 
classifiers include nearest neighbor naive bayesian classifier 
experiments compared performance weight adjustment schemes performance achieved classifiers 
obtained results linear classifiers rocchio respectively learn weight vectors 
case rocchio wh initial vector :10.1.1.43.9670

ran svm lite polynomial kernel rbf kernel data sets :10.1.1.11.6124
feature weight adjustment schemes 
classifiers include nearest neighbor naive bayesian classifier 
naive bayesian rainbow multinomial event model 
feature weighting schemes run 
comparing results clustering svm results see svm poly scheme micro average percent weight adjustment 
svm rbf better 
weight adjustment schemes dominate svm rbf top classes 
weight adjustment schemes achieve performance 
earn acq money fx grain crude trade interest wheat ship corn micro average top micro average average top table effect clustering ohsumed results table gives data table ohsumed data set :10.1.1.92.8815
ohsumed data documents id title title 
classification task considered assign document multiple categories mesh diseases categories 
entries data set 
training set remaining formed test set 
data sets tr tr tr tr tr tr tr fbis la la la new derived trec trec trec collections 
data sets re re reuters text categorization test collection distribution 
removed dominant classes earn acq shown relatively easy classify 
divided remaining classes sets 
data sets oh oh oh oh ohsumed collection subset medline database :10.1.1.92.8815:10.1.1.92.8815
data set wap webace project wap 
document corresponds web page listed subject hierarchy yahoo 

table shows performance weight adjustment scheme way classifier 
part reason purity changes relatively slowly mechanism weight transfer 
number iterations small data sets means changes purity added probably substantial difference classification accuracy 
efficiency advantages weight adjusted centroid scheme speed 
discussed section model learning time linear number non zero terms document term matrix classification time linear number classes 
comparison running time performed svm lite code polynomial kernel rbf kernel centroid scheme reported table :10.1.1.11.6124
times obtained mhz workstation running memory similar load conditions 
looking table see times faster svm rbf learning phase times faster classification phase 
nb knn centroid west west west oh oh oh oh re re tr tr tr tr tr tr tr la la la fbis wap new table classification accuracy report showed weight adjustment scheme improves accuracy centroid classifier 
scheme retains power centroid classifier enhancing ability 
available www url www cs umn edu karypis 
hong han 
text categorization weight adjusted nearest neighbor classification 
phd thesis university minnesota october 
hersh buckley leone :10.1.1.92.8815
ohsumed interactive retrieval evaluation new large test collection research 
sigir pages 
tokunaga 
cluster text categorization comparison category search strategies 
sigir pages 
tokunaga 
cluster text categorization comparison category search strategies 
sigir pages 
joachims :10.1.1.11.6124
text categorization support vector machines learning relevant features 
proc 
european conference machine learning 
kira rendell 
third annual symposium document analysis information retrieval 
lewis 
reuters text categorization test collection distribution 
www research att com lewis 
david lewis robert james callan ron papka :10.1.1.43.9670
training algorithms linear text classifiers 
proceedings th annual international acm sigir conference research development information retrieval pages pages 
lowe 
similarity metric learning variable kernel classifier 
