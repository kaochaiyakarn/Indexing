planning learning coordination multiagent decision processes craig boutilier department computer science university british columbia vancouver bc canada cs ubc ca www cs ubc ca spider craig html growing interest ai design multiagent systems especially multiagent cooperative planning 
investigate extent methods single agent planning learning applied multiagent settings 
survey number different techniques decision theoretic planning reinforcement learning describe number interesting issues arise regard coordinating policies individual agents 
describe multiagent markov decision processes general model frame discussion 
special person cooperative games agents share utility function 
discuss coordination mechanisms imposed conventions social laws learning methods coordination 
focus decomposition sequential decision processes coordination learned imposed locally level individual states 
discuss structured problem representations role generalization learned conventions approximation 
growing interest ai design systems multiple autonomous agents interact various ways pursue ends seek compatible goals 
special interest systems individual agents share goals utility function fully cooperative settings agents collectively act common desired ends 
general problems involving interaction potentially self interested agents received bulk fully cooperative problems naturally arise task distribution 
example user assign number autonomous mobile robots software agents task share utility function user certain purposes may sense model business organization similar way 
important class multiagent problems multiagent planning multiagent sequential decision making problem devising action policies strategies set share common ends 
key aspect problem coordinating actions individual agents shared goals achieved efficiently 
course problem multiagent planning falls squarely setting ofn person cooperative game theory 
perspective game theory interested inn person games players shared joint utility function 
words outcome game equal value players 
assuming game fully cooperative sense interesting problems cooperative game theory coalition formation negotiation disappear 
standard player decision problem collection viewed single player trying optimize behavior nature 
planning sequential decision making studied extensively ai context single agent systems assuming fully cooperative games profitably viewed collective single agent problems question naturally arises extent methods single agent decision making extended cooperative multiagent setting 
contribution answer question surveying techniques single agent case making proposals extending certain techniques suggesting number directions research cooperative multiagent decision making take 
interested planning uncertainty competing objectives potentially indefinite infinite horizon adopt markov decision processes mdps underlying single agent decision model 
mdps basis decision theoretic planning dtp techniques computing optimal policies adapted ai planning tasks :10.1.1.48.6957
furthermore mdps form foundation reinforcement learning rl agents learn optimal policies experience environment 
extension mdps cooperative multiagent case straightforward 
treating collection agents single agent joint actions disposal allows compute learn optimal joint policies standard methods provided agents mind unfortunately rarely case generally expect agents plan learn independently 
choices separately may jointly suboptimal 
real problem extending single agent methods cooperative settings determining methods coordination 
ensure individual decisions coordinated joint optimality achieved 
note agents interested coordination jointly optimal action individually optimal agent 
solutions coordination problem divided general classes communication convention learning 
example agents communicate order determine task allocation conventions social laws imposed system designed optimal joint action assured coordinated policy conventions learned repeated interaction :10.1.1.135.717
focus primarily imposed conventions learned coordination behavior especially sequential decision processes 
ultimately interested extent models representations computational schemes dtp rl applied solving cooperative problems requiring coordination 
section survey number methods solution single agent mdps including dtp rl 
section define multiagent markov decision processes discuss coordination problem describe mmdp decomposed local state games 
section discuss possible solutions coordination problem 
describe imposed conventions example lexicographic convention 
discuss possible learning methods coordination special attention learning locally coordinated policies level state games 
discuss issues convergence describe experiments regard 
briefly look possible rl methods coordination 
section briefly describe factored representations states actions natural specification mdps problems exploited computationally 
special interest potential allow generalization learned conventions 
conclude discussion possible extensions model 
large extent describes starting point investigation fully cooperative multistage stochastic games foundation multiagent planning stochastic domains 
ultimate goal explore successively weaker versions model fewer assumptions capabilities shared utilities agents approximation methods solving problems 
note considerable amount cooperative noncooperative game theory relevant problems address 
techniques applied directly 
focus ai perspective extension models ai point relevant connections game theory literature 
surely number game theoretic methods suited issue multiagent planning coordination remain 
viewed merely start gap planning machine learning game theoretic approaches coordination start ai side gap 
successful bridge require additional sides 
single agent decision processes increasingly research planning directed problems initial conditions effects actions known certainty multiple potentially conflicting objectives traded determine optimal courses action 
decision theoretic planning generalizes classical ai planning addressing issues 
particular theory markov decision processes mdps considerable popularity conceptual computational model pr dtp :10.1.1.48.6957
addition reinforcement learning viewed means learning act optimally incrementally constructing optimal plan repeated interaction environment 
mdps form underlying model 
review mdps associated decision methods section application mdps rl 
markov decision processes consider dtp problems modeled completely observable mdps 
assume finite set states sof system interest finite set agent reward 
action takes agent state effects actions predicted certainty write denote probability thats reached actiona performed states 
complete observability entails agent knows state assume bounded real valued reward denoting immediate utility states 
rewards reflect relative importance various objectives 
purposes mdp consists ofs rand set transition ag 
typical classical planning problems viewed mdps actions deterministic competing objectives single goal reward 
plan policy mapping denotes action agent perform state mdp agent ought adopt optimal policy maximizes expected rewards accumulated partially observable processes realistic cases tractable computationally 
consider see concluding section 
restrict attention stationary policies 
problems consider optimal stationary policies exist 
performs specified actions 
concentrate discounted infinite horizon problems current value rewards discounted factor want maximize expected accumulated discounted rewards infinite time period 
expected value fixed shown satisfy value initial computed solving system linear equations 
policy optimal alls sand policies 
optimal value mdp value function optimal policy vi max techniques constructing optimal policies discounted problems studied 
algorithms modified policy iteration practice especially simple algorithm value iteration bellman principle optimality discuss value iteration simplicity close relationship bears rl techniques describe 
start random value assigns value 
value state swe afr xt spr vi sequence linearly optimum value limit 
finite iterations choice maximizing action optimal policy value step embodied equation dubbed bellman backup dtp focused developing representations computational techniques solving mdps optimally approximately suited ai problems 
discuss representation problem detail section 
computational problem fact quite severe large state spaces 
standard mdp algorithms tend converge relatively iterations iteration requires computation time linear size state space value iteration algorithms 
generally impractical state spaces grow exponentially number problem features interest called curse dimensionality 
emphasis dtp research placed issue speeding computation solutions proposed including restricting search local regions state space reducing state space abstraction clustering states 
approaches reduce state space way allows mdp solution techniques generate approximately optimal solutions accuracy bounded xt spr important note critically discount factor 
assume fixed discussion 
appropriate stopping criteria discussed detail puterman 
priori 
certain computational techniques exploit compact representations mdps explore section 
reinforcement learning dtp methods mdps availability agent system model 
instance planning agent available certain actions disposal model effects agent actions exogenous changes system relative desirability particular system states 
furthermore model available agents may adapt small changes environmental dynamics 
reinforcement learning popular way addressing challenges allowing policies learned basis experience 
assume fully observable mdp models system dynamics agent knows possible states actions mdp transition probabilities reward structure 
acts continuously receiving training samples ri taken states resulting state received states 
sequence subsequence samples agent decide action perform current state 
roughly rl schemes divided model free approaches model approaches 
model free rl agent attempt directly learn pr orr parts mdp unknown 
agent learns optimal value function optimal policy directly 
popular related methods temporal difference td learning learning 
best think td methods learning ri value function fixed policy combined rl method value function policy improvement 
letv denote current estimated value fixed policy riis received performing states simplest td method known td update estimated value learning rate governing extent new sample replaces current estimate recall discount factor 
decreased slowly learning td converge true value states sufficiently sampled 
generally update values states visited prior tos justs eligibility trace essentially encoding history recorded 
basis td parameter captures degree past states influenced current sample 
addition variants truncated eligibility traces 
learning straightforward elegant method combining value function learning td methods policy learning 
state action ai assume value provides estimate value max performing states assuming currently optimal action subsequently reached states performed 
generally xt spr define optimal function determined equation target value function 
estimate optimal values agent updates formula similar equation fq goals rl combine appropriate action learning current estimated values determine best estimated choice action perform action 
permits agent adopt best policy current knowledge system time updating knowledge 
unfortunately expect convergence optimal policy greedy approach adopted learning guaranteed converge optimal function optimal policy state sampled sufficiently 
exploration strategy adopted ensure states low value estimates visited 
learning research ad hoc exploration strategies adopted choosing estimated best action fixed fraction time actions uniformly remainder boltzmann distribution estimated values actions determining likelihood selected 
considerations optimal exploration strategies optimal learning explored especially relation tok armed bandit problems 
model approaches rl samples generate model mdp estimated transition prob br xt max value function policy 
possible manner incorporate models rl called certainty equivalence approach treats current estimated model accurate constructs optimal policy current estimate 
clear approach generally wildly infeasible requires recomputation optimal policy sample address issue exploration 
sutton dyna technique adopts extreme approach 
riis obtained estimated bris statistically updated number ofq values revised new model 
particular stateaction chosen randomly values updated current model assumed experienced state action pairs ais updated 
analogous dynamic programming steps current model opposed computing optimal policy model 
interesting refinement model prioritized sweeping 
updating random values states updated largest changes value effect sample estimated model 
absolute difference previous estimated value new value computed updated model 
potential priority direct true priority higher higher priority 
backups performed priority states updating priorities proceed 
prioritized sweeping requires additional computational effort bookkeeping generally results considerably faster convergence see details 
intuitively backups focused states current model values changed change model induced sample 
pressing issues rl generalization value function approximation 
solution mdps standard models usually explicit state space formulation 
computing backups state infeasible large problems rl problem exacerbated need sample state large number times obtain accurate models value estimates 
generalization provides means apply state value value model parameters learned state number states known inferred problem characteristics 
reduces need visit state 
discuss generalization section 
reader interested additional details rl referred nice survey area 
multiagent mdps coordination problem multiagent planning typically assumes number heterogeneous agents set actions task solved 
generally agent goals assume problem fully cooperative 
utility particular system state agents 
uncertainty general utility models model problem multiagent markov decision process mmdp mdp action chosen state consists individual action components performed agents 
defining propose useful framework study coordination mechanisms 
think decision processes games existence joint utility function 
fact person stochastic games payoff function agents 
form stochastic game closely related general framework repeated games discussed myerson pr sa ans 
tions partially observable mdps 
occasion exploit perspectives generalization single agent mdps specialization ofn person stochastic games 
definition multiagent markov decision process tuple hs pr sand finite sets states agents respectively finite set actions available transition function andr ris realvalued reward function 
mmdp duba set joint actions 
intuitively stage process agents select individual action perform execute 
resulting joint action influences evolution system 
transition function describes probability transition occurrence joint actiona 
usually write pr denote quantity ai component joint 
bto denote joint action formed individual ai 
reward function determines received entire collection agents alternatively agent system states 
mmdp want produce behavior individual agents maximizes expected reward receive system progresses time 
stationary individual policy chooses states probability distribution agent actions 
individual policy deterministic action probability state randomized joint policy induced set individual obvious mapping states joint actions distributions joint actions 
value joint policy equation single agent case obvious modification randomized policies 
joint utility function useful think collection agents single agent goal produce optimal policy joint mdp 
optimal deterministic policies exist restricting attention joint policies agents choose randomized individual policies independently rule optimal joint action clearly best thing agents individually collectively adopt optimal joint policy 
take optimal value joint mdp gold standard performance multiagent system 
interesting problem letting effects individual stated providing mechanisms determine joint effect set individual actions automatically see natural way specify problem 
address issue assume joint effects see section 
policies correspond behavioral strategies restricted stationary 
randomized policies joint mdp modeled collection randomized individual policies correlated action choice agents captured way 
home agent coordination problem difficulty treating mmdp standard mdp actions implemented distributed fashion lies coordination 
general number distinct optimal policies mdp 
agents choose individual policy induced optimal joint policy guarantee agents select joint policy actual joint policy selected fact optimal 
illustrate extremely simple example consider mmdp designed agents optimal policies 
policies agree choice joint action states fors perform move left right see 
assume adopt optimal joint actions states value joint policy possible actions taken table optimal values joint actions 
hr ri joint policy induced individual policies states optimal 
words coordinate policies states 
number simple ways ensure coordination 
central controller compute optimal joint policy communicate individual policies induced corresponding agents 
second method agent agent case communicate choice action 
assume central controller feasible communication possible individual agents determine policies 
treating mmdp ann person game easy see determining optimal joint policy problem equilibrium selection 
particular optimal joint policy nash equilibrium stochastic game agents adopt individual components optimal policy incentive deviate choice 
equilibria usually taken form basic solution concept games classic problem game theory selecting usual assumption multiagent settings fact offers advantages cases utility function system dynamics change new agents added system cases desirable individual policies line incrementally 
addition adding means communication agents prove costly timeliness action adversely affected communication delays 
particular fa xt spr equilibrium set equilibria 
coordination problem simply equilibrium selection 
determining possible multistage games generally difficult problem 
special structure cooperative setting relatively easy optimal joint policies computed straightforward dynamic programming techniques described section 
additional assumption coordination problem tractable 
assume agent knows structure game compute optimal value joint mdp allows decompose coordination problem follows 
agent readily determine set optimal joint actions states simply set potentially individually optimal pio actions actions belong optimal joint actions fors 
assume time agents select actions pio set 
denote set pio 
say weakly dependent pio choice 
case agent choice may require coordination agents states 
define state simple matrix game consisting agents weakly dependent set pio state payoff function combination choices agents 
payoff expectation value resulting state ja joint choice assumption agents included game adopt unique pio action ats 
instance state game example matrix values assigns coordinated choice uncoordinated choice 
profitable view coordination problem finding coordinated joint action equilibrium selection state games arise mmdp finding coordinated global policy 
local nature state game problems perspective feasible 
emphasize potentially useful decomposition state games ignores certain dependencies true solution state games 
elaborate section 
conventions learned coordination section address coordination problem 
particular focus conventions means coordinate choices agents 
adopt view conventions social laws restrictions possible action choices agents various circumstances 
example think traffic rule states important issue distributed computation addressed 
drive righthand side road useful convention prevents accidents 
setting coordination problem restrict agents choose pio actions constitute optimal joint action state 
consider general means applying conventions coordination problem imposition conventions system designer learning coordinated policies agents repeated interaction 
addition briefly discuss application rl techniques agents know system model 
designed conventions social laws conventions examined detail lewis 
shoham tennenholtz address issue computational perspective 
social law restriction set possible actions agent adopt state useful social law ensures agent construct successful plans actions adopted agents 
agent knowledge restrictions placed agents ensure chosen actions interfered actions course laws permit planning agent latitude achieve goals 
general problem quite difficult restrictive settings assumptions permit general convention imposed agents mmdp 
setting simple convention applied quite generally take liberty system designer give agents identify assumptions allow convention imposed 
set agents ordered 

set actions available ordered 

orderings known agents 
information adopting lexicographic convention coordinating pio action choices 
intuitively states set agents weakly dependent coordinate follows agent agent ordering set adopt action ordering pio action available 
agent adopt pio action consistent agent choice point view agent sorts set joint optimal actions adopts component joint action policy states 
agents involved state game adopt single pio action states 
trivial observation take part assumption complete state observability different agents may different policies action sets able distinguish agent important part predicting effects joint actions 
actions selected simultaneously turn implied 
proposition individual specified lexicographic convention induced joint policy optimal joint mdp 
example suppose thata andb 
convention states choose states lat convention rely values attached state convention ensures optimal equilibrium achieved consideration values suboptimal joint moves 
furthermore lexicographic convention allows reduce number agents moves coordinated 
say individually optimal states optimal joint bis optimal ats 
pio set fori 
assuming agents coordinate individually optimal choice exists need bother coordinating choice agents 
say state strongly dependent optimal choice fori pio choices 
reduced lexicographic convention identical coordination restricted agents strongly dependent agents merely weakly dependent choose freely optimal action choices 
proposition holds reduced convention 
lexicographic conventions general domain independent mechanisms coordinating agents 
furthermore implementable sense adopted agent offline construction policy 
choices need online implementing policy coordination assured automatic 
stands sharp contrast mechanisms communication negotiation appeal arbiter central controller necessarily delay execution concrete actions effect optimality choice say negotiated 
assume consistent knowledge required orderings agents 
assumption especially plausible certainly restricted systems homogeneous agents similar capabilities 
instance imagine user deploying set agents come say division labor solve number ongoing tasks 
furthermore deal withthe loss additional agents easily envisaged putting new agents ordering 
learned coordination conventions settings designer unable unwilling impose orderings knowledge agent capabilities accompanied knowledge ordering 
cases agents may learn coordinate repeated experience learning individual policies adopted agents 
agent ordering imply existence hierarchical master slave relationship agents 
research learned coordination emergent conventions studied quite extensively ai game theory 
distinct classes models studied agents large population randomly matched evolve strategies response expected play population fixed set agents repeatedly interact 
ai shoham tennenholtz explored emergent conventions setting large population randomly matched individuals 
experiment simple coordination game investigate large number learning conventions internal success strategy predicted best response population large limited memory models 
models studied quite extensively game theory including experimental formal analysis convergence 
notion fictitious play offers simple learning model agents keep track frequency opponents particular strategies point time adopt best response randomized strategy profile corresponding observed frequencies 
models particular especially relevant enterprise 
young uses matching model ideas clearly apply setting 
model agents sample fixed amount past history incomplete sampling order determine frequency agents play various strategies adopt appropriate best responses 
shows conditions method converge pure strategy equilibrium include coordination games 
kalai lehrer considerable importance 
model repeated game true stochastic game performance learning accounted determining best response 
assume agents prior distribution strategies opponents update beliefs warranted experience adopting best responses beliefs 
consider ways similar ideas applied setting eye computational issues 
assume agent prior beliefs policies agents ensure priors obvious readily computable beliefs updated agents act interact 
want agents converge optimal joint policy cases deterministic agents forced adopt randomized policies learning obvious reasons 
instance initial example confidence choice action randomized choice prevent deadlock proposed kalai lehrer stage game agents update beliefs agents policies adopt best response set updated policies 
strategy eventually lead nash equilibrium general repeated game furthermore case hope convergence optimal equilibrium important aspect model kalai lehrer fact repeated interactions agents point time strung form stochastic game 
agent decision point game influenced immediate outcome local game impact choices 
issues sacrificing local optimality sake exploration arise agents aim globally optimal moves horizon interest may include exploration certain situations 
idealized perspective leads number practical difficulties 
representing strategies infinite time horizon requirement agent compute new best response step updated beliefs agents strategies fact agents access mmdp optimal value function break mmdp state games require coordination provide means cheat learning global equilibria 
propose agents learn coordination policies learning coordinated action choice individual state games independently 
state games incorporate optimal value function longrange consequences action choices ignored myopic learning repeated games 
obviates need exploration environment 
state games fashion merely approximates true uncoordinated decision process 
agents learning coordination state game values reflect optimal coordinated joint policy current policy 
values represent desired target values presuming action choices eventually coordinated true limiting values 
furthermore computational advantages associated method 
beliefs updated need recompute new policy best response agent needs compute new best response state game 
conventions learning need take place state games coordination necessary strongly dependent states agents coordination depends 
describe method type detail 
assume agent knows optimal value function pio choices available agents state 
restrict agents choose pio choices state agent want agents example adopt randomized policy includes choosing probability constitutes equilibrium certainly optimal policy 
stylized settings repeated prisoner dilemma interesting strategies tit tat 
computational difficulty similar certainty equivalence approach model rl described section 
general may reasons see 
strongly dependent part 
assume agent involved state prior distribution set randomized strategies adopted agents involved ings 
beliefs agent gs probability distribution set randomized strategies adopt randomized pio 
denote beli aj degree belief perform states 
general rule reasonable prior provided rule choice action state game 
consider case agent uses simple prior dirichlet distribution 
represented small number parameters updated quite easily 
cardinality pio set 
beliefs represented dirichlet njn capturing density function see strategies 
expectation action adopted 
intuitively viewed number times case observed 
initial parameters adopted prior beliefs strategy 
simplicity assume prior parameters set uniformly reflecting uniform expectation actions uniform prior strategies course 
agent finds coordination required compute set pure best responses beliefs agents strategies 
determine actions maximum expected utility expected actions agents agent choose pure best responses pr ai randomly assume uniformly execute 
pr aj ai pr aj agents executed pr ai selected individual actions states induced joint action causes state transition agent observes 
observation provides information regarding actions selected agents ats 
actions stochastic outcomes presuppose directly observe action performed consider simpler case direct action observation 
update beliefs strategy simple application bayes rule 
computes probability pio knows resulting performed prior probabilities computed beliefs beli ak arbitrary joint transition probabilities 
updates distribution note require details belief distribution simply expectations dirichlet parameters 
stands contrast learning equilibria emergent conventions 
strategies observation 
standard dirichlet requires simply increment observed 
indirect observation individual actions pr intuitively fractional outcome special case actions directly observable need compute probabilities action occurrences simply incrementing appropriate dirichlet parameter example suppose initial beliefs strategy states probability 
suppose right agent resulting state 
move observed certainty update belief parameters beh 
ifb moves error prone say probability moving right chosen chosen thena belief parameters 
case best response time encountered isr 
decomposition mmdp state games essentially means coordination restricted independent coordination local action choice individual states opposed global coordination policy 
get feeling process experimenting individual state games 
illustrate symmetric coordination games similar example described state game haven agents moves 
set moves agents rewarded execute move smaller value dif 
coordinated joint actions 
assume agent observe exact action performed agents 
results experiments showing convergence shown 
axis shows number times game encountered axis shows average error probability chance uncoordinated joint action adopted agents best response strategies point 
action observable symmetric games quite easy see convergence optimal joint action quite rapid roughly agents choose random actions coordinated action 
instance game coordination assured fourth play game equilibrium fractional parameters give defined dirichlet expectations result fact weighted combination dirichlet distributions result standard update positive probability outcomes 
expectations suits needs 
important note agents updating sampled distribution stationary 
convergence ensured properties best responses 
experiments quite similar nature kind described model adopted 
fact larger values ofn faster convergence due likelihood initial randomization produce unique coordinated action leading immediate error probability convergence symmetric coordination games number interactions convergence action observable symmetric coordination games 
results averaged trials 
reached agents diverge 
conventions adopted stable 
difficult show simple setting agents converge probability quickly approaching coordinated action see instance 
decomposition mmdp optimal value function guarantees independent local coordination state games ensures convergence global coordination jointly optimal policy 
proposition mmdp state game strongly dependent states action observable symmetric coordination game 
learning scheme described converge policy form 
rate convergence adversely affected game symmetric 
example consider asymmetric game agents start prior beliefs moves thena initial best response isa isb 
agents chance coordinate actions randomize pure best responses probability toa 
integer nature updates happen sixth interaction seventh interaction 
rate convergence automatically slowed 
long values state game finite precision convergence guaranteed 
convergence globally optimal policy assured symmetric nn conditions dropped 
convergence 
possible way enhancing convergence consider notion best responses 
allows agents randomize actions close best responses current beliefs 
example beliefs agents hover point randomize belief remain close toh reach point dirichlet parameters sum multiple 
allowing best responses gives agents ample opportunity break cycles 
possibility allow small amount experimentation agents point time agent choose random pio action probability adopting best response option probability 
note agents pop equilibria small probability coordination achieved experimentation best responses probability diminishes time case 
note relationship learning type solution games fictitious play 
fictitious play guaranteed converge non zero sum games general noted young converge coordination games circumstances 
best responses experimentation crucial cases actions directly observable 
general update dirichlet parameters probabilities action occurrences nondeterministic action unobservable setting agents generally reach point randomization possible 
example consider original coordination problem success probability left right action 
case initial randomization lead coordinated joint action agents remain equilibrium 
successfully coordinate interaction updated agent assigns equal probability actions 
chance coordination matter times game played shows performance randomizing pure best responses symmetric coordination game nondeterministic actions success probabilityof various values 
note base case improve time initial coordination fails hope coordination lost 
question exists convergence particular action observable problem guaranteed limiting probability 
shows results problem experimentation performed different probabilities 
important note error probabilities include chance uncoordinated action due experimental choice 
experimentation appears effective means ensuring setting observable actions 
conjecture decayed experimentation rates cutting experimentation certain point enhance performance 
error probability convergence stochastic game epsilon best response true best response epsilon epsilon epsilon epsilon epsilon number interactions convergence stochastic game best response 
results averaged trials 
error probability convergence stochastic game experimentation number interactions epsilon epsilon epsilon epsilon convergence stochastic game random experimentation 
results averaged trials 
agents beliefs strategies converge nash equilibrium optimal coordinated pure strategy equilibrium 
aa bb ab ba dd dd dd dd potential difficulty pio actions assumed independence state games optimal value function produce state game values deserves discussion 
structure state games pio actions considered candidates coordination 
mentioned assumption justified long run values reflect truly coordinated policies 
point coordination assured strongly dependent states current policies agents necessarily lower value states 
adopt precise model model learning process part mmdp order ensure optimal performance learning 
example agents may decide learning coordinate sense described may worth risk 
restricting attention agents pio actions may preclude considerations 
instance consider agent mmdp shown 
state pio actions agent coordinated choice ensure repeated transition high reward state 
typical assumptions initial probability coordinated action 
discount parameter small optimal choice initial inability coordinate agent choose regardless quickly convergence coordinated choice arise 
restricting attention pio actions considerations accounted restriction computational advantages 
small part reduces number actions agents involved coordination 
significant fact agent needs recompute value function mmdp state games beliefs agent strategy updated 
computing value function order determine best response daunting task perform interaction 
final justification restriction pio choices state games related interpretation discounting 
certain ai advocated average reward optimality criterion opposed discounted total reward 
criterion example ensure agents want coordinate state convergence learning process slow 
multiagent reinforcement learning clearly apply ideas reinforcement learning solution agents lack knowledge model 
case agent know effects joint actions rewards associated particular states little done initially way restricting attention particular actions beliefs agents 
way rl agent learn policy standard means learning regard agents 
words agents simply treated part environment 
applying learning multiagent systems adopt just approach 
instance mataric describes experiments mobile robots learning applied cooperative task results 
similar vein yanco stein reported experimental results hierarchical learning order learn cooperative communication leader follower agent see 
works convergence achieved 
slightly different semi cooperative application learning reported set agents straightforward learning agents orthogonal interests 
experiments learning converges optimal solution similar results obtained fully cooperative setting convergence policies obtained learned policies generally suboptimal due inadequate exploration 
little theoretical analysis learning multiagent settings applied success fully cooperative noncooperative games 
learning difficulty multiagent learning fact distributions stationary agents react changing strategies 
fully cooperative setting conjecture straightforward learning lead optimal policies appropriate exploration 
result interest learning related methods automatic agent programming course may prove useful semi cooperative settings clear learning generally converge pareto optimal solutions 
area zero sum games little attempt apply learning 
exception littman addresses person zero sum games 
experience particular state results updated value determined small linear program solve analogue local state game 
conjectured precisely solutions generally pareto optimal certain agents done better agents cooperative 
agents incentive 
learning converge equilibrium 
interesting question extent modelbased approaches rl applied 
certain sense view learning approaches andn person games generally type model rl 
point agent updates model agents strategies plays best response strategy profile point time 
analogy dyna architecture compelling 
may case simultaneous learning system model agents strategies profitable method attacking problem 
structured representation computation pressing problems solution mdps rl need problem representations structured computational methods approximation techniques 
mentioned earlier planning problems typically specified terms number domain features random variables propositions size state space grows exponentially terms natural problem size 
called curse dimensionality plagues attempts solve mdps standard methods specified explicitly terms state space 
just rl problem severe conventions learned repeated interaction states required problem considered solved 
way addressing problem case known unknown models aggregation methods generalization number states grouped similar identical value action choice 
aggregates treated single state dynamic programming algorithms mdps related methods rl 
aggregations number different problem features similarity states domain metric generally assume states grouped optimal value 
addition schemes exact approximate adaptive fixed uniform nonuniform generated priori problem characteristics learned generalizations 
large problems expect agents learn coordinate actions separately state 
lessons learned state applied similar states learning enhanced experiences similar states merged increase confidence learned strategies 
section briefly describe structured problem representations solution mdps preliminary suggestions ideas exploited learning coordinated policies mdps generalization 
research generalization approximation crucial learning strategies applied realistic problems 
brief summary earlier structured representation computation mdps 
assume set atomic system inducing state space size jpj stage temporal dynamic bayesian networks describe actions action bayes net set nodes representing system state prior action node variable set representing world action performed directed arcs representing causal influences sets 
post action node associated conditional probability table cpt quantifying influence action corresponding variable value influences see detailed discussion representation illustrates representation single action lack arc pre action post action network independence ofa effect prior value ofx 
capture additional independence assuming structured cpts 
particular decision tree represent function maps combinations parent variables conditional probabilities 
instance trees show true post action true 
additional regularities transition probabilities provide compact representation usual locally exponential cpts matrices 
similar representation represent reward shown 
algorithm solving mdps exploits representation 
decision trees compact function representations policies value functions essentially representing values clusters regions state space opposed individual states 
representation form structured dynamic programming implemented bellman backups performed regions 
note regions fixed policy construction adapted value function updated 
result compact region representation optimal policy value function illustration policy corresponding value function shown 
example policy represented associating actions distinct regions state space states 
extension bayesian network representation actions multiagent mdps straightforward structured computation optimal value functions policies joint mdp 
intuitively offers tremendous potential learning coordinated policies 
imagine instance tree represents possible optimal joint policies 
furthermore suppose region hcu survey issues structured computation representation mdps see 
simplify presentation consider binary variables 
toy domain robot supposed get coffee coffee shop street get wet raining umbrella rewarded brings coffee user requests penalized lesser extent gets wet 
network describes action fetching coffee 
see similar approach rl goal deterministic problems 
wc true false key hc reward wc hc wc hc wc hc hc hc hc matrix tree representation action network tree structured cpts reward tree hcu hcu delc cr cr delc go go key go go structured optimal policy value function leftmost branch admits pio action choices agents 
agents solve mmdp way determine value function structure recognize jointly optimal action choice applied state region 
experience state region applied agents update beliefs regarding states region 
coordinating agents need set beliefs region additional advantage decomposing coordination problem state games 
representations exploited value function approximation construction approximately optimal policies 
generally speaking value trees pruned computation keep computational costs sacrificing optimality 
error bounds resulting policies 
offers compaction value function additional generalization learning 
additional difficulties may arise example agents approximate value function differently convergence learning may drastically affected 
issues great interest 
note research carried rl community generalization learned values action choices value function approximation especially continuous domains see example 
survey provides nice discussion issues 
concluding remarks surveyed number issues arise application single agent planning learning techniques setting fully cooperative multiagent planning 
number models methods planning learning game theory applied directly extended coordination problem 
great number extremely interesting avenues research need exploration 
regard learning coordination experimental different learning strategies crucial 
proofs convergence methods speeding convergence specific schemes especially nice computational properties forthcoming instance circumstances learning converge 
interesting question extent specially designed priors prevent guarantee speed convergence coordinated equilibria 
investigation needed extent problem decomposition methods described affect optimality performance learning principled ways address tradeoffs base level performance computational demands 
consider example related right half tree agents may divide tasks getting coffee picking mail exactly way regard raining 
consider different convention changes variables absurd case 
probably pressing needs study coordination learning structured problem representations generalization approximation techniques 
area synthesis ai representations game theoretic models holds promise 
assumptions underlying mmdp model may realistic cases 
instance knowing agents involved problem actions disposal action models may inappropriate 
unrealistic assumption full observability 
view point departure investigation special case assumptions gradually relaxed effort find computationally effective coordination mechanisms general settings 
included investigation learned communication languages protocols 
yoav shoham encouragement 
david poole michael littman comments discussion ideas 
research supported nserc research ogp 
astrom 
optimal control markov decision processes incomplete state estimation 
math 
anal 
appl 
robert axelrod 
evolution cooperation 
basic books new york 
barto bradtke singh 
learning act real time dynamic programming 
artificial intelligence 
richard bellman 
dynamic programming 
princeton university press princeton 
bertsekas 
adaptive aggregation infinite horizon dynamic programming 
ieee transactions automatic control 
craig boutilier thomas dean steve hanks 
planning uncertainty structural assumptions computational leverage 
proceedings third european workshop planning italy 
craig boutilier richard dearden 
abstractions decision theoretic planning time constraints 
proceedings twelfth national conference artificial intelligence pages seattle 
extended version appear artificial intelligence 
craig boutilier richard dearden 
approximating value trees structured dynamic programming 
manuscript 
craig boutilier richard dearden goldszmidt 
exploiting structure policy construction 
proceedings fourteenth international joint conference artificial intelligence pages montreal 
craig boutilier david poole 
computing optimal policies partially observable decision processes compact representations 
manuscript 
craig boutilier martin puterman 
process average reward optimality 
proceedings fourteenth international joint conference artificial intelligence pages montreal 
justin boyan andrew moore 
generalization reinforcement learning safely approximating value function 
tesauro touretzky leen editors advances neural information processing systems 
mit press cambridge 
anthony cassandra leslie pack kaelbling michael littman 
acting optimally partially observable stochastic domains 
proceedings twelfth national conference artificial intelligence pages seattle 
david chapman leslie pack kaelbling 
input generalization delayed reinforcement learning algorithm performance comparisons 
proceedings twelfth international joint conference artificial intelligence pages sydney 
peter dayan 
convergence td general machine learning 
peter dayan geoffrey hinton 
feudal reinforcement learning 
advances neural information processing systems 
morgan kaufmann san mateo 
thomas dean leslie pack kaelbling jak kirman ann nicholson 
planning deadlines stochastic domains 
proceedings eleventh national conference artificial intelligence pages washington 
thomas dean kanazawa 
model reasoning persistence causation 
computational intelligence 
thomas dean hong lin 
decomposition techniques planning stochastic domains 
proceedingsof fourteenth international joint conferenceon artificial intelligence pages montreal 
thomas dean michael wellman 
planning control 
morgan kaufmann san mateo 
richard dearden craig boutilier 
integrating planning execution stochastic domains 
proceedings tenth conference uncertainty artificial intelligence pages seattle 
thomas dietterich nicholas flann 
reinforcement learning unified approach 
proceedingsof twelfth international conferenceon machine learning pages lake tahoe 
ephrati jeffrey rosenschein 
divide conquer multiagent planning 
proceedings twelfth national conference artificial intelligence pages seattle 
drew fudenberg david levine 
steady state learning nash equilibrium 
econometrica 
john reinhard selten 
general theory equilibrium selection games 
mit press cambridge 
ronald howard 
dynamic programming markov processes 
mit press cambridge 
leslie pack kaelbling 
learning embedded systems 
mit press cambridge 
leslie pack kaelbling michael littman andrew moore 
reinforcement learning survey 
appear 
ehud kalai ehud lehrer 
rational learning leads nash equilibrium 
econometrica 
kreps wilson 
sequential equilibria 
econometrica 
david lewis 
conventions philosophical study 
harvard university press cambridge 
michael littman 
markov games framework multiagent reinforcement learning 
proceedings eleventh international conference machine learning pages new brunswick nj 
sridhar mahadevan 
reinforcement learning case study comparing learning learning 
proceedings eleventh international conference machine learning pages new brunswick nj 
maja mataric 
reward functions accelerated learning 
proceedings eleventh international conference machine learning pages new brunswick nj 
george rafael rob 
learning mutation long run equilibria games 
econometrica 
andrew moore 
variable resolution dynamic programming efficiently learning action maps multivariate realvalued state spaces 
proceedingsof eighth international conference machine learning pages evanston il 
andrew moore christopher atkeson 
prioritized sweeping data time 
machine learning 
andrew moore christopher atkeson 
algorithm variable resolution reinforcement learning multidimensional state spaces 
machine learning 
appear 
roger myerson 
game theory analysis conflict 
harvard university press cambridge 
radford neal 
probabilistic inference methods markov chain monte carlo methods 
technical report crg tr department computer science university toronto toronto 
guillermo owen 
game theory 
academic press new york 
martin puterman 
markov decision processes discrete stochastic dynamic programming 
wiley new york 
martin puterman shin 
modified policy iteration algorithms discounted markov decision problems 
management science 
paul schweitzer martin puterman kyle 
iterative aggregation discounted semi markov reward processes 
operations research 
sandip sen 
multiagent coordination learning classifier systems 
proceedings ijcai workshop adaptation learning multiagent systems pages montreal 
sandip sen john hale 
learning coordinate sharing information 
proceedings twelfth national conference artificial intelligence pages seattle 
yoav shoham moshe tennenholtz 
emergent conventions multi agent systems initial experimental results observations 
proceedings third international conference principles knowledge representation reasoning pages cambridge 
yoav moshe tennenholtz 
synthesis useful social laws artificial agent societies 
proceedings tenth national conference artificial intelligence pages san jose 
reid simmons sven koenig 
probabilistic robot navigation partially observable environments 
proceedings fourteenth international joint conference artificial intelligence pages montreal 
satinder singh tommi jaakkola michael jordan 
reinforcement learning soft state aggregation 
hanson cowan giles editors advances neural information 
morgan kaufmann san mateo 
richard smallwood edward sondik 
optimal control partially observable markov processes finite horizon 
operations research 
edward sondik 
optimal control partially observable markov processes infinite horizon discounted costs 
operations research 
richard sutton 
learning predict method temporal differences 
machine learning 
richard sutton 
integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning pages austin 
jonathan stuart russell 
control strategies stochastic planner 
proceedingsof twelfth national conference artificial intelligence pages seattle 
christopher watkins peter dayan 
learning 
machine learning 
gerhard wei 
learning coordinate actions multi agent systems 
proceedings thirteenth international joint conference artificial intelligence pages chambery fr 
holly yanco lynn andrea stein 
adaptive communication protocol cooperating mobile robots 
meyer roitblat wilson editors animals animats proceedings second international conference simulation adaptive behavior pages 
mit press cambridge 
peyton young 
evolution conventions 
econometrica 
