herodotus peer peer web archival system timo burkard bachelor science computer science engineering massachusetts institute technology submitted department electrical engineering computer science partial fulfillment requirements degree master engineering electrical engineering computer science massachusetts institute technology may timo burkard 
rights reserved 
author mit permission reproduce distribute publicly electronic copies thesis document part 
author 
department electrical engineering computer science may certified 
robert morris assistant professor thesis supervisor accepted 
arthur smith chairman department committee graduate students herodotus peer peer web archival system timo burkard submitted department electrical engineering computer science may partial fulfillment requirements degree master engineering electrical engineering computer science thesis design implementation herodotus peer peer web archival system 
wayback machine website currently offers web archive herodotus periodically crawls world wide web stores copies downloaded web content 
wayback machine herodotus rely centralized server farm 
individual nodes spread internet collaboratively perform task crawling storing content 
allows large group people contribute idle computer resources jointly achieve goal creating internet archive 
herodotus uses replication ensure persistence data nodes join leave 
herodotus implemented top chord distributed peer peer lookup service 
written freebsd 
analysis estimated size world wide web shows set nodes required archive entire web assuming node typical home broadband internet connection contributes gb storage 
thesis supervisor robert morris title assistant professor acknowledgments thesis advisor robert morris guidance advice thesis 
helped define topic thesis focus real problems 
worked closely frans kaashoek david karger invaluable feedback assistance 
members pdos group lcs supported great deal 
specifically frank dabek answered countless questions chord 
david mazi res helped questions sfs libraries 
russ cox emil sit helpful questions chord rpc 
doug de couto chuck blake helped find machines disk space perform crawls 
david andersen ron testbed answered latex questions 
david ziegler walsh proofreading thesis providing valuable feedback 
special david bailey truly 
chapter internet archive wayback machine started archiving world wide web evolved time 
non profit organization funded companies wayback machine captures snapshots popular web sites html graphics periodic intervals 
search engine web archive increases value internet 
search engine facilitates finding certain pieces information web archive ensures data published web stored persistently remains accessible indefinitely 
web provides wealth information 
sites frequently taken restructured result potentially interesting information unavailable 
web archive constantly crawls web keeps copy content allows users type url date see site looked past 
archiving internet difficult vast amount storage bandwidth required accomplish goal 
website wayback machine reports total hardware expenses date servers central download site 
order download tb month available bandwidth mbit required 
current bandwidth prices translates internet access costs year 
figures show run centralized site large investment commercial government scale necessary 
herodotus solution achieves goal total amount resources massively distributes task archiving web thousands collaborating peer peer nodes 
large group people institutions universities corporations small contributions hardware bandwidth resources order collaboratively archive html image content world wide web 
wayback machine joint effort parties party contributes money operate central data center 
herodotus hand allows participants contribute machine bandwidth resources directly 
scheme economically efficient providing excess resources little cost participating parties 
distributed peer peer web archive faces challenges centralized system 
fetching storing pages partitioned nodes 
links newly downloaded pages forwarded efficient manner node responsible part url space 
user queries forwarded node stores actual url 
peer peer nodes unreliable join leave system crucial replication achieve persistent storage downloaded content time 
herodotus addresses issues 
way herodotus automatically replicates content nodes join go eliminates need maintenance staff centralized solution requires 
long new nodes join herodotus network accommodate storage bandwidth needs herodotus automatically manages resource allocation achieves fault tolerance 
built working version herodotus small scale download content mit domain totaling urls 
due immense effort necessary recruit large number participating peers deployed herodotus larger scale 
remainder thesis organized follows 
looking related data gathered mit crawls understand nature problem archiving entire world wide web 
design herodotus 
describe status current implementation 
analyze nodes required herodotus archive entire internet describe requirements nodes terms storage space available bandwidth uptime 
reviewing accomplished 
chapter related mentioned internet archive wayback machine currently system archiving web 
operational 
early years small subset web archived rate sites downloaded ranged days months 
wayback machine crawling web aggressively adding tb new data month current data repository tb 
project run non profit organization backed companies little public internals wayback machine 
information available website clear run central site requiring investment hardware large amount available bandwidth expensive dedicated support staff manage server farm 
contrast herodotus operates peer peer fashion allowing large number small parties contribute resources collaboratively archive web 
self managed application automatically allocates peers replicates data need dedicated support staff eliminated 
subproblem archiving web crawling web 
popular existing crawler projects include google mercator 
systems operate lan environments high speed links collaborating machines 
machines environment typically high level reliability machine failures really considered issue 
contrast herodotus distributed crawler operates peer peer fashion 
setting links cooperating machines expensive machines frequently join leave set collaborating nodes due temporary outages machine failures 
herodotus uses number techniques adequately address issues 
herodotus built top peer peer lookup system chord 
chord provides framework peer peer machines organized fault tolerant communication scheme 
applications built top chord provided hash function maps key unique chord node responsible key 
application decide keys constitute correspond data tasks divided chord machines 
herodotus chord partition url space participating nodes 
see chord hash function applied url determine node responsible url 
set currently participating hosts changes joining leaving chord nodes chord signals affected chord nodes responsibilities certain hash values reassigned state associated hash values transferred accordingly 
application chord cfs cooperative file storage 
cfs achieves faulttolerant distributed storage chord nodes 
herodotus cfs store downloaded content decided simpler approach stores downloaded data local disks 
design goals cfs achieve load balancing downloads nodes 
underlying assumption cfs relatively data files inserted system data highly popular shared music storage systems 
herodotus opposite true 
vast amount data needs stored fault tolerant manner effect accesses negligible 
bulk operations constitute download storage data retrieval 
storing data locally efficient compared cfs inserting new data triggers large amount information communicated nodes store new data locations 
chapter problem chapter analyze complexity problem archiving entire internet 
understanding data volume system needs able deal helps understand requirements design need satisfy order adequately solve problem 
analysis data obtained archiving web pages mit domain 
findings related research extrapolate numbers obtained mit experiment estimate corresponding metrics entire world wide web 
chapter organized follows 
describe setup mit experiment give data obtained 
derive similar metrics entire web 
final part discuss implications design 
mit experiment order understand composition world wide web rate changes archived html files images mit domain course week 
initially started main page web mit edu followed links download store entire mit domain 
interested world wide web limited download html files graphics jpeg gif 
order avoid overloading web servers requests dynamic content potentially crawling infinitely pages generated fly download dynamic content cgi 
downloading url downloaded see changed conditional gets download pages updated 
followed new links seen 
table summarizes results obtained crawls day period 
limited download mit content urls pointing non mit content discarded 
property value number unique urls downloaded crawls percentage urls images average size html file kbyte average size image file kbyte average percentage html files changed day average percentage images changed day new objects day percentage existing objects gzip compressed size html files gzip compressed size image files average number links embedded html page average length urls characters table data obtained mit experiment 
data shows html page images 
size image significantly larger html file 
furthermore applying gzip html files yields compression ratio image files gzip effect attributed fact image files compressed form 
section data extrapolate properties entire world wide web 
extrapolation entire world wide web section extrapolate results obtained previous section entire world wide web 
specifically estimate size world wide web amount storage required capture changes time 
summarize results tabular form 
subsequent section estimates derive design requirements herodotus 
size world wide web attempt crawl entire internet purpose analysis rely sources information 
popular search engine google claims indexed little web pages 
size matches rough estimate pages founders google gave research 
douglis feldmann krishnamurthy report average size html document kbytes 
mit data shows average size html document kbytes 
research report dates back higher number reflective fact web pages complex past years 
consequently mit calculations 
multiplying average size number web pages gives expected size html documents web tb 
mit experiment shown html files compressed factor 
herodotus store web efficient manner snapshot require tb storage 
world wide web consists html embedded images account 
unfortunately research number average size images 
extrapolate numbers obtained mit study entire population web pages 
average images html page expect web contain image files 
average image size kbytes images web amount roughly tb data 
html images compressed format 
herodotus save storage space applying additional compression downloaded data 
combining numbers yields size tb stored tb storage space gzip compression html content 
rate change world wide web order estimate rate change internet consider different types changes updates existing objects newly created objects 
look updates existing web objects 
unfortunately relatively research results available topic 
sample pages observed long period time estimates web pages change day 
mit experiment shows pages change mit domain day 
mit figures biased mit academic institution web pages change frequently commercial web sites 
conservative effect updates assume rate change day html files 
mit study shows image files change neglect effect 
expect produce tb uncompressed data day updates existing objects gb accounting compression 
notice calculations assumed page changes store compressed version new html file 
pages change slightly efficient store diffs pages relative previous versions 
look changes due newly created objects 
research results base estimate numbers mit study 
mit study shows average current number web pages images added day 
translates gb uncompressed data day gb day accounting compression html files estimated size web previous subsection 
goal capture changes web pages daily basis expect need download tb new uncompressed data day 
compression data stored gb storage space day 
month period means total storage capacity roughly tb data 
sanity check compare numbers statistics wayback machine 
wayback machine claims add tb data month order magnitude tb 
wayback machine query url day bases download frequency url rate changed past 
wayback machine capture changes large fraction 
clear number referring compressed data uncompressed data 
addition wayback machine claims total tb data claims adding tb month 
wayback machine operational total amount data tb extremely low compared additional tb month 
fact wayback machine stored small subset internet account discrepancy 
summary results subsection summarize figures previous subsections ease 
table html updated refers html changed download html added refers newly created html pages html new refers sum 
mit study shows effect modified images neglected category images images new 
refers newly created images 
description objects size uncompressed size compressed snapshot html tb tb snapshot images tb tb snapshot total tb tb html updated day tb gb html added day gb gb html new day tb gb images new day gb gb total new day tb gb html new month tb tb images new month tb tb total new month tb tb table summary extrapolated characteristics entire www 
design implications section requirements herodotus satisfy 
discuss general properties distributed web archival system 
second part discuss additional constraints numbers previous sections impose design order feasible 
general properties previous section shown archiving world wide web day day basis involves processing storing large amounts data 
pointed peer topeer system suited want achieve goal having large number small nodes collaborate 
herodotus need address issues arise result distributing load set peers 
keep track set active peers 
peers fail new peers join system herodotus need keep track active peers distributed correctly 
distribute downloading archiving objects 
individual node deal small fraction entire web herodotus provide way partition job downloading archiving data day currently active peers 
particular herodotus ensure unnecessarily duplicated having peers download object needs done completed node 
balance load peers 
distributing peers herodotus needs balance workload assigned node account storage download capacity node 
important avoid overloading certain nodes ensure system complete entire crawls web timely fashion 
replicate content achieve fault tolerance 
peer peer system individual nodes permanently fail disappear 
imperative archive retain historical data extended periods time herodotus replication store data multiple peers redundant fashion 
peers holding certain pieces data disappear herodotus needs replicate data additional peers maintain high level fault tolerance losses highly 
ensuring stored data persistent replication allows herodotus serve historical data users peers holding data temporarily unavailable reboot maintenance 
provide user interface 
order allow users access historical web pages stored herodotus herodotus needs provide simple interface fetches requested content peer node keeping information 
design consequences estimated dimensions web subsection describe additional constraints herodotus needs satisfy considering dimensions web outlined previous section 
large number nodes 
tabulated results entire www show tremendous amount storage required operate herodotus 
assumed compressed storage year operation consume tb storage archive images html 
replication necessary achieve fault tolerance number level replication choose 
level replication example pb total storage space necessary 
node stores gb data means nodes necessary 
important herodotus scales large number nodes 
distributed list seen urls 
key component crawler list urls processed avoid duplicate downloads 
ideally want store list node identify links encountered early need waste bandwidth send peers 
tabulated results estimate total urls 
decide store sha hash values url sufficient identify urls encountered translates gb storage sha hash value bits long 
consequence impossible keep entire list urls encountered 
node maintain complete list urls encountered urls responsible downloading storing 
order avoid resending popular urls appear node decide additionally cache popular urls urls responsible 
cost replacing nodes 
suppose node stores gb data 
node leaves new node joins network gb data need new node maintain level replication 
assume nodes downstream bandwidths mbit take days download entire data nodes 
large number peers necessary peers lower bandwidth capabilities 
conclude nodes need remain part system long periods time order months restoring lost state nodes permanently left system consume resources 
addition node remains part system month joined 
causes peers dedicate significant portion bandwidth bring speed contributes little long term archival process 
deal temporarily unavailable nodes 
previous point demonstrated nodes need part herodotus months 
machines internet connections permanent months 
herodotus robust tolerate temporary outages nodes 
particular data persistently stored node replication restore information node missed unavailable 
chapter design chapter describe design herodotus 
give overview general way herodotus operates 
overview conveys general scheme herodotus operates details left open 
sections fill gaps describing certain aspects design detail 
parts design fully implemented 
sections give multiple possible design choices describe ones picked 
overview herodotus performs main functions continuously crawling web replicating content achieve fault tolerance providing users interface view archived web content 
functions require collaborating peers organized network topology herodotus uses chord achieve goal 
nutshell chord enables nodes find know 
external interface chord exports lookup function allows mapping kind data node chord network 
mapping function participating machines 
internally chord nodes organized ring structure see section 
herodotus uses chord lookup function determine node responsible url delegate task downloading storing url node 
local url database incoming links nodes url seen 
queue download engine chord ring outgoing links nodes link parser web object storage design overview chord ring left operations performed inside node crawl right 
gives overview herodotus continuously crawls web 
left hand side shows collaborating peers distributed chord ring 
right hand side shows goes peer node 
node receives urls responsible peers chord 
node processed url day url simply discarded avoid multiple downloads 
url put queue objects need downloaded 
download engine maintains number concurrent connections web servers download queued objects 
download object completed data stored web object storage local file system document html page forwarded link parser 
link parser identifies html files images 
extracted links sent chord network nodes responsible respective urls 
overview gives general picture herodotus operates sections describe certain aspects detail 
section describe chord way peers maintained detail 
describe chord lookup function employed map urls nodes 
examine links sent nodes efficient manner 
see herodotus keeps state nodes persistent manner safely recover temporary outages reboots 
address herodotus uses replication achieve fault tolerance protection node failures 
examine operational issues optimizations reduce bandwidth usage framework continuously crawl web daily basis 
describe users herodotus retrieve archived versions web pages browser 
chord maintain peers herodotus uses chord maintain set participating peers distribute peers locate archived content 
chord supports just operation key determine node responsible key 
chord store keys values provides primitive allows higher layer software build variety applications require load balancing peer peer network 
herodotus chord primitive 
herodotus node link url page downloaded applies chord lookup function url determine node responsible 
forwards url node download store 
sections describe processes detail 
section summarizes chord works 
detailed description chord please refer chord publication 
explain chord relates consistent hashing 
elaborate chord implements lookup function maintains peers 
address chord provides protection attackers want replace chosen content 
describe chord achieves load balancing 
consistent hashing chord node unique bit node identifier id obtained hashing node ip address virtual node index 
chord views ids occupying circular identifier space 
keys mapped id space hashing bit key ids 
chord defines node responsible key successor key id successor id node smallest id greater equal wrap consistent hashing 
consistent hashing lets nodes enter leave network minimal movement keys 
maintain correct successor mappings node joins network certain keys previously assigned successor assigned node leaves network assigned keys reassigned successor 
changes assignment keys nodes need occur 
consistent hashing straightforward implement constant time lookups nodes date list nodes 
system scale chord provides scalable distributed version consistent hashing 
chord lookup algorithm chord node uses data structures perform lookups successor list finger table 
successor list required correctness chord careful maintain accuracy 
finger table accelerates lookups need accurate chord aggressive maintaining 
discussion describes perform correct slow lookups successor list describes accelerate finger table 
discussion assumes malicious participants chord protocol believe possible nodes verify routing information chord participants send algorithms left 
chord node maintains list identities ip addresses immediate successors chord ring 
fact node knows successor means node process lookup correctly desired key node successor node key successor lookup forwarded successor moves lookup strictly closer destination 
new node learns successors joins chord ring asking existing node perform lookup successor asks successor successor list 
entries list provide fault tolerance node immediate successor respond node substitute second entry successor list 
successors simultaneously fail order disrupt chord ring event improbable modest values implementation fixed chosen log foreseeable maximum number nodes main complexity involved successor lists notifying existing node new node successor 
stabilization procedure described way guarantees preserved connectivity chord ring successor pointers 
lookups performed successor lists require average message exchanges number servers 
reduce number messages required log node maintains finger table table entries 
ith entry table node contains identity node succeeds id circle 
node knows identities nodes power intervals id circle position 
new node initializes finger table querying existing node 
existing nodes finger table successor list entries refer new node find periodic lookups performed part asynchronous ongoing stabilization process 
shows pseudo code look successor node identifier id main loop find predecessor sends preceding node list rpcs succession nodes rpc searches tables node nodes closer id iteration set node current id preceding node list returns id greater id process overshoot correct successor 
may shoot especially new node joined id just id case check id successor ensures find predecessor persists finds pair nodes straddle id aspects lookup algorithm robust 
rpc preceding node list node returns list nodes believes desired id progress successor id unresponsive lookup fail 
second loop ensures find predecessor keep trying long find node closer id long nodes careful ask node find id successor finds id predecessor asks predecessor successor 
find successor id find predecessor id return successor ask node find id predecessor 
find predecessor id id successor preceding node list id maxn alive return ask node list nodes finger table successor list precede id preceding node list id return fingers successors id pseudo code find successor node identifier id remote procedure calls preceded remote node 
maintain correct successor pointers find predecessor eventually succeed 
usual case nodes correct finger table information iteration loop eliminates half remaining distance target 
means hops early lookup travel long distances id space hops travel small distances 
worthwhile note algorithm provide lg bound structure finger table algorithm examines guarantees hop cover half remaining distance 
behavior source algorithm logarithmic properties 
theorems show success performance chord lookups affected massive simultaneous failures 
theorems assume successor list length log 
chord ring stable node successor list correct 
theorem network initially stable node fails probability high probability find successor returns closest living successor query key 
theorem network initially stable node fails probability expected time execute find successor log 
theorems chord ensure failing peers cause herodotus network partitioned separate subnetworks know 
node id authentication chord nodes arbitrary ids attacker take control chosen url space choosing node id corresponding url space successor 
control successor attacker effectively determine archived contents users see 
trick employed people want information published website inaccessible users system 
limit opportunity attack chord node id form sha hash function node ip address concatenated virtual node index 
virtual node index fall small maximum 
result node easily control choice chord id new node joins system existing nodes may decide add finger tables 
part process existing node sends message claimed ip address containing nonce 
node ip address admits having id claimed ip address virtual node index hash id existing node accepts defense place attacker control roughly ip addresses total nodes chord system order chance targeting arbitrary blocks 
owners large blocks ip address space tend easily identifiable malicious individuals 
load balancing see subsequent sections herodotus spread url space evenly id space hash function uniformly distributes urls 
participating herodotus node id fact ids uniformly distributed mean server carry roughly download storage burden 
desirable participating nodes greatly varying storage bandwidth capacities 
accommodate heterogeneous node capacities herodotus uses notion real server acting multiple virtual servers 
cfs uses exact scheme account varying node capabilities 
herodotus service operates virtual server level 
virtual server uses chord id derived hashing real server ip address index virtual server real server 
herodotus server administrator configures server number virtual servers proportion server storage bandwidth capacity 
mapping url node section discuss issue distribution crawling web peer peer fashion purpose creating web archive 
describe simple solution uses direct mapping urls nodes chord lookup function 
discuss sophisticated assignment scheme urls assigned chord nodes domain names 
describe design choice justify 
distribution herodotus distribute downloading processing storing web objects way participating hosts efficiently possible duration crawl 
specifically load distributed way nodes finish share crawl time 
general case scheme take account available bandwidth machine sizes download jobs 
resource vary participating nodes disk size especially important goal herodotus create persistent archive web 
assigning node store web objects available resources machines taken account 
initially assume machines roughly amount bandwidth storage available 
account large differences available machine resources concept virtual chord servers seen previous section achieve better available resources 
concept load balancing achieved simply spreading chord id space uniformly possible 
subsequent sections show ways done 
simple direct mapping chord provides lookup function maps keys node identifiers successor identifier responsible node 
simply apply chord lookup function url applying hash function sha delegate job downloading object node returned lookup 
node responsible maintaining part url space means download process store associated web object 
way urls assigned nodes scheme extremely simple expect achieve load balancing due hash function properties 
host assigned roughly equal amount high probability assuming amortized urls assigned node size object roughly equal 
believe scheme effective easy implement best suited herodotus 
section explore sophisticated alternative approach 
domain mapping scheme described previous section extremely simple implement important property links fact large share links local domain 
chord lookup function applied hash entire url urls objects residing host spread chord ring 
links domain sense map urls nodes way objects residing host crawled node 
reduce number links forwarded nodes expensive network connections opposed processed node 
simple way accomplish applying chord lookup function hostname url opposed entire url previous scheme 
scheme domain name node mapping urls distributed nodes domain name mapping 
method severe problem renders unusable practice 
problem skewed distribution size websites distributed unevenly participating nodes 
particular sites huge amount data cnn com yahoo com mapped single node 
clearly individual node handle large site terms available bandwidth terms available disk storage 
domain mapping scheme scale skewed distribution domain sizes see internet today scheme needs refined domains exceed certain threshold size split multiple nodes 
node responsible domain name provide level indirection forwarding urls domain second chord lookup 
lookup performed entire url domain name plus prefix uri letters entire directory cnn com sports 
design choice scaling problems domain mapping large domains chosen simple direct mapping scheme herodotus 
direct mapping scheme simpler easy see distributes url space evenly chord ring 
domain mapping improved tricks described previous section feel unnecessarily complicates design requiring additional state kept 
addition see ways greatly reduce actual number links sent alleviate slight disadvantage simple direct mapping 
exchanging links overview seen url space split participating peers peers discovered url responsible forward url peer responsible 
previous section established urls mapped peers seen links transmitted recipient peers efficient manner 
discussed 
describe batching forwarding urls important 
discuss methods perform forwarding operation direct lookup ring forwarding 
state design choice herodotus 
batching independent forwarding schemes forwarding efficient batching 
means urls forwarded extracted webpages put batch queue respective recipient 
queue recipient exceeds certain length timer expires contact node send links accumulated 
advantage send multiple links time reducing amortized overhead looking node establishing communication channel expensive compared small amount bandwidth required exchange single link 
illustrate batching improves bandwidth usage needed link exchanges take look actual numbers 
suppose know ip address recipient node 
issuing rpc request chord framework recipient involves sending udp packet bytes rpc chord headers actual url 
chapter seen average length url characters 
issue separate rpc url header overhead 
batching simply concatenate urls special separator symbol amortizes byte overhead urls sent time 
set peers example storing urls sending reduces amortized cost sending url roughly bytes bytes 
direct lookup intuitive forwarding scheme perform chord lookup url encounter batch urls node 
accumulated urls node send rpc recipient node 
chord lookup requires log bandwidth number nodes algorithm cost log urls 
look actual numbers 
chapter seen order store images web content need order hosts 
number nodes stoica experimentally determined average chord lookup path length 
rpc packet sent chord lookup requires approximately bytes lookup cost url roughly bytes batching 
batching cost sending single url scheme bytes means overhead send link times large actual payload roughly bytes 
batching perform chord lookup url determine node send 
batching large number nodes means overhead bytes perform lookup url 
batching amortized cost url times large actual payload 
ring forwarding main idea ring forwarding batching accumulate large number urls walk chord ring lookups distribute urls respective nodes asking node successor 
ring forwarding asymptotically better schemes described requires larger url buffer previous schemes 
idea apply sha hash function url actual chord lookup 
soon total number urls collected exceeds number nodes times constant urls sent recipients algorithm 
node contacts successor sends urls responsible 
operation performed time proportional number urls sent node 
addition recipient reports successor sending node sending node contacts node sends urls responsible 
scheme continues urls distributed successor node reports originating node 
time node walks ring accumulated urls cost sending urls amortized cost sending url constant 
scheme effectively eliminates log cost performing chord lookups ships urls directly recipients 
design choice terms asymptotic cost ring forwarding efficient amortized cost shipping single link opposed log methods exhibit 
problem ring forwarding scale large number nodes node collect number urls order number nodes urls shipped 
context herodotus expect order tens thousands nodes 
chosen ring forwarding herodotus 
amortize rpc overhead nodes accumulates aggregate roughly urls node 
resulting number urls need stored simultaneously order 
average size url bytes accomplished megabytes memory acceptable 
order perform temporary storage retrieval urls efficient manner priority queue 
allows quickly serve urls node visit ring responsible 
crawl queue contain entries means manage accumulate large number links start distributing set links 
timeout ensures links distributed 
experiments mit domain show soon start crawling essentially continuously traverse ring distribute urls due large number urls extracted 
overhead distributing small number links ring higher amortized cost start neglected 
clever ring forwarding scheme essentially eliminated costly chord lookups 
addition batching reduced amortized overhead rpc chord headers roughly 
compared naive direct lookup scheme managed reduce bandwidth required exchange links factor 
persistence individual node seen chapter crucial herodotus robust face short outages temporary network failures machines crashing rebooting 
achieve goal maintaining state herodotus persistent manner disk 
node crashes contents file just changed lost expect rest data remain intact hard disk failure 
achieve robustness crashes herodotus stores downloaded content current state queue disk 
subsections look individually 
downloaded content obviously store downloaded content disk 
fault tolerance impossible hold data memory 
ways data stored disk 
option database want avoid having deploy database thousands participating nodes 
local file system organize vast amounts data 
order facilitate handling decided store downloaded object separate file 
chapter shows reasonable network size nodes node responsible urls average 
initial snapshot expect store significant number page changes url time 
commonly file system accommodate large number files single directory decided hierarchical directory structure 
main download directory herodotus automatically creates subdirectories names ff representing byte sha value corresponding url 
subdirectory additional subdirectories naming scheme corresponding second byte sha value url 
directories store data number entries directory remain small traditional file system problem 
file names structured follows 
consist sha value url concatenated sha value content concatenated day downloaded unix time divided number seconds day 
choice filename allows herodotus determine download timestamp file changed simply looking filename directory having open file 
file contains line header actual url downloaded completeness 
queue queue contains list elements need downloaded 
herodotus node receives new links herodotus nodes puts urls memory data structures stores disk 
achieved creating text files separate queue directory 
queue split separate files consisting elements special pointer file indicates current head queue 
new links created appended file file stored disk 
links dequeued pointer file updated reflect current head queue 
event crash reboot herodotus analyzes queue directory restart 
uses queue entries seed url seen module special pointer file continue crawling correct queue position 
replication previous section described achieve persistence node addressed issue nodes joining leaving network 
chapter identified crucial archive retain stored content long periods time 
especially peer peer environment frequently changing set active nodes important store data redundantly achieve fault tolerance 
nodes chord fail url space automatically falls successor 
natural keep replicated content successors respective node primarily responsible content 
node primary responsibility fails successor replicated data addressed chord lookup function able serve requests data 
node permanently disappears successors holding replicas responsibility creating new replica successor achieve level replication 
number replicas kept highly dependent failure uptime characteristics participating peers long remain network average 
typical peer peer assumptions level replication node primarily responsible plus backup copies successors 
chapter provide assumptions underlying choice 
order achieve level replication node effectively hold data url space corresponding different nodes chord ring 
ways backup copies created peers open connections replicate content data fetched origin servers multiple times 
feel solution appropriate reduces intra node communication fact total bandwidth usage herodotus content provider provide data herodotus times 
consequently adapt ring forwarding scheme way link forwarded node responsible successors 
achieved efficiently maintaining rotating array pointers linked lists containing links respective previous nodes 
sending links nodes reduces probability links potentially get lost due rpc errors 
shown previous section nodes go temporarily correctly maintain state 
missed urls sent nodes downtime 
nodes periodically day compare set downloaded data successor predecessor 
result poten tially large data transfers chosen tcp sockets rpc function calls achieve replication 
node opens connection successor regular basis 
communicating successor predecessor node determines urls peer urls stored 
peers compare set urls directory structure described previous section 
directory calculate sha value list sha value url timestamp pairs files contained directory 
described timestamps unix times divided number seconds day downloads occurred different times day timestamp 
sha checksums match directory contents equivalent 
transmit directory entries determine difference request corresponding file missing 
reason entire filename components filename sha url timestamp peers downloaded url day received different content 
case care sha checksum downloaded content 
new nodes join replication algorithm restore information responsible maintaining 
data stored nodes stored nodes replication protocol designed delete files node longer responsible 
nodes leave permanently successors needs assume additional storage responsibilities 
replication scheme automatically restore conditions 
optimizations reduce bandwidth usage table determined total number web objects 
goal herodotus capture changes web daily basis query urls day 
naive solution simply start known url www yahoo com continue download link distribution scheme described 
involve downloading web objects communicating links peers day 
section mention improve ments significantly reduce bandwidth usage 
improvements related objects downloaded objects stored links distributed 
object download peer querying web server changes url node responsible maintaining archived version url conditional gets greatly reduces bandwidth usage 
conditional gets specified date download modified header 
object changed server simply responds modified returns normal ok updated content 
urls previous version archived traditional gets 
object storage mentioned filenames contain sha content hash url 
supposedly modified web object downloaded herodotus calculates sha content hash compares archived version 
match web server apparently responded conditional get incorrectly new version simply discarded 
link distribution description suggests node downloads objects receives links peers extracted html pages 
nodes responsible set urls day wasteful send links network 
nodes send links contained html documents changed download peers conditional get returned content hash match previous version 
method improved observation 
pages change huge fraction links new page appear old page 
know responsible nodes know links old page 
send links new version page appear old page 
consequence scheme nodes crawl urls receive peers day urls seen 
describe detail section 
daily crawls aspects design suggest system crawl web especially url seen test discards pages downloaded 
order cause herodotus download pages daily basis url seen test reset day 
addition improvements link distribution described previous section new day starts initialize queue urls seen 
goals achieved easily considering way queue represented disk 
special pointer file points currently active portion queue disk 
simply reset pointer queue element reinitialize queue way herodotus restarts urls queue constitute active queue registered url seen test avoid multiple downloads url 
user interface parts design described far crawl archive web reliable fault tolerant fashion way users interested archived versions urls access view 
section describes design enables users interactively view contents archive 
herodotus node listens designated port port chord service plus constant offset 
port accepts connections 
opening root page herodotus node user start page url submission form 
user submits form herodotus node provided html page process request 
entered url turns reside herodotus node node determines ip address port node responsible url chord lookup sends response moved permanently cause web server query correct herodotus node requested object 
herodotus server responsible node find entries match url disk file storage return html page listing dates changes page recorded 
entries hyperlinks allow user request respective versions url 
archived version page requested embedded objects automatically rewritten represent correct herodotus urls corresponding archived version 
contain timestamp 
links html page rewritten new links date html page version requested 
user follows link manually clicking automatically case embedded graphics corresponding herodotus node return newest object archive newer specified date 
chapter implementation chapter discuss implementation herodotus 
give overview implementation 
conclude giving current status implementation 
overview herodotus implemented lines code counting chord implementation linked library 
program integrated location service daemon lsd cfs 
running additional service top chord just dhash 
depicts relationship 
dhash chord lsd integration herodotus service lsd 
herodotus chord lookups link distribution transactions communicated udp rpc package provided sfs toolkit 
chord lookups link distribution node sends short messages different nodes short period time udp better suited tcp overhead connection setup 
replication occurs adjacent peers chord ring sessions involve potentially large amounts data exchanged replication occurs tcp connections 
downloads origin servers user interface transactions connections done tcp 
herodotus maintains maximum simultaneous tcp connections origin servers 
number concurrent connections raised memory required store partially completed downloads significant 
reason decided limit number concurrent downloads 
internally program uses asynchronous callbacks separate threads 
herodotus tested freebsd platform chord cfs designed 
status herodotus currently fairly stable crawl web objects mit domain set machines hours problems 
crash recovery replication services user interface tested small scale 
little information herodotus behaves larger settings 
herodotus designed scale tens thousands peers resources time run herodotus larger scale settings 
chord simulations useful 
simulations hundreds thousands nodes typically run single machine 
settings node see typical traffic volume running dedicated machine due hardware limitations high number concurrent nodes machine behavior internode communication patterns different 
herodotus works mit experiments prototype stage 
deployed actual production system run years herodotus robust 
addition system administrators allowed configure parameters manner comfortable changing source code 
chapter analysis chapter analyze feasible deploy actual system design implementation described previous chapters 
order proceed follows 
relate numbers obtained chapter properties herodotus determine number participating peer peer nodes necessary bandwidth storage uptime requirements 
analysis shows substantial number nodes necessary light version herodotus deployed smaller number nodes 
describe nodes recruited system herodotus particular take look suitable gnutella nodes running herodotus 
deploying herodotus section relate numbers chapter characteristics herodotus 
estimate required minimum number herodotus nodes total storage capacity assumed average contribution storage space node 
lower bound number nodes calculate estimates bandwidth requirements node 
address requirements uptime node 
final part summarize requirements derived 
number nodes chapter shows required storage capacity tb crawl plus tb month changes 
months operation expect store tb data 
web content replicated successors node fault tolerance total storage volume comes tb 
assuming machines contribute gb virtual node run expect need nodes 
want conservative risk running disk space set requirement nodes months 
hard predict growth numbers chapter change months 
addition expect participating nodes upgrade available hard disk bandwidth moore law doubling months 
assume initial set nodes sufficient years 
statistics obtained operation herodotus give meaningful numbers indicate additional machines required 
bandwidth requirements set nodes replicated copies object fault tolerance node responsible processing daily crawl volume 
section established total number web pages images daily volume data changed tb uncompressed 
node issue conditional gets expected download mb uncompressed new web content day 
mit experiments show urls average length characters 
conditional get uses bytes network traffic total 
bytes upstream downstream 
means conditional gets node combined amount mb upstream mb downstream 
total bandwidth daily downloads amounts roughly mb upstream mb downstream 
numbers correspond kbit upstream kbit downstream amortized hours 
addition communication nodes web servers account communication nodes exchanging links achieve replication 
chapter shows average web page contains embedded web objects 
addition average url characters long 
ring forwarding scheme ensures aggregate bandwidth cost shipping url slightly length url 
shipping urls contained downloaded html documents nodes replication responsible amounts total bytes 
roughly equal size html page 
table shows daily download volume corresponds updated html added html new images 
design section explained updated html ship new links appear old version 
mit data shows links updated html new 
consequently ship full set links downloads 
cost sending links kbit rate receive downloaded content kbit node sends receives links corresponding downstream bandwidth kbit bandwidth requirement crawling downloads link exchanges roughly kbit upstream kbit downstream 
component bandwidth usage data sent nodes replication 
assume node stores gb 
new node joins network needs download gb neighboring nodes 
done period time days downstream bandwidth amounts kbit day period 
data provided successor predecessor means provide roughly kbit upstream 
assume nodes stay long node deal join permanent leave days total bandwidth requirement roughly kbit upstream kbit downstream 
nodes total downstream bandwidth herodotus gbit estimated wayback machine needs mbit shows herodotus total bandwidth times large centralized system 
reason higher total bandwidth usage replication overhead exchange urls 
need download object times 
addition conservative assumptions average participation times nodes result immense amount network traffic maintain fault tolerance 
uptime requirements previous section shown replication nodes join network fail permanently costly 
established nodes deal join permanent leave day period cost replication grow reasonable bounds 
replication adjacent nodes provide data set nodes want permanent failure new join needs restored day period 
assume nodes join permanently leave network rates maintain network size nodes nodes stay period year average 
node leaves permanently replaced new node replication occur places location ring old node disappeared location new node joined 
node leaves content needs store replicated day period probability nodes certain piece information fail days 
degree replication uptime requirements nodes minimal fraction archived data gets lost 
permanently leaving ring nodes temporarily reboot 
want url available probability nodes time uptime probability nodes keep copies url failing 
main reason replication 
nodes long need replication bring date rises aggregate bandwidth usage due long downtime 
summary requirements subsection summarize requirements established previous subsections 
assumed virtual node contribute gb disk space 
bandwidth requirements virtual node kbit upstream kbit downstream 
node participating days average time period uptime 
bandwidth requirements typical home broadband internet connections 
corporations universities typically huge multiples bandwidths 
perfectly reasonable individuals broadband connection idle time contribute resources project 
universities research institutions corporations higher available bandwidths provide larger number nodes 
analysis shows limiting factor storage capacity bandwidth 
gb significant amount storage volunteer contribute bandwidth rates commonplace today 
herodotus light previous section shown nodes required herodotus deployment feasible 
recruiting huge number machines challenging 
order get herodotus running smaller scale examine section requirements decide store html images 
contribution gb storage space node identified previous section bandwidth really issue 
doesn change limit download html data images accounted daily download volume 
cost having ship links download relatively html increases 
bandwidth requirements replication identical 
decide store html need tb initial download addi tional tb month 
month period amounts total tb 
replication level required storage capacity tb 
storage capacity gb node means need order nodes 
number lower nodes high 
hand ensure nodes recruit reliable assumed previous section replication level sufficient 
case nodes job 
especially controlled environments universities probably assume higher average participation times 
context recommendation nodes realistic 
recruiting nodes section discuss nodes recruited participating herodotus system 
previous sections shown node contribute significant amount disk space order gb 
criteria satisfied node contribute little herodotus effort 
addition identified bandwidth requirements moderate today broadband lan internet connections 
crucial criteria uptime average time node contributes herodotus 
analysis clear nodes participate months cause nodes achieve replication contribute herodotus 
recruiting nodes important idea availability characteristics new nodes 
possibility includes really new nodes production herodotus system just monitoring uptime period time 
nodes pass screening may part herodotus 
set nodes deployed controlled environments universities easily meet criteria want look suitable gnutella nodes herodotus 
gnutella dat percentile rank percent uptime percentile rank gnutella nodes different uptime levels saroiu conducted extensive measurement study peer peer systems gnutella napster 
find set gnutella nodes roughly disappeared hours 
perform long term tracking 
addition nature gnutella changed dramatically late 
limewire numerous attempts gnutella stable scalable ultrapeers better routing schemes aggressive propagation queries 
caused number music file sharing systems morpheus gnutella proprietary protocols 
order obtain current numbers uptime nodes longer time periods connected gnutella network gathered ip addresses nodes connected gnutella network time represents subset nodes online point time 
ip addresses obtained connecting small number nodes known issuing ping messages gathering replies 
continued iteratively connect nodes asking nodes knew forth unique ips ports 
attempted open gnutella connections ips hour course day period 
shows percent uptime plotted percentile rank gnutella nodes fall category 
graph shows gnutella nodes uptime level greater 
uptime levels average gnutella nodes small subset satisfy requirements imposed previous section 
gnutella dat percentile rank uptime days percentile rank gnutella nodes uptime level time days shows percentile nodes satisfying uptime level changed course days 
hard predict graph look time ranges weeks months 
may th limewire reported estimated total size gnutella network nodes accepted incoming connections 
hosts accepting incoming connections typically low bandwidth clients dial connections download files really consider nodes accepting connections reliable peers satisfy bandwidth requirements 
assume nodes satisfy requirements mean gnutella peers day 
assumptions unclear 
nodes expected uptime weeks data long term predictions availability past period 
issue gnutella uptime data motivation users 
gnutella goal node achieve high uptime 
important large number nodes point time 
people really care gnutella client running 
herodotus people told keep herodotus running time 
similar seti home tells members system pc idle 
seti home uses online rankings busiest members incentive people contribute resources possible 
addition seti home website shows large portion people signed service year ago active members 
herodotus create similar level user awareness importance high uptime levels enhance quality participating nodes 
chapter thesis design implementation herodotus distributed peer peer web archival system 
empirical data crawls mit domain gauge characteristics task archiving entire html image contents world wide web 
characteristics derived set requirements 
main section described design adequately addresses issues involved peer peer archival system 
implemented herodotus tested small scale archiving mit web pages days set machines 
analyzed herodotus archive entire web roughly machines needed initially 
defined storage bandwidth requirements participating node 
number peers high come html solution assumes robust peers 
scenario peers suffice 
due political administrative effort involved recruiting large number nodes deploy herodotus large scale production system 
provided meaningful insights understanding complexity archiving web designed scalable solution chord 
refine design 
particular improve way content stored diffs store slight differences versions web site efficiently 
current implementation robust proof concept needs improved ready large deployment 
recruiting peer peer nodes deploying running herodotus constitutes interesting challenge 
provide interesting data show design scales anticipate adequately solves difficult problem archiving web distributed fashion 
bibliography gnutella 
gnutella wego com 
google 
www google com 
internet archive 
wayback machine 
www archive org 
limewire extensions gnutella 
www limewire com index jsp tech papers 
limewire statistics 
www limewire com index size 
seti home 
ssl berkeley edu 
krishna bharat bay wei chang monika henzinger matthias ruhl 
links mining linkage web sites 
ieee international conference data mining icdm san jose california november 
sergey brin lawrence page 
anatomy large scale hypertextual web search engine 
computer networks isdn systems 
frank dabek frans kaashoek david karger robert morris ion stoica 
wide area cooperative storage cfs 
proceedings th acm symposium operating systems principles sosp chateau lake louise banff canada october 
fred douglis anja feldmann balachander krishnamurthy jeffrey mogul 
rate change metrics live study world wide web 
usenix symposium internet technologies systems 
brian george 
dynamic web 
estimating information highway speed limit 
allan heydon marc najork 
mercator scalable extensible web crawler 
world wide web 
david karger eric lehman frank thomson leighton rina panigrahy matthew levine daniel lewin 
consistent hashing random trees distributed caching protocols relieving hot spots world wide web 
acm symposium theory computing pages may 
david mazi res 
toolkit user level file systems 
proc 
usenix technical conference pages june 
stefan saroiu krishna gummadi steven gribble 
measurement study peer peer file sharing systems 
proceedings multimedia computing networking mmcn san jose ca usa january 
ion stoica robert morris david karger frans kaashoek hari balakrishnan 
chord scalable peer peer lookup service internet applications 
proceedings acm sigcomm conference san diego california august 

