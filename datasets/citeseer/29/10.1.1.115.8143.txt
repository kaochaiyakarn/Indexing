concept indexing fast dimensionality reduction algorithm applications document retrieval categorization technical report department computer science engineering university minnesota eecs building union street se minneapolis mn usa tr concept indexing fast dimensionality reduction algorithm applications document retrieval categorization george karypis sam han march concept indexing fast dimensionality reduction algorithm applications document retrieval categorization george karypis hong sam han university minnesota department computer science army research center minneapolis mn technical report karypis han cs umn edu updated march am years seen tremendous growth volume text documents available internet digital libraries news sources wide intranets 
led increased interest developing meth ods efficiently categorize retrieve relevant information 
retrieval techniques dimensionality reduction latent semantic indexing lsi shown improve quality information retrieved capturing latent meaning words documents 
unfortunately high computa tional requirements lsi inability compute effective dimensionality reduction supervised setting limits applicability 
fast dimensionality reduction algorithm called concept indexing ci equally effective unsupervised supervised dimensionality reduction 
ci computes dimensional representation collection documents clustering documents groups cen vectors clusters derive axes reduced dimensional space 
problems arise fact ideas document related concepts described words description 
effective retrieval methods match concept query concepts documents 
allow retrieval documents part desired concept contain query terms prevent documents belonging unrelated concepts retrieved contain query terms 
concept centric nature documents reasons problem document categorization assigning document pre determined class topic particularly challenging 
years variety document categorization algorithms developed ma chine learning information retrieval ir community :10.1.1.42.7488:10.1.1.104.1209
surprising result research naive bayesian relatively simple classification algorithm performs document categorization compared algorithms capable learning substantially complex models :10.1.1.46.1529
robust performance attributed fact naive bayesian able model un concepts various classes summarizing characteristics class probabilistic framework exploit concept centric nature documents 
techniques dimensionality reduction explored capturing concepts collection 
main idea techniques map document query test document lower dimensional space explicitly takes account dependencies terms 
effective retrieval methods match concept query concepts documents 
allow retrieval documents part desired concept contain query terms prevent documents belonging unrelated concepts retrieved contain query terms 
concept centric nature documents reasons problem document categorization assigning document pre determined class topic particularly challenging 
years variety document categorization algorithms developed ma chine learning information retrieval ir community :10.1.1.42.7488:10.1.1.104.1209
surprising result research naive bayesian relatively simple classification algorithm performs document categorization compared algorithms capable learning substantially complex models :10.1.1.46.1529
robust performance attributed fact naive bayesian able model un concepts various classes summarizing characteristics class probabilistic framework exploit concept centric nature documents 
techniques dimensionality reduction explored capturing concepts collection 
main idea techniques map document query test document lower dimensional space explicitly takes account dependencies terms 
associations lower dimensional representation improve retrieval categorization perfor mance 
associations lower dimensional representation improve retrieval categorization perfor mance 
various dimensionality reduction techniques classified supervised unsupervised 
su dimensionality reduction refers set techniques take advantage class membership information computing lower dimensional space 
techniques primarily document classification improving retrieval performance pre categorized document collections 
examples techniques include variety feature selection schemes reduce dimensionality selecting subset original features techniques create new features clustering terms :10.1.1.51.6297:10.1.1.15.4629:10.1.1.32.9956
hand unsupervised dimensionality reduction refers set techniques compute lower dimensional space class membership information 
techniques primarily improving retrieval performance lesser extent document categorization 
examples techniques include principal com ponent analysis pca latent semantic indexing lsi kohonen self organizing map sofm multi dimensional scaling mds :10.1.1.117.3373
context document data sets lsi probably widely techniques experiments shown significantly improves retrieval performance wide variety document collections 
techniques primarily document classification improving retrieval performance pre categorized document collections 
examples techniques include variety feature selection schemes reduce dimensionality selecting subset original features techniques create new features clustering terms :10.1.1.51.6297:10.1.1.15.4629:10.1.1.32.9956
hand unsupervised dimensionality reduction refers set techniques compute lower dimensional space class membership information 
techniques primarily improving retrieval performance lesser extent document categorization 
examples techniques include principal com ponent analysis pca latent semantic indexing lsi kohonen self organizing map sofm multi dimensional scaling mds :10.1.1.117.3373
context document data sets lsi probably widely techniques experiments shown significantly improves retrieval performance wide variety document collections 
new fast dimensionality reduction algorithm called concept indexing ci supervised unsupervised dimensionality reduction 
key idea dimensionality reduction scheme express document function various concepts collection 
achieved finding groups similar documents group potentially representing different concept collection groups derive axes reduced dimensional space 
leading eigenvectors correspond linear combinations original variables account largest amount term variability 
disadvantage pca high memory computational requirements 
requires memory dense covariance matrix km finding leading eigenvectors 
requirements unacceptably high document data sets number terms tens thousands 
latent semantic indexing lsi dimensionality reduction technique extensively information retrieval domain similar nature pca :10.1.1.117.3373
lsi finding truncated singular value decomposition covariance matrix finds truncated singular value decomposition original document term matrix uses singular eigenvectors axes lower dimensional space 
lsi require calculation covariance matrix smaller memory cpu requirements 
experiments shown lsi substantially improves retrieval performance wide range data sets 
reason lsi robust performance understood currently active area research :10.1.1.1.4458:10.1.1.184.4759
latent semantic indexing lsi dimensionality reduction technique extensively information retrieval domain similar nature pca :10.1.1.117.3373
lsi finding truncated singular value decomposition covariance matrix finds truncated singular value decomposition original document term matrix uses singular eigenvectors axes lower dimensional space 
lsi require calculation covariance matrix smaller memory cpu requirements 
experiments shown lsi substantially improves retrieval performance wide range data sets 
reason lsi robust performance understood currently active area research :10.1.1.1.4458:10.1.1.184.4759
techniques include kohonen self organizing feature map sofm data set principal direction data set problem pca lsi classification data sets 
multidimensional scaling mds 
sofm scheme neural networks projects high dimensional input data feature map smaller dimension proximity relationships input data preserved 
mds transforms original data smaller dimensional space trying preserve rank ordering distances data points 
data sets principle direction computed lsi pca direction variance 
projection data set principal direction lead worst possible classification projection second data set lead perfect classification 
limitation techniques super data characteristic variables describe smaller classes tend lost result dimensionality reduction 
classification accuracy smaller classes bad reduced dimensional space 
general supervised dimensionality reduction performed various feature selection techniques :10.1.1.51.6297:10.1.1.15.4629:10.1.1.32.9956
techniques broadly classified groups commonly referred filter wrapper approaches 
filter approaches different features ranked variety criteria highest ranked features kept 
variety techniques developed ranking features words collection including document frequency number documents word occurs mutual information statistics :10.1.1.21.7950:10.1.1.32.9956:10.1.1.46.1529
main disadvantage filter approaches features selected independent actual classification algorithm 
classification accuracy smaller classes bad reduced dimensional space 
general supervised dimensionality reduction performed various feature selection techniques :10.1.1.51.6297:10.1.1.15.4629:10.1.1.32.9956
techniques broadly classified groups commonly referred filter wrapper approaches 
filter approaches different features ranked variety criteria highest ranked features kept 
variety techniques developed ranking features words collection including document frequency number documents word occurs mutual information statistics :10.1.1.21.7950:10.1.1.32.9956:10.1.1.46.1529
main disadvantage filter approaches features selected independent actual classification algorithm 
consequently criteria ranking measure effectiveness feature classification task criteria may optimal classification algorithm 
limitation approach criteria measure effectiveness feature independent features features effective classification conjunction features selected 
contrast filter approaches wrapper schemes find subset features classification algorithm black box 
document vectors unit length formula simplified cos di di set documents corresponding vector representations define centroid vector vector obtained averaging weights various terms document set refer supporting set centroid analogously individual documents similarity document centroid vector computed cosine measure follows cos 
note document vectors length centroid vectors necessarily unit length 
intuitively document centroid similarity function tries measure similarity document documents belonging supporting set centroid 
careful analysis equation reveals similarity captures number interesting characteristics 
particular similarity ratio dot product divided length supporting set easily shown cos di cos di :10.1.1.34.6746
dot product average similarity documents length centroid vector square root average pairwise similarity documents including self similarity 
note documents scaled unit length 
equation measures similarity document centroid set average similarity document documents amplified function depends average pairwise similarity documents average pairwise similarity small amplification high average pairwise similarity high amplification small 
important features amplification parameter captures degree dependency terms 
discussed section supervised dimensionality reduction particularly useful improve retrieval formance pre categorized document collection improve accuracy document classification algorithms 
experiments section show performance traditional classification algorithms nearest neighbor improves dramatically reduced space ci 
analysis discussion order understand dimensionality reduction scheme necessary understand things 
need understand encapsulated centroid vectors second need understand meaning reduced dimensional representation document 
rest discussion assume clustering algorithm returns reasonably clusters set documents :10.1.1.122.7439:10.1.1.34.6746
mean clusters tends contain similar documents documents belonging different clusters similar belonging cluster 
set documents centroid vector provides mechanism summarize content 
particular prominent dimensions vector terms highest weights correspond terms important set 
examples centroid vectors different collections documents shown table collections described section 
centroid described relative small number keyword terms 
direct consequence fact supporting sets centroid correspond clusters similar documents just random subsets documents 
second terms quite effective providing summary topics discussed documents weights provide indication central topics 
example looking centroids re see cluster contains documents talk export products second cluster contains energy related documents third cluster contains documents related coffee production 
feature centroid vectors successfully past build accurate summaries improve performance clustering algorithms :10.1.1.164.606:10.1.1.34.6746
third prevalent terms various centroids contain terms act synonyms context topic describe 
easily seen clusters new 
example terms russian russia centroid terms vw second centroid terms drug nineteenth centroid 
note terms may necessarily single document terms easily appear centroid vectors interchangeably describe underlying topic 
note documents close original space tend close reduced space match different concepts degree 
centroids capture latent associations terms describing concept documents similar somewhat different terms close reduced space may close original space improving retrieval relevant information 
similarly documents close original space due polysemous words apart reduced dimensional space eliminating incorrect retrievals 
fact experiments section show ci able improve retrieval performance compared achieved original space 
finding clusters years variety document clustering algorithms developed varying time quality trade offs :10.1.1.34.6746
partitional document clustering algorithms gained wide spread acceptance provide reasonably clusters near linear time complexity 
reason clustering algorithm ci derived general class partitional algorithms 
partitional clustering algorithms compute way clustering set documents directly recursive 
direct way clustering computed follows 
refinement process terminates predetermined small number iterations iteration document moved clusters 
way partitioning recursive obtained recursively applying algorithm compute way clusterings bisections 
initially documents partitioned clusters clusters selected 
process continues times leading clusters 
number different schemes developed selecting initial set seed documents :10.1.1.44.4060:10.1.1.34.6746
commonly scheme select seeds random 
schemes small number different sets non reader fda responsible regulating food products prescription drugs 
random seeds selected clustering solution computed sets best solutions selected final clustering 
quality partitional clusterings evaluated computing similarity document centroid vector cluster belongs 
divided labels sets constructed data sets accordingly 
data set selected documents single label 
data sets oh oh oh oh ohsumed collection subset medline database contains documents indexed unique categories 
took different subsets categories construct data sets 
data set wap webace project wap :10.1.1.15.4629:10.1.1.122.7439:10.1.1.30.5135:10.1.1.39.8061
document corresponds web page listed subject hierarchy yahoo 

data sets list remove common words words stemmed porter suffix stripping algorithm 
unsupervised dimensionality reduction goals dimensionality reduction techniques ci lsi project documents collection low dimensional space similar documents documents part topic come closer relative documents belonging different topics 
re re wap fbis new size ci size ci size ci size ci size ci la la size ci size ci size ci table class ri measures various data sets supervised dimensionality reduction 
dimensional space supervised setting significantly consistently outperforms classification results obtained lower dimensional spaces obtained lsi 
included results knn feature selection techniques due inconsistent perfor mance schemes data sets 
particular right number dimensions different data sets varies considerably 
detailed experiments showing characteristics feature selection schemes text categorization readers advised see :10.1.1.32.9956:10.1.1.104.1209
directions new fast dimensionality reduction technique called concept indexing equally reducing dimensions supervised unsupervised setting 
ci reduces dimensionality document collection concepts collection expresses document function various concepts 
analysis shown lower dimensional representation computed ci capable capturing actual latent information available document collections 
particular lsi reduced space original space ci reduced space nn nn nn nb west west west oh oh oh oh re re tr tr tr tr tr tr la la fbis wap new table classification accuracy original reduced dimensional data sets 
area currently investigating develop robust clustering algorithms compute way clustering directly recursive 
techniques hold promise improving quality lower dimensional representation especially small classes reducing low computational requirements ci 
second supervised dimensionality reductions computed ci improved techniques adjust importance different features supervised setting 
variety techniques developed context nearest neighbor classification scale various dimensions prior dimensionality reduction computing centroid vectors scale reduced dimensions final classification 
aggarwal stephen gates philip yu :10.1.1.164.606
merits building categorization systems supervised clustering 
proc 
fifth acm sigkdd int conference knowledge discovery data mining pages 
almuallim dietterich 
sigir 
berry brien krishna 
version user guide 
www netlib org index html 
berry dumais brien :10.1.1.117.3373
linear algebra intelligent information retrieval 
siam review 
boley gini gross han hastings karypis kumar mobasher moore 
document query generation world wide web webace 
