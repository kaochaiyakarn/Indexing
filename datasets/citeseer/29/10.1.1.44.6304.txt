journal artificial intelligence research submitted published system induction oblique decision trees murthy murthy cs jhu edu simon kasif kasif cs jhu edu steven salzberg salzberg cs jhu edu department computer science johns hopkins university baltimore md usa article describes new system induction oblique decision trees 
system oc combines deterministic hill climbing forms randomization find oblique split form hyperplane node decision tree 
oblique decision tree methods tuned especially domains attributes numeric adapted symbolic mixed symbolic numeric attributes 
extensive empirical studies real artificial data analyze oc ability construct oblique trees smaller accurate axis parallel counterparts 
examine benefits randomization construction oblique decision trees 

decision trees particularly useful tool context perform classification sequence simple easy understand tests semantics intuitively clear domain experts 
decision trees classification tasks 
breiman book classification regression trees cart quinlan id quinlan provided foundations large body research central techniques experimental machine learning 
variants decision tree dt algorithms introduced decade 
concentrated decision trees node checks value single attribute breiman friedman olshen stone quinlan :10.1.1.167.3624
quinlan initially proposed decision trees classification domains symbolic valued attributes extended numeric domains 
attributes numeric tests form attributes example constant 
class decision trees may called axis parallel tests node equivalent axis parallel hyperplanes attribute space 
example decision tree shows tree partitioning creates attribute space 
consider issue complexity selecting optimal oblique hyperplane single node tree 
domain training instances described real valued attributes delta gamma delta distinct dimensional oblique splits hyperplanes divide training instances uniquely nonoverlapping subsets 
upper bound derives observation subset size points define dimensional hyperplane hyperplane rotated slightly directions divide set points possible ways 
illustrates upper limits points dimensions 
axis parallel splits delta distinct possibilities axis parallel methods quinlan cart breiman exhaustively search best split node :10.1.1.34.6358
problem searching best oblique split difficult searching best axis parallel split 
fact problem np hard 
precisely heath proved problem np hard set labelled examples find hyperplane minimizes number misclassified examples hyperplane 
result implies method finding optimal oblique split exponential cost assuming np 
user specify bias input parameter oc 
murthy kasif salzberg details impurity measures oc attempts divide dimensional attribute space homogeneous regions regions contain examples just category 
goal adding new nodes tree split sample space minimize impurity training set 
algorithms measure goodness impurity difference goodness values maximized impurity minimized 
different measures impurity studied breiman quinlan mingers buntine niblett fayyad irani heath :10.1.1.167.3624
oc system designed large class impurity measures 
stated simply impurity measure uses counts examples belonging category sides split oc 
see murthy salzberg ways mapping kinds impurity measures class impurity measures 
user plug impurity measure fits description 
weiss kapouleas obtained accuracies data back propagation nn respectively 
housing costs boston 
data set available part uci ml repository describes housing values boston function continuous attributes binary attribute harrison 
category variable median value owner occupied homes continuous discretized category value 
uses data see quinlan :10.1.1.34.6358
diabetes diagnosis 
data catalogs presence absence diabetes pima indian females years older function numeric valued attributes 
original source data national institute diabetes diseases available uci repository 
smith 
significantly increasing computational cost algorithm 
randomization beneficial axis parallel tree methods 
note find optimal test respect impurity measure node tree complete tree may optimal known problem finding smallest tree np complete rivest 
axis parallel decision tree methods produce ideal decision trees 
quinlan suggested windowing algorithm way introducing randomization algorithm designed purpose quinlan :10.1.1.34.6358
windowing murthy kasif salzberg algorithm selects random subset training data builds tree 
believe randomization powerful tool context decision trees experiments just example exploited 
process conducting experiments quantify accurately effects different forms randomization 
clear ability produce oblique splits node capabilities decision tree algorithms especially regards domains numeric attributes 
