factored language models tutorial kirchhoff jeff bilmes kevin bilmes ee washington edu dept ee university washington seattle wa uw electrical engineering technical report number uweetr february department electrical engineering university washington box seattle washington phn fax url www ee washington edu factored language models tutorial kirchhoff jeff bilmes kevin bilmes ee washington edu dept ee university washington seattle wa university washington dept ee uweetr february factored language model flm flexible framework incorporating various information sources morphology part speech language modeling 
flms far successfully applied tasks speech recognition machine translation potential wide variety problems estimating probability tables sparse data 
tutorial serves comprehensive description flms related algorithms 
document flm functionalities implemented sri language modeling toolkit provide introductory walk flms actual dataset 
goal provide easy understand tutorial researchers interested applying flms problems 
overview tutorial describe factored language model section generalized backoff section complementary techniques attempt improve statistical estimation reduce parameter variance language models attempt better describe way language sequences words produced 
researchers algorithms flms may skip section describes flm programs file formats publicly available sri language modeling srilm toolkit 
section step step walkthrough flm examples real language modeling dataset 
may useful users flms 
section discusses problem automatically tuning flm parameters real datasets refers existing software 
may interest advanced users flms 
factored language models background standard language models case sentence viewed sequence words 
wt goal produce probability model sentence way denoted 
wt concisely matlab notation ranges words 
case chain rule probability factor joint distribution wt wt ht tutorial currently documents flm functionalities srilm version 
note core flm functionalities implemented sri language modeling toolkit additional extensions added 
update tutorial reflect new extensions 
srilm toolkit may downloaded www speech sri com projects srilm 
ht 
key goal produce set models wt ht wt current word ht multi word immediately preceding history leading word wt 
standard gram model assumed immediate past preceding words sufficient capture entire history current word 
particular conditional independence assumptions word strings gram model assumed wt wt 
trigram model example goal produce conditional probability representations form wt wt wt 
small estimation conditional probability table cpt wt wt wt quite difficult 
standard large vocabulary continuous speech recognition system example vocabulary size words words 
number entries required cpt gigabytes quite conservative estimate byte entry 
table far large obtain low variance estimates significantly training data typically available 
training data available storage large table prohibitive 
fortunately number solutions proposed utilized past 
class language models example words bundled classes order improve parameter estimation robustness 
words assumed conditionally independent words current word class 
assuming ct class word wt time class model represented follows wt wt wt ct ct ct wt ct wt ct ct ct ct wt ct ct ct ct wt get second equality assumed ct wt ct 
order simplify number additional assumptions 
exactly possible class word deterministic mapping lets say function words classes 
ct wt ct wt dirac delta function 
wt ct wt wt ct wt 

class contain word 
assumption fact class language models easier estimate 
assumptions class language model 
wt wt wt wt wt wt modeling attempts improve situation include particle language models maximum entropy language models mixture different ordered models backoff procedure briefly reviewed section :10.1.1.131.5458
surveyed :10.1.1.131.5458
factored language models section describe novel form model entitled factored language model flm 
flm introduced incorporating various morphological information arabic language modeling applicability general 
factored language model word seen collection bundle parallel factors wt 

factors word including morphological classes stems roots linguistic features correspond decompose word 
factor word probabilistic language model words decompositional factors 
clear decomposition useful languages highly inflected arabic factored form applied language 
data driven word classes simple word stems semantic features occur language factors 
factored representation quite general widely applicable 
set factors words selected goal flm produce statistical model individual factors uweetr gram formalism goal produce accurate models form form seen flm opens possibility modeling options addition permitted standard gram model words 
example chain rule probability represent term follows represents possible chain rule ordering factors 
number possible chain rule orderings number factor permutations 
approximately number possible options subsets variables right conditioning bar nk see flm represents huge family statistical language models approximately nk 
simplest cases flm includes standard grams standard class language models choose factors word class ensure word variable depends class class depends previous class 
flm 
main issues forming flm 
flm user design decisions 
choose appropriate set factor definitions 
done data driven techniques linguistic knowledge 

second find best statistical model factors mitigates statistical estimation difficulties standard language models describes statistical properties language way word predictions accurate true word strings highly probable highly probable word strings minimal word error 
mt mt mt st st st st wt wt wt wt example factored language model seen directed graphical model words wt morphological factors wt stems st shows dependencies variables time simplicity 
problem identifying best statistical model set factors described instance structure learning problem graphical models 
directed graphical model graph depicts probability model 
conditional probability depicted directed graph node graph random variable arrows point parents child 
give examples flms corresponding graphs 
examples factors word variable time wt words morphological class mt morph call words stem st example corresponds model wt st mt st mt wt wt mt wt wt shown 
example factored class language model word class variable represented factored form stem st morphological class mt ideal case word entirely determined stem morph model wt st mt low perplexity uweetr unity 
models st mt wt wt mt wt wt prediction uncertainty apparent 
forming model goal product perplexities respective models perplexity word trigram 
achievable case estimation st mt wt wt mt wt wt inherently easier wt wt wt 
total number stems morphs total number words 
event see model 
example useful add mt st additional parents mt reduce perplexity mt model 
theory addition parents reduce entropy term reduce perplexity 
hand difficulty estimation problem dimensionality corresponding cpt increases addition parents 
goal finding appropriate flm finding best compromise predictability reduces model entropy perplexity estimation error 
standard problem statistical inference bias variance traded 
mt mt mt st st st st wt wt wt wt example factored language model seen directed graphical model words wt morphological factors wt stems st shows child variable wt parents 
shows stem morph deterministic functions parents case corresponding word dashed lines 
example flm wt wt wt st st mt mt 
model similar standard word trigram previous stems morphs added parents 
typical stems morphs predicted deterministically word 
word wt stem st morphological class mt determined certainty model shown 
word variable wt shown child parents wt wt st st mt mt 
deterministic variables shown parents dashed lines 
interesting issues apparent examining model 
previous morph class stem variables deterministic functions parents previous words 
case useful additional parents 
additional information gleaned new parent deterministic function set parents exist 
example model deterministic function models exactly entropy 
case model benefit standard word trigram 
reason word trigrams occur training data word preceded corresponding stems morphs occur training data set 
model appropriate way detected preceding word history occur corresponding stem morph history occur 
benefit backoff algorithm just simply backing bigram unigram see section brief overview standard backoff algorithm 
second interesting issue arises 
considering just set parents word trigram wt wt considering backoff algorithm reasonably argued best parent drop cases predictable certainty small number different possibilities 
uweetr backing distant parent wt current word wt 
rests assumption distant parent predictive power ability reduce perplexity distant parent 
hand considering set parents word wt wt st st mt mt shown immediately obvious reasonable priori assumption determine parent dropped backing cases times 
circumstances best drop wt cases best drop wt parent 
words backoff order chosen general model 
section addresses issues introducing notion call generalized backoff 
generalized backoff flms section describe generalized backoff procedure initially developed factored language models mind turns generally applicable typical word language models forming arbitrary smoothed conditional probability tables arbitrary random variables 
briefly review standard backoff language models generalized backoff algorithms 
background backoff smoothing common methodology statistical language model idea backoff 
backoff insufficient data fully estimate high order conditional probability table attempting estimate entire table portion table estimated remainder constructed lower order model dropping variables right conditioning bar going trigram wt wt wt bigram wt wt 
higher order model estimated maximum likelihood setting probabilities simply equal ratio counts word string left probability lower order model 
probability mass essentially stolen away higher order model distributed lower order model way conditional probability table valid sums unity 
procedure applied recursively uniform distribution words 
general way presenting backoff strategy follows backoff trigram bigram illustrative purposes general idea applies gram gram 
goal distribution wt wt wt 
maximum likelihood estimates distribution pml wt wt wt wt wt wt wt wt wt wt wt equal number times count word string wt wt wt occurred training data 
see definition values wt wt pml wt wt leading valid probability mass function 
backoff language model higher order distribution count particular string words exceeds specified threshold 
particular trigram wt wt wt threshold set higher depending amount training data available 
procedure produce backoff model wt wt wt simply described follows wt wt wt dn wt wt wt pml wt wt wt wt wt wt wt wt wt wt seen maximum likelihood trigram distribution trigram count high application discount wt wt wt number generally value 
discount factor steals probability away trigram bigram distribution 
discount determines smoothing methodology determines higher order maximum likelihood model probability mass smoothed lower order models 
note uweetr form backoff discount dn wt wt wt represent possible smoothing methods including turing absolute discounting constant discounting natural discounting modified kneser ney smoothing particular see table gives smoothing method :10.1.1.131.5458
quantity wt wt sure entire distribution sums unity 
starting constraint wt wt wt wt simple obtain derivation wt wt wt wt dn wt wt pml wt wt wt wt wt wt wt dn wt wt pml wt wt wt wt wt mathematically equivalent second form preferred actual computations fewer trigrams count exceed threshold trigrams count exceed threshold implied form 
wt wt wt wt wt wt wt wt wt example backoff path taken gram language model words 
graph shows gram language model attempted wt wt wt wt long string wt wt wt wt occurred times language model training data 
case model backs trigram model wt wt wt string wt wt wt occurs sufficiently training data 
process recursively applied unigram language model wt 
word test data training data wt model backed uniform distribution 
additional details regarding backoff describe report see crucial issue typical backoff procedure backoff distribution wt wt wt distribution wt wt doing drop distant random variable wt set right conditioning bar 
graphical models terminology say drop distant parent wt child variable wt 
procedure described graph backoff graph shown 
graph shows path case gram case parents unigram parents 
step procedure parent distant time child wt removed statistical model 
backoff graphs backoff paths described factored language model distributions needed form jk tk values 
general case arbitrary 
notational simplicity point clearer re write model form 
fn 
jn tn general form conditional probability table cpt set random variables child variable parent variables fn note possible value variable possible vector value set parents uweetr mentioned backoff procedure random variables question words reasonable assume drop temporally distant word variable distant variable amount mutual information child remaining parents 
distant words best chance able predict high precision word 
applying backoff general cpt random variables words want flm immediately obvious variables dropped going higher order model lower order model 
possibility choose arbitrary order backoff order sub optimal 
left example general backoff graph cpt showing possible backoff paths moving model 
right example backoff graph subset paths top bottom allowed 
graph model allowed backoff 
means parents may dropped parent backoff models parent 
similarly model may backoff parent may dropped case 
notion specifying subset parents may dropped determine constrained set backoff paths srilm language model toolkit extensions described section 
assumption drop parent time see quite large number possible orders 
refer backoff path viewed path graph node corresponds particular statistical model 
paths depicted get define backoff graph shown left shows explicitly fact node backoff graph corresponds particular statistical model 
training data statistical model sufficient threshold training data option node backoff graph 
example node options corresponding dropping parents 
nodes options nodes option 
see number possible options choosing backoff path include 
choose fixed path top bottom reasonable shown 
example drop temporally distant parent moving levels backoff graph done typical word gram model 
choose particular path linguistic knowledge domain 
stems example dropped morphological classes believed morph classes possess information word stems 
possible choose backoff path data driven manner 
parent dropped possess information information theoretic sense child variable 
alternatively possible choose parent drop tradeoff statistical predictability statistical estimation 
example choose drop parent raise entropy child model particularly easy estimate current training data child model low statistical variance 
possible drop parent backoff step see section example 
possible add parents backoff step 
case considered report 
uweetr example general backoff graph cpt showing possible backoff paths moving model 
example individual nodes backoff graph show directed graphical model node 
instantiation child variable parents exists language model training corpus number times language model hit corresponding node backoff graph particular model provides probability 
node nodes attempted 

generalized child backoff 
case multiple child nodes backoff graph chosen run time described section 

generalized constrained child backoff 
case subset child nodes backoff graph chosen run time described section 
generalized backoff workshop novel form backoff developed implemented order produce backoff models flms cpt 
call methodology generalized backoff 
choosing fixed path moving higher lower level backoff graph shown possible choose multiple different paths dynamically run time 
general options 
case path chosen time run time depending particular sequence words factors need probability backoff path change 
words set factor instances particular backoff path different path 

possible multiple backoff paths simultaneously runtime 
means multiple paths backoff graph produce probability 
set multiple paths change depending particular factor sequence 
approach improve choosing fixed backoff path path chosen best suit factor instances 
procedure developed generalization standard backoff probabilities formed technique equation dn pml uweetr examples fixed backoff path backoff graphs 
backoff graph shows fixed path top level parents bottom level parents 
example shows separate language model generally cpt path fixed times 
wt wt wt wt left example corresponds standard backoff gram language model definitions shows factors words possible backoff schemes correspond dropping temporally distant word 
pml maximum likelihood distribution equal ratio counts seen training data userspecified threshold determine hit occurs current level backoff level 
function backoff distribution non negative function discuss 
function produced ensure entire distribution valid non negative integrates unity simply derived way similar standard equation dn pml noted denominator equation simplified sum form guarantee sum unity longer probability distribution guaranteed positive 
inability lead significant computational increases language models 
seen sections computational costs prohibitive today fast computers experiments run just portable laptop computer pentium ghz mb ram workshop 
srilm toolkit extensions described section attempted form certain functions possible 
choice functions different backoff strategies determined 
example typical backoff procedure chose 
factors corresponded words mapping factors words corresponds dropping temporally distant word typical 
general non negative function essentially backoff weight algorithm computing 
allowed negative standard algorithm producing longer works determination negative depend may 
event see generalization standard backoff quite broad assumption non negative functions 
number possible considered seen section implemented workshop 
include fixed backoff path standard backoff case backoff path fixed times described section 
notationally may described follows uweetr means exclusive choice taken choice decided user fixed time 
case includes standard backoff word gram allows arbitrary fixed single backoff paths specified see 
max counts case backoff probability model chosen number counts current gram random variable assignments 
specifically argmax fm fm slightly notationally cumbersome describe approach case parents giving definition 
case equations written argmax fj words set factor values backoff model chosen corresponds factor values maximum counts 
means different backoff path instance backoff path maximum counts 
approach tends prefer backoff models better non relative statistical estimation properties ignores statistical predictability resulting backoff models 
max normalized counts case absolute counts normalized counts determine backoff model 
fm fm argmax argmax pml fm fm fm fm approach chooses backoff model highest probability score maximum likelihood distribution 
approach favors models yield high predictability possibly expense statistical estimation quality 
max num words normalized counts case counts normalized number possible factor values set parent values computed training set 
set possible factor values parent context expressed set number possible words equal cardinality set 
standard notation express set cardinality obtain equations fm fm argmax fm fm note factors correspond words normalization equal number possible words history 
uweetr max backoff graph node probability approach choose backoff model gives factor parent context question maximum probability 
written argmax fm fm approach chooses backoff graph node supplies highest probability probability obtained backoff procedure 
max product cardinality normalized counts approach counts normalized product cardinalities underlying random variables 
case term cardinality random variable mean number possible values random variable may take training set 
set notation notate cardinality random variable cardinality random variable taken leads fm fm argmax fm fm max sum cardinality normalized counts case sum random variable cardinalities normalization giving fm fm argmax fm fm max sum log cardinality normalized counts case sum random variable natural log cardinalities normalization giving fm fm argmax ln ln fm ln fm functions implemented including minimum versions identical argmin argmax function 
specifically min counts min normalized counts min num words normalized counts min backoff graph node probability min product cardinality normalized counts min sum cardinality normalized counts min sum log cardinality normalized counts uweetr functions implemented take min max arguments 
include sum fm fm average arithmetic mean fm fm product geometric mean fm fm fm fm weighted mean weights specified user 
fm fm note examples correspond approaches multiple backoff paths form final probability score single backoff path factor value context described earlier section 
note sum respectively average product children backoff graph beneficial take sum respectively average product priori specified subset children 
seen subset functionality implemented extensions sri toolkit described section 
readers interested perplexity experiments details implementation may wish skip section 
flm programs sri language model toolkit significant code extensions sri language modeling toolkit srilm jeff bilmes andreas stolcke order support factored language models generalized backoff 
essentially sri toolkit extended graphical models syntax similar specify statistical model 
graph syntax specify child variable parents specific syntax specify possible node backoff graph set node options type smoothing set nodes possible backoff graph children 
allowed possibility create backoff graphs sort shown right 
sections serve provide complete documentation extensions srilm toolkit 
new programs fngram fngram count new programs added srilm program suite 
fngram fngram count 
programs analogous normal srilm programs ngram ngram count behave somewhat different ways 
program ngram count take factored language data file format described section produce count file optionally language model file 
options program described 
uweetr thing note options ngram count fngram count include options language model smoothing discounting language model level 
potentially different set options needs specified node backoff graph 
smoothing discounting options specified language model description file described section 
options fngram count program program takes text stream feature bundles estimates factored count files factored language models 
factor file str gives name flm description file describes flm see section 
debug int gives debugging level program 
sort sort ngrams written output file 
text str text file read containing language model training information 
read counts try read counts information counts file 
note counts file specified flm file section 
write counts write counts file 
note counts file specified flm file section 
write counts lm train write counts file lm training 
note effect kneser ney modified kneser ney smoothing effect smoothing method change internal counts 
means option non kneser ney counts written 
lm estimate write lm file 
just counts computed 
kn counts modified input counts modified kn smoothing don internally 
virtual sentence virtual start sentence context sentence 
flm description file describes model child variable set parent variables 
pn parent variables reside distance past relative parent variable 
say maximum number feature bundles time slots past equal 
child variable corresponds time slot greater possible obtain true parent variable value lm training 
near sentences time slots values parent variable 
normal behavior case assume virtual start sentence value parent values think sentence containing infinite number start sentence tokens extending past relative sentence 
example flm specified simple trigram model words contexts sentence 
typical behavior srilm case trigram order recover typical behavior option meaning add virtual start sentence tokens sentence 
note flm simulating word gram want get exactly smoothed language model probabilities standard srilm programs need include options virtual sentence sure gtmin options flm specified command line usual srilm programs 
keep symbol unk lm 
unk language model factor automatically contain special unk symbol part valid set values tag 
unk vocabulary probabilities typically instance symbol occur training data backoff procedure smoothing assuming flm options backoff 
unk language model occur training data zero probabilities returned case 
happens child equal unk value considered vocabulary instance special counter incremented corresponding statistics reported 
similar standard behavior srilm 
uweetr remove null lm 
normally special word null included special value factors flm 
means language model smoothing smooth give probability null token 
option says remove null special token flm encountered training data particular factor 
note flm simulating word gram want get exactly smoothed language model probabilities standard srilm programs need include options virtual sentence sure gtmin options flm specified command line usual srilm programs 
meta tag meta tag input count count information 
similar srilm programs 
map vocabulary lowercase 
similar srilm programs 
vocab vocab file 
read vocabulary specified vocab file 
means vocabulary closed vocabulary seen oov 
non event non event word 
ability specify word set special non event word 
non event word smoothed vocabulary won effect lm probabilities 
note working flms need specify corresponding tag word order get correct behavior 
example need specify non event word noun part speech factor tag 
non events don count part vocabulary size vocabulary contexts parent random variable values contain non events estimated included resulting language model 
non event vocabulary file 
specifies file contains list non event words need specify additional word non event word 
write vocab write vocab file 
says current vocabulary written file 
options fngram program 
program uses counts factored language models previously computed compute perplexity file re score best lists 
best list rescoring similar factor file build factored lm factors file 
fngram count 
debug debugging level lm 
fngram count 
skip gram contexts containing oovs 
unk vocabulary contains unk fngram count 
remove null lm 
fngram count 
map vocabulary lowercase 
fngram count 
ppl text file compute perplexity 
specifies text file sequence feature bundles perplexity computed 
note multiple flms flm file compute perplexity file simultaneously 
see section 
escape escape prefix pass data 
lines printed output 
seed seed randomization 
vocab vocab file 
fngram count 
non event non event word 
fngram count 
non event vocabulary file 
fngram count 
write lm re write lm file 
write lm lm files specified flm specification file 
uweetr write vocab write lm vocab file 
fngram count 
rescore hyp stream input file rescore 
similar rescore option program ngram ngram supports number different ways doing best rescoring fngram supports corresponding rescore option 
separate lm scores print separate lm scores best file 
flm file contain multiple flms option says separate scores flm item best file printed combining score 
useful stage combine scores doing language model weighting 
rescore rescoring lm weight 
weight apply language model scores doing rescoring 
rescore rescoring word transition weight 
weight apply word transition doing rescoring 
noise noise tag skip similar normal srilm programs 
noise vocab noise vocabulary skip list contained file 
similar normal srilm programs 
factored language model training data file format typically language data consists set words train language model 
flm word accompanied number factors 
stream words stream vectors specified factored language model seen model multiple streams data stream corresponding factor evolves time 
support representation language model files assumed stream feature bundles feature bundle separated colon character feature consists tag value pair 
tag string length toolkit automatically identify tag strings training data file corresponding string language model specification file described 
value string having exist training file correspond valid value particular tag 
tag value separated dash character 
language model training file may contain tag value pairs language model specification file extra tag value pairs simply ignored 
feature bundle may instance particular tag 
feature tag missing assumed special tag typically indicate value word 
happen time bundle 
tag value pair may missing feature bundle 
case assumed tag exists special value null indicates missing tag note behavior start sentence word different see 
value may specified explicitly feature bundle null 
nulls training file reduce file size explicitly stating implicit mechanism 
number examples training files brown dog ate bone example language model file consists string presumably words tagged 
assumed words 
sentence equivalent word tags explicitly brown dog ate bone wanted include part speech information separate tag string identify part speech specified follows note tag tag purely data driven word class need necessarily lexical tag linguistic part ofspeech tag 
name tag represent feature 
uweetr article brown adjective dog noun ate verb article bone noun order individual features bundle matter example identical article adjective brown noun dog verb ate article noun bone generally string tagged version call home arabic corpus 
note example shows feature bundle line formatting purposes document 
actual file entire sentence feature bundles line newline character separates sentence 
masc sg noun masc sg masc sg verb subj st plural klm masc sg crb prep cl pro nom rd plural hm act plural cwz ah ah ah ah ah il noun fem sg article klm verb subj nd masc sg bi il bi il bi il bi il bi il prep cl pro nom rd plural hm verb pres rd plural null null example shows sentence arabic provides words tag name tagged morphological class tag name stem tag name root word tag name word pattern tag name 
just normal word file srilm sentence begins special word token indicate start sentence indicate sentence 
purposes flm file giving start sentence tags means tags start sentence value 
note distinct standard missing tag behavior see missing tag filled values null 
note srilm expects words feature bundles format 
word fewer features factors word doesn morph factor dummy factor inserted 
factored language model file flm file consists specifications flm 
multiple flms specified file simultaneously trained perplexity computation sentence best scoring depending program called command line options see section 
section describes format options files 
flm file may contain comments consist lines character pair ignored 
flm file begins integer specifies number flm specifications follow 
flms remainder file ignored considered comment 
integer specifies number flms simultaneously flm programs 
flm specification consists specification child number parents set parent names count file name language model file name sequence nodes corresponding backoff gram set node options 
provide number simple examples uweetr simple unigram flm simple word unigram word unigram word gram count gz word gram lm gz kndiscount gtmin unigram words tag 
unigram zero parents 
uses count file named word gram count gz language model named word gram lm gz specifies additional backoff gram node specification follows line 
backoff gram node specification corresponds node parents number binary string saying parents 
number indicates set parents may dropped case ignored parents says kneser ney discounting minimum gtmin count unity backing case unigram 
simple trigram example shows flm standard word trigram number options 
normal trigram lm word gram count gz word gram lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin case child word variable tag parents word previous time slot word time slots past 
uses count file named word gram count gz language model file named word gram lm gz specifies options backoff gram nodes 
backoff graph node corresponds complete model meaning parents preceding words 
syntax means preceding word words ago 
time slot index negative positive indices shorten line lengths case need specify model parents coming past 
event backoff graph node corresponds normal trigram parents string 
string line set parents may dropped going backoff gram node nodes level 
case parent may dropped corresponds normal trigram language model parents dropped priority order distant temporal parent 
set node options node 
says kneser ney discounting minimum count case produce interpolated language model backoff graph node nodes 
line specifies node identifier case meaning model containing previous word 
string meaning parent may dropped go level set node options 
line gives unigram model parents specified number additional parents may dropped set node options 
trigram time reversed backoff path example shows flm word trigram parent variables dropped distant time order reverse order done typical trigram 
trigram time reversed backoff path word gram count gz word gram lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin hand specifying specify words 
uweetr seen backoff graph node case parents node says parent dropped case parent 
means node backoff graph corresponds model wt wt 
understood exactly counts produce model 
wt wt corresponds counts distance words essentially integrating possible words words 
words model counts wt wt wt wt wt wt produce maximum likelihood case smoothing model pml wt wt wt wt wt wt wt wt wt 
note model quite distinct case model wt wt word time slots ago substituted position previous word srilm skip gram 
notation distinction 
flm corresponds case wt wt wt wt backoff graph node probability 
skip gram hand utilize sake interpolation wt wt wt wt random variable wt taken value word wt time slots past 
case actual counts wt wt produce probability word value changed 
case entirely different count quantity wt wt 
note skip gram case parents correspond different types random variables parents words stems morphs 
example parent word stem substitute stem place word super class created union words stems 
super class effect smoothing particular kneser ney case table storage size 
flm allows different random variable types treated differently parent variable allows counts computed separately subset parents 
stem morph word context example child parent variables type words 
example corresponds probability model st mt wt wt st stem time mt morphological class time wt word time model specified follows stem morph word word count gz lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin kndiscount gtmin case stem time child variable parents morph current time word previous time word time slots ago 
produces uses count file named count gz language model file named lm gz specifies options backoff graph nodes 
node corresponds parent variables 
node back case parent dropped 
uses kneser ney smoothing minimum count unity interpolation 
node line case parents backoff graph node parent dropped 
node may drop parent similar additional node options 
lines specify backoff graph node options nodes case parents 
options clear point discussion 
note examples encountered far correspond fixed single backoff path root leaf node shown examples 
examples indicate multiple backoff paths 
trigram generalized backoff example shows trigram allows backoff path meaning backoff paths backoff graph traversed order obtain final probability 
case specify combining functions mentioned section 
example general backoff max trigram lm word gen gram max count gz word gen gram max lm gz kndiscount gtmin combine max strategy bog node prob interpolate uweetr kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin kn count parent case backoff graph nodes specified 
case parents case 
field says parents allowed drop descend level 
words field set parents may dropped implicitly defines set lower nodes backoff graph 
followed set node options kneser ney discounting minimum count combination strategy takes maximum max lower nodes backoff graph probabilities bog node prob backoff graph node probability 
option corresponds max backoff graph node probability case section 
remaining nodes backoff graph specified way examples 
dropping parent time skipping level flm backoff graph mechanism minimum counts gtmin possible produce flms skip entire level backoff graph altogether 
words case desirable drop parent time going say model backing directly 
provides example done 
word word morph stem dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin combine mean kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent case backoff graph node containing parents corresponding wt wt mt st says word variable may dropped moving node model wt mt st 
node large minimum count threshold gtmin 
means node language model hit scores backoff graph node combination lower level backoff graph nodes 
node says parents may dropped meaning models lower level backoff graph corresponding models wt mt wt st 
mean probabilities models produce score node specified string combine mean 
seen combination restricted set parents may dropped backoff graph node large gtmin count wide variety backoff graphs skip levels specified 
general description flm syntax examples gave taste language models specified syntax 
section describes syntax functionality general case 
models described section 
flm consists model specifier line text followed node specifier lines text 
model specifier line graphical model inspired syntax specifying statistical model tags exist set training data 
line takes form child num parents par par par cnt file lm file num bg nodes name child child random variable corresponds tags exist language model training data 
example tags specified callhome training file section child strings field number parent random variables model integer 
set fields parents form parent name time offset 
string parent name tags exist training data 
time offset value integer gives time slots past parent reside indicates previous factor indicates factor 
uweetr follows name count file keeping language model counts name language model file 
note count files language model file specified counts certain cases described section needed determine backoff probability 
field line num bg nodes number backoff graph node specifications 
number lines assumed part flm file ignored assumed part flm specified file 
careful ensure number actual backoff graph node specifiers equal number easy add node specifiers forgetting update field 
model specifier followed num bg nodes model specifiers 
model specifier consists form parent list drop list node options parent list comma separated list spaces parent variables correspond node 
words node backoff graph identified set parent variables string identifies node giving set 
parents specified shortened syntax parent name tags file absolute value time offset space 
examples parent lists include give preceding words give preceding word morph stem 
field drop list gives list parents may dropped time backoff graph node 
field specifies set lower level backoff graph nodes case language model hit level 
example parent list drop list lower level node parent list 
hand drop list parent list nodes lower level nodes parent lists 
node options option backoff graph node 
backoff graph node discounting smoothing combination options 
node options similar command line options program ngram count 
options included flm file different set node 
gives list node options currently supported 
gtmin num lower gt discounting cutoff 
turing discounting gives threshold count needs meet exceed order language model hit backoff graph node 
gtmin corresponds variables equations section 
num upper gt discounting cutoff 
note gtmin specified turing smoothing 
gt filename string turing discount parameter file node 
double says constant discounting gives discounting constant 
says natural discounting 
says witten bell discounting 
kndiscount says modified kneser ney discounting 
says unmodified original normal kneser ney discounting 
kn counts modified says input counts modified kn smoothing similar ngram count 
kn counts modify says counts turned kn counts discount estimation 
words option specified quantities typically modified kn discounting computed usual counts meta meta counts needed kneser ney smoothing 
uweetr kn filename string kneser ney discount parameter file backoff graph node 
kn count parent parent spec backoff graph parent compute meta needed kneser ney smoothing 
normally parent node immediately current node backoff graph useful specify different parent especially set parents correspond random variables different times parents truly different random variables 
interpolate interpolated estimates computing probabilities node 
words probabilities interpolated hits current nodes function returns lower node probabilities 
generalization interpolated option ngram count 
write filename string write counts just node file 
combine option option active multiple backoff paths backoff graph children possible drop list specifies parent variable 
see section 
case option max maximum backoff graph level nodes 
default combine option 
min maximum backoff graph level nodes sum maximum backoff graph level nodes avg mean arithmetic average mean backoff graph level nodes prod product backoff graph level nodes geometric mean backoff graph level nodes node node 
weighted mean backoff graph level nodes 
weights providing node specification list comma separated parents identify node weight node 
example means backoff node weight node weight 
strategy option option active multiple backoff paths backoff graph children possible combine option min max 
see section information 
case option counts sum counts norm max min normalized counts cases section 
default case strategy option 
counts norm max min counts cases section 
counts sum num words norm max min num words normalized counts cases section 
counts prod card norm max min product cardinality normalized counts cases section 
counts sum card norm max min sum cardinality normalized counts cases section 
counts sum log card norm max min sum log cardinality normalized counts cases section 
bog node prob max min backoff graph node probability cases section 
ways specify backoff graph nodes drop sets 
backoff graph node typically identified comma separated list parents 
similarly set parents may dropped specified list 
cases specifying lists tedious concise albeit error prone specify nodes sets numeric representation 
parent list may specified integer integer bit representation parents set 
line flm file form uweetr child num parents par par par cnt file lm file num bg nodes par corresponds bit par corresponds bit par corresponds bit element bit vector 
means left parent corresponds significant bit right parent significant bit 
reason string parent list right parent highest order parent 
bit vectors may specified numerically string form list parents 
numeric form may decimal integer hexadecimal integer preceded binary integer preceded 
example normal trigram specified follows word gram rev count gz word gram rev lm kndiscount gtmin kndiscount gtmin kndiscount gtmin construct equivalent word gram rev count gz word gram rev lm kndiscount gtmin kndiscount gtmin kndiscount gtmin combination decimal hexadecimal binary string form flm file valid 
word gram rev count gz word gram rev lm kndiscount gtmin kndiscount gtmin kndiscount gtmin way useful specifying large backoff graphs little sparsity 
example example shows flm generalize backoff parents backoff paths taken 
seen numeric representation specify parents greatly facilitates declaration model 
step step walk flms language modeling section number perplexity experiments flms generalized backoff callhome arabic corpus 
intended step step walk demonstrates various types flms real data 
callhome arabic corpus prepared factored text file words tag name morphological classes tag name stems tag name word roots tag name words patterns tag name 
details regarding data prepared see 
case design issues flm section solved factors defined 
necessary find statistical model factors minimizes perplexity say word error rate speech recognition system 
order report information regarding flm necessary report factors model structure factors generalized backoff method smoothing options node backoff graph 
information flm definition see section flm definitions provide information 
providing perplexity simple optimized unigram bigram trigram language model 
normal unigram lm logprob ppl ppl word gram count gz word gram lm gz kndiscount gtmin uweetr word word word morph morph stem stem model backoff paths 
gtmin set number bits set node 
model take days train typical machine 
count gz lm gz xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max xff gtmin kndiscount strategy bog node prob combine max large flm model parents backoff nodes utilized 
normal bigram lm logprob ppl ppl word gram count gz word gram lm gz kndiscount gtmin interpolate kndiscount gtmin trigram normal trigram lm logprob ppl ppl word gram count gz word gram lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin seen unigram achieves perplexity previous word parent uweetr results previous words achieves baseline section 
appears addition additional parent yield significant additional amount information 
produce results distance bigram context reversed trigram order verify assumption 
distance bigram lm logprob ppl ppl word gram count gz word gram lm gz kndiscount gtmin interpolate kndiscount gtmin distance bigram lm logprob ppl ppl word gram count gz word gram lm gz kndiscount gtmin interpolate gtmin context reversed trigram lm logprob ppl ppl word gram count gz word gram lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin note models utilize distance counts flm corresponds case probability model wt wt wt wt previous word entirely 
means counts table correspond quantity wt wt 
verbatim section shows models uni gram node uses kneser ney discounting case 
seen perplexities beat case unigram language model perplexity 
mean parent informative means additional information provides sufficient offset loss estimation quality 
third example shows context reversed trigram model wt wt wt backs wt wt wt 
example corresponds different backoff path backoff graph 
see bit better distance bigram choosing wrong backoff parent order case reverse time perplexity increase 
example shows crucial obtain language model set parent random variables balance predictability time decrease estimation accuracy 
predictability improved example choosing parents high degree mutual information child 
parent large number values estimation quality decrease amount training data value parent variable reduced 
balance course seen context language model backoff backoff attempt utilize high predictability context estimation quality high backoff lower order model essentially different sets parent values pooled order improve estimation quality 
results word depends number different factors time order determine factors yield information word 
word logprob ppl ppl count gz lm gz kndiscount gtmin interpolate gtmin word logprob ppl ppl count gz lm gz kndiscount gtmin interpolate gtmin uweetr word logprob ppl ppl count gz lm gz kndiscount gtmin interpolate gtmin word logprob ppl ppl count gz lm gz kndiscount gtmin interpolate gtmin seen perplexities quite low meaning factors word known time word quite informative 
see stem wt st predictive word perplexity 
followed root wt rt perplexity pattern wt pt perplexity morphological class wt mt perplexity 
note perplexities low decoding language model necessary obtain probability models corresponding parent 
example wt st model decoding necessary utilize model st parents 
case possible combine parents shows note case witten bell smoothing better 
word stem morph logprob ppl ppl count gz lm gz gtmin interpolate gtmin gtmin word stem morph logprob ppl ppl count gz lm gz gtmin interpolate gtmin gtmin word stem root logprob ppl ppl count gz lm gz gtmin interpolate gtmin gtmin word stem root logprob ppl ppl count gz lm gz gtmin interpolate gtmin gtmin seen best example uses parents stem morph parents correspond best individual parents previous example stem root 
example indicates best parents redundant respect 
example shows effect different backoff paths 
stem morph model chooses drop morph stem perplexity second stem morph example drops stem morph perplexity 
different backoff orders lead differences perplexity times quite dramatic 
uweetr investigate number different models current morph stem mt parents st parents parents come past see lead low perplexity combined models wt st mt 
set utilize previous words possibly corresponding factors 
morph word word logprob ppl ppl count gz lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin morph stem word word logprob ppl ppl count gz lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin kndiscount gtmin stem word word logprob ppl ppl count gz lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin stem morph word word logprob ppl ppl count gz lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin kndiscount gtmin model mt wt wt shows perplexity 
second model adds additional parent mt st wt wt reducing perplexity 
sound encouraging model model st wt wt perplexity 
models wt st mt yield total perplexity value higher baseline trigram perplexity 
alternatively mt wt wt model st mt wt wt model leading graph model shown 
apparently perplexity cost split word factors model factor separately possibly due fact multiple occurring 
note exactly form class language model precisely factored class language model word class mt st represented factored form class model 
provide perplexities word language models factors additional words parents 
number models category leading bigram model ultimately improve baseline 
word word word morph morph logprob ppl ppl count gz lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin interpolate uweetr kndiscount gtmin interpolate kndiscount gtmin word word word stem stem logprob ppl ppl count gz lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin word word word morph stem logprob ppl ppl count gz lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin word word word stem morph logprob ppl ppl count gz lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin note set parent node identifiers backoff selectors specified binary strings comma separated list parents see section 
cases additional parents increase perplexity relative baseline 
general question asked point 
include additional parents model parents deterministic function pre existing parents 
example stem deterministic function word means wt wt st wt wt current word conditionally independent previous stem previous word 
answer true probability distributions 
models estimated backoff method 
word context exist training data corresponding stems words occur context training exist training data words stems 
context words suppose context wt wt wt wt wt occur training data wt occur training data 
suppose wt wt corresponding stems st st 
case wt wt wt need backoff unigram wt wt wt st st backoff wt st st presumably providing better prediction wt unigram model 
argument seen standard bias variance tradeoff statistics add bias model form backoff order reduce variance estimate 
try exploit fact 
consider model 
logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin interpolate uweetr kndiscount gtmin interpolate kndiscount gtmin interpolate kn count parent kndiscount gtmin kndiscount gtmin kn count parent model seen improve baseline perplexity small amount 
model attempts utilize entire context stems morphs deterministic functions words context identical 
context exist training data count drop parents large min counts described section land node 
node attempted hit occurs previous word dropped moving unigram 
example improves follows 
logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin combine mean kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin interpolate kndiscount gtmin combine mean kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent example uses complete context hit occur drops parent 
node uses generalized backoff take mean lower nodes parents dropped 
nodes tried respectively drop meaning backoff model 
node attempted hit occur parent dropped 
node generalized backoff backing mean model unigram 
perplexity achieved 
trigram improves 
logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin combine mean kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin interpolate kndiscount gtmin combine max strategy bog node prob kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent example identical mean combination node max combination 
seen resulting perplexity achieved 
example improves bit simpler model 
best perplexity logprob ppl ppl count gz lm gz kndiscount gtmin interpolate kndiscount gtmin interpolate kndiscount gtmin combine max strategy bog node prob kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent uweetr case trigram add additional parents 
entire context attempted removing hits occur 
generalized backoff inspired finish unigram 
perplexity achieved best perplexity achieved data model 
expected possible produce significant additional perplexity reductions trigram models trigram depend farther past previous words deterministic functions previous words model search space large 
due time limitations week workshop decided investigate bigrams remainder time 
recall baseline bigram perplexity higher achieved certainly higher baseline trigram 
example bigram beats baseline trigram perplexity achieving 
conditioning previous morph class stem addition word model wt wt mt st uses generalized backoff methods bog node prob combining maximum 
attempts context consisting previous word equivalent deterministic functions 
language model hit level jumps directly context consisting 
jump node takes maximum probability contexts large gtmin parameter node contribute real probability just acts combine lower level backoff graph nodes 
lastly notice kn count parent specify original node form kneser ney meta counts default node 
general specifying kn count parent node included word variable beneficial effect perplexity model word model wt left conditioning bar wt parents 
logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin strategy bog node prob combine max kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent example shows simpler bigram improves baseline bigram matches baseline trigram 
case generalized backoff 
word context augmented morph variable 
bigram gets trigram 
logprob ppl ppl count gz lm gz kndiscount gtmin interpolate kndiscount gtmin kndiscount gtmin kn count parent examples show perplexity depend adjusting gtmin parameter 
case changing gtmin nodes parents lead perplexity increases decreases example shows 
logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin strategy bog node prob combine max kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate uweetr kndiscount gtmin strategy bog node prob combine max kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin strategy bog node prob combine max kndiscount gtmin kn count parent interpolate kndiscount gtmin kn count parent interpolate kndiscount gtmin kn count parent logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin strategy bog node prob combine max kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent example shows different combination method mean case possible retain perplexity improvement 
particularly relevant mean combination rule lead costly language model evaluate combination procedures 
logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin strategy bog node prob combine mean kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent example shows weighted mean possible achieve better perplexity 
case weight node node 
logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin strategy bog node prob combine kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent changing strategy counts prod card norm show appreciable change 
logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin strategy counts prod card norm combine max kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent changing strategy counts sum counts norm combining method leads slight increase perplexity 
uweetr logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin strategy counts sum counts norm combine kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin strategy counts sum counts norm combine mean kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin strategy counts sum counts norm combine mean kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent weighted mean counts sum counts norm improves things bit 
logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin strategy counts sum counts norm combine kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent mean produces slight modification 
logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin combine mean kndiscount gtmin kn count parent kndiscount gtmin kn count parent kndiscount gtmin kn count parent example shows best bigram achieving perplexity better baseline trigram close expensive generalized backoff trigram perplexity seen 
logprob ppl ppl dev count gz dev lm gz kndiscount gtmin interpolate kndiscount gtmin combine max strategy bog node prob kndiscount gtmin kn count parent uweetr kndiscount gtmin kn count parent kndiscount gtmin kn count parent model increases gtmin node yielding improvements 
seen space possible models quite large searching model space hand quite time consuming 
expect automatic data driven structure learning procedures produce models yield improvements models derived specified hand 
data driven search flm parameters order flm types parameters need specified initial conditioning factors backoff graph smoothing options 
initial conditioning factors specifies factors shall estimating gram probabilities design issue section 
backoff graph smoothing options indicate procedures robust estimation case insufficient data design issue 
goal data driven search find parameter combinations create flms achieve low perplexity unseen test data 
resulting model space extremely large factored word representation total factors possible subsets initial conditioning factors 
set conditioning factors 
backoff paths smoothing options 
small exhaustive search infeasible 
nonlinear interactions parameters difficult guide search particular direction 
instance particular backoff path may kneser ney smoothing slightly different path may find poor performance method 
structures ultimately dependent data parameter sets corpus necessarily expected perform 
developed efficient search algorithm called ga flm tuning flms 
genetic algorithms takes inputs factored training text factored development text attempts find flm parameters minimize perplexity development text 
code publicly available ee washington edu people research html 
recommend flm users ga procedure data driven search methods hand tuning flms time consuming 
provide brief description ga flm code 
information 
ga flm code overview general flow program illustrated 
initialize population genes 
gene represents flm specific conditioning factors backoff graph smoothing options 
gene print factor file factor file train test language model conventional fngram count fngram tools srilm 
perplexity user supplied development set determine fitness value gene 
fitness evaluation selection crossover mutation occurs population genes ready evaluated 
program runs convergence maximum generation specified user 
elitist selection implemented best gene generation spot generation 
refer details flm encoded gene 
ga flm general requirements installation sure sri language modeling toolkit version 
executables needed package fngram count fngram builds tests factored language models respectively 
ga flm tool downloaded ee washington edu people research html install simply download unzip 
type root directory 
create executable ga flm supporting scripts 
uweetr population strings flm flm flm flm string flm conversion flm training perplexity eval string selection perplexity crossover mutation new population program flow ga search flm structure 
ga flm running program executables running genetic algorithm search 
general perl wrapper version convenient allows repeated genetic algorithm runs prints summary files run 
running executable directly ga flm ga flm seed specifies filename stores ga parameters population max generation crossover selection type mutation crossover rate specifies path factor file data stored 
default file ga params 
tells ga flm factored language model specification factors available train dev set default file flm params 
include specialized options pmake program helps distribute parallel jobs compute cluster 
specific pmake program 
pmake set pmake option 
alternatively implement parallel execution function ga flm code 
default file pmake params 
seed file specifying number seed genes 
usually genetic algorithms randomly initialized population 
seed genes user specified genes directly inserted initial population 
file line represents gene see example seed example 
number seed genes seed file empty case genes randomly generated 
default file seed 
careful length gene seed total length gene expected program 
factors available total initial factor bits backoff bits 
final length length specified smoothing options 
information genetic algorithm factored language model contained parameter files 
general user find sufficient ga flm just modify parameter files flm params ga params pmake params 
running perl wrapper repeat pl perl repeat pl num seed uweetr repeat pl perl wrapper ga flm 
additions perl wrapper flexible 
user specify num call ga flm repeatedly num times 
useful user wants run genetic algorithm experiments 

perl wrapper produces summary file summarizes perplexity results entire genetic algorithm run 
easy read summary file convenient way acquire best factor files genetic algorithm run 
parameters simply passed ga flm 
default num omitted 
example runs ga flm examples illustrates ga flm find factored language model structures 
data conll shared task corpus 
included example data serial execution example try command perl repeat pl run ga flm twice 
options defaults ga params flm params pmake params seed read 
note pmake params specifies pmake current implementation implies serial execution evaluating language model fitness 
may bit slow set population generation ga params small ga flm take long 
ga params know results stored example serial 
go directory 
ll find bunch files flm files files factor file perplexity file specific gene respectively 
flm files generated transforming gene sequence factored language model 
obtained training factored language model train set testing dev set 
logfile files contain information population fitness generations 
summary file contains factor files genes entire genetic algorithm run corresponding perplexities 
file contains list genes descending order perplexity 
complexity files represents number parameters factor file bayesian info criteria bic fitness function 
fitness function specified files needed 
parallel pmake execution example genetic algorithms suited parallel processing 
building testing factored language model done independently parallel machines communication machines needed 
try command pmake parallel jobs distribution program available perl repeat pl example ga params parallel example pmake params parallel note changed ga params pmake params files 
specifically pmake params file tells ga flm parallel processing 
ga params file specifies results stored example parallel 
pmake implement interface parallel processing program exist computer system 
serial processing inherently slow recommended included package demonstration debugging purposes 
see section instructions 
uweetr ga flm parameters files parameter files read ga flm 
allow user specific particular ga options flm options search 
shall explain parameter file detail 
ga params file example ga params file population size maximum generations crossover mutation crossover type point selection type sus tournament selection fitness scaling constant fitness function bic bic constant path ga flm files example serial parameters quite useful general user 
divided groups 
genetic algorithm operators parameters standard parameters genetic algorithm 
usually find factored language models needs experiment different crossover selection mutation population sizes 
detail crossover type may point point uniform selection type may sus roulette tournament tournament selection number genes picked tournament selection fitness function parameters define fitness function fitness scaling constant linear scaling constant sure fitness values reasonable ranges 
high want high convergence threshold genetic algorithm stops quicker lower want fine tune generations 
fitness function currently bic different methods evaluate fitness language model 
bic float number 
bayesian info criteria weight complexity penalty 
ignored selected fitness function 
fitness function implemented fitness scaling constant 
bic fitness function implemented perplexity log uweetr fitness scaling constant weighting vs complexity number parameters factored language model evaluation 
total number gram types factored language model taken complexity file 
may wish implement fitness function 
hack part code ga flm cc 
add fitness function bic fitness functions 
quantities logprob complexity ppl perplexity available fitness computation 
fitness function may need input specified parameter files 
case add code read ga params file portion ga flm cc initialize void 
refer bic value implementation example 
path ga flm files path ga flm files ga pmake factor files stored flm params file example flm params file data path example data wsj train factored txt wsj dev factored txt fngram count path fngram count fngram path fngram fngram count options fngram options factor predict total available factors context factors smoothing options length discount options kndiscount default discount max cutoff max gtmin parameters relate factored language model training testing divided general sections parameters regarding data parameters regarding sri language modeling program parameters regarding factored language model specification data location data path directory path training dev sets 
filenames training dev set 
files factored assumed srilm fngram programs 
sentences composed factored bundles word pos chunk 
see example data examples 
sri language modeling program options path fngram path paths fngram count fngram files respectively 
options fngram options may contain command line options training testing language model 
options passed directly fngram count fngram 
example options unk options needed options quotes uweetr factored language model specifications factor predict factor wish predict language model 
usually word may factor 
total available factors context factors specify tags factors ga optimization 
ways specify parameters 
specify factors context desired total available factors context ll get factors 

specify factor tags directly specifying context total available factors context note case set context 
advantage method typing 
advantage method specify arbitrary factor combinations 
example specify possible method 
sure tags numbers context separated commas delimiter signals program tell tags apart 
smoothing options length number smoothing option parameters 
defines length smoothing options gene 
smoothing defined tuples discount gtmin number 
believe smoothing options important optimization gene long 
discount options usual fngram discounting options kndiscount kndiscount 
total available factors sure commas delimit discounting options 
number discounting options may 
default discount unigram cases backoff edges smooth number smoothing options specified smoothing options length 
max cutoff indicates max possible value gtmin 
gtmin min cutoff count ngram included language model estimation 
pmake params file example pmake params file pmake 
num pmake jobs scratch space example parallel export attributes linux ram ram pmake 
current implementation evaluate flms serially pmake 
enter pmake available 
enter serial adapt code parallel processing platform 
num pmake jobs max number pmake jobs option pmake scratch space place storing big language model files count files train dev set data 
pmake implementation files stored locally scratch space computer 
deleted fitness evaluation 
export attributes pmake export attributes 
linux ram ram 
uweetr may wish implement parallel execution code 
hack part code ga flm cc 
queue object containing list commands genes need evaluated generation 
commands independent executed parallel mutual communication 
long ensure commands executed ga flm cc exits ga flm fine 
refer serial execution example implementations 
implementing code sure modify pmake variable pmake params file tell ga flm execution mode desire 
berger della pietra della pietra 
maximum entropy approach natural language processing 
computational linguistics 
bilmes zweig 
graphical models toolkit open source software system speech timeseries processing 
proc 
ieee intl 
conf 
acoustics speech signal processing 
bilmes 
graphical models automatic speech recognition 
rosenfeld ostendorf khudanpur johnson editors mathematical foundations speech language processing 
springer verlag new york 
jeff bilmes kirchhoff 
factored language models generalized parallel backoff 
proceedings hlt naacl pages 
chen goodman 
empirical study smoothing techniques language modeling 
joshi martha palmer editors proceedings fourth annual meeting association computational linguistics pages san francisco 
association computational linguistics morgan kaufmann publishers 
chen goodman 
empirical study smoothing techniques language modeling 
technical report tr center research computing technology harvard university cambridge massachusetts august 
kevin kirchhoff 
automatic learning language model structure 
proceedings international conference computational linguistics coling 
friedman koller 
learning bayesian networks data 
nips tutorial notes 
neural information processing systems vancouver canada 
jelinek 
statistical methods speech recognition 
mit press 
kirchhoff novel speech recognition models arabic jhu summer workshop final report 
lauritzen 
graphical models 
oxford science publications 
stolcke 
srilm extensible language modeling toolkit 
proc 
int 
conf 
spoken language processing denver colorado september 
whittaker woodland 
particle language modeling 
proc 
int 
conf 
spoken language processing beijing china 
uweetr 
