machine learning kluwer academic publishers 
manufactured netherlands 
bayesian network classifiers nir friedman computer science division soda hall university california berkeley ca nir cs berkeley edu dan geiger computer science department technion haifa israel goldszmidt sri international ave menlo park ca cs technion ac il sri com editor provan langley smyth 
supervised learning shown surprisingly simple bayesian classifier strong assumptions independence features called naive bayes competitive state art classifiers 
fact raises question classifier restrictive assumptions perform better 
evaluate approaches inducing classifiers data theory learning bayesian networks 
example variables attributes class variable 
consider graph structure class variable root attribute class variable unique parent ai structure depicted 
type graph structure equation yields pr pr pr ai 
bayesian network classifiers definition conditional probability get pr pr pr ai normalization constant 
fact definition naive bayes commonly literature langley :10.1.1.135.7718
problem learning bayesian network informally stated training set un instances find network best matches common approach problem introduce scoring function evaluates network respect training data search best network function 
general optimization problem intractable chickering 
certain restricted classes networks efficient algorithms requiring polynomial time number variables network 
take advantage efficient algorithms section propose particular extension naive bayes 
problem learning bayesian network informally stated training set un instances find network best matches common approach problem introduce scoring function evaluates network respect training data search best network function 
general optimization problem intractable chickering 
certain restricted classes networks efficient algorithms requiring polynomial time number variables network 
take advantage efficient algorithms section propose particular extension naive bayes 
main scoring functions commonly learn bayesian networks bayesian scoring function cooper herskovits heckerman function principle minimal description length mdl lam bacchus suzuki see friedman goldszmidt account scoring function :10.1.1.156.9918:10.1.1.88.4436
scoring functions asymptotically equivalent sample size increases furthermore asymptotically correct probability equal learned distribution converges underlying distribution number samples increases heckerman geiger 
depth discussion pros cons scoring function scope 
henceforth concentrate mdl scoring function 
mdl principle rissanen casts learning terms data compression 
data set noticeable improvement lymphography smoothed version accuracy compared smoothing 
note particular data set smoothed version tan accuracy compared smoothing 
complete results smoothed version naive bayes reported table 
tan performs better naive bayes naive bayes comparable quinlan state art decision tree learner may infer tan perform comparison 
confirm prediction performed experiments comparing tan selective naive bayesian classifier langley sage john kohavi :10.1.1.43.3692
approach searches subset attributes naive bayes best performance 
results displayed figures table show tan competitive approaches lead significant improvements cases 

bayesian classifiers tan approach forces relations attributes different instances class variable immediate generalization different friedman geiger goldszmidt augmenting edges tree structures case tan class collection networks classifier 

discussion section review related expand issue conditional log likelihood scoring function 
additionally discuss extend methods deal complicating factors numeric attributes missing values 

related naive bayes interest explaining surprisingly performance naive bayesian classifier domingos pazzani friedman :10.1.1.144.7475
analysis provided friedman particularly illustrative focuses characterizing bias variance components estimation error combine influence classification bayesian network classifiers table 
experimental results primary approaches discussed 
performance 
naive bayesian classifier shows certain conditions low variance associated classifier dramatically mitigate effect high bias results strong independence assumptions 
change score true restricted class structures 
leaf network outgoing arcs easy prove setting parameters equation maximizes fixed network structure 
outgoing arcs describe pb product parameters pb involves normalization constant requires sum values consequence decompose maximize choice conditional probability table independently 
closed form equation choosing optimal parameters conditional log likelihood score 
implies maximize choice parameters fixed network structure resort search methods gradient descent space parameters techniques binder :10.1.1.52.9439
learning network structure search repeated structure candidate rendering method computationally expensive 
find heuristic approaches allow effective learning conditional log likelihood remains open question 
difference procedures maximize log likelihood ones maximize conditional log likelihood similar standard distinction statistics literature 
dawid describes paradigms estimating 
assume values missing random rubin marginal likelihood probability assigned parts instance observed basis scoring models 
values missing random careful modeling exercised order include mechanism responsible missing data 
source difficulty learning incomplete data marginal likelihood decompose 
score written sum local terms equation 
evaluate optimal choice parameters candidate network structure perform nonlinear optimization em lauritzen gradient descent binder :10.1.1.52.9439
problem selecting best structure usually intractable presence missing values 
efforts geiger chickering heckerman examined approximations marginal score evaluated efficiently 
additionally friedman proposed variant em selecting graph structure bayesian network classifiers efficiently search candidates 
computational cost associated methods directly related problem inference learned networks 
