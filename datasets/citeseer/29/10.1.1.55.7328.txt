multivariate decision trees brodley paul utgoff department computer science university massachusetts amherst massachusetts usa coins technical report december multivariate decision trees overcome representational limitation univariate decision trees univariate decision trees restricted splits instance space orthogonal feature axis 
discusses issues constructing multivariate decision trees representing multivariate test including symbolic numeric features learning coefficients multivariate test selecting features include test pruning multivariate decision trees 
new review known methods forming multivariate decision trees 
methods compared variety learning tasks assess method ability find concise accurate decision trees 
results demonstrate multivariate methods effective 
addition experiments confirm allowing multivariate tests improves accuracy resulting decision tree univariate trees 
breiman 
quinlan mingers buntine niblett fayyad irani discuss compare different partition merit criteria 
addition univariate multivariate decision trees wants avoid overfitting decision tree training data domains contain noisy instances 
noisy instance class label incorrect number attribute values incorrect combination 
noise caused different factors include faulty measurements ill defined thresholds subjective interpretation quinlan :10.1.1.167.3624
overfitting occurs training data contain noisy instances decision tree algorithm induces classifier classifies instances training set correctly 
tree usually perform poorly previously unseen instances 
avoid overfitting tree pruned back reduce estimated classification error 
symbolic numeric features desires decision tree algorithm able handle unordered symbolic ordered numeric features 
continues eliminating features specified stopping criterion met 
table shows general sequential backward elimination algorithm 
determine feature eliminate step need find coefficients linear combination tests different feature removed 
choices implement sbe algorithm choice merit criterion function stopping criterion 
example merit criterion function may measure accuracy test applied training data measure entropy gini breiman ratio quinlan criteria :10.1.1.167.3624
stopping criterion determines eliminating features linear combination test 
example search continue feature remains search halted value merit criterion test gamma features test features 
process eliminating features best linear combination test minimum number features best saved 
feature elimination ceases test decision node saved linear combination test 
learning coefficients section compare coefficient learning algorithms pocket algorithm rls procedure thermal training procedure 
aim experiment compare coefficient training methods linear combinations omit cart training procedure comparison 
cart chooses linear combinations univariate tests 
experiment ran coefficient learning algorithms conjunction sbe sfs gsbe feature selection methods assess accuracy running time size trees table description data sets data set classes instances features type missing breast bupa cleveland glass hepatitis iris led segment produced 
feature selection algorithms ratio merit criterion quinlan discrimination criteria described section :10.1.1.167.3624
addition feature selection algorithms gsbe requires input features normalized normalize instances node retain normalization information testing 
prune trees reduced error pruning algorithm quinlan uses set instances prune set independent training instances estimate error decision tree 
primary focus experiment evaluate coefficient learning algorithms defer comparing feature selection methods section 
section seeks answer questions coefficient learning methods 
experiment compares multivariate feature selection methods 
second experiment compares multivariate univariate multivariate plus univariate feature selection methods 
multivariate methods experiment rls coefficient learning method class data sets 
thermal training procedure multiclass data sets discussed section rls restricted binary partitions data 
algorithm uses gain ratio merit criterion quinlan discrimination criteria reduced error pruning :10.1.1.167.3624
algorithms differ feature selection procedure 
section seeks answer questions 
sbe better sfs starting informed position 

