bayesian approach learning causal networks david heckerman microsoft research bldg redmond wa microsoft com acausal bayesian networks represent probabilistic independence causal bayesian networks represent causal relationships 
examine bayesian methods learning types networks 
bayesian methods learning acausal networks fairly developed 
methods employ assumptions facilitate construction priors including assumptions parameter independence parameter modularity likelihood equivalence 
show assumptions appropriate learning causal networks need additional assumptions order learn causal networks 
introduce sufficient assumptions called mechanism independence component independence 
show new assumptions combined parameter independence parameter modularity likelihood equivalence allow apply methods learning acausal networks learn causal networks 
great deal interest bayesian methods learning bayesian networks data spiegelhalter lauritzen cooper herskovits buntine spiegelhalter madigan raftery heckerman 
methods take prior knowledge domain statistical data construct bayesian network models domain 
concentrated bayesian networks interpreted representation probabilistic conditional independence 
researchers proposed causal interpretation bayesian networks pearl verma spirtes heckerman shachter 
researchers show having causal interpretation important allows predict affects interventions domain done causal interpretation 
extend bayesian methods learning acausal bayesian networks causal bayesian networks 
offer contributions 
show acausal causal bayesian networks acausal causal networks short significantly different semantics inappropriate blindly apply methods learning acausal networks causal networks 
despite differences identify circumstances methods learning acausal networks applicable learning causal networks 
section describe causal interpretation bayesian networks developed heckerman shachter consistent pearl causal theory interpretation pearl verma pearl 
show causal network represented special type influence diagram 
section review bayesian methods learning acausal networks showing various assumptions properties parameter independence parameter modularity hypothesis equivalence facilitate learning task 
section show methods learning acausal networks adapted learn ordinary influence diagrams 
section identify problems approach learning influence diagrams correspond causal networks 
identify assumptions called mechanism independence component independence circumvent problems 
section argue assumption parameter modularity reasonable learning causal networks property hypothesis equivalence replaced weaker assumption called likelihood equivalence 
show assumptions parameter independence parameter modularity likelihood equivalence mechanism independence component independence methods learning acausal networks learn causal networks 
assume reader familiar concept random sample distinction subjective objective probability call probability physical probability respectively distinction chance decision variables 
refer decision variable simply decision consider problem modeling relationships domain consisting chance variables decision variables lower case letters represent single variables upper case letters represent sets variables 
write denote variable state observe state variable set call set observations state write leave state variable set variables implicit 
denote subjective probability person state information pp denote physical probability conditional event 
influence diagram domain model domain having structural component probabilistic component 
structure influence diagram directed acyclic graph containing square decision oval chance nodes corresponding decision chance variables respectively information relevance arcs 
information arcs point decision nodes represent known time decisions 
relevance arcs point chance nodes represent absence assertions conditional independence 
associated chance node influence diagram probability distributions pa pa parents diagram 
distributions combination assertions conditional independence determine joint distributions 
special kind chance node deterministic node depicted double oval 
node deterministic node corresponding variable deterministic function parents 
influence diagram may contain single distinguished node called utility node encodes decision maker utility state node parents 
utility node deterministic function predecessors children 
influence diagram formed decisions totally ordered influence diagram structure 
details see howard 
acausal bayesian network influence diagram contains decision nodes information arcs 
acausal bayesian network represents assertions conditional independence 
details see pearl 
battery 
start 
fuel 
move 
causal network 
corresponding influence diagram 
double ovals denote deterministic nodes 
causal networks section describe causal bayesian networks represent influence diagrams 
influence diagram representation describe identical pearl causal theory exception discussed 
representation directly follow approach heckerman shachter proceedings define cause effect develop definition influence diagram representation causal networks 
roughly speaking causal network domain chance variables directed acyclic graph nodes correspond chance variables nonroot node direct causal effect parents pearl verma 
example causal network shown 
diagram indicates car starts caused condition battery fuel supply car moves caused starts model condition battery fuel supply causes 
example assume variables binary 
develop influence diagram representation causal network need introduce concepts unresponsiveness set decision mapping variable cause causal mechanism canonical form 
understand notion unresponsiveness consider simple decision bet heads tails outcome coin flip variable represent win 
deterministic function win outcome coin matches bet 
assume coin fair heads person flips coin know bet 
example uncertain coin come heads certain outcome choose bet differently 
say unresponsive claim relationship know depends sense bet differently different 
example know win betting heads loose betting tails 
say responsive general determine chance variable unresponsive decision answer query outcome matter choose queries form simple type counterfactual query discussed philosophical literature lewis 
interesting cases easy answer query uncertain outcome note unresponsive probabilistically independent converse hold 
understand concept set decision consider chance variable battery 
automobile example 
assume states bad battery 
chance variable imagine action force variable possible states 
action side effects variables model required causal interactions domain say setting variable 
example force battery fail blowing car 
action force variable fuel 
empty qualify setting battery 
contrast force battery fail emptying battery fluid ground side effects follow causal interactions domain 
consequently action qualifies setting battery 
extend idea setting variable decision variable 
set decision variable simply choosing alternatives 
set decision chance variable denoted decision variable alternatives set state example set decision corresponding battery 
alternatives set battery set battery bad pearl verma introduce concepts setting variable set decision primitives 
heckerman shachter proceedings formalize concepts terms unresponsiveness 
understand concept mapping variable suppose collection variables may include chance decision variables chance variable imagine setting states observing observing maps mapping variable chance variable states correspond possible mappings example consider variables start 
move 
automobile example 
states mapping variable shown table states mapping variable 
state state state state start move table 
state represents normal situation 
car start sense set action move prevent car starting move 
second state represents situation regardless car start car move 
state occur example parking attendant placed restraint car tires 
note definition deterministic function mapping variable variables example state 
observe mapping variable directly 
see car moves start car 
general mapping variables fully observed 
example consider decision continue quit smoking chance variable representing get lung cancer reach years age 
case fully observe mapping variable observe get lung cancer possible choices 
general mapping variable represents counterfactual set possible outcomes observe 
rubin howard define concepts similar mapping variable 
concepts heckerman shachter proceedings say set variables causes respect decisions minimal set variables unresponsive roughly speaking cause respect way affects affected explication cause unusual conditioned set decisions 
heckerman shachter discuss advantages approach 
causes respect call mapping variable causal mechanism simply mechanism 
chance variables decisions heckerman shachter show construct influence diagram represents causes caused variable follows 
add node diagram corresponding variable order variables xn variables unresponsive come 
variable xi order xi responsive add causal mechanism node xi ci diagram ci xi xi deterministic function ci xi ci 
assess dependencies variables unresponsive show resulting influence diagram properties chance nodes responsive descendants decision nodes nodes descendants decision nodes deterministic nodes 
influence diagrams satisfy conditions said canonical form 
note information arcs utility node may added canonical form influence diagrams constructs needed representation cause discussion 
influence diagram canonical form represent causal relationships depicted causal network 
suppose set chance variables corresponding collection set decisions causal network pa parents causal network 
interpret causal network mean pa set causes respect 
construct influence diagram canonical form described ordering consistent causal network obtain influence diagram variable deterministic function set decision pa causal mechanism pa 
definition set decision simplify deterministic relationship replacing causal mechanism pa pa denotes mappings pa set example automobile domain state 
transformation causal network canonical form influence diagram automobile domain illustrated 
call variables original causal network domain variables 
domain variable appears influence diagram function set decision parents causal network pa mapping variable pa 
note 
mechanisms set decisions independent required canonical form mechanisms unresponsive set decisions 
required canonical form representation mechanisms mutually independent example 
general influence diagram representation causal network identical pearl causal theory exception pearl requires mechanisms calls disturbances independent 
desirable consequence restriction variables causal network exhibit conditional independencies obtain interpreting causal network acausal network spirtes pearl 
example independence causal mechanisms example yield conditional independencies obtain independencies interpret causal network acausal network 
shall illustrate dependent mechanisms excluded general 
learning acausal networks correspondence previous section see learning causal networks special case learning influence diagrams canonical form 
section review methods learning acausal bayesian networks described spiegelhalter lauritzen cooper herskovits buntine spiegelhalter madigan raftery heckerman 

sections show methods extended learn arbitrary influence diagrams influence diagrams canonical form 
suppose domain consisting chance variables 
xn 
suppose database cases 
cm case cl contains observations variables basic assumption underlying bayesian approach database random sample joint physical probability distribution pp 
done traditionally characterize physical probability distribution finite set parameters example contains continuous variables pp may multivariate gaussian distribution parameters specifying distribution means covariances 
limit discussion domains containing discrete variables 
parameters correspond exactly physical probabilities distribution pp 
shall pp notation interchangeably 
general bayesian approach learning uncertain parameters assess prior distributions compute posterior distributions database 
paradigm learning acausal bayesian networks add twist general approach assume physical probability distribution pp constrained encoded acausal network structure identity possibly uncertain 
start special case suppose pp encoded known acausal network structure bs uncertain values probabilities associated network structure 
say database random sample bs 
situation turns database separated set random samples random samples determined structure bs 
example consider domain consisting variables variables possible states 
case case case case conditional independencies associated assertion database random sample structure binary 
additional assumption parameter independence 
tion database random sample structure equivalent assertion database separated random samples observations binomial sample parameter observations cases binomial sample parameter observations cases binomial sample parameter 
contains acausal network illustrates conditional independencies database cases network parameters assertion 
decomposition random samples update parameter independently conditions parameters independent assumption call parameter independence database complete variable observed case 
assumption parameter independence illustrated 
examine updating arbitrary structure bs domain discuss situation data may missing section 
ri number states variable xi qi xl pa xi rl number states pa xi 
ijk denote parameter corresponding physical probability xi pa xi ijk ri ijk 
addition define ij ri ijk bs qi ij parameters bs correspond physical probabilities acausal network structure bs 
illustrate updating approach suppose variable set ij dirichlet distribution ij ri ijk ijk bh assertion hypothesis database random sample network structure bs normalization constant 
parameter independence complete database nijk number cases database xi pa xi obtain ij ijk nijk ijk normalization constant 
furthermore expectation ijk respect distribution ij obtain probability xi pa xi cm case cm seen seeing database cm qi ijk ij nijk nij ij ri ijk nij ri nijk 
suppose uncertain probabilities uncertain structure encodes 
express uncertainty assigning prior probability bh possible hypothesis bh update probabilities see cases 
doing learn structure domain 
bayes theorem normalization constant 
product rule cl cl evaluate term right hand side equation equation assumption database complete 
posterior probability bh obtain bh qi ij ij nij ri ijk ijk nijk posterior probabilities equation compute probability distribution case observed seen database 
expansion rule obtain cm cm bh database contains missing data compute exactly summing result equation possible completions database see section 
unfortunately approach intractable observations missing 
consequently approximate methods filling missing data data titterington cowell em algorithm dempster gibbs sampling york madigan raftery 
believe network structures possible approach discussed essentially learning network structure 
directly assess priors possible network structures parameters subsequently equations generalizations continuous variables missing data 
number network structures domain containing variables exponential consequently exclude network structures need efficient methods assigning priors structures parameters buntine spiegelhalter heckerman search methods identifying structures contribute significantly sum equation cooper herskovits heckerman 
review efficient method described heckerman assigning priors parameters possible network structures 
approach user assesses prior network acausal bayesian network case seen database assumption constraints parameters 
formally prior network represents joint probability distribution sc bsc network structure containing missing arcs 
user assesses equivalent sample size prior network 
measure user confidence assessment prior network 
network structure bs xi parents pa xi compute dirichlet exponents equation relation ijk xi pa xi sc probability computed prior network 
heckerman derive approach assumption parameter independence additional assumption called parameter modularity property called hypothesis equivalence 
property hypothesis equivalence stems fact acausal network structures equivalent represent exactly sets probability distributions verma pearl 
example variable domain network structures represents distributions conditionally independent equivalent 
definition hypothesis bh follows hypotheses corresponding equivalent structures property hypothesis equivalence 
assumption parameter modularity says network structures bs bs xi parents bs bs ij ij qi 
heckerman call property parameter modularity says distributions parameters ij depend structure network local variable xi ij depends xi parents 
section examine appropriateness hypothesis equivalence parameter modularity learning causal networks 
learning influence diagrams consider problem learning influence diagrams correspond causal networks examine task learning arbitrary influence diagrams 
task straightforward observations 
definitions information arc utility node information arcs predecessors utility node known certainty decision maker learned 
need learn relevance arc structure physical probabilities associated chance nodes 
definition decision states decision variables known decision maker case 
assuming decisions recorded complete data case database 
observations follows problem learning influence diagrams domain reduces problem learning acausal bayesian networks interpret decision variables chance variables 
caveat learned relevance arc structures constrained influence diagram semantics 
particular relevance arc structure eligible learned corresponding hypothesis nonzero prior node root node structure combined information arc structure declared decision maker contains directed cycles 
note constraints satisfied canonical form representations causal networks 
simplicity presentation assume information arc utility node structure identical cases database 
decomposition mapping variable 
assumption component independence 
learning causal network parameters section consider aspects learning influence diagrams peculiar influence diagrams canonical form 
discussion assume structure influence diagram known need learn parameters structure 
difficulty associated learning influence diagrams canonical form occurs domains set variables small number times mechanisms fully observable 
example recall decision continue quit smoking denotes decision denotes get lung cancer age 
case fully observe mapping variable observe get lung cancer possible choices 
choice observation exclude states 
consequently learning difficult impossible 
understand difficulty way 
mapping variable states decompose set variables 
kq variable represents variable set state call variables mechanism components 
example illustrates components mechanism variable binary variable 
note definition mechanism component pp pp analogous equation holds subjective probabilities 
decomposition setting observation equivalent observation exactly components 
set smoking example observe multiple mechanism components 
consequently learn physical probabilities characterize dependencies components 
holland calls problem albeit different mathematical formalism fundamental problem causal inference circumvent problem assume mechanism components independent assumption call component independence 
assumption incorrect learn correct counterfactual relationships 
regardless assumption correctness correctly quantify affects single setting action 
example smoking decision mechanism components clearly dependent knowing quit got lung cancer gotten lung cancer continued 
suppose assume components independent learn physical probabilities database cases 
learn incorrect counterfactual relationships independent learn correct marginal physical probabilities associated mechanism components 
equation learn correct physical probability get cancer continue smoke correct physical probability cancer quit smoking 
second complication learning influence diagrams canonical form possible dependency different mechanisms 
example suppose model voltages logic circuit containing buffers series shown 
represent input output voltages circuit respectively represents voltage buffers 
causal network circuit corresponding influence diagram canonical form shown 
causal mechanism represents possible mappings input output buffer 
possible states output normal output zero output output inverted 
causal mechanism representation working status buffer 
similarly mapping variable represents working status second buffer 
mechanisms dependent buffer function dependent example possible circuit cause buffers fail 
note equation assumption component independence fill probability tables associated canonical form representation causal network copying probabilities associated causal network 
assumption canonical form representation requires additional probability assessments 
logic circuit containing buffers series 
causal network circuit represented influence diagram canonical form 
dependent mechanisms lead practical problems 
large number states typically associated mapping variables assessment priors difficult require vast amounts data learn 
fortunately introduce additional domain variables order render mechanisms independent 
circuit example add domain variable representing temperature circuit new mechanisms independent 
solution creates problem learning may able observe variables introduce 
address issue section 
mechanism independence component independence mechanisms chance variables remain canonical form influence diagram mutually independent mechanism components 
consequently assume parameter independence problem learning causal network essentially reduces learning acausal network 
illustrate equivalence consider binary variable domain assume database random sample influence diagram corresponding causal network assumptions mechanism component parameter independence influence diagram deterministic functions set set definitions set decision mechanism component 
suppose set decisions situation integrate mechanism variables diagram discussed shachter obtain influence diagram shown 
structure equivalent shown learning acausal network update parameters causal network just update corresponding acausal network 
result generalizes arbitrary causal networks 
particular set decisions particular case cl say observations domain variables cl non experimental data 
say observations experimental data 
case non experimental data update parameters causal network just parameters corresponding acausal network assuming mechanism component parameter independence 
updating procedure experimental data slightly different non experimental data 
variable example set observe set influence diagram shown 
arcs removed set variable consequently updated data 
general update parameters canonical form influence diagram experimental data set xi break arcs xi update parameters acausal network 
learning causal network structure section saw assumptions parameter independence parameter modularity hypothesis equivalence assess priors parameters possible acausal network structures constructing single prior network case seen database assessing equivalent sample size confidence prior network 
discussion previous section follows prior network methodology establish priors causal network learning provided assume mechanism independence component independence parameter independence parameter modularity hypothesis equivalence 
section examine assumptions parameter modularity likelihood equivalence learning causal networks 
assumption parameter modularity compelling justification context causal networks 
suppose domain variable parents pa possible causal network structures 
reasonable believe causal mechanism pa structure 
follows parameters pa structures prior distributions parameter modularity hold 
set mechanism independence component independence parameter independence associated causal network 
corresponding diagrams contrast property hypothesis equivalence applied causal networks 
example variable domain causal network represents assertion causes causal network represents assertion causes possible cause vice versa variables deterministically related consider variables pressure volume closed physical system 
barring deterministic relationships hypotheses corresponding network structures mutually exclusive 
consequently hypothesis equivalence hold 
know little structure domain reasonable assume data help distinguish equivalence network structures 
express assumption formally denote parameters joint space denote hypothesis database random sample influence diagram corresponding causal network structure cs 
causal network structures cs cs equivalent interpreted acausal networks 
call assumption likelihood equivalence 
heckerman show prior network methodology justified replace assumption hypothesis equivalence likelihood equivalence 
assumptions mechanism component parameter independence assumption likelihood equivalence interesting characterization 
consider variable domain 
suppose know domain having uninformative dirichlet priors parameters network structures dirichlet exponents arbitrarily close 
suppose adopt assumption likelihood equivalence network structures suppose obtain single case experimental data set observe 
updating procedure described previous section network structure update parameter parameter 
contrast network structure update parameter parameter 
result posterior distributions longer satisfy likelihood equivalence 
show domain uninformative dirichlet prior domain database containing experimental data resulting posterior distributions violate likelihood equivalence 
assess likelihood equivalence holds asking prior knowledge equivalent having seen non experimental data 
note assumption likelihood equivalence tends reasonable familiar domains 
example doctor may uncertain disease causes disease vice versa may defined hypotheses causes disease vice versa 
case assumption likelihood equivalence unreasonable 
emphasize experimental data crucial learning causal structure 
variable domain suppose believe causes causes set different states learn probability depends learn causes verify relation set different states check probability remains 
conversely set different states learn probability depends learn causes may need experimental data quantify effects intervention example learn physical probability distribution pp 
causal structure situations quantify effects intervention observational data pearl verma pearl 
learning hidden variables section saw remove dependencies causal mechanisms adding additional domain variables 
situations observe variables 
say variables hidden 
discussed methods learning acausal networks missing data known exact em gibbs sampling 
methods applied databases containing hidden variables 
assumptions mechanism independence component independence parameter independence parameter modularity likelihood equivalence learn causal networks hidden variables methods conjunction prior network methodology 
illustrate approach consider simple medical domain containing observable variables representing presence absence heart disease lung disease respectively hidden variable representing presence absence gene diseases 
possible causal network structures domain shown 
network structure labeled cs causes hidden common cause diseases 
cs disease variables related hidden common cause 
suppose network structure hypotheses possible equally priori 
addition suppose prior network domain cs probabilities shown equivalent sample size network 
suppose database containing cases cases set decisions heart disease lung disease 
cases compute posterior probabilities network structure hypotheses exactly equation applies complete databases equation relation subscripts variables denote case numbers 
example equations term sum cs performing sums applying bayes theorem obtain ch ch 
heart disease 
genotype 
lung disease 
possible causal networks explain observed dependence heart lung disease 
domains containing hidden variables pearl suggested generalization assumption likelihood equivalence says causal networks equivalent respect distributions encode observed variables parameters observed variables identical priors 
call property strong likelihood equivalence 
property hold simple medical example 
network structures cs cs equivalent respect variables structures represent joint distribution variables 
saw previous example observations help discriminate network structures 
assumptions mechanism component independence strong likelihood equivalence consistent prior network methodology 
strong likelihood equivalence consistent assumptions parameter independence parameter modularity 
consequently strong likelihood equivalence may lead method assessing priors parameters alternative prior network approach 
learning general causal models presentation concentrated domains variables root nodes causes 
emphasize restriction unnecessary definition cause heckerman shachter proceedings 
particular shown researchers relationships domains variables causes encoded canonical form 
consequently apply learning methods described general domains 
acknowledgments motivated conversations max chickering greg cooper dan geiger judea pearl 
jack breese max chickering provided useful suggestions earlier versions manuscript 
buntine buntine 

theory refinement bayesian networks 
proceedings seventh conference uncertainty artificial intelligence los angeles ca pages 
morgan kaufmann 
buntine buntine 

operations learning graphical models 
journal artificial intelligence research 
cooper herskovits cooper herskovits 

bayesian method induction probabilistic networks data 
machine learning 
cooper herskovits cooper herskovits 
january 
bayesian method induction probabilistic networks data 
technical report smi section medical informatics stanford university 
cowell cowell dawid sebastiani 

comparison sequential learning methods incomplete data 
technical report department statistical science university college london 
dempster dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
heckerman heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
proceedings tenth conference uncertainty artificial intelligence seattle wa pages 
morgan kaufmann 
heckerman heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
machine learning appear 
heckerman shachter heckerman shachter 

decision view causality 
proceedings tenth conference uncertainty artificial intelligence seattle wa pages 
morgan kaufmann 
heckerman shachter heckerman shachter 

definition graphical representation causality 
proceedings 
howard matheson howard matheson 

influence diagrams 
howard matheson editors readings principles applications decision analysis volume ii pages 
strategic decisions group menlo park ca 
lewis lewis 

counterfactual dependence time arrow 
pages 
madigan raftery madigan raftery 

model selection accounting model uncertainty graphical models occam window 
journal american statistical association 
pearl pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san mateo ca 
pearl pearl 

causal diagrams empirical research 
biometrika appear 
pearl pearl 

personal communication 
pearl verma pearl verma 

theory inferred causation 
allen fikes sandewall editors knowledge representation reasoning proceedings second international conference pages 
morgan kaufmann new york 
shachter shachter 

evaluating influence diagrams 
operations research 
spiegelhalter spiegelhalter dawid lauritzen cowell 

bayesian analysis expert systems 
statistical science 
spiegelhalter lauritzen spiegelhalter lauritzen 

sequential updating conditional probabilities directed graphical structures 
networks 
spirtes spirtes glymour scheines 

causation prediction search 
springer verlag new york 
titterington titterington 

updating diagnostic system unconfirmed cases 
applied statistics 
verma pearl verma pearl 

equivalence synthesis causal models 
proceedings sixth conference uncertainty artificial intelligence boston ma pages 
morgan kaufmann 
york york 

bayesian methods analysis misclassified incomplete multivariate discrete data 
phd thesis department statistics university washington seattle 
