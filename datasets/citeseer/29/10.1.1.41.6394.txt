theory applications agnostic pac learning small decision trees peter auer institute theoretical computer science technische austria igi tu ac robert holte computer science dept university ottawa ottawa canada holte csi ca wolfgang maass institute theoretical computer science technische austria maass igi tu ac exhibit theoretically founded algorithm agnostic pac learning decision trees levels computation time linear size training set 
evaluate performance learning algorithm common real world datasets show datasets provides simple decision trees little loss predictive power compared 
fact datasets continuous attributes error rate tends lower 
best knowledge time pac learning algorithm shown applicable real world classification problems 
prove agnostic algorithm guaranteed produce close optimal level decision trees sufficiently large training sets 
distribution data 
algorithm developed purpose described section article results performance real world classification problems discussed subsequent sections 
define basic notions theoretical applied machine learning address obstacles overcome order combine approaches 
mentioned context apparently pac learning algorithm tested real world classification problems previously fruitful migration various ideas pac learning theory applications see dss 
applied machine learning concrete datasets quite diverse application domains viewed prototypes real world classification problems 
performance practical learning algorithms datasets described number interesting comparative studies see min wk wk bn hol :10.1.1.103.7226
dataset list items typically dozen item consisting attribute values typically associated classification 
attributes continuous ranging categorical ranging finite set 
items attribute values missing unknown 
denote items hx xn pg classification possible classes typically ff ff value attribute ff 
targets approximation learning algorithm hypothesis class succeed setting learning algorithm required find hypothesis approximates target distribution nearly close possible 
algorithm efficient agnostic learning algorithm hypothesis class target distribution find hypothesis arbitrarily close best approximation target distribution polynomial time sample 
precisely pac learning theory says agnostic pac learning algorithm exists function theta bounded polynomial ffi distribution xn theta pg sequence train ffi items drawn train gamma inf hn probability gamma ffi regard random drawing train 
says efficient agnostic pac learning formally test test fi fi fi tg fi fi algorithm train computed number computation steps bounded polynomial bit length size representation train kss shown efficient pac learning algorithm family hypothesis classes hn vc dimension hn grows polynomially polynomial time algorithm computes set train items hypothesis hn minimizes err train 
attempts original version focuses hypothesis classes hn distributions target concept hn realistic extended include certain noise models see ek kea kl dec target concept hn large amount white noise small comparison desired error rate learner fraction arbitrary malicious noise :10.1.1.114.5693
version white noise model situation encounters considered real world classification problems systematic noise reported dp 
hand model malicious noise pac learner achieve error rates large point view applied machine learning 
obvious model agnostic pac learning adequate investigation real world classification tasks relatively results known model 
reason exist remarkable negative results 
leaves labelled classifications pg 
noted attributes may queried altogether level tree delta max leaves 
discuss side hypothesis class tree functions xn pg defined level trees 
level tree single attribute ff queried root tree similarly nodes level level trees ff outgoing edges ff categorical attribute edges ff continuous attribute 
note tree hypothesis class holte learning algorithm hol :10.1.1.103.7226
experiments chosen 
furthermore identify decision tree function xn pg computed occasionally write tree tree 
algorithm essentially tries possible assignments attributes query nodes level tree done careful manner gives rise factor time bound 
assignment attributes query nodes algorithm computes log steps constant factors fast sorting list continuous attributes optimal assignment labels edges leaves precisely computes endpoints max intervals continuous attributes assigns classifications pg leaves resulting number misclassifications items minimal level trees assignment attributes query nodes 
theorem design algorithm td computes delta delta gamma delta log computation steps list items xn theta pg tree tree minimal number misclassifications items analogously computes delta delta delta log steps optimal tree tree 
td algorithms efficient agnostic pac learning hypothesis classes tree 
evaluation real world classification problems section performance diverse set realworld datasets experimentally compared qui state art heuristic machine learning algorithm 
hypothesis class includes decision trees arbitrary depth binary splits continuous attributes 
fifteen datasets experiments bc ch hd ir la se hol new :10.1.1.103.7226
ap dataset kindly supplied weiss rutgers university 
new datasets obtained uci repository 
table gives datasets main characteristics 
size total number examples dataset 
ch sp special cases absolutely require complex tree attain high accuracy 
framework defined bb hypotheses produced learning algorithms predict unseen instances summarize contents dataset 
intriguing question common real world classification problems fact characterized accurately simple hypothesis type see 
unfortunately little information type available point usually time consuming compute err interesting classes simple hypotheses common datasets exception see case production rules length dataset ap 
noted opt bb rw hol heuristic measures values underestimate ability simple rules summarize dataset :10.1.1.103.7226
help algorithms td section feasible calculate true capacity summarization hypothesis spaces tree small values average sky values table datasets 
clearly excellent algorithm task summarizing datasets defined bb 
learning curves decision trees small depth heart learning theory certain statistical results due tal dl provide distribution size training set train drawn upper bound difference true error hypothesis minimizes train true error inf hypothesis previous lack algorithms sufficiently efficient compute hypotheses interesting hypothesis classes real world data theories far tested artificial data see ct 
new algorithm permits time compute hypothesis tree minimizes err train evaluate predictions essential piece pac learning theory real world classification problems 
comp vol 

haussler kearns seung tishby rigorous learning statistical mechanics proc 
th annual acm conference computational learning theory acm press 
hol holte simple classification rules perform commonly datasets machine learning vol :10.1.1.103.7226

hsv simon van horn robust single neurons preprint kea kearns efficient noise tolerant learning statistical queries proc 
th acm symp 
theory computing 
