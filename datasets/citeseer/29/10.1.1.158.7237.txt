feature ranking methods information entropy parzen windows aw duch adam sebastian pa comparison feature ranking methods artificial real dataset 
ranking methods entropy statistical indices including pearson correlation considered 
parzen window method estimation mutual information indices gives similar results discretization separability index results strongly dependent smoothing parameter 
quality feature subsets highest ranks evaluated decision tree naive bayes nearest neighbour classifiers 
significant differences cases single best index works best data classifiers 
sure subset features giving highest accuracy selected different indices recommended 
keywords discretization feature selection parzen windows feature ranking methods feature selection fundamental problem different areas especially bioinformatics document classification forecasting object recognition modelling complex technological processes 
applications datasets thousands features uncommon 
features may important problems target concept small subset features usually relevant 
overcome curse dimensionality problem dimensionality feature space reduced 
may done creating new features contain maximum information class label original ones selecting subset relevant features 
methodology named feature selection called feature extraction includes linear principal component analysis independent component analysis non linear feature extraction methods 
feature selection algorithms may divided filters wrappers embedded approaches 
wrapper methods require application classifier trained feature subset evaluate quality selected features filters method evaluate quality independently classification algorithm 
embedded methods perform feature selection learning optimal parameters example neural network weights input hidden layer 
common measure relevance feature selection algorithms mutual information considered indicator relations input feature target variable compute mutual information continuous variables integrate probability density functions pdfs input output variables 
literature find different methods estimate mutual information histogram estimators kernel estimators parametric methods 
histogram method may estimation pdfs sufficient data 
known accurate calculation mutual information histogram methods requires large computer memory 
reason highquality estimation pdfs histograms practically impossible 
order avoid practical obstacles evaluate mutual information high accuracy gaussian parzen window method discretization estimate distribution feature values 
classification algorithms inherited ability focus relevant features ignore irrelevant ones 
decision trees primary example class algorithms multi layer perceptron mlp neural networks strong regularization input layer may exclude irrelevant features automatic way 
methods may benefit independent feature selection 
hand algorithms provisions feature selection 
nearest neighbour algorithm nn family methods classify novel examples retrieving nearest training example strongly relaying feature selection methods remove noisy features 
structured follows 
section contains discussion entropy feature ranking methods approximation probability density functions pdf methodology parzen windows 
section datasets numerical experiments described 
results comparing entropy methods indices estimations parzen windows simple pearson correlation coefficient feature values target variables section 
summary results plans feature section 
international conference research applied informatics august september poland theoretical framework ranking methods information theory typical ranking process consists steps 
initialize set set features 
empty set 

features compute coefficient 

find feature maximizes move 
repeat cardinal criterion function specific algorithm gives measure dependency features classes 
index uses normalized information gain called asymmetric dependency coefficient adc mi adc classes information entropy mutual information mi defined shannon formula mi ci lg ci lg sum features take discrete values continuous features replaced integral discretization performed estimate probabilities 
ranking algorithm proposed setiono uses normalized gain ratio mi joint entropy variables 
normalization may calculate gain ratio dividing class feature entropy uh mi index mutual information called symmetrical uncertainty mi su value implies strong dependent value implies independent 
mantaras proposed interesting criterion dml fulfils axioms distance may defined dml fi fi fi fi fi conditional entropy defined shannon 
criterion weighted joint entropy index introduced chi defined chi xk xk ci lg xk ci ef xk international conference research applied informatics august september poland average entropy calculated discretization bin distribution feature values 
ranking methods statistical measures statistical measures dependence random variables provide alternative indices information theory 
common measure dependence case relationships feature class values may statistics 
coefficient xj ci xj ci xj ci ij probabilities 
growing values signify stronger dependence feature values class labels index may ranking features 
statistics previously discretization process setiono liu 
statistical measure pearson linear correlation coefficient cov standard deviation covariance cov defined cov xi xy xi yi equal independent equal variables linearly dependent 
absolute value correlation coefficient direction correlation important feature ranking 
important realize linear correlation may completely fail distributions feature class values 
parzen window density estimate parzen window density estimate continuous feature approximate probability density distribution value feature involves superposition normalized windows function centred set random samples 
set dimensional training vectors xn pdf estimate parzen window xi window function window width parameter 
parzen showed converges true density selected properly 
window function normalized dy width parameter required function lim lim international conference research applied informatics august september poland gaussian window function exp zt ranking methods dimensional gaussian window exp standard deviation arbitrary values tests 
choice log proposed positive constant example number samples 
choice satisfies condition 
datasets described translates hypothyroid abalone 
practice clear significant differences various coefficients measuring similarity distributions 
answer questions computational experiments described performed 
datasets testing artificial datasets called gauss gauss generated compare different feature ranking feature selection methods data importance features completely known 
real datasets tests hypothyroid abalone data taken uci repository machine learning databases 
artificial data gauss gauss datasets features respectively 
dataset gaussian functions unit dispersion generate vectors dimensions gaussian cluster representing separate class 
gaussian centred respectively 
dataset contains vectors class 
case ideal ranking give order 
real data second dataset gauss extension containing features 
additional linearly dependent features created xi xi uniform noise unit variance 
case ideal ranking give order 
hypothyroid data contains results real medical screening tests thyroid problems 
class distribution normal primary hypothyroid compensated hypothyroid type 
data offers mixture nominal continuous features 
total cases training results year cases testing results year data collection 
abalone age abalone predicted physical measurements 
essentially approximation problem age quantized bins number rings equivalent age full years may treated classification problem large number classes 
cases features including nominal feature continuous measurement values feature number values created randomly range 
experiments results datasets described numerical tests 
case ranking methods information theory correlation coefficient 
different methods parameter estimation datasets discrete continuous feature 
continuous features parzen window different values smoothing parameter 
international conference research applied informatics august september poland presents dependence adc coefficient number bins equi width discretization estimated values adc coefficient different value smoothing parameter 
values adc coefficients parzen windows estimations larger equi width discretization non monotonic dependence noted 
larger values may expected values adc coefficient value adc coefficient different discretization number feature adc coefficient function number bins equi width discretization smoothing parameters 
sophisticated discretization method sufficient relative comparison entropy correlation methods 
behavior verified artificial datasets gauss gauss 
prediction accuracy balanced accuracy hypothyroid dataset strong unbalanced data abalone classes 
results artificial data correct ranking linear correlation index data monotonic change feature values class numbers possible 
surprisingly chi index placed redundant features providing perfect selection ranking algorithm 
average entropy noisy versions features happens case larger increase entropy due overlap distributions effect depend value parameter determining overlap 
results table method important adc index index uh index dml index chi index su index index index results feature ranking gauss data see description text 
rankings show preference gaussian distributions larger values standard deviation important 
errors ranking features may result numerical deviations 
observations provide interesting starting point search corrections indices rankings 
results datasets hypothyroid dataset large training test part strongly unbalanced training normal primary compensated hypothyroid similar distribution test 
features binary continuous results obtained medical test 
parzen window smoothing parameter applied features changed significant influence results best results obtained log ranking features obtained training data international conference research applied informatics august september poland indices tab 

features continuous consistently ranked top 
significant differences observed order remaining features 
method important adc index index uh index dml index chi index su index index index table results feature ranking hypothyroid data see description text 
ranking method investigation classification accuracy test data function best features done 
classifiers nbc naive bayes classifier decision tree implemented weka www cw waikato ac nz ml weka nearest neighbour implemented package www com pl 
classifiers give deterministic results simplifying comparison contrast methods neural classifiers give slightly different results restart averaging variance results taken account 
classification results fig 

feature number predicted adc uh su indices definitely important 
features methods give similar results su indices provide important features indices leading significant degradation results 
features evidently confuse nearest neighbour classifier omitted type methods 
features selected important dml chi best features gives poor result classifiers feature added best methods artificial data completely fail case 
peak performance reached features nbc features needed 
removes useful features automatically pruning tree accuracy drop stays peak level long important features included 
results abalone datasets similar calculations performed abalone dataset 
ranking algorithms applied dataset test datasets classification accuracy estimated fold crossvalidation 
purpose comparing different ranking methods approach sufficient producing ranking index 
real application lead overfitting ranking classification done separately training partition 
generalization may obtained selecting features highly ranked data partitions 
ranking features indices tab 

abalone dataset quality classification obviously quite low errors small getting predicted age abalone wrong years 
number data vectors ages years significantly larger young old bigger errors outside range 
table method important adc index index uh index dml index chi index su index index index results ranking methods abalone dataset standard deviation 
international conference research applied informatics august september poland adc index index index ml index chi index index index index accuracy number features adc index index index ml index chi index index index index accuracy number features accuracy adc index index index ml index chi index index index index number features balanced classification accuracy hypothyroid dataset upper nn classifier middle naive bayes classifier lower tree classifier 
international conference research applied informatics august september poland classification accuracy classifiers growing number features fig 

feature added abalone dataset random values ranked dml indices 
unfortunately ranking methods give similar poor results balanced accuracy 
case simple ranking sufficient obtain results 
ranking methods may filter features leading reduced dimensionality feature space 
especially effective classification methods inherent feature selections build nearest neighbour methods types neural networks 
entropy statistical indices feature ranking evaluated compared different types classifiers artificial real benchmark data 
accuracy classifiers significantly influenced choice ranking indices fig 

experiments real data experiments reported different ranking methods emerge winner 
simple statistical test correlation coefficient gives usually best results similar results may obtained entropy indices 
computational point view correlation coefficient slightly expensive entropy indices practice may important require discretization continuous features 
algorithms datasets selected precise criteria entropy algorithms datasets real artificial nominal binary continuous features 
real datasets illustrated fact best indices may select worst feature dataset 
classifiers evaluation feature subsets generated ranking procedures deterministic avoid additional source variance 
may draw study 
best ranking index different datasets different classifiers accuracy curves function number features may significantly differ 
evaluation ranking indices fast 
way sure highest accuracy obtained practical problems requires testing classifier number feature subsets obtained different ranking indices 
number tests needed find best feature subset small comparing cost wrapper approach larger number features 
improvements ranking methods possible results ranking algorithms depend discretization procedure continuous features better discretization 
crossvalidation techniques may select features important rankings partitions 
features lowest ranking values various indices may safely rejected 
remaining features analyzed selection methods allow elimination redundant correlated features 
ranking indices similar evaluate similarity statistical distributions may introduced linear dependency 
average group ranking indices may introduced 
recommendations tested larger datasets various classification algorithms near 
partially supported polish committee scientific research 
international conference research applied informatics august september poland adc index index index ml index chi index index index index accuracy number features adc index index index ml index chi index index index index accuracy number features adc index index index ml index chi index index index index accuracy number features balanced classification accuracy abalone dataset upper nn classifier middle naive bayes lower tree classifier 
international conference research applied informatics august september poland battiti mutual information selecting features supervised neural net learning ieee trans 
neural networks vol 
pp 
july 
breiman friedman olshen stone classification regression trees 
wadsworth brooks monterey ca 
chi entropy feature evaluation selection technique proc 
th australian conf 
neural networks pp 

duch gr new methodology extraction optimization application crisp fuzzy logical rules 
ieee transactions neural networks vol 
pp 

duch feature ranking selection discretization 
int 
conf 
artificial neural networks icann int 
conf 
neural information processing iconip istanbul june pp 

duch comparision feature ranking methods information entropy proc 

int 
joint conference neural networks ijcnn budapest ieee press pp 

kohavi john wrappers feature subset selection 
artificial intelligence vol 
pp 
lopez de mantaras distance attribute selecting measure decision tree induction 
machine learning vol 
pp 

murphy uci repository machine learning databases www ics uci edu pl mlearn html 
irvine ca university california dep 
information computer science 
parzen estimation probability density function mode ann 
math 
statistic vol 
pp 

press teukolsky vetterling numerical cambridge university press cambridge 
setiono liu improving backpropagation learning feature selection 
applied intelligence international journal artifical intelligence neural networks complex problem solving technologies vol 
pp 

setiono liu chi feature selection discretization numeric attributes 
proc 
th ieee international conf 
tools artificial intelligence washington pp 
nov 
shannon weaver mathematical theory communication 
urbana il university illinois press 
bartlett information theoretic subset selection 
computers chemical engineering vol 
pp 

quinlan programs machine learning 
san mateo morgan kaufman 
almuallim dietterich learning irrelevant features 
proc 
aaai anaheim ca pp 

kira rendell feature selection problem methods new algorithm 
proc 
aaai san jose ca pp 

blum langley selection relevant features examples machine learning 
artificial intelligence vol pp 

mars van aragon time delay estimation non linear systems average amount mutual information analysis 
signal processing vol 
pp 

relevance feedback category search image 
proc 
ieee int 
conf 
content access video image 
florence italy pp 
june 
estimation entropy mutual information continuous distribution 
signal processing vol 
pp 

moon rajagopalan lall estimation mutual information kernel density estimators 
phys 
rev vol 
pp 

aw duch department informatics university toru poland school computer engineering nanyang technological university singapore 
phys uni pl adam sebastian pa department university technology ul 
poland pl international conference research applied informatics august september poland 
