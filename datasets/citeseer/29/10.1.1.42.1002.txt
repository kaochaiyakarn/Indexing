representation behavioral history learning nonstationary conditions francois michaud maja matari department electrical computer engineering universit de qu canada computer science department university southern california los angeles ca robot having operate nonstationary conditions needs learn modify control policy adapt changing dynamics environment 
behavior approach manage interactions robot environment propose method models interactions adapts selection behaviors history behavior 
learning interaction model validated vision sonar pioneer robot context multi robot foraging task 
results show effectiveness approach advantage regularities experienced world leading fast adaptable specialization learning robot 
robot having operate real life situations requires ability adapt changing dynamics nonstationary conditions control policy change time :10.1.1.32.7692
mobile robots designed behavior paradigm shown performance adapting operating open environments approach robustness simplicity construction 
perception learning robot limited sonars color detection information position pen position robots configuration environment 
considering effects noise imprecision actuators action state different occasions result different states experiments making non deterministic 
station arity difficult explain definitions usually relation learning algorithm 
indicate environment stationary probabilities making state transitions receiving specific reinforcement signals change time 
sutton barto explain nonstationary means true values actions actual values expected mean reward action selected change time :10.1.1.32.7692
changing reinforcement signals experiment way creating nonstationary conditions 
changes occur environment modified optimal policy change 
wanted validate doing experiment static conditions starting block center continued pen divided regions policy learned configuration valid second 
changes occurred multi robot experiments distribution robots pen 
stationary conditions moving objects pen remained uniformly distributed environment effect learning robot 
case started evenly distributed pen changed position move center pen block move near home region causing lot difficulties learning robot deposit block move remote area learning robot 
behavior selection policy continuously adapted changes 
approach shares similarities differs reinforcement learning paradigm 
learning done manner similar monte carlo learning algorithms averaging observed returns eventually converge expected value :10.1.1.32.7692
learning algorithm decision derived stored past experiences current situation tree action takes form behavior activation reward internal measure performance deduced behavior time 
different reinforcement learning methods immediate sensations states commands actions 
sutton barto indicate reason restrict state representation immediate sensations typical applications expect state representation able inform agent :10.1.1.32.7692
approach different dynamic programming approaches aim finding probability state action table maximizing utility values 
approach shares similarities differs reinforcement learning paradigm 
learning done manner similar monte carlo learning algorithms averaging observed returns eventually converge expected value :10.1.1.32.7692
learning algorithm decision derived stored past experiences current situation tree action takes form behavior activation reward internal measure performance deduced behavior time 
different reinforcement learning methods immediate sensations states commands actions 
sutton barto indicate reason restrict state representation immediate sensations typical applications expect state representation able inform agent :10.1.1.32.7692
approach different dynamic programming approaches aim finding probability state action table maximizing utility values 
case behaviors serve abstraction state behavior action behavior selection decision process optimize fixed utility function 
result having state representation behavior combines representation environment sensory information behavior control policy behavior depends activation behavior organization 
behavior activation actions interaction model similar effect combine control signals issued newly activated behavior sensory inputs influence control signals issued newly activated behavior possibly behavior subsume 
works gating function fixed learning occurs behaviors supplying individual reinforcement functions 
works involve learning gating function 
main difference approaches uses history behavior percepts states selecting behaviors 
compared learning approaches pomdps approach tackles problem hidden state particularly important mobile robots equipped limited noisy sensors 
pomdps try compensate limitation markov property states maintaining belief states form probability distribution :10.1.1.32.7692
theory reinforcement learning non markovian model exact solutions medium sized pomdps generally considered infeasible difficult optimization task 
require exponential amount space time compute policy policy require exponential size representation exact solutions pomdps tens states take minutes days convergence obtained shown exact solution problems intractable 
cassandra pomdp model mobile robot navigate real office environment 
learning involved pomdp model valid static environment conditions 
russell norvig artificial intelligence modern approach 
singh jaakkola jordan learning state estimation partially observable markovian decision process proc 
th int conf 
machine learning 
sutton barto reinforcement learning :10.1.1.32.7692
mit press bradford books cambridge ma 

