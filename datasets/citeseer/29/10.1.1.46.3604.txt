multiagent framework planning reacting learning gerhard wei institut ur informatik technische universit unchen unchen germany tum de technical report september 
main approaches activity coordination multiagent systems plan coordination reactive coordination having speci advantages disadvantages 
describes framework called dyna aims combining approaches advantages retained disadvantages avoided 
key idea dyna multiagent variant single agent framework known dyna achieve combination agents capability learn information relevant planning reacting activities 
dyna ers integrated view planning reacting learning multiagent systems 
keywords 
keywords 
dyna multiagent systems reactive coordination plan coordination learning 
past years witnessed rapidly growing interest multiagent systems mas systems interacting intelligent autonomous entities called agents pursue goals carry tasks 
key question area mas multiple agents appropriately coordinate individual activities resulting activity patterns useful respect attaining goals accomplishing tasks 
contrary main approaches aim answering question concentrates plan coordination see reactive coordination see :10.1.1.40.8245
basic idea plan coordination agents jointly generate hypothetical activity sequences basis world model order nd advance acting real world actions possible actions promising 
advantage approach probability carrying unsuccessful activity sequences additionally may expensive kept low 
di culty approach limited accuracy world model agents 
di culty tends time consuming computation communication costs coordinating planning activities multiple agents grow enormously length planning sequences 
start state goal state reward table state action space sas 
start state goal state goal state reward state reward state table state action space sas 
figures show results sas sas respectively 
learning proceeds repeated execution episodes episode de ned sequence real working cycles transform start state goal state successful sequence non goal state unsuccessful sequence 
parameter setting follows sas avg reward random episodes experimental results sas :10.1.1.40.8245
avg reward random episodes experimental results sas :10.1.1.40.8245
state sas states 
initial values zero 
gure shows curves random random walk maximum length state action space hypothetical working cycles real working cycle hypothetical experiences real experience hypothetical experiences :10.1.1.40.8245
start state goal state goal state reward state reward state table state action space sas 
figures show results sas sas respectively 
learning proceeds repeated execution episodes episode de ned sequence real working cycles transform start state goal state successful sequence non goal state unsuccessful sequence 
parameter setting follows sas avg reward random episodes experimental results sas :10.1.1.40.8245
avg reward random episodes experimental results sas :10.1.1.40.8245
state sas states 
initial values zero 
gure shows curves random random walk maximum length state action space hypothetical working cycles real working cycle hypothetical experiences real experience hypothetical experiences :10.1.1.40.8245
data point shows mean reward obtained previous episodes averaged independent runs 
parameter setting follows sas avg reward random episodes experimental results sas :10.1.1.40.8245
avg reward random episodes experimental results sas :10.1.1.40.8245
state sas states 
initial values zero 
gure shows curves random random walk maximum length state action space hypothetical working cycles real working cycle hypothetical experiences real experience hypothetical experiences :10.1.1.40.8245
data point shows mean reward obtained previous episodes averaged independent runs 
main observations follows 
clearly performed better uninformed random search achieved average reward level case sas case sas 
indicates general performance capacity dyna 
clearly performed better uninformed random search achieved average reward level case sas case sas 
indicates general performance capacity dyna 
second reached maximum reward level di erent reward levels case sas 
indicates robustness dyna local performance maxima 
third performed better performed better :10.1.1.40.8245
average reached maximum reward level episodes case sas cycles case sas :10.1.1.40.8245
indicates learning hypothetical experiences contributes dyna performance 
dyna aims bringing plan reactive coordination explicitly integrates joint planning reacting learning single mas framework 
integration dyna approach unique different number related approaches multiagent activity coordination including approaches rely pure planning pure reaction see provided section approaches rely combination planning learning approaches rely combination reacting learning 
indicates general performance capacity dyna 
second reached maximum reward level di erent reward levels case sas 
indicates robustness dyna local performance maxima 
third performed better performed better :10.1.1.40.8245
average reached maximum reward level episodes case sas cycles case sas :10.1.1.40.8245
indicates learning hypothetical experiences contributes dyna performance 
dyna aims bringing plan reactive coordination explicitly integrates joint planning reacting learning single mas framework 
integration dyna approach unique different number related approaches multiagent activity coordination including approaches rely pure planning pure reaction see provided section approaches rely combination planning learning approaches rely combination reacting learning 
obviously dyna considered generalization approaches ers maximum coordination exibility 
uller editors decentralized pages 
amsterdam 
george communication interaction multi agent planning 
proceedings third national conference arti cial intelligence aaai pages 
goldberg matari coordinating mobile robot group behavior model interaction dynamics :10.1.1.40.8245
proceedings third international conference autonomous agents agents pages 
horling lesser vincent xuan 
diagnosis integral part multi agent adaptability 
technical report computer science department university massachussetts amherst 
