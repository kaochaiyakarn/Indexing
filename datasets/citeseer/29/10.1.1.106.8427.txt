machine learning automated text categorisation sebastiani delle italy automated categorisation classification texts topical categories long history dating back early 
late effective approach problem manually building automatic classifiers means techniques manually defining set rules encoding expert knowledge classify documents set categories 
production availability line documents automated text categorisation witnessed increased renewed interest prompted machine learning paradigm automatic classifier construction emerged definitely knowledge engineering approach 
machine learning paradigm general inductive process called learner automatically builds classifier called rule hypothesis learning set previously classified documents characteristics categories 
advantages approach effectiveness considerable savings terms expert domain independence 
survey look main approaches taken automatic text categorisation general machine learning paradigm 
advantages approach accuracy comparable human performance considerable savings terms expert intervention knowledge engineers domain experts needed 
current day tc may seen meeting point machine learning information retrieval ir mother disciplines concerned automated content document management 
tc enjoys quite rich literature fairly scattered 
international journals devoted special issues topic carbonell lewis hayes 
textbooks journals devoted tc textbook machine learning mitchell section brief section :10.1.1.55.2128:10.1.1.55.2128
note warn reader term automatic text classification literature mean quite different things ones discussed 
aside automatic assignment documents predefined set categories main topic term mean ii automatic definition set categories nowadays universally referred clustering iii automatic assignment documents set categories predefined task nowadays universally referred free text indexing 
reader keep mind especially browsing early literature dating period terminology settled 
organised follows 
news article clinton case filed politics gossip depending subjective judgment classifier 
mentioned notion relevance document category basically coincides notion relevance document information need ir saracevic 
single label multi label categorisation different constraints may enforced categorisation task depending application 
instance want integer element assigned exactly elements instance happens want categories evenly populated want populated certain degree see section 
importantly want integer exactly elements assigned element case baker mccallum cohen guthrie koller sahami joachims larkey li jain moulinier ganascia sch tze called single label case non overlapping categories case general case number categories may assigned document dubbed multi label case :10.1.1.21.988:10.1.1.21.988:10.1.1.21.7950:10.1.1.21.7950:10.1.1.33.4944
theoretical point view single label case general multi label case sense algorithm single label classification multi label classification simply transforming problem classification categories cm independent problems single label classification categories ci ci requires categories stochastically independent dj depend dj usually assumed case exceptions rule dealt section 
converse true general algorithm performing multi label classification machine learning automated text categorisation case single label classification 
fact obvious choose single best category categories classifier attached document ii documents equal 
general techniques consider applicable constraints discussed section enforced assume constraint sort enforced 
category pivoted document pivoted categorisation important distinction want fill decision matrix row time category pivoted categorisation fill column time document pivoted categorisation 
quite obviously distinction pragmatic conceptual important sense sets categories documents available entirety right start 
relevance choice method building classifier methods nn method section allow construction classifiers definite slant classification style 
suitable documents available time long span time case user submits document time categorisation submitting batch 
case categorisation task takes form ranking categories decreasing order estimated appropriateness document dj called category ranking classification line classification yang :10.1.1.109.2516
suitable consider possibility new category cm inserted previously existing set categories cm number documents evaluated categorisation means documents need evaluated cm larkey 
case categorisation task take form ranking documents decreasing order estimated appropriateness category cm symmetrically previous case may called document ranking classification 
commonly case documents submitted time common case newer categories dynamically crop 
specific techniques apply proportional thresholding method discussed section applies exception rule techniques discuss allow construction classifiers capable working mode 
sebastiani 
applications document categorisation automatic tc goes back early maron seminal 
number different applications 
briefly review important ones 
applications explicitly discuss reasons space speech categorisation means combination speech recognition tc schapire singer multimedia document categorisation analysis author identification literary texts unknown authorship forsyth gasp :10.1.1.134.3024
automatic essay grading larkey 
automatic indexing boolean information retrieval systems automatic text classifiers put application spawned early research field borko lustig field gray heaps maron automatic document indexing information retrieval ir systems relying controlled dictionary 
prominent example ir systems course boolean systems 
systems document assigned keywords keyphrases describing content keywords keyphrases belong finite set words called controlled dictionary consisting hierarchical thesaurus nasa thesaurus aerospace discipline mesh thesaurus covering medical field 
usually assignment performed trained human indexers extremely costly activity 
entries thesaurus viewed categories document indexing instance document categorisation task may addressed automatic techniques described 
recalling section note case typical constraint may keywords assigned document 
document pivoted categorisation typically best option new documents may classified available 
various automatic document classifiers explicitly addressed document indexing applications described literature see fuhr knorz fuhr fuhr robertson harding hartmann :10.1.1.31.3592
issue automatic indexing controlled dictionaries closely related topic automated metadata generation 
digital libraries usually interested tag documents metadata describe variety aspects creation date document type format availability 
usually metadata thematic role describe semantics document means bibliographic codes keywords keyphrases 
generation metadata may viewed problem document indexing controlled dictionary tackled means automatic tc techniques 
similarly mail filter trained classify previously filtered mail topical categories interest user cohen 
document filtering system may installed producer case role route information interested consumers consumer case role block delivery information deemed uninteresting user 
case system build update sebastiani profile consumer serves case common refer rest section single profile needed 
profile may initially specified user resembling standing ir query usually updated system feedback information provided user relevance non relevance delivered messages 
trec community lewis hull called adaptive filtering case user specified profile available called routing batch filtering depending documents ranked decreasing order estimated relevance just accepted rejected :10.1.1.16.3103
information science document filtering tradition dating back addressed systems varying degrees automation dealing multi consumer case discussed variously called selective dissemination information current awareness see korfhage chapter 
explosion availability digital information particularly internet boosted importance systems 
nowadays different contexts including creation personalised web newspapers junk mail blocking selection usenet news 
construction information filtering systems means machine learning techniques widely discussed literature see crestani hull hull lang lewis schapire sch tze singhal weiss :10.1.1.133.5960:10.1.1.133.5960:10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.48.5566:10.1.1.48.5566:10.1.1.64.7852:10.1.1.64.7852:10.1.1.22.6286:10.1.1.104.8304:10.1.1.104.8304
trec community lewis hull called adaptive filtering case user specified profile available called routing batch filtering depending documents ranked decreasing order estimated relevance just accepted rejected :10.1.1.16.3103
information science document filtering tradition dating back addressed systems varying degrees automation dealing multi consumer case discussed variously called selective dissemination information current awareness see korfhage chapter 
explosion availability digital information particularly internet boosted importance systems 
nowadays different contexts including creation personalised web newspapers junk mail blocking selection usenet news 
construction information filtering systems means machine learning techniques widely discussed literature see crestani hull hull lang lewis schapire sch tze singhal weiss :10.1.1.133.5960:10.1.1.133.5960:10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.48.5566:10.1.1.48.5566:10.1.1.64.7852:10.1.1.64.7852:10.1.1.22.6286:10.1.1.104.8304:10.1.1.104.8304
word sense disambiguation word sense disambiguation wsd refers activity finding occurrence text ambiguous polysemous word sense particular word occurrence 
instance english word bank may different senses bank england financial institution bank river engineering artifact 
wsd task decide senses occurrence bank week borrowed money bank refers 
wsd important number applications including indexing documents word senses words ir content document management applications 
quite obviously case exactly category needs assigned document document pivoted categorisation right choice 
wsd viewed tc task number different works literature see gale hearst 
wsd just example general issue resolving natural language ambiguities important problems computational linguistics 
instances problem may tackled means tc techniques lines discussed wsd context sensitive spelling correction prepositional phrase attachment part speech tagging word choice selection machine translation 
see excellent roth field :10.1.1.53.6829
machine learning automated text categorisation wheat farm wheat wheat commodity wheat export wheat wheat agriculture wheat wheat wheat wheat winter soft wheat fig 

classifier wheat category construe system keywords occurring documents indicated italic categories indicated small caps apt 
yahoo style search space categorisation automatic document categorisation lot interest possible internet applications 
yahoo style search space categorisation automatic document categorisation lot interest possible internet applications 
automatically classifying web pages sites categories commercial hierarchical catalogues embodied yahoo infoseek web documents way addressing generic query general purpose web search engine searcher may find easier navigate hierarchy categories issue search restrict search particular category interest 
automatically classifying web pages obvious advantages manual categorisation large subset web problematic say 
previous applications case typically want category populated set documents category centered categorisation may allow new categories added obsolete ones deleted 
automatic categorisation web pages sites yahoo hierarchical catalogues discussed papers see attardi baker mccallum chakrabarti mccallum mladeni extensively discussed section :10.1.1.14.5443:10.1.1.14.5443:10.1.1.62.359:10.1.1.29.8904
machine learning approach text categorisation main approach realisation automatic document classifiers consisted manual construction knowledge engineering techniques manually building expert system capable categorisation decisions 
expert system typically consisted set manually defined rules category type dnf boolean formula category effect document satisfied dnf boolean formula dnf standing disjunctive normal form classified category typical example approach construe system hayes built carnegie group reuters news agency 
sample rule construe illustrated effectiveness measured benchmark selected authors reported 
examples manual approach construction text classifiers goodman rau jacobs 
chosen set categories 
set categories updated trained professionals intervene classifier ported completely different domain set categories repeated anew 
hand suggested approach give effectiveness results hayes report breakeven result see section subset reuters test collection outperforms best classifiers built late machine learning techniques 
classifier tested dataset construe see table clear dataset selected reuters collection random favourable subset collection 
convincingly argued yang results allow confidently say effectiveness results may obtained general case :10.1.1.109.2516:10.1.1.109.2516
early new approach construction automatic document classifiers machine learning approach gained prominence eventually dominant see mitchell comprehensive machine learning :10.1.1.55.2128
approach general inductive process called learner automatically builds classifier category ci observing characteristics set documents previously classified manually ci domain expert characteristics inductive process characteristics novel document order classified ci 
machine learning terminology classification problem activity supervised learning learning process driven supervised knowledge categories training instances belong advantages approach previous evident engineering effort goes construction classifier automatic builder classifiers 
means original set categories updated system ported completely different domain needed inductive automatic construction new classifier different set manually classified documents required intervention domain expert knowledge engineer 
set categories updated trained professionals intervene classifier ported completely different domain set categories repeated anew 
hand suggested approach give effectiveness results hayes report breakeven result see section subset reuters test collection outperforms best classifiers built late machine learning techniques 
classifier tested dataset construe see table clear dataset selected reuters collection random favourable subset collection 
convincingly argued yang results allow confidently say effectiveness results may obtained general case :10.1.1.109.2516:10.1.1.109.2516
early new approach construction automatic document classifiers machine learning approach gained prominence eventually dominant see mitchell comprehensive machine learning :10.1.1.55.2128
approach general inductive process called learner automatically builds classifier category ci observing characteristics set documents previously classified manually ci domain expert characteristics inductive process characteristics novel document order classified ci 
machine learning terminology classification problem activity supervised learning learning process driven supervised knowledge categories training instances belong advantages approach previous evident engineering effort goes construction classifier automatic builder classifiers 
means original set categories updated system ported completely different domain needed inductive automatic construction new classifier different set manually classified documents required intervention domain expert knowledge engineer 
terms effectiveness classifiers built means machine learning techniques nowadays achieve impressive levels performance see section making automatic classification qualitatively economically viable alternative manual classification 
evaluation purposes stage classifier construction initial corpus typically divided sets necessarily equal size training set tr dg 
set example documents observing characteristics classifiers various categories induced test set te dg ds 
set purpose testing effectiveness induced classifiers 
document te fed classifiers classifier decisions compared expert decisions measure classification effectiveness values aij obtained classifiers match values provided experts 
note order give scientific character experiment documents te participate way inductive construction classifiers condition satisfied experimental results obtained probably mitchell page :10.1.1.55.2128
approach called train test approach 
alternative approach fold cross validation approach see mitchell page different classifiers induced partitioning initial corpus disjoint sets te tek iteratively applying train test approach pairs tri resulting classifiers different generated different training sets averaged way yield final classifier :10.1.1.55.2128
approach clearly reminiscent classifier committee approach discussed section usually adopted initial corpus small training process afford lose information test documents 
hardly case tc applications initial corpora usually large 
set purpose testing effectiveness induced classifiers 
document te fed classifiers classifier decisions compared expert decisions measure classification effectiveness values aij obtained classifiers match values provided experts 
note order give scientific character experiment documents te participate way inductive construction classifiers condition satisfied experimental results obtained probably mitchell page :10.1.1.55.2128
approach called train test approach 
alternative approach fold cross validation approach see mitchell page different classifiers induced partitioning initial corpus disjoint sets te tek iteratively applying train test approach pairs tri resulting classifiers different generated different training sets averaged way yield final classifier :10.1.1.55.2128
approach clearly reminiscent classifier committee approach discussed section usually adopted initial corpus small training process afford lose information test documents 
hardly case tc applications initial corpora usually large 
sebastiani train test approach case order optimise classifier internal parameters tuned testing values parameters yield best effectiveness 
order optimisation possible time scientific standards set dg may split true training set tr df classifier inductively constructed validation set va df dg called hold set repeated tests induced classifier aimed parameter optimisation performed corpus may define generality ci category ci percentage documents belong ci ci dj dj training set generality ci validation set generality gva ci test set generality gte ci category ci may defined obvious way substituting tr va respectively equation 
includes document submitted classifier operating phase 
machine learning automated text categorisation weighted index terms simply terms occur document differences various approaches accounted different ways understand term different ways weight terms 
typical choice issue identify terms words occurring document 
referred bag words approach document representation 
number experiments apt dumais lewis representations sophisticated yield worse categorisation effectiveness confirming similar results ir salton buckley :10.1.1.101.9086:10.1.1.101.9086:10.1.1.161.6020:10.1.1.161.6020:10.1.1.161.6020
particular number authors tried noun phrases individual words indexing terms experimental results date encouraging notion phrase motivated syntactically phrase grammar language see fuhr lewis hartmann statistically phrase grammatically composed set sequence words occur contiguously high frequency collection see sch tze :10.1.1.31.3592
quite convincingly lewis argues reason results indexing languages phrases superior semantic qualities inferior statistical qualities respect indexing languages single words 
notwithstanding results investigations effectiveness phrase indexing actively pursued 
true especially statistically motivated phrases cohen singer mladeni schapire case lewis argument applies smaller degree :10.1.1.104.8304:10.1.1.104.8304:10.1.1.14.6535
machine learning automated text categorisation weighted index terms simply terms occur document differences various approaches accounted different ways understand term different ways weight terms 
typical choice issue identify terms words occurring document 
referred bag words approach document representation 
number experiments apt dumais lewis representations sophisticated yield worse categorisation effectiveness confirming similar results ir salton buckley :10.1.1.101.9086:10.1.1.101.9086:10.1.1.161.6020:10.1.1.161.6020:10.1.1.161.6020
particular number authors tried noun phrases individual words indexing terms experimental results date encouraging notion phrase motivated syntactically phrase grammar language see fuhr lewis hartmann statistically phrase grammatically composed set sequence words occur contiguously high frequency collection see sch tze :10.1.1.31.3592
quite convincingly lewis argues reason results indexing languages phrases superior semantic qualities inferior statistical qualities respect indexing languages single words 
notwithstanding results investigations effectiveness phrase indexing actively pursued 
true especially statistically motivated phrases cohen singer mladeni schapire case lewis argument applies smaller degree :10.1.1.104.8304:10.1.1.104.8304:10.1.1.14.6535
issue weights usually range loss generality assume 
number experiments apt dumais lewis representations sophisticated yield worse categorisation effectiveness confirming similar results ir salton buckley :10.1.1.101.9086:10.1.1.101.9086:10.1.1.161.6020:10.1.1.161.6020:10.1.1.161.6020
particular number authors tried noun phrases individual words indexing terms experimental results date encouraging notion phrase motivated syntactically phrase grammar language see fuhr lewis hartmann statistically phrase grammatically composed set sequence words occur contiguously high frequency collection see sch tze :10.1.1.31.3592
quite convincingly lewis argues reason results indexing languages phrases superior semantic qualities inferior statistical qualities respect indexing languages single words 
notwithstanding results investigations effectiveness phrase indexing actively pursued 
true especially statistically motivated phrases cohen singer mladeni schapire case lewis argument applies smaller degree :10.1.1.104.8304:10.1.1.104.8304:10.1.1.14.6535
issue weights usually range loss generality assume 
particular case authors apt koller sahami lewis ringuette li jain moulinier moulinier ganascia schapire singer sch tze binary weights due symbolic non numeric nature learning systems employ case denotes presence absence term document :10.1.1.134.3024:10.1.1.134.3024:10.1.1.134.3024:10.1.1.21.988:10.1.1.21.988:10.1.1.49.860:10.1.1.49.860
frequent case non binary indexing determining weight wkj term tk document dj ir style indexing technique represents document vector weighted terms may 
times standard tfidf weighting function see salton buckley defined tfidf tk dj tk dj log tr tr tk tk dj denotes number times tk occurs dj tr tk denotes number documents tr tk occurs known document frequency term tk :10.1.1.101.9086:10.1.1.101.9086
quite convincingly lewis argues reason results indexing languages phrases superior semantic qualities inferior statistical qualities respect indexing languages single words 
notwithstanding results investigations effectiveness phrase indexing actively pursued 
true especially statistically motivated phrases cohen singer mladeni schapire case lewis argument applies smaller degree :10.1.1.104.8304:10.1.1.104.8304:10.1.1.14.6535
issue weights usually range loss generality assume 
particular case authors apt koller sahami lewis ringuette li jain moulinier moulinier ganascia schapire singer sch tze binary weights due symbolic non numeric nature learning systems employ case denotes presence absence term document :10.1.1.134.3024:10.1.1.134.3024:10.1.1.134.3024:10.1.1.21.988:10.1.1.21.988:10.1.1.49.860:10.1.1.49.860
frequent case non binary indexing determining weight wkj term tk document dj ir style indexing technique represents document vector weighted terms may 
times standard tfidf weighting function see salton buckley defined tfidf tk dj tk dj log tr tr tk tk dj denotes number times tk occurs dj tr tk denotes number documents tr tk occurs known document frequency term tk :10.1.1.101.9086:10.1.1.101.9086
function encodes intuitions term occurs document representative sebastiani content document ii documents term occurs discriminating note formula indexing formulae weights importance term document terms occurrence considerations null importance order terms occur document syntactic role play words semantics document reduced lexical semantics terms occur disregarding issue compositional semantics exception representation techniques foil system cohen sleeping experts system cohen singer :10.1.1.25.4340:10.1.1.14.6535:10.1.1.14.6535
order weights fall interval documents represented vectors equal length weights resulting tfidf normalised cosine normalisation tfidf tk dj wkj tfidf ts dj set terms occur tr 
true especially statistically motivated phrases cohen singer mladeni schapire case lewis argument applies smaller degree :10.1.1.104.8304:10.1.1.104.8304:10.1.1.14.6535
issue weights usually range loss generality assume 
particular case authors apt koller sahami lewis ringuette li jain moulinier moulinier ganascia schapire singer sch tze binary weights due symbolic non numeric nature learning systems employ case denotes presence absence term document :10.1.1.134.3024:10.1.1.134.3024:10.1.1.134.3024:10.1.1.21.988:10.1.1.21.988:10.1.1.49.860:10.1.1.49.860
frequent case non binary indexing determining weight wkj term tk document dj ir style indexing technique represents document vector weighted terms may 
times standard tfidf weighting function see salton buckley defined tfidf tk dj tk dj log tr tr tk tk dj denotes number times tk occurs dj tr tk denotes number documents tr tk occurs known document frequency term tk :10.1.1.101.9086:10.1.1.101.9086
function encodes intuitions term occurs document representative sebastiani content document ii documents term occurs discriminating note formula indexing formulae weights importance term document terms occurrence considerations null importance order terms occur document syntactic role play words semantics document reduced lexical semantics terms occur disregarding issue compositional semantics exception representation techniques foil system cohen sleeping experts system cohen singer :10.1.1.25.4340:10.1.1.14.6535:10.1.1.14.6535
order weights fall interval documents represented vectors equal length weights resulting tfidf normalised cosine normalisation tfidf tk dj wkj tfidf ts dj set terms occur tr 
tfidf far popular indexing functions including probabilistic indexing methods fuhr techniques indexing structured documents larkey croft :10.1.1.47.8517
functions different tfidf especially needed training set available entirety start document frequency data unavailable adaptive filtering case empirical substitutes tfidf usually employed dagan section :10.1.1.11.5378:10.1.1.11.5378
issue weights usually range loss generality assume 
particular case authors apt koller sahami lewis ringuette li jain moulinier moulinier ganascia schapire singer sch tze binary weights due symbolic non numeric nature learning systems employ case denotes presence absence term document :10.1.1.134.3024:10.1.1.134.3024:10.1.1.134.3024:10.1.1.21.988:10.1.1.21.988:10.1.1.49.860:10.1.1.49.860
frequent case non binary indexing determining weight wkj term tk document dj ir style indexing technique represents document vector weighted terms may 
times standard tfidf weighting function see salton buckley defined tfidf tk dj tk dj log tr tr tk tk dj denotes number times tk occurs dj tr tk denotes number documents tr tk occurs known document frequency term tk :10.1.1.101.9086:10.1.1.101.9086
function encodes intuitions term occurs document representative sebastiani content document ii documents term occurs discriminating note formula indexing formulae weights importance term document terms occurrence considerations null importance order terms occur document syntactic role play words semantics document reduced lexical semantics terms occur disregarding issue compositional semantics exception representation techniques foil system cohen sleeping experts system cohen singer :10.1.1.25.4340:10.1.1.14.6535:10.1.1.14.6535
order weights fall interval documents represented vectors equal length weights resulting tfidf normalised cosine normalisation tfidf tk dj wkj tfidf ts dj set terms occur tr 
tfidf far popular indexing functions including probabilistic indexing methods fuhr techniques indexing structured documents larkey croft :10.1.1.47.8517
functions different tfidf especially needed training set available entirety start document frequency data unavailable adaptive filtering case empirical substitutes tfidf usually employed dagan section :10.1.1.11.5378:10.1.1.11.5378
indexing removal function words topic neutral words articles prepositions conjunctions performed 
frequent case non binary indexing determining weight wkj term tk document dj ir style indexing technique represents document vector weighted terms may 
times standard tfidf weighting function see salton buckley defined tfidf tk dj tk dj log tr tr tk tk dj denotes number times tk occurs dj tr tk denotes number documents tr tk occurs known document frequency term tk :10.1.1.101.9086:10.1.1.101.9086
function encodes intuitions term occurs document representative sebastiani content document ii documents term occurs discriminating note formula indexing formulae weights importance term document terms occurrence considerations null importance order terms occur document syntactic role play words semantics document reduced lexical semantics terms occur disregarding issue compositional semantics exception representation techniques foil system cohen sleeping experts system cohen singer :10.1.1.25.4340:10.1.1.14.6535:10.1.1.14.6535
order weights fall interval documents represented vectors equal length weights resulting tfidf normalised cosine normalisation tfidf tk dj wkj tfidf ts dj set terms occur tr 
tfidf far popular indexing functions including probabilistic indexing methods fuhr techniques indexing structured documents larkey croft :10.1.1.47.8517
functions different tfidf especially needed training set available entirety start document frequency data unavailable adaptive filtering case empirical substitutes tfidf usually employed dagan section :10.1.1.11.5378:10.1.1.11.5378
indexing removal function words topic neutral words articles prepositions conjunctions performed 
concerning stemming collapsing words share morphological root controversial beneficial step tc 
similarly unsupervised term clustering see section instance stemming reported hurt effectiveness baker mccallum tendency adopt larkey croft ng sch tze wiener yang considerably reduces dimensionality term space see section level stochastic dependence terms see section :10.1.1.47.8517:10.1.1.47.8517:10.1.1.54.6608:10.1.1.54.6608:10.1.1.109.2516:10.1.1.109.2516
times standard tfidf weighting function see salton buckley defined tfidf tk dj tk dj log tr tr tk tk dj denotes number times tk occurs dj tr tk denotes number documents tr tk occurs known document frequency term tk :10.1.1.101.9086:10.1.1.101.9086
function encodes intuitions term occurs document representative sebastiani content document ii documents term occurs discriminating note formula indexing formulae weights importance term document terms occurrence considerations null importance order terms occur document syntactic role play words semantics document reduced lexical semantics terms occur disregarding issue compositional semantics exception representation techniques foil system cohen sleeping experts system cohen singer :10.1.1.25.4340:10.1.1.14.6535:10.1.1.14.6535
order weights fall interval documents represented vectors equal length weights resulting tfidf normalised cosine normalisation tfidf tk dj wkj tfidf ts dj set terms occur tr 
tfidf far popular indexing functions including probabilistic indexing methods fuhr techniques indexing structured documents larkey croft :10.1.1.47.8517
functions different tfidf especially needed training set available entirety start document frequency data unavailable adaptive filtering case empirical substitutes tfidf usually employed dagan section :10.1.1.11.5378:10.1.1.11.5378
indexing removal function words topic neutral words articles prepositions conjunctions performed 
concerning stemming collapsing words share morphological root controversial beneficial step tc 
similarly unsupervised term clustering see section instance stemming reported hurt effectiveness baker mccallum tendency adopt larkey croft ng sch tze wiener yang considerably reduces dimensionality term space see section level stochastic dependence terms see section :10.1.1.47.8517:10.1.1.47.8517:10.1.1.54.6608:10.1.1.54.6608:10.1.1.109.2516:10.1.1.109.2516
depending application full text document selected parts may indexed 
tfidf far popular indexing functions including probabilistic indexing methods fuhr techniques indexing structured documents larkey croft :10.1.1.47.8517
functions different tfidf especially needed training set available entirety start document frequency data unavailable adaptive filtering case empirical substitutes tfidf usually employed dagan section :10.1.1.11.5378:10.1.1.11.5378
indexing removal function words topic neutral words articles prepositions conjunctions performed 
concerning stemming collapsing words share morphological root controversial beneficial step tc 
similarly unsupervised term clustering see section instance stemming reported hurt effectiveness baker mccallum tendency adopt larkey croft ng sch tze wiener yang considerably reduces dimensionality term space see section level stochastic dependence terms see section :10.1.1.47.8517:10.1.1.47.8517:10.1.1.54.6608:10.1.1.54.6608:10.1.1.109.2516:10.1.1.109.2516
depending application full text document selected parts may indexed 
option rule exceptions exist 
instance patent categorisation application larkey considers title lines background summary section containing claims novelty described invention 
approach possible fact documents describing patents structured 
depending application full text document selected parts may indexed 
option rule exceptions exist 
instance patent categorisation application larkey considers title lines background summary section containing claims novelty described invention 
approach possible fact documents describing patents structured 
similarly document title available possible pay extra importance words appearing apt cohen singer weiss tfidf function class functions differ terms normalisation correction factors applied :10.1.1.14.6535
formula just possible instances class see salton buckley variations theme :10.1.1.101.9086:10.1.1.101.9086
machine learning automated text categorisation 
applications documents flat identification relevant part document non obvious task 
dimensionality reduction ir tc high dimensionality term space fact number terms occur corpus high may problematic 
option rule exceptions exist 
instance patent categorisation application larkey considers title lines background summary section containing claims novelty described invention 
approach possible fact documents describing patents structured 
similarly document title available possible pay extra importance words appearing apt cohen singer weiss tfidf function class functions differ terms normalisation correction factors applied :10.1.1.14.6535
formula just possible instances class see salton buckley variations theme :10.1.1.101.9086:10.1.1.101.9086
machine learning automated text categorisation 
applications documents flat identification relevant part document non obvious task 
dimensionality reduction ir tc high dimensionality term space fact number terms occur corpus high may problematic 
fact typical matching algorithms ir cosine matching scale high values said sophisticated learning algorithms classifier induction llsf algorithm yang chute 
example classifier category cars sale trained just positive examples concerned sale yellow car resulting classifier deem clearly contingent property particular training data property category 
experimentation shown order avoid overfitting number training examples roughly proportional number terms needed fuhr buckley page suggested training examples term may needed tc tasks 
means dr performed overfitting may avoided smaller amount training examples 
various dr functions information theory linear algebra literature proposed relative merits tested experimentally evaluating variation categorisation effectiveness classifier undergoes application function term space operates 
quite distinct ways viewing dr depending task approached locally individual category isolation globally local dimensionality reduction category ci terms chosen support classification category ci see apt lewis ringuette li jain ng sch tze wiener :10.1.1.49.860:10.1.1.49.860:10.1.1.49.860:10.1.1.54.6608:10.1.1.54.6608:10.1.1.54.6608
conceptually mean document dj different representation category ci practice means different subsets dj original representation classifying different categories 
authors adopted approach tend adopt values 
overfitting problem referred curse dimensionality 
third way viewing dr exemplified koller sahami briefly discussed section :10.1.1.21.988:10.1.1.21.988
quite distinct ways viewing dr depending task approached locally individual category isolation globally local dimensionality reduction category ci terms chosen support classification category ci see apt lewis ringuette li jain ng sch tze wiener :10.1.1.49.860:10.1.1.49.860:10.1.1.49.860:10.1.1.54.6608:10.1.1.54.6608:10.1.1.54.6608
conceptually mean document dj different representation category ci practice means different subsets dj original representation classifying different categories 
authors adopted approach tend adopt values 
overfitting problem referred curse dimensionality 
third way viewing dr exemplified koller sahami briefly discussed section :10.1.1.21.988:10.1.1.21.988
sebastiani global dimensionality reduction terms chosen support classification categories cm see mladeni yang yang pedersen :10.1.1.109.2516:10.1.1.109.2516
distinction usually impact kind technique chosen dr dr techniques local global dr alike 
second orthogonal distinction may drawn terms nature resulting terms dimensionality reduction term selection chosen terms subset original terms dimensionality reduction term extraction terms subset original terms 
usually homogeneous original terms words chosen terms may words obtained combinations transformations original ones 
conceptually mean document dj different representation category ci practice means different subsets dj original representation classifying different categories 
authors adopted approach tend adopt values 
overfitting problem referred curse dimensionality 
third way viewing dr exemplified koller sahami briefly discussed section :10.1.1.21.988:10.1.1.21.988
sebastiani global dimensionality reduction terms chosen support classification categories cm see mladeni yang yang pedersen :10.1.1.109.2516:10.1.1.109.2516
distinction usually impact kind technique chosen dr dr techniques local global dr alike 
second orthogonal distinction may drawn terms nature resulting terms dimensionality reduction term selection chosen terms subset original terms dimensionality reduction term extraction terms subset original terms 
usually homogeneous original terms words chosen terms may words obtained combinations transformations original ones 
quite obviously previous distinction different ways doing dr tackled quite distinct techniques tackle separately sections 
usually homogeneous original terms words chosen terms may words obtained combinations transformations original ones 
quite obviously previous distinction different ways doing dr tackled quite distinct techniques tackle separately sections 
dimensionality reduction term selection fixed techniques term selection called term space reduction tsr select original set terms terms document indexing yield smallest reduction effectiveness respect effectiveness obtained full blown representations 
results published literature yang pedersen shown moderate increase effectiveness term space reduction performed depending classifier reduction tsr technique 
moulinier experimented called wrapper term selection method term set identified means learning method inducing classifier john :10.1.1.30.3875
starting initial term set new term set generated adding removing term 
new term set generated classifier induced tested validation set 
term set results best effectiveness chosen 
approach advantage definition tuned learning algorithm local dimensionality reduction different numbers terms different categories may chosen depending category easily separable 
new term set generated classifier induced tested validation set 
term set results best effectiveness chosen 
approach advantage definition tuned learning algorithm local dimensionality reduction different numbers terms different categories may chosen depending category easily separable 
approach surely qualifies brute force method number possible different term sets renders cost prohibitive standard tc applications 
computationally easier alternative wrapper approach filtering approach john keeping terms score highest predetermined numerical function measures importance term categorisation task :10.1.1.30.3875
explore solution rest section 
document frequency 
simple surprisingly effective global tsr function document frequency tr tk term tk apt machine learning automated text categorisation systematically studied yang pedersen 
note section interpret event space set documents training set probabilistic terms document frequency may written tk 
simple surprisingly effective global tsr function document frequency tr tk term tk apt machine learning automated text categorisation systematically studied yang pedersen 
note section interpret event space set documents training set probabilistic terms document frequency may written tk 
yang pedersen shown adopted classifier initial corpus tr tk term selection technique possible reduce dimensionality term space factor loss effectiveness reduction factor brings just small loss 
result state basically valuable terms categorisation occur frequently collection 
contradict ir informative terms low medium document frequency salton buckley :10.1.1.101.9086:10.1.1.101.9086
results contradict known see salton overwhelming majority words occur corpus extremely low document frequency means performing tsr factor document frequency words removed words low medium high document frequency preserved 
course word removal needs performed form dimensionality reduction attempted topic neutral words remain reduction mladeni 
note slightly empirical form term selection document frequency adopted authors remove consideration terms occur training documents popular values range form dimensionality reduction ittner applying sophisticated form dumais li jain wiener :10.1.1.119.4275:10.1.1.54.6608:10.1.1.54.6608:10.1.1.161.6020:10.1.1.161.6020
variant policy removing considerations terms occur times collection dagan joachims joachims popular values ranging baker mccallum apt cohen :10.1.1.25.4340:10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378:10.1.1.21.7950:10.1.1.21.7950
result state basically valuable terms categorisation occur frequently collection 
contradict ir informative terms low medium document frequency salton buckley :10.1.1.101.9086:10.1.1.101.9086
results contradict known see salton overwhelming majority words occur corpus extremely low document frequency means performing tsr factor document frequency words removed words low medium high document frequency preserved 
course word removal needs performed form dimensionality reduction attempted topic neutral words remain reduction mladeni 
note slightly empirical form term selection document frequency adopted authors remove consideration terms occur training documents popular values range form dimensionality reduction ittner applying sophisticated form dumais li jain wiener :10.1.1.119.4275:10.1.1.54.6608:10.1.1.54.6608:10.1.1.161.6020:10.1.1.161.6020
variant policy removing considerations terms occur times collection dagan joachims joachims popular values ranging baker mccallum apt cohen :10.1.1.25.4340:10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378:10.1.1.21.7950:10.1.1.21.7950
information theoretic term selection functions 
sophisticated information theoretic functions literature chi square sch tze yang pedersen yang liu correlation coefficient ng ruiz srinivasan information gain larkey lewis lewis ringuette mladeni moulinier ganascia yang pedersen yang liu mutual information dumais lam larkey croft lewis ringuette li jain moulinier ruiz srinivasan yang pedersen odds ratio mladeni ruiz srinivasan relevancy score wiener simplified chi square fuhr :10.1.1.47.8517:10.1.1.47.8517:10.1.1.47.8517:10.1.1.47.8517:10.1.1.49.860:10.1.1.49.860:10.1.1.11.9519:10.1.1.11.9519:10.1.1.54.6608:10.1.1.54.6608:10.1.1.161.6020:10.1.1.161.6020
mathematical definitions measures summarised convenience table 
contradict ir informative terms low medium document frequency salton buckley :10.1.1.101.9086:10.1.1.101.9086
results contradict known see salton overwhelming majority words occur corpus extremely low document frequency means performing tsr factor document frequency words removed words low medium high document frequency preserved 
course word removal needs performed form dimensionality reduction attempted topic neutral words remain reduction mladeni 
note slightly empirical form term selection document frequency adopted authors remove consideration terms occur training documents popular values range form dimensionality reduction ittner applying sophisticated form dumais li jain wiener :10.1.1.119.4275:10.1.1.54.6608:10.1.1.54.6608:10.1.1.161.6020:10.1.1.161.6020
variant policy removing considerations terms occur times collection dagan joachims joachims popular values ranging baker mccallum apt cohen :10.1.1.25.4340:10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378:10.1.1.21.7950:10.1.1.21.7950
information theoretic term selection functions 
sophisticated information theoretic functions literature chi square sch tze yang pedersen yang liu correlation coefficient ng ruiz srinivasan information gain larkey lewis lewis ringuette mladeni moulinier ganascia yang pedersen yang liu mutual information dumais lam larkey croft lewis ringuette li jain moulinier ruiz srinivasan yang pedersen odds ratio mladeni ruiz srinivasan relevancy score wiener simplified chi square fuhr :10.1.1.47.8517:10.1.1.47.8517:10.1.1.47.8517:10.1.1.47.8517:10.1.1.49.860:10.1.1.49.860:10.1.1.11.9519:10.1.1.11.9519:10.1.1.54.6608:10.1.1.54.6608:10.1.1.161.6020:10.1.1.161.6020
mathematical definitions measures summarised convenience table 
probabilities interpreted usual event space documents tk ci means probability random document term tk occur belongs category ci estimated counting occurrences training set 
course word removal needs performed form dimensionality reduction attempted topic neutral words remain reduction mladeni 
note slightly empirical form term selection document frequency adopted authors remove consideration terms occur training documents popular values range form dimensionality reduction ittner applying sophisticated form dumais li jain wiener :10.1.1.119.4275:10.1.1.54.6608:10.1.1.54.6608:10.1.1.161.6020:10.1.1.161.6020
variant policy removing considerations terms occur times collection dagan joachims joachims popular values ranging baker mccallum apt cohen :10.1.1.25.4340:10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378:10.1.1.21.7950:10.1.1.21.7950
information theoretic term selection functions 
sophisticated information theoretic functions literature chi square sch tze yang pedersen yang liu correlation coefficient ng ruiz srinivasan information gain larkey lewis lewis ringuette mladeni moulinier ganascia yang pedersen yang liu mutual information dumais lam larkey croft lewis ringuette li jain moulinier ruiz srinivasan yang pedersen odds ratio mladeni ruiz srinivasan relevancy score wiener simplified chi square fuhr :10.1.1.47.8517:10.1.1.47.8517:10.1.1.47.8517:10.1.1.47.8517:10.1.1.49.860:10.1.1.49.860:10.1.1.11.9519:10.1.1.11.9519:10.1.1.54.6608:10.1.1.54.6608:10.1.1.161.6020:10.1.1.161.6020
mathematical definitions measures summarised convenience table 
probabilities interpreted usual event space documents tk ci means probability random document term tk occur belongs category ci estimated counting occurrences training set 
functions try capture intuition valuable terms categorisation ci distributed differently sets positive negative examples category 
sebastiani function denoted mathematical form document frequency tk ci tk ci information gain ig tk ci tk ci log tk ci ci tk tk ci log tk ci ci tk mutual information mi tk ci log tk ci tk ci chi square tk ci correlation coefficient cc tk ci tk ci tk ci tk ci tk ci tk tk ci ci tk ci tk ci tk ci tk ci tk tk ci ci relevancy score rs tk ci log tk ci tk ci odds ratio tk ci tk ci tk ci tk ci tk ci simplified chi square tk ci tk ci tk ci tk ci tk ci table 
experimental results show approach perform better 
anyway lsi approaches shown perform better simple term selection technique relevancy score measure see table 
sch tze experimentally compared lsi term extraction term selection different classifier induction techniques linear discriminant analysis logistic regression neural networks routing application 
results showed lsi far effective techniques methods performed equally case neural network classifier 
works tc lsi similar term extraction techniques hull li jain sch tze weigend yang :10.1.1.133.5960:10.1.1.133.5960:10.1.1.159.7380

methods inductive construction problem inductive construction text classifier tackled variety different ways 
describe detail methods proven popular literature time try mention existence alternative standard approaches 
inductive construction classifier category ci usually consists different phases definition function document dj returns categorisation status value number roughly speaking represents evidence fact dj classified ci 
sebastiani probabilistic classifiers start presentation methods classifier induction discussion probabilistic approach 
apt start text classifier reported literature maron probabilistic 
probabilistic classifiers view dj terms probability ci dj document represented vector dj binary weighted terms falls category ci attempt compute probability application bayes theorem ci dj ci dj ci dj equation probabilities interpreted space documents accordingly dj represents probability randomly picked document vector dj representation ci probability randomly picked document falls category ci 
estimation dj ci equation problematic number possible vectors dj high holds dj reasons clear shortly concern 
order alleviate problem common assumption coordinates document vector viewed random variables statistically independent independence assumption encoded equation dj ci wkj ci probabilistic classifiers assumption usually called na bayes classifiers account probabilistic approaches tc reported literature see joachims koller sahami larkey croft lewis lewis gale li jain robertson harding :10.1.1.47.8517:10.1.1.47.8517:10.1.1.47.8517:10.1.1.47.8517:10.1.1.21.988
na character classifier due fact usually assumption quite obviously verified naturally occurring document corpora 
best known na bayes approaches binary independence classifier robertson sparck jones results binary valued vector representations documents 
case write pki short ci wkj ci factors equation may written wkj ci wkj ki pki wkj pki pki wkj pki may observe tc application considering document space partitioned categories ci complement ci ci dj ci dj 
plug equations equation take logs obtain log ci dj log ci cooper pointed full independence assumption equation na bayes classifier assumption wired na bayes weaker linked dependence assumption may written dj ci dj ci wkj ci wkj ci machine learning automated text categorisation pki wkj log log pki log dj pki log ci dj log ci pki wkj log log pki log dj ki write pki short ci 
note general classification document require compute sum factors presence wkj log pki ki ki pki imply fact factors wkj may disregarded usually accounts vast majority document vectors usually sparse 
binary independence classifier just illustrated just variants na bayes approach common denominator may taken equation 
lewis excellent roadmap various directions research na bayes classifiers taken 
important directions lewis highlights ones aiming relax constraint term vectors representing documents 
looks quite natural weighted indexing techniques see fuhr fuhr buckley salton buckley account importance term tk document dj play key role ir :10.1.1.101.9086:10.1.1.101.9086
introduce document length normalisation model 
fact clear equation value log ci dj ci dj tends higher long documents documents wkj values semantic relatedness ci 
document length account easy approaches classification see section problematic probabilistic ones see lewis section :10.1.1.11.8264:10.1.1.11.8264
possible answer true fixed thresholding method section adopted 
important directions lewis highlights ones aiming relax constraint term vectors representing documents 
looks quite natural weighted indexing techniques see fuhr fuhr buckley salton buckley account importance term tk document dj play key role ir :10.1.1.101.9086:10.1.1.101.9086
introduce document length normalisation model 
fact clear equation value log ci dj ci dj tends higher long documents documents wkj values semantic relatedness ci 
document length account easy approaches classification see section problematic probabilistic ones see lewis section :10.1.1.11.8264:10.1.1.11.8264
possible answer true fixed thresholding method section adopted 
fact case categories document dj chosen 
means fixed document dj third factor formula different different categories may influence choice categories file dj 
sebastiani adopted baker mccallum switch interpretation na bayes documents events terms events 
fact case categories document dj chosen 
means fixed document dj third factor formula different different categories may influence choice categories file dj 
sebastiani adopted baker mccallum switch interpretation na bayes documents events terms events 
case multiple binomial random variables standing terms case single multinomial random variable standing documents 
accounts document length naturally noted lewis drawback different occurrences word document viewed independent assumption strikingly implausible standard word independence assumption :10.1.1.11.8264
similar solution suffering problem proposed guthrie 
relax independence assumption 
may hardest route follow inevitably produces classifiers higher computational cost characterised harder parameter estimation problems 
earlier efforts direction field probabilistic ir van rijsbergen shown performance improvements hoped 
study joachims shown na bayes classifier working term space consisting terms ranked lowest information gain measure performed way better random classifier 
showing uninformative words useful text categorisation joachims concluded may indicate informative terms useful bayes classifier 
baker mccallum discussed section contradict hypothesis may said lewis ringuette 
safe conclude systematic experiments needed say conclusive issue 
thorough discussion na bayes classifiers variations see excellent lewis :10.1.1.11.8264
decision tree classifiers probabilistic induction methods essentially quantitative numeric nature effective may readily interpretable humans 
class algorithms suffer agriculture wheat wheat machine learning automated text categorisation soft wheat farm winter wheat commodity export wheat wheat wheat fig 

decision tree equivalent dnf rule 

decision tree equivalent dnf rule 
edges labelled terms leaves labelled categories denotes negation 
problem symbolic non numeric algorithms inductive rule learners discuss section decision tree inducers important examples 
decision tree text classifier see mitchell section consists tree internal nodes labelled terms branches departing labelled tests weight term representation test document leaf nodes labelled necessarily different categories :10.1.1.55.2128:10.1.1.55.2128
classifier test document dj recursively testing weights terms labeling internal nodes representation dj leaf node reached label leaf node assigned dj 
text classifiers assume binary document representation consist binary trees 
example tree illustrated 
number standard packages induction decision tree training set decision tree approaches tc package 
classifier test document dj recursively testing weights terms labeling internal nodes representation dj leaf node reached label leaf node assigned dj 
text classifiers assume binary document representation consist binary trees 
example tree illustrated 
number standard packages induction decision tree training set decision tree approaches tc package 
popular packages id fuhr cohen cohen singer joachims lewis catlett li jain :10.1.1.52.2415:10.1.1.33.4944:10.1.1.14.6535
tc experimental decision tree packages include dumais lewis ringuette weiss :10.1.1.49.860:10.1.1.49.860:10.1.1.64.7852:10.1.1.64.7852:10.1.1.161.6020:10.1.1.161.6020:10.1.1.161.6020
possible procedure induction decision tree category ci set training examples consists divide conquer strategy recursively checking training examples label ci ci ii selecting term tk partitioning training examples classes documents value tk placing class separate subtree 
process recursively repeated subtrees leaf node sebastiani tree generated contain training examples assigned category ci chosen label leaf node 
key step process choice term tk operate partition choice generally information gain entropy criterion 
text classifiers assume binary document representation consist binary trees 
example tree illustrated 
number standard packages induction decision tree training set decision tree approaches tc package 
popular packages id fuhr cohen cohen singer joachims lewis catlett li jain :10.1.1.52.2415:10.1.1.33.4944:10.1.1.14.6535
tc experimental decision tree packages include dumais lewis ringuette weiss :10.1.1.49.860:10.1.1.49.860:10.1.1.64.7852:10.1.1.64.7852:10.1.1.161.6020:10.1.1.161.6020:10.1.1.161.6020
possible procedure induction decision tree category ci set training examples consists divide conquer strategy recursively checking training examples label ci ci ii selecting term tk partitioning training examples classes documents value tk placing class separate subtree 
process recursively repeated subtrees leaf node sebastiani tree generated contain training examples assigned category ci chosen label leaf node 
key step process choice term tk operate partition choice generally information gain entropy criterion 
fully grown tree may prone overfitting branches may excessively specific training data 
process recursively repeated subtrees leaf node sebastiani tree generated contain training examples assigned category ci chosen label leaf node 
key step process choice term tk operate partition choice generally information gain entropy criterion 
fully grown tree may prone overfitting branches may excessively specific training data 
decision tree induction method includes method growing tree pruning removing overly specific branches minimise probability misclassifying test documents 
variations basic schema tree induction abound interested reader referred mitchell section :10.1.1.55.2128:10.1.1.55.2128
decision tree text classifiers main classification tool fuhr lewis catlett lewis ringuette baseline classifiers cohen singer joachims members classifier committees li jain schapire schapire singer weiss :10.1.1.134.3024:10.1.1.134.3024:10.1.1.49.860:10.1.1.64.7852:10.1.1.64.7852:10.1.1.52.2415:10.1.1.104.8304:10.1.1.104.8304:10.1.1.14.6535
air project 
decision trees tc literature special place occupied air system fuhr 
system important constitutes final result air project important history tc 
key step process choice term tk operate partition choice generally information gain entropy criterion 
fully grown tree may prone overfitting branches may excessively specific training data 
decision tree induction method includes method growing tree pruning removing overly specific branches minimise probability misclassifying test documents 
variations basic schema tree induction abound interested reader referred mitchell section :10.1.1.55.2128:10.1.1.55.2128
decision tree text classifiers main classification tool fuhr lewis catlett lewis ringuette baseline classifiers cohen singer joachims members classifier committees li jain schapire schapire singer weiss :10.1.1.134.3024:10.1.1.134.3024:10.1.1.49.860:10.1.1.64.7852:10.1.1.64.7852:10.1.1.52.2415:10.1.1.104.8304:10.1.1.104.8304:10.1.1.14.6535
air project 
decision trees tc literature special place occupied air system fuhr 
system important constitutes final result air project important history tc 
air project spanning duration years fuhr knorz fuhr fuhr produced system employed classification corpora scientific literature documents important theoretical spin offs field probabilistic indexing fuhr fuhr buckley approach tc taken air known darmstadt indexing approach dia 
set clauses dnf classifier ci obviously scores tremendously high terms overfitting 
induction algorithm employs process generalisation rule simplified series modifications removing premises clauses merging clauses maximise compactness time affecting covering property classifier 
process pruning phase similar spirit employed decision trees applied ability correctly classify training examples traded generality 
individual dnf rule learners vary widely terms methods heuristics criteria employed generalisation pruning 
inductive dnf rule learners applied tc moulinier ganascia dl li ripper cohen cohen cohen singer moulinier swap apt :10.1.1.25.4340:10.1.1.33.4944:10.1.1.14.6535:10.1.1.14.6535
mentioned dnf induction methods mentioned deal rules propositional logic level research carried rules order logic obtainable inductive logic programming methods 
cohen extensively compared propositional order induction tc application instance comparing propositional learner ripper order version concluded additional representational power order logic brings modest benefits 
sebastiani regression models various tc regression models see ittner lewis gale sch tze :10.1.1.119.4275:10.1.1.119.4275
statistical learning community regression refers problem approximating real valued function means function fits training data mitchell page :10.1.1.55.2128
individual dnf rule learners vary widely terms methods heuristics criteria employed generalisation pruning 
inductive dnf rule learners applied tc moulinier ganascia dl li ripper cohen cohen cohen singer moulinier swap apt :10.1.1.25.4340:10.1.1.33.4944:10.1.1.14.6535:10.1.1.14.6535
mentioned dnf induction methods mentioned deal rules propositional logic level research carried rules order logic obtainable inductive logic programming methods 
cohen extensively compared propositional order induction tc application instance comparing propositional learner ripper order version concluded additional representational power order logic brings modest benefits 
sebastiani regression models various tc regression models see ittner lewis gale sch tze :10.1.1.119.4275:10.1.1.119.4275
statistical learning community regression refers problem approximating real valued function means function fits training data mitchell page :10.1.1.55.2128
describe model linear squares fit llsf proposed yang chute 
llsf document dj vectors associated input vector dj standard vector weighted terms output vector dj consisting vector weights representing categories weights vector binary case training documents standard non binary case test documents 
classification may seen task determining output vector dj test document dj input vector dj building classifier boils computing matrix dj dj 
inductive dnf rule learners applied tc moulinier ganascia dl li ripper cohen cohen cohen singer moulinier swap apt :10.1.1.25.4340:10.1.1.33.4944:10.1.1.14.6535:10.1.1.14.6535
mentioned dnf induction methods mentioned deal rules propositional logic level research carried rules order logic obtainable inductive logic programming methods 
cohen extensively compared propositional order induction tc application instance comparing propositional learner ripper order version concluded additional representational power order logic brings modest benefits 
sebastiani regression models various tc regression models see ittner lewis gale sch tze :10.1.1.119.4275:10.1.1.119.4275
statistical learning community regression refers problem approximating real valued function means function fits training data mitchell page :10.1.1.55.2128
describe model linear squares fit llsf proposed yang chute 
llsf document dj vectors associated input vector dj standard vector weighted terms output vector dj consisting vector weights representing categories weights vector binary case training documents standard non binary case test documents 
classification may seen task determining output vector dj test document dj input vector dj building classifier boils computing matrix dj dj 
llsf computes matrix training data computing linear squares fit minimizes error training set formula arg min mi arg min stands usual argument minimum def vij represents called norm matrix matrix columns input vectors training documents matrix columns output vectors training documents 
linear classifiers called profile classifiers rely extraction explicit profile prototypical document category training set 
obvious advantages terms interpretability profile representation readily interpretable human say neural network classifier 
linear classifiers partitioned broad classes batch classifiers line classifiers 
batch induction methods build classifier analysing training set 
tc literature example batch induction method linear discriminant analysis model stochastic dependence terms relies covariance matrices various categories hull sch tze :10.1.1.133.5960:10.1.1.133.5960
foremost example batch linear classifier rocchio classifier importance tc literature discussed separately section 
section concentrate line classifiers 
line aka incremental induction methods build classifier soon examining training document incrementally refine examine new ones 
may advantage tc applications training set available entirety right start meaning category may change time adaptive filtering 
section concentrate line classifiers 
line aka incremental induction methods build classifier soon examining training document incrementally refine examine new ones 
may advantage tc applications training set available entirety right start meaning category may change time adaptive filtering 
extremely apt applications may expect user classifier provide feedback test documents classified case training may performed operating phase exploiting user feedback 
applications kind may interactive classification larkey croft see section adaptive filtering :10.1.1.47.8517
simple example line method perceptron algorithm proposed tc applications sch tze wiener subsequently experimented dagan ng :10.1.1.54.6608:10.1.1.54.6608:10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378
algorithm classifier ci initialised setting weights positive value 
training example dj represented vector binary weights examined classifier built far attempts classify result classification examined 
correct done wrong weights classifier modified dj positive example ci weights active terms terms tk wkj promoted increasing fixed quantity called learning rate dj negative example ci weights demoted decreasing 
line aka incremental induction methods build classifier soon examining training document incrementally refine examine new ones 
may advantage tc applications training set available entirety right start meaning category may change time adaptive filtering 
extremely apt applications may expect user classifier provide feedback test documents classified case training may performed operating phase exploiting user feedback 
applications kind may interactive classification larkey croft see section adaptive filtering :10.1.1.47.8517
simple example line method perceptron algorithm proposed tc applications sch tze wiener subsequently experimented dagan ng :10.1.1.54.6608:10.1.1.54.6608:10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378
algorithm classifier ci initialised setting weights positive value 
training example dj represented vector binary weights examined classifier built far attempts classify result classification examined 
correct done wrong weights classifier modified dj positive example ci weights active terms terms tk wkj promoted increasing fixed quantity called learning rate dj negative example ci weights demoted decreasing 
note certain stage training phase classifier reached reasonable level effectiveness fact weight low means tk negatively contributed classification behaviour far may discarded representation 
algorithm classifier ci initialised setting weights positive value 
training example dj represented vector binary weights examined classifier built far attempts classify result classification examined 
correct done wrong weights classifier modified dj positive example ci weights active terms terms tk wkj promoted increasing fixed quantity called learning rate dj negative example ci weights demoted decreasing 
note certain stage training phase classifier reached reasonable level effectiveness fact weight low means tk negatively contributed classification behaviour far may discarded representation 
may see perceptron algorithm incremental induction methods matter allowing sort fly term space reduction dagan section :10.1.1.11.5378:10.1.1.11.5378
perceptron classifier shown effectiveness experiments quoted 
perceptron algorithm additive weight updating algorithm 
multiplicative variant perceptron positive winnow dagan differs perceptron different constants sebastiani promoting weights respectively promotion demotion achieved multiplying adding :10.1.1.11.5378:10.1.1.11.5378
balanced winnow dagan koster variant positive winnow classifier consists weights ki ki term tk final weight computing inner product difference ki ki misclassification positive instance active terms ki weight promoted ki weight demoted case negative instance ki gets promoted ki gets demoted rest positive winnow :10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378
note certain stage training phase classifier reached reasonable level effectiveness fact weight low means tk negatively contributed classification behaviour far may discarded representation 
may see perceptron algorithm incremental induction methods matter allowing sort fly term space reduction dagan section :10.1.1.11.5378:10.1.1.11.5378
perceptron classifier shown effectiveness experiments quoted 
perceptron algorithm additive weight updating algorithm 
multiplicative variant perceptron positive winnow dagan differs perceptron different constants sebastiani promoting weights respectively promotion demotion achieved multiplying adding :10.1.1.11.5378:10.1.1.11.5378
balanced winnow dagan koster variant positive winnow classifier consists weights ki ki term tk final weight computing inner product difference ki ki misclassification positive instance active terms ki weight promoted ki weight demoted case negative instance ki gets promoted ki gets demoted rest positive winnow :10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378
balanced winnow allows negative weights perceptron algorithm positive winnow weights positive 
experiments conducted dagan positive winnow showed better effectiveness perceptron algorithm turn outperformed dagan version balanced winnow 
experimental results confirm various analytical intuitions concerning algorithms discussed theoretical learning literature 
may see perceptron algorithm incremental induction methods matter allowing sort fly term space reduction dagan section :10.1.1.11.5378:10.1.1.11.5378
perceptron classifier shown effectiveness experiments quoted 
perceptron algorithm additive weight updating algorithm 
multiplicative variant perceptron positive winnow dagan differs perceptron different constants sebastiani promoting weights respectively promotion demotion achieved multiplying adding :10.1.1.11.5378:10.1.1.11.5378
balanced winnow dagan koster variant positive winnow classifier consists weights ki ki term tk final weight computing inner product difference ki ki misclassification positive instance active terms ki weight promoted ki weight demoted case negative instance ki gets promoted ki gets demoted rest positive winnow :10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378
balanced winnow allows negative weights perceptron algorithm positive winnow weights positive 
experiments conducted dagan positive winnow showed better effectiveness perceptron algorithm turn outperformed dagan version balanced winnow 
experimental results confirm various analytical intuitions concerning algorithms discussed theoretical learning literature 
examples line classifiers widrow hoff classifier refinement called exponentiated gradient classifier applied time tc lewis sleeping experts algorithm cohen singer koster version balanced winnow :10.1.1.43.9670:10.1.1.14.6535
balanced winnow dagan koster variant positive winnow classifier consists weights ki ki term tk final weight computing inner product difference ki ki misclassification positive instance active terms ki weight promoted ki weight demoted case negative instance ki gets promoted ki gets demoted rest positive winnow :10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378
balanced winnow allows negative weights perceptron algorithm positive winnow weights positive 
experiments conducted dagan positive winnow showed better effectiveness perceptron algorithm turn outperformed dagan version balanced winnow 
experimental results confirm various analytical intuitions concerning algorithms discussed theoretical learning literature 
examples line classifiers widrow hoff classifier refinement called exponentiated gradient classifier applied time tc lewis sleeping experts algorithm cohen singer koster version balanced winnow :10.1.1.43.9670:10.1.1.14.6535
additive weight updating algorithm second third multiplicative 
key differences previously described algorithms algorithms update classifier misclassifying training example classifying correctly ii update weights corresponding terms just active ones 
linear classifiers lend category pivoted document pivoted tc 
case classifier ci query set test documents dn case test document dj query set classifiers cm 
case classifier ci query set test documents dn case test document dj query set classifiers cm 
worth noticing induction linear classifier may typically preceded local term space reduction important terms category ci selected measure 
ci variants functions illustrated table usually employed ci variant tsr measure mean measure computed positive training examples category ci entire training set 
rocchio classifier rocchio classifier relies adaptation tc case rocchio formula relevance feedback vector space model tc method roots lie exclusively ir tradition machine learning 
adaptation proposed hull rocchio classifier authors object research right ittner joachims koster schapire singhal baseline classifier cohen singer fuhr joachims lewis schapire singer sch tze member classifier committee larkey croft see section :10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.47.8517:10.1.1.119.4275:10.1.1.119.4275:10.1.1.134.3024:10.1.1.134.3024:10.1.1.134.3024:10.1.1.48.5566:10.1.1.48.5566:10.1.1.21.7950:10.1.1.104.8304:10.1.1.104.8304:10.1.1.14.6535
machine learning automated text categorisation rocchio method computes classifier category ci means formula dj wkj dj dj dj wkj weight term tk document dj 
formula control parameters allow setting relative importance positive negative examples 
instance set dumais hull joachims sch tze corresponds viewing profile ci centroid positive training examples :10.1.1.133.5960:10.1.1.133.5960:10.1.1.161.6020:10.1.1.161.6020
general rocchio classifier rewards closeness test document centroid positive training examples distance centroid negative training examples 
rocchio classifier rocchio classifier relies adaptation tc case rocchio formula relevance feedback vector space model tc method roots lie exclusively ir tradition machine learning 
adaptation proposed hull rocchio classifier authors object research right ittner joachims koster schapire singhal baseline classifier cohen singer fuhr joachims lewis schapire singer sch tze member classifier committee larkey croft see section :10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.47.8517:10.1.1.119.4275:10.1.1.119.4275:10.1.1.134.3024:10.1.1.134.3024:10.1.1.134.3024:10.1.1.48.5566:10.1.1.48.5566:10.1.1.21.7950:10.1.1.104.8304:10.1.1.104.8304:10.1.1.14.6535
machine learning automated text categorisation rocchio method computes classifier category ci means formula dj wkj dj dj dj wkj weight term tk document dj 
formula control parameters allow setting relative importance positive negative examples 
instance set dumais hull joachims sch tze corresponds viewing profile ci centroid positive training examples :10.1.1.133.5960:10.1.1.133.5960:10.1.1.161.6020:10.1.1.161.6020
general rocchio classifier rewards closeness test document centroid positive training examples distance centroid negative training examples 
times role negative examples de emphasised setting high value low cohen singer ittner joachims 
method quite easy implement resulting classifiers tend quite efficient schapire :10.1.1.104.8304
terms effectiveness drawbacks documents category tend occur disjoint clusters set newspaper articles falling sports category dealing rock climbing rocchio classifier may centroid documents may fall outside clusters see 
formula control parameters allow setting relative importance positive negative examples 
instance set dumais hull joachims sch tze corresponds viewing profile ci centroid positive training examples :10.1.1.133.5960:10.1.1.133.5960:10.1.1.161.6020:10.1.1.161.6020
general rocchio classifier rewards closeness test document centroid positive training examples distance centroid negative training examples 
times role negative examples de emphasised setting high value low cohen singer ittner joachims 
method quite easy implement resulting classifiers tend quite efficient schapire :10.1.1.104.8304
terms effectiveness drawbacks documents category tend occur disjoint clusters set newspaper articles falling sports category dealing rock climbing rocchio classifier may centroid documents may fall outside clusters see 
generally rocchio classifier linear classifiers disadvantage basically divides space documents subspaces document falling case rocchio sphere classified ci documents falling 
situation graphically depicted documents classified ci fall circle 
note positive training examples classified correctly classifier 
issue application rocchio formula profile extraction set negative training instances dj tr considered entirety chosen sample set near positives defined positive negative training examples selected 
case contribution dj dj wkj factor tends significant near positives difficult documents tell apart relevant documents 
near positives corresponds query zoning method proposed ir singhal 
method originates observation original rocchio formula relevance feedback ir near positives tend generic negatives documents user judgments available tend ones scored highest previous ranking 
early applications rocchio formula tc ittner generally distinction near positives generic negatives :10.1.1.119.4275
schapire near positives issuing wkj sebastiani fig 

comparison categorisation behaviour rocchio classifier nn classifier 
small crosses circles denote positive negative training instances respectively 
rocchio query consisting centroid positive training examples document base consisting negative training examples top ranked ones similar centroid near positives 
fuhr identify near positives category ci positive examples sibling categories ci application web page categorisation hierarchical catalogues see section notion sibling category ci defined 
similar policies adopted ng ruiz srinivasan 
query zoning method plus enhancements term selection statistical phrases method called dynamic feedback optimisation schapire experimentally shown rocchio classifier achieve levels effectiveness comparable state art machine learning method boosting see section times quicker train 
results doubt bring renewed interest rocchio classifier previously bag sophisticated learning methods cohen singer joachims lewis sch tze yang :10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.109.2516:10.1.1.109.2516:10.1.1.14.6535
neural networks neural network classifier network units input units usually represent terms output unit represent category categories interest weights edges connect units represent conditional dependence relations 
classifying test document dj term weights wkj assigned input units activation units propagated forward network value output unit take consequence determines categorisation decision 
typical way training neural networks backpropagation term weights training document loaded input units misclassification occurs error machine learning automated text categorisation change parameters network eliminate minimise error 
simplest type neural network classifier perceptron dagan ng linear classifier extensively discussed section :10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378
results doubt bring renewed interest rocchio classifier previously bag sophisticated learning methods cohen singer joachims lewis sch tze yang :10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.109.2516:10.1.1.109.2516:10.1.1.14.6535
neural networks neural network classifier network units input units usually represent terms output unit represent category categories interest weights edges connect units represent conditional dependence relations 
classifying test document dj term weights wkj assigned input units activation units propagated forward network value output unit take consequence determines categorisation decision 
typical way training neural networks backpropagation term weights training document loaded input units misclassification occurs error machine learning automated text categorisation change parameters network eliminate minimise error 
simplest type neural network classifier perceptron dagan ng linear classifier extensively discussed section :10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378
types linear neural network classifiers implementing form logistic regression proposed experimented sch tze wiener usually effectiveness results 
non linear neural network ruiz srinivasan sch tze weigend wiener yang liu network additional layers units tc usually represent higherorder interactions terms network able learn :10.1.1.11.9519:10.1.1.11.9519:10.1.1.11.9519:10.1.1.54.6608:10.1.1.54.6608
comparative experiments relating non linear neural networks linear counterparts performed yielded improvement sch tze small improvements wiener :10.1.1.54.6608:10.1.1.54.6608
example classifiers example classifiers build explicit declarative representation category interest parasite categorisation judgments experts training documents similar test document 
classifying test document dj term weights wkj assigned input units activation units propagated forward network value output unit take consequence determines categorisation decision 
typical way training neural networks backpropagation term weights training document loaded input units misclassification occurs error machine learning automated text categorisation change parameters network eliminate minimise error 
simplest type neural network classifier perceptron dagan ng linear classifier extensively discussed section :10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378
types linear neural network classifiers implementing form logistic regression proposed experimented sch tze wiener usually effectiveness results 
non linear neural network ruiz srinivasan sch tze weigend wiener yang liu network additional layers units tc usually represent higherorder interactions terms network able learn :10.1.1.11.9519:10.1.1.11.9519:10.1.1.11.9519:10.1.1.54.6608:10.1.1.54.6608
comparative experiments relating non linear neural networks linear counterparts performed yielded improvement sch tze small improvements wiener :10.1.1.54.6608:10.1.1.54.6608
example classifiers example classifiers build explicit declarative representation category interest parasite categorisation judgments experts training documents similar test document 
methods called lazy learning systems defer decision generalize training data new query instance encountered mitchell pag :10.1.1.55.2128
example methods called reasoning methods tc literature due masand colleagues masand tc methods employed include joachims lam larkey larkey li jain yang pedersen yang liu :10.1.1.11.9519:10.1.1.11.9519
typical way training neural networks backpropagation term weights training document loaded input units misclassification occurs error machine learning automated text categorisation change parameters network eliminate minimise error 
simplest type neural network classifier perceptron dagan ng linear classifier extensively discussed section :10.1.1.11.5378:10.1.1.11.5378:10.1.1.11.5378
types linear neural network classifiers implementing form logistic regression proposed experimented sch tze wiener usually effectiveness results 
non linear neural network ruiz srinivasan sch tze weigend wiener yang liu network additional layers units tc usually represent higherorder interactions terms network able learn :10.1.1.11.9519:10.1.1.11.9519:10.1.1.11.9519:10.1.1.54.6608:10.1.1.54.6608
comparative experiments relating non linear neural networks linear counterparts performed yielded improvement sch tze small improvements wiener :10.1.1.54.6608:10.1.1.54.6608
example classifiers example classifiers build explicit declarative representation category interest parasite categorisation judgments experts training documents similar test document 
methods called lazy learning systems defer decision generalize training data new query instance encountered mitchell pag :10.1.1.55.2128
example methods called reasoning methods tc literature due masand colleagues masand tc methods employed include joachims lam larkey larkey li jain yang pedersen yang liu :10.1.1.11.9519:10.1.1.11.9519
presentation example approach nn nearest neighbours algorithm implemented yang system 
types linear neural network classifiers implementing form logistic regression proposed experimented sch tze wiener usually effectiveness results 
non linear neural network ruiz srinivasan sch tze weigend wiener yang liu network additional layers units tc usually represent higherorder interactions terms network able learn :10.1.1.11.9519:10.1.1.11.9519:10.1.1.11.9519:10.1.1.54.6608:10.1.1.54.6608
comparative experiments relating non linear neural networks linear counterparts performed yielded improvement sch tze small improvements wiener :10.1.1.54.6608:10.1.1.54.6608
example classifiers example classifiers build explicit declarative representation category interest parasite categorisation judgments experts training documents similar test document 
methods called lazy learning systems defer decision generalize training data new query instance encountered mitchell pag :10.1.1.55.2128
example methods called reasoning methods tc literature due masand colleagues masand tc methods employed include joachims lam larkey larkey li jain yang pedersen yang liu :10.1.1.11.9519:10.1.1.11.9519
presentation example approach nn nearest neighbours algorithm implemented yang system 
deciding dj classified ci nn looks training documents similar dj classified ci answer positive large proportion positive categorisation decision taken negative decision taken 
yang distance weighted version nn see mitchell section fact similar document classified ci weighted similarity test document :10.1.1.55.2128:10.1.1.55.2128
non linear neural network ruiz srinivasan sch tze weigend wiener yang liu network additional layers units tc usually represent higherorder interactions terms network able learn :10.1.1.11.9519:10.1.1.11.9519:10.1.1.11.9519:10.1.1.54.6608:10.1.1.54.6608
comparative experiments relating non linear neural networks linear counterparts performed yielded improvement sch tze small improvements wiener :10.1.1.54.6608:10.1.1.54.6608
example classifiers example classifiers build explicit declarative representation category interest parasite categorisation judgments experts training documents similar test document 
methods called lazy learning systems defer decision generalize training data new query instance encountered mitchell pag :10.1.1.55.2128
example methods called reasoning methods tc literature due masand colleagues masand tc methods employed include joachims lam larkey larkey li jain yang pedersen yang liu :10.1.1.11.9519:10.1.1.11.9519
presentation example approach nn nearest neighbours algorithm implemented yang system 
deciding dj classified ci nn looks training documents similar dj classified ci answer positive large proportion positive categorisation decision taken negative decision taken 
yang distance weighted version nn see mitchell section fact similar document classified ci weighted similarity test document :10.1.1.55.2128:10.1.1.55.2128
mathematically classifying document means nn comes computing dj rsv dj dz dz dj dj set documents dz rsv dj dz maximum values correct decision matrix section 
methods called lazy learning systems defer decision generalize training data new query instance encountered mitchell pag :10.1.1.55.2128
example methods called reasoning methods tc literature due masand colleagues masand tc methods employed include joachims lam larkey larkey li jain yang pedersen yang liu :10.1.1.11.9519:10.1.1.11.9519
presentation example approach nn nearest neighbours algorithm implemented yang system 
deciding dj classified ci nn looks training documents similar dj classified ci answer positive large proportion positive categorisation decision taken negative decision taken 
yang distance weighted version nn see mitchell section fact similar document classified ci weighted similarity test document :10.1.1.55.2128:10.1.1.55.2128
mathematically classifying document means nn comes computing dj rsv dj dz dz dj dj set documents dz rsv dj dz maximum values correct decision matrix section 
turn rsv dj dz represents measure semantic relatedness test document dj training document dz matching function probabilistic larkey croft vector yang ranked ir system may purpose :10.1.1.47.8517
nn method may represented graphically 
sebastiani knn classifier request rj request weights terms training documents weights training documents category assignments categories fig 
presentation example approach nn nearest neighbours algorithm implemented yang system 
deciding dj classified ci nn looks training documents similar dj classified ci answer positive large proportion positive categorisation decision taken negative decision taken 
yang distance weighted version nn see mitchell section fact similar document classified ci weighted similarity test document :10.1.1.55.2128:10.1.1.55.2128
mathematically classifying document means nn comes computing dj rsv dj dz dz dj dj set documents dz rsv dj dz maximum values correct decision matrix section 
turn rsv dj dz represents measure semantic relatedness test document dj training document dz matching function probabilistic larkey croft vector yang ranked ir system may purpose :10.1.1.47.8517
nn method may represented graphically 
sebastiani knn classifier request rj request weights terms training documents weights training documents category assignments categories fig 

graphical representation nn method 
instance larkey croft yang yield best effectiveness 
various experiments shown increasing value significantly degrade performance 
note nn linear classifiers subdivide document space just subspaces suffer problem discussed section 
graphically depicted local character nn respect rocchio appreciated 
remarkable effectiveness proven number different experiments see section advantages nn efficiency classification document categories interest machine learning automated text categorisation performed time linear cardinality training set yang :10.1.1.109.2516
remarked lazy learning methods nn efficient eager methods classification time training phase perform computation classification time 
example techniques 
various nearest neighbour techniques tc literature 
cohen implement example classifier extending standard relational dbms technology similarity soft joins 
small crosses circles represent positive negative training examples respectively lines represent decision surfaces 
decision surface indicated line shown best possible middle element widest set parallel decision surfaces minimum distance training example maximum 
small boxes indicate support vectors 
exploits superior effectiveness graphically illustrated nn linear classifiers time avoiding sensitivity nn presence outliers positive instances ci lie region document space positive instances ci located training documents 
building classifiers support vector machines application support vector machine method tc proposed joachims subsequently dumais yang liu :10.1.1.11.9519:10.1.1.11.9519:10.1.1.161.6020:10.1.1.161.6020
geometrical terms method may seen attempt find surfaces dimensional space separate positive negative training examples decision surfaces surface best possible way 
best means separates positives negatives widest possible margin separation property invariant respect widest possible shift yielding parallel surface method practical application socalled structural risk minimization principle decision surface minimise true error probability misclassification randomly selected unseen test example 
concept best understood case positives negatives linearly separable case decision surfaces hyperplanes 
dimensional case various sets parallel lines may chosen decision surfaces 
classifiers may different terms indexing approach followed terms inductive method applied order induce 
tc avenue explored knowledge 
different combination rules experimented literature 
simplest possible rule majority voting mv binary classification judgments obtained classifiers pooled classification decision reaches majority votes taken obviously needs odd number li jain liere tadepalli 
method particularly suited case committee includes classifiers characterised empirical attempt implementing principle version balanced winnow algorithm proposed dagan :10.1.1.11.5378:10.1.1.11.5378
classifiers touched previous sections fit picture sleeping experts cohen singer widrow hoff lewis discussed section :10.1.1.43.9670:10.1.1.14.6535:10.1.1.14.6535
sebastiani binary decision function 
second rule weighted linear combination weighted sum individually produced classifiers yields final 
weights wj meant reflect expected relative effectiveness classifier wj 
tc avenue explored knowledge 
different combination rules experimented literature 
simplest possible rule majority voting mv binary classification judgments obtained classifiers pooled classification decision reaches majority votes taken obviously needs odd number li jain liere tadepalli 
method particularly suited case committee includes classifiers characterised empirical attempt implementing principle version balanced winnow algorithm proposed dagan :10.1.1.11.5378:10.1.1.11.5378
classifiers touched previous sections fit picture sleeping experts cohen singer widrow hoff lewis discussed section :10.1.1.43.9670:10.1.1.14.6535:10.1.1.14.6535
sebastiani binary decision function 
second rule weighted linear combination weighted sum individually produced classifiers yields final 
weights wj meant reflect expected relative effectiveness classifier wj 
typically weights optimised validation set larkey croft :10.1.1.47.8517
classifiers touched previous sections fit picture sleeping experts cohen singer widrow hoff lewis discussed section :10.1.1.43.9670:10.1.1.14.6535:10.1.1.14.6535
sebastiani binary decision function 
second rule weighted linear combination weighted sum individually produced classifiers yields final 
weights wj meant reflect expected relative effectiveness classifier wj 
typically weights optimised validation set larkey croft :10.1.1.47.8517
possible policy dynamic classifier selection dcs committee classifier yields best effectiveness validation examples similar dj selected judgment adopted committee li jain 
different policy intermediate dcs adaptive classifier combination acc judgments classifiers committee summed individual contribution weighted effectiveness shown validation examples similar dj li jain 
classifier committees mixed results tc far 
larkey croft combinations rocchio na bayes nn pairwise combinations rule 
case committee formed na bayes subspace classifier combined means acc committee outperformed narrow margin best individual classifier attempted classifier combination anyway acc gave better results mv dcs 
especially light fact committee approach computationally expensive cost trivially amounts sum computational costs individual classifiers plus cost incurred computation combination rule 
remarked small size experiment test sets documents allow draw definitive approaches adopted 
boosting 
boosting method schapire schapire singer occupies special place classifier committees literature classifiers forming committee obtained means different learning methods learning method called weak learner :10.1.1.134.3024:10.1.1.104.8304:10.1.1.104.8304
instance decision tree classifier weak learner resulting committee formed decision tree classifiers 
key intuition boosting classifiers trained conceptually parallel independent way classifier committees described sequentially 
way training classifier may take account classifiers perform training examples concentrate machine learning automated text categorisation getting right examples performed worst 
specifically induction classifier dj ci pair attributed importance weight ij ij set equal dj ci pairs meant represent hard get correct decision pair classifiers 
specifically induction classifier dj ci pair attributed importance weight ij ij set equal dj ci pairs meant represent hard get correct decision pair classifiers 
weights exploited induction classifier specially tuned solve correctly pairs higher importance weight 
induced classifier applied training documents result weights ij updated ht ij update operation pairs correctly classified importance weight decreased pairs misclassified weight increased 
classifiers constructed weighted linear combination rule applied yield final committee weight attributed decision contributed classifier function effectiveness shown training set 
boostexter system schapire singer different boosting algorithms provided experimented level decision tree weak learner :10.1.1.134.3024
algorithm adaboost mh simply called adaboost schapire explicitly geared maximization microaveraged effectiveness adaboost aimed minimizing ranking loss getting correct category ranking individual document :10.1.1.104.8304
experiments conducted different test collections schapire shown adaboost outperform sleeping experts classifier proved quite effective experiments cohen singer :10.1.1.14.6535:10.1.1.14.6535
experiments schapire singer showed adaboost outperform aside sleeping experts na bayes classifier standard non enhanced rocchio classifier joachims prtfidf classifier 
boosting li jain decision tree classifier weak learner authors reported significant improvement effectiveness pure weak learner 
weights exploited induction classifier specially tuned solve correctly pairs higher importance weight 
induced classifier applied training documents result weights ij updated ht ij update operation pairs correctly classified importance weight decreased pairs misclassified weight increased 
classifiers constructed weighted linear combination rule applied yield final committee weight attributed decision contributed classifier function effectiveness shown training set 
boostexter system schapire singer different boosting algorithms provided experimented level decision tree weak learner :10.1.1.134.3024
algorithm adaboost mh simply called adaboost schapire explicitly geared maximization microaveraged effectiveness adaboost aimed minimizing ranking loss getting correct category ranking individual document :10.1.1.104.8304
experiments conducted different test collections schapire shown adaboost outperform sleeping experts classifier proved quite effective experiments cohen singer :10.1.1.14.6535:10.1.1.14.6535
experiments schapire singer showed adaboost outperform aside sleeping experts na bayes classifier standard non enhanced rocchio classifier joachims prtfidf classifier 
boosting li jain decision tree classifier weak learner authors reported significant improvement effectiveness pure weak learner 
approach similar boosting employed weiss 
induced classifier applied training documents result weights ij updated ht ij update operation pairs correctly classified importance weight decreased pairs misclassified weight increased 
classifiers constructed weighted linear combination rule applied yield final committee weight attributed decision contributed classifier function effectiveness shown training set 
boostexter system schapire singer different boosting algorithms provided experimented level decision tree weak learner :10.1.1.134.3024
algorithm adaboost mh simply called adaboost schapire explicitly geared maximization microaveraged effectiveness adaboost aimed minimizing ranking loss getting correct category ranking individual document :10.1.1.104.8304
experiments conducted different test collections schapire shown adaboost outperform sleeping experts classifier proved quite effective experiments cohen singer :10.1.1.14.6535:10.1.1.14.6535
experiments schapire singer showed adaboost outperform aside sleeping experts na bayes classifier standard non enhanced rocchio classifier joachims prtfidf classifier 
boosting li jain decision tree classifier weak learner authors reported significant improvement effectiveness pure weak learner 
approach similar boosting employed weiss 
authors experiment committees decision trees having average leaves complex simple leaves trees schapire singer experiment eventually combined simple mv rule combination rule :10.1.1.134.3024
experiments conducted different test collections schapire shown adaboost outperform sleeping experts classifier proved quite effective experiments cohen singer :10.1.1.14.6535:10.1.1.14.6535
experiments schapire singer showed adaboost outperform aside sleeping experts na bayes classifier standard non enhanced rocchio classifier joachims prtfidf classifier 
boosting li jain decision tree classifier weak learner authors reported significant improvement effectiveness pure weak learner 
approach similar boosting employed weiss 
authors experiment committees decision trees having average leaves complex simple leaves trees schapire singer experiment eventually combined simple mv rule combination rule :10.1.1.134.3024
similarly boosting mechanism documents misclassified previous decision trees enforced 
authors experimentally determined approach yields excellent effectiveness gains individual decision tree case excellent effectiveness court gains rising trees case reaching plateau trees 
methods previous sections tried give overview complete possible approaches proposed automated tc literature hardly possible exhaustive respect 
explosion discipline brought fragmentation terms learning ap schapire show simple modification policy allows evaluation classifier utility see section effectiveness 
authors experimentally determined approach yields excellent effectiveness gains individual decision tree case excellent effectiveness court gains rising trees case reaching plateau trees 
methods previous sections tried give overview complete possible approaches proposed automated tc literature hardly possible exhaustive respect 
explosion discipline brought fragmentation terms learning ap schapire show simple modification policy allows evaluation classifier utility see section effectiveness 
sebastiani proaches adopted fall class algorithms remained isolated attempts 
reasons space discuss detail want mention existence approaches bayesian inference networks dumais lam hartmann genetic algorithms :10.1.1.31.3592:10.1.1.161.6020:10.1.1.161.6020

determining thresholds various possible policies determining threshold discussed section depending constraints imposed application 
possible policy thresholding cohen singer schapire wiener called probability thresholding case probabilistic classifiers lewis yang :10.1.1.54.6608:10.1.1.54.6608:10.1.1.109.2516:10.1.1.104.8304:10.1.1.104.8304:10.1.1.14.6535
case threshold value function 
sebastiani proaches adopted fall class algorithms remained isolated attempts 
reasons space discuss detail want mention existence approaches bayesian inference networks dumais lam hartmann genetic algorithms :10.1.1.31.3592:10.1.1.161.6020:10.1.1.161.6020

determining thresholds various possible policies determining threshold discussed section depending constraints imposed application 
possible policy thresholding cohen singer schapire wiener called probability thresholding case probabilistic classifiers lewis yang :10.1.1.54.6608:10.1.1.54.6608:10.1.1.109.2516:10.1.1.104.8304:10.1.1.104.8304:10.1.1.14.6535
case threshold value function 
lewis considered fixed threshold equal ci noted result assigning test documents category ci assigning single test document category cj 
considered different thresholds different categories ci established normalising probability estimates suggestion maron 
experimental results show considerable difference effectiveness variants 
considered different thresholds different categories ci established normalising probability estimates suggestion maron 
experimental results show considerable difference effectiveness variants 
yang uses different thresholds different categories ci 
threshold optimised testing different values validation set choosing value yields best value chosen effectiveness function 
second popular policy proportional thresholding tokunaga larkey lewis lewis ringuette wiener called yang :10.1.1.49.860:10.1.1.49.860:10.1.1.54.6608:10.1.1.54.6608:10.1.1.109.2516
aim policy set threshold test set generality gte ci category ci close possible training set generality ci 
idea encodes quite sensible principle percentage documents training test set classified ci 
drawback thresholding policy obvious reasons lend document pivoted categorisation 
yang proposes refined version policy factor equal ci multiplied ci obtain gte ci 
drawback thresholding policy obvious reasons lend document pivoted categorisation 
yang proposes refined version policy factor equal ci multiplied ci obtain gte ci 
yang claims factor value empirically determined experimentation validation set allows smoother trade recall precision obtained see section 
nn llsf optimal values lie range 
depending application fixed thresholding policy known doc thresholding lewis yang applied stipulated fixed number categories equal dj assigned document dj :10.1.1.109.2516
instance applications tc automated document indexing field lam 
strictly speaking thresholding policy sense defined section happen classified ci 
quite clearly policy home document pivoted categorisation 
suffers certain coarseness machine learning automated text categorisation fact equal documents allow system fine tuning 
yang thresholding superior proportional thresholding possibly due category specific optimisation validation set fixed thresholding consistently inferior policies 
course fact results obtained different classifiers doubt reinforce 
general aside considerations choice thresholding policy may influenced application instance applying text classifier document indexing boolean systems fixed thresholding policy chosen proportional thresholding method chosen web page classification yahoo catalogues 
final note recall applications thresholding needed may better return original non binary value produced function binary value obtained means thresholding 
typically case interactive classification systems larkey croft :10.1.1.47.8517
document dj classify system meant suggest ranked list categories apt classifying dj expert takes final categorisation decision 
case list categories ci ranked dj value useful expert flat set relevant categories produced thresholding 
interactive classification systems useful especially quality training data low training data trusted representative sample unseen data come results completely automatic classifier trusted completely 

re rei superscript stands 
important recognise methods may give quite different results especially different categories populated instance classifier performs just categories large number positive test instances effectiveness probably better 
complete agreement authors better 
believe microaveraged performance somewhat misleading 
frequent topics weighted heavier average wiener page favour rewards classifiers perform robustly presence skewed category distributions :10.1.1.54.6608:10.1.1.54.6608
majority researchers believe topics count proportionally lean 
assume drop subscript pr re symbols clear say rest section may adapted case obvious way 
measures categorisation effectiveness 
measures alternative pr re commonly machine learning literature tp tn accuracy estimated tp tn fp fn error estimated fp fn er tp tn fp fn widely tc 
assume drop subscript pr re symbols clear say rest section may adapted case obvious way 
measures categorisation effectiveness 
measures alternative pr re commonly machine learning literature tp tn accuracy estimated tp tn fp fn error estimated fp fn er tp tn fp fn widely tc 
reason yang noted typically large value denominator takes tc insensitive variation number correct decisions tp tn pr re 
yang shows ac evaluation measure frequent case low value def tp fn tp tn fp fn gte ci represents average percentage categories test document trivial classifier sets aij classifying document category tends outperform non trivial classifiers see cohen section :10.1.1.25.4340:10.1.1.109.2516
consequence adopting ac parameter tuning non trivial classifier validation set result parameter choices classifier behave trivial 
non standard effectiveness measure proposed section suggest base precision recall absolute values suc sebastiani cess aij failure aij values relative success dj dj words case positive example dj category ci classifier score binary decision taken degree confidence dj classifier fact dj classified ci way system receives partial credit answer system correct direction directly increasing system confidence correct decision increases 
consider modified method revealing offers way evaluate system confidence decisions 
page proposed measure reward choice thresholding policy autonomous fully automatic classification systems 
general criteria different effectiveness seldom classifier evaluation 
instance efficiency criterion important applicative purposes seldom sole yardstick evaluate compare classifiers due volatility parameters evaluation rests 
efficiency may useful criterion choosing classifiers similar effectiveness 
respect interesting evaluation carried dumais compared different classifier induction methods different dimensions effectiveness training efficiency average time takes build classifier category ci training set tr induction method classification efficiency average time takes classify new document dj category ci classifier 
refer interested reader dumais details :10.1.1.161.6020:10.1.1.161.6020
exception dominance effectiveness criterion tc evaluation utility class measures known decision theory extend effectiveness economic criteria gain loss 
utility defining utility matrix table numeric values represent economic gain brought true positive false positive false negative true negative respectively greater 
standard effectiveness particular case utility 
trivial cases utility instance schapire experiment different utility matrices machine learning automated text categorisation category set expert judgments cm classifier judgments table 
trivial cases utility instance schapire experiment different utility matrices machine learning automated text categorisation category set expert judgments cm classifier judgments table 
utility matrix 
matrix matrix matrix corresponds effectiveness 
utility measures tc discussed detail lewis 
works utility measures employed crestani cohen singer hull lewis catlett :10.1.1.52.2415:10.1.1.14.6535
matter fact utility text filtering community trec filtering track evaluations utility measures lewis hull :10.1.1.16.3103:10.1.1.16.3103
problem utility pure effectiveness values utility matrix extremely application dependent 
needless say evaluation measure adds element difficulty classification systems see section classifiers experimentally comparable utility matrices 
mention fact effectiveness measures different ones discussed section occasionally literature include adjacent score larkey coverage schapire singer error schapire singer pearson product moment correlation larkey recall larkey croft top candidate larkey croft top larkey croft :10.1.1.47.8517:10.1.1.134.3024
utility matrix 
matrix matrix matrix corresponds effectiveness 
utility measures tc discussed detail lewis 
works utility measures employed crestani cohen singer hull lewis catlett :10.1.1.52.2415:10.1.1.14.6535
matter fact utility text filtering community trec filtering track evaluations utility measures lewis hull :10.1.1.16.3103:10.1.1.16.3103
problem utility pure effectiveness values utility matrix extremely application dependent 
needless say evaluation measure adds element difficulty classification systems see section classifiers experimentally comparable utility matrices 
mention fact effectiveness measures different ones discussed section occasionally literature include adjacent score larkey coverage schapire singer error schapire singer pearson product moment correlation larkey recall larkey croft top candidate larkey croft top larkey croft :10.1.1.47.8517:10.1.1.134.3024
attempt discuss detail effectiveness measures relationships standard ones 
works utility measures employed crestani cohen singer hull lewis catlett :10.1.1.52.2415:10.1.1.14.6535
matter fact utility text filtering community trec filtering track evaluations utility measures lewis hull :10.1.1.16.3103:10.1.1.16.3103
problem utility pure effectiveness values utility matrix extremely application dependent 
needless say evaluation measure adds element difficulty classification systems see section classifiers experimentally comparable utility matrices 
mention fact effectiveness measures different ones discussed section occasionally literature include adjacent score larkey coverage schapire singer error schapire singer pearson product moment correlation larkey recall larkey croft top candidate larkey croft top larkey croft :10.1.1.47.8517:10.1.1.134.3024
attempt discuss detail effectiveness measures relationships standard ones 
points fact tc discipline making consistent efforts experimentation protocols far universal agreement evaluation issues consequence far understanding precisely relative merits various methods 
combined effectiveness measures 
precision recall sense isolation 
various measures proposed frequent effectiveness computed interpolated point average precision 
threshold successively set values recall takes values 
different thresholds precision computed averaged resulting values 
methodology completely analogous standard evaluation methodology ranked retrieval systems may categories place ir queries 
frequently classifiers rank test documents appropriateness category see sch tze yang yang yang pedersen test documents place ir queries categories place documents :10.1.1.109.2516:10.1.1.109.2516
frequently classifiers rank categories appropriateness test document see lam larkey croft schapire singer wiener :10.1.1.47.8517:10.1.1.47.8517:10.1.1.47.8517:10.1.1.134.3024:10.1.1.134.3024:10.1.1.54.6608:10.1.1.54.6608:10.1.1.54.6608
note case needs positives 
symmetry precision recall point view classifier judgment positives vs negatives dichotomy tp interest trivial acceptor vs trivial symmetric recall precision tp fn tp tn precision precision 
fact recall tp fp fp tn precision trivial acceptor precision recall trivial 
threshold successively set values recall takes values 
different thresholds precision computed averaged resulting values 
methodology completely analogous standard evaluation methodology ranked retrieval systems may categories place ir queries 
frequently classifiers rank test documents appropriateness category see sch tze yang yang yang pedersen test documents place ir queries categories place documents :10.1.1.109.2516:10.1.1.109.2516
frequently classifiers rank categories appropriateness test document see lam larkey croft schapire singer wiener :10.1.1.47.8517:10.1.1.47.8517:10.1.1.47.8517:10.1.1.134.3024:10.1.1.134.3024:10.1.1.54.6608:10.1.1.54.6608:10.1.1.54.6608
note case needs positives 
symmetry precision recall point view classifier judgment positives vs negatives dichotomy tp interest trivial acceptor vs trivial symmetric recall precision tp fn tp tn precision precision 
fact recall tp fp fp tn precision trivial acceptor precision recall trivial 
course fact may arbitrarily set threshold increase recall expense precision true classifiers employ thresholds 
symmetry precision recall point view classifier judgment positives vs negatives dichotomy tp interest trivial acceptor vs trivial symmetric recall precision tp fn tp tn precision precision 
fact recall tp fp fp tn precision trivial acceptor precision recall trivial 
course fact may arbitrarily set threshold increase recall expense precision true classifiers employ thresholds 
decision tree classifiers dnf rule classifiers thresholds produce binary decisions 
tuning possible anyway difficult see weiss page :10.1.1.64.7852
machine learning automated text categorisation redefined document category basis 
note measure sense classifiers perform ranking recall absolute number may varied 
effectiveness computed breakeven point value pr equals re apt cohen singer dagan joachims joachims lewis lewis ringuette moulinier ganascia ng yang :10.1.1.49.860:10.1.1.49.860:10.1.1.11.5378:10.1.1.11.5378:10.1.1.20.9305:10.1.1.109.2516:10.1.1.109.2516:10.1.1.14.6535
reasonable classifiers value pr re equal exist increasing pr usually increases monotonically def gte ci value near re decreases monotonically 
decision tree classifiers dnf rule classifiers thresholds produce binary decisions 
tuning possible anyway difficult see weiss page :10.1.1.64.7852
machine learning automated text categorisation redefined document category basis 
note measure sense classifiers perform ranking recall absolute number may varied 
effectiveness computed breakeven point value pr equals re apt cohen singer dagan joachims joachims lewis lewis ringuette moulinier ganascia ng yang :10.1.1.49.860:10.1.1.49.860:10.1.1.11.5378:10.1.1.11.5378:10.1.1.20.9305:10.1.1.109.2516:10.1.1.109.2516:10.1.1.14.6535
reasonable classifiers value pr re equal exist increasing pr usually increases monotonically def gte ci value near re decreases monotonically 
values pr re exactly equal set value pr re closest interpolated breakeven computed average values re 
noted yang value pr re close interpolated breakeven may reliable indicator effectiveness classifier effectiveness computed value function cohen cohen singer lewis gale lewis moulinier ruiz srinivasan pr re pr re formula may seen relative degree importance attributed pr re coincides pr coincides re :10.1.1.25.4340:10.1.1.16.3103:10.1.1.109.2516:10.1.1.109.2516:10.1.1.14.6535
usually value attributes equal importance pr re 
note measure sense classifiers perform ranking recall absolute number may varied 
effectiveness computed breakeven point value pr equals re apt cohen singer dagan joachims joachims lewis lewis ringuette moulinier ganascia ng yang :10.1.1.49.860:10.1.1.49.860:10.1.1.11.5378:10.1.1.11.5378:10.1.1.20.9305:10.1.1.109.2516:10.1.1.109.2516:10.1.1.14.6535
reasonable classifiers value pr re equal exist increasing pr usually increases monotonically def gte ci value near re decreases monotonically 
values pr re exactly equal set value pr re closest interpolated breakeven computed average values re 
noted yang value pr re close interpolated breakeven may reliable indicator effectiveness classifier effectiveness computed value function cohen cohen singer lewis gale lewis moulinier ruiz srinivasan pr re pr re formula may seen relative degree importance attributed pr re coincides pr coincides re :10.1.1.25.4340:10.1.1.16.3103:10.1.1.109.2516:10.1.1.109.2516:10.1.1.14.6535
usually value attributes equal importance pr re 
shown moulinier yang breakeven value classifier equal value :10.1.1.109.2516:10.1.1.109.2516:10.1.1.109.2516
effectiveness measure chosen classifier tuned thresholds internal parameters set resulting effectiveness best achievable classifier 
tuning parameter threshold normally done experimentally 
reasonable classifiers value pr re equal exist increasing pr usually increases monotonically def gte ci value near re decreases monotonically 
values pr re exactly equal set value pr re closest interpolated breakeven computed average values re 
noted yang value pr re close interpolated breakeven may reliable indicator effectiveness classifier effectiveness computed value function cohen cohen singer lewis gale lewis moulinier ruiz srinivasan pr re pr re formula may seen relative degree importance attributed pr re coincides pr coincides re :10.1.1.25.4340:10.1.1.16.3103:10.1.1.109.2516:10.1.1.109.2516:10.1.1.14.6535
usually value attributes equal importance pr re 
shown moulinier yang breakeven value classifier equal value :10.1.1.109.2516:10.1.1.109.2516:10.1.1.109.2516
effectiveness measure chosen classifier tuned thresholds internal parameters set resulting effectiveness best achievable classifier 
tuning parameter threshold normally done experimentally 
means performing repeated experiments validation set experimental conditions values parameters pk fixed default value case tuned parameter pk chosen value parameter pk tuned different values parameter process value yielded best effectiveness chosen benchmark collections experimentation purposes standard benchmark collections initial corpora tc available public domain 
widely reuters collection consisting set newswire stories classified categories related economics 
table lists results experiments known performed major versions reuters benchmark reuters column reuters column reuters column reuters column reuters column experiments computed breakeven result listed popular effectiveness measures readily compare 
note results belonging column directly comparable 
particular yang shows experiments carried reuters column directly comparable versions includes significant percentage unlabelled test documents negative examples categories tend effectiveness 
experiments performed reuters column comparable collection consists restriction reuters populated categories obviously easier collection 
collections frequently tc evaluation ohsumed collection set hersh joachims lam ho lam lewis ruiz srinivasan yang pedersen documents titles title medical journals ohsumed consists subset medline document base categories terms mesh thesaurus :10.1.1.43.9670:10.1.1.43.9670
reuters collection may freely downloaded experimentation purposes www research att com lewis reuters html considered standard variant reuters 
decided cover experiments performed variants reuters benchmark different listed small number authors experimented variant reported results difficult interpret 
includes experiments performed original reuters hayes reuters cohen singer :10.1.1.14.6535:10.1.1.14.6535
ohsumed collection may freely downloaded experimentation purposes ftp edu pub ohsumed machine learning automated text categorisation system type results reported documents training documents test documents categories word non learning yang probabilistic dumais etal :10.1.1.109.2516
experiments performed reuters column comparable collection consists restriction reuters populated categories obviously easier collection 
collections frequently tc evaluation ohsumed collection set hersh joachims lam ho lam lewis ruiz srinivasan yang pedersen documents titles title medical journals ohsumed consists subset medline document base categories terms mesh thesaurus :10.1.1.43.9670:10.1.1.43.9670
reuters collection may freely downloaded experimentation purposes www research att com lewis reuters html considered standard variant reuters 
decided cover experiments performed variants reuters benchmark different listed small number authors experimented variant reported results difficult interpret 
includes experiments performed original reuters hayes reuters cohen singer :10.1.1.14.6535:10.1.1.14.6535
ohsumed collection may freely downloaded experimentation purposes ftp edu pub ohsumed machine learning automated text categorisation system type results reported documents training documents test documents categories word non learning yang probabilistic dumais etal :10.1.1.109.2516
probabilistic joachims probabilistic lam etal 
mf probabilistic lewis probabilistic li probabilistic li nb probabilistic yang liu decision trees dumais etal :10.1.1.11.9519
decision trees joachims ind decision trees lewis ringuette swap decision rules apt etal :10.1.1.49.860
collections frequently tc evaluation ohsumed collection set hersh joachims lam ho lam lewis ruiz srinivasan yang pedersen documents titles title medical journals ohsumed consists subset medline document base categories terms mesh thesaurus :10.1.1.43.9670:10.1.1.43.9670
reuters collection may freely downloaded experimentation purposes www research att com lewis reuters html considered standard variant reuters 
decided cover experiments performed variants reuters benchmark different listed small number authors experimented variant reported results difficult interpret 
includes experiments performed original reuters hayes reuters cohen singer :10.1.1.14.6535:10.1.1.14.6535
ohsumed collection may freely downloaded experimentation purposes ftp edu pub ohsumed machine learning automated text categorisation system type results reported documents training documents test documents categories word non learning yang probabilistic dumais etal :10.1.1.109.2516
probabilistic joachims probabilistic lam etal 
mf probabilistic lewis probabilistic li probabilistic li nb probabilistic yang liu decision trees dumais etal :10.1.1.11.9519
decision trees joachims ind decision trees lewis ringuette swap decision rules apt etal :10.1.1.49.860
ripper decision rules cohen singer decision rules cohen singer dl decision rules li decision rules moulinier ganascia decision rules moulinier etal :10.1.1.14.6535:10.1.1.14.6535
decided cover experiments performed variants reuters benchmark different listed small number authors experimented variant reported results difficult interpret 
includes experiments performed original reuters hayes reuters cohen singer :10.1.1.14.6535:10.1.1.14.6535
ohsumed collection may freely downloaded experimentation purposes ftp edu pub ohsumed machine learning automated text categorisation system type results reported documents training documents test documents categories word non learning yang probabilistic dumais etal :10.1.1.109.2516
probabilistic joachims probabilistic lam etal 
mf probabilistic lewis probabilistic li probabilistic li nb probabilistic yang liu decision trees dumais etal :10.1.1.11.9519
decision trees joachims ind decision trees lewis ringuette swap decision rules apt etal :10.1.1.49.860
ripper decision rules cohen singer decision rules cohen singer dl decision rules li decision rules moulinier ganascia decision rules moulinier etal :10.1.1.14.6535:10.1.1.14.6535
llsf regression yang llsf regression yang liu line linear dagan etal :10.1.1.11.9519:10.1.1.109.2516
widrow hoff line linear lam ho rocchio batch linear cohen singer batch linear dumais etal :10.1.1.14.6535:10.1.1.14.6535
includes experiments performed original reuters hayes reuters cohen singer :10.1.1.14.6535:10.1.1.14.6535
ohsumed collection may freely downloaded experimentation purposes ftp edu pub ohsumed machine learning automated text categorisation system type results reported documents training documents test documents categories word non learning yang probabilistic dumais etal :10.1.1.109.2516
probabilistic joachims probabilistic lam etal 
mf probabilistic lewis probabilistic li probabilistic li nb probabilistic yang liu decision trees dumais etal :10.1.1.11.9519
decision trees joachims ind decision trees lewis ringuette swap decision rules apt etal :10.1.1.49.860
ripper decision rules cohen singer decision rules cohen singer dl decision rules li decision rules moulinier ganascia decision rules moulinier etal :10.1.1.14.6535:10.1.1.14.6535
llsf regression yang llsf regression yang liu line linear dagan etal :10.1.1.11.9519:10.1.1.109.2516
widrow hoff line linear lam ho rocchio batch linear cohen singer batch linear dumais etal :10.1.1.14.6535:10.1.1.14.6535
rocchio batch linear joachims rocchio batch linear lam ho rocchio batch linear li classi neural network ng etal 
ohsumed collection may freely downloaded experimentation purposes ftp edu pub ohsumed machine learning automated text categorisation system type results reported documents training documents test documents categories word non learning yang probabilistic dumais etal :10.1.1.109.2516
probabilistic joachims probabilistic lam etal 
mf probabilistic lewis probabilistic li probabilistic li nb probabilistic yang liu decision trees dumais etal :10.1.1.11.9519
decision trees joachims ind decision trees lewis ringuette swap decision rules apt etal :10.1.1.49.860
ripper decision rules cohen singer decision rules cohen singer dl decision rules li decision rules moulinier ganascia decision rules moulinier etal :10.1.1.14.6535:10.1.1.14.6535
llsf regression yang llsf regression yang liu line linear dagan etal :10.1.1.11.9519:10.1.1.109.2516
widrow hoff line linear lam ho rocchio batch linear cohen singer batch linear dumais etal :10.1.1.14.6535:10.1.1.14.6535
rocchio batch linear joachims rocchio batch linear lam ho rocchio batch linear li classi neural network ng etal 
nnet neural network yang liu neural network wiener etal :10.1.1.11.9519
probabilistic joachims probabilistic lam etal 
mf probabilistic lewis probabilistic li probabilistic li nb probabilistic yang liu decision trees dumais etal :10.1.1.11.9519
decision trees joachims ind decision trees lewis ringuette swap decision rules apt etal :10.1.1.49.860
ripper decision rules cohen singer decision rules cohen singer dl decision rules li decision rules moulinier ganascia decision rules moulinier etal :10.1.1.14.6535:10.1.1.14.6535
llsf regression yang llsf regression yang liu line linear dagan etal :10.1.1.11.9519:10.1.1.109.2516
widrow hoff line linear lam ho rocchio batch linear cohen singer batch linear dumais etal :10.1.1.14.6535:10.1.1.14.6535
rocchio batch linear joachims rocchio batch linear lam ho rocchio batch linear li classi neural network ng etal 
nnet neural network yang liu neural network wiener etal :10.1.1.11.9519
gis example lam ho nn example joachims nn example lam ho nn example yang nn example yang liu svm dumais etal :10.1.1.11.9519:10.1.1.109.2516
mf probabilistic lewis probabilistic li probabilistic li nb probabilistic yang liu decision trees dumais etal :10.1.1.11.9519
decision trees joachims ind decision trees lewis ringuette swap decision rules apt etal :10.1.1.49.860
ripper decision rules cohen singer decision rules cohen singer dl decision rules li decision rules moulinier ganascia decision rules moulinier etal :10.1.1.14.6535:10.1.1.14.6535
llsf regression yang llsf regression yang liu line linear dagan etal :10.1.1.11.9519:10.1.1.109.2516
widrow hoff line linear lam ho rocchio batch linear cohen singer batch linear dumais etal :10.1.1.14.6535:10.1.1.14.6535
rocchio batch linear joachims rocchio batch linear lam ho rocchio batch linear li classi neural network ng etal 
nnet neural network yang liu neural network wiener etal :10.1.1.11.9519
gis example lam ho nn example joachims nn example lam ho nn example yang nn example yang liu svm dumais etal :10.1.1.11.9519:10.1.1.109.2516
svm joachims svm li svm yang liu adaboost mh committee schapire singer committee weiss etal :10.1.1.134.3024:10.1.1.11.9519
ripper decision rules cohen singer decision rules cohen singer dl decision rules li decision rules moulinier ganascia decision rules moulinier etal :10.1.1.14.6535:10.1.1.14.6535
llsf regression yang llsf regression yang liu line linear dagan etal :10.1.1.11.9519:10.1.1.109.2516
widrow hoff line linear lam ho rocchio batch linear cohen singer batch linear dumais etal :10.1.1.14.6535:10.1.1.14.6535
rocchio batch linear joachims rocchio batch linear lam ho rocchio batch linear li classi neural network ng etal 
nnet neural network yang liu neural network wiener etal :10.1.1.11.9519
gis example lam ho nn example joachims nn example lam ho nn example yang nn example yang liu svm dumais etal :10.1.1.11.9519:10.1.1.109.2516
svm joachims svm li svm yang liu adaboost mh committee schapire singer committee weiss etal :10.1.1.134.3024:10.1.1.11.9519
bayesian net dumais etal 
bayesian net lam etal 
llsf regression yang llsf regression yang liu line linear dagan etal :10.1.1.11.9519:10.1.1.109.2516
widrow hoff line linear lam ho rocchio batch linear cohen singer batch linear dumais etal :10.1.1.14.6535:10.1.1.14.6535
rocchio batch linear joachims rocchio batch linear lam ho rocchio batch linear li classi neural network ng etal 
nnet neural network yang liu neural network wiener etal :10.1.1.11.9519
gis example lam ho nn example joachims nn example lam ho nn example yang nn example yang liu svm dumais etal :10.1.1.11.9519:10.1.1.109.2516
svm joachims svm li svm yang liu adaboost mh committee schapire singer committee weiss etal :10.1.1.134.3024:10.1.1.11.9519
bayesian net dumais etal 
bayesian net lam etal 
mf table 
widrow hoff line linear lam ho rocchio batch linear cohen singer batch linear dumais etal :10.1.1.14.6535:10.1.1.14.6535
rocchio batch linear joachims rocchio batch linear lam ho rocchio batch linear li classi neural network ng etal 
nnet neural network yang liu neural network wiener etal :10.1.1.11.9519
gis example lam ho nn example joachims nn example lam ho nn example yang nn example yang liu svm dumais etal :10.1.1.11.9519:10.1.1.109.2516
svm joachims svm li svm yang liu adaboost mh committee schapire singer committee weiss etal :10.1.1.134.3024:10.1.1.11.9519
bayesian net dumais etal 
bayesian net lam etal 
mf table 
comparative results different classifiers obtained different version reuters collection 
mf table 
comparative results different classifiers obtained different version reuters collection 
noted entries indicate microaveraged breakeven point parentheses indicates indicates measure 
boldface indicates best performer collection 
sebastiani newsgroups collection set lang baker mccallum joachims mccallum nigam mccallum nigam schapire singer :10.1.1.14.5443:10.1.1.134.3024:10.1.1.13.8629:10.1.1.14.1043:10.1.1.21.7950
documents messages posted usenet newsgroups categories newsgroups 
ap collection cohen cohen cohen singer lewis catlett lewis gale lewis schapire singer schapire :10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.25.4340:10.1.1.134.3024:10.1.1.134.3024:10.1.1.134.3024:10.1.1.52.2415:10.1.1.104.8304:10.1.1.14.6535
trec routing collection lewis schapire sch tze :10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.104.8304:10.1.1.104.8304
cover experiments performed collections reasons illustrated footnote case significant number authors experimented collection experimental conditions making comparisons difficult 
noted entries indicate microaveraged breakeven point parentheses indicates indicates measure 
boldface indicates best performer collection 
sebastiani newsgroups collection set lang baker mccallum joachims mccallum nigam mccallum nigam schapire singer :10.1.1.14.5443:10.1.1.134.3024:10.1.1.13.8629:10.1.1.14.1043:10.1.1.21.7950
documents messages posted usenet newsgroups categories newsgroups 
ap collection cohen cohen cohen singer lewis catlett lewis gale lewis schapire singer schapire :10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.25.4340:10.1.1.134.3024:10.1.1.134.3024:10.1.1.134.3024:10.1.1.52.2415:10.1.1.104.8304:10.1.1.14.6535
trec routing collection lewis schapire sch tze :10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.104.8304:10.1.1.104.8304
cover experiments performed collections reasons illustrated footnote case significant number authors experimented collection experimental conditions making comparisons difficult 
classifier best 
published experimental results especially listed table allow attempt considerations comparative performance tc methods discussed 
boldface indicates best performer collection 
sebastiani newsgroups collection set lang baker mccallum joachims mccallum nigam mccallum nigam schapire singer :10.1.1.14.5443:10.1.1.134.3024:10.1.1.13.8629:10.1.1.14.1043:10.1.1.21.7950
documents messages posted usenet newsgroups categories newsgroups 
ap collection cohen cohen cohen singer lewis catlett lewis gale lewis schapire singer schapire :10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.25.4340:10.1.1.134.3024:10.1.1.134.3024:10.1.1.134.3024:10.1.1.52.2415:10.1.1.104.8304:10.1.1.14.6535
trec routing collection lewis schapire sch tze :10.1.1.43.9670:10.1.1.43.9670:10.1.1.43.9670:10.1.1.104.8304:10.1.1.104.8304
cover experiments performed collections reasons illustrated footnote case significant number authors experimented collection experimental conditions making comparisons difficult 
classifier best 
published experimental results especially listed table allow attempt considerations comparative performance tc methods discussed 
order bear mind comparisons tend reliable concern experiments performed author carefully controlled conditions 
order bear mind comparisons tend reliable concern experiments performed author carefully controlled conditions 
problematic involve different experiments performed different people 
case various background conditions extraneous learning algorithm proper may influenced experimental results 
may include different choices pre processing stemming indexing dimensionality reduction classifier parameter values different standards compliance safe scientific practice tuning parameters test set separate validation set discussed published papers 
result different methods may applied comparison classifiers yang direct comparison classifiers may compared tested benchmark collection bc typically team researchers background conditions :10.1.1.109.2516
method yields reliable results 
indirect comparison classifiers may compared tested collections bc bc respectively typically different teams researchers possibly different background conditions 
baseline classifiers tested tc tc direct comparison method 
test gives indication relative hardness collections results test may obtain indication relative effectiveness reasons discussed method yields reliable results 
batch linear classifiers rocchio probabilistic na bayes classifiers definitely look worst learning classifiers 
rocchio results confirm earlier results sch tze classifiers linear discriminant analysis linear regression neural networks perform better rocchio 
mentioned results schapire singer rank rocchio best performers near positives training 
data table hardly sufficient say decision tree classifiers 
noted dumais version decision tree classifiers shown perform nearly top performing system support vector machine classifier probably interest type text classifiers interest previous results reported earlier literature cohen singer joachims lewis catlett lewis ringuette :10.1.1.49.860:10.1.1.52.2415:10.1.1.14.6535
far lowest performance displayed word mock classifier implemented yang including learning component shows learning techniques definitely way go automatic tc 
concerning point completeness recall highest performances reported literature reuters collection breakeven belongs construe manually constructed classifier 
unfortunately classifier tested standard variants reuters mentioned table clear yang small test set reuters mod hayes breakeven value obtained chosen randomly word comparison documents category names treated vector weighted terms vector space model :10.1.1.109.2516
word implemented yang purpose determining difference effectiveness adding learning component classifier brings 
data table hardly sufficient say decision tree classifiers 
noted dumais version decision tree classifiers shown perform nearly top performing system support vector machine classifier probably interest type text classifiers interest previous results reported earlier literature cohen singer joachims lewis catlett lewis ringuette :10.1.1.49.860:10.1.1.52.2415:10.1.1.14.6535
far lowest performance displayed word mock classifier implemented yang including learning component shows learning techniques definitely way go automatic tc 
concerning point completeness recall highest performances reported literature reuters collection breakeven belongs construe manually constructed classifier 
unfortunately classifier tested standard variants reuters mentioned table clear yang small test set reuters mod hayes breakeven value obtained chosen randomly word comparison documents category names treated vector weighted terms vector space model :10.1.1.109.2516
word implemented yang purpose determining difference effectiveness adding learning component classifier brings 
word called str yang yang chute 
non learning classifier proposed wong 
sebastiani safe scientific practice demand 
word implemented yang purpose determining difference effectiveness adding learning component classifier brings 
word called str yang yang chute 
non learning classifier proposed wong 
sebastiani safe scientific practice demand 
consequence fact may indicative performance construe manual approach represents convincingly questioned yang :10.1.1.109.2516
important bear mind considerations absolute final judgments may comparative effectiveness tc methods 
reasons particular applicative context may exhibit different characteristics ones reuters different classifiers may respond differently characteristics 
experimental study joachims involving support vector machines nn decision trees rocchio na bayes showed classifiers similar effectiveness categories positive training examples category 
fact experiment involved methods scored best support vector machines nn worst rocchio na bayes table results indicative fact conditions different reuters may invalidate drawn 
tests useful verifying strongly experimental results support claim system better system verifying difference experimental setup affects measured effectiveness system 
hull sch tze direction validating results means anova test friedman test aimed determining significance difference effectiveness methods terms ratio difference effectiveness variability categories conducts similar test rank positions method category 
yang liu define full suite significance tests apply microaveraged effectiveness 
apply systematically comparison different classifiers able infer fine grained relative effectiveness 
examples significance testing tc see cohen cohen cohen joachims koller sahami lewis wiener :10.1.1.43.9670:10.1.1.43.9670:10.1.1.25.4340:10.1.1.21.988:10.1.1.54.6608:10.1.1.54.6608:10.1.1.21.7950:10.1.1.33.4944

automatic categorisation web pages application tc raised considerable interest categorisation web pages sites hierarchically organised sets categories 
reason discuss separately rest rise set techniques specific tackling problems diverse indexing term selection classifier induction evaluation 
reasons application rise specific techniques web pages special kinds documents consist text set incoming outgoing pointers 
idea redefine effectiveness considering success aij notation section failure aij near success defined case category ci attributed system dj correct category ck sibling ci correct 

automated tc established major areas information systems discipline number reasons domains application numerous important proliferation documents digital form bound increase dramatically number importance 
indispensable applications sheer number documents classified short response time imposed application manual alternative implausible 
dramatically improve productivity human classifiers applications classification decision taken final human expert judgment larkey croft providing tools quickly suggest plausible decisions :10.1.1.47.8517
reached effectiveness levels comparable obtained manual tc involvement trained professionals 
effectiveness manual tc anyway cleverdon importantly going improved progress research 
contrary levels effectiveness automated tc growing steady pace plausible eventually reach plateau level plateau probably higher effectiveness levels manual tc 
reasons early onwards effectiveness levels text classifiers dramatically improved entrance tc arena machine sebastiani learning methods backed strong theoretical motivations 
lewis gale 
sequential algorithm training text classifiers 
proceedings sigir th acm international conference research development information retrieval dublin pp 

see lewis :10.1.1.16.3103
lewis hayes 
guest editorial special issue text categorization 
acm transactions information systems 
lewis ringuette 
