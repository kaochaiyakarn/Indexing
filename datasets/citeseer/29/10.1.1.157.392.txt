scaling clustering algorithms large databases bradley usama fayyad cory reina microsoft research redmond wa usa cs wisconsin edu fayyad microsoft com practical clustering algorithms require multiple data scans achieve convergence 
large databases scans prohibitively expensive 
scalable clustering framework applicable wide class iterative clustering 
require scan database 
framework instantiated numerically justified popular means clustering algorithm 
method identifying regions data compressible regions maintained memory regions discardable 
algorithm operates confines limited memory buffer 
empirical results demonstrate scalable scheme outperforms sampling approach 
scheme data resolution preserved extent possible size allocated memory buffer fit current clustering model data 
framework naturally extended update multiple clustering models simultaneously 
empirically evaluate synthetic publicly available data sets 
clustering important application area fields including data mining statistical data analysis kr br fhs compression vector quantization business applications 
clustering formulated various ways machine learning pattern recognition dh optimization bms si statistics literature kr br 
fundamental clustering problem grouping clustering similar data items 
general approach view clustering density estimation problem br 
assume addition observed variables data item hidden unobserved variable indicating cluster membership 
data assumed arrive mixture model hidden cluster identifiers 
general mixture model having clusters ci assigns probability data point pr pr ci wi mixture weights 
problem estimating parameters individual ci assuming number clusters known 
clustering optimization problem finding parameters individual ci maximize likelihood database mixture model 
copyright american association artificial intelligence www aaai org 
rights reserved 
general assumptions distributions clusters em algorithm dlr cs popular technique estimating parameters 
assumptions addressed classic means algorithm cluster effectively modeled spherical gaussian distribution data item assigned cluster mixture weights wi assumed equal 
note means dh defined numeric continuous valued data ability compute mean required 
discrete version means exists referred harsh em 
mean algorithm finds locally optimal solution problem minimizing sum distance data point nearest cluster center distortion si equivalent maximizing likelihood assumptions listed 
various approaches solving optimization problem 
iterative refinement approaches include em means effective 
basic algorithm follows initialize model parameters producing current model decide memberships data items clusters assuming current model correct re estimate parameters current model assuming data memberships obtained correct producing new model current model new model sufficiently close terminate go 
means parameterizes cluster ci mean points cluster model update step consists computing mean points assigned cluster 
membership step consists assigning data points cluster nearest mean measured metric 
focus problem clustering large databases large loaded ram 
data scan iteration extremely costly 
focus means algorithm method extended accommodate algorithms bfr :10.1.1.157.392:10.1.1.157.392
means known algorithm originally known forgy method extensively pattern recognition dh 
standard technique wide array applications way initialize expensive em clustering cs mh bf 
framework satisfies data mining desiderata 
require scan database possible single data scan considered costly early termination appropriate highly desirable 

line anytime behavior best answer available status information progress expected remaining time provided 
incremental progress saved resume stopped job 

ability incrementally incorporate additional data existing models efficiently 

confines limited ram buffer 

utilize variety possible scan modes sequential index sampling scans available 

ability operate forward cursor view database 
necessary database view may result expensive join query potentially distributed data warehouse processing required construct row case 
clustering large databases scalable framework clustering scalable framework clustering notion effective clustering solutions obtained selectively storing important portions database summarizing portions 
size allowable prespecified memory buffer determines amount summarizing required internal book keeping 
assume interface database allows algorithm load requested number data points 
obtained sequential scan random sampling preferred means provided database engine 
process proceeds follows 
obtain available possibly random sample db filling free space ram buffer 

update current model contents buffer 

updated model classify singleton data elements needs retained buffer retained set rs discarded updates sufficient statistics discard set ds reduced compression summarized sufficient statistics compression set cs 

determine stopping criteria satisfied terminate go 
illustrate process 
basic insight data points data equal importance model constructed 
data points need time rs thought similar notion support vectors classification regression 
discardable reducible efficient hopefully equivalent representation 
framework accommodates clustering algorithms focus evaluating means algorithm 
applications em bfr :10.1.1.157.392
components scalable architecture idea iterate random samples database merge information computed previous samples information computed current sample 
dimensionality data records 
assuming covariance matrix cluster diagonal 
step algorithm basically generalization classic means algorithm call extended means section operating data sufficient statistics previous reduced data means dbms subset data represented mean diagonal covariance matrix 
data compression primary data compression determines items discarded discard set ds 
secondary data compression takes place data points compressed primary phase 
data compression refers representing groups points sufficient statistics purging points ram 
compressed representation constitutes set cs 
group singleton data elements deemed compressed called sub cluster 
remaining elements defy primary secondary compression members retained set rs singleton data points 
sub cluster sufficient statistics storage sub clusters compressed locally fitting gaussian 
set singleton points compressed 
sufficient statistics triple sum sumsq sum sumsq xj triple sum sumsq computations sub cluster mean sub covariance diagonal straightforward 
sub clusters sub having sufficient statistics sum sumsq sum sumsq sum sum sumsq sumsq respectively merged sufficient statistics 
ds ds ds dsk list elements element stores sufficient statistics sub clusters determined primary phase section 
similarly cs cs cs csh list sufficient statistics triples sub clusters determined secondary data compression phase section 
model update sample sufficient statistics step scalable clustering algorithm outline consists performing means iterations sufficient statistics compressed discarded retained points ram 
extended means algorithm updates singleton points classic means algorithm updates 
updates sufficient statistics involve treating scalable clustering system ram buffer models consideration data compression discard set ds compress set cs retained set rs cluster data model sufficient stats updated models secondary clustering termination criteria overview scalable clustering framework triplet sum sumsq data point weight items 
details bfr :10.1.1.157.392
convergence extended means number clusters say members reset singleton data buffer furthest respective cluster centroids 
iterations resumed convergence ram buffer contents 
primary data compression primary data compression intended discard data points change cluster membership iterations 
cluster associate list summarizes sufficient statistics data discarded cluster methods primary data compression exist 
thresholding mahalanobis radius dh estimated cluster center compressing data points radius 
mahalanobis distance point mean gaussian covariance matrix jj diagonal covariance matrix 
idea primary compression approach determine mahalanobis radius collapses newly sampled singleton data points assigned cluster data items radius sent discard set 
sufficient statistics data points discarded method merged points previously compressed phase past data samples 
second primary compression method creates worst case scenario perturbing cluster means computed confidence intervals 
data point buffer perturb estimated cluster means respective confidence intervals resulting situation worst case scenario singleton data point 
data point belong cluster perturbation consists moving mean cluster far away data point possible confidence interval moving mean cluster near data point possible 
resulting perturbation data point nearest perturbed center enters set points discarded 
approach motivated assumption mean move outside computed confidence interval 
assignment data point cluster remains fixed current configuration estimated cluster means change worst way assume assignment robust data samples discard point 
confidence interval computations obtain confidence interval mean dimensions univariate confidence intervals computed 
level confidence intervals determined exploiting bonferroni inequality 
th component mean wish determine univariate confidence interval probability fall respective interval greater equal wish satisfy bonferroni inequality relationship suffices univariate intervals computed confidence secondary data compression purpose secondary data compression identify tight sub clusters points data discard primary phase 
singleton points designating sub cluster change cluster assignment cluster means updated exactly sufficient statistics sub clusters 
length list cs varies depending density points compressed primary phase 
secondary fundamental parts locate candidate dense portions space compressed primary phase applying tightness dense criterion candidates 
candidate sub clusters determined applying vanilla means algorithm larger number clusters initialized randomly selecting elements items buffer 
secondary takes place remaining singleton data clusters individual cluster 
sub clusters determined tightness criteria requiring sub cluster covariances bounded threshold applied 
consider sub clusters containing elements described triple sum sumsq sum max sumsq sub cluster tight 
suppose sub clusters pass filter 
sub clusters merged existing clusters cs 
performed hierarchical agglomerative clustering cs clusters 
nearest sub clusters determined merged merge results new sub cluster violate tolerance condition see section post merge sufficient statistics obtained merged sub cluster kept smaller ones removed 
scan solutions scheme involves updating single model database 
machinery developed compression sufficient reduced representation admits possibility updating multiple models simultaneously single data scan 
means members iterative clustering algorithms known extremely sensitive initial starting condition dh 
study initialization problem bf 
standard practice usually calls trying multiple solutions multiple random starting points 
support standard practice clustering support ability explore multiple models 
key insights generalization retained points rs sets cs representing local dense structures shared models model say mi discarded data sets sets cluster model models discard sets sufficient statistics discarded data sets models mi simply viewed members global cs models mi 
space requirements permit presentation multiple model update 
provides insight 
architecture remains shown model updating data compression steps performed multiple models 
observations data compression item worthy discussion data discard order multiple models 
algorithm decides individual data point basis discard set fits best 
data point qualifies discard item models simply goes discard set model fits best 
data point allowed enter discard set accounted multiple times 
qualify discard item models 
admitted model feel effect point twice ds time updates ds treated part cs far concerned 
similarly 
entering discard set say ds point affects updates cs ds 
complexity scalability considerations data set clustering algorithm requires iter iterations cluster time complexity iter 
small subsample typically requires significantly fewer iteration cluster 
empirically reasonable expect iter iter 
total time required cluster samples size generally time required cluster single set data points 
algorithm easily scales large databases 
memory requirement hold small sub sample ram 
clustering primary secondary occurs contents buffer 
approach run small ram buffer effectively applied largescale databases 
observed running multiple solutions typically results improved performance compression schemes synergy models explored allow added opportunity compression 
turn frees space buffer allows algorithm maximize exposure new data model updates 
prefer obtain random sample database server 
sounds simple reality challenging task 
guarantee records database ordered property random sampling expensive scanning entire database scheme reservoir sampling 
database environment data view entity familiar machine learning researchers may materialized 
result query involving joins groupings sorts 
cases database operations impose special ordering result set randomness resulting database view assumed 
case random sampling available may desirable seed scheme true random samples obtained means buffers 
evaluation data sets straightforward way scale algorithm large database sample data 
comparisons mainly targeted showing proposed scalable approach performs better simple random sampling 
clustering particularly sensitive choice sample 
due lack space omit demonstration formal arguments increased sensitivity 
see bfr details :10.1.1.157.392
synthetic data sets section purposefully chosen best case scenario sampling approach 
demonstrating improvement random sampling scenario convincing scenarios true data distributions known 
synthetic data sets synthetic data created dimension 
value data sampled gaussians elements mean vectors true means sampled uniform distribution 
elements diagonal covariance matrices sampled uniform distribution 
fairly separated gaussians ideal situation means 
random weight vector elements determined components sum wj total number data points sampled gaussian worthwhile emphasizing fact data drawn separated gaussians low dimensions certainly best case scenario behavior random sampling approach scaling means algorithm 
note introducing noise providing algorithm correct number clusters real world data sets scenario behavior worse 
experimental methodology quality obtained clustering solution measured fits data measure estimated means true gaussian means generating synthetic data 
measuring degree fit set clusters data set done distortion log likelihood data model 
log likelihood assumes means model represents mixture gaussians associated diagonal covariances computes probability data model 
comparing distance solution clustering solution requires ability match clusters optimal way prior computing distance 
suppose want measure distance set clusters obtained true gaussian means 
true gaussian means means estimated clustering algorithm 
permutation determined minimizes score simply quantity divided giving average distance true gaussian means set clusters 
results solutions compare solution provided scalable scheme solution obtained line means solution obtained sampler means working sample equal size buffer 
note distortion sum distances squared data items mean assigned cluster table 
results synthetic data dimensions points clusters points clusters algorithm ratio ratio likelihood likelihood ultimate goal find solution closer true generating means 
goal demonstrate method scales increases dimensionality number clusters parameters estimated clustering 
fist show results data sets 
shown distance true solutions ratios relative best solutions best bold change log likelihood likelihood measured best solution 
results represent averages randomly chosen starting points 
sampling solution chance sample different random samples population 
random sampler results total trials 
buffer size percentage size data set 
worthy note results show scalable algorithm robust behavior ram buffer limited small sample 
line means data sets 
measuring distortion results trends follow log likelihood 
table 
results data 
method ratio results data sets dimensions clusters table 
results real world data sets computational results publicly available real world data sets 
primarily interested large databases 
large mean hundreds dimensions tens thousands millions records 
data sets method exhibits greatest value 
large publicly available data set available reuters news service 
wanted demonstrate results irvine machine learning repository data sets 
majority data sets data sets easy low dimensional small number records 
experiments quality solution determined 
case synthetic data measure distance true solution truth known 
average class purity cluster measure quality 
measure dependent classification distortion data clusters 
quality scoring methods distortion loglikelihood information gain 
estimates amount information gained clustering database measured reduction class entropy 
recall class labels data known ideal clustering clusters pure terms mix classes appearing 
scored weighted entropy entire clustering size weighted entropy information gain total entropy weighted entropy 
rev digits recognition dataset dataset consists data items dimensions 
record represents gray scale level image handwritten digit record tagged classes digit 
rev results scalable mean algorithm run ram buffer equivalent rev dataset size 
comparisons online mean algorithm means algorithm operating random samples size 
results averaged randomly chosen initial solutions trials 
ran experiments buffer sizes samples 
results shown table 
compare best solutions best solutions 
buffer size best solution skm gain versus best gain sampler means 
best gain 
case sampler badly time managed find best solution 
scalable method produced solutions times informative random sampling solutions times better line kmeans 
note memory requirements driven really low maintain performance 
table 
results rev digits data set 
method ave std dev skm buffer sample skm buffer sample reuters information retrieval dataset reuters text classification database derived original reuters data set publicly available part reuters corpus reuters carnegie group david lewis see www research att com lewis reuters readme txt details data set 
data consists documents 
document news article topic earnings commodities acquisitions grain copper categories belong higher level categories hierarchy categories 
reuters database consists word counts documents 
hundreds thousands words purposes experiments selected frequently occurring words instance dimensions indicating integer number times corresponding word occurs document 
document ir reuters database classified categories 
clustering purposes reflect top level categories 
task find best clustering 
reuters results data set clustering entire database requires large amount time chose evaluate results randomly chosen starting conditions 
results shown table 
chart shows significant decrease total distortion measure 
show ratios distortion measured data 
note line means case failed find solution initial points 
manage find starting points manually resulted better clustering comparison exactly initial points methods 
distortion roughly better scalable approach corresponding random sampling approach 
measure degree fit log likelihood data model derived means gaussians diagonal covariances 
model fit data log likelihood goes extremely negative overflows 
table 
results reuters data 
method log likelihood skm buffer skm buffer skm buffer sample sample sample ratio distortion ratios reuters means means means line means happened line means sampling approaches 
scalable runs produced finite likelihoods samples indicating better model 
document belongs category categories measure quality achieved clustering measuring gain information reduction cluster impurity categories cluster gives pure clusters informative 
information gain scalable scheme average times better solution obtained sampling approaches 
compares best best get best scalable solution times better best random sampling solution 
line means failed converge solutions starting points landed solutions put data cluster improve impurity 
give ratio 
better manually chosen starting points line kmeans converge better solutions 
related means historically applied small data sets state practice appears try various random starting points 
traditionally means initialize expensive algorithms em 
fact methods initialize em including hierarchical agglomerative clustering hac dh set initial points 
choice comparing random starting points approach 
regardless starting point comes prior knowledge initialization scheme method effective efficient scalable means proceed solution 
statistics schemes aware appear memory 
book dedicated topic clustering large data sets kr presents algorithm clara clustering large databases 
algorithm limited cases maximum kr 
options available literature scale large databases random sampling line means 
compare methods section 
line means essentially works memory buffer case 
show results section methods compare alternatives 
data mining literature relevant approach birch 
scalable clustering schemes include clarans nh dbscan 
targeted clustering spatial data primarily 
compare schemes demonstrate higher efficiency 
fundamental difference birch method proposed data compression step performed prior independent clustering notion allocated memory buffer birch depending data memory usage grow significantly birch requires scans data perform clustering statistics maintained represent simpler local model strictly spherical gaussians models built scheme requires 
extended notion classes data rs cs ds viewed generalization birch discard strategy 
able perform comparative study birch certainly plan 
birch expensive step maintenance updating 
case updates simpler 
scheme requires secondary clustering step 
secondary clustering case limited small sample data 
suspect total run times lower birch due fact require data scans maintain large cf tree structure 
claim needs supported empirical evaluation similar machines data sets 
br banfield raftery model gaussian non gaussian clustering biometrics vol 
pp 

bishop 
neural networks pattern recognition 
oxford university press 
brachman piatetsky shapiro simoudis industrial applications data mining knowledge discovery communications acm 

bms bradley mangasarian street 

clustering concave minimization advances neural information processing systems mozer jordan petsche eds 
pp mit press 
bf bradley fayyad refining initial points means clustering proc 
th international conf machine learning morgan kaufmann 
bfr bradley fayyad reina scaling clustering algorithms large databases microsoft research report may :10.1.1.157.392
cs cheeseman stutz bayesian classification autoclass theory results advances knowledge discovery data mining fayyad piatetsky shapiro smyth uthurusamy eds pp 

mit press 
dlr dempster laird rubin maximum likelihood incomplete data algorithm 
journal royal statistical society series 
dh duda hart pattern classification scene analysis 
new york john wiley sons 
ester kriegel xu database interface clustering large spatial databases proc 
international knowledge discovery data mining kdd aaai press 
fhs fayyad haussler stolorz 
mining science data communications acm 
fayyad piatetsky shapiro smyth uthurusamy eds 
advances knowledge discovery data mining 
mit press 
fayyad weir application classification clustering sky survey cataloging analysis computing science statistics vol 
wegman eds pp 
fairfax va interface foundation north america 
fayyad cory reina paul bradley refining initialization clustering algorithms proc 
th international conf 
knowledge discovery data mining aaai press 
fisher 
knowledge acquisition incremental conceptual clustering 
machine learning 
forgy cluster analysis multivariate data efficiency vs interpretability classifications biometrics 

fukunaga statistical pattern recognition san diego ca academic press 
glymour madigan pregibon smyth 

statistical themes lessons data mining data mining knowledge discovery vol 

jones note sampling tape file 
communications acm vol 
kr kaufman rousseeuw 
finding groups data new york john wiley sons 
macqueen methods classification analysis multivariate observations 
proceedings fifth berkeley symposium mathematical statistics probability 
volume statistics le cam neyman eds 
university california press 
mh meila heckerman 
experimental comparison clustering methods microsoft research technical report msr tr redmond wa 
nh ng han efficient effective clustering methods spatial data mining proc vldb 
pe pregibon elder statistical perspective knowledge discovery databases advances knowledge discovery data mining fayyad piatetsky shapiro smyth uthurusamy eds pp 

mit press 
rasmussen clustering algorithms information retrieval data structures algorithms frakes baeza yates eds pp 
upper saddle river nj prentice hall 
multivariate observations new york john wiley sons 
scott multivariate density estimation new york wiley 
si ismail means type algorithms generalized convergence theorem characterization local optimality 
ieee trans 
pattern analysis machine intelligence vol 
pami 
silverman density estimation statistics data analysis london chapman hall 
zhang ramakrishnan livny 
birch new data clustering algorithm applications data mining knowledge discovery 

