rapid language model development new task domains eric james allen department computer science university rochester rochester ny usa james cs rochester edu www cs rochester edu research trains data sparseness regularly primary problem statistical language modelling 
go step consider situation text data available target domain 
techniques building efficient language models quickly new domains 
technique context free grammar generate corpus word collocations 
second adaptation technique domain corpora estimate target domain language models 
report results successfully techniques individually combination build efficient models spontaneous speech recognition task medium sized vocabulary domain 

language models continuous speech recognition usually built large set training sentences specific domain 
authors pointed difficulty getting sufficiently large textual corpus 
consequently lot research domain adaptation estimating domain dependent models domain data 
approaches adaptation rudnicky crespo ito assume availability general model specific incorporating knowledge text corpus new domain 
approaches iyer ostendorf gish iyer ostendorf ge perform adaptation text data different domains trying pick relevant information combined information text data new domain 
contrast propose techniques domain adaptation corpus available new domain 
show successful application language modelling task medium vocabulary domain 
technique proposed context free grammar cfg generate word grams 
second domain corpora 
techniques separately conjunction technique allowing fast inexpensive prototyping low perplexity recognition accuracy language models 

generating artificial corpora trying build language models lms new domains data available customary wizard oz procedure obtain small text corpus utterances new domain generate language model corpus interpolate general model 
rapid prototyping incremental adaptation done bootstrapping approach reduced set sentences adaptation lm adapted incrementally domain sentences available 
propose alternative method obtaining performance short time build cfg new domain generate gram language model 
idea dates back hasn received attention community 
development voyager system zue employing tina parser generation mode obtain artificial corpus 
took different approach closer jurafsky 
hand coded task specific cfg generated artificial corpus obtained gram lm text data corpus 
jurafsky 
authors suggest probabilistic cfg language model stand combination statistical lm 
grammar developed methodology shown meant source realistic word collocations constraining brittle lm 
advantage technique requires time expertise doesn need parameter tuning expensive hand crafting corpus learning assume target domain corpus available 
continue describing process obtaining cfg specific domain examples paci domain 
wrote sentences words type appear conversations system functionality 
trips dialog system ferguson allen testbed experiments 
truck move people long helicopter take get exodus rescue people exodus long take get delta truck go back get fly forest starting hand coded grammar rules trips domain methodology similar developed rayner carter semi automatic adaptation grammars 
procedure summarized steps 
divide sentences smaller parts reasonably thought units may replaced words phrases related task domain 
truck move people long helicopter take get exodus 
tag sentence parts 
tags denote semantic concepts specific task domain 
introduced grammar non terminals rules added expansion word sequences differentiate rules ones generate sentences ll call tag rules sentence rules respectively 
vehicle transport long vehicle take move vehicle truck people vehicle helicopter 
necessary regroup rules produced previous step 
step may involve generalizing rules splitting rules may generate long sentences 
rules affected may sentence rules tag rules 
transport transport plan intro vehicle transport det group people people groups number pl groups people final grammar contained slightly sentence rules tag rules 
note order reduce generative power grammar long patterns split step multiple shorter patterns generate natural sentences 
apparent loss coverage real problem main purpose grammar provide realistic gram occurrences 
side note find methodology building grammar useful fixing vocabulary new domain 
aforementioned authors generate artificial corpus parser generative mode 
see fact doesn depend grouping separating alternatives denote optional expressions 
non terminals written italics actual words capitalized 
availability task specific parser obtained easily advantage approach 
eventually general purpose parser filter ungrammatical sentences artificial corpus turned improvements brought additional step significant 

reusing corpora similar domains solution adaptation problem outof domain ood text data 
estimating language model particular domain require huge quantities data unavailable especially building models new domain 
fortunately increasing amounts data available domains 
problem get relevant information non adapted domain lms proven worse model 

relevance domain data domain data improve lms generated insufficient domain data usually pointed useful degree similarity relevance target domain 
attempts quantify relevance 
symmetrical measures text similarity syllable models yamada shikano 
word models directed notion relevance appropriate 
iyer measures relevance prefer notion similarity content relevance account word distributions compared corpora style relevance posterior probability pos grams modified version pos grams replaced word grams attempts account content style 
think relevance domain corpus judged respect language modelling technique ultimately information language model affect recognition statistical language models contain part information original corpus 
statistical language models evaluated terms perplexity domain data set speech recognition terms word error rate 
indicator relevance domain corpus perplexity domain text corpus wrt back language model derived word error rate produced speech recognizer language model domain speech corpus haven attempted develop parser allen syntactic unification grammar tied way paci domain 
trips speech recognizer sphinx ii huang 
formal measure relevance certainly important aspect consider near 
text lm building lm 
class lms observed corpora differences usually studied terms content style iyer 
content naturally characterized terms vocabulary 
style differences domains studied biber terms occurrence patterns groups words grouping done part speech pos classes 
medium sized vocabulary speech recognition systems common way classifying words terms semantic concepts specific task domain ward 
kind class account style addition provide powerful means adapting content domain corpora 
usually class lms generalize observed word sequences unseen sequences compensate insufficient data 
show technique adaptation section 
general procedure generating class gram models follows roughly steps 
text corpus tagged predefined class tag dictionary 
back gram class model computed tagged text corpus 
class model converted word model word class mappings class tag dictionary 
proposed methodology order maximize overlap mappings different domains put general dictionary common words functional words pronouns common words 
words domain specific domain specific senses grouped separate dictionaries task domain 
class tags assigned hand 
domain vocabularies may differ significantly tagged corpora different domains look similar 
words domain specific easily spotted 
examples words different domains share tags tag domain atis tdc city corning seattle transport bus train truck cab scheme propose domain text corpora build lms new domains summarized steps text tagged text remote domain text tagged text word models remote domain tag dictionary tag dictionary lm building class models lm building target domain tag dictionary lm class lm target domain lm class lm adapted class models ood corpora block diagrams different lm generation procedures 

corpora tagged general tag dictionary appropriate domainspecific tag dictionary 
back class gram lms obtained tagged text corpora 
class gram lms converted word gram lm domain word class dictionary 
shows diagram process 
clear step tags expanded domain words acts filter grams discarded 
relevant information ood corpora contribute lm built 
possible ood corpus look relevant target domain sparse 
ood corpora combining information gotten 
combination done merging counts interpolating language models cf 
iyer ostendorf gish 
tried methods ll report results interpolated models performed better 
lm building cmu cambridge statistical language model toolkit clarkson rosenfeld 

experimental results ran perplexity recognition experiments trips transportation logistics domain ferguson allen 
ood corpora selected atis atis air travel information corpora dahl consisting utterances word tokens trains transportation scheduling domain corpus allen consisting utterances word tokens tdc human human spoken dialog corpus heeman allen consisting utterances word tokens 
setting trips system collected small domain corpus call trips utterances word tokens 
randomly selected utterances test data 
rest utterances build class bigram models trips domain 
model built small training corpus accurate performance provide indication model hope build 
able judge performance new models comparing perplexity recognition results trips models 
addition compare new language models null model words probability occurrence 
models open vocabulary bigram back models witten bell discounting witten bell 

testing artificial corpus approach test technique section grammar generate corpus sentences monte carlo sampling 
corpus filtered trips parser 
final artificial corpus call pac contained sentences word tokens obtain class back bigram lms comprehensive task specific vocabularies 
results shown table show significant improvement zero knowledge model null sufficiently close trips model performance 
trips model performance optimal reduction word error rate null model case class pac model task specific vocabulary advocated earlier parser crucial 
experiments similar corpus sentences generated grammar filtered parser gave comparable perplexity recognition results perplexity worse relative accuracy better relative proportion fully parsable sentences second corpus just 
proves generative grammar provides coverage possible word collocations bigram model encodes 
approach provides bigram coverage language model parameters may adapted target domain 
assume domain corpus comprehensive vocabulary task specific vocabulary null pac trips table test set perplexities word error rates pac model compared null trips models 
pac trips class models null word base model 
comprehensive vocabulary task specific vocabulary atis tdc table test set perplexities word error rates atis tdc word models comprehensive task specific vocabularies 
available 
ll show section interpolating artificial language model models derived ood spoken dialog corpora may help remedy deficiency quite 

testing usage domain corpora obtained perplexity recognition results tables types language models derived ood corpora word models class models comprehensive vocabulary words adapted word class models vocabulary words 
ood word lms word error rates better model improvement relative shows respective domains somewhat relevant trips domain 
performance extremely poor 
perplexity improvements baseline model significant remain order magnitude higher aim 
class models better relative compared word models 
time perplexity reduced relative 
furthermore vocabulary restricted target domain adaptation procedure described section word error rates go lower relative case class models 
simultaneously perplexity reduced relative 
similar results obtained technique case word models significantly poorer performance corresponding class models 
ood corpora artificial corpus comprehensive vocabulary task specific vocabulary atis tdc table test set perplexities word error rates atis tdc class models comprehensive task specific vocabularies 
red red tdc atis tdc pac atis tdc atis tdc table test set perplexities word error rates interpolated models relative reductions compared corresponding results best component 
different characteristics expect combining corresponding language models obtain better results 
linear interpolation various combinations models provided significantly better performance table 
individual models adapted class models 
cases interpolated models lower perplexity word error rates individual component models 
reduction word error rate relative compared best component model ood models interpolated relative compared pac model interpolated ood models 
time reductions perplexity compared best component perplexity ood models compared pac model interpolated ood models 
interpolation weights obtained posteriori em algorithm minimize perplexity trips test data relative interpolated model shows results possible doesn provide technique guessing right combination target domain data available case 
note data testing obtaining interpolation weights reasonable expect results table optimistic 
time conference trips data available re run experiments disjoint data sets training interpolation weights testing noted reduction perplexity accompanied reduction word error rate 
trips tdc pac trips atis tdc pac trips table effect interpolating domain lm adapted lms test set perplexities word error rates 
interpolated models 
consequently reliable trips model baseline 
started investigate adapted models improve domain models obtained limited amounts data 
results table show reduction perplexity 
caveats respect interpolation testing phases data set apply 
interpolated model obtained minimizing perplexity trips test data relative interpolated model didn provide reduction word error rate 
increase small significant 
intend investigate issue 

results obtained encouraging 
able build rapidly sufficiently medium sized vocabulary language models new task domain having domain specific data 
language model adaptation techniques proposed artificial corpora domain corpora provided performance results combination showed significant improvement 
system collect real data contrast wizard oz technique 
soon target domain data available think better results obtained adaptation models obtained data 
intend devote subsequent adaptation 
interesting area research find better ways judging relevance ood corpora better techniques filtering irrelevant parts better methods combining ood models maximize benefit provided model 

acknowledgments nathaniel martin initial collection utterances domain initiating abstraction cfg rules trains trips group university rochester helping collect transcribe speech data system support 
indebted cmu sphinx ii tools research 
funded part arpa rome laboratory contract 
onr 
nsf 
iri 

allen trains parsing system user manual 
trains tn department computer science university rochester 
allen miller sikorski 

robust system natural spoken dialogue 
proc 
acl pp 

biber 

variations speech writing 
cambridge university press 
clarkson rosenfeld 
statistical language modeling cmu cambridge toolkit 
proc 
eurospeech pp 

crespo alvarez 

language model adaptation conversational speech recognition automatically tagged pseudo morphological classes 
proc 
icassp pp 

dahl 
expanding scope atis task atis corpus 
proc 
arpa human language technology workshop pp 

ferguson allen 

trips intelligent integrated problem solving assistant 
proceedings fifteenth national conference artificial intelligence aaai madison wi july 
appear 
heeman allen 

trains dialogues 
trains tn corpus available ldc 
huang hwang rosenfeld 
overview sphinx ii speech recognition system 
proc 
arpa human language technology workshop pp 



estimation language models new spoken language applications 
proc 
icslp pp 

ito saitoh katoh 
gram language model adaptation small corpus spoken dialog recognition 
proc 
eurospeech pp 

iyer 
improving predicting performance statistical language models sparse domains 
phd dissertation boston university 
iyer ostendorf 
transforming domain estimates improve domain language models 
proc 
eurospeech pp 

iyer ostendorf gish 
domain data improve domain language models 
tr ece boston university 
jurafsky wooters segal 
berkeley restaurant project 
proc 
icslp pp 

jurafsky wooters segal stolcke morgan stochastic context free grammar language model speech recognition 
proc 
icassp pp 

multi site data collection spoken language corpus 
proc 
darpa speech natural language workshop pp 

yamada shikano 
task adaptation stochastic language models continuous speech recognition 
proc 
icassp pp 


language modelling task oriented domains 
proc 
eurospeech pp 

rayner carter 
hybrid language processing spoken language translator 
proc 
icassp pp 

rudnicky 
language modeling limited domain data 
proc 
arpa spoken language technology workshop 
ward 
class language model speech recognition 
proc 
icassp pp 

ge 
experiments adaptation language models commercial applications 
proc 
eurospeech pp 

witten bell 
zero frequency problem estimating probabilities novel events adaptive text compression 
ieee transactions information theory 
zue glass leung phillips polifroni seneff 
integration speech recognition natural language processing mit ager system 
proc 
icassp pp 

