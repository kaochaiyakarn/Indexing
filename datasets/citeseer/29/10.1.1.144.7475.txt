independence conditions optimality simple bayesian classi er pedro domingos dept information computer science university california irvine irvine ca ics uci edu simple bayesian classi er sbc commonly thought assume attributes independent class apparently contradicted surprisingly performance exhibits domains contain clear attribute dependences 
explanation proposed far 
show sbc fact assume attribute independence optimal assumption violated wide margin 
key nding lies distinction classi cation probability estimation correct classi cation achieved probability estimates contain large errors 
show previously assumed region optimality sbc second order nitesimal fraction actual 
followed derivation necessary su cient conditions sbc 
example sbc optimal learning arbitrary conjunctions disjunctions violate independence assumption 
reports empirical evidence sbc competitive performance domains containing substantial degrees attribute dependence 
simple bayesian classifier bayes theorem tells optimally predict class previously unseen example training sample duda hart 
chosen class maximizes ci ci ith class test example jx denotes conditional michael pazzani dept information computer science university california irvine irvine ca pazzani ics uci edu probability ofy probabilities estimated training sample 
example attributes 
attributes independent class decomposed product jci vj value jth attribute example predict class maximizes ci ay procedure called naive bayesian classi er 
prefer term simple abbreviate sbc 
sbc commonly thought tobe optimal sense achieving best possible accuracy independence assumption holds close optimal attributes slightly dependent 
restrictive condition inconsistent sbc surprisingly performance wide variety domains including clear dependencies attributes 
study datasets uci repository reported section sbc accurate domains similarly cn pebls 
authors similar observations clark niblett langley iba thompson kasif salzberg aha dougherty kohavi sahami interpretation proposed far 
extensions sbc introduced goal increasing tolerance attribute dependences kononenko langley langley sage pazzani usually moderate success 
shed light matter showing sbc fact optimal independence assumption grossly violated applicable broader range domains previously thought 
essentially due fact cases eq 
may produce poor probability estimates correct class highest estimate leading correct classi cation 
empirical section showing simple example illustrates key points 
section contains fundamental result derivation necessary su cient conditions local optimality sbc optimality example 
result generalized necessary su cient condition sbc global optimality optimality dataset 
show fundamental limitations sbc optimal learning conjunctions disjunctions 
empirical evidence order investigate sbc performance compared classi ers relate degree attribute dependence data empirical study carried large varied selection datasets uci repository murphy aha 
sbc numeric values discretized equal length intervals observed value whichever 
give results assuming normal distributions done pattern recognition literature dougherty kohavi sahami 
missing values treated having value training testing times 
avoids losing potentially useful information 
example medical domains missing values indicate doctors considered corresponding tests unnecessary 
null attribute probabilities replaced ci number training examples done clark niblett 
sbc compared state art representatives major approaches classi cation learning decision tree induction quinlan instance learning pebls cost salzberg rule induction cn clark boswell 
default classi er assigns frequent class test examples included 
runs conducted dataset randomly selecting data training remainder testing 
accuracies obtained shown table 
results summarized table 
rst line shows number domains sbc accurate corresponding classi er versus number 
example sbc accurate domains 
second line considers domains accuracy di erence signi cant level tailed paired test 
example sbc signi cantly accurate datasets 
measures sbc wins approaches exception number signi cant wins vs pebls 
line shows average rank algorithm computed domain assigning rank accurate algorithm rank second best including default classi er 
sbc best ranked algorithms indicating win tends second best 
sbc quite competitive approaches 
remarkably result simple apparently limited classi er 
due datasets representing easy concepts holte disprove notion sbc relies assumption attribute independence 
investigate need measure degree attribute dependence data way 
measuring high order dependencies di cult relevant probabilities apt small reliably represented data 
rst feasible approach consists measuring pairwise dependencies dependencies pairs attributes class 
attributes am class variable possible measure degree pairwise dependence am wan wong kononenko am represents cartesian product attributes am derived attribute possible value corresponding combination values am classes attribute values ci ci aj log ci aj am am completely independent increases degree dependence maximum occurring class attribute completely determine 
computed classes attribute pairs dataset uniform discretization ignoring missing values excluding pairings attribute 
results shown table 
comparison purposes rst column shows annealing audiology domains omitted relevant entropies computed 
table empirical results average accuracies standard deviations 
superscripts denote signi cance levels di erence accuracy sbc corresponding algorithm tailed paired test 
domain sbc default pebls cn audiology annealing breast cancer credit screening chess endgames pima diabetes echocardiogram glass heart disease hepatitis horse colic thyroid disease iris labor neg 
lung cancer liver disease led lymphography post operative promoters primary tumor solar sonar soybean splice junctions voting records wine zoology sbc rank domain accurate algorithm second accurate second column shows maximum value observed dataset 
third column shows percentage attributes exhibited degree dependence attribute 
fourth column shows average attribute pairs dataset 
table leads important observations 
sbc achieves higher accuracy sophisticated approaches domains substantial attribute dependence reason comparative performance attribute dependences data 
correlation aver value commonly threshold attributes considered signi cantly dependent 
age degree attribute dependence di erence accuracy sbc algorithms small pebls cn attribute dependence predictor sbc di erential performance vs approaches take account 
sbc surprisingly performance remains unexplained 
remainder shed light matter 
table 
summary accuracy results 
measure sbc pebls cn 
wins 
sig 
wins rank table empirical measures attribute dependence 
domain rank dmax hi 
breast cancer credit screening chess endgames pima diabetes echocardiogram glass heart disease hepatitis horse colic thyroid disease iris labor neg 
lung cancer liver disease led lymphography post operative promoters primary tumor solar sonar soybean splice junctions voting records wine zoology example consider boolean concept described attributes classes equiprobable 
example aj shorthand aej ae value attribute instance similarly attributes 
independent completely dependent 
ignored optimal classi cation procedure test instance assign class aj cj aj cj class inequality opposite sign classi cation arbitrary sides equal 
hand sbc take account independent equivalent counting twice 
sbc assign instance class aj cj aj cj 
applying bayes theorem aj re expressed ja similarly probabilities 
canceling terms leads equivalent expressions sbc optimal decision boundaries sbc optimal classi er 
ja jc ja jc optimal decision ja jc ja jc sbc 
ja jc class win pq equivalent sbc win equivalent fig 

remarkable fact independence assumption decisively violated sbc disagrees optimal procedure narrow regions curves performs correct classi cation 
problems fall small regions sbc ectively optimal 
contrast independence assumption optimal expressions identical isolated points curves cross curves shown 
shows sbc range applicability fact broader previously thought 
section examine general case formalize result 
local optimality necessary de nitions 
de nition bayes rate example lowest error rate achievable classi er example duda hart 
de nition classi er locally optimal example error rate example equal bayes rate 
de nition classi er globally optimal sample dataset locally optimal example sample 
classi er globally optimal problem domain globally optimal possible samples problem datasets extracted domain 
consider class case general 
classes je qa qa refer eq 
section derive necessary su cient condition local optimality sbc show volume sbc region optimality space valid values half total volume space 
key results lies distinction classi cation probability estimation 
equation yields correct estimate class probabilities independence assumption holds purposes classi cation class probability estimates diverge widely true values long maximum estimate corresponds maximum true probability 
example suppose classes je je true class probabilities example optimal decision assign class 
suppose equation gives estimates je je 
independence assumption violated wide margin sbc optimal decision 

region optimality sbc 
theorem sbc locally optimal exam ple proof 
sbc optimal error rate minimum possible 
je minimum error obtained assigning class 
sbc assigns class je je eq 
sbc optimal 
conversely je minimum error obtained assigning class sbc sbc optimal decision optimal inequalities generalized shown 
corollary sbc locally optimal half volume space values 
proof 
probabilities takes values unit cube region cube satisfying condition theorem shown shaded fig 
easily seen occupy half total volume cube 
pairs correspond valid probability combinations 
unconstrained projection space valid probability combinations planes 
theorem region optimality planes region vice versa planes optimal region projections photographic negative optimal region projections 
area projection area optimal region area optimal region total volume region optimality 
probability combination region optimality symmetric contrast independence assumption sbc optimal line planes intersect 
previously assumed region optimality sbc secondorder nitesimal fraction actual 
global optimality extension theorem global optimality immediate 
example indexed pe re se 
theorem sbc globally optimal sample dataset pe re se 
re se pe proof 
de nition theorem 
verifying condition directly test sample general possible involves nding true class probabilities examples sample 
verifying domain possible samples extracted domain general involve computation size proportional number possible examples exponential number attributes computationally infeasible 
remainder section dedicated investigating concrete conditions optimality sbc necessary su cient 
necessary conditions number attributes number classes maximum number values attribute number di erent numbers representable machine implementing sbc 
example numbers represented bits 
theorem sbc globally optimal av di erent problems 
proof 
sbc state composed av probabilities probability di erent values sbc av states distinguish number concepts 
av large signi cant restriction concept classes size doubly exponential arbitrary dnf formulas boolean domains due extremely rapid growth function sbc capacity exceeded commonly occurring values hand restriction compatible concept classes size grows exponentially conjunctions 
result re ects sbc limited information storage capacity contrasted case classi ers instance rule decision tree learners memory size proportional sample size 
shows condition theorem satis ed exponentially decreasing fraction possible domains increases 
consistent fact local optimality veri ed possible combination attribute values sbc globally optimal domain de nition probability decreases exponentially starting 
similar statement true learners simply re ects fact optimally learn wide class concepts 
sbc information storage capacity iso 
training set size learners memorize individual examples equivalent storage capacity ea ability principle converge optimal 
nite value fraction problems learners optimal starts decrease exponentially theorem symbolic domains sbc globally optimal linearly separable problems 
proof 
de ne boolean feature bjk attribute value bjk aj kth value attribute aj 
logarithm eq 
sbc equivalent linear machine duda hart discriminant function class ci log ci log aj bjk weight ofeach boolean feature log probability corresponding attribute value class 
su cient condition sbc learn linearly separable concepts 
example narrowly fails concept concept composed examples boolean attributes true kohavi 
boolean domains sbc range subset perceptron duda hart 
numeric domains sbc restricted linearly separable problems example classes normally distributed nonlinear boundaries multiple disconnected regions arise sbc able identify see duda hart 
sufficient conditions section establish sbc optimality common concept classes 
theorem sbc globally optimal classes ci examples va qa 
result restated completeness 
crucial point condition su cient necessary 
theorem sbc globally optimal learning conjunctions literals 
proof 
suppose literals lj conjunction 
literal may boolean attribute negation 
addition may bea irrelevant attributes simply cause line truth table lines values class relevant attributes lines corresponding possible combination irrelevant attributes 
simplicity ignored assumed loss generality 
recall truth table conjunction class false ln true 
bar denote negation literal truth table number number times times class number times literal minus time corresponds number times class 
arbitrary example conjunction literals true simplicity factor omitted probabilities 
cje cje notice cje cje class wins 
cje cje class wins 
sbc correct decision globally optimal 
notice conjunctive concepts verify independence assumption class class 
example jc jc inspection truth table 
conjunctions example class concepts sbc fact optimal required attribute independence 
theorem sbc globally optimal learning disjunctions literals 
proof 
similar theorem letting number disjunction literals false conversely disjunctions verify independence assumption class class example sbc independence assumption violated 
corollaries sbc optimal negated conjunctions negated disjunctions identity negation functions number irrelevant attributes 
veri ed sbc performs quite practice strong attribute dependences showed part due fact contrary previous assumptions sbc depend attribute independence optimal 
derived number necessary number su cient conditions sbc optimality 
particular showed sbc optimal learner conjunctive disjunctive concepts violate independence assumption 
ideally wewould set necessary su cient conditions optimality sbc ciently veri able real problems 
previous section began goal 
sbc optimal limit nite sample classi ers may converge faster bayes rate certain problems 
investigating behavior sbc probability estimates employs imperfect due niteness sample interest 
important area research concerns nding conditions sbc optimal comes close example wrong prediction small fraction examples 
optimal sbc perform relative algorithms long closer optimum 
may explain results empirical section 
summary reported demonstrates sbc greater range applicability previously thought 
advantages terms learning speed classi cation speed storage space incrementality suggests considered 
partly supported praxis xxi scholarship 
authors grateful provided datasets empirical study 
clark boswell 

rule induction cn improvements 
proceedings sixth european working session learning pp 
porto portugal 
springer verlag 
clark niblett 

cn induction algorithm 
machine learning 
cost salzberg 

weighted nearest neighbor algorithm learning symbolic features 
machine learning 
dougherty kohavi sahami 

supervised unsupervised discretization continuous features 
proceedings twelfth international conference machine learning pp 
tahoe city ca 
morgan kaufmann 
duda hart 

pattern classi cation scene analysis 
new york ny wiley 
holte 

simple classi cation rules perform commonly datasets 
machine learning 
kohavi 

wrappers performance enhancement oblivious decision graphs 
phd thesis department computer science stanford university stanford ca 
kononenko 

semi naive bayesian 
proceedings sixth european working session learning pp 
porto portugal 
springer verlag 
langley 

induction recursive bayesian classi ers 
proceedings eighth european conference machine learning pp 
vienna austria 
springer verlag 
langley iba thompson 

analysis bayesian classi ers 
proceedings tenth national conference onarti cial intelligence pp 
san jose ca 
aaai press 
langley sage 

induction selective bayesian classi ers 
proceedings tenth conference uncertainty arti cial intelligence pp 
seattle wa 
morgan kaufmann 
murphy aha 

uci repository machine learning databases 
machinereadable data repository department information computer science university california irvine irvine ca 
pazzani 

searching attribute dependencies bayesian classi ers 
preliminary papers fifth international workshop arti cial intelligence statistics pp 
fort lauderdale fl 
society arti cial intelligence statistics 
quinlan 

programs machine learning 
san mateo ca morgan kaufmann 
kasif salzberg aha 

better understanding memory reasoning systems 
proceedings eleventh international conference machine learning pp 
new brunswick nj 
morgan kaufmann 
wan wong 

measure concept dissimilarity applications machine learning 
proceedings international conference computing information pp 
toronto canada 
north holland 
