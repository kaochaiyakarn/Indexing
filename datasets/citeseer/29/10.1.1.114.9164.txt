combining labeled unlabeled data training blum school computer science carnegie mellon university pittsburgh pa cs cmu edu consider problem large unlabeled sample boost performance learning algorithm small set labeled examples available 
particular consider setting description example partitioned distinct views motivated task learning classify web pages 
example description web page partitioned words occurring page words occurring hyperlinks point page 
assume view example su cient learning labeled data goal views allow inexpensive unlabeled data augment smaller set labeled examples 
speci cally presence distinct views example suggests strategies learning algorithms trained separately view algorithm predictions new unlabeled examples enlarge training set 
goal provide pac style analysis setting broadly style framework general problem learning labeled unlabeled data 
provide empirical results real web page data indicating unlabeled examples lead signi cant improvement practice 
part analysis provide new re extended version appeared proceedings th annual conference computational learning theory pages 
research supported part darpa hpkb program contract nsf national young investigator ccr 
tom mitchell school computer science carnegie mellon university pittsburgh pa mitchell cs cmu edu sults learning misclassi cation noise believe interest 
machine learning settings unlabeled examples signi cantly easier come labeled ones :10.1.1.122.539
example web page classi cation 
suppose want program electronically visit web site download web pages interest cs faculty member pages course home pages university :10.1.1.35.6633
train system automatically classify web pages typically rely hand labeled web pages 
labeled examples fairly expensive obtain require human ort 
research supported part darpa hpkb program contract nsf national young investigator ccr 
tom mitchell school computer science carnegie mellon university pittsburgh pa mitchell cs cmu edu sults learning misclassi cation noise believe interest 
machine learning settings unlabeled examples signi cantly easier come labeled ones :10.1.1.122.539
example web page classi cation 
suppose want program electronically visit web site download web pages interest cs faculty member pages course home pages university :10.1.1.35.6633
train system automatically classify web pages typically rely hand labeled web pages 
labeled examples fairly expensive obtain require human ort 
contrast web hundreds millions unlabeled web pages gathered web crawler 
learning algorithm able take advantage unlabeled data possible 
problem characteristics mentioned availability unlabeled data existence di erent somewhat redundant sources information examples suggest learning strategy 
initial small set labeled examples nd weak predictors kind information instance nd phrase research interests web page weak indicator page faculty home page nd phrase advisor link indicator page pointed faculty page 
attempt bootstrap weak predictors unlabeled data 
instance search pages pointed links having phrase advisor probably positive examples train learning algorithm words text page vice versa 
call type bootstrapping cotraining close connection bootstrapping incomplete data expectation maximization setting see instance :10.1.1.56.6066
question raises reason believe training help 
goal address question developing pac style theoretical framework better understand issues involved approach 
process provide new results learning presence classi cation noise 
give preliminary empirical results classifying pages see section encouraging context 
training scenario speci notion compatibility previous section especially natural imagine forms compatibility settings 
discuss relations model analyzing labeled unlabeled data 
standard setting problem analyzed assume data generated simple known parametric model 
assumptions form castelli cover precisely quantify relative values labeled unlabeled data bayesian optimal learners 
em algorithm widely practice learning data missing information analyzed :10.1.1.133.4884
instance common speci assumption positive examples generated dimensional gaussian centered point negative examples generated gaussian centered point unknown learning algorithm 
examples generated choosing positive point negative proba bility 
case bayes optimal hypothesis linear separator de ned hyperplane orthogonal line segment 
parametric model rigid pac compatibility setting sense incorporates noise bayes optimal hypothesis perfect classi er 
instance case gaussians consider class linear separators really concepts compatible underlying distribution unlabeled examples optimal negation 
words knew underlying distribution possible target concepts left 
view surprising unlabeled data helpful set assumptions 
proposal compatibility function concept probability distribution attempt broadly consider distributions completely commit target function completely uncommitted 
approach unlabeled data yarowsky word sense disambiguation problem closer spirit cotraining nicely viewed model :10.1.1.122.539
problem yarowsky considers 
words quite di erent dictionary de nitions 
instance plant mean type life form factory 
text document instance word plant goal algorithm determine meaning intended 
problem yarowsky considers 
words quite di erent dictionary de nitions 
instance plant mean type life form factory 
text document instance word plant goal algorithm determine meaning intended 
yarowsky unlabeled data observation xed document highly instances word plant intended meaning whichever meaning happens :10.1.1.122.539
uses observation learning algorithm learns predictions local context achieve results labeled examples unlabeled ones 
think yarowsky approach context training follows 
example instance word plant described distinct representations 
rst representation unique id document word 
generally views achieve tradeo number labeled unlabeled examples needed 
consider graph gs graph edge observed example see observe unlabeled examples number connected components drop components merge nally components gd 
furthermore set random subset label probability label random st example chosen remaining portion deduced algorithm cj gs jsj sj sj jsj plausible context web pages think document small set attributes document 
sj cj jsj formula approximately cj gs sj jsj sj jsj analogy equation 
fact results study random graph processes describe quantitatively expect components gs converge gd see unlabeled examples properties distribution connected component gd value minimum cut minimum cuts sum weights edges cut :10.1.1.114.5693
words probability random example cross speci minimum cut 
clearly sample contain spanning tree include component edge minimum cut 
expected number unlabeled samples needed occur course spanning tree include edge cut 
karger shows nearly su :10.1.1.114.5693
fact results study random graph processes describe quantitatively expect components gs converge gd see unlabeled examples properties distribution connected component gd value minimum cut minimum cuts sum weights edges cut :10.1.1.114.5693
words probability random example cross speci minimum cut 
clearly sample contain spanning tree include component edge minimum cut 
expected number unlabeled samples needed occur course spanning tree include edge cut 
karger shows nearly su :10.1.1.114.5693
speci cally theorem shows log unlabeled samples su cient ensure spanning tree high probability :10.1.1.114.5693
hg log unlabeled samples su cient ensure number connected components sample equal number minimizing number labeled examples needed 
instance suppose points positive negative similarly distribution uniform subject placing zero probability illegal examples 
case legal example probability reduce observed graph connected components need see edges 
words probability random example cross speci minimum cut 
clearly sample contain spanning tree include component edge minimum cut 
expected number unlabeled samples needed occur course spanning tree include edge cut 
karger shows nearly su :10.1.1.114.5693
speci cally theorem shows log unlabeled samples su cient ensure spanning tree high probability :10.1.1.114.5693
hg log unlabeled samples su cient ensure number connected components sample equal number minimizing number labeled examples needed 
instance suppose points positive negative similarly distribution uniform subject placing zero probability illegal examples 
case legal example probability reduce observed graph connected components need see edges 
spanning trees 
assumption ensures quantity small 
proof 
proof just straightforward calculation 
prd jf prd jf experiments order test idea training applied problem learning classify web pages 
particular experiment motivated larger research ort apply machine learning problem extracting information world wide web :10.1.1.35.6633
data experiment consists web pages collected computer science department web sites universities cornell university university wisconsin 
pages hand labeled number categories 
experiments considered category course home page target function course home pages positive examples pages negative examples 
dataset web pages course pages 
dataset web pages course pages 
example web page considered bag multi set words appearing web page bag words underlined links pointing web page pages database 
classi ers trained separately naive bayes algorithm 
refer page hyperlink classi ers respectively 
naive bayes algorithm empirically observed successful variety text categorization tasks :10.1.1.49.860
training algorithm described table 
set labeled examples set unlabeled examples algorithm rst creates smaller pool containing unlabeled examples 
iterates procedure 
train distinct classi ers 
prune away incompatible target concepts reduce number labeled examples needed learn 
open question extent consistency constraints model mutual independence assumption section relaxed allow provable results utility training unlabeled data 
preliminary experimental results suggest method unlabeled data potential signi cant bene ts practice studies clearly needed 
conjecture practical learning problems approximately training model 
example consider problem learning classify segments television broadcasts say learning identify segments containing president :10.1.1.46.5528
set possible video images set possible audio signals cross product 
small sample labeled segments learn weakly predictive recognizer spots full frontal images president face recognizer spots voice background noise 
training applied large volume unlabeled television broadcasts improve accuracy classi ers 
similar problems exist perception learning tasks involving multiple sensors 
pattern recognition letters january 
castelli cover 
relative value labeled unlabeled samples unknown mixing parameter 
ieee transactions information theory november 
craven freitag mccallum mitchell nigam :10.1.1.35.6633
learning extract symbolic knowledge world wide web 
technical report carnegie mellon university january 

pac learning classi cation noise applications decision tree induction 
technical report carnegie mellon university january 

pac learning classi cation noise applications decision tree induction 
proceedings fourteenth international conference machine learning pages july 
dempster laird rubin :10.1.1.133.4884
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
richard duda peter hart 
pattern classi cation scene analysis 
jackson tomkins 
computational model teaching 
proceedings fifth annual workshop computational learning theory pages 
morgan kaufmann 
karger :10.1.1.114.5693
random sampling cut ow network design problems 
proceedings sixth annual acm symposium theory computing pages may 
karger 
random sampling cut ow network design problems 
journal version draft 
kearns 
cient noise tolerant learning statistical queries 
proceedings fifth annual acm symposium theory computing pages 
lewis ringuette :10.1.1.49.860
comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pages 
joel :10.1.1.122.539
learning mixture labeled unlabeled examples parametric side information 
proceedings fifth annual acm symposium theory computing pages 
lewis ringuette :10.1.1.49.860
comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pages 
joel :10.1.1.122.539
learning mixture labeled unlabeled examples parametric side information 
proceedings th annual conference computational learning theory pages 
acm press new york ny 
witbrock hauptmann 
acm press new york ny 
witbrock hauptmann 
improving acoustic models watching television 
technical report cmu cs carnegie mellon university march 
yarowsky :10.1.1.122.539
unsupervised word sense disambiguation supervised methods 
proceedings rd annual meeting association computational linguistics pages 
