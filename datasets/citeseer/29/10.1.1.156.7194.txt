replica management system scheme flexible dynamic replication little department computing science university newcastle tyne newcastle tyne ne ru uk xerox webster new york appear proceedings second workshop distributed systems pittsburgh march 
actual gains achieved replication complex function number replicas placement replicas replication protocol nature transactions performed replicas availability performance characteristics machines networks composing system 
describes design implementation replica management system allows programmer specify quality service required replica groups terms availability performance 
quality service specification information replication protocol data characteristics underlying distributed system rms computes initial placement replication level 
machines communications systems detected failed recovered performance characteristics change rms re invoked compute updated mapping replicas preserves desired quality service 
result flexible dynamic dependable replication system 
keywords replication reliability configuration system management 

placement objects distributed system critical effect reliability performance applications objects 
objects high reliability requirements placed machines high estimated reliability 
particularly important information may replicated nodes network employing suitable additional protocol ensure consistency 
actual increase availability results replication function number replicas placement replicas 
example replicating critical name server object machines receive power wall socket increase tolerance power failures 
placing replicas unreliable nodes decrease performance replica consistency protocol recover failures may result decreased availability 
locating objects generally improve performance replicas object generally increase reliability reduces number nodes remain operational application complete successfully 
choices involved providing high availability conflict high performance goals 
conflicts ignored application suffer overhead imposed having replicas drastically reduce performance application 
factors dynamic changing system progresses evolves 
placement object correct application started may incorrect executing time 
investigating design replica management system rms allows programmer specify quality service required replicated objects terms availability performance 
quality service specification information replication protocol data characteristics underlying distributed system rms computes initial placement replication level replicated object 
machines communications links detected failed recovered user interactions object change rms re invoked compute updated mapping replicas preserves desired quality service 
describe conducted identifying various factors relevant replica placement designed rms take account 

related christian describes service called availability manager attempts maintain level availability detecting failure recovery replicas adjusting replication level accordingly 
availability manager focuses mechanism maintaining level replication directly address issue maintaining level availability maintaining constant level replication ensure constant level availability 
fact describe increasing replication level may decrease availability 
mars system advanced terms consideration placement decisions attempts place objects nodes provide reliability characteristics consistent aims application 
analysis placement performed compile time fixed duration execution application 
approach eminently sensible real time embedded systems applications mars targeted considering general distributed system environment long running applications may require dynamic adaptive replication policy ensure long term availability performance 
performance affected placement decisions protocol choices 
consider effects placing replicas unreliable nodes 
resulting unreliability replicas generally require replica consistency protocols harder increasing network message traffic processing overheads 
performance suffer replication protocol ensure replicas consistent despite failures availability may decrease despite increased number replicas 
coffman discuss effects system performance varying number replicas distributed data base system 
provide convincing evidence effect performance increasing number replicas 
address availability effects effects changing replication protocols 
wolfson describe algorithm dynamically varying degree data replication depending characteristics user interactions 
consider implications reliability components performance effects different replication protocols 
rest structured follows section briefly describes types replication protocol issues involved replica placement 
section describes issues identified important placement replicas need accommodate rms 
section describes rms associated infra structure 
section gives preliminary results indications 

object replication basically types replica consistency protocol active passive 

active replication functioning member replica group receives requests performs processing sends replies 
active replication object requires conditions met agreement non faulty replicas object receive identical input messages order non faulty replicas process messages identical order 
non faulty replicas object identical initial states identical output messages identical order produced provided course computation performed object selected message deterministic 
underlying principle state machine approach active replication 
active replication preferred choice supporting high availability real time services masking replica failures minimum time penalty considered highly desirable 

passive replication member replica group coordinator primary performs processing checkpoints state rest secondary replicas backups 
primary fails new primary elected backups 
advantages passive replication implemented recourse complex order preserving communication protocols 
single member group processes requests computation performed replicated objects need deterministic primary imposes results backups guaranteeing functioning members group possess state 
disadvantages protocol primary fails time taken elect new primary considerable overhead 
replica consistency protocols object oriented systems relatively understood shall consider protocols function rest 

replication protocols assuming mechanism exists generating replicas object keeping consistent application programmer facility increase availability critical object 
systems programmer simply specifies replication level copies run time system determines random pre determined placement replicas network 
systems programmer may able specify locations replicas 
approaches inadequate reasons programmer possible basis choosing replicas availability vary proportionally number replicas 
placement described fails take account reliability performance failure interdependencies nodes replicas placed 
mapping static failing compensate going changes network topology load 
availability performance replicated object complex function factors including number replicas placement replicas reliability nodes located load average nodes replicas located failure independence nodes network reliability system components consistency recovery protocols maintain replicas object interdependencies 
programmer attempting improve availability object replicating parameters typically controlled number placement replicas choice replication protocol 
rms developing provides better indication optimum number replicas location distributed environment 
addition dynamically control number location replicas information take account changes network load user interactions applications run typically giving better performance possible manual intervention 

replica placement considerations describing rms shall examine factors identified relevant replica placement giving indication intend take account designing rms 

component reliability term component refer major hardware software components distributed system nodes processors communication links networks 
visible useful reliability node dependent reliability related components system 
example workstation fails year seconds connected rest network communication link loses messages minutes appear unreliable 
necessary consider interactions components determine reliability specific component 
logical component refer set components dependant failure means failure rest high probability 

node reliability gross measures reliability node mean time failure mttf mean time recovery mttr 
node low mttf high mttr unsuitable placing object replicas active long time require high reliability give values figures call reliability values node necessary monitor system components nodes case time continue monitor applications run 
tolerance value computed statistic indicate accuracy 
system nodes continually re compute values keep date possible 
reliability value outside computed tolerances reconfiguration may required 
assumed literature reliability definite quantity consistently viewed users user determines machine reliability rz user reliability value simplification may true 
example fails roughly hour user interacts minutes observe reliability rz user interacts hours determine reliability rz rz typically greater rz probability user attempting contact failed probability user 
believe making assumption user user view reliability node typically pessimistic view reduces possible effectiveness distributed system 
example machine may assumed low reliability result machines network may overloaded 
means specifying expected rate interaction subsequent monitoring required 
expected rate interaction weight determine perceived reliability values machines network 
assumption mttf mttr past performance useful predictor performance assumption may true systems 

communication links nodes reliability communication links likewise monitored 
statistics obtained expressed mttf mttr manner believe informative express probability message loss probability network partitioning 
node reliability values tolerances assigned figures 
message loss typically result problems communications media occurring receiving node example message buffer overflow 
able differentiate causes message loss difficult simplicity shall assume characteristic communications link 
research necessary determine exact effects assumption 

common modes failure reliability values described give indication reliability individual system components 
indicate dependencies example workstations 
failure independence nodes network assumed analytical simplifies calculation 
rarely matches reality system configurations 
express failure dependencies nodes suggest methods network administrator manually enters failure dependencies interest policy process 
envision form directed graph indicating various nodes system explicitly showing dependencies shown 
identified kinds inter node failure dependencies common modes failure including shared power source common sub network common machine architecture operating system network disk dependencies node booting diskless accessing critical software remote file system 
dependency arcs labelled probability common failure implies failure node head arc certainly causes failure node tail arc 
kinds failure dependencies defined 
research needed determine kinds dependency significant network 
example interdependencies listed static properties network topology individual machines dynamic factors affect reliability studies indicate probability failure node increases direct proportion load 
currently investigating graphical display methods false colour display contour map overlayed representation distributed environment 
scheme may give better indication possible hot spots environment 
graphical representation node interdependencies ii system attempts automatic detection common failure patterns develops failure correlation matrix 
believe combining methods possible obtain better indication dependencies distributed system 
system builds dependency data input system administrator continually refining new data acquired 
initial dependency data system administrator obviously rms know inter node dependencies initial placements may accurate 
system runs data obtained rms develops failure correlation matrix re evaluates object placements 
aid system administrator possible obtain failure correlation matrix rms time 
displayed manipulated number different ways possibly re rms 

object interdependencies related component interdependencies importance greater likelihood inter object dependencies 
object invokes method object said dependant object 
degree dependency degree coupling objects different factors including frequency method invocation properties object invoked 
dependencies objects change rapidly machine interdependencies vary execution application 
dynamic quality means monitoring object interdependencies necessity affect performance availability object 
example mentioned performance availability typically increased locating objects dependant 
objects migrated machine machine example rms decides reconfiguration required associated dependency data may indicate objects migrated performance availability maintained addition dependency information may indicate migration possible example object migrated may critically dependant object migrated 
may decided migrate 

node performance values performance node complex function load node number active processes executing node configuration speed processor amount memory available 
load balancing techniques documented literature 
concerned improved availability performance object require global picture distributed environment considering characteristics single node 
example placing object lightly loaded machine connected rest network low bandwidth communications link may object requires distributed interactions reduce performance object 
addition object dependencies described section 
need taken account 
example object excessive resources reside relatively heavily load node may sense place object node overhead involved placing object overloaded node may outweighed overhead incurred remotely communicating required resources 
general resource utilisation patterns objects placed resources provided nodes placed need taken account 
obviously may difficult application initially executed access patterns resource utilisation mappings may evolve time 
static mapping may better mapping 
investigating ways obtaining dynamic mapping object resource utilisation corresponding resources available machines distributed environment 

communication link performance performance communication link expressed terms fast deliver message reliability value bandwidth possesses requirements different applications communications layer expanding changing rapidly new communications media 
example new fddi atm technologies offer advantages real time multi media applications exist 
monitoring performance various communications media available distributed environment essential performance maintained 
monitor performance communications links system currently believe machine performance dynamic factor statically allocated 
discussion see great deal information relevant replicated object placement 
continue address factor separately rest shall talk terms component attribute values 
shall describe replica management system obtains uses information mentioned arrive optimum configuration replica group 

replica management system shows replica management system single node various components interactions indicated 
sections shall examine component separately give indication interact 
replica management system network replica management system 
acquiring maintaining attribute values efficiency fault tolerance purposes rms available multiple nodes 
ensure relevant system attribute values available machine distributed environment regardless rms currently residing 
way time taken create new rms minimal 
nodes physically remote necessary provide means node determine attribute values nodes system 
similarly new node added system discover attributes nodes system 
done variety ways 
shall assume lazy distribution attributes hours node system broadcasts node attribute values reliability values average load associated tolerances 
insist reliable communications protocol employed delivery communication done required node receive new value time ask timeout period simply continue possibly date values previous broadcast 
new node connected system simply contact existing node get complete set attribute values node contains attribute values broadcast nodes date reply receives 
new node broadcast attribute values piggybacked initial request 
recall attribute value associated tolerance 
node determines attribute values fallen tolerances originally specified initiate early broadcast inform nodes new values 
note frequency attribute value updates altered reflect nature distributed system considered 
example volatile network nodes continually added deleted show characteristics mean stay tolerances long may necessary increase frequency updates 
dissemination information required delivered nodes system example network manager determines node database attribute values date possible reliable communication protocol ensure high degree probability operational nodes receive information despite failures 

monitor daemon key components rms monitor daemon regular intervals logs current time date attribute value information current load average 
similar described 
node recovery daemon examines log computes mttf mttr values 
logging information possible determine failure due crash possibly regular clean shutdown 
able differentiate various causes node unavailability important able specify accurate mttf mttr values 
addition able monitor local machine progress daemon instructed monitor progress machines system 
configuration information supplied daemon started executing 
system utilities certain disk partitions mail application monitored 
monitoring information reasonably accurate picture distributed environment built 

dependency tracker addition monitor daemon described designed implemented dependency tracker takes input data various monitor daemons outputs various graphical text formats information component dependencies exist distributed environment 
addition searching dependencies current system data checks periodicities may exist failure dependencies maintaining complex history data 
periodicities important may possible predict ahead time machine going unavailable specific period time maintenance move objects available 
algorithm determine possible dependencies perceived availability nodes period time 
collating information various monitor daemons possible build node availability list node system 
list nodes agreed monitor daemons failed specified time period necessary daemons possible daemon may possess different view availability system components 
node unavailable time node unavailable dependant probability failure results failure 
note relation circular examine may dependant various experiments decided value appears give reasonable results tuned system basis may need modified system configuration changes 
investigating algorithms dependency decision possibly described 

placement policy depicts inputs outputs ppm managing availability 
policy decision require user required quality service fail individual reliability values components system mttf mttr estimate read write ratio operations accessing object expected frequency operations performed object useful described section indication type replication protocol maintain consistency replicas 
placement policy module 
placement policy module placement policy module responsible computing number placement replicas 
expect conflicts performance availability generally resolved attempting placement maximize performance certain availability limit 
typically user specify availability factor expecting system get maximum performance constraint 
believe means ranking availability performance provided application basis 
may mean cases user interested having particular application run faster expense reliability 
may need means ranking applications share objects different applications objects may require different availability performance constraints objects 
ppm attempt placement consistent requests may possible certain application requests may fail 
retried modified set requirements ppm may able achieve placement consistent user requirements 
output placement policy replication level placement node list achieve desired level availability 
availability nodes computed mttr mttf values represents probability entire duration operation tend node operational 
call duration availability node 
duration availability function probability node failed tend repair occurred node continued function tend 
details calculations 
individual node availabilities calculated node interdependencies accounted obtain final availability value node logical components described section 
form basis placement model 
currently investigating placement policies described 
addition ppm modelled neural net investigating described better algorithm learn rapidly previous placement decisions 
way individual nodes duration probabilities combined calculate probability availability replicated object determined replication protocol 
mentioned section 
relative frequency operations taken account purposes discussion shall assume duration availability values observed users 
example available copies replication protocol tolerate node failures just replicas placement need include single node duration probability greater equal required availability 
majority voting protocol majority nodes duration probability greater equal required availability 
furthermore replication protocols behave differently read operations write operations calculations take account estimated read write ratio object 

object management module applications executing distributed environment attribute values individual nodes may drift tolerances far warrant re distribution objects maintain desired levels availability performance 
case object replicas may migrated current locations reliable faster nodes new replicas may need created 
new nodes added system may possible downsize replica groups moving replicas new nodes maintaining required quality service 
object management module responsible monitoring system determining reconfiguration required 
research distributed load sharing led load changes may rapid frequent practical adjustments system reliability measures mttf mttr tend fairly stable relatively long periods time 
availability sharing adjustments maintain relatively constant level availability possible 
replicas created example number read requests exceeded certain level effect reducing load individual replicas improving read response time 
number write requests increased assuming availability requirement met may possible number replicas reduced improving response time write operations 
possible user change values originally ppm alter constraints provide additional fault tolerance 
support user driven migration require kind user interface management processes control placement objects system 
rms currently support intend add interface 

dynamic group changes entities accurately determine read write ratio object objects 
system replicas responsible determining example overloaded read write ratio exceeded tolerance values originally set 
shall see decision need arrived simultaneously complex agreement protocol need run replicas 
number read operations number write operations current replica group handle read requests timely manner replica may decide inform simply create new replicas share load move existing replicas faster machine 
write operations outnumber read operations reduce size group move replicas faster machines 
cases obviously necessary ensure new group meets quality service old group initially met 
appropriate protocol group membership change dynamically group change group quiescent 

group change rates agreement protocol executed various members replica group possible different replicas group may come different decisions size group required 
lead situation newly created replicas deleted replica group 
overcome synchronisation problem ensure group membership changes idempotent operations 
achieved requiring replica specify current group size size requires attempting update global replicated group membership list 
measure prevent oscillations size group replicas different views alternating growing shrinking group 
control oscillations introduce form imposing restriction group membership changed change direction time value set object application basis 
typically necessary replicas synchronize requesting group membership change 
replica asks group increase overloaded replicas increased group size eventually come attention members operations running slower ask group size decreased 
selection replicas eliminate group function availability performance tradeoff invoke policy logic described previously 

initial implementation results implementing components rms real distributed environment decided build simulation system experiment various placement policy algorithms 
distributed systems simulation described written sim simulation language 
simulated system consists set nodes set objects replicated placement policy algorithms 
object assigned desired quality service created 
set transactions executed accessing subset objects fixed duration 
transaction arrival times execution durations drawn exponential distributions 
transactions interrupted due failure replicated object restarted 
measure performance average response time completed transactions best performance possible availability number successfully completed transactions 
nodes fail recover mttf mttr reliability values assigned nodes created 
values tolerance change simulation run change allowed tolerance described previously trigger reassignment replicas 
node interdependencies assigned statically dynamically required 
simulated distributed environment machines mttf mttr values gave availability characteristics 
clients objects transactions 
client start transaction invoke operation replica group 
replica failed operation replication protocol determined sufficient replicas remained operational client operation succeed obviously depends type operation 
case client transaction aborted guarantee consistency 
distributed environment simulated constructed rms experimented various placement policy algorithms 
simulated policies random policy number replicas fixed programmer placement selected random replica placed required availability computed policy number placement calculated rms 
order compare differences rms quality service replicated object obtained results replicated group interactions replication protocols available copies weighted voting passive replication 
space limitations shall show results available copies 
available copies replication protocol users read single copy write copies 

non replicated interactions table shows availability performance non replicated object distributed environment 
single object placed randomly machine expected availability 
availability read write operations availability single object 
table availability performance non replicated object 

availability unshared replicated object quality service required replicated object available time required best possible performance availability constraint 
read write ratio set client group 
table shows results placement policies 
random placement scheme assigned replicas 
probability replicas unavailable invocation zero read availability group 
availability replica group write operations client contact replicas group probability unavailable high 
replicas write operations performance perceived client subsequently worse 
table availability performance replicated object 
rms decided replicas needed 
random placement read availability performance 
replicas write operations performance availability improved 

performance shared replicated object determined performance replica groups multiple clients involved decided clients series results 
case replicas controlled familiar multiple readers single writer locking policy possible clients find unable group client locked conflicting mode 
results displayed table 
table performance shared replicated object 

dynamic reconfiguration previous sections rms statically allocated number placement replicas 
described real goal obtain rms dynamically reconfigure system take account changes load experiment read write ratio initially set required availability set 
rms computed replicas placed give optimum performance achieve best possible availability 
actual read write ratio changed execution 
seen table alteration read write ratio accounted performance group suffers write operations replicas group 
group dynamically reconfigured increase number write operations results number location replicas altered benefit performance 
table performance comparison dynamic reconfiguration 

current status implementing rms top distributed system 
rms fully implemented simulated environment expect problems translation 
monitor daemon dependency tracker implemented tested data fed back rms running simulated network giving promising results 
addition considering extending rms indicated examine effects weak consistency replication protocols placement policy 
availability performance replicated object significantly affected placement number replicas choice consistency protocol 
crude calculations mttf mttr machines distributed system improve accuracy placement decisions random programmer determined placement decisions 
sophisticated techniques take account common modes failure nodes failure characteristics components hardware software produce better placement decisions 
described replica management system tool computing appropriate placements dynamically reconfiguring replica group take account changing conditions distributed system changes load availability machines 
dynamic placement decisions ensure characteristics availability performance maintained long periods system operation 
acknowledgments supported part uk mod science engineering research council 
gr esprit basic research project broadcast 

object mobility distributed computer system phd thesis wayne state university 

day principle resilient sharing distributed resources proceedings second international conference software engineering pp 


ansa model groups isa project apm rc june 

bernstein hadzilacos goodman concurrency control recovery database systems addison wesley 

birman implementing fault tolerant distributed objects ieee transactions software engineering vol 
se june pp 


birman joseph reliable communication presence failures acm transactions computer systems vol february pp 


birman joseph schmuck isis distributed programming user guide manual isis project department computer science cornell university ithaca ny march 

cristian automatic reconfiguration presence failures proceedings iee international workshop configurable distributed systems london march pp 


coffman plateau optimization number copies distributed data base ieee transactions software engineering vol se january pp 


eager lazowska zahorjan limited performance benefits migrating active processes load sharing acm pp 


gifford weighted voting replicated data th symposium operating system principles pacific grove december pp 


golding weak consistency group communication membership phd thesis university california santa cruz december 

kopetz tolerating transient faults mars proceedings th international symposium fault tolerant computing pp 


lin raghavendra state aggregation method analyzing dynamic load balancing policies proceedings th international conference distributed computing systems pittsburgh may pp 


liskov distributed programming argus communications cacm vol march pp 


little shrivastava replicated resilient objects proceedings st ieee workshop management replicated data houston november pp 


little object replication distributed system phd thesis department computing science newcastle university september 

little construction simulation package newcastle university department computing science technical report july 

long providing performance guarantees fddi network proceedings th international conference distributed computing systems pittsburgh may pp 


little computing replica placement distributed systems proceedings nd ieee workshop management replicated data monterey november pp 


generation dependability models design specifications distributed real time systems phd thesis fakult technische universit wien vienna austria 

nishikawa steenkiste general architecture load balancing distributed memory environment proceedings th international conference distributed computing systems pittsburgh may pp 


oki replication highly available distributed systems phd thesis mit laboratory computer science august 

ramamritham stankovic zhao distributed scheduling tasks deadlines resource requirements ieee transactions computers vol 
august pp 


ramamritham stankovic efficient scheduling algorithms real time multiprocessor systems ieee transactions parallel distributed systems vol 
april pp 


schneider implementing fault tolerant services state machine approach tutorial acm computing surveys december pp 


shrivastava dixon overview distributed programming system ieee software january pp 


trivedi probability statistics reliability queuing computer science applications prentice hall englewood cliffs nj 

wang intelligent job selection distributed scheduling proceedings th international conference distributed computing systems pittsburgh may pp 


wolfson jajodia algorithm dynamic data distribution proceedings nd ieee workshop management replicated data monterey november pp 


