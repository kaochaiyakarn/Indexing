appears fourteenth national conference onarti cial intelligence providence rhode island aaai press 
empirical evaluation bagging boosting richard maclin computer science department university minnesota mn email umn edu ensemble consists set independently trained classi ers neural networks decision trees predictions combined classifying novel instances 
previous research shown ensemble accurate single ensemble 
bagging breiman boosting freund schapire relatively new popular methods producing ensembles :10.1.1.133.1040:10.1.1.32.9399
evaluate methods neural networks decision trees classi cation algorithms 
results clearly important facts 
evaluate methods neural networks decision trees classi cation algorithms 
results clearly important facts 
rst bagging produces better classi er individual component classi ers relatively tting generalize better baseline neural network ensemble method 
second boosting technique usually produce better ensembles bagging susceptible noise quickly data set 
researchers investigated technique combining predictions multiple classi ers produce single classi er breiman perrone wolpert :10.1.1.32.9399
resulting classi er referred ensemble generally accurate individual classi ers making ensemble 
popular methods creating ensembles bagging breiman boosting arcing freund schapire :10.1.1.133.1040:10.1.1.32.9399
methods rely resampling techniques obtain di erent training sets classi ers 
previous demonstrated methods ective decision trees drucker cortes breiman quinlan little empirical testing neural networks es copyright american association arti cial intelligence www aaai org :10.1.1.32.9399
rst bagging produces better classi er individual component classi ers relatively tting generalize better baseline neural network ensemble method 
second boosting technique usually produce better ensembles bagging susceptible noise quickly data set 
researchers investigated technique combining predictions multiple classi ers produce single classi er breiman perrone wolpert :10.1.1.32.9399
resulting classi er referred ensemble generally accurate individual classi ers making ensemble 
popular methods creating ensembles bagging breiman boosting arcing freund schapire :10.1.1.133.1040:10.1.1.32.9399
methods rely resampling techniques obtain di erent training sets classi ers 
previous demonstrated methods ective decision trees drucker cortes breiman quinlan little empirical testing neural networks es copyright american association arti cial intelligence www aaai org :10.1.1.32.9399
rights reserved 
david opitz department computer science university mt email opitz cs edu new boosting algorithm 
researchers investigated technique combining predictions multiple classi ers produce single classi er breiman perrone wolpert :10.1.1.32.9399
resulting classi er referred ensemble generally accurate individual classi ers making ensemble 
popular methods creating ensembles bagging breiman boosting arcing freund schapire :10.1.1.133.1040:10.1.1.32.9399
methods rely resampling techniques obtain di erent training sets classi ers 
previous demonstrated methods ective decision trees drucker cortes breiman quinlan little empirical testing neural networks es copyright american association arti cial intelligence www aaai org :10.1.1.32.9399
rights reserved 
david opitz department computer science university mt email opitz cs edu new boosting algorithm 
comprehensive evaluation bagging boosting methods creating ensembles neural networks compare results similar tests decision trees 
tested algorithms data sets nd interesting results 
classi er ensembles illustrates basic framework classi er ensemble 
example neural networks basic classi cation method conceptually classi cation method substituted place networks 
network ensemble network network case trained training instances network 
example predicted output networks oi combined produce output ensemble 
researchers breiman krogh lincoln demonstrated ective combining scheme simply average predictions network :10.1.1.32.9399
combining output classi ers useful disagreement 
obviously combining identical classi ers produces gain 
hansen salamon proved average error network ensemble output combine network outputs network network input classi er ensemble neural networks 
rate example component classi ers ensemble independent production errors expected error example reduced zero number classi ers combined goes nity assumptions rarely hold practice 
result methods creating ensembles center producing classi ers disagree predictions 
generally methods focus altering training process hope resulting classi ers produce di erent predictions 
example neural network techniques employed include methods training di erent topologies di erent initial weights di erent parameters training portion training set drucker hansen salamon maclin shavlik 
concentrate popular methods bagging boosting try generate disagreement classi ers altering training set classi er sees 
bagging classi ers bagging breiman bootstrap efron tibshirani ensemble method creates individuals ensemble training classi er random redistribution training set :10.1.1.32.9399
classi er training set generated randomly drawing replacement examples size original training set original examples may repeated resulting training set may left 
individual classi er ensemble generated di erent random sampling training set 
boosting classi ers boosting freund schapire encompasses family methods :10.1.1.133.1040
focus methods produce series classi ers 
concentrate popular methods bagging boosting try generate disagreement classi ers altering training set classi er sees 
bagging classi ers bagging breiman bootstrap efron tibshirani ensemble method creates individuals ensemble training classi er random redistribution training set :10.1.1.32.9399
classi er training set generated randomly drawing replacement examples size original training set original examples may repeated resulting training set may left 
individual classi er ensemble generated di erent random sampling training set 
boosting classi ers boosting freund schapire encompasses family methods :10.1.1.133.1040
focus methods produce series classi ers 
training set member series chosen performance earlier classi er series 
boosting examples incorrectly predicted previous classi ers series chosen examples correctly predicted 
boosting attempts produce new classi ers ensemble better able correctly predict examples current ensemble performance poor 
training set member series chosen performance earlier classi er series 
boosting examples incorrectly predicted previous classi ers series chosen examples correctly predicted 
boosting attempts produce new classi ers ensemble better able correctly predict examples current ensemble performance poor 
note bagging resampling training set dependent performance earlier classi ers 
examine new powerful forms boosting arcing breiman ada boosting freund schapire :10.1.1.133.1040:10.1.1.32.9399
bagging methods choose training set size classi er probabilistically selecting replacement examples original training examples 
bagging probability selecting example equal training set 
probability depends example misclassi ed previous classi ers 
methods initially set probability example methods recalculate probabilities trained classi er added ensemble 
ada boosting combines classi ers ck weighted voting ck weight log 
revision described breiman reset weights equal restart 
arcing updates probabilities somewhat di erently 
ith example training set value mi refers number times example misclassi ed previous classi ers 
probability pi selecting example part classi er training set de ned pi mi mj breiman chose value power empirically trying di erent values breiman :10.1.1.32.9399
ada boosting arcing combines classi ers unweighted voting 
table summary data sets 
shown number examples data set number output classes discrete features describing examples number input output hidden units networks network trained 
features neural network dataset cases classes continuous discrete inputs outputs epochs breast cancer credit credit diabetes glass heart cleveland hepatitis house votes hypo ionosphere iris kr vs kp labor letter promoters bind satellite segmentation sick sonar soybean splice vehicle results evaluate performance bagging boosting performed experiments number data sets drawn uci data set repository murphy aha 
plan compare bagging boosting methods methods introduced 
particular intend examine stacking wolpert method training combining function avoid ect having weight classi ers 
compare bagging boosting methods approach creating ensemble 
approach uses genetic search nd classi ers accurate di er predictions 
presents empirical evaluation bagging breiman boosting freund schapire neural networks decision trees :10.1.1.133.1040:10.1.1.32.9399
results demonstrate bagging ensemble nearly outperforms single classi er 
results show arcing ensemble greatly outperform bagging ensemble single classi er 
data sets arcing shows small zero gain performance single classi er 
similarly show ada boosting ensemble outperform methods including arcing ada boosting produce worse performance single classi er 
