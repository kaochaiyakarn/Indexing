neural network approach topic spotting erik wiener jan pedersen andreas weigend xerox palo alto research center dept computer science cb university colorado boulder boulder wiener cs colorado edu xerox palo alto research center coyote hill rd palo alto ca pedersen parc xerox com institute cognitive science dept computer science campus box university colorado boulder boulder andreas cs colorado edu www cs colorado edu andreas home html presents application nonlinear neural networks topic spotting 
neural networks allow model higherorder interaction document terms simultaneously predict multiple topics shared hidden features 
context model compare approaches dimensionality reduction representation term selection latent semantic indexing lsi 
different methods proposed improving lsi representations topic spotting task 
find term selection modified lsi representations lead similar topic spotting performance performance equal better published results corpus 
topic spotting problem identifying set predefined topics natural language document 
neural networks statistical models operate tens thousands input variables corresponding individual terms dimensionality reduction prime importance 
compare approaches term selection picks subset original terms features latent semantic indexing lsi constructs new features combinations large number original terms 
organized follows 
section describes related section describes corpus section discusses representational approach section discusses neural network models section presents experimental results section concluding discussion 
related classification methods text categorization bayesian belief networks decision trees nearest neighbor algorithms bayesian classifiers boolean decision rules :10.1.1.49.860:10.1.1.39.6139
systems words word couples features lewis hartmann example incorporated phrasal structure document representations 
describe detail text categorization studies reuters corpus subject experiments 
lewis ringuette compared categorization techniques bayesian classifier fairly simple conditional probability model decision trees :10.1.1.49.860
cases separate classifier constructed topic 
section describes related section describes corpus section discusses representational approach section discusses neural network models section presents experimental results section concluding discussion 
related classification methods text categorization bayesian belief networks decision trees nearest neighbor algorithms bayesian classifiers boolean decision rules :10.1.1.49.860:10.1.1.39.6139
systems words word couples features lewis hartmann example incorporated phrasal structure document representations 
describe detail text categorization studies reuters corpus subject experiments 
lewis ringuette compared categorization techniques bayesian classifier fairly simple conditional probability model decision trees :10.1.1.49.860
cases separate classifier constructed topic 
represented documents binary word vectors important drastically reduce number words representation successful classification 
information gain measure pick words high individual predictive power different set words selected predicting topic 
results showed decision tree method give slightly higher performance bayesian classifier authors noted decision tree method resulted classifier rules easier interpret 
cases separate classifier constructed topic 
represented documents binary word vectors important drastically reduce number words representation successful classification 
information gain measure pick words high individual predictive power different set words selected predicting topic 
results showed decision tree method give slightly higher performance bayesian classifier authors noted decision tree method resulted classifier rules easier interpret 
apte damerau weiss rule induction technique called swap produce disjunctive binary decision rules relating presence certain combinations terms presence topics :10.1.1.39.6139
previous approach separate classifiers trained topic local feature set 
authors classification performance improved word frequency values binary presence absence features performance improved selecting local representation frequent words topic informative words 
information gain measure system mutual information presence topic presence word documents corpus 
neural network approach topic spotting corpus experiments reuters corpus reuters newswire stories standard testbed text categorization research 
authors classification performance improved word frequency values binary presence absence features performance improved selecting local representation frequent words topic informative words 
information gain measure system mutual information presence topic presence word documents corpus 
neural network approach topic spotting corpus experiments reuters corpus reuters newswire stories standard testbed text categorization research 
stories full collection stories topic assigned left training testing 
better full collection including stories deemed human predefined topics believe partitioning supported meaningful experiments group :10.1.1.39.6139
stories mean length words standard deviation 
topics appear training set cover areas commodities interest rates foreign exchange 
documents fourteen topics assigned mean topics document 
frequency occurrence varies greatly topic topic earnings example appears roughly documents topic training documents 
erik wiener jan pedersen andreas weigend number terms networks selected term representation performance peaks terms 
performance averaged networks predicting topics varying frequency 
scores indicate useful terms discrimination 
terms yielded average best classification performance measured independent test set see 
number falls range feature set sizes little smaller number :10.1.1.49.860:10.1.1.39.6139
note decrease error training set including terms classification performance sample data quickly falls terms due overfitting 
overfitting occurs network starts memorize training patterns starts fitting peculiarities training data decreasing performance sample data 
overfitting typically problem free parameters weights model compared training samples 
see example 
example best individual predictors contain redundant information 
term may appear poor predictor may turn great discriminative power combination terms 
possible find set terms predicting individual topics term selection scales models predict large number topics single representation 
address problems applied latent semantic indexing reduce dimensionality feature set 
latent semantic indexing lsi originally developed address problems created synonymy polysemy standard vector space model information retrieval seeks transform original document vectors meaningful lower dimensional space analyzing structure terms document collection :10.1.1.108.8490
transformation computed applying singular value decomposition svd original term document matrix 
dimensions new space direct interpretability rotations original space 
orthogonal 
terms related lsi extent travel corpus 
dimensions new space direct interpretability rotations original space 
orthogonal 
terms related lsi extent travel corpus 
neural network approach topic spotting uments different terminology talk concept positioned near new space 
see details lsi technique :10.1.1.108.8490
property lsi vectors higher dimensions capture increasingly variance original data dropped minimal loss 
original experiments lsi ir tasks retaining dimensions gave optimal performance results suggest dimensions better tasks :10.1.1.108.8490
topic spotting experiments performance continues improve dimensions improvement rapidly slows dimensions 
process constructing lsi representations term document matrix straightforward 
terms related lsi extent travel corpus 
neural network approach topic spotting uments different terminology talk concept positioned near new space 
see details lsi technique :10.1.1.108.8490
property lsi vectors higher dimensions capture increasingly variance original data dropped minimal loss 
original experiments lsi ir tasks retaining dimensions gave optimal performance results suggest dimensions better tasks :10.1.1.108.8490
topic spotting experiments performance continues improve dimensions improvement rapidly slows dimensions 
process constructing lsi representations term document matrix straightforward 
svd computed training set new document vectors test set transformed simply projecting left singular matrix produced original decomposition 
cost computing svd incurred corpus 
relatively high error example arise examples perfectly separated probability estimates mid range near 
force binary decisions thresholding probabilities compute precision recall figures contingency tables constructed range decision thresholds 
recall percentage documents topic predicted having erik wiener jan pedersen andreas weigend topic precision percentage documents predicted having topic topic 
pick decision thresholds ways 
proportional assignment pick different set thresholds topic varying integer parameter set values level setting decision threshold just probability value kp th highest ranked document fraction positive examples topic training set :10.1.1.49.860
fixed recall level approach set recall levels want compute precision analyze ranked documents test set determine decision thresholds topic lead desired set recall levels 
satisfying method picking operation threshold suffice characterizing effectiveness classifier full range potential thresholds 
set contingency tables computed range decision thresholds topic summarize performance means adding contingency tables topics certain threshold computing precision recall meaning compute precision recall individually topic take average topics 
proportional assignment picking decision thresholds fixed set recall levels 
cases summary values obtained particular topics averaging precision evenly spaced recall levels 
results experiments reported features lsi representations generic task directed features selected term representations 
shows microaveraged precision recall curves networks generic lsi representation topic directed lsi representation td lsi relevancy weighted lsi representation rel lsi selected term representation 
breakeven points respectively 
corpus roughly compare results best algorithm didn give special weight words headlines stories reported breakeven point :10.1.1.39.6139
results quite microaveraged performance somewhat misleading task frequent topics weighted heavier average 
classifiers generally perform best high frequency topics case optimistic measure 
shows performance classifiers 
selected term network appears closer performance 
prime example circumstance front modular network draw large variety information order determine high level class document 
major challenge finding ways global representations lsi biased particular tasks losing flexibility global representations 
believe task directed approaches represent meaningful step direction 
acknowledgment wray buntine david hull tom landauer eric schutze john tukey helpful discussions 
apte damerau weiss :10.1.1.39.6139
language independent automated learning text categorization models 
proceedings th annual acm sigir conference 
erik wiener jan pedersen andreas weigend deerwester dumais furnas landauer harshman :10.1.1.108.8490
indexing latent semantic analysis 
acknowledgment wray buntine david hull tom landauer eric schutze john tukey helpful discussions 
apte damerau weiss :10.1.1.39.6139
language independent automated learning text categorization models 
proceedings th annual acm sigir conference 
erik wiener jan pedersen andreas weigend deerwester dumais furnas landauer harshman :10.1.1.108.8490
indexing latent semantic analysis 
journal american society information science 
dumais 
improving retrieval information external sources 
lewis 
representation learning information retrieval 
phd thesis computer science dept univ massachussetts amherst february 
technical report 
lewis ringuette :10.1.1.49.860
comparison learning algorithms text categorization 
symposium document analysis information retrieval 
masand waltz 
classifying news stories memory reasoning 
