learning mixture hierarchies andrew lippman mit media laboratory ames st cambridge ma media mit edu www media mit edu hierarchical representation data various applications domains data mining machine vision information retrieval 
introduce extension expectation maximization em algorithm learns mixture hierarchies computationally efficient manner 
efficiency achieved progressing bottom fashion clustering mixture components level hierarchy obtain level 
clustering requires knowledge mixture parameters need resort intermediate samples 
addition practical applications algorithm allows new interpretation em clear relationship non parametric kernel estimation methods provides explicit control trade bias variance em estimates offers new insights behavior deterministic annealing methods commonly em escape local minima likelihood 
practical applications statistical learning useful characterize data hierarchically 
establish likelihood virtual sample model usual em literature assume samples different blocks independent xjm jm ensure constraint equation enforced samples block assigned component assuming knowledge assignment samples drawn independently corresponding mixture component likelihood block jm jz ij jz ij ij binary variable value block assigned th component th data point combining equations obtain incomplete data likelihood sample xjm jz ij equation similar incomplete data likelihood standard em main difference having hidden variable sample point sample block 
likelihood complete data theta jz ij ij vector containing ij log likelihood log ij log jz ij relying em estimate parameters leads step ij ij jx ij jx jz ij jz ik key quantity compute jz ij 
logarithm log jz ij log jz ij em log xjz ij law large numbers em expected value th mixture component drawn 
easy computation densities commonly mixture modeling 
shown gaussian case leads ij sigma gamma sigma gamma sigma sigma gamma sigma gamma sigma sigma expression gaussian mean covariance sigma :10.1.1.50.8
step consists maximizing ij log jz ij subject constraint 
relatively simple task common mixture models show gaussian case leads parameter update equations ij ij ij sigma ij ij sigma ij gamma gamma notice equation equations depend explicitly underlying sample computed directly parameters algorithm efficient computational standpoint number mixture components typically smaller size sample bottom hierarchy 
relationships standard em interesting relationships algorithm derived standard em procedure 
thing notice making sigma steps obtained applying standard em sample composed points standard em seen particular case new algorithm learns level mixture hierarchy 
