constraint rule mining large dense databases roberto bayardo jr ibm almaden research center bayardo mit edu appears proc 
th int conf 
data engineering 
constraint rule find rules dataset meeting user specified constraints minimum support confidence 
describe new algorithm directly exploits user specified constraints including minimum support minimum confidence new constraint ensures mined rule offers predictive advantage simplifications 
algorithm maintains efficiency low supports data dense relational data 
previous approaches apriori variants exploit minimum support constraint result ineffective dense data due combinatorial explosion frequent itemsets 

mining rules data problem attracted considerable interest rule provides concise statement potentially useful information easily understood users 
database literature focus developing association rule algorithms identify conjunctive rules meeting user specified constraints minimum support statement generality minimum confidence statement predictive ability 
algorithms initially developed tackle data sets primarily domain market basket analysis interest applying algorithms domains including telecommunications data analysis census data analysis classification predictive modeling tasks general :10.1.1.41.407:10.1.1.25.3707
data market basket analysis data sets tend dense properties frequently occurring items sex male strong correlations items items record 
data sets cause exponential blow resource consumption standard association rule mining algorithms including apriori variants 
combinatorial explosion result fact algorithms effectively mine rules satisfy minimum support constraint number 
rule constraints market basket data dense particularly incorporates information convenience card applications mining rules personal attributes items purchased 
combinatorial explosion result fact algorithms effectively mine rules satisfy minimum support constraint number 
rule constraints market basket data dense particularly incorporates information convenience card applications mining rules personal attributes items purchased 
rakesh agrawal ibm almaden research center rakesh agrawal ieee org gunopulos ibm almaden research center dg cs ucr edu typically enforced solely post processing filter step 
approach mining dense data sets directly enforce user specified rule constraints mining 
example association rule allow users set minimum predictive ability mined rule specified minimum confidence alternative measure lift known interest conviction :10.1.1.25.3707
algorithm exploit predictive ability mining vastly improved efficiency 
strong support predictive ability rules satisfying constraints dense dataset numerous mined efficiently user 
remedy problem algorithm exploits constraint eliminates rules uninteresting contain conditions strongly contribute predictive ability rule 
illustrate useful concept consider rule bread butter milk confidence rule confidence says people purchase bread butter purchase item consequent rule milk 
section concludes summary contributions 

related previous mining rules data extensive 
review numerous proposals greedy heuristic rule mining decision tree induction focus algorithms provide completeness guarantees 
refer reader interested heuristic approaches mining large data sets scalable algorithms proposed :10.1.1.104.152
numerous papers presenting improvements manner apriori algorithm enumerates frequent itemsets address problem combinatorial explosion number frequent itemsets resulting applying techniques dense data :10.1.1.25.3707
works show identify maximal frequent itemsets data sets frequent itemsets long numerous :10.1.1.107.1120
unfortunately association rules efficiently extracted maximal frequent itemsets require performing intractable task enumerating computing support subsets 
srikant ng investigated incorporating item constraints set frequent itemsets faster association rule mining :10.1.1.12.8836:10.1.1.41.407:10.1.1.25.3707

related previous mining rules data extensive 
review numerous proposals greedy heuristic rule mining decision tree induction focus algorithms provide completeness guarantees 
refer reader interested heuristic approaches mining large data sets scalable algorithms proposed :10.1.1.104.152
numerous papers presenting improvements manner apriori algorithm enumerates frequent itemsets address problem combinatorial explosion number frequent itemsets resulting applying techniques dense data :10.1.1.25.3707
works show identify maximal frequent itemsets data sets frequent itemsets long numerous :10.1.1.107.1120
unfortunately association rules efficiently extracted maximal frequent itemsets require performing intractable task enumerating computing support subsets 
srikant ng investigated incorporating item constraints set frequent itemsets faster association rule mining :10.1.1.12.8836:10.1.1.41.407:10.1.1.25.3707
constraints restrict items combinations items allowed participate mined rules orthogonal exploited approach 
related previous mining rules data extensive 
review numerous proposals greedy heuristic rule mining decision tree induction focus algorithms provide completeness guarantees 
refer reader interested heuristic approaches mining large data sets scalable algorithms proposed :10.1.1.104.152
numerous papers presenting improvements manner apriori algorithm enumerates frequent itemsets address problem combinatorial explosion number frequent itemsets resulting applying techniques dense data :10.1.1.25.3707
works show identify maximal frequent itemsets data sets frequent itemsets long numerous :10.1.1.107.1120
unfortunately association rules efficiently extracted maximal frequent itemsets require performing intractable task enumerating computing support subsets 
srikant ng investigated incorporating item constraints set frequent itemsets faster association rule mining :10.1.1.12.8836:10.1.1.41.407:10.1.1.25.3707
constraints restrict items combinations items allowed participate mined rules orthogonal exploited approach 
believe classes constraints part rule mining tool application 
refer reader interested heuristic approaches mining large data sets scalable algorithms proposed :10.1.1.104.152
numerous papers presenting improvements manner apriori algorithm enumerates frequent itemsets address problem combinatorial explosion number frequent itemsets resulting applying techniques dense data :10.1.1.25.3707
works show identify maximal frequent itemsets data sets frequent itemsets long numerous :10.1.1.107.1120
unfortunately association rules efficiently extracted maximal frequent itemsets require performing intractable task enumerating computing support subsets 
srikant ng investigated incorporating item constraints set frequent itemsets faster association rule mining :10.1.1.12.8836:10.1.1.41.407:10.1.1.25.3707
constraints restrict items combinations items allowed participate mined rules orthogonal exploited approach 
believe classes constraints part rule mining tool application 
ranking association rules interest measures gives indication measures exploited mining dense data sets feasible :10.1.1.42.342:10.1.1.25.3707
goodman describe constraint rule miner exploits information theoretic constraint heavily penalizes long rules order control model search complexity 
unfortunately association rules efficiently extracted maximal frequent itemsets require performing intractable task enumerating computing support subsets 
srikant ng investigated incorporating item constraints set frequent itemsets faster association rule mining :10.1.1.12.8836:10.1.1.41.407:10.1.1.25.3707
constraints restrict items combinations items allowed participate mined rules orthogonal exploited approach 
believe classes constraints part rule mining tool application 
ranking association rules interest measures gives indication measures exploited mining dense data sets feasible :10.1.1.42.342:10.1.1.25.3707
goodman describe constraint rule miner exploits information theoretic constraint heavily penalizes long rules order control model search complexity 
incorporate constraints effects easily understood user allow efficient mining long rules satisfy constraints 
proposals constraint rule mining machine learning data mining focus address issue efficiently dealing large data sets 
webb provides survey class algorithms presents opus framework extends set enumeration search framework additional generic pruning methods 
support itemset denoted sup number transactions data set contain association rule just rule short consists itemset called antecedent itemset disjoint antecedent called consequent 
rule denoted antecedent consequent 
support association rule support itemset formed union antecedent consequent 
confidence association rule probability items antecedent appear items consequent dataset 
specifically conf sup sup measures predictive ability include lift known interest conviction :10.1.1.25.3707
conviction lift rule expressed function rule confidence frequency consequent functions monotone confidence sup lift conf sup sup sup conviction sup conf frame remainder terms confidence easily recast terms measure monotonicity property 
association rule mining problem produce association rules data set meet specified support confidence 
restrict problem ways order render solvable dense data 
consequent constraint require mined rules consequent specified user 
conviction lift rule expressed function rule confidence frequency consequent functions monotone confidence sup lift conf sup sup sup conviction sup conf frame remainder terms confidence easily recast terms measure monotonicity property 
association rule mining problem produce association rules data set meet specified support confidence 
restrict problem ways order render solvable dense data 
consequent constraint require mined rules consequent specified user 
restriction item constraint exploited proposals reduce set frequent itemsets considered :10.1.1.12.8836:10.1.1.41.407
frequent itemset set items support exceeds minimum support threshold 
frequent itemsets numerous dense data item constraint 
algorithm leverages consequent constraint pruning functions enforcing confidence support improvement defined constraints mining phase 
minimum improvement constraint algorithm runs efficiently dense datasets restriction result easily thousands rules indication ones 
completely expanded set enumeration tree items ordered lexically 

dense miner top level 
input parameters minconf minsup assumed global 
dense miner set transactions returns frequent confident large improvement rules set rules set groups generate initial groups non empty scan process groups prune groups section generate level extract rules prune groups section return post process section terminology draw machinery developed previous framed problem mining maximal frequent itemsets databases set enumeration tree search problem :10.1.1.107.1120
node tree represented itemsets called group 
itemset called head simply itemset rule enumerated node 
second itemset called tail ordered set consists items potentially appended head form viable rule enumerated sub node 
example root tree head itemset empty tail itemset consists items head tail group denoted tg respectively 
groups representing entire level tree pro 
procedure expanding level set enumeration tree 
generate level set groups returns set groups representing level set enumeration tree set groups gc group reorder items tg section item gc new group hg hg tg jj follows ordering gc gc gc return gc pass data set 
systematic traversal set enumeration tree dense miner uses breadth traversal limit number database passes length longest frequent itemset 
hash trees implementation details efficient group processing described :10.1.1.107.1120
generate initial groups simply produce root node consists empty head tail 
implementation seeds search second level tree optimized phase rapidly computes support item rules antecedents array data structures hash trees 
generate level generates groups comprise level set enumeration tree 
note tail items group reordered children expanded 
framed theorem manner exploited exact support information available 
useful wish prune group processed previously gathered support information 
example unprocessed group compute value compute lower bound value 
parent node superset lower bound observation 
observation group parent set enumeration tree conveniently support information required apply fact immediately available candidate set observation apply support lower bounding theorem obtain support information provided candidate set observation group parent set enumeration tree tg hg tg sup hg tg gp gp tg hg tg gp sup hg tg sup hg tg gp sup hg tg gp gp sup hg sup hg sup hg tg attempting prune unprocessed group dense miner computes lower bounds uses greater theorem :10.1.1.107.1120
bounding improvement propose complementary methods bound improvement frequent rule derivable group technique uses primarily value described second directly establishes upper bound improvement definition 
dense miner computes retaining smaller bounds provided techniques 
bounding improvement confidence bound theorem shows obtain upper bound improvement reusing value value greater confidence sub rule hg greatest confidence 
theorem value max hg conf upper bound improvement rule derivable proof rs denote sub rule hg greatest confidence 
dense miner sets computed function parent denotes single item itemset 
computation requires value previously computed parent supports candidate set members order compute applying theorem prune unprocessed group dense miner computes 
lacks necessary support information compute computes value described section 
bounding support value comparatively easy compute support anti monotone respect rule containment 
dense miner simply uses value anti monotone constraints discussed exploited similar ease :10.1.1.12.8836:10.1.1.25.3707

item ordering motivation reordering tail items generate level function effect force rules portion search tree 
reason strategy critical order group prunable sub node group represent rule fails satisfy constraints 
arbitrary ordering policy result roughly distribution rules sup hg minsup min max minsup sup hg imp sup hg tg sup hg im im im hg sup hg im im minf gp sup hg gp hg hg gp hg hg hg hg sup hg sup hg tg sup hg satisfy constraints search tree yielding little pruning opportunities 
useful proper sub rule responsible rule improvement value 
arbitrary set rules post processor determines exact improvement rule associates rule proper greatest confidence original rule set 
rule sets guaranteed high improvement rules extracted decision tree sub rules potentially simplify improve generality improve predictive ability 

evaluation section provides evaluation dense miner real world data sets particularly dense :10.1.1.107.1120
hg conf hg hg conf conf hg data set compiled census data obtained washington edu census comp html consists transactions items transaction transaction representing answers census questionnaire 
answers include age status status income sex status location residence 
similar data sets targeted marketing identifying population respond particular promotion 
continuous attributes discretized described frequently occurring items discarded :10.1.1.25.3707
evaluation section provides evaluation dense miner real world data sets particularly dense :10.1.1.107.1120
hg conf hg hg conf conf hg data set compiled census data obtained washington edu census comp html consists transactions items transaction transaction representing answers census questionnaire 
answers include age status status income sex status location residence 
similar data sets targeted marketing identifying population respond particular promotion 
continuous attributes discretized described frequently occurring items discarded :10.1.1.25.3707
second data set connect data set irvine machine learning database repository www ics uci edu mlearn mlrepository html 
consists transactions items transaction 
data set interesting size density minority consequent item tie games accurately predicted rules low support 
experiments partner item consequent data set tie games item connect data set consequents consistently yields qualitatively similar results 
ali srikant 
partial classification association rules 
proc 
rd int conference knowledge discovery databases data mining 
bayardo :10.1.1.107.1120
efficiently mining long patterns databases 
proc 
acm sigmod int conf 
management data 
proc 
acm sigmod int conf 
management data 
berry michael 
data mining techniques marketing sales customer support john wiley sons brin motwani ullman tsur :10.1.1.25.3707
dynamic itemset counting implication rules market basket data 
proc 
acm sigmod int conf 
management data 
finding interesting rules large sets discovered association rules 
proc 
third int conf 
information knowledge management 
ng lakshmanan han pang :10.1.1.12.8836:10.1.1.25.3707
exploratory mining pruning optimizations constrained association rules 
proc acm sig mod int conf 
management data 

search systematic set enumeration 
proc 
third int conf 
principles knowledge representation reasoning 
shafer agrawal mehta :10.1.1.104.152
sprint scalable parallel classifier data mining 
proc 
nd conf 
large data bases 
large data bases 
goodman 
information theoretic approach rule induction databases 
ieee transactions knowledge data engineering 
srikant vu agrawal :10.1.1.41.407
mining association rules item constraints 
proc 
third int conf 
knowledge discovery databases data mining 
