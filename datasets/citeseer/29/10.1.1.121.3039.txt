journal arti cial intelligence research submitted published connectionist theory re nement genetically searching space network topologies david opitz opitz cs edu department computer science university montana mt usa jude shavlik shavlik cs wisc edu computer sciences department university wisconsin dayton st madison wi usa algorithm learns set examples ideally able exploit available resources abundant computing power domain speci knowledge improve ability generalize 
connectionist theory re nement systems background knowledge select neural network topology initial weights proven ective exploiting domain speci knowledge exploit available computing power 
weakness occurs lack ability re ne topology neural networks produce limiting generalization especially impoverished domain theories 
algorithm uses domain speci knowledge help create initial population knowledge neural networks genetic operators crossover mutation speci cally designed knowledge networks continually search better network topologies 
experiments real world domains indicate new algorithm able signi cantly increase generalization compared standard connectionist theory re nement system previous algorithm growing knowledge networks 


data prior knowledge available cpu cycles system learns set labeled examples called inductive learner alternately supervised empirical similarity learner 
output example provided teacher set labeled examples learner called training set 
task inductive learning generate training set concept description correctly predicts output examples just training set 
inductive learning algorithms previously studied michalski quinlan rumelhart :10.1.1.167.3624
algorithms di er concept representation language method bias constructing concept language 
di erences important determine concepts classi er induce 
alternative inductive learning paradigm build concept description set examples querying experts eld directly assembling set rules describe concept build expert system waterman 
problem building expert systems theories derived experts tend approximately correct 
reasons develop anytime learning algorithms continually improve quality answer time 
dean boddy de ned criteria anytime algorithm algorithm suspended resumed minimal overhead algorithm stopped time return answer algorithm return answers improve output opitz shavlik input classical regression example smooth function solid curve noisy data points probably better predictor high degree polynomial dashed curve 
time 
criteria created planning scheduling algorithms apply inductive learning algorithms 
standard inductive learners rumelhart id quinlan unable continually improve answers receive additional training examples :10.1.1.167.3624:10.1.1.167.3624
fact run long algorithms tend training set holder 
tting occurs learning algorithm produces concept captures information training examples general characteristics domain 
concepts great job classifying training instances poor job generalizing new examples ultimate measure success 
help illustrate point consider typical regression case shown 
regent rapture able dynamically re ne topology network 
algorithm add new nodes network 
aside designed probabilistic rules rapture di ers regent adds nodes intention completely learning training set generalizing 
rapture training set learned regent continually searches topology space looking network minimizes scoring function error 
rapture initially creates links speci ed domain theory explicitly adds links id quinlan information gain metric :10.1.1.167.3624
regent hand fully connect consecutive layers networks allowing rule possibility adding antecedents training 
algorithm towell shavlik extension kbann uses domain theory help train kbann network 
kbann ective dropping antecedents adding tries nd potentially useful inputs features mentioned domain theory 
backing errors lowest level domain theory computing correlations features 
systems ginsberg ourston mooney ptr feldman segre donoho rendell able handle types re nements 
discuss system representative propositional systems 
connectionist theory refinement theory revision operators removing antecedents rule adding antecedents rule removing rules rule base inventing new rules 
uses operators revisions domain theory correctly classify previously misclassi ed training examples unde ning correctly classi ed examples 
uses inductive learning algorithms invent new rules currently uses id quinlan induction component :10.1.1.167.3624
regent mutation operator add nodes manner analogous symbolic system adds antecedents rules underlying learning algorithm connectionist 
towell showed kbann outperformed promoter task regent outperformed kbann article 
kbann power domain largely attributed ability ne grain re nements domain theory towell 
di culty domain ba es mooney extension called mofn able learn rules rules true antecedents true 
