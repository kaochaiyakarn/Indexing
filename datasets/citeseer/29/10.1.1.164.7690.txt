letter communicated joachim buhmann learning overcomplete representations michael lewicki computer science dept center neural basis cognition carnegie mellon univ mellon inst fifth ave pittsburgh pa terrence sejnowski howard hughes medical institute computational neurobiology laboratory salk institute la jolla ca overcomplete basis number basis vectors greater dimensionality input representation input unique combination basis vectors 
overcomplete representations advocated greater robustness presence noise sparser greater flexibility matching structure data 
overcomplete codes proposed model response properties neurons primary visual cortex 
previous focused finding best representation signal fixed overcomplete basis dictionary 
algorithm learning overcomplete basis viewing probabilistic model observed data 
show overcomplete bases yield better approximation underlying statistical distribution data lead greater coding efficiency 
viewed generalization technique independent component analysis provides method bayesian reconstruction signals presence noise blind source separation sources mixtures 
common way represent real valued signals linear superposition basis functions 
efficient way encode highdimensional data space representation distributed 
bases fourier wavelet provide useful representation signals limited specialized signals consideration 
alternative potentially general method signal representation uses called overcomplete bases called overcomplete dictionaries allow greater number basis functions called dictionary elements samples input signal simoncelli freeman adelson heeger mallat zhang chen donoho saunders 
overcomplete bases typically constructed merging set complete neural computation massachusetts institute technology michael lewicki terrence sejnowski bases fourier wavelet gabor adding basis functions complete basis adding frequencies fourier basis 
overcomplete basis decomposition signal unique offer advantages 
greater flexibility capturing structure data 
small set general basis functions larger set specialized basis functions relatively required represent particular signal 
form compact representations basis function describe significant amount structure data 
example signal largely sinusoidal compact representation fourier basis 
similarly signal composed chirps naturally represented chirp basis 
combining bases single overcomplete basis allow compact representations types signals coifman wickerhauser mallat zhang chen 
possible obtain compact representations overcomplete basis contains single class basis functions 
overcomplete fourier basis minimum number sinusoids compactly represent signals composed small numbers frequencies achieving superresolution chen 
additional advantage overcomplete representations increased stability representation response small perturbations signal simoncelli 
case complete basis signal decomposition defined unique finding best representation terms overcomplete basis challenging problem 
requires objective decomposition algorithm achieve objective 
decomposition expressed finding solution signal matrix basis functions vectors vector coefficients representation signal 
developing efficient algorithms solve equation active area research 
approach removing degeneracy equation place constraint daubechies chen example finding satisfying equation minimum norm 
different approach construct iteratively sparse representation signal coifman wickerhauser mallat zhang 
approaches decomposition nonlinear function data 
overcomplete bases flexible terms signal represented guarantee hand selected basis vectors matched structure data 
ideally basis adapted data signal class interest basis function captures maximal amount structure 
success lines developed olshausen field viewpoint learning sparse codes 
adapted learning overcomplete representations natural images basis functions shared properties neurons primary visual cortex suggesting overcomplete representations useful model neural population codes 
article algorithm learning overcomplete basis viewing probabilistic model observed data 
approach provides natural solution decomposition see equation finding maximum posteriori representation data 
prior distribution basis function coefficients removes redundancy representation leads representations sparse nonlinear function data 
probabilistic approach decomposition leads natural method denoising 
model derive simple robust learning algorithm maximizing data likelihood basis functions 
generalizes algorithm olshausen field deriving algorithm learning overcomplete bases direct approximation data likelihood 
allows learning arbitrary input noise levels allows objective comparison different models provides way estimate model coding efficiency 
show overcomplete representations provide better efficient representation better approximate underlying statistical density input data 
generalizes technique independent component analysis jutten comon bell sejnowski provides method identification sources mixtures 
model assume data vector xl described overcomplete linear basis plus additive noise matrix assume gaussian additive noise 
data likelihood log noise variance 
criticism overcomplete representations redundant data point possible representations redundancy removed proper choice prior probability basis coefficients specifies probability alternative representations 
density determines underlying statistical structure modeled nature representation 
standard approaches signal representation specify prior coefficients michael lewicki terrence sejnowski complete bases assuming zero noise representation signal unique 
invertible decomposition signal expensive compute strong incentive find basis matrices easy invert restricting basis functions orthogonal restricting basis functions fast algorithms fourier wavelet analysis 
general approach afforded probabilistic formulation 
parameter values internal states maximizing posterior distribution argmax argmax probable decomposition signal 
formulation problem offers advantage model fit general types distributions 
simplicity assume independence coefficients sm 
advantage formulation process finding probable representation determines best representation noise level defined automatically performs denoising encodes underlying signal noise 
noise level set arbitrary levels including zero 
possible bayesian methods infer probable noise level pursued 
relation prior representation 
shows different priors induce different representations data 
standard choice gaussian prior equivalent factor analysis yields advantage having overcomplete representation underlying assumption data gaussian 
alternative choice advocated authors field olshausen field chen priors assume sparse representations 
accomplished priors high kurtosis laplacian sm exp sm 
compared gaussian distribution puts greater weight values close zero result representations sparser greater number small valued coefficients 
overcomplete bases expect priori representation sparse nonzero coefficients needed represent arbitrary dimensional input patterns 
case zero noise gaussian maximizing equation equivalent mins subject 
solution obtained pseudoinverse linear function case zero noise laplacian maximizing equation equivalent mins subject 
gaussian prior solution obtained simple linear operation 
learning overcomplete representations different priors induce different representations 
data distribution dimensional main arms 
overlayed axes form overcomplete representation 
optimal scaled basis vectors data point gaussian laplacian prior respectively 
assuming low noise gaussian equivalent finding minimum norm 
solution pseudoinverse linear function data 
laplacian prior sm exp sm finds minimum norm 
nonlinear operation essentially selects subset basis vectors represent data chen resulting representation sparse 
sample segment natural speech fit overcomplete fourier representation basis functions see section 
plot shows rank order distribution coefficients gaussian prior dashed laplacian prior solid 
gaussian prior nearly coefficients solution nonzero far fewer nonzero laplacian prior 
inferring internal state general approach optimizing case finite noise nongaussian gradient log posterior optimization algorithm daugman olshausen field 
suitable initial condition michael lewicki terrence sejnowski alternative method prior laplacian view problem linear program chen min subject 
letting objective function linear program sm corresponds maximizing log posterior likelihood laplacian prior 
converted standard linear program positive coefficients separating positive negative coefficients 
making substitutions equation min subject replaces basis vector matrix contains positive negative copies vectors 
separates positive negative coefficients solution positive variables respectively 
solved efficiently exactly interior point linear programming methods chen 
quadratic programming approaches type problem suggested osuna freund girosi similar problems 
linear programming gradient methods 
linear programming methods superior finding exact solutions case zero noise 
standard implementation handles noiseless case generalized chen 
methods faster obtaining approximate solutions 
advantage easily adapted general models positive noise levels different priors 
learning derive learning algorithm specify appropriate objective function 
natural objective maximize probability data model 
set independent data vectors xk xk 
probability single data point computed marginalizing states network xk ds xk 
formulates problem density estimation equivalent minimizing kullback leibler divergence model density learning overcomplete representations distribution data 
implementation related issues synaptic noise ignored equivalent methods redundancy reduction maximizing mutual information input representation nadal parga cardoso advocated researchers barlow hinton sejnowski daugman linsker atick :10.1.1.31.5414
fitting bases data distribution 
understand overcomplete representations yield better approximations underlying density helpful contrast different techniques adapting basis particular data set 
principal component analysis pca models data multivariate gaussian 
representation commonly find directions data largest variation principal components data gaussian structure 
basis functions eigenvectors covariance matrix restricted orthogonal 
extension pca called independent component analysis ica jutten comon bell sejnowski allows learning nonorthogonal bases data nongaussian distributions 
ica highly effective applications blind source separation mixed audio signals jutten bell sejnowski decomposition electroencephalographic eeg signals makeig jung bell sejnowski analysis functional magnetic resonance imaging fmri data mckeown 
techniques number basis vectors equal number inputs 
bases span input space complete sufficient represent data see representation limited 
illustrates different data densities modeled various approaches simple dimensional data space 
pca assumes gaussian structure data nongaussian structure vectors point directions contain data probability density defined model predict data occur 
means model underestimate likelihood data dense regions overestimate sparse regions 
shannon theorem limit efficiency representation 
ica assumes coefficients nongaussian structure allows vectors nonorthogonal 
example basis vectors point high density regions data space 
density complicated case armed density pca ica captures underlying structure 
possible represent point dimensional space linear combination vectors underlying density described specifying complicated prior basis function coefficients 
overcomplete basis allows efficient representation underlying density simple priors basis function coefficients 
michael lewicki terrence sejnowski fitting dimensional data different bases 
pca assumption data gaussian distribution 
optimal basis vectors orthogonal efficient representing nonorthogonal density distributions 
ica require vectors orthogonal fit general types densities 
data distributions modeled adequately pca ica 
allowing basis overcomplete allows armed distribution fit simple independent distributions basis vector coefficients 
approximating data probability 
deriving learning algorithm problem general integral equation intractable 
special case zero noise complete representation invertible integral solved leads known ica algorithm mackay pearlmutter parra olshausen field cardoso :10.1.1.134.6077
case overcomplete basis solution possible 
approaches tried approximate integral evaluating maximum olshausen field ignores volume information posterior 
means representations determined sharp posterior distribution treated way ill determined introduce biases learning 
problem leads trivial solution magnitude basis functions diverge 
problem somewhat circumvented adaptive normalization olshausen field setting adaptation rates tricky practice important learning overcomplete representations guarantee desired objective function optimized 
approach take approximate equation gaussian posterior mode yields log log log log log det hessian log posterior log 
saddle point approximation 
see appendix derivation details 
care taken log prior curvature det arising fact rank assumed rectangular 
accuracy approximation determined closely posterior mode approximated gaussian 
approximation poor multiple modes excessive skew kurtosis 
methods addressing issues section 
learning rule obtained differentiating log respect see appendix leads expression aa log zk log sk sk 
rule contains matrix inverses vector involves derivative log prior 
case square form rule exactly natural gradient ica learning rule basis matrix amari cichocki yang 
difference general case rectangular coefficients calculated 
standard ica learning algorithm square zero noise coefficients wx filter matrix 
learning rule optimization maximize posterior distribution see equation 
examples demonstrate algorithm simple data sets 
shows results fitting different dimensional data sets 
learning procedure follows 
initial bases random normalized vectors constraint vectors closer angle degrees 
basis adapted data learning rule equation 
bases adapted iterations simple gradient descent batch size step size iterations reduced 
probable coefficients obtained publicly available interior point michael lewicki terrence sejnowski examples illustrating fitting distributions overcomplete bases 
gray vectors show true basis vectors generate distribution 
black vectors show learned basis vectors 
clarity vectors rescaled 
learned basis vectors true vectors positive negative coefficients allowed 
linear programming package 
convergence learning algorithm rapid usually reaching solution fewer iterations 
solution discarded magnitude basis vector dropped zero 
happened occasionally vector trapped representing data region 
example shown 
data generated true basis vectors shown gray 
illustrate better arms distribution elements drawn exponential distribution positive coefficients unit mean 
direction learned vectors matched true generating vectors 
magnitude smaller true basis vectors possibly due approximation see equation 
identical results obtained coefficients generated laplacian prior positive negative coefficients 
second example see arms generated true basis vectors laplacian positive negative coefficients unit variance 
learned vectors close true vectors showed consistent bias 
occur data dense distinct direction approximation 
bias greatly reduced coefficients drawn distribution greater sparsity 
example uses data set underlying vectors generated generalized laplacian distribution sm exp sm 
distribution fitted wavelet subband coefficients images simoncelli range 
varying exponent varies kurtosis distribution measure sparseness 
learning overcomplete representations data generated 
arms data set distinct directions match generating vectors 
quantifying efficiency representation advantage probabilistic framework provides natural means comparing objectively alternative representations models 
section compare methods comparing coding efficiency learned basis functions 
estimating coding cost objective function probability data model natural measure comparing different models 
helpful convert value intuitive form 
shannon coding theorem probability code word gives lower bound length code word assumption model correct 
number bits required encode pattern bits log log dimensionality input pattern specifies quantization level encoding 
accurate approximation 
learning algorithm derived making gaussian approximation posterior maximum practice sufficiently accurate generate useful gradient learning overcomplete bases 
caution necessarily relation local curvature volume general priors gaussian prior 
comparing alternative models basis functions may want accurate approximation 
approach works method second differences 
rely analytic hessian estimate volume volume estimated directly calculating change posterior maximum second differences 
computing entire hessian manner require evaluations posterior operation 
reduce number posterior evaluations estimate curvature direction eigenvectors hessian 
obtain accurate volume estimate volume gaussian approximation match closely possible volume posterior maximum 
requires choice step size eigenvector direction 
principle fit optimized example minimizing cross entropy significant computational expense 
relatively quick reliable estimate obtained choosing step length produces michael lewicki terrence sejnowski fixed drop value posterior log likelihood argmin log log iei step length direction eigenvector ei 
method choosing requires line search typically done function evaluations 
examples optimal value approximating laplacian gaussian 
procedure computes new hessian accurate estimate posterior curvature 
estimated hessian substituted equation obtain accurate estimate log 
shows cross sections true posterior gaussian approximation direct hessian second differences method 
direct hessian method produced poor approximations directions curvature sharp estimating volume second differences produced better estimate eigenvector directions 
guarantee best estimate exist directions poorly approximated 
example plots figures labeled smp show direction convergence tolerance convergence tolerance direction gradient optimization terminated direction coefficients values poorly determined 
maximum reached volume direction underestimated 
second differences method produces accurate estimate log analytic hessian obtains accurate estimates relative coding efficiencies different models 
application test data 
quantify efficiency different representations various test data sets methods described 
estimated coding cost calculated sets randomly sampled data points encoding precision 
relative encoding efficiency different representations depends relative difference predictions closely matches underlying distribution 
little difference predictive distribution little difference expected coding cost 
table shows estimated coding costs various models data sets 
uniform distribution gives upper bound coding cost assuming equal probability points range spanned data points 
simple bivariate gaussian model data points equivalent code principal components data significantly better 
remaining table entries assume model defined equation laplacian prior basis coefficients 
basis matrix learning overcomplete representations smp smp solid lines show cross sections dimensional posterior distribution directions eigenvectors hessian matrix evaluated see equation 
posterior resulted fitting overcomplete representation sample segment natural speech see section 
cross section direction tolerance probable tolerance labeled smp 
remaining cross sections order eigenvalues showing sample smallest largest 
dots show gaussian approximation cross section 
indicates position note axes different scales 
gaussian approximation obtained analytic hessian estimate curvature 
cross sections gaussian approximation calculated method described text 
equivalent ica solution assumption laplacian prior 
matrices overcomplete representations 
bases learned methods discussed 
probable coefficients calculated linear programming method done learning 
table shows coding efficiency estimates obtained approximation reasonably accurate compared estimated michael lewicki terrence sejnowski table estimated coding costs 
bits pattern model uniform gaussian ica bits pattern uniform gaussian models expected coding costs exact 
apart uniform model differences predicted densities subtle show large differences coding costs 
expected densities predicted various models similar 
data sets vector basis gives greater expected coding efficiencies indicates higher degrees yield efficient coding 
vector basis improvement coding efficiency 
reflects fact prior distribution limited laplacian inherent limitation model degrees freedom increasing number basis functions increase space distributions modeled 
estimating coding cost coefficient entropy 
probability data gives lower bound code length specify code 
alternative method estimating coding efficiency estimate entropy proposed code 
models natural choice code vector coefficients approach distribution coefficients fit training data set estimated function 
coding cost test data set computed estimating entropy fitted coefficients quantization level needed maintain encoding noise level advantage give better approximation observed distribution better laplacian assumed model 
examples assume coefficients distribution straightforward separate function distribution estimate coefficient 
note caution technique include cost 
basis fully span data space result poor fits data yield low entropy 
bases consideration fully span data space entropy yield reasonable estimate coding cost 
compute entropy precision coefficient encoded needs specified 
ideally quantized maintain learning overcomplete representations encoding noise level examples approximation si ai 
relates change si change data xi exact orthogonal 
mean value si obtain single quantization level coefficients 
function applying kernel density estimation silverman distribution coefficients fit training data set 
laplacian kernel window width straightforward method estimating coding cost bits pattern sum individual entropies bits ni log quantized index ranges bins quantization ni number counts observed bin coefficients fit test data set consisting patterns 
distinction test training data set necessary probabilities code words need specified priori 
data set test training underestimates cost large data sets difference minimal 
alternative method computing coding cost entropy fact sparse overcomplete representations subset coefficients zero 
coding cost computed summing entropy nonzero coefficients plus cost identifying bits ni log min log quantized density estimate nonzero coefficients ith bin index ranges bins 
method appropriate cost identifying nonzero coefficients small compared cost sending zero valued coefficients 
application test data 
quantify efficiency various representations data sets shown computing entropy coefficients 
estimated coding cost calculated randomly sampled data points encoding precision 
separate training data set consisting data points estimate distribution coefficients 
entropy estimated computing entropy nonzero coefficients see equation yielded lower estimated coding costs overcomplete bases compared summing individual entropies see equation 
table shows estimated coding costs learned bases data sets table 
second column lists cost labeling michael lewicki terrence sejnowski table estimated entropy nonzero coefficients 
bits pattern labeling model cost nonzero coefficients 
columns list coding cost values nonzero coefficients 
total coding cost sum numbers 
matrices ica solution entropy estimate yields approximately coding cost estimate indicates complete case computation accurate 
entropy estimate yields higher coding overcomplete bases 
coding cost computed suggests lower average coding costs achievable 
strategy lowering average coding cost avoid identifying nonzero coefficients pattern example grouping patterns share common nonzero coefficients 
illustrates advantages contrasting minimal coding cost estimated probability particular code 
learning sparse representations speech speech data obtained timit database speech single speaker speaking different example sentences 
speech segments samples duration msecs sampling frequency hz 
preprocessing done 
complete overcomplete basis vectors overcomplete basis basis vectors learned 
bases initialized standard fourier basis overcomplete fourier basis composed twice number sine cosine basis functions linearly spaced frequency nyquist range 
larger problems desirable learning faster 
simple modifications basic gradient descent procedure produced rapid reliable convergence 
gradient see equation step size computed amax amax element basis matrix largest absolute value 
parameter reduced iterations fixed remaining iterations measure data range defined standard deviation data 
learning rule replaced term equation approximation aa ah see equation suggested lewicki learning overcomplete representations example fitting overcomplete representation segments natural speech 
speech segments consisted samples duration msecs sampling frequency hz 
random sample basis vectors scaled full range 
power spectral densities hz corresponding basis olshausen ah bq diag bq log obtained singular value decomposition yielded solutions similar equation resulted fewer zero length basis vectors 
patterns estimate learning step 
learning terminated steps 
probable basis function coefficients obtained modified conjugate gradient routine press teukolsky vetterling flannery 
basic routine modified replace line search approximate newton step 
approach resulted substantial speed improvement produced better solutions fixed amount time standard routine 
improve speed convergence criterion conjugate gradient routine started value reduced course learning 
shows sample learned basis vectors overcomplete basis 
waveforms show random sample learned basis vectors corresponding power spectral densities hz shown 
immediate observation fourier basis vectors learned basis vectors broad bandwidth multiple spectral peaks 
indication basis functions contain broadband harmonic structure inherent speech signal 
way contrasting learned representation fourier representation shown 
shows log coefficient magnitudes duration speech sentence taken timit database overcomplete fourier middle plot overcomplete learned michael lewicki terrence sejnowski dark suit wash water year coefficient number fourier basis log learned basis coefficient number log comparison coefficient values duration speech sample timit database 
top amplitude envelope sentence speech 
vertical lines rough indication word boundaries listed database 
middle plots log coefficient magnitudes overcomplete fourier basis duration speech example nonoverlapping blocks samples windowing 
bottom shows log coefficient magnitudes overcomplete learned basis 
largest coefficients terms magnitude plotted 
basis vectors ordered terms power spectra higher frequencies top 
representations bottom plot 
middle plot similar standard spectrogram windowing performed windows overlapping 
shows coefficient values learned basis uniformly fourier basis 
consistent learning objective optimizes basis vectors coefficient values independent possible 
consequence correlations frequency exist fourier representation reflecting harmonic structure speech apparent learned representation 
comparing efficiencies representations 
table shows estimated number bits sample encode speech segment precision bit amplitude range 
table shows learning overcomplete representations table estimated bits sample speech data 
estimation method basis log ba log total entropy nonzero entropy learned fourier learned fourier estimated coding costs probability data see equation entropy computed summing individual entropy estimates coefficients see equation total 
shown entropy nonzero coefficients 
estimates complete overcomplete learned basis perform significantly better corresponding fourier basis 
surprising observed redundancy fourier representation see 
bases coding cost estimates derived entropy coefficients lower derived 
possible reason coefficients sparser laplacian prior assumed model 
supported looking histogram coefficients see shows density kurtosis higher distribution basis coefficients learned fourier distribution basis function coefficients learned fourier bases 
note log scale axis 
michael lewicki terrence sejnowski sumed laplacian 
entropy method obtain lower estimate fits observed density column table shows entropy coefficient overcomplete bases complete bases basis yields improvement coding efficiency 
reasons 
reason due independence assumption model sm 
assumes structure learned basis vectors 
apparent considerably structure left values basis vector coefficients 
possible generalizations capturing kind structure discussed 
discussion algorithm learning overcomplete bases 
cases overcomplete representations allow basis approximate better underlying statistical density data lead representations better capture underlying structure data greater coding efficiency 
probabilistic formulation basis inference problem offers advantages assumptions prior distribution basis coefficients explicit 
estimating probability data model entropy basis function coefficients different models compared objectively 
algorithm generalizes ica model accounts additive noise allows basis overcomplete 
standard ica internal states computed inverting basis function matrix model transformation data internal representation nonlinear 
occurs model generalized account additive noise basis allowed overcomplete internal representation ambiguous 
internal representation obtained maximizing posterior probability assumptions model general nonlinear operation 
advantage formulation possible denoise data 
inference procedure finding probable coefficients automatically separates data underlying signal additive noise 
setting noise appropriate levels model specifies structure data ignored structure modeled basis functions 
derivation allows noise level set zero 
case model attempts account variability data 
approach denoising applied successfully natural images lewicki olshausen 
potential application blind separation sources mixtures 
example dimensional examples viewed source separation problem number sources mixed smaller number channels learning overcomplete representations figures 
overcomplete bases allow model capture underlying statistical structure data space 
true source separation limited sources mapped smaller subspace necessarily loss information 
possible separate speakers channels fidelity lee lewicki girolami sejnowski 
shown case natural speech learned bases better coding properties commonly representations fourier basis 
examples learned basis resulted estimated coding efficiency near lossless compression bits sample better fourier basis 
reflects fact spectral representations speech redundant spectral structures speaker harmonics 
complete case model equivalent ica additive noise 
emphasize numbers reflect simple coding scheme sample speech segments 
coding schemes achieve greater compression 
analysis serves compare relative efficiency code fourier representation learned representation 
learned overcomplete representations showed greater coding efficiency overcomplete fourier representation show greater coding efficiency complete representation 
possible reason approximations inaccurate 
approximation estimate probability data see equation works learning basis functions gives reasonable estimates exact quantities available 
possible approximation increasingly inaccurate higher degrees 
open challenge inference problems obtain accurate computationally tractable approximations data probability 
plausible reason lack coding efficiency overcomplete representations assumptions model inaccurate 
obvious prior distribution coefficients 
example observed coefficient distribution speech data see sparser laplacian assumed model 
generalizing models better captures kind structure improve accuracy model allow fit broader range distributions 
generalizing techniques handle general prior distributions straightforward difficulties optimization techniques finding probable coefficients approximation data probability log 
directions currently investigating 
need question fundamental assumption coefficients statistically independent 
true structures speech high degree nonstationary statistical structure 
type structure clearly visible spectrogram plots 
michael lewicki terrence sejnowski cause coefficients assumed independent possible capture mutually exclusive relationships exist different speech sounds 
example different bases specialized different types phonemes 
capturing type structure requires generalizing model represent learn higher order structure 
applications overcomplete representations common assume coefficients independent laplacian distributions mallat zhang chen 
results show overcomplete representations potentially yield benefits practice limited inaccurate prior assumptions 
improved model data distribution lead greater coding efficiency allow accurate inference 
bayesian framework article provides direct method evaluating validity assumptions suggests ways models generalized 
approach taken overcomplete representation may useful understanding nature neural codes cerebral cortex 
ganglion cells optic nerve represented cells primary visual cortex monkey 
possible stage cortical processing visual world large factor 
appendix approximating 
integral ds approximated gaussian integral ds log indicates determinant indicates hessian evaluated solution see equation 
log log sp sp 
learning overcomplete representations hessian laplacian 
case laplacian prior sm log log sm log sm 
log piece wise linear curvature zero discontinuity derivative zero 
approximate volume contribution weuse log tanh sm sm equivalent approximation sm cosh sm 
large approximates true laplacian prior staying smooth zero 
leads diagonal expression hessian log sm sech sm 
derivation learning rule 
set independent data vectors xk expand posterior probability density approximation log log xk kl log km log log sk xk sk log det sk learning rule obtained differentiating log respect letting clarity letting see main derivatives need considered log log det 
consider turn 
deriving log 
term learning rule specifies change representation probable 
assume prior distribution high kurtosis component change weights representation sparser 
michael lewicki terrence sejnowski chain rule log log sm sm sm assuming sm 
obtain sm need describe function basis complete assume low noise simply invert obtain 
overcomplete simple expression approximation 
certain priors probable solution yield nonzero elements 
effect procedure computing selects complete basis best accounts data hypothetical reduced basis derive learning algorithm full basis matrix represent reduced basis particular composed basis vectors nonzero coefficients 
precisely cl indices nonzero coefficients fewer nonzero coefficients zero valued coefficients included loss generality 
ci equal zero valued elements removed 
obtained removing columns corresponding zero valued elements allows results obtained case invertible 
note construction reduced basis mathematical device derivation 
final gradient equation depend construction 
mackay sk ij ij kl xl identity kl aij ki jl sk ij ki sj 
letting zk log sk sk log ij ki jl xl 
zk ki sj 
changing back matrix notation log 
learning overcomplete representations derivative expressed terms original variables 
invert mapping obtain reduced coefficients zm remaining values defined zero 
define matrix ci remaining rows equal zero 
log zs 
gives gradient zero columns zero valued coefficients 
deriving second term specifies change minimize data misfit 
letting ek results notation aij ek ek sl akl aij 
gradient component arising error term expected residual error structure 
deriving log det third term learning rule specifies change weights minimize width posterior distribution increase probability data 
chain rule log det aij det mn element defined hmn det hmn hmn aij 
cmn log mn 
identity det hmn det nm obtain log det aij cmn nm aij 
aij mn michael lewicki terrence sejnowski considering term square brackets cmn aij ain aim aij 
mn cmn mn aij fact mj notation mn cmn mn ah mj aim mj aim jm aim jj aij jm due symmetry hessian 
matrix derive bmm aij 
assuming sm log diagonal mn nm aij bmm mm sm sm 
aij letting ym mm bmm sm result reduced representation see equation mm bmm mm 
putting results cmm bmm back equation log det ah 
gathering components log yields expression learning rule log ah 
learning overcomplete representations stabilizing simplifying learning rule 
gradient equation directly problematic due matrix inverses impractical unstable 
alleviated multiplying gradient appropriate positive definite matrix amari 
components gradient preserves direction valid optimization 
fact allows eliminate equation learning rule aa log az aa ah ay 
additional simplifications 
large low noise hessian dominated aa ah 
possible obtain accurate approximations term lewicki olshausen 
vector hides computation involving inverse hessian 
basis vectors randomly distributed dimensionality increases basis vectors approximately orthogonal consequently hessian approximately diagonal 
case ym hmm bmm sm bmm sm 
km bmm log derivatives smooth ym vanishes large 
practice excluding learning rule yields stable learning 
conjecture term ignored possibly represents curvature components unrelated volume 
obtain expression learning rule aa log az 
acknowledgments bruno olshausen tony bell helpful discussions 
michael lewicki terrence sejnowski amari cichocki yang 

new learning algorithm blind signal separation 
mozer hasselmo eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
atick 

information theory provide ecological theory sensory processing 
network computation neural systems 
barlow 

possible principles underlying transformation sensory messages 
rosenbluth ed communication pp 

cambridge ma mit press 
barlow 

unsupervised learning 
neural computation 
bell sejnowski 

information maximization approach blind separation blind deconvolution 
neural computation 
simoncelli 

image compression joint statistical characterization wavelet domain tech 
rep 
philadelphia university pennsylvania 
cardoso 

infomax maximum likelihood blind source separation 
ieee signal processing letters 
chen donoho saunders 

atomic decomposition basis pursuit technical rep 
stanford ca department statistics stanford university 
coifman wickerhauser 

entropy algorithms best basis selection 
ieee transactions information theory 
comon 

independent component analysis new concept 
signal processing 
daubechies 

wavelet transform time frequency localization signal analysis 
ieee transactions information theory 
daugman 

complete discrete gabor transforms neural networks image analysis compression 
ieee transactions acoustics speech signal processing 
daugman 

entropy reduction decorrelation visual coding oriented neural receptive fields 
ieee trans 
bio 
eng 
field 

goal sensory coding 
neural computation 
hinton sejnowski 

learning relearning boltzmann machines 
rumelhart mcclelland eds parallel distributed processing vol 
pp 

cambridge ma mit press 
jutten 

blind separation sources 

adaptive algorithm neuromimetic architecture 
signal processing 
lee lewicki girolami sejnowski 

blind source separation sources mixtures overcomplete representations 
ieee sig 
proc 
lett 
lewicki olshausen 

inferring sparse overcomplete image codes efficient coding framework 
kearns jordan solla learning overcomplete representations eds advances neural information processing systems 
san mateo ca morgan kaufmann 
lewicki olshausen 

probabilistic framework adaptation comparison image codes 
opt 
soc 
am 
optics image science vision 
linsker 

self organization perceptual network 
computer 
mackay 

maximum likelihood covariant algorithms independent component analysis 
unpublished manuscript 
cambridge university cambridge cavendish laboratory 
available online ftp wol ra phy cam ac uk pub mackay ica ps gz 
makeig jung bell sejnowski 

blind separation event related brain response components 
psychophysiology 
mallat zhang 

matching pursuits time frequency dictionaries 
ieee transactions signal processing 
mckeown jung makeig brown kindermann lee sejnowski 

spatially independent activity patterns functional magnetic resonance imaging data stroop color naming task 
proc 
natl 
acad 
sci 
usa 


interior point linear programming solver 
code available online ftp ftp netlib org opt tar gz 
nadal 
parga 

nonlinear neurons low noise limit factorial code maximizes information transfer 
network 
nadal parga 

redundancy reduction independent component analysis conditions cumulants adaptive approaches 
network 
olshausen field 

emergence simple cell receptive field properties learning sparse code natural images 
nature 
olshausen field 

sparse coding overcomplete basis set strategy employed 
vision res 
osuna freund girosi 

improved training algorithm support vector machines 
proc 
ieee pp 

pearlmutter parra 

maximum likelihood blind source separation context sensitive generalization ica 
mozer jordan petsche eds advances neural information processing systems 
san mateo ca morgan kaufmann 
press teukolsky vetterling flannery 

numerical recipes art scientific programming nd ed 
cambridge cambridge university press 
silverman 

density estimation statistics data analysis 
new york chapman hall 
simoncelli freeman adelson heeger 

shiftable multiscale transforms 
ieee trans 
info 
theory 
received february accepted february 
