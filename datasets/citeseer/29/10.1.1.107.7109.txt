power word clusters text classification school computer science engineering interdisciplinary center neural computation hebrew university jerusalem israel naftali tishby school computer science engineering interdisciplinary center neural computation hebrew university jerusalem israel introduced information bottleneck method provides information theoretic framework extracting features variable relevant values variable 
previous works suggested applying method document clustering gene expression data analysis spectral analysis 
novel implementation method supervised text classification 
specifically apply information bottleneck method find word clusters preserve information document categories clusters features classification 
previous similar clustering procedure show word clusters significantly reduce feature space dimensionality minor change classification accuracy 
similar results go show training sample small word clusters yield significant improvement classification accuracy performance words directly 
known count matrices tend highly sparse noisy especially training data relatively small 
result documents usually represented high dimensional sparse feature space far optimal classification algorithms 
standard procedure reduce feature dimensionality feature selection 
approach selects subset words pre defined criterion uses selected words features classification see 
latent semantic indexing probabilistic lsi methods dimensionality reduction information retrieval tasks :10.1.1.1.4458
alternative approach reduce feature dimensionality grouping similar words smaller number word clusters clusters features 
crucial stage procedures determine similarity words 
introduced information bottleneck ib method show give question formal optimal solution purely information theoretical considerations 
ib method simple idea 
context natural measure similarity words similarity joint distributions topic variable 
specifically set words set topics document categories word category define number occurrences word category calculate simply sum occurrences word training documents belong category denoting document set get number occurrences word document roughly speaking words similar distributions categories belong cluster 
mentioned intuition simple 
different words similar distributions classes play similar role classification process clustered 
formulation finding cluster hierarchy members set words similarity conditional distributions members set categories introduced called distributional clustering :10.1.1.14.1729:10.1.1.14.1729
issue selecting right distance measure distributions remains unresolved earlier 
tishby pereira proposed principled approach problem avoids arbitrary choice distortion distance measures 
new approach empirical joint distribution variables random looks compact representation preserves information possible variable relevant simple intuitive idea natural information theoretic formulation find clusters members set denoted mutual information maximized constraint information extracted rd european colloquium information retrieval research power word clusters text classification mutual information random variables symmetric functional consistent statistical measure information variable contains variable vice versa 
measured compactness representation determined quality clusters fraction information capture surprisingly general problem exact optimal formal solution assumption origin distribution joint 
denote specific hard cluster define distributions easily evaluate mutual information set word clusters eq 
general framework applied agglomerative greedy hierarchical clustering algorithm 
algorithm starts trivial partitioning singleton clusters cluster contains exactly element step merge components current partition single new component way locally minimizes loss formally defined mutual information categories merger principle optimal clustering solution soft means word assigned cluster normalized probability 
account natural language properties words disambiguation natural apply approach context algorithms exist directly solve optimal equations eqs 
:10.1.1.14.1729
preliminary tests showed algorithms usually tend produce similar results sake simplicity focus simple hard clustering algorithm rd european colloquium information retrieval research equations power word clusters text classification decrease information mutual due merger defined information values merger respectively 
little algebra see functional jensen shannon divergence see defined case divergence non negative equals zero arguments identical 
upper bounded interpreted multiplication distance symmetric metric 
note merger cost weight merged elements introducing information optimization criterion resulting similarity measure directly emerges analysis 
advances neural information processing nips pages 
freund seung shamir tishby 
information prediction query committee 
advances neural information processing systems nips 
hofmann :10.1.1.1.4458
probabilistic latent semantic indexing 
acm sigir pages 
joachims probabilistic analysis rocchio algorithm tfidf text categorization 
int 
machine learning 
rd european colloquium information retrieval research power word clusters text classification lin 
divergence measures shannon entropy 
ieee transactions information theory 
pereira tishby lee :10.1.1.14.1729
distributional clustering english words 
th annual meeting association computational linguistics columbus ohio pages 
schapire singer 
boostexter boosting system text categorization 
