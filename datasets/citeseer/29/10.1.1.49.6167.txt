appear proceedings th ieee los alamitos ca ieee computer society press 
integrating boosting stochastic attribute selection committees improving performance decision tree learning zheng geoffrey webb kai ming ting school computing mathematics university victoria australia webb edu au techniques constructing classifier committees including boosting bagging demonstrated great success especially boosting decision tree learning 
type technique generates classifiers form committee repeated application single base learning algorithm 
committee members vote decide final classification 
boosting bagging create different classifiers modifying distribution training set 
sasc stochastic attribute selection committees uses alternative approach generating classifier committees stochastic manipulation set attributes considered node tree induction keeping distribution training set unchanged 
synergy sasc effectively increases model diversity boosting 
experiments representative collection natural domains show average combined technique outperforms boosting sasc terms reducing error rate decision tree learning 

order increase prediction accuracy classifiers classifier committee learning techniques developed great success committees referred ensembles 
especially boosting :10.1.1.133.1040:10.1.1.33.353
type technique generates classifiers form committee single base learning algorithm 
classification stage committee members vote final decision 
training set described set attributes conventional classifier learning algorithms decision tree learning algorithms build classifier 
usually classifier correct parts instance space incorrect small parts instance space 
classification stage committee members vote final decision 
training set described set attributes conventional classifier learning algorithms decision tree learning algorithms build classifier 
usually classifier correct parts instance space incorrect small parts instance space 
classifiers committee partition instance space differently points instance space correctly covered majority committee committee lower error rate individual classifiers 
bagging boosting representative methods type significantly decrease error rate decision tree learning boosting generally better bagging :10.1.1.133.1040:10.1.1.33.353:10.1.1.31.2869:10.1.1.32.9399:10.1.1.37.5438:10.1.1.32.8918
repeatedly build different classifiers base learning algorithm decision tree generator changing distribution training set 
bagging generates different classifiers different bootstrap samples 
boosting builds different classifiers sequentially 
weights training examples creating classifier modified performance previous classifiers 
bagging generates different classifiers different bootstrap samples 
boosting builds different classifiers sequentially 
weights training examples creating classifier modified performance previous classifiers 
objective generation classifier concentrate training examples misclassified previous classifiers 
main difference bagging boosting adaptively changes distribution training set performance previously created classifiers uses function performance classifier weight voting uses breiman refers boosting arcing adaptively resample combine method :10.1.1.30.9410
equal weight voting 
contrast bagging boosting sasc stochastic attribute selection committees adopts alternative approach generating different classifiers form committee 
builds different classifiers modifying set attributes considered node tree induction distribution training set kept unchanged 
selection attribute set carried stochastically 
new approach called sascb stochastic attribute selection committees boosting combination boosting sasc techniques 
sasc boosting improve accuracy decision tree learning different mechanisms expect combining take advantage 
show synergy performs average better sasc boosting 
analysis suggests improvement achieved increasing model diversity sasc addition boosting 
classifier committee learning approaches generating multiple trees manually changing learning parameters output codes generating different classifiers randomizing base learning process similar sasc :10.1.1.72.7289:10.1.1.38.2702
reviews related methods provided dietterich ali 
collection research area 
section briefly describe boosting sasc techniques decision tree learning 
section presents sascb method combining boosting sasc 
summarize 

boosting sasc sascb technique combination boosting sasc briefly discuss boosting sasc section describing approach combining 
classification process boosting sasc section 
details boosting see :10.1.1.31.2869
details sasc see 

boosting boosting general framework improving base learning algorithms decision tree learning rule learning neural networks 
key idea boosting section 

boosting boosting general framework improving base learning algorithms decision tree learning rule learning neural networks 
key idea boosting section 
describe implementation boosting algorithm decision tree learning called boost 
follows boosted algorithm adaboost uses new boosting equation shown equation derived :10.1.1.31.2869
training set consisting instances integer number trials boost builds pruned trees trials repeatedly invoking 
denote weight instance trial trial instance weight trial decision tree built distribution error ffl calculated summing weights instances misclassifies divided ffl greater equal re initialized bootstrap sampling boosting process continues 
note tree ffl discarded tree ffl accepted committee 
weight instance trial computed equation 

experimental domains methods natural domains uci machine learning repository 
include domains quinlan studying boosting 
test suite covers wide variety different domains respect dataset size number classes number attributes types attribute 
domain stratified fold carried algorithm :10.1.1.133.9187
result reported algorithm domain average value trials 
algorithms run training test set partitions default option settings 
boost sasc sascb probabilistic predictions voting weights voting decide final classification 
schapire show test accuracy boosting increases increases training error reaches zero :10.1.1.31.2869
domain stratified fold carried algorithm :10.1.1.133.9187
result reported algorithm domain average value trials 
algorithms run training test set partitions default option settings 
boost sasc sascb probabilistic predictions voting weights voting decide final classification 
schapire show test accuracy boosting increases increases training error reaches zero :10.1.1.31.2869
interesting see performance improvement achieved orders magnitude increase computation 
number trials parameter set experiments boost sasc sascb 
probability attribute selected subset parameter set default sasc sascb 

computer science univ california irvine 
bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
appear machine learning available reality sgi com ronnyk vote ps gz 
breiman :10.1.1.30.9410
arcing classifiers 
technical report department statistics university california berkeley ca available www stat 
berkeley edu users breiman 
breiman :10.1.1.32.9399
breiman :10.1.1.30.9410
arcing classifiers 
technical report department statistics university california berkeley ca available www stat 
berkeley edu users breiman 
breiman :10.1.1.32.9399
bagging predictors 
machine learning 
breiman friedman olshen stone 
classification regression trees 
edu papers html portland oregon 
dietterich 
machine learning research 
ai magazine 
dietterich bakiri :10.1.1.72.7289
solving multiclass learning problems error correcting output codes 
journal artificial intelligence research 
dietterich kong 
machine learning bias statistical bias statistical variance decision tree algorithms 
menlo park ca aaai press 
schapire 
strength weak learnability 
machine learning 
schapire freund bartlett lee :10.1.1.31.2869
boosting margin new explanation effectiveness voting methods 
proceedings fourteenth international conference machine learning pages 
san francisco ca morgan kaufmann 
zheng webb 
