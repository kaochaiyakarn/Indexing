discriminatively trained multiscale deformable part model pedro felzenszwalb university chicago cs uchicago edu david mcallester toyota technological institute chicago mcallester org uc irvine ics uci edu describes discriminatively trained multiscale deformable part model object detection 
system achieves fold improvement average precision best performance pascal person detection challenge 
outperforms best results challenge categories 
system relies heavily deformable parts 
deformable part models quite popular value demonstrated difficult benchmarks pascal challenge 
system relies heavily new methods discriminative training 
combine margin sensitive approach data mining hard negative examples formalism call latent svm 
latent svm hidden crf leads non convex training problem 
latent svm semi convex training problem convex latent information specified positive examples 
believe training methods eventually possible effective latent information hierarchical grammar models models involving latent dimensional pose 

consider problem detecting localizing objects generic category people cars static images 
developed new multiscale deformable part model solving problem 
models trained discriminative procedure requires bounding box labels positive examples 
models implemented detection system highly efficient accurate processing image seconds achieving recognition rates significantly better previous systems 
system achieves fold improvement average precision winning system pascal person detection challenge 
system outperforms best results challenge material supported national science foundation 

example detection obtained person model 
model defined coarse template higher resolution part templates spatial model location part 
object categories 
shows example detection obtained person model 
notion objects modeled parts deformable configuration provides elegant framework representing object categories 
models appealing conceptual point view difficult establish value practice 
difficult datasets deformable models outperformed conceptually weaker models rigid templates bag features 
main goals address performance gap 
models include coarse global template covering entire object higher resolution part templates 
templates represent histogram gradient features 
train models discriminatively 
system semi supervised trained framework rely feature detection 
describe simple effective strategy learning parts weakly labeled data 
contrast computationally demanding approaches learn model hours single cpu 
contribution new methodology discriminative training 
generalize svms handling latent variables part positions introduce new method data mining hard negative examples training 
believe handling partially labeled data significant issue machine learning computer vision 
example pascal dataset specifies bounding box positive example object 
treat position object part latent variable 
treat exact location object latent variable requiring classifier select window large overlap labeled bounding box 
latent svm hidden crf leads nonconvex training problem 
hidden crf latent svm semi convex training problem convex latent information specified positive training examples 
leads general coordinate descent algorithm latent svms 
system overview system uses scanning window approach 
model object consists global root filter part models 
part model specifies spatial model part filter 
spatial model defines set allowed placements part relative detection window deformation cost placement 
score detection window score root filter window plus sum parts maximum placements part part filter score resulting subwindow minus deformation cost 
similar classical part models :10.1.1.12.6365
root part filters scored computing dot product set weights histogram gradient hog features window 
root filter equivalent dalal triggs model 
features part filters computed twice spatial resolution root filter 
model defined fixed scale detect objects searching image pyramid 
training set images annotated bounding boxes instance object 
reduce detection problem binary classification problem 
example scored function form 
vector model parameters latent values part placements 
learn model define generalization svms call latent variable svm 
important property training problem convex fix latent values positive examples 
coordinate descent algorithm 
practice iteratively apply classical svm training triples xn zn yn zi selected best scoring latent label xi model learned previous iteration 
initial root filter generated bounding boxes pascal dataset 
parts initialized root filter 

model underlying building blocks models histogram oriented gradient hog features 
represent hog features different scales 
coarse features captured rigid template covering entire image pyramid hog feature pyramid 
hog feature pyramid object hypothesis defined terms placement root filter near top pyramid part filters near bottom pyramid 
detection window 
finer scale features captured part templates moved respect detection window 
spatial model part locations equivalent star graph fan coarse template serves position 

hog representation follow construction define dense representation image particular resolution 
image divided non overlapping pixel regions cells 
cell accumulate histogram gradient orientations pixels cell 
histograms capture local shape properties somewhat invariant small deformations 
gradient pixel discretized orientation bins pixel votes orientation gradient strength depends gradient magnitude 
color images compute gradient color channel pick channel highest gradient magnitude pixel 
histogram cell normalized respect gradient energy neighborhood 
look blocks cells contain particular cell normalize histogram cell respect total energy blocks 
leads vector length representing local gradient information inside cell 
define hog feature pyramid computing hog features level standard image pyramid see 
features top pyramid capture coarse gradients fairly large areas input image features bottom pyramid capture finer gradients small areas 

filters filters rectangular templates specifying weights subwindows hog pyramid 
filter vector weights 
score filter defined dot product weight vector features subwindow hog pyramid 
system uses single filter define object model 
system detects objects particular class scoring subwindow hog pyramid thresholding scores 
hog pyramid cell th level pyramid 
denote vector obtained concatenating hog features subwindow top left corner score detection window 
denote dimensions clear context 

deformable parts consider models defined coarse root filter covers entire object higher resolution part filters covering smaller parts object 
illustrates placement model hog pyramid 
root filter location defines detection window pixels inside cells covered filter 
part filters placed levels pyramid hog cells level half size cells root filter level 
higher resolution features defining part filters essential obtaining high recognition performance 
approach part filters represent finer resolution edges localized greater accuracy compared edges represented root filter 
example consider building model face 
root filter capture coarse resolution edges face boundary part filters capture details eyes nose mouth 
model object parts formally defined root filter set part models 
pn pi fi vi si ai bi 
fi filter th part vi dimensional vector specifying center box possible positions part relative root position si gives size box ai bi twodimensional vectors specifying coefficients quadratic function measuring score possible placement th part 
illustrates person model 
placement model hog pyramid 
pn pi xi yi li location root filter location th part 
assume level part hog cell level half size hog cell root level 
score placement scores filter data term plus score placement part relative root spatial term fi pi ai xi yi bi xi yi xi yi vi si gives location th part relative root location 
xi yi 
large exponential number placements model hog pyramid 
dynamic programming distance transforms techniques compute best location parts model function root location 
takes nk time number parts model number cells hog pyramid 
detect objects image score root locations best possible placement parts threshold score 
score placement expressed terms dot product vector model parameters vector 
fn 
bn 

pn 
xn yn 
representation learning model parameters connection deformable models linear classifiers 
interesting aspect spatial models defined allow coefficients ai bi negative 
general quadratic spring cost previous 

learning pascal training data consists large set images bounding boxes instance object 
reduce problem learning deformable part model data binary classification problem 

xn yn set labeled examples yi xi specifies hog pyramid xi range xi valid placements root part filters 
construct positive example bounding box training set 
examples define xi root filter placed overlap bounding box 
negative examples come images contain target object 
placement root filter image yields negative training example 
note positive examples treat part locations exact location root filter latent variables 
allowing uncertainty root location training significantly improves performance system see section 

latent svms latent svm defined follows 
assume example scored function form max vector model parameters set latent values 
deformable models define score placing model analogy classical svms train labeled examples 
xn yn optimizing objective function argmin max yif xi 
restricting latent domains xi single choice linear obtain linear svms special case latent svms 
latent svms instances general class energy models 

semi convexity note defined maximum functions linear 
convex 
implies hinge loss max yif xi convex yi 
loss function convex negative examples 
call property loss function semi convexity 
consider latent domains xi positive examples restricted single choice 
loss due positive example convex 
combined semi convexity property convex 
labels positive examples fixed compute local optimum coordinate descent algorithm 
holding fixed optimize latent values positive examples zi argmax xi 

holding zi fixed positive examples optimize solving convex problem defined 
shown steps improve maintain value objective function 
steps maintain value strong local optimum sense step searches exponentially large space latent labels positive examples step simultaneously searches weight vectors exponentially large space latent labels negative examples 

data mining hard negatives object detection vast majority training examples negative 
infeasible consider negative examples time 
common construct training data consisting positive instances hard negative instances hard negatives data mined large set possible negative examples 
describe general method data mining examples svms latent svms 
method iteratively solves subproblems hard instances 
innovation approach theoretical guarantee leads exact solution training problem defined complete training set 
results require margin sensitive definition hard examples 
results described apply classical svms problem defined step coordinate descent algorithm latent svms 
omit proofs theorems due lack space 
results related working set methods 
define hard instances relative yf 
training examples incorrectly classified near margin classifier defined 
show depends hard instances 
theorem 
subset examples 
implies principle train model small set examples 
set defined terms optimal model 
fixed approximate 
suggests iterative algorithm repeatedly compute model hard instances defined model iteration 
justified fixed point theorem 
theorem 

initial cache examples 
practice take positive examples random negative examples 
consider iterative algorithm 


shrink letting 

grow adding examples memory limit theorem 
iteration step algorithm converge finite time 

implementation details ideas discussed approximately implemented current system 
practice training latent svm iteratively apply classical svm training triples xn zn yn zi selected best scoring latent label xi model trained previous iteration 
triples leads example xi zi yi training linear classifier 
allows highly optimized svm package svmlight 
single cpu entire training process takes hours object class pascal datasets including initialization parts 
root filter initialization category automatically select dimensions root filter looking statistics bounding boxes training data 
train initial root filter svm latent variables 
positive examples constructed unoccluded training examples labeled pascal data 
examples scaled size aspect ratio filter 
random subwindows negative images generate negative examples 
root filter update initial root filter trained bounding box training set find best scoring placement filter significantly overlaps bounding box 
original un scaled images 
retrain new positive set original random negative set iterating twice 
part initialization employ simple heuristic initialize parts root filter trained 
select area equals area root filter 
greedily select rectangular region area root filter positive energy 
zero weights region repeat parts selected 
part filters initialized root filter values subwindow selected part filled handle higher spatial resolution part 
initial deformation costs measure squared norm displacement ai bi 
model update update model construct new training data triples 
positive bounding box training data apply existing detector positions scales overlap bounding box 
select highest scoring placement positive example corresponding training bounding box 
negative examples selected finding high scoring detections images containing target object 
add negative examples cache encounter file size limits 
new model trained running svmlight positive negative examples labeled part placements 
update model times cache scheme described 
iteration keep hard instances previous cache add new hard instances possible memory limit 
final iterations able include hard instances cache 
picked simple heuristic cross validating object classes 
set model aspect common mode aspect data 
set model size largest size larger data 

image left shows optimization latent variables positive example 
dotted box bounding box label provided pascal training set 
large solid box shows placement detection window smaller solid boxes show placements parts 
image right shows hard negative example 

results evaluated system pascal voc comp challenge datasets protocol 
refer details emphasize challenges widely acknowledged difficult testbeds object detection 
dataset contains images realworld scenes 
datasets specify ground truth bounding boxes object classes detection considered correct overlaps groundtruth bounding box 
scores system average precision ap precision recall curve testset 
pedestrian detection tended report detection rates versus false positives window measured cropped positive examples negative images objects interest 
scores tied resolution scanning window search ignore effects non maximum suppression making difficult compare different systems 
believe pascal scoring method gives reliable measure performance 
challenge object categories 
entered preliminary version system official competition obtained best score categories 
current system obtains highest score categories second highest score categories 
table summarizes results 
system performs rigid objects cars highly deformable objects persons horses 
note system successful large small amount training data 
roughly positive training examples person category sofa category 
shows models learned 
shows example detections 
evaluated different components system longer established person dataset 
top ap score aero bike bird boat bottle bus car cat chair cow table dog horse person plant sheep sofa train tv rank score darmstadt inria normal inria plus irisa mpi center mpi oxford table 
pascal voc results 
average precision scores system systems entered competition 
empty boxes indicate method tested corresponding class 
best score class shown bold 
current system ranks classes 
preliminary version system ranked classes official competition 
sofa bicycle bottle car 
models learned pascal voc dataset 
show total energy orientation hog cells root part filters part filters placed center allowable displacements 
show spatial model part bright values represent cheap placements dark values represent expensive placements 
pascal competition obtained rigid template model hog features 
best previous result adds segmentation verification step 
summarizes performance models trained 
root model equivalent model scores slightly higher 
performance jumps model trained selects latent position scale positive example 
suggests useful rigid templates allow self adjustment detection window training examples 
adding deformable parts increases performance ap factor best previous score 
trained model parts root filter obtained ap 
illustrates advantage multiscale representation 
investigated effect spatial model allowable deformations person dataset 
recall si allowable displacement part measured hog cells 
trained rigid model high resolution parts setting si 
model outperforms system 
increase amount allowable displacements deformation cost start approach bag features 
performance peaks si suggesting useful constrain part displacements 
optimal strategy allows larger displacements explicit deformation cost 
follow 
results pascal dataset 
row shows detections model specific class person bottle car sofa bicycle horse 
columns show correct detections column shows false positives 
system able detect objects wide range scales cars poses horses 
system detect partially occluded objects person bush 
note false detections quite reasonable example detecting bus car model bicycle sign bicycle model dog horse model 
general part filters represent meaningful object parts localized detection head person model 
precision pascal person root root latent parts latent root parts latent recall 
evaluation system pascal voc person dataset 
root uses root filter latent placement detection windows positive examples 
root latent uses root filter latent placement detection windows 
parts latent part system latent detection windows root filter 
root parts latent includes root part filters latent placement detection windows 
ing table shows ap function freely allowable deformation columns 
column gives performance quadratic deformation cost allowable displacement hog cells 
si quadratic cost ap 
discussion introduced general framework training svms latent structure 
build recognition system multiscale deformable models 
experimental results difficult benchmark data suggests system current state art object detection 
allow exploration additional latent structure recognition 
consider deeper part hierarchies parts parts mixture models frontal vs side cars dimensional pose 
train detect multiple classes shared vocabulary parts visual words 
plan search efficiently search latent parameters detection 
amit 
pop patchwork parts models object recognition 
ijcv november 
burl weber perona 
probabilistic approach object recognition local photometry global geometry 
eccv pages ii 
crandall felzenszwalb huttenlocher 
spatial priors part recognition statistical models 
cvpr pages 
crandall huttenlocher 
weakly supervised learning part spatial models visual object recognition 
eccv pages 
dalal triggs 
histograms oriented gradients human detection 
cvpr pages 
ullman 
semantic hierarchies recognizing objects parts 
cvpr 
van gool williams zisserman 
pascal visual object classes challenge voc results 
www org challenges voc voc workshop 
zisserman williams van gool 
pascal visual object classes challenge voc results 
www org challenges voc voc results pdf 
felzenszwalb huttenlocher 
distance transforms sampled functions 
cornell computing information science technical report tr september 
felzenszwalb huttenlocher 
pictorial structures object recognition 
ijcv 
felzenszwalb mcallester 
generalized architecture 
jair 
fergus perona zisserman 
object class recognition unsupervised scale invariant learning 
cvpr 
fischler elschlager 
representation matching pictorial structures 
ieee transactions computer january 
perona 
discriminative framework modelling object classes 
cvpr pages 
ioffe forsyth 
probabilistic methods finding people 
ijcv june 
jin geman 
context hierarchy probabilistic image model 
cvpr pages ii 
joachims 
making large scale svm learning practical 
sch lkopf burges smola editors advances kernel methods support vector learning 
mit press 
lecun chopra marc huang 
tutorial energy learning 
sch lkopf smola taskar editors predicting structured data 
mit press 
wang collins darrell 
hidden conditional random fields 
pami october 

segmentation verify object hypotheses 
cvpr pages 
sminchisescu 
training deformable models localization 
cvpr pages 
schneiderman kanade 
object detection statistics parts 
ijcv february 
zhang lazebnik schmid 
local features kernels classification texture object categories comprehensive study 
ijcv june 

