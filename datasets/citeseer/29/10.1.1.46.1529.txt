comparison event models naive bayes text classification andrew mccallum mccallum justresearch com kamal nigam cs cmu edu just research henry street pittsburgh pa school computer science carnegie mellon university pittsburgh pa text classification different order probabilistic models classification naive bayes assumption 
multi variate bernoulli model bayesian network dependencies words binary word features larkey croft koller sahami 
multinomial model uni gram language model integer word counts lewis gale mitchell :10.1.1.16.3103
aims clarify confusion describing differences details models empirically comparing classification performance text corpora 
find multi variate bernoulli performs small vocabulary sizes multinomial performs usually performs better larger vocabulary sizes providing average reduction error multi variate bernoulli model vocabulary size 
various simple bayesian classifiers gaining popularity lately perform surprisingly friedman friedman sahami langley 
calculating probability document multiplies probability words occur 
understand individual word occurrences events document collection word events 
call multinomial event model 
approach traditional statistical language modeling speech recognition called unigram language model 
approach text classification numerous people lewis gale croft joachims guthrie walker li mitchell nigam mccallum :10.1.1.21.7950:10.1.1.16.3103
aims clarify confusion approaches explaining models detail 
extensive empirical comparison corpora including web pages usenet articles reuters newswire articles 
results indicate multi variate bernoulli model performs better multinomial small vocabulary sizes 
multinomial usually outperforms multi variate bernoulli large vocabulary sizes beats multi variate bernoulli vocabulary size chosen optimally 
