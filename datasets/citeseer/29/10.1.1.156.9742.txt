learning mixtures dag models bo thiesson christopher meek david maxwell chickering david heckerman microsoft research redmond wa thiesson meek dmax microsoft com describe computationally efficient methods learning mixtures component directed acyclic graphical model mixtures dags 
argue simple search score algorithms infeasible variety problems introduce feasible approach parameter structure search interleaved expected data treated real data 
approach viewed combination cheeseman stutz asymptotic approximation model posterior probability expectation maximization algorithm 
evaluate procedure selecting synthetic real examples 
decade statisticians computer scientists directed acyclic graph dag models learning data cooper herskovits spirtes glymour scheines spiegelhalter dawid lauritzen cowell buntine heckerman geiger chickering 
consider mixtures dag models mdag models methods choosing models class 
mdag models generalize dag models accurately model domains containing multiple distinct populations 
general hope mdag models lead better predictions accurate insights causal relationships 
concentrate prediction 
take decidedly bayesian perspective problem learning mdag models 
principle learning straightforward compute posterior probability model class data criterion average models select models 
computational perspective learning extremely difficult 
problem number possible model structures grows super exponentially number random variables domain 
second problem available methods computing posterior probability mdag model including monte carlo large sample approximations slow 
combination problems simple search score learning algorithms intractable mdag models 
introduce heuristic method mdag model selection addresses difficulties 
method guaranteed find mdag model highest probability experiments suggest identifies 
approach handles missing data component dag models contain hidden latent variables 
approach learn dag models single component mdag models incomplete data 
multi dag models mixtures dag models section describe dag multi dag mdag models 
introduce notation 
denote random variable uppercase letter xi value corresponding random variable letter lower case xi 
discrete denote number values refer value state 
denote set random variables bold face capitalized letter letters pai 
corresponding boldface lower case letter letters pa denote assignment value random variable set 
say configuration shorthand denote probability probability density denote probability distribution mass functions density functions refers probability probability density probability distribution clear context 
suppose problem domain consists random variables xn 
dag model graphical factorization joint probability distribution model consists components structure set local distribution families 
structure directed acyclic graph represents conditional independence assertions factorization joint distribution xi pa pa configuration parents xi structure consistent local distribution families associated dag model equation 
discussion assume local distribution families parametric 
denote collective parameters local distributions rewrite equation xi pa exception discussed section parametric family corresponding variable determined discrete continuous model structure 
consequently suppress parametric family notation refer dag model simply structure denote assertion hypothesis true joint distribution represented dag model precisely conditional independence assertions implied find useful include structure hypothesis explicitly factorization joint distribution compare model structures 
particular write xi pai notation unnecessary argument term pa simpler expression possible 
structure dag model encodes limited form conditional independence call context conditional independence 
particular structure implies sets random variables independent configuration random variables independent configuration general form conditional independence sets random variables may independent configuration dependent configuration multi dag model called bayesian multinet geiger heckerman generalization dag model encode context specific conditional independence 
particular multi dag model distinguished random variable set component dag models encodes joint distribution state distribution multi dag model encodes joint distribution encode context specific conditional independence random variables structure component dag model may different 
denote structure parameters multi dag model addition bc denote structure parameters cth dag model component multi dag model 
denote hypothesis true joint distribution represented mdag model precisely conditional independence assertions implied joint distribution encoded multi dag model 
parameters multi dag model bh shorthand conjunction events sh dag models structure refer multi dag model 
follows assume distinguished random variable multinomial distribution 
addition exception discussed section limit structure component dag models parametric families local distributions follows 
xi discrete random variable require random variable pai component model discrete local distribution families set multinomial distributions configuration pai 
xi continuous random variable require local distribution family xi set linear regressions xi continuous parents gaussian error regression configuration xi discrete parents 
lauritzen refers set restrictions conditional gaussian distribution dag model 
concentrate special case distinguished random variable hidden 
situation interested joint distribution joint distribution mixture distributions determined component dag models mixture weights 

hidden say multi dag model mixture dag models mdag model important subclass dag models gaussian dag model shachter 
subclass local distribution family random variable parents linear regression gaussian noise 
known gaussian dag model xn uniquely determines multivariate gaussian distribution random variables 
model structure dag model part determines shape multivariate gaussian distribution 
mdag model class includes mixtures multivariate gaussian distributions component may different shape 
learning multi dag models sections consider bayesian approach learning multi dag models mdag models 
assume data exchangeable reason data random sample true joint distribution 
addition assume true joint distribution encoded multi dag model uncertain structure parameters 
define discrete random variable states correspond possible true model hypotheses encode uncertainty structure probability distribution 
addition model define continuous vector valued random variable configurations correspond possible true parameters 
encode uncertainty probability density function 
random sample 
xn true distribution compute posterior distributions bayes rule 
model posterior probability various forms model comparison including model averaging bernardo smith 
limit selection model high posterior probability 
follows concentrate model selection posterior model probability 
simplify discussion assume possible model structures equally priori case selection criterion marginal likelihood marginal likelihood criterion consider dag model encodes conditional gaussian distribution denote random variables corresponding parameters local distribution family xi 
buntine heckerman geiger shown parameters mutually independent parameter priors conjugate data complete log marginal likelihood closed form computed efficiently 
observation extends multi dag models 
ic denote set random variables corresponding local distribution family xi component denote set random variables 
corresponding mixture weights 


mutually independent parameter priors ic conjugate data complete marginal likelihood closed form 
particular log log log data restricted variable data restricted variables cases term marginal likelihood trivial dag model having single discrete node terms sum log marginal likelihoods component dag models multi dag 
closed form 
structure search important issue regarding model selection search models structures high posterior probabilities 
consider problem finding dag model highest marginal likelihood set models node parents 
chickering shown problem np hard 
follows immediately problem finding multi dag model highest marginal likelihood set multi dags node component parents np hard 
consequently researchers heuristic search algorithms including greedy search greedy search restarts best search monte carlo methods 
various model selection criteria including log marginal likelihood assumptions just described factorable 
say criterion crit multi dag structure factorable written follows crit gc xi paci data restricted set pa parents xi component xi pac data restricted random variables xi pa cases gc functions 
criterion factorable search efficient reasons 
component dag models non interacting may search dag structure component separately 
search structure component need reevaluate criterion component 
example greedy search dag structure iteratively transform graph choosing transformation improves model criterion transformation possible 
typical transformations include removal reversal addition arc constrained resulting graph acyclic 
factorable criterion need reevaluate gc xi parents changed 
learning simple approach learning multi dag models complete data marginal likelihood closed form 
contrast learning assumption data complete hold distinguished random variable hidden 
data incomplete tractable closed form marginal likelihood available 
approximate marginal likelihood monte carlo large sample methods kass raftery wasserman 
straightforward class algorithm choosing mdag model search structures perform greedy search approximation marginal likelihood 
shall refer class simple score algorithms 
shall see simple search score algorithms mdag model selection computationally infeasible practice 
consider approximation marginal likelihood help motivate tractable class algorithms consider section 
approximation examine large sample approximation proposed cheeseman stutz completion data set 
approximation heuristic chickering heckerman give argument may perform practice furthermore provide empirical study multinomial mixtures shows approximation quite 
experiments accurate accurate standard approximation obtained laplace method tierney kadane 
important idea cheeseman stutz approximation treat data completed em algorithm real data 
idea underlies step em algorithm 
shall see section idea applied structure search 
learning practical approach simple search score algorithms selecting mdag models inefficient reasons 
computing approximations marginal likelihood slow 
approximations factor 
consequently time transformation applied structure search entire structure may need chickering heckerman discuss version cheeseman stutz approximation correction dimension 
pick initial structure run em parameter search compute expected sufficient statistics complete model search structure pretending expected sufficient statistics real sufficient statistics complete data schematic approach mdag model selection 

section consider heuristic approach addresses problems 
basic idea approach interleave parameter search structure search 
schematic approach shown 
choose initial model parameter values 
perform iterations em algorithm find fairly values parameters structure 
parameter values current model compute expected sufficient statistics complete mdag encodes conditional independence facts 
call statistics current model parameters data expected complete model sufficient statistics denote quantity 
detailed discussion computation quantity appendix 
treat expected sufficient statistics sufficient statistics complete data set perform structure search 
pretend data set complete model scores closed form factorable making structure search efficient 
structure search reestimate parameters new structure map parameters expected sufficient statistics 
em computation structure search parameter reestimation steps iterated convergence criterion satisfied 
remainder section discuss variations approach 
addition examine criterion model search initialization structure parameters approach determining number mixture components number states hidden variables component models 
search criterion log marginal likelihood expected complete model sufficient statistics crit log model evaluated model parameters compute expected complete model sufficient statistics 
compute sufficient statistics complete model want possible dependencies data reflected statistics 
compute sufficient statistics incomplete constrained model models visited dur ing search violates constraints supported data 
criterion equation related cheeseman stutz approximation marginal likelihood rewrite log log log argument chickering heckerman suggests equation accurate approximation log marginal likelihood equation accurate criterion practical reasons 
include likelihood ratio correction term equation criterion factor 
just term equation need compute map configuration structure evaluate 
contrast equation compute map configuration 
despite shortcuts experiments described section suggest criterion equation guides structure search models 
approach requires initial structure initial parameterization chosen 
consider structural initialization 
initialize structure component model placing arc hidden variable observable variable exception nodes corresponding continuous random variables point nodes corresponding discrete random variables 
simpler choice initial graph component consists empty graph graph containing arcs 
initialization restricted set priors conjecture approach unable discover connections hidden observable variables 
consider parameter initialization 
mixture components contain hidden continuous variables initialize parameters component dag structure follows 
remove hidden nodes adjacent arcs creating model 
determine map configuration data data complete respect compute map closed form 
create conjugate distribution configuration maximum value agrees map configuration just computed equivalent sample sizes specified user 
non hidden node xi configuration xi hidden discrete parents initialize parameters local distribution family xi drawing conjugate distribution just described 
hidden discrete node xi configuration xi possible parents initialize multinomial parameters associated local distribution family xi fixed distribution uniform 
mixture components contain hidden continuous variables simpler approach initializing parameters random drawing distribution prior 
methods initializing parameters distinguished random variable include setting parameters equal setting parameters prior means drawing parameters dirichlet distribution 
mentioned approach variations 
source variation heuristic algorithm search computed 
options simple search score algorithms include greedy search greedy search restarts best search monte carlo methods 
preliminary studies greedy search effective analysis real data section technique 
source variation schedule alternate parameter structure search 
respect parameter search run em convergence step fixed number steps number steps depends times performed search phase 
respect structure search perform model structure transformations fixed number steps number steps depends times performed search phase find local maximum 
iterate steps consisting em computation structure search mdag structure change consecutive search phases approximate marginal likelihood resulting mdag structure increase 
second schedule algorithm guaranteed terminate marginal likelihood increase indefinitely 
schedule know proof algorithm terminate 
experiments greedy structure search schedule halts 
find convenient describe schedules regular grammar ec denote step step computation structure search respectively 
example em ecs denote case outer iteration run em convergence compute expected complete model sufficient statistics run structure search convergence perform step 
schedule examine em ecs schedule run em steps computing expected complete model sufficient statistics 
technical report companion thiesson meek chickering heckerman evaluate various combinations schedules 
structure search leaves model structure unchanged force iteration outer loop run em convergence steps 
model structure changes forced iteration continue iterate em steps 
experiments indicate necessary run em convergence structure search single em step structure searches selects models lower prediction accuracy 
schedule em ecs works variety problems 
algorithm described compare models contain different random variables models random variable different number states 
perform additional search number states discrete hidden variable applying algorithm initial models different numbers states hidden variables 
discard discrete hidden variable model setting number states 
best mdag initialization identified select best structure criterion 
relatively small number alternatives considered computationally expensive approximation marginal likelihood cheeseman stutz approximation monte carlo method 
example section evaluate predictive performance mdag models real data 
addition evaluate assumptions underlying method learning models 
domain consider observable random variables continuous 
consequently focus attention mixtures gaussian dag models 
accommodate outliers contained data set analyze mixture models consider noise component addition gaussian components 
noise component modeled multivariate uniform distribution viewed empty dag model distribution function random variables uniform 
compare predictive performance mixtures dag models mdag mixtures multivariate gaussian distributions covariance matrices diagonal mixtures multivariate gaussian distributions covariance matrices full 
model classes correspond mdag models fixed empty structures fixed complete structures respectively gaussian components 
suffix indicates existence uniform noise component 
perform outer search identify number components mixture model described section 
particular learn component model gaussian noise component increase number gaussian mixture components model score clearly decreasing 
choose best number components cheeseman stutz criterion 
measure predictive ability chosen model logarithmic scoring rule log xl set test cases number test cases 
approximate xl xl likelihood evaluated map parameter configuration 
learning mdag models em ecs search schedule learning models run em algorithm convergence 
experiments deem em converged ratio change log likelihood proceeding step change log likelihood initialization falls initialize structure parameters search procedures described section equivalent sample sizes equal 
example consider addresses digital encoding handwritten digits hinton dayan revow 
domain random variables corresponding gray scale values scaled smoothed pixel pixel images handwritten digits obtained cedar postal service database hull 
applications joint prediction include image compression digit classification 
sample sizes digits range 
digit samples training remaining samples testing 
relatively diffuse normal wishart parameter prior gaussian components models 
notation degroot prior values set rough assessment average gray scale value pixels set identity matrix 
choose sum number observed variables compute map parameter values closed form 
parameter priors gaussian components mdag models normal wishart priors specified hyperparameters described methods described heckerman geiger 
uniform prior number components mixture learning mdag models uniform prior structure component dag models 
know values variables constrained range fix parameters uniform distribution noise model accordingly 
hyperparameters 
dirichlet prior mixture weights distinguished variable noise component 
gaussian components 
predictive logarithmic score test set digit shown 
number gaussian components model dimension currently implementing monte carlo method average parameter configurations 
logarithmic score digits mdag logarithmic predictive scores test sets digit data 
mdag digit table number gaussian components parameters learned models digit data 
best model class displayed table 
indicates mdag models average improve predictive accuracy models models 
note gains predictive accuracy models obtained reducing average number parameters third 
mhz computer time taken learn mdag models single digit including time needed find optimal number components average hours respectively 
times improved clever search optimal number mixture components 
better understand differences distributions mixture models represent examine individual gaussian components learned mdag models digit 
row shows means components models 
mean values variables component displayed grid shade grey indicates value mean 
displays indicate components type model capturing distinctive types 
reveal dependency structure component models 
help visualize dependencies drew samples component type model 
grid sample shaded indicate sampled values 
samples components look 
surprising variables conditionally independent component 
samples components indicate multiple types represented compo cheeseman stutz criterion structural search step cheeseman stutz criterion intermediate model obtained structure search learning component model digit 
abrupt increases steps occur structure search transitions new component 
nent 
samples look blurred appear multiple superimposed 
generally samples mdag component look distinct style closely resemble mean 
turn attention evaluation key assumptions underlying learning method 
discussed criterion guide structure search equation heuristic approximation true model posterior 
investigate quality approximation evaluate model posterior cheeseman stutz approximation believe accurate approximation intermediate models visited structure search 
heuristic criterion cheeseman stutz criterion increase structure search progresses 
perform evaluation learning component mdag model digit em ecs schedule 
model transitions cheeseman stutz approximation decreased 
shown cheeseman stutz score progresses upward apparent convergence 
obtain similar results data sets 
results suggest heuristic criterion equation useful guide structure search 
statistics experiment able estimate time take learn mdag model simple search score approach described section 
find time learn component mdag model digit cheeseman stutz approximation model comparison approximately years mhz computer previous claim intractability simple score approaches 
natural question cheeseman stutz approximation marginal likelihood accurate model selection 
answer important mdag models select evaluate chosen approximation 
evidence reasonableness approximation provided fact vary number components mdag models cheeseman stutz predic weight mean mdag samples means samples components learned mdag models digit 
tive scores roughly rise fall synchrony usually peaking number components 
structure learning preliminary study mentioned computer scientists statisticians techniques learn structure dag models observational non experimental data 
pearl verma spirtes 
argued set simple reasonable assumptions structures learned infer cause effect relationships 
interesting possibility results generalized may structure learned mdag models infer causal relationships mixed populations populations subgroups different causal relationships 
section preliminary investigation approach learn mdag structure 
perform analysis follows 
construct gold standard mdag model model generate data sets varying size 
data set approach learn mdag model noise component 
compare structure learned model model measure minimum number arc manipulations additions deletions reversals needed transform learned component structure corresponding gold standard structure 
gold standard model mdag model continuous random variables 
model mixture components 
structure third components comp comp identical structure shown 
structure second component comp shown 
dags parameterized spatial overlap 
particular unconditional means comp comp zero means comp equal linear coefficients conditional variances see shachter 
construct data set size sampling graphical structure third components gold standard mdag 
graphical structure second component 
sample weight arc differences size largest comp 
comp comp comp table performance task structure learning function sample size 
cases component gold standard model 
iteratively subsample data creating data sets size 
table shows results learning models data sets em ecs schedule 
columns table contain number components learned mdag sum mixture weights largest components minimum number arc manipulations additions deletions reversals needed transform learned component structure corresponding gold standard structure components largest mixture weights 
arc manipulations lead model different structures family distributions included count 
learned mdag structures close gold standard model 
addition apparent table structure learned component additional arcs comparison gold standard model sample sizes larger 
interesting note essentially structure recovered sample size low 
related dag models single component mdag models hidden variables generalize known statistical models including linear factor analysis latent factor models probabilistic principle component analysis tipping bishop 
mdag models generalize variety mixtures models including naive bayes models clustering cheeseman stutz mixtures factor analytic models hinton dayan revow mixtures probabilistic principle component analytic models tipping bishop 
related learning methods 
idea interleaving parameter structure search learn graphical models discussed jordan morris singh friedman 

consider problem learning mixtures dag models discrete random variables component spanning tree 
similar approach treat expected data real data produce completed data set structure search 
replace heuristic model search polynomial algorithm finding best spanning tree components completed data 
likelihood selection criterion take account complexity model 
singh concentrates learning single dag model discrete random variables incomplete data 
consider continuous variables mixtures dag models 
contrast approach singh uses monte carlo method produce completed data sets structure search 
friedman describes general algorithms learning dag models incomplete data provides theoretical justification methods 
similar approach approach 
friedman treats expected data real data produce completed data sets 
contrast approach friedman obtains expected sufficient statistics new model current model 
statistics calculated performing probabilistic inference current dag model statistics obtained cache previous inferences 
approach need perform inference case missing values compute expected complete model sufficient statistics 
statistics computed model scores arbitrary structures computed additional inference 
discussion described mixtures dag models class models general dag models novel heuristic method choosing models class 
evaluations examples especially ones containing discrete variables needed preliminary evaluations suggest model selection expanded model class lead substantially improved predictions 
result fortunate evaluations show simple search score algorithms models evaluated time monte carlo large sample approximations model posterior probability intractable real problems 
important observation evaluations practical selection criterion introduce marginal likelihood sufficient statistics guide model search 
interesting question 
hope stimulate theoretical answer question uncover better approximations guiding model search 
friedman initial insight 
possibly related challenge theoretical study apparent accuracy cheeseman stutz approximation marginal likelihood 
discussed experiments multinomial mixtures chickering heckerman approximation accurate accurate standard laplace approximation 
evaluations provided evidence cheeseman stutz approximation accurate criterion model selection 
evaluations considered situations component dag models contain hidden variables 
order learn models class methods structure search needed 
situations number possible models significantly larger number possible dags fixed set variables 
constraining set possible models hidden variables instance restricting number hidden variables number possible models infinite 
positive note spirtes 
shown constraint methods suitable assumptions indicate existence hidden common cause variables 
may possible constraint methods suggest initial set plausible models containing hidden variables subjected bayesian analysis 
section saw recover structure mdag model fair degree accuracy 
observation raises intriguing possibility infer causal relationships population consisting subgroups governed different causal relationships 
important issue needs addressed structural identifiability 
example mdag models may superficially different structures may statistically equivalent 
criteria structural identifiability single component dag models known criteria understood mdag models 
appendix expected complete model sufficient statistics appendix examine complete model sufficient statistics closely 
shall limit discussion multi dag models component dag models conditional gaussian distributions 
extension noise component straightforward 
consider multi dag model random variables denote set continuous variables denote configuration nc denote number variables 
denote set discrete variables including distinguished variable denote number possible configurations 
addition 
yn yi configuration observed variables case note different variables may observed different cases 
dempster 
xi denote ith complete case configuration ith case 
consider complete model sufficient statistics complete case denote 
multi dag model vector 
nm rm sm triples nj scalars rj vectors length nc sj square matrices size nc nc 
particular discrete variables take jth configuration nj rj sj nk rk sk complete data set compute expected complete model sufficient statistics xi yi expectation taken respect joint distribution random variables observations current case 
expectation computed performing probabilistic inference multi dag model 
inference simple extension lauritzen 
sum expectations simply scalar vector matrix additions appropriate triple coordinates vector 
note computation described require statistic triple possible configuration discrete variables 
practice sparse representation store triples complete observations consistent data 
bernardo smith 

bayesian theory 
john wiley sons new york 
buntine 

operations learning graphical models 
journal artificial intelligence research 
cheeseman stutz 

bayesian classification auto class theory results 
fayyad shapiro smyth uthurusamy editors advances knowledge discovery data mining pages 
aaai press menlo park ca 
chickering 

learning bayesian networks data 
phd thesis university california los angeles ca 


latent class models 
handbook statistical modeling social behavioral sciences pages 
plenum press new york 
cooper herskovits 

bayesian method induction probabilistic networks data 
machine learning 
kass raftery wasserman 
july 
computing bayes factors combining simulation asymptotic approximations 
technical report department statistics carnegie mellon university pa friedman 

learning belief networks presence missing values hidden variables 
proceedings fourteenth international conference machine learning 
morgan kaufmann san mateo ca 
friedman 

bayesian structural em algorithm 
proceedings fourteenth conference uncertainty artificial intelligence learning 
morgan kaufmann san mateo ca 
appear 
geiger heckerman 

bayesian networks similarity networks bayesian multinets 
artificial intelligence 


rational decisions 
statist 
soc 

heckerman geiger 

likelihoods priors bayesian networks 
technical report msr tr microsoft research redmond wa 
heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
machine learning 
hinton dayan revow 

modeling manifolds images handwritten digits 
ieee transactions neural networks 
hull 

database handwritten text recognition research 
ieee transactions pattern analysis machine intelligence 
lauritzen 

propagation probabilities means variances mixed graphical association models 
journal american statistical association 
jordan morris 

estimating dependency structure hidden variable 
technical report massachusetts institute technology artificial intelligence laboratory 
shachter 

gaussian influence diagrams 
management science 
singh 

learning bayesian networks incomplete data 
proceedings aaai fourteenth national conference artificial intelligence providence ri pages 
aaai press menlo park ca 
spiegelhalter dawid lauritzen cowell 

bayesian analysis expert systems 
statistical science 
spirtes glymour scheines 

causation prediction search 
springer verlag new york 
thiesson meek chickering heckerman 
december 
learning mixtures dag models 
technical report msr tr microsoft research redmond wa 
tierney kadane 

accurate approximations posterior moments marginal densities 
journal american statistical association 
tipping bishop 

mixtures probabilistic principle component analysers 
technical report ncrg neural computing research group 
