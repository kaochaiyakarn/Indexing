evaluating database selection techniques testbed experiment james french allison powell department computer science university virginia va alp gg cs virginia edu charles viles school information library science university north carolina chapel hill chapel hill nc viles ils unc edu kevin prey department computer science university virginia va fg cs virginia edu describe testbed database selection techniques experiment conducted testbed 
testbed decomposition trec tipster data allows analysis data multiple dimensions including collection temporal analysis 
characterize subcollections testbed terms number documents queries documents evaluated relevance distribution relevant documents 
initial results study conducted testbed examines effectiveness ggloss approach database selection 
databases testbed ranked ggloss techniques compared ggloss ideal baseline baseline derived trec relevance judgements 
examined degree ggloss estimate functions approximate baselines 
decompose large collections smaller subcollections serve hypothetical sites distributed information retrieval test environment 
specific details decomposition discussion characteristics environment follow 
number researchers working issues distributed information retrieval systems done connection trec tipster data 
large corpora usenet news groups 
distributed ir research encompasses important problems ffl database collection selection ffl collection fusion results merging ffl dissemination collection information increase retrieval effectiveness :10.1.1.53.7407:10.1.1.26.106:10.1.1.31.1173
examination table shows variety test environments employed researchers gives insight impossible compare findings different research efforts 
description data harman example 
group sources db queries gravano news groups user moffat zobel trec source disk viles french trec random trec source callan trec source disk trec source fox trec source trec source month table summary attributes distributed document collections sampling previous :10.1.1.26.106
test data effectiveness experiments possible sources data relatively limited 
large corpora usenet news groups 
distributed ir research encompasses important problems ffl database collection selection ffl collection fusion results merging ffl dissemination collection information increase retrieval effectiveness :10.1.1.53.7407:10.1.1.26.106:10.1.1.31.1173
examination table shows variety test environments employed researchers gives insight impossible compare findings different research efforts 
description data harman example 
group sources db queries gravano news groups user moffat zobel trec source disk viles french trec random trec source callan trec source disk trec source fox trec source trec source month table summary attributes distributed document collections sampling previous :10.1.1.26.106
test data effectiveness experiments possible sources data relatively limited 
relevance judgements needed researchers limited traditional ir test collections trec tipster data 
hand relevance judgements needed number data sources potentially greater 
interested efficiency effectiveness trec tipster data realistic starting point 
final detail topic set ziff subcollection ziff excluded portion documents come disk associated relevance judgements 
database selection experiment section describes preliminary experiments database selection testbed described 
problem distributed searching decomposed fundamental activities choosing specific databases search searching chosen databases merging results cohesive response 
considerable interest aspects focus specifically activity 
callan call collection selection problem gravano refer text database resource discovery problem :10.1.1.26.106
callan gravano formulate problem similarly :10.1.1.26.106
assume group databases candidates search process query 
query rank databases decide order search databases group candidates 
assume preferred order search databases nature preferred order cast differently different researchers 
database selection experiment section describes preliminary experiments database selection testbed described 
problem distributed searching decomposed fundamental activities choosing specific databases search searching chosen databases merging results cohesive response 
considerable interest aspects focus specifically activity 
callan call collection selection problem gravano refer text database resource discovery problem :10.1.1.26.106
callan gravano formulate problem similarly :10.1.1.26.106
assume group databases candidates search process query 
query rank databases decide order search databases group candidates 
assume preferred order search databases nature preferred order cast differently different researchers 
evaluation phase predicted ranks queries compared preferred orderings decide particular ranking methodology worked 
assume preferred order search databases nature preferred order cast differently different researchers 
evaluation phase predicted ranks queries compared preferred orderings decide particular ranking methodology worked 
unfortunately nature comparison differs research group research group 
point developed fully section evaluation 
experiments investigate ggloss methodology different test environment compare performance standard proposed developers called ideal ranks standard callan called optimal ranks :10.1.1.26.106
described fully 
document query coverage testbed subcollections partitioned original source date 
ggloss gravano proposed gloss glossary servers server approach database selection problem 
gloss originally assumed boolean retrieval model generalized ggloss handle vector space information retrieval model 

ggloss estimators predict ideal 

ideal predict relevance ranking 
relevance ranking called optimal ranking callan rel gravano :10.1.1.26.106
simply mean databases ordered number relevant documents contain 
order avoid overloaded terms optimal ideal acronym rbr relevance ranking 
evaluation methodology tested ggloss methodology different benchmarks ideal relevance ranking full subcollections trec data described earlier 
trec topics test query set 
metrics comparison general agreement type comparison done 
general problem baseline ranking query ranking produced collection selection algorithm 
goal decide candidate ranking approximates baseline ranking 
chosen approaches literature new approach proposed 
mean squared error callan reported comparisons mean squared error predicted ranks desired ranks :10.1.1.26.106
group databases rank candidate ranking compute mse base rank db gamma est rank db base rank db baseline desired rank est rank db predicted rank db recall precision analogs section discuss performance metrics analogous known ir metrics recall precision 
introducing terminology notation tries analysis neutral generalizes include variety baselines 
recall query provide baseline ranking say represents desired goal query plan 
query id mse mean squared error sum compared ideal algorithm produces estimated ranking goal decide approximates assume database db collection merit merit db query expect baseline expressed terms merit expect estimated ranking formulated implicitly explicitly estimating merit 
information processing management 
chris buckley 
smart version 
ftp ftp cs cornell edu pub smart 
james callan lu bruce croft :10.1.1.26.106
searching distributed collections inference networks 
proceedings th international conference research development information retrieval pages seattle wa 
edward fox prabhakar joseph shaw russell rao 
combining evidence multiple searches 
