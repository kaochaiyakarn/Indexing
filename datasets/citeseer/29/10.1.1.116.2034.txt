maximum entropy markov models information extraction segmentation andrew mccallum mccallum justresearch com dayne freitag dayne justresearch com just research henry street pittsburgh pa usa fernando pereira pereira research att com labs research park ave florham park nj usa hidden markov models hmms powerful probabilistic tool modeling sequential data applied success text related tasks part speech tagging text segmentation information extraction 
cases observations usually modeled multinomial distributions discrete vocabulary hmm parameters set maximize likelihood observations 
presents new markovian sequence model closely related hmms allows observations represented arbitrary overlapping features word capitalization formatting part speech defines conditional probability state sequences observation sequences 
maximum entropy framework fit set exponential models represent probability state observation previous state 
positive experimental results segmentation faq 
large volume text available internet causing increasing interest algorithms automatically process mine information text 
presents new markovian sequence model closely related hmms allows observations represented arbitrary overlapping features word capitalization formatting part speech defines conditional probability state sequences observation sequences 
maximum entropy framework fit set exponential models represent probability state observation previous state 
positive experimental results segmentation faq 
large volume text available internet causing increasing interest algorithms automatically process mine information text 
hidden markov models hmms powerful tool representing sequential data applied significant success text related tasks including part speech tagging kupiec text segmentation event tracking yamron carp lowe van named entity recognition bikel schwartz weischedel information extraction leek freitag mccallum :10.1.1.17.2148
hmms probabilistic finite state models parameters state transition probabilities state specific observation probabilities 
greatly contributing popularity availability straightforward procedures training maximum likelihood baum welch trained models find hidden state sequence corresponding observation sequence viterbi 
text related tasks observation probabilities typically represented multinomial distribution discrete finite vocabulary words baum welch training learn parameters maximize probability observation sequences training data 
problems traditional approach 
introduces maximum entropy markov models address concerns 
allow non independent difficult enumerate observation features move away generative joint probability parameterization hmms conditional model represents probability reaching state observation previous state 
conditional probabilities specified exponential models arbitrary observation features 
exponential models follow maximum entropy argument trained generalized iterative scaling gis darroch ratcliff similar form computational cost expectation maximization em algorithm dempster laird rubin 
classic problems rabiner hmms straightforwardly solved new model new variants forward backward viterbi baum welch algorithms :10.1.1.131.2084
remainder describes alternative model detail explains fit parameters gis known unknown state sequences presents variant forward backward procedure solutions classic problems follow naturally 
give experimental results problem extracting question answer pairs lists frequently asked questions faqs showing model increases precision recall factor 

maximum entropy markov models hidden markov model hmm finite state automaton stochastic state transitions observations rabiner :10.1.1.131.2084
classic problems rabiner hmms straightforwardly solved new model new variants forward backward viterbi baum welch algorithms :10.1.1.131.2084
remainder describes alternative model detail explains fit parameters gis known unknown state sequences presents variant forward backward procedure solutions classic problems follow naturally 
give experimental results problem extracting question answer pairs lists frequently asked questions faqs showing model increases precision recall factor 

maximum entropy markov models hidden markov model hmm finite state automaton stochastic state transitions observations rabiner :10.1.1.131.2084
automaton models probabilistic generative process sequence observations produced starting state emitting observation selected state transitioning new state emitting observation designated final state reached 
formally hmm finite set states set possible observations conditional probability distributions state transition probability observation probability states observations represented features defer discussion refinement 
st st 
dependency graph traditional hmm conditional maximum entropy markov model 
representation information shared different source states reducing number parameters improving generalization 
furthermore proposal require difficult step hand crafting parameter tying scheme graphical model state transition function required hmms graphical models 
observations states transitions 
combining transition emission parameters single function transition probabilities represented traditional multinomial influence observations represented maximum entropy exponential method correcting simple multinomial prior adding extra features maximum entropy previously various statistical language modeling problems 
include combination traditional trigrams trigger word features rosenfeld combination arbitrary features sentences trigram models chen rosenfeld :10.1.1.41.4403
note observation previous state treated independent evidence current state 
approach put observations back states transitions 
reduce number parameters useful training data especially sparse 
environmental model reinforcement learning 
focus exclusively techniques probabilistic models 
non probabilistic methods memory techniques dagan transformation learning brill winnow combinations linear classifiers roth give normalized scores decision combined scores decision sequences 
support standard dynamic programming methods finding best segmentation viterbi resort sub optimal methods label observation sequence 
furthermore support hidden variable reestimation baum welch methods required missing incomplete training labels 
exponential models derived maximum entropy applied considerable success natural language tasks including language modeling speech recognition rosenfeld chen rosenfeld segmentation newswire stories beeferman part speech tagging prepositional phrase attachment parsing ratnaparkhi :10.1.1.41.4403
hmms successful similar tasks including part speech tagging kupiec named entity recognition bikel information extraction leek freitag mccallum 
know previous general method combines rich state representation markov models flexible feature combination exponential models 
named entity recognizer sterling grishman uses exponential model label word label indicating position word labeled entity class start inside singleton conditioning information include previous label model 
closer stateless model 
