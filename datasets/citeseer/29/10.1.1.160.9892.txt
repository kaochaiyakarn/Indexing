ieee 
personal material permitted 
permission reprint republish material advertising promotional purposes creating new collective works resale redistribution servers lists reuse copyrighted component works obtained ieee 
material ensure timely dissemination scholarly technical 
copyright rights retained authors copyright holders 
persons copying information expected adhere terms constraints invoked author copyright 
cases works may explicit permission copyright holder 
ieee transactions pattern analysis machine intelligence vol 
september constructing boosting algorithms svms application class classification gunnar ra tsch member ieee sebastian mika lkopf robert mu ller show equivalence mathematical programs support vector sv algorithm equivalent boosting algorithm versa 
exemplify translation procedure new algorithm class leveraging starting class support vector machine svm 
step boosting framework 
building called barrier methods known theory constrained optimization returns function written convex combination base hypotheses characterizes test point generated distribution underlying training data 
simulations class classification problems demonstrate usefulness approach 
index terms boosting svms class classification novelty detection 
boosting methods successfully applied classification problems regression estimation 
high accuracy ease implementation wide applicability placed standard toolbox machine learning neural networks learning methods support vector machines svms :10.1.1.11.2062
focuses points 
algorithmic side propose boosting class classification algorithm technique called barrier optimization standard books optimization 
class classification trains unlabeled data trying assess test point belong distribution underlying training data cf 

problem unsupervised learning thought simplified version problem density estimation 
class classification far studied context svms boosting 
fact second focus derive boosting algorithms equivalence svm boosting level mathematical programs underlying algorithms 
general connection described schapire nowadays considered folklore statement boosting community :10.1.1.107.3285:10.1.1.31.2869
ra tsch australian national university canberra act australia 
mail gunnar anu edu au 
mika 
mu ller fraunhofer str 
berlin germany 
mail sebastian mika klaus robert mueller fraunhofer de 

mu ller department computer science university potsdam august 
potsdam germany 
lkopf max planck institute biological cybernetics 
tu bingen germany 
mail bernhard tuebingen mpg de 
manuscript received nov revised aug accepted dec 
recommended acceptance brodley 
information obtaining reprints article please send mail tpami computer org ieeecs log number 
attempt elaborate insight developing general mechanism convert svm algorithms boosting algorithms vice versa 
potential advantage boosting class classifier boosting techniques prior knowledge choice base hypotheses weak learners 
easier incorporate prior knowledge weak learner kernel function svm 
specifically context class classification advantageous interpretable simple base hypotheses decision stumps linear cuts possible extract simple interpretable rules decision boundary cf 
section 
section connections boosting svms presents general recipe convert svm algorithm boosting algorithm 
relation derive linear programming formulation class problem section develop boosting technique section building results connect boosting barrier optimization techniques 
experimental section shows validity approach 
give brief 
svms boosting sections review basic ideas corresponding optimization problems svms boosting methods 
show relation approaches discuss different norms svms boosting 
xn yn denotes training sample size set ir see appendix summary notation 
support vector machines consider dimensional feature space subset ir mapping ina support vector sv setting corresponds mercer kernel implicitly computing dot product goal svms find ieee ieee transactions pattern analysis machine intelligence vol 
september separating hyperplane described vector feature space fw finding hyperplane cast quadratic optimization problem min kwk subject yn xn simplicity omitted bias offset 
selects hyperplane set small vc capacity case achieved maximizing margin 
margin kwk defined minimum distance training points separating hyperplane 
generally margin example xn yn defined distance pattern separating hyperplane 
positive margin corresponds correct classification larger margin larger confidence classifier correctly classifies input :10.1.1.156.2440:10.1.1.31.2869
connection pattern term yn xn theorem mangasarian 
ir point plane jh 
ij kz kp kz denotes distance plane measured respect dual norm 
satisfying maximizing minimal distance training patterns separating hyperplane equivalent solving optimization problem max ir yn xn conditions theorem 
turns equivalent solving 
case shown solution expressed linear combination training patterns positive coefficients xn xn turns coefficient vector usually sparse 
hold weight vector feature space 
boosting consider boosting algorithms 
particular focus arc gv algorithm 
adaboost similar connections see discussion 
class base hypotheses fhj ir jg 
boosting seeks linear combination fw xj vector hypothesis weights 
done calling base learner selects hypothesis ht iteration computing corresponding hypothesis weight wt cf 

base learner training set set weights dt dt updated iteration starting uniform distribution 
ideally finds hypothesis minimizes weighted training error cf 
weighted minimization 
algorithm hypothesis weights normalized sum 
details weights wt computed adaboost arc gv cf 
respectively 
arc gv shown asymptotically find linear combination solves linear optimization problem lp commonly stated max ir ir pj xn kwk ir denotes nonnegative half space ir 
solution sparse base hypotheses combined 
number bounded number patterns independently size usually smaller 
connections boosting svms common folklore statement pointed boosting svms essentially way measure margin way optimize weight vector svms norm boosting employs norm :10.1.1.107.3285:10.1.1.31.2869
think solely influences imposed regularization 
recall connection making explicit 
show svms boosting unique strategies handle high infinite dimensional spaces 
svms need norm implicitly compute scalar products feature space help kernel trick 
norm expressed terms scalar products 
boosting contrast performs computation explicitly feature space 
known prohibitive solution sparse feature space high infinite dimensional depending size base hypothesis set 
norm sparseness inducing regularization functional 
believed cf 
true adaboost problem separable employs weighted minimization discussion see 

added constraints context boosting 
assumes closure complementation hypothesis set adding constraints change solution 

reason induced sparseness fact vectors far coordinate axes larger respect norm respect 
example consider vectors 
norm norm note norm regularizer optimal solution vertex solution expressed tends sparse 
ra tsch constructing boosting algorithms svms application class classification mandatory 
boosting relies fact hypotheses necessary express solution boosting tries find iteration 
basically boosting considers salient dimensions feature space spanned hypotheses efficient 
level mathematical programs see relation boosting svms clearly similar 
explicit note countable hypothesis set implies mapping 
hj kernel pj hj hj 
hypothesis set spans feature space furthermore feature space spanned mapping corresponding hypothesis set constructed hj pj pj denotes projection jth dimension feature space 
class svms toone class lps class svms years svms generalized various ways deal number different learning problems problem unsupervised learning data unlabeled 
hold view unsupervised learning essentially synonymous density estimation density estimated statistical properties regularity underlying data readily deduced 
density estimation high dimensional spaces known hard 
distributions necessarily possess density 
considerations formed motivation class svm svm approach 
deals problem easier density estimation problem data estimating regions high probability support certain quantile distribution 
nutshell goal estimate region contain large fraction training data keeping value regularizer small 
regularizer sv style large margin regularizer 
geometrically amounts finding hyperplane separates unlabeled training data origin threshold estimates function fw decides pattern belongs class fw find threshold quadratic program 
similar line reasoning :10.1.1.107.3285:10.1.1.31.2869
norm boosting motivated bound generalization error relies fact examples feature space small norm 
svms assumes exists small ball data 
exploits fact corresponding generalization error bounds 

suitably chosen regularizer imply region contains certain fraction test data see details 
typical applications class approaches novelty detection condition monitoring 
applications type case negative data measurements power plant left normal range rare expensive impossible case desirable learn description positive data assess novel point generated process training data 
min ir ir kwk xn optimization problem incorporates intuition large fraction training patterns satisfying fw having small regularization term kwk 
parameter controls trade show fraction points estimated region 
related approach called support vector data description svdd proposed seeking hyperplane hypersphere contains possible training data keeping radius small 
radial basis function rbf kernels shown equivalent current approach 
linear programming approach intend consider high dimensional feature spaces prohibitive solution vectors explicitly carrying computations want forced take dimensions account decide new pattern belongs class 
propose modify regularization term weight vector norm feature space induced hypothesis set norm norm implies sparsity solution vector see discussion section 
obtain optimization problem min ir ir xn kwk fix kwk objective linear kwk optimal solution trivial unbounded kwk 
note exists version kwk uses 
shown possess solution set provided optimal solution satisfies 
furthermore worthwhile observe solutions property shown 
proposition 
assume solution satisfies 
statement holds fw xn fw xn indicator function value argument true cf 
appendix 
statement equivalent conjunction statements 
large fraction outliers training points xn fw xn 

larger fraction training points inside outside boundary estimated region points xn fw xn 
ieee transactions pattern analysis machine intelligence vol 
september proof analogous ones 
proof 
variable optimization problem solution particular optimal respect fix vary observing value objective function changes 
start value larger optimal 
decrease term increases proportionally hand term decrease proportionally fraction points nonzero outliers values decreased amount satisfying constraints 
long fraction outliers greater pays decrease case effect second term larger 
optimum know fraction outliers hand increase starting small value correspondingly increase nonzero correspond points precisely boundary xn 
argument see optimum fraction points outliers boundary estimated region tu emphasize particular unsupervised learning important interpretable regularization constants model selection case poses difficult problem validation data long unlabeled unclear trade size estimated region fraction validation points lie inside 
practice parameter selected knowledge problem hand see section 
similar algorithm proposed independently finds additional term pn fw xn objective 
property holds case derivations boosting algorithm sections adapted easily 
mapping induces hypothesis space fhj ir jg 
easily replace xn pj xn inequality constraints 
furthermore assume hj xn usual hypothesis space complementation closed 
enforce wj changing problem get linear optimization problem min ir ir ir xn kwk looks similar solved arc gv cf 

leveraging approaches section obtained linear programming formulation class problem serve basis deriving boosting algorithm 
despite fact algorithm pac boosting property works similar adaboost 
confusing terms leveraging introduced boosting principally leveraging approach iteratively selects hypothesis hat time updates weight vector implemented different ways 
essentially alternatives 
ideally solves optimization problem hypothesis coefficients selected iterations proposed 
greedy approach original adaboost arc gv algorithm 
updates weight hypothesis selected minimizing exponential cost function 
shown relates coordinate descent methods barrier optimization techniques 
section consider example leveraging approaches categories adapt class problem 
turn approaches certain drawbacks 
section develop new method tries combine advantages approaches avoiding shortcomings 
arc column generation adaboost note formulation similar problems underlying arc column generation adaboost cg adaboost 
difference simply label yn appears additional factor left hand side constraints cf 
available unsupervised learning 
class boosting approach reformulation min max problem 
problems solved arc algorithm turn employs arc gv 
second approach technique borrowed mathematical programming 
go detail section proposed methods primarily serve motivation barrier approach proposed section 
approach class arc observation arc gv algorithm arc maximizes minimum margin 
maximum margin problem min max problem 
ideas reformulates min max problem max ir ir ir xm kwk solution 
idea arc supervised learning arc gv solve version uses labels motivated fact able solve particular min max problem 
order proposed replace occurrences hypothesis output pj xm arc gv quantity soft margin 
applied current case left hand side constraints continually arc gv algorithm ra tsch constructing boosting algorithms svms application class classification details especially arc see 
arc gv computes weighting training set obtain hypothesis computes weight minimizing certain error function 
arc gv exploited optimization tool solve min max problem large hypothesis classes 
known algorithm converges optimal solution 
column generation approach adaboost starts dual problem linear program similar uses technique known optimization community column generation 
just briefly recapitulate algorithm show applied modified problem 
dual cf 
appendix derivation max ir ir xn dn pn dn constraint hypothesis box constraints dimensional simplex equality constraint 
algorithm iteratively selects hypothesis hj column constructs subsets fh solves problem exactly simplex method 
original boosting algorithm iteration generates pattern weighting find hypothesis 
easily seen hypothesis choice progress largest pn xn called edge supervised learning 
clearly hypotheses added problem solved 
due smart selection rule sparseness solution convergence algorithm faster point hypothesis left change current solution pn xn algorithm converged solution full problem 
note careful definition primal problem case infinite hypothesis spaces regression setting considered 
far new approaches boosting techniques supervised learning 
approach advantage easy implement guarantee convergence optimal solution 
second approach guarantees convergence empirically fast performance impossible implement availability lp solver 
go detail algorithms situations useful 
take motivation develop sections different new method barrier optimization techniques combining algorithms advantages avoiding disadvantages 
class barrier algorithm boosting algorithms adaboost arc gv explained barrier optimization techniques see 
going techniques solve class problem show convergence algorithm 
briefly recall basic statements barrier optimization 
goal barrier optimization find optimal solution problem min fxj ci mg convex function nonempty convex set feasible solutions 
problem solved socalled barrier function 
exp barrier function cf 
particularly useful choice purposes xm exp ci denotes penalty parameter 
finding sequence unconstrained minimizers sequence shown minimizers converge global solution original problem 
conditions convergence relaxed shown 
proposition 
assume ci differentiable convex functions 
minimizer exp ci xt limit point solution 
fx apply exp barrier technique problem 
get barrier functional xn exp exp pj xn 
omitted terms corresponding constraints kwk maintain outside barrier optimization 
order reduce number variables optimized find optimal slack variables minimizing setting solving obtain closed form solution log exp log expected satisfies lim max 
plugging obtain simplified barrier objective 
ease notation denote ht hypothesis selected tth iteration hj jth hypothesis hypothesis set notation corresponding weights wt wj respectively 

clear section 
ieee transactions pattern analysis machine intelligence vol 
september log exp contain variables anymore 
variables left optimize 
furthermore functional form similar loss logistic regression cf 
case just additional offset scaling factor 
usually barrier algorithm optimize parameters directly desired precision reached cf 
proposition 
requires knowing hypotheses advance 
propose leveraging algorithm finds new hypothesis ht weight wt iteration 
parameter wt determined iteration 
proving convergence exploits needs minimizer turn base learner helps estimate norm gradient 
optimization simplex special care needs taken additional constraints kwk implement jw kw jw element wise absolute value assume omit absolute value 
fw pj hj 
fw kw fw get objective xn log kw exp fw wn lemma 
derivatives computed kw fw xn hj xn dn fw xn exp dn fw xn exp proof see appendix 
function differentiable needs subgradients 
notation overly heavy try avoid presentation note possible derivations exact 
note gradient respect sign hj xn flipped 
simplicity sections having mind abovementioned projection simplex 
evaluate gradient weight vector satisfies kwk 
apply lemma omit term kw front sum computing gradient 
conclude section 
particularly concerns 
lemma 
suppose ir kwk ir satisfy wj 
global minimizer probability simplex 
proof directly follows appendix showing duality gap zero case 
feasible optimal 
selecting hypothesis gradient respect wj computed cf 
lemma wj xn dn fw xn hj xn dns 
reduce iteratively fixed may choose hypothesis ht negative gradient component ht argmax xn argmax wh wh denotes weight hypothesis finds hypothesis responsible size duality gap maxh wh duality gap primal problem dual cf 
appendix 
nonnegative 
sum called edge hypothesis 
supervised boosting techniques maximizing edge corresponds minimizing training error pattern weighting order find hypothesis unsupervised setting scalar product weight vector hypothesis outputs needs maximized cf 
section 
selection rule related called gauss method full problem see 
selects coordinate largest gradient component 
proof convergence shown need assume base learner 
allowed return hypothesis large necessarily largest edge 
formally base learner needs return hypothesis ht satisfies dn ht xn fw xn max dn xn fw xn continuous strictly monotonically increasing function 
instance small 
base learner needs return hypothesis edge slightly higher edge combined hypothesis 
hypothesis positive left hand side long reached optimality 
require base learner 
base learner satisfies say optimal 
fact criteria formulate conditions base learner 
detailed analysis conditions base learner achieve different rates convergence see see ra tsch constructing boosting algorithms svms application class classification 
obviously expect fastest convergence 
going exploit fact consider general optimality condition defined arbitrary functions finding weights hypothesis ht weight wt computed 
essentially works adaboost space kwk argmin wt chosen constraints fulfilled tth unit vector 
dimensional subset dimensional probability simplex defined freely change coefficient current hypothesis takes care constraints kwk 
may chose wt larger wt cg approach minimization implemented efficiently 
may improve convergence speed practice 
analysis consider simpler case 
derivatives bounded case lower bound improvement choosing weights 
result relating improvement gradient respect chosen weight lemma 
ir strictly convex twice continuously differentiable function open convex set ir 
assume derivative lipschitz continuous jq lj holds fixed 
assume argmin exists 
jq proof shown appendix 
apply lemma case starting 
variable optimized 
gradient smaller zero base learner returns hypothesis large edge larger zero left open boundary matter 
right boundary matter minimizing 
special case follow proof lemma show jq conclude lower bound progress loss function terms gradient selected hypothesis 
proof lemma see sufficient approximate solution minimization min achieve result 

gradient respect meant 
easily shown gradient respect normalized weight vector computation simpler needs know lipschitz constant 
discussed parts algorithm summarized algorithm pseudocode 
line marked clear proof convergence theorem 
algorithm 
class leveraging algorithm argument sample fw number iterations quantile parameter factor decreasing barrier parameter returns convex combination solving class problem 
function class leverage set set ht pn dtn ht xn xn endif wt arg min wt 
xn exp exp log exp fw xn fw xn xn endfor return pt fw xn proof convergence show convergence algorithm combine results technically quite involved 
consider case fixed show barrier loss minimized 
consider question decrease eventually show convergence algorithm proposition 
assume fixed 
show convergence minimum called auxiliary function technique 
define auxiliary function sequence iterates wt continuous function satisfying optimal lemma shows lower bound progress decreasing auxiliary function 
particular case possible bound terms dimensional minimization implemented easily standard line search techniques omit details simplification 
ieee transactions pattern analysis machine intelligence vol 
september eventually reach point close solution finite number steps 
show adapt result 
lemma 
auxiliary function sequence 
assume wts lie dimensional probability simplex 
limit point sequence minimizer probability simplex 
proof 
wt nonincreasing sequence bounded sequence differences wt wt converges zero 
wt converge zero 
iterates wt lie compact space sequence limit point 
continuity 
limit point optimal 
holds true limit point completed proof lemma 
tu show find auxiliary function sequence generated algorithm 
application lemma conclude algorithm generate optimal solutions 
proposition 
assume fixed algorithm 
suppose base learner finds iteration hypothesis satisfies optimality criterion admissible function limit point sequence wt global minimizer simplex 
proof 
shown progress bounded terms negative gradient components 
propose function define auxiliary function xj wj continuous function defined max 
wt cf 
lipschitz constant 
constant exists exists lipschitz constant re bounded exist constant 
exists lemma optimize wt cf 
optimality ht holds pn dt ht xn xn 
may bound wt wt dn ht xn fw xn follow derivations case 
assume difficult case 
base learner returns hypothesis ht satisfying satisfies dn ht xn fw xn strictly increasing max dn xn fw xn combining yields ar constructed function satisfies condition auxiliary functions cf 
positivity obviously satisfied 
algorithm determined pn dt easily verified case holds exists xn second condition satisfied wt iteration wj cf 
lemma 
auxiliary function iterates generated algorithm 
apply lemma conclude limit point sequence global minimizer tu note basic properties functional form proof holds functions 
similar result obtained relying stronger assumptions base learner 
related results obtained optimization box constraint sets see 
theorem combines previous results shows algorithm converges solution original optimization problem barrier approach supposed solve cf 

theorem 
suppose complementation closed bounded finite hypothesis set base learner lh called algorithm returns hypothesis fulfills optimality 
limit point output algorithm solution 
proof 
idea proof follows fixed algorithm converges minimum proposition 
smallest gradient duality gap converge zero 
small reduces factor cf 
step algorithm 
holds conclude 
show reduced gradient full problem respect variables small 
show gradients change drastically reduces gradients full problem converge zero 
apply proposition conclude limit point optimal solution 
ra tsch constructing boosting algorithms svms application class classification bound gradient full problem terms edge returned optimal base learner 
xn dn ht xn fw xn max dn xn fw xn dn hj xn fw xn sum taken hypotheses indices negative gradient components second sum line greater zero leading duality gap cf 
appendix 
full gradient bounded pn dn ht xn fw xn condition algorithm barrier parameter reduced gradient full problem smaller function strictly monotonically increasing continuous 
proposition conclude gradient call smaller finite number iterations 

furthermore gradients just decreasing converging 
limit point subsequence solves 
left show gradient change slightly decreasing factor necessary show limit point solves 
need consider change pattern weighting cf 
computed exp fw xn consider taylor expansion weight changing exp exp exp exp small reducing factor changing little dn dn exp dn exp exp exp fw xn exp decreases faster small dn dn asymptotically gradient small conclude limt kr wt 
apply proposition conclude limit point sequence generated algorithm optimal solution 
tu examples base learner section briefly look base learners algorithm 
depends course problem solved 
important note approach general specialized hypothesis classes designed particular problems performance interpretability desired 
consider base learners linear combinations fixed functions kq hal xq functions kq kernel functions kq xq centered training patterns 
find optimal hypothesis weighting term pn xn needs maximized cf 

needs bounded kq need bounded may maximize xn xn const paragraphs consider special cases particular simple solutions exist 
sparse combinations case minimization simple solution 
shown max xn solution xn gq maxq gq gq pn xn maximum gq maxq gq assumed unique 
implies case eventually useful optimize linear combination functions kq selected anyway 
smaller simpler hypothesis set qg obtains solution 
squares assumption khk xn constant pointed 
concept active kernels proposed known moving centers optimizing centers kernel functions computed 
hypothesis space infinite dimensional needs deeper treatment primal dual problems convergence corresponding optimization algorithms cf 

restrict finite hypothesis case 
ieee transactions pattern analysis machine intelligence vol 
september fig 

class approach tanh decision stumps finding outliers toy problem different values percent top line percent middle line percent bottom line different values class barrier algorithm columns lp right column 
points initially unlabeled points outside region estimated algorithm marked asterisks 
fraction outliers algorithms 
solved minimizing square error weight vector hypothesis output 
usually assumption hold 
may solve argmin argmin xn dn xn xn ck norm solution regularizer effectively bounds cf 

problem simple pn xn 
case particular interesting neural networks linearly combine units output layer 
structure network may implement prior knowledge problem hand 
decision stumps simpler highly interpretable hypothesis class consider decision stumps frequently boosting community slightly different form hc tanh xn possible dim stump considering coordinate splitting position essentially linear combinations hypotheses describe boxes input space subspaces cut directions coordinate axes interpreted easily 
experiments experiments illustrate algorithms detecting novelty anomalies outliers 
conduct experiments evaluate result objective measurement outliers detected normal patterns classified outliers need labels evaluation purposes 
toy data toy experiment show basic properties algorithms 
data cf 
fig 
generated uniform distributions differently sized supports patterns 
leads large density patterns center lower density periphery 
toy model considered outliers 
decision stumps form described section different stumps 
fig 
fourth column shows result barrier algorithm cf 
algorithm 
fifth column fig 
shows result directly solving linear programming problem stumps simplex method intractable examples higher dimensional data 
illustrates parameter top ra tsch constructing boosting algorithms svms application class classification fig 

experiments postal service ocr data set 
recognizer digit output histogram exemplars training test set test exemplars digits 
axis gives output values argument sign decision function 
percent get percent outliers consistent proposition percent true positive test examples false positive class digits 
percent get percent outliers 
case true positive rate percent false positive rate increases percent 
threshold marked graphs 
plots show parzen windows density estimate output histograms 
reality examples sit exactly 
bottom controls fraction outliers marked asterisks 
value algorithm shrinks region small outliers fraction resulting decision boundary superposition soft decision stumps yielding readily interpretable result translated simple rules characterizing normality novelty outlier exceptional event 
note optimal solution uses stumps combined hypothesis 
barrier algorithm selects stumps swiftly reduces weight stumps zero instance iterations weight stumps 
furthermore show snapshots barrier algorithm certain levels plots show output barrier algorithm converges output linear problem solver cplex right 
note cases result barrier algorithm describes data intuitively better case looks bit appropriate lp solution 
conjecture due implicit regularization properties barrier approach 
experiment usps database second experiment utilized usps database handwritten characters 
base hypothesis set rbf kernel functions xn exp kx centered training patterns shown useful independent studies cf 
fig 
shows plot outputs training test sets postal service database handwritten digits 
database 
dual barrier optimization problem appendix sees forcing distribution vector small relative entropy uniform distribution cf 
details 
note differently lp approach favors sparse patterns weights barrier approach tries avoid sparseness exponential form cf 

contains digit images size constitute test set 
fed algorithm training instances digit 
testing done digit digits 
shown fig 
percent leads false positive learning machine seen non training correctly identifies non recognizing percent digits test set 
higher recognition rates achieved smaller percent get percent correct recognition digits test set fairly moderate false positive rate percent 
similar experiments done svm comparable results 
model selection serious problem class approach unsupervised learning technique select model 
note commonly model selection algorithms cross validation easily applicable class problem 
find appropriate base learner 
smaller problem derive specialized algorithm certain problems rbf kernels ocr 
furthermore find optimal parameter due meaning may able infer optimal problem hand set estimated fraction outliers training data 
cases fraction unknown 
case propose simple heuristic may help find appropriate idea follows class classifier find boundary irrespective quality separation set accepted rejected patterns 
intuitively separation achieved clear distance sets large 
propose exactly intuition criterion model selection 
measures average output classifier sets selects separates best 
ieee transactions pattern analysis machine intelligence vol 
september fig 

digit data set digits randomly sampled digits class lp classifier applied 
separation distance error rate misclassification set different values shown 
maximum separation distance coincides minimum test error 
illustrate idea conduct experiment usps data set cf 
section separate digit rest 
training digits additionally include different fractions digits 
data sets compute solution class lp different values compute separation distances fw fw fw fw respective number patterns sets chose largest separation distance 
example maximum distinct cf 
fig 
coincides minimum test error cf 
fig 
fig 

cases characterized small gap outliers observed maximum erroneously achieved 
expected model selection heuristic assumes clear separation true data outliers 
studied correspondence svms boosting methods 
highdimensional feature spaces 
differ deal algorithmic problems cause 
think boosting sv approach highdimensional feature space spanned base hypotheses 
problem tractable effective norm regularizer 
induces sparsity really full space small subspace 
hand think svm boosting approach high dimensional space 
svms kernel trick explicitly feature space 
svms get away having norm regularizers kernel allows computation norm feature space 
methods lead sparse 
optimization packages cplex useful feature allows changing coefficients objective function case obtaining updated solution low computational effort 
fig 

data set digits different fractions axis randomly sampled digits dashed graph shows selected maximum separation criterion 
shows minimizes test error 
solutions sample coefficient space svms feature space boosting methods adapted algorithmically exploit form sparsity produce 
providing insight correspondence concrete practical benefits designing new algorithms 
fact gives rise simple recipe transfer algorithms sv kernel replaced appropriately constructed hypothesis space leveraging optimization analogous mathematical program done norm 
vice versa hypothesis set translated sv kernel 
employed recipe devise leveraging algorithm novelty detection 
new algorithm combines ideas benefits arc column generation algorithm boosting 
converges solution linear program similar svm program 
experiments shown promise new research direction boosting unsupervised learning 
focus contribution seen theoretical conceptual side 
practical side conducted experiments toy data real world ocr data demonstrate proof concept 
interesting realworld class applications challenging splice site detection problem dna cf 

theoretical research dedicated incorporate prior knowledge obtain class algorithms eventually faster better general easier understand interpret 
particular extension algorithms infinite hypothesis classes derivation theoretically motivated model selection methods class classification problem challenging 
appendix notation notational conventions counter number patterns counter number hypotheses dimensionality counter number iterations ra tsch constructing boosting algorithms svms application class classification input space ir space nonnegative real numbers training pattern label hj set base hypotheses element feature space hypothesis weight vector slack variable pattern xn fw combined hypothesis weighting weighting training set quantile parameter determines number outliers margin barrier penalty parameter norm scalar product feature space indicator function true false appendix proof lemma proof 
xn xn kw exp fw xn kw exp hj xn kw kw fw xn fw xn fw xn kw exp xn fw xn exp hj xn dn dn assumed wj 
tu proof lemma proof 
optimal jq sign jq jd strictly convex strictly monotone 
jq jq lipschitz differentiability holds jq lj jq sign jq follows jq jq combine results appendix jq jjq jjq jq primal dual problems duality barrier problem start problem show dual optimization problem min pn dn log dn qn log qn hd dn qn hj xn simplicity 
denote nth row lagrangian problem xn dn log dn qn log qn hd hw introduced sets lagrange multipliers say require 
note positive may add additional constraints changing solution 
setting derivatives respect variables zero yields pj wj nonlinear constraint sets dn exp qn exp tu sign jq jd plugging log dn log qn yields sign sign jq jjq jq jd optimality assumption jq jq lj xn dn log dn qn log qn hd hd log hd hq ieee transactions pattern analysis machine intelligence vol 
september get dual program stated max exp kwk duality linear program exp derive dual follow derivations optimization problem 
brevity omit detailed derivation just show obtained letting go zero 
consider terms sum fw xn log exp fw xn max fw xn 
problems pj wj equivalent 
may conclude dual problems equivalent arrive dual problem min hd dn removed variables rewrote inequality constraint 
equality constraint dn qn duality gap duality gap difference primal dual objective primal dual variables 
variables feasible duality gap zero reached optimal primal dual solution 
compute duality gap 
easily seen variable feasible optimal maxj 
duality gap xn dn log dn qn log qn dual objective xn dn qn negative primal objective xn xn max dn qn line equivalent maxj dn wj 
acknowledgments authors manfred warmuth alex smola bob williamson demiriz valuable discussions 
anonymous referees thorough reviews valuable comments suggestions significantly improved 
partially funded dfg contract ja ja mu eu neurocolt project 
furthermore ra tsch anu university california santa cruz warm hospitality 
drucker schapire simard boosting performance neural networks int pattern recognition artificial intelligence vol 
pp 

lecun jackel bottou cortes denker drucker guyon mu ller sa simard vapnik comparison learning algorithms handwritten digit recognition proc 
int conf 
artificial neural networks fogelman soulie gallinari eds vol 
ii pp 

maclin opitz empirical evaluation bagging boosting proc 
th nat conf 
artifical intelligence pp 

schwenk bengio neural networks proc 
int conf 
artificial neural networks gerstner 
nicoud eds vol 
pp 

bauer kohavi empirical comparison voting classification algorithm bagging boosting variants machine learning vol 
pp 

dietterich experimental comparison methods constructing ensembles decision trees bagging boosting randomization machine learning vol 

duffy helmbold leveraging regression proc 
th ann 
conf 
computer learning theory pp 

ra tsch warmuth mika onoda 
mu ller barrier boosting proc 
th ann 
conf 
computer learning theory pp 

demiriz bennett shawe taylor linear programming boosting column generation machine learning research special issue support vector machines kernel methods 
haykin neural networks comprehensive foundation 
new york macmillan 
bishop neural networks pattern recognition 
oxford univ press 
neural networks tricks trade lecture notes computer science orr mu ller eds vol 

boser guyon vapnik training algorithm optimal margin classifiers proc 
fifth ann 
acm workshop computational learning theory haussler ed pp 

vapnik nature statistical learning theory 
new york springer verlag 
lkopf support vector learning 
munich oldenbourg verlag 
vapnik statistical learning theory 
new york wiley 
cristianini shawe taylor support vector machines 
cambridge cambridge univ press 
lkopf smola learning kernels :10.1.1.11.2062
cambridge mass mit press 
burges tutorial support vector machines pattern recognition knowledge discovery data mining vol 
pp 


mu ller mika ra tsch tsuda lkopf kernel learning algorithms ieee trans 
neural networks vol 
pp 

luenberger linear nonlinear programming second ed 
reading mass addison wesley may 
lkopf platt shawe taylor smola williamson estimating support high dimensional distribution neural computation vol 
pp 

ra tsch constructing boosting algorithms svms application class classification tax duin data domain description vectors proc 
european symp 
artificial neural network verleysen ed pp 

campbell bennett linear programming approach novelty detection advances neural information processing systems leen dietterich tresp eds vol 
pp 

schapire freund bartlett lee boosting margin new explanation effectiveness voting methods annals statistics vol :10.1.1.31.2869
pp 
oct 
freund schapire short boosting japanese soc :10.1.1.107.3285
artificial intelligence vol 
pp 
sept 
appeared japanese translation naoki abe 
schapire singer improved boosting algorithms confidence rated predictions proc :10.1.1.156.2440
ann 
conf 
computer learning theory pp 

mangasarian arbitrary norm separating plane operation research letters vol 
pp 

valiant theory learnable comm 
acm vol 
pp 
nov 
schapire design analysis efficient learning algorithms phd thesis mit press 
freund schapire decision theoretic generalization line learning application boosting proc 
eurocolt european conf 
computational learning theory 
breiman prediction games arcing algorithms neural computation vol 
pp 

technical report statistics dept univ calif berkeley 
ra tsch warmuth marginal boosting neurocolt technical report royal holloway college london july 
extended version accepted colt 
freund schapire game theory line prediction boosting proc 
ann 
conf 
computer learning theory pp 

ra tsch robust boosting convex optimization phd thesis univ potsdam 
anu edu au thesis ps gz 
bennett mangasarian robust linear programming discrimination linearly inseparable sets optimization methods software vol 
pp 

chen donoho saunders atomic decomposition basis pursuit siam scientific computing vol 
pp 

bradley mangasarian rosen parsimonious norm approximation computational optimization applications vol 
pp 

ra tsch onoda 
mu ller soft margins adaboost machine learning vol 
pp 
mar 
neurocolt technical report nc tr 
mangasarian mathematical programming data mining data mining knowledge discovery vol 
pp 

lkopf smola 
mu ller nonlinear component analysis kernel eigenvalue problem neural computation vol 
pp 

ra tsch lkopf smola mika onoda 
mu ller robust ensemble learning advances large margin classifiers smola bartlett lkopf schuurmans eds pp 
cambridge mass mit press 
schapire strength weak learnability machine learning vol 
pp 

duffy helmbold geometric approach leveraging weak learners proc 
computational learning theory fourth european conf 
eurocolt fischer simon eds pp 
mar 
grove schuurmans boosting limit maximizing margin learned ensembles proc 
th nat conf 
artifical intelligence 
kivinen warmuth boosting entropy projection proc 
th ann 
conf 
computer learning theory pp 

friedman hastie tibshirani additive logistic regression statistical view boosting annals statistics vol 
pp 

discussion pp 
technical report dept statistics sequoia hall stanford univ mason baxter bartlett frean functional gradient techniques combining hypotheses advances large margin classifiers smola bartlett lkopf schuurmans eds pp 
cambridge mass mit press 
ra tsch mika warmuth convergence leveraging neurocolt technical report royal holloway college london aug 
shorter version accepted nips 
zhang general greedy approximation algorithm applications advances neural information processing systems vol 
mit press press 
bregman relaxation method finding common point convex sets application solution problems convex programming ussr computational math 
math 
physics vol 
pp 

censor zenios parallel optimization theory algorithms application numerical math 
scientific computation oxford univ press 
collins schapire singer logistic regression adaboost bregman distances proc 
ann 
conf 
computer learning theory pp 

nash linear nonlinear programming 
new york mcgraw hill 
ra tsch demiriz bennett sparse regression ensembles infinite finite hypothesis spaces machine learning vol 
nos 
pp 

neurocolt technical report nc tr 

stable exponential penalty algorithm superlinear convergence application theory optimization vol 
nov 
interior proximal algorithm exponential multiplier method semidefinite programming siam optimization vol 
pp 

penalty barrier multiplier algorithm semidefinite programming optimization methods software 

luo tseng convergence coordinate descent method convex differentiable minimization optimization theory applications vol 
pp 

della pietra della pietra lafferty inducing features random fields ieee trans 
pattern analysis machine intelligence vol 
pp 
apr 
poggio girosi extensions theory networks approximation learning dimensionality reduction clustering technical report aim mit ai lab mar 
smola learning kernels phd thesis technische universita berlin 
salzberg method identifying splice sites translational start sites eukaryotic mrna computational applied bioscience vol 
pp 

sonnenburg ra tsch jagota 
mu ller splice site recognition support vector machines published proc 
int conf 
artificial neural networks 
hayton lkopf tarassenko support vector novelty detection applied jet engine vibration spectra advances neural information processing systems leen dietterich tresp eds vol 
pp 

ieee transactions pattern analysis machine intelligence vol 
september gunnar ra tsch computer science university potsdam germany prize best student faculty natural sciences 
years phd degree natural sciences boosting done fraunhofer research institute computer architecture technology berlin germany 
working postdoctoral fellow research school information sciences australian national university canberra australia 
scientific interests areas include machine learning bioinformatics drug design 
member ieee 
sebastian mika doctoral student fraunhofer berlin 
computer science technical university berlin 
scientific interests fields machine learning kernel methods 
bernhard lkopf msc degree mathematics andthe lionel cooper memorial prize university london followed physics universita tu bingen 
phd degree computer science technical university berlin 
thesis support vector learning won annual dissertation prize german association computer science gi 
bell labs gmd berlin australian national university microsoft research cambridge united kingdom technologies new york humboldt university technical university berlin 
director max planck institute biological cybernetics 
scientific interests include machine learning 
klaus robert mu ller degree mathematical physics andthe phd degree theoretical computer science university karlsruhe germany 
postdoctoral fellow gmd berlin started buildup intelligent data analysis ida group 
european community stp research fellow university tokyo professor amari lab 
department head ida group gmd fraunhofer berlin joint associate professor position gmd andthe university potsdam 
humboldt university technical university berlin andthe university potsdam 
annual national prize pattern recognition prize awarded german pattern recognition society dagm 
serves editorial board computational statistics ieee transactions biomedical engineering program committees various international conferences 
research areas include statistical physics statistical learning theory neural networks support vector machines learning techniques 
interests expanded time series analysis blind source separation techniques statistical denoising methods analysis biomedical data 
information computing topic please visit digital library computer org publications dlib 
