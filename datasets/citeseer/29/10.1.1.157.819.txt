entropy pruning backoff language models andreas stolcke speech technology research laboratory sri international menlo park california criterion pruning parameters gram backoff language models developed relative entropy original pruned model 
shown relative entropy resulting pruning single gram computed exactly efficiently backoff models 
relative entropy measure expressed relative change training set perplexity 
leads simple pruning criterion grams change perplexity threshold removed model 
experiments show production quality hub lm reduced original size increasing recognition error 
compare approach heuristic pruning criterion seymore rosenfeld show approach interpreted approximation relative entropy criterion 
experimentally approaches select similar sets grams overlap exact relative entropy criterion giving marginally better performance 

gram backoff models despite shortcomings dominate technology choice state art speech recognizers 
sources performance improvements higher order models darpa hub sites gram gram models inclusion training data sources hub models typically include broadcast news wsj data 
approaches lead model sizes impractical sort parameter selection technique 
case gram models goal parameter selection chose grams explicit conditional probability estimates assigned model maximize performance minimize perplexity recognition error minimizing model size 
pointed pruning selecting parameters full gram model higher order amounts building variable length gram model training set contexts uniformly represented grams length 
seymore rosenfeld showed selecting grams conditional frequency effective traditional absolute frequency thresholding 
revisit problem gram parameter selection deriving criterion satisfies desiderata 
soundness criterion optimize wellunderstood information theoretic measure language model quality 
efficiency gram selection algorithm fast take time proportional number grams consideration 
self practical consideration want able prune grams existing language models 
means pruning criterion information contained model 
remainder describe pruning algorithm relative entropy distance gram distributions section investigate quantities required pruning wjh wjh criterion obtained efficiently exactly section show criterion highly effective reducing size state art language models negligible performance penalties section investigate relation pruning criterion seymore rosenfeld section draw section 

gram pruning relative entropy gram language model represents probability tuples preceding words 
finite set grams conditional probabilities explicitly represented model 
remaining grams assigned probability recursive backoff word distant backoff weight associated determined wjh 
goal gram pruning remove explicit estimates wjh model reducing number parameters minimizing performance loss 
note pruning retained explicit gram probabilities unchanged backoff weights recomputed changing values implicit backed probability estimates 
pruning approach chosen conceptually independent estimator chosen determine explicit gram wi wi hj logp logp estimates 
goals prune gram models access statistics contained model natural criterion minimize distance distribution embodied original model pruned model 
standard measure divergence distributions relative entropy kullback leibler distance see 
strictly distance metric non negative continuous function zero distributions identical 
denote conditional probabilities assigned original model andp probabilities pruned model 
relative entropy models summation histories contexts hj 
goal select grams pruning minimized 
feasible maximize ph wp logp wjh pp ph wp logp wjh possible subsets grams 
assume grams affect relative entropy roughly independently computed due individual gram 
rank grams effect model entropy prune increase relative entropy 
pp pp choose pruning thresholds helpful look intuitive interpretation ofd terms perplexity average branching factor language model 
perplexity original model evaluated distribution embodies perplexity pruned model original distribution relative change model perplexity expressed terms relative entropy pp ed suggests simple thresholding algorithm gram pruning 
select threshold 
compute relative perplexity increase due pruning gram individually 

remove grams raise perplexity recompute backoff weights 
relation choice relative entropy optimization criterion means new 
relative entropy minimization guise likelihood maximization basis model proposed past text compression markov model induction 
kneser suggested applying backoff gram models shown section heuristic pruning algorithm seymore rosenfeld amounts approximate relative entropy minimization 
algorithm described section novel removes approximations employed previous approaches 
specifically algorithm assumes backoff weights unchanged pruning consider effect changed backoff weight gram probabilities pruned effect discussed detail section 
main approximation remains algorithm greedy aspect consider possible interactions selected grams prune solely relative entropy due removing single gram avoid searching exponential space gram subsets 

computing relative entropy show relative due pruning single gram parameter computed exactly efficiently 
consider effect removing gram consisting 
entails changes probability estimates 
backoff weight associated changed affecting backed estimates involving 
notation bo wi denote case original model contain explicit logp wjh logp wjh gram estimate wi 
original backoff weight backoff weight pruned model 
bo wi wi logp logp explicit wjh replaced backoff history obtained dropping word inh 
estimates involving unchanged estimates bo wi true 
substituting get wi logp logp wi wjh wjh np wjh logp wjh logp wjh bo wi logp logp np wjh logp wjh log logp wjh np wjh logp wjh log logp wjh log log bo wi log log wi bo wi wi bo wi wi bo wi wi wi bo wi gram requires summation vocabulary infeasible large vocabularies models 
plugging terms backed estimates see sum factored allow efficient computation 
bo wi sum line represents total probability mass backoff numerator computing needs computed done efficiently summing non backoff estimates marginal history obtained multiplying conditional jh 
need able compute revised backoff weights efficiently constant time gram 
recall obtained dropping term pruned gram summation numerator denominator 
compute original numerator denominator wjh andp wjh respectively obtain 

experiments evaluated relative entropy language model pruning broadcast news domain sri hub evaluation system 
best lists generated bigram language model various pruned versions large gram language model 
system partly due time constraints partly system generated best lists large trigram language model rescoring experiments smaller language models meaningful 
bigrams trigrams grams pp wer table perplexity pp word error rate wer function pruning threshold language model sizes 
noted section pruning algorithm applicable irrespective particular gram estimator 
turing smoothing investigate possible interactions smoothing methods pruning 
table shows model size perplexity word error results determined development test set various pruning thresholds 
rows table give performance full gram pure trigram model respectively 
note perplexity refers independent test set training set perplexity underlies pruning criterion 
shown pruning highly effective 
obtain model size original model degradation recognition performance perplexity logp wjh logp wjh increase 
comparing pruned gram model full trigram model see better include non redundant grams larger number trigrams 
pruned gram perplexity lower word error full trigram 

comparison seymore rosenfeld approach seymore rosenfeld proposed different pruning scheme backoff models henceforth called sr criterion opposed relative entropy re criterion 
sr approach grams ranked weighted difference log probability estimate pruning discounted frequency training 
comparing wheren gram observed expansion ofd see criteria related 
assume roughly proportional top ranking purposes equivalent 
difference log corresponds quantity 
major difference approaches sr criterion include effect grams considered due changes backoff weight 
trigrams sr re 
trigrams 
shared trigrams table comparison seymore rosenfeld sr relative entropy re pruning perplexities function number trigrams 
evaluate effect ignoring backed estimates pruning criterion compared performance sr re criterion broadcast news development test set best rescoring system described 
methods comparable adopted seymore rosenfeld approach ranking grams criterion question retain specified number grams top ranked list 
sake simplicity trigram version hub language model earlier restricted pruning trigrams 
verified discounted replaced model gram probability changing ranking significantly chosen grams 
means sr criterion entirely information model making convenient model post processing 
tables show model perplexity word error rates respectively pruning methods function number trigrams model 
terms perplexity see small albeit consistent advantage relative entropy method expected optimized criterion 
difference negligible comes recognition performance results identical differ non significantly 
conclude practical purposes sr criterion approximation re criterion 
looked overlap grams chosen 
trigrams sr re table comparison seymore rosenfeld sr relative entropy re pruning word error rate function number trigrams 
table overlap selected trigrams sr re methods 
criteria shown table 
percentage common trigrams ranges decrease model size increases 
expect frequent grams shared making surprise methods perform similarly 

developed algorithm gram selection backoff gram language models minimizing relative entropy full pruned model 
experiments show algorithm highly effective eliminating parameters hub gram model significantly affecting performance 
pruning criterion seymore rosenfeld seen approximate version relative entropy criterion empirically methods perform 
acknowledgments sponsored darpa naval command control ocean surveillance center contract 
roni rosenfeld seymore clarifications discussions regarding 
hermann ney dietrich pointing similarities 

bell cleary witten 
text compression 
prentice hall englewood cliffs 

cover thomas 
elements information theory 
john wiley sons new york 


population frequencies species estimation population parameters 
biometrika 

jelinek 
trigrams 
struggle improved language models 
proc 
eurospeech pp 
genova italy 

katz 
estimation probabilities sparse data language model component speech recognizer 
ieee assp 

kneser 
statistical language modeling variable context length 
editors proc 
ic slp vol 
pp 
philadelphia 

ron singer tishby 
power amnesia 
cowan tesauro alspector editors nips pp 

morgan kaufmann san mateo ca 
log 
sankar stolcke 
acoustic modeling sri hub partitioned evaluation continuous speech recognition system 
speech pp 
va 

seymore rosenfeld 
scalable models 
editors proc 
icslp vol 
pp 
philadelphia 

stolcke omohundro 
hidden markov model induction bayesian model merging 
hanson cowan giles editors nips pp 

morgan kaufmann san mateo ca 
erratum published error second equation section 
instances quantity mistakenly typeset log 
information incorrect 
