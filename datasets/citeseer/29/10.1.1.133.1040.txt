machine learning proceedings thirteenth international conference 
experiments new boosting algorithm 
earlier introduced new boosting algorithm called adaboost theoretically significantly reduce error learning algorithm consistently generates classifiers performance little better random guessing 
introduced related notion pseudo loss method forcing learning algorithm multi label concentrate labels hardest discriminate 
describe experiments carried assess adaboost pseudo loss performs real learning problems 
performed sets experiments 
theory error weak learning algorithm consistently generates classifiers need little bit better random guessing 
despite potential benefits boosting promised theoretical results true practical value boosting assessed testing method real machine learning problems 
experimental assessment new boosting algorithm called adaboost 
boosting works repeatedly running weak learning algorithm various distributions training data combining classifiers produced weak learner single composite classifier 
provably effective boosting algorithms schapire freund :10.1.1.153.7626:10.1.1.37.5438
described analyzed adaboost argued new boosting algorithm certain properties practical easier implement predecessors 
algorithm experiments described detail section 
home page www research att com people uid 
expected change www research att com uid near uid 
home page www research att com people uid 
expected change www research att com uid near uid 
term weak learning algorithm practice boosting combined quite strong learning algorithm 
yoav freund robert schapire laboratories mountain avenue murray hill nj research att com describes distinct sets experiments 
set experiments described section compared boosting bagging method described breiman works general fashion repeatedly weak learning algorithm combining computed classifiers constructs simpler manner :10.1.1.32.9399:10.1.1.32.9399
details 
compared boosting bagging methods combining classifiers 
comparison allows separate effect modifying distribution round done differently algorithm effect voting done 
experiments compared boosting bagging number different weak learning algorithms varying levels sophistication 
details 
compared boosting bagging methods combining classifiers 
comparison allows separate effect modifying distribution round done differently algorithm effect voting done 
experiments compared boosting bagging number different weak learning algorithms varying levels sophistication 
include algorithm searches simple prediction rules test single attribute similar holte simple classification rules algorithm searches single decision rule tests conjunction attribute tests similar flavor rule formation part cohen ripper algorithm widmer irep algorithm quinlan decision tree algorithm :10.1.1.103.7226
tested algorithms collection benchmark learning problems taken uci repository 
main experiments boosting performs significantly uniformly better bagging weak learning algorithm generates fairly simple classifiers algorithms 
combined boosting outperform bagging slightly results compelling 
boosting simple rules algorithm construct classifiers quite relative say 
helpful learning problems having properties 
property holds real world problems observed examples tend varying degrees hardness 
problems boosting algorithm tends generate distributions concentrate harder examples challenging weak learning algorithm perform harder parts sample space 
second property learning algorithm sensitive changes training examples significantly different hypotheses generated different training sets 
sense boosting similar breiman bagging performs best weak learner exhibits unstable behavior :10.1.1.32.9399:10.1.1.32.9399
bagging boosting tries actively force weak learning algorithm change hypotheses changing distribution training examples function errors previously generated hypotheses 
second effect boosting variance reduction 
intuitively weighted majority hypotheses trained different samples taken training set effect reducing random variability combined hypothesis 
bagging boosting may effect producing combined hypothesis variance significantly lower produced weak learner 
second effect boosting variance reduction 
intuitively weighted majority hypotheses trained different samples taken training set effect reducing random variability combined hypothesis 
bagging boosting may effect producing combined hypothesis variance significantly lower produced weak learner 
bagging boosting may reduce bias learning algorithm discussed 
see kong dietterich discussion bias variance reducing effects voting multiple hypotheses breiman comparing boosting bagging terms effects bias variance :10.1.1.57.5909
set experiments compare boosting bagging try comparison separate bias variance reducing effects boosting 
previous 
drucker schapire simard performed experiments boosting algorithm 
schapire original boosting algorithm combined neural net ocr problem :10.1.1.153.7626
see kong dietterich discussion bias variance reducing effects voting multiple hypotheses breiman comparing boosting bagging terms effects bias variance :10.1.1.57.5909
set experiments compare boosting bagging try comparison separate bias variance reducing effects boosting 
previous 
drucker schapire simard performed experiments boosting algorithm 
schapire original boosting algorithm combined neural net ocr problem :10.1.1.153.7626
followup comparisons ensemble methods done drucker 
drucker cortes adaboost decision tree algorithm ocr task 
jackson craven adaboost learn classifiers represented sparse perceptrons tested algorithm set benchmarks 
quinlan conducted independent comparison boosting bagging combined collection uci benchmarks :10.1.1.49.2457
schapire original boosting algorithm combined neural net ocr problem :10.1.1.153.7626
followup comparisons ensemble methods done drucker 
drucker cortes adaboost decision tree algorithm ocr task 
jackson craven adaboost learn classifiers represented sparse perceptrons tested algorithm set benchmarks 
quinlan conducted independent comparison boosting bagging combined collection uci benchmarks :10.1.1.49.2457
algorithm adaboost input sequence examples xm ym labels yi kg weak learning algorithm weaklearn integer specifying number iterations initialize 
call weaklearn providing distribution dt 

get back hypothesis ht 
default options pruning turned 
expects unweighted training sample resampling 
attempt adaboost designed minimize error pseudo loss 
furthermore expect pseudoloss helpful weak learning algorithm strong algorithm usually able find hypothesis error 
bagging compared boosting breiman bootstrap aggregating bagging method training combining multiple copies learning algorithm :10.1.1.32.9399
briefly method works training copy algorithm bootstrap sample sample size chosen uniformly random replacement original training set size 
multiple hypotheses computed combined simple voting final composite hypothesis classifies example class assigned underlying weak hypotheses 
see details 
method quite effective especially breiman unstable learning algorithms small change data effects large change computed hypothesis 
note non binary classification problems boosting simple classifiers done effectively sophisticated pseudo loss 
starting complex algorithm boosting improve performance compelling advantage bagging 
boosting combined complex algorithm may give greatest improvement performance reasonably large amount data available note instance boosting performance letter recognition problem training examples 
naturally needs consider improvement error worth additional computation time 
rounds boosting quinlan got results rounds :10.1.1.49.2457
boosting may applications reducing error classifier 
instance saw section boosting find small set prototypes nearest neighbor classifier 
described boosting combines effects 
reduces bias weak learner forcing weak learner concentrate different parts instance space reduces variance weak learner averaging hypotheses generated different subsamples training set 
jason catlett william cohen extensive advice design experiments 
ross quinlan suggesting comparison boosting bagging 
leo breiman cortes har ris drucker jeff jackson michael kearns warren smith david wolpert anonymous icml reviewers helpful comments suggestions criticisms 
contributed datasets 
leo breiman :10.1.1.32.9399
bagging predictors 
technical report department statistics university california berkeley 
leo breiman 
bias variance arcing classifiers 
international journal pattern recognition artificial intelligence 
harris drucker robert schapire simard 
improving performance neural networks boosting algorithm 
advances neural information processing systems pages 
yoav freund :10.1.1.37.5438
boosting weak learning algorithm majority 
information computation 
yoav freund robert schapire 
decision online learning application boosting 
ieee transactions information theory pages 
peter hart 
condensed nearest neighbor rule 
ieee transactions information theory may 
robert holte :10.1.1.103.7226
simple classification rules perform commonly datasets 
machine learning 
jeffrey jackson mark craven 
learning sparse perceptrons 
advances neural information processing systems 
michael kearns mansour 
boosting ability top decision tree learning algorithms 
proceedings eighth annual acm symposium theory computing 
bae kong thomas dietterich :10.1.1.57.5909
error correcting output coding corrects bias variance 
proceedings twelfth machine learning pages 
ross quinlan 
programs machine learning 
proceedings twelfth machine learning pages 
ross quinlan 
programs machine learning 
morgan kaufmann 
ross quinlan :10.1.1.49.2457
bagging boosting 
proceedings fourteenth national conference artificial intelligence 
robert schapire :10.1.1.153.7626
strength weak learnability 
morgan kaufmann 
ross quinlan :10.1.1.49.2457
bagging boosting 
proceedings fourteenth national conference artificial intelligence 
robert schapire :10.1.1.153.7626
strength weak learnability 
machine learning 
simard le cun john denker 
efficient pattern recognition new transformation distance 
