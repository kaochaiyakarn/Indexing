efficient lattice representation generation weng andreas stolcke sankar speech technology research laboratory sri international menlo park california stolcke speech sri com large vocabulary multi pass speech recognition systems desirable generate word lattices incorporating large number hypotheses keeping lattice sizes small 
describe new techniques reducing word lattice sizes eliminating hypotheses 
technique algorithm reduce size non deterministic bigram word lattices 
algorithm iteratively combines lattice nodes transitions local properties show change set allowed hypotheses 
bigram word lattices generated hub broadcast news speech reduces lattice sizes half average 
produce smaller lattices standard finite state automaton determinization minimization algorithms 
second technique improved algorithm expanding lattices trigram language models 
giving nodes unique trigram context algorithm creates unique contexts trigrams explicitly represented model 
backed trigram probabilities encoded node duplication factoring probabilities bigram probabilities backoff weights 
experiments broadcast news show method reduces trigram lattice sizes factor reduces expansion time factor 
compared conventionally expanded lattices recognition compactly expanded lattices faster affecting recognition accuracy 

large vocabulary speech recognition systems high accuracy recognition achieved reasonable time space demands multi pass process lattices intermediate representation 
approach discussed time acoustic information removed lattices word lattice word graph generated retaining language model lm probabilities 
lattice constrained lm subsequent recognition passes 
incorporate higher order lm probabilities lattices typically undergo expansion node duplication process 
desirable generate lattices containing large number paths minimize errors result multi pass search 
large reported funded darpa contract lattices expansion higher order gram lm prohibitive subsequent recognition passes slow 
address problem developed algorithms reduce lattice sizes changing set hypotheses encoded 
algorithms evaluated darpa hub corpus 
technique described section reduces size bigram lattices generated recognition pass 
compare new algorithm classic finite state automaton fsa determinization minimization algorithms 
second technique described section evaluated section consists compact expansion trigram lattices 
section concludes 

bigram lattice reduction sri latest hub decipher recognition system bigram lattices generated backward pass forward backward pass search word best algorithm 
backward pruning thresholds control lattice sizes 
lattice generation method similar described 
especially noisy speech generated bigram lattices quite large costly expand 
common approach case tighten backward pruning threshold lattice error rates minimum word error path lattice increase 
alternatively try reduce lattice sizes removing redundant nodes transitions changing set word hypotheses allowed lattice 
algorithms similar goal developed 
approach view lattices finite state automata fsas apply classical fsa minimization algorithm 
algorithms minimizing reducing weighted word lattices developed 
directly applicable case decipher word lattices words nodes transitions non deterministic successor node uniquely determined word 
fsa determinization required process exponential worst case time space complexity report performance practice 
chose develop fast reduction algorithm operates directly non deterministic lattices eliminating local redundancies 
total reduction reduction table bigram lattice sizes reduction 
key observation underlying algorithm nodes lattice word label set successor predecessor nodes merged changing set word hypotheses encoded lattice 
depending nodes merged predecessor node set successor node set forward backward reduction pass respectively 
get approach forward backward passes iterated redundancies 
brevity describe backward reduction algorithm forward version symmetrical 
bigram lattice expansion 
conventional trigram expansion 
backward reduction algorithm sout set successor nodes node denote word name lattice node lattice node reverse topological order starting final node pair predecessor nodes node word word sout sout merge nodes runtime algorithm proportional number nodes times constant depends maximum fan fan lattice nodes 
aggressive reduction algorithm obtained sout sout certain percentage overlap outgoing node sets required node merging 
produce smaller lattices add new hypotheses lattice 
evaluated tradeoffs associated variant 
experimentally eventual size reduction occurred single pass backward algorithm 
explained way recognizer operates 
multiple hypotheses word tend generated starting different neighboring frames time 
furthermore lattices tend bushy utterance utterance pruning eliminated proportionally larger number hypotheses 
effects lead node merging successors effective 
evaluated effectiveness reduction algorithm lattices generated darpa hub development test set 
high bandwidth planned speech spontaneous speech conditions set included generally giving considerably larger lattices 
single backward reduction pass reduced lattice sizes shown table 
recognition reduced lattices gave small statistically significant reduction word error result fewer search errors 
conventional expansion bigram lattice trigram lattice incoming nodes label 
compared local reduction algorithm fsa determinization minimization approach 
converted word lattices dual fsa representation process maps node exactly fsa transition 
performed fsa determinization minimization fsm toolkit 
bigram probabilities retrofitted word lattice changing structure set transition probabilities effectively turning weighted fsa operations classical non weighted counterparts 
comparison purposes transformed minimized fsa back node word lattice 
local reduction algorithm produced slightly smaller lattices fsa determinization minimization 
average number transitions fsa processing larger hub lattices larger lattices 
recognition accuracies kinds lattices identical expected 
regard recognition speed determinization minimization approach advantageous determinism narrows search space word transitions 
balanced overhead recognizer proportional lattice size time input 
test set decipher recognizer non deterministic reduced minimized lattices gave virtually identical recognition times 

trigram lattice expansion second approach obtain smaller expanded gram lattices optimize expansion step 
purpose gram lattice expansion allow higher order gram probabilities reverse conversion constructs node unique pair fsa node incoming transition symbol 
produces best results fsa deterministic 
conversions back forth representations designed exact inverses 
assigned word transitions increase accuracy subsequent recognition passes 
discussion trigram lattices simplicity ideas generalize higherorder grams 
place trigram probabilities lattice transitions create unique word context transition 
example node labeled transitions duplicated guarantee uniqueness trigram contexts placing probabilities jac transitions 
central node label predecessor nodes labeled word additional node corresponding outgoing transitions duplicated 
conventional trigram expansion algorithm performs node duplication exhaustively follows 
bigram lattice expansion trigram lm explicit trigram probability conventional lattice expansion algorithm node lattice topological order predecessor node successor node node word created trigram context word word word connect node node create node label word connect node node node node put word word transition remove node incoming outgoing transitions algorithm correctly implements trigram probabilities lattice considerable increase lattice size 
hub development set number lattice transitions increased fold conventional approach 
conventional expansion algorithm ensures unique trigram histories copying node labeled wi appears trigram wi 
copy predecessor wi lattice created predecessors trigram lm 
contrast new expansion algorithm creates copy wi explicit trigram wi lm 
key factor backed trigram probabilities wi wi backoff weight bo wi wi bigram probability wi multiply backoff weight weight wi wi transition keeping bigram probability wi wi transition 
node duplication required trigrams 
backoff weights probabilities combine multiplicatively total score path wi wi wi amounts correct trigram probability wi wi 
illustrates compact expansion idea explicit trigram probability 
notice node labeled incoming transition node outgoing transition node created 
placed conventional trigram expansion 
compact trigram expansion 
compact expansion bigram lattice trigram lattice 
transition new node node 
weight transition copied 
explicit trigrams processed outgoing transitions original node weighted corresponding bigram probabilities djc 
furthermore bigram backoff weights bo ac bo bc bo fc multiplied corresponding incoming transitions original node 
compact lattice expansion algorithm weight aggregate probability transition 
node lattice topological order predecessor node successor node explicit trigram probability word word word node word created trigram context word word word connect node node create node label word connect node node node node set weight word word mark transitions transition marked remove set weight weight bo word word successor node transition marked remove set weight word incoming transitions marked remove node incoming outgoing transitions 
algorithm total conventional compact conventional minimized compact minimized table trigram lattice sizes terms average number transitions 
potential problem approach explicit trigram probabilities lattice retains path backoff transitions higher weight correct trigram transition preferred search 
example paths labeled search incorrect lower path chosen djc bo ac 
proper solution preprocess trigram lm prune trigram probabilities lower corresponding improper backoff estimate renormalize lm 
experiments hub data showed practice eliminates small fraction trigrams significantly changing power lm final recognition results 
leaving improper paths lattice significant effect recognition accuracy compared pruned lm 

lattice expansion experiments experiments conducted conventional compact trigram expansion algorithms 
trigram lm expansion sri hub word trigram lm described 
bigram lattices hub development test sets input lattices algorithms compact expansion algorithm times faster conventional algorithm 
furthermore shown table size compact trigram lattices sixth conventional ones 
applied weighted determinization minimization algorithms described implemented conventionally compactly expanded trigram lattices 
shown rows table determinization minimization reduced size conventional lattices 
determinization minimization doubled size compact trigram lattices 
result backoff structure introduces non determinism lattice see 
recognition experiments carried nondeterministic conventional compact trigram lattices sri hub unadapted acoustic models 
word recognition error rates showed difference performance conventional compact trigram lattices 
recognition speed compact lattices faster conventional lattices 
time memory limitations compact lattice expansion step allowed relax pruning initial lattice generation resulting decreased lattice error rates 
previously lattices limited word error condition condition 
pruning efficient expansion lattice errors reduced respectively 
recognition system observe lower final best error rates 

described algorithms keep word lattices small sacrificing lattice word recognition error rates 
bigram lattice reduction algorithm merges lattice nodes shown locally redundant halving size lattices obtained recognizer 
experimentally algorithm superior alternative approach fsa determinization minimization 
furthermore developed new trigram lattice expansion algorithm reduces trigram lattice sizes factor expansion time factor 
recognition resulting lattices faster compared conventional trigram lattices 
due reduced resource demands able significantly lower lattice error rate new algorithm 


hopcroft ullman 
automata theory languages computation 
addison wesley reading ma 

mohri pereira riley 
fsm library general purpose finite state machine software tools 
www research att com sw tools fsm 

mohri riley 
weighted determinization minimization large vocabulary speech recognition 
kokkinakis editors proc 
eurospeech vol 
pp 
rhodes greece 

digalakis weintraub 
dictation sri decipher speech recognition system progressive search techniques 
proc 
icassp vol 
ii pp 
minneapolis 

ney aubert 
word graph algorithm large vocabulary continuous speech recognition 
proc 
icslp pp 
yokohama 

odell 
context large vocabulary speech recognition 
ph thesis cambridge university engineering department cambridge 

sankar weng rivlin stolcke gadde 
development sri broadcast news transcription system 
proceedings darpa broadcast news transcription understanding workshop pp 
va 

schwartz austin 
comparison approximate algorithms finding multiple best sentence hypotheses 
proc 
icassp vol 
pp 
toronto 

str automatic continuous speech recognition rapid speaker adaptation human machine interaction 
ph thesis kth stockholm 

weng stolcke sankar 
hub language modeling domain interpolation data clustering 
proceedings darpa speech recognition workshop pp 
va 

weng stolcke sankar 
new lattice search strategies sri hub system 
proceedings darpa broadcast news transcription understanding workshop pp 
va 
