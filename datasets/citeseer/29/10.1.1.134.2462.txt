journal arti cial intelligence research submitted published reinforcement learning survey leslie pack kaelbling lpk cs brown edu michael littman cs brown edu computer science department box brown university providence ri usa andrew moore cs cmu edu smith hall carnegie mellon university forbes avenue pittsburgh pa usa surveys eld reinforcement learning computer science perspective 
written accessible researchers familiar machine learning 
historical basis eld broad selection current summarized 
reinforcement learning problem faced agent learns behavior trial error interactions dynamic environment 
described resemblance psychology di ers considerably details word reinforcement 
discusses central issues reinforcement learning including trading exploration exploitation establishing foundations eld markov decision theory learning delayed reinforcement constructing empirical models accelerate learning making generalization hierarchy coping hidden state 
suboptimal action picked leaving true optimal action data superiority discovered 
agent explore ameliorate outcome 
useful heuristic optimism face actions selected greedily strongly optimistic prior beliefs put payo strong negative evidence needed eliminate action consideration 
measurable danger optimal action risk arbitrarily small 
techniques reinforcement learning algorithms including interval exploration method kaelbling described shortly exploration bonus dyna sutton curiosity driven exploration schmidhuber exploration mechanism prioritized sweeping moore atkeson :10.1.1.134.8196:10.1.1.48.6005
randomized strategies simple exploration strategy take action best estimated expected reward default probability choose action random 
versions strategy start large value encourage initial exploration slowly decreased 
objection simple strategy experiments non greedy action try promising alternative clearly hopeless alternative 
slightly sophisticated strategy boltzmann exploration 
step guaranteed strictly improve performance policy 
improvements possible policy guaranteed optimal 
jaj jsj distinct policies sequence policies improves step algorithm terminates exponential number iterations puterman 
important open question iterations policy iteration takes worst case 
known running time xed discount factor polynomial bound total size mdp littman :10.1.1.41.1514
enhancement value iteration policy iteration practice value iteration faster iteration policy iteration takes fewer iterations 
arguments put forth ect approach large problems 
puterman modi ed policy iteration algorithm puterman shin provides method trading iteration time iteration improvement smoother way 
basic idea expensive part policy iteration solving exact value nding exact value perform steps modi ed value iteration step policy held xed successive iterations 
computational complexity reinforcement learning survey value iteration works producing successive approximations optimal value function 
iteration performed steps faster sparsity transition function 
number iterations required grow exponentially discount factor condon discount factor approaches decisions results happen farther farther 
practice policy iteration converges fewer iterations value iteration iteration costs jsj prohibitive 
tight worst case bound available policy iteration littman :10.1.1.41.1514
modi ed policy iteration puterman shin seeks trade cheap ective iterations preferred 
linear programming extremely general problem mdps solved general purpose linear programming packages ho man karp 
advantage approach commercial quality linear programming packages available time space requirements quite high 
theoretic perspective linear programming known algorithm solve mdps polynomial time theoretically cient algorithms shown cient practice 
strategy wait reward actions taken result result bad 
ongoing tasks di cult know require great deal memory 
insights value iteration adjust estimated value state kaelbling littman moore ahc rl architecture adaptive heuristic critic 
immediate reward estimated value state 
class algorithms known temporal di erence methods sutton :10.1.1.132.7760
consider di erent temporal di erence learning strategies discounted nite horizon model 
adaptive heuristic critic td adaptive heuristic critic algorithm adaptive version policy iteration barto sutton anderson value function computation longer implemented solving set linear equations computed algorithm called td 
block diagram approach 
consists components critic labeled ahc reinforcement learning component labeled rl 
williams baird explored convergence properties class ahc related algorithms call incremental variants policy iteration williams baird 
remains explain critic learn value policy 
de ne hs experience tuple summarizing single transition environment 
agent state transition choice action instantaneous reward receives resulting state 
value policy learned sutton td algorithm sutton uses update rule state visited estimated value updated closer instantaneous reward received estimated value occurring state :10.1.1.132.7760
analogous sample backup rule value iteration di erence sample drawn real world simulating known model 
key idea sample value reinforcement learning survey correct incorporates real learning rate adjusted properly slowly decreased policy held xed td guaranteed converge optimal value function 
td rule really instance general class algorithms called td 
td looks step ahead adjusting value estimates eventually arrive correct answer take quite 
general td rule similar td rule applied state eligibility just immediately previous state version eligibility trace de ned tx sk sk ifs sk eligibility degree visited past reinforcement received update states visited eligibility 
equivalent 
roughly equivalent updating states number times visited run 
note update eligibility online follows current state computationally expensive execute general td converges considerably faster large dayan dayan sejnowski 
making updates cient changing de nition td consistent certainty equivalent method singh sutton discussed section :10.1.1.51.4764
learning components ahc accomplished uni ed manner watkins learning algorithm watkins watkins dayan 
learning typically easier implement 
order understand learning additional notation 
expected discounted reinforcement action state continuing choosing actions optimally 
dyna prioritized sweeping permitted take backups transition 
prioritized sweeping priority queue emptied backups 
kaelbling littman moore prioritized sweeping queue dyna dyna great improvement previous methods su ers relatively undirected 
particularly goal just reached agent dead continues update random state action pairs concentrating interesting parts state space 
problems addressed prioritized sweeping moore atkeson queue dyna peng williams independently developed similar techniques :10.1.1.134.8196
describe prioritized sweeping detail 
algorithm similar dyna updates longer chosen random values associated states value iteration state action pairs learning 
appropriate choices store additional information model 
state remembers predecessors states non zero transition probability action 
issues discussed boyan moore give simple examples value function errors growing arbitrarily large generalization value iteration 
solution applicable certain classes problems divergence permitting updates estimated values shown near optimal battery monte carlo experiments 
thrun schwartz function approximation value functions dangerous errors value functions due generalization compounded max operator de nition value function 
results gordon tsitsiklis van roy show appropriate choice function approximator guarantee convergence necessarily optimal values 
baird residual gradient technique baird provides guaranteed convergence locally optimal solutions :10.1.1.48.3256
counter examples misplaced 
boyan moore report counter examples problem speci hand tuning despite unreliability algorithms provably converge discrete domains 
sutton shows modi ed versions boyan moore examples converge successfully 
open question general principles ideally supported theory help understand value function approximation succeed 
open question general principles ideally supported theory help understand value function approximation succeed 
sutton com reinforcement learning survey experiments boyan moore counter examples changes aspects experiments 
small changes task speci cations 

di erent kind function approximator cmac albus weak generalization :10.1.1.128.457

di erent learning algorithm sarsa value iteration 

di erent training regime 
maes brooks architecture individual behaviors xed priori gating function learned reinforcement 
mahadevan connell dual approach xed gating function supplied reinforcement functions individual behaviors learned 
lin dorigo colombetti approach rst training behaviors training gating function 
hierarchical learning methods cast framework 
feudal learning feudal learning dayan hinton watkins involves hierarchy modules :10.1.1.19.8208
simplest case high level master low level slave 
master receives reinforcement external environment 
actions consist commands kaelbling littman moore structure behaviors 
give low level learner 
small markov requirement handled learning possible construct simple environments cause learning oscillate chrisman kaelbling littman moore littman 
possible model approach act policy gather statistics transitions observations solve optimal policy observations 
unfortunately environment isnot markovian transition probabilities depend policy executed new policy induce new set transition probabilities 
approach may yield plausible results cases guarantees 
reasonable ask optimal policy mapping observations actions case np hard littman nd mapping best mapping poor performance :10.1.1.135.717
case agent trying get printer instance deterministic state free policy takes nite number steps reach goal average 
state free stochastic policies improvement gained considering stochastic policies mappings observations probability distributions actions 
randomness agent actions get stuck hall forever 
jaakkola singh jordan developed algorithm nding locally optimal stochastic policies nding globally optimal policy np hard 
dorigo colombetti applied classi er systems moderately complex problem learning robot behavior immediate reinforcement dorigo dorigo colombetti 
finite history window approach way restore markov property allow decisions history observations actions 
lin mitchell xed width nite history window learn pole balancing task 
mccallum describes utile su memory learns variable width window serves simultaneously model environment nite memory policy 
system excellent results complex driving simulation domain mccallum :10.1.1.54.132
ring neural network approach uses variable history window adding history necessary disambiguate situations 
pomdp approach strategy consists hidden markov model hmm techniques learn model environment including hidden state model construct perfect memory controller cassandra kaelbling littman :10.1.1.135.717
chrisman showed forward backward algorithm learning hmms adapted learning pomdps 
mccallum gave heuristic rules attempt learn smallest possible model environment 
lin mitchell xed width nite history window learn pole balancing task 
mccallum describes utile su memory learns variable width window serves simultaneously model environment nite memory policy 
system excellent results complex driving simulation domain mccallum :10.1.1.54.132
ring neural network approach uses variable history window adding history necessary disambiguate situations 
pomdp approach strategy consists hidden markov model hmm techniques learn model environment including hidden state model construct perfect memory controller cassandra kaelbling littman :10.1.1.135.717
chrisman showed forward backward algorithm learning hmms adapted learning pomdps 
mccallum gave heuristic rules attempt learn smallest possible model environment 
resulting model integrate information agent observations order decisions 
illustrates basic structure perfect memory controller 
left problem nding policy mapping belief states action 
problem formulated mdp di cult solve techniques described earlier input space continuous 
chrisman approach take account uncertainty yields policy small amount computation 
standard approach operations research literature solve kaelbling littman moore optimal policy close approximation thereof representation convex function belief space 
method computationally intractable may serve inspiration methods approximations cassandra littman cassandra kaelbling :10.1.1.41.1514

reinforcement learning applications reason reinforcement learning popular serves theoretical tool studying principles agents learning act 
anumber researchers practical computational tool constructing autonomous systems improve experience 
applications ranged robotics industrial manufacturing combinatorial search problems computer game playing 
prior knowledge build system algorithms capable knowledge 
examine set practical applications reinforcement learning bearing questions mind 
game playing game playing dominated arti cial intelligence world problem domain eld born 
player games established reinforcement learning framework optimality criterion games maximizing reward face xed environment maximizing reward optimal adversary minimax 
reinforcement learning algorithms adapted general class games littman researchers reinforcement learning environments :10.1.1.135.717
application far ahead time samuel checkers playing system samuel 
learned value function represented linear function approximator employed training scheme similar updates value iteration temporal di erences learning 
tesauro applied temporal di erence algorithm backgammon 
backgammon approximately states making table reinforcement learning impossible 
robot learned policy independently explicit communication 
thirdly state space quantized small number discrete states values small number pre programmed boolean features underlying sensors 
performance learned policies simple hand crafted controller job 

learning elevator dispatching task crites barto :10.1.1.17.5519
problem implemented simulation stage involved elevators servicing 
objective minimize average squared wait time passengers discounted time 
problem posed discrete markov system states simpli ed version problem 
crites barto neural networks function approximation provided excellent comparison study learning approach popular sophisticated elevator dispatching algorithms 
