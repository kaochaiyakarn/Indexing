multi agent reinforcement learning weighting partitioning ron sun todd peterson university alabama department computer science april contact author ron sun university alabama department computer science 
fax 
phone 
email cs ua edu 
running title multi agent reinforcement learning conducted author funded office naval research supplemental research university alabama included research second author 
anonymous reviewers comments 
analyzes reinforcement learning tasks different ways partitioning task agents selectively partitioning 
analysis heuristic methods described experimentally tested 
find line heuristic methods performed best significantly better single agent models 
keywords weighting averaging neural networks partitioning gating reinforcement learning multiple agents problems lieu single agent 
goal complex learning task easier achieve better performance whitehead combining outcomes multiple agents see breiman schapire wolpert jacobs jordan jacobs :10.1.1.133.8090
existing literature combination done various ways accordance problem characteristics classification problems voting numerical prediction problems averaging 
combining outcomes weighting adopted agent carries different weight krogh tresp breiman 
goal sure combination optimal sense 
furthermore weighting need done uniformly problem space tresp uniform weighting may disadvantageous differing characteristics different regions problem space case agent may perform particular region space ideally weighted region 
existing proposals related involved tasks reinforcement learning rl learning autonomous agents basis simple feedback environment limited singh dayan hinton dorigo gambardella dietterich discussions 
need exploration multi agent approaches tasks reinforcement learning 
main aims uniform perspective various multi agent approaches including weighting partitioning mentioned earlier reinforcement learning new methods motivated developed light perspective 
remainder analyze weighting show optimality weighting schemes 
relate partitioning mixture expert gating jacobs boosting freund schapire decision trees mccallum chrisman address issues concerning optimality :10.1.1.133.1040
light analyses proceed propose new ways achieving optimal partitioning optimal weighting 
experiments new methods discussed 
advantage methods lies fact help learning task easier manageable achieve better performance whitehead require little priori domain specific knowledge existing approaches require detailed domain knowledge initialize partitioning case knowledge rbf networks tresp kubat priori partitioning problem space singh priori domain specific division subsequences case gating reinforcement learners priori determination domain specific goal subgoal structures hierarchical rl approaches dayan hinton dietterich 
methods suitable incremental reinforcement learning changes domain structures characteristics occur involve priori structuring tends changes easy 
suggestions regarding point see breiman 
note avg output averaging 
means average error individual agent greater equal error combined outcome averaging agents 
scheme simple averaging weights agents identical 
may vary weights hope getting better results weighted averaging suggested wolpert breiman :10.1.1.133.8090
sure differential weighting scheme beneficial need show avg gamma gamma weighting scheme reduces error weights subject constraints 
direct way guarantee minimize gradient descent error error gamma weights subject constraints 
obviously minimization guaranteed needed inequality holds total error reduced derived earlier avg gamma gamma alternative way optimization common minimize error error gamma weights subject constraints 
shown krogh gamma gamma gamma minimizing criterion adjusting combination weights equivalent minimizing sum squared error combined outcome term earlier approach plus minimizing weighted averages variances agents second term 
discussed context reinforcement learning different ways constitute different algorithms obtaining weights weighted averaging performing gradient descent gamma performing gradient descent gamma deltaw gamma usual hope estimated errors individual averaged agent indicative true errors 
deltaw gamma ensuring combined outcome better individual agent average ways go ensure combination weights optimal sense minimizing weighted errors expressed forms 
issue closely related weighting diversity 
choosing diverse set agents uncorrelated agents opposed set identical highly similar agents averaging weighted averaging schemes justified theoretically basis bias variance decomposition see breiman ueda nakano intrator 
heuristics creating independent agents embedded number known approaches bagging diversity achieved repeated random re sampling training data set unstable easily varied agents breiman boosting diversity achieved repeated resampling changing sampling probabilities favor data points misclassified freund schapire drucker :10.1.1.133.1040
certainly combination functions linear combination equally applicable 
example averaging weighted averaging idea stacking proposed wolpert see breiman 
weighted averaging outcomes different agents arbitrarily complex combination functions adopted allow flexible combinations outcomes backpropagation network combining outcomes agents trained gradient descent usual backpropagation networks cross validation errors 
due complexity combination methods harder ensure accuracy convergence optima simple averaging weighted averaging 
look error function discussed section error gamma case encourage binarization 
kg sk sk gamma error sk gamma sk gamma sk resulting optimization problem basically optimizing partitioning optimizing agents simultaneously 
error function variation follows jacobs error gamma log gamma gammaa derivation leads case 
apply methods rl 
second boosting freund schapire way viewed soft partitioning input space :10.1.1.133.1040
boosting agent focuses different regions input space due fact iteration boosting different inputs weighted differently create focus particular region broadly defined input space 
case gating soft partitioning combination weights fixed respect agents function inputs 
formally iteration instances sampled sampling weight distribution train agent weak learner freund schapire 
starting uniform distribution sampling weights training instances instances weights iteration modified performance current agent current iteration ffl gammaffl instance correctly classified iteration agent number ffl total error rate current agent ffl algorithm terminated 
reinforcement learning difficult due things complex value functions large state spaces result complex real world scenarios 
function approximation value functions accuracy approximation complexity learning seriously affected complexity lack smoothness value functions approximated discussed boyan moore 
need find ways reduce complexity value functions 
partitioning reinforcement learning task way help reduce complexity improve learning terms speeding learning 
discuss possibilities partitioning reinforcement learning tasks synthesizing various existing proposals breiman wolpert jacobs singh new possibilities ffl partition input space state space different inputs located different regions state input space handled different agents gating model see jacobs jordan jacobs :10.1.1.133.8090
ffl partition sequence subsequence handled different agent schmidhuber singh thrun schwartz 
partitioning subsequences predetermined singh better automatically determined part reinforcement learning schmidhuber 
ffl partition actions action space deal action oriented tasks agent responsible certain limited types actions 
example dayan hinton limited agent certain actions certain level abstraction 
ffl partition outputs 
divide outputs multiple sets overlapping disjoint probabilistically deterministically 
partitioning done variety ways example set equal agents entire input space simplest way discussed section see breiman regarding bagging inputs input space partitioned partitioning outputs result partitioning inputs see jacobs see discussion methods particular outputs subsets agents see erickson 
combination outputs agents erickson involves combination exemplar nodes rule nodes gating determined exemplar nodes independent rule nodes 
involved partitioning done averaging breiman weighted averaging breiman complex methods wolpert :10.1.1.133.8090:10.1.1.133.8090
discussed previous sections mainly concerned partitioning outputs averaging weighted averaging weighting top partitioning input space sake better weighted averaging better single agent extreme case region 
basic motivation partitioning inputs outputs rl learning easier especially neural network agent 
divide conquer generally idea lead improved performance 
particular whitehead showed learning time rl dependent size state space minimum distance starting state goal state 
simplified version ga algorithms regions specified logical conjunction simple inequalities concerning input dimensions 
simple type region alternative types regions example specified radial basis functions 
linear mapping linear perceptron network specifying regions 
extend general arbitrarily shaped regions example specified backpropagation network serve gating mechanism 
extension analogous extending weighted averaging general stacking wolpert input side opposed stacking output side :10.1.1.133.8090
measure optimality partitioning algorithms resulting performance example pre set success rate criterion number steps trials reach success rate criterion transfer performance test generalization abilities guarantee optimality 
quality results region splitting algorithm dependent quality heuristics 
quality results ga algorithm relies optimality ga se 
empirical evidence literature numerous domains indicates ga weak search learning algorithm formal guarantee optimal 
relevant especially line partitioning methods chrisman mccallum 
approaches different criteria splitting adopted example differences value distribution regard different actions mccallum chrisman variance error amount error error consistency methods ratio difference sum absolute errors vs sum errors 
van der formed tree structures multi resolution hierarchies splitting error exceeds preset threshold merging training self organizing map som algorithms 
devised evolutionary divide conquer method independently developed similar ga method 
comparing weighting partitioning rarely dealt rl breiman wolpert van der jacobs jordan jacobs extends reinforcement learning tasks complex lack clear learning target :10.1.1.133.8090
fact rl moving targets changing constantly learning 
models chrisman mccallum specifically designed reinforcement learning especially learning 
comparing existing involving rl approach differences relative advantages 
approach require priori partitioning input state space done singh 
