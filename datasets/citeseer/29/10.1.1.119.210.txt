frans coetzee eric glover steve lawrence lee giles 
feature selection web applications roc powerset pruning symposium applications internet saint san diego california january ieee computer society los alamitos ca pp 

feature selection web applications roc powerset pruning frans coetzee eric glover steve lawrence lee giles nec research institute independence way princeton nj information sciences technology pennsylvania state university university park pa mail research nj nec com coetzee lawrence giles basic problem information processing selecting features ensure events accurately represented classification problems simultaneously minimizing storage processing irrelevant marginally important features 
address problem feature selection procedures perform search feature power set find smallest subset meeting performance requirements 
major restrictions existing procedures typically explicitly implicitly assume fixed operating point limited statistical structure feature power set 
algorithm produces natural method boolean representation minimal feature combinations best describe data near operating point 
representations especially appropriate describing data common text related features useful web thresholded tfidf data 
show results perform automatic boolean query modification generation distributed databases niche metasearch engines 
basic problem information processing selecting features ensure events accurately represented classification problems simultaneously minimizing number irrelevant marginally important features 
selection smallest size feature set meeting performance requirements reduces storage requirements computational complexity required ensure generalization final classifiers :10.1.1.30.3875
feature selection problem acute operating point recall precision setting user unknown 
metasearch engines example load allocation micro payment schedules bandwidth requirements result different desired operating points querying secondary sources :10.1.1.127.4459:10.1.1.41.3775
similarly storing indexing massive quantities financial news available web risk assessment exact cost risk functions users known advance 
address feature selection problem class classification applications discrete feature sets 
show results perform automatic boolean query modification generation distributed databases niche metasearch engines 
basic problem information processing selecting features ensure events accurately represented classification problems simultaneously minimizing number irrelevant marginally important features 
selection smallest size feature set meeting performance requirements reduces storage requirements computational complexity required ensure generalization final classifiers :10.1.1.30.3875
feature selection problem acute operating point recall precision setting user unknown 
metasearch engines example load allocation micro payment schedules bandwidth requirements result different desired operating points querying secondary sources :10.1.1.127.4459:10.1.1.41.3775
similarly storing indexing massive quantities financial news available web risk assessment exact cost risk functions users known advance 
address feature selection problem class classification applications discrete feature sets 
objective select parsimonious subset features meet range classification performance requirements 
contrast feature selection procedures explicitly implicitly specific operating point assumed consider optimization complete receiver operating curve roc :10.1.1.155.2293:10.1.1.43.3300:10.1.1.48.2488
metasearch engines example load allocation micro payment schedules bandwidth requirements result different desired operating points querying secondary sources :10.1.1.127.4459:10.1.1.41.3775
similarly storing indexing massive quantities financial news available web risk assessment exact cost risk functions users known advance 
address feature selection problem class classification applications discrete feature sets 
objective select parsimonious subset features meet range classification performance requirements 
contrast feature selection procedures explicitly implicitly specific operating point assumed consider optimization complete receiver operating curve roc :10.1.1.155.2293:10.1.1.43.3300:10.1.1.48.2488
receiver operating curve roc minimal complete representation classification performance subset features 
dimensional curve completely summarizes optimal tradeoff precision recall independent dimension feature space 
roc parameterized classifiers feature subset yield maximal recall level precision roc depends statistics data 
roc curve captures possible ratios priori probabilities true false class changes probabilities occur dynamic environments web flexibly compensated redesign classifiers 
second optimally efficient procedure calculating roc single subset class conditional densities long known neyman pearson np design procedure little known computing roc data finite 
third computing roc subset expensive large sets features class densities known 
elements combine feature selection remarkably challenging problem 
existing feature selection procedures ignored problem complete roc characterization implicitly assuming operating point 
approaches classification rate single classifier scalar metric kullback leibler divergence proxy performance subset :10.1.1.155.2293
excellent approach uses sophisticated notion markov blanket implementation metric entropy estimate dependence 
assumptions result significantly inferior performance different operating point cost function lack flexibility adjusting operating environment changes 
unfortunately classification performance completely summarized scalar 
obvious problem results incomplete understanding performance existing induction procedures small data sets 
example localization probability posterior probability occurs interval set operating points obtained estimating detection false alarm rates classifiers produced np design procedure define curve 
curve estimated performance curve epc 
result implies epc accurate reflection performance set classifiers sufficient number data points available irrespective number features 
result consistent general theories generalization pac learning theory 
methods palo mean level hill climbing ensure ranking classifiers takes account error estimate variation :10.1.1.52.7865
methods address problem ensuring optimal classifier set 
issue discussed 

set classifiers produced np design depends order sort likelihood estimates feature set size increases sort order corrupted sampling error sub optimal classifiers 
naive estimated operating performance curve reuses training data estimate performance estimates true performance 
increases classifiers memorize training data approaches perfect performance 
algorithm guarantee ranking classifiers subset single operating point evaluated 
especially true backward selection procedures 
case common assumption irrelevant features easily detected depends commonly recognized requirement induction algorithm returning reliable estimate performance single classifier :10.1.1.43.3300
subtle crucial point induction algorithm provide guarantee classifier structures rank different subsets removal feature removal feature optimal 
exhaustive enumeration classifier space subset completely impractical satisfies requirement arbitrarily large feature subsets 
induction methods existing feature selection methods appears meet requirement 
guarantee lesser requirement consistency ranking amount data increases guaranteed np procedure 
set size reaches fraction additional test removes subsets classification performance significantly worse obtained best subset obtained far 
test removes random label assignments consideration stages pruning improves rate convergence run 
example synthetic test case illustrate performance algorithm simple example introduced section 
add irrelevant features classes additional features uniformly distributed white variables 
problem known cause worst case performance classifier generalization :10.1.1.30.3875
roc curve subsets containing equal roc subset expect algorithm rapidly prune search space sets elements searched extra features irrelevant classification 
implemented algorithm dataset samples class samples class total features 
datasets classes split samples training samples testing respectively 
division allows estimating performance classifier test data cross validation 
interface user queries automatically augmented search terms operators effectively restrict query results relevant category database 
query modification technique especially valuable customizing existing databases integration multiple distributed databases occurs building category specific metasearch engines 
metasearch engine forwards query large number secondary search engines ranks results centrally presentation user 
adding query modifications forwarding user query secondary search engines fewer irrelevant results returned metasearch engine reduces bandwidth consumption computational load meta engine site 
advantages especially significant generation search services inquirus inquirus download web pages associated queries returned metasearch engine :10.1.1.41.7499:10.1.1.41.3775
metasearch framework important predict recall false alarm rate implied query modifier particular search engine 
information allocate server loads predict quality expected quantity results produced search engine order results user 
query modifications fact search engines support boolean search 
formally query modifications generated conjoining user query disjunction conjunctive modifiers generated feature bit strings document feature 
approach directly constructing classifiers characterizing groups features useful size features 
possible incorporate components approach sophisticated search approaches sample power set increase size feature subsets processed 
existing approaches turn benefit significantly restriction search space resulting es critical size limit exists induction approach 
construction backward selection procedures note care taken eliminate features consideration ranking performance procedure statistically warranted 
john kohavi pfleger irrelevant features subset selection problem machine learning proceedings eleventh international conference :10.1.1.30.3875
blum langley selection relevant features examples machine learning artificial intelligence pp 

gravano garcia molina tomasic effectiveness gloss text database discovery problem proc 
acm sigmod 
th international conference machine learning pp 
morgan kaufmann 
almuallim dietterich learning irrelevant features proceedings ninth national conference artificial intelligence pp 

langley sage oblivious decision trees cases working notes aaai workshop case reasoning aaai press :10.1.1.43.3300
van trees detection estimation modulation theory vol 

wiley sons 
duda hart pattern recognition scene analysis 
coetzee lawrence giles bayesian classification feature selection finite data sets sixteenth conference uncertainty artificial intelligence uai stanford ca pp 
june july 
greiner palo probabilistic hill climbing algorithm artificial intelligence vol 

kohavi feature subset selection search probabilistic estimates proc :10.1.1.52.7865
aaai fall symposium relevance 
topology course 
prentice hall 
isbn 
