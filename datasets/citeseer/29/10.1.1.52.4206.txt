maximum entropy language model integrating grams topic dependencies conversational speech recognition jun wu center language speech processing johns hopkins university baltimore md mail jhu edu compact language model incorporates local dependencies form grams long distance dependencies dynamic topic conditional constraints 
constraints integrated maximum entropy principle 
issues assigning topic test utterance investigated 
recognition results corpus showing small increase number model parameters reduction word error rate language model perplexity achieved trigram models 
analysis follows demonstrating gains larger content bearing words 
results compared obtained interpolating topic specific gram models 
