active learning natural language parsing information extraction thompson mary elaine califf raymond mooney february csli hall stanford university stanford ca csli stanford edu department applied computer science illinois state university normal il edu department computer sciences university texas austin tx mooney cs utexas edu natural language acquisition difficult gather annotated data needed supervised learning unannotated data fairly plentiful 
active learning methods attempt select annotation training informative examples potentially useful natural language applications 
existing results active learning considered standard classification tasks 
reduce annotation effort maintaining accuracy apply active learning non classification tasks natural language processing semantic parsing information extraction 
show active learning significantly reduce number annotated examples required achieve level performance complex tasks 
keywords active learning natural language learning learning parsing learning information extraction email address contact author csli stanford edu phone number contact author electronic version postscript icml ijs si active learning emerging area machine learning explores methods relying benevolent teacher random sampling actively participate collection training examples 
show active learning significantly reduce number annotated examples required achieve level performance complex tasks 
keywords active learning natural language learning learning parsing learning information extraction email address contact author csli stanford edu phone number contact author electronic version postscript icml ijs si active learning emerging area machine learning explores methods relying benevolent teacher random sampling actively participate collection training examples 
primary goal active learning reduce number supervised training examples needed achieve level performance 
active learning systems may construct examples request certain types examples determine set unsupervised examples usefully labeled 
approach selective sampling cohn atlas ladner particularly attractive natural language learning abundance text annotate informative sentences :10.1.1.119.2797
language learning tasks annotation particularly time consuming requires specifying complex output just category label reducing number training examples required greatly increase utility learning 
increasing number researchers successfully applying machine learning natural language processing see brill mooney overview 
utilized active learning addressed particular tasks part speech tagging dagan engelson text categorization lewis catlett liere tadepalli :10.1.1.30.6148
fundamentally classification tasks tasks address semantic parsing information extraction 
active learning systems may construct examples request certain types examples determine set unsupervised examples usefully labeled 
approach selective sampling cohn atlas ladner particularly attractive natural language learning abundance text annotate informative sentences :10.1.1.119.2797
language learning tasks annotation particularly time consuming requires specifying complex output just category label reducing number training examples required greatly increase utility learning 
increasing number researchers successfully applying machine learning natural language processing see brill mooney overview 
utilized active learning addressed particular tasks part speech tagging dagan engelson text categorization lewis catlett liere tadepalli :10.1.1.30.6148
fundamentally classification tasks tasks address semantic parsing information extraction 
language learning tasks require annotating natural language text complex output parse tree semantic representation filled template 
application active learning tasks requiring complex outputs studied 
research shows active learning methods applied problems demonstrates significantly decrease annotation costs important realistic natural language tasks 
section suggests directions research 
section describes related research section presents 
background active learning relative ease obtaining line text focus selective sampling methods active learning 
case learning begins small pool annotated examples large pool unannotated examples learner attempts choose informative additional examples annotation 
existing area emphasized approaches certainty methods lewis catlett committee methods freund seung shamir tishby liere tadepalli dagan engelson cohn :10.1.1.30.6148
certainty paradigm system trained small number annotated examples learn initial classifier 
system examines unannotated examples attaches predicted annotation examples 
examples lowest user annotation retraining 
methods attaching typically attempt estimate probability classifier consistent prior training data classify new example correctly 
