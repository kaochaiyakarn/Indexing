international computer science institute center st ffl suite ffl berkeley california ffl ffl fax unsupervised learning dyadic data thomas hofmann international computer science institute berkeley ca computer science division uc berkeley hofmann cs berkeley edu jan puzicha institut fur informatik iii university bonn germany jan cs uni bonn de tr december dyadic data refers domain finite sets objects observations pairs element set 
includes event occurrences histogram data single stimulus preference data special cases 
dyadic data arises naturally applications ranging computational linguistics information retrieval preference analysis computer vision 
systematic domain independent framework unsupervised learning dyadic data statistical mixture models 
approach covers different models flat hierarchical latent class structures unifies probabilistic modeling structure discovery 
mixture models provide parsimonious flexible parameterization probability distributions generalization performance sparse data structural information data inherent grouping structure 
simplest case focus sequel elementary observation consists occurrence cases may provide scalar value strength preference association rating 
cases additional observations features objects may available restrict attention case purely dyadic observations 
exemplary application areas dyadic data ffl computer vision particular context image segmentation corresponds image locations discretized categorical feature values denotes occurrence feature particular image location 
ffl text information retrieval corresponds document collection vocabulary represents occurrence token content document 
ffl computational linguistics corpus statistical analysis word occurences applications probabilistic language modeling word clustering ptl word sense disambiguation hin dlp discrimination sch automated thesaurus construction sp :10.1.1.14.1729
ffl preference analysis consumption behavior identifying individuals objects 
correspond single stimulus preferences 
type data starting point machine learning technique known collaborative filtering 
modeling goals principles different domains main tasks play fundamental role unsupervised learning dyadic data probabilistic modeling learning joint conditional probability model theta ii structure discovery identifying clusters data hierarchies 
sparseness problem urgent case higher order occurrences triplets observed gram language modeling 
typical state art techniques natural language processing apply smoothing deal zero frequencies unobserved events 
prominent techniques example katz back method kat model interpolation held data jm similarity smoothing techniques es dlp 
contrast propose model statistical approach family latent class finite mixture models mb principled approach deal data sparseness problem 
mixture clustering models dyadic data investigated titles class gram models distributional clustering ptl aggregate markov models sp natural language processing :10.1.1.14.1729
approaches recovered special cases general learning framework 
close relation clustering methods qualitative data information clustering approach cf 

modeling principle latent variable models specification joint probability distribution latent observable variables 
reflects additional induced grouping objects related models previous brown proposed information criterion class gram models language modeling 
word classes formed mutual information classes adjacent words maximized 
model closely related sided clustering model main difference word classes predicting predicted word identified 
brown proposed non probabilistic hard clustering variant introduced greedy cluster merging algorithm 
ideas pioneering pereira tishby lee ptl distributional clustering :10.1.1.14.1729
ptl aspect model mixture distribution starting point latent aspect variables introduced corresponding em algorithm derived :10.1.1.14.1729
pereira propose clustering method similar sided clustering model different principles posterior class distributions obtained minimizing cross entropy criterion equivalently kullback leibler divergence entropic regularization parameter fi results maximum entropy cluster memberships exp fi yjx log fi identification advantageous terms predictive log likelihood doubtful 
algorithmically identification poses additional problems alternation scheme proposed sided clustering model longer applies 
model yjx aspect ajx yja sided clustering pfc hierarchical clustering pfc ajx yja sided clustering pfc pfd oe table systematic overview dyadic mixture models 
word classes formed mutual information classes adjacent words maximized 
model closely related sided clustering model main difference word classes predicting predicted word identified 
brown proposed non probabilistic hard clustering variant introduced greedy cluster merging algorithm 
ideas pioneering pereira tishby lee ptl distributional clustering :10.1.1.14.1729
ptl aspect model mixture distribution starting point latent aspect variables introduced corresponding em algorithm derived :10.1.1.14.1729
pereira propose clustering method similar sided clustering model different principles posterior class distributions obtained minimizing cross entropy criterion equivalently kullback leibler divergence entropic regularization parameter fi results maximum entropy cluster memberships exp fi yjx log fi identification advantageous terms predictive log likelihood doubtful 
algorithmically identification poses additional problems alternation scheme proposed sided clustering model longer applies 
model yjx aspect ajx yja sided clustering pfc hierarchical clustering pfc ajx yja sided clustering pfc pfd oe table systematic overview dyadic mixture models 
differ step equations sided clustering model missing factor exponent class prior probabilities 
pereira propose clustering method similar sided clustering model different principles posterior class distributions obtained minimizing cross entropy criterion equivalently kullback leibler divergence entropic regularization parameter fi results maximum entropy cluster memberships exp fi yjx log fi identification advantageous terms predictive log likelihood doubtful 
algorithmically identification poses additional problems alternation scheme proposed sided clustering model longer applies 
model yjx aspect ajx yja sided clustering pfc hierarchical clustering pfc ajx yja sided clustering pfc pfd oe table systematic overview dyadic mixture models 
differ step equations sided clustering model missing factor exponent class prior probabilities 
reason objects treated equally particular irrespective number observations sample set ptl centroid condition derived maximum likelihood principle inserting maximum entropy cluster membership probabilities :10.1.1.14.1729
principles considered complementary re estimation equations iterated pseudo em scheme 
ptl remains unresolved underlying common objective function exists :10.1.1.14.1729
new formulation tp types equations derived rigorously rate theoretic mutual information principle 
offer perspectives foundation unsupervised learning maximum likelihood principle basis 
model yjx aspect ajx yja sided clustering pfc hierarchical clustering pfc ajx yja sided clustering pfc pfd oe table systematic overview dyadic mixture models 
differ step equations sided clustering model missing factor exponent class prior probabilities 
reason objects treated equally particular irrespective number observations sample set ptl centroid condition derived maximum likelihood principle inserting maximum entropy cluster membership probabilities :10.1.1.14.1729
principles considered complementary re estimation equations iterated pseudo em scheme 
ptl remains unresolved underlying common objective function exists :10.1.1.14.1729
new formulation tp types equations derived rigorously rate theoretic mutual information principle 
offer perspectives foundation unsupervised learning maximum likelihood principle basis 
saul pereira sp independently proposed model called aggregate markov model utilized back model kat context language modeling 
approach equivalent aspect model asymmetric parameterization restricted class bigram model 
approach equivalent aspect model asymmetric parameterization restricted class bigram model 
derived em algorithm employing annealing methods cf 
section 
generalizations extensions annealed em annealed em important generalization em model fitting pursues main goals ffl avoiding overfitting controlling effective model complexity ffl reducing sensitivity em local maxima ffl generating tree topologies hierarchical clustering model 
annealed em closely related deterministic annealing technique applied clustering problems including clustering rgf rgf bk pairwise clustering hb context occurrence data distributional clustering ptl :10.1.1.14.1729
key idea introduce temperature parameter replace minimization combinatorial objective function substitute known free energy 
annealing methods statistical physics 
consider general case maximum likelihood estimation em algorithm 
step definition computes posterior average complete data log likelihood maximized step 
pattern recognition 
muller 
annealed competition experts segmentation classification switching dynamics 
neural computation 
ptl pereira tishby lee :10.1.1.14.1729
distributional clustering english words 
proceedings acl pages 
pw peters walker 
iterative procedure obtaining maximum likelihood estimates parameters mixture normal distributions 
