wide area cooperative storage cfs frank dabek frans kaashoek david karger robert morris ion stoica mit laboratory computer science chord mit edu pdos lcs mit edu chord cooperative file system cfs new peer peer readonly storage system provides provable guarantees efficiency robustness load balance file storage retrieval 
cfs completely decentralized architecture scale large systems 
cfs servers provide distributed hash table dhash block storage 
cfs clients interpret dhash blocks file system 
dhash distributes caches blocks fine achieve load balance uses replication robustness decreases latency server selection 
dhash finds blocks chord location protocol operates time logarithmic number servers 
cfs implemented sfs file system toolkit runs linux openbsd freebsd 
experience globally deployed prototype shows cfs delivers data clients fast ftp 
controlled tests show cfs scalable servers looking block data involves contacting servers 
tests demonstrate nearly perfect robustness performance half servers fail 

existing peer peer systems napster gnutella freenet demonstrate benefits cooperative storage serving fault tolerance load balance ability harness idle storage network resources :10.1.1.10.4919
accompanying benefits arc number design challenges 
peer peer architecture symmetric decentralized operate unmanaged volunteer participants 
finding desired data large system fast servers able join leave system frequently affecting robustness efficiency load balanced available servers 
peer peer systems common solve problems university california berkeley research sponsored defense advanced research projects agency darpa space naval warfare systems center san diego contract 
permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page copy republish post servers redistribute lists requires prior specific permission fee 
sosp banff canada acm isbn solves 
cfs cooperative file system new design meets challenges 
cfs file system exists set blocks distributed available cfs servers 
cfs client software interprets stored blocks file system data meta data presents ordinary read file system interface applications 
core cfs software consists layers dhash chord 
dhash distributed hash layer performs block fetches client distributes blocks servers maintains cached replicated copies 
dhash uses chord distributed lookup system locate servers responsible block :10.1.1.105.3673
table summarizes cfs software layering layer fs dhash chord responsibility interprets blocks files presents file system interface applications 
stores unstructured data blocks reliably 
maintains routing tables find blocks 
chord implements hash operation maps block identifiers servers 
chord assigns server identifier drawn bit identifier space block identifiers 
identifiers thought points circle 
mapping chord implements takes block id yields block successor server id closely follows block id identifier circle 
implement mapping chord maintains server table information log servers total number servers 
chord lookup sends messages log servers consult tables 
cfs find data efficiently large number servers servers join leave system table updates 
dhash layers block management top chord 
dhash provides load balance popular large files arranging spread blocks file servers 
balance load imposed popular small files dhash caches block servers consulted chord lookups block 
dhash supports pre fetching decrease download latency 
dhash replicates block small number servers provide fault tolerance 
dhash enforces weak quotas amount data server inject deter abuse 
dhash allows control number virtual servers server provide control data server store behalf 
cfs implemented sfs toolkit 
reports experimental results small international deployment cfs servers large scale controlled test bed 
results confirm contributions cfs design aggressive approach load balance spreading file blocks randomly servers download performance internet wide prototype deployment fast standard ftp provable efficiency provably fast recovery times failure simple algorithms achieve results 
cfs operational prompt refinements design 
potential area improvement ability chord lookup algorithm tolerate malicious participants verifying routing information received servers 
area cfs currently address anonymity expected anonymity needed layered top basic cfs system 
remainder organized follows 
section discusses related 
section outlines design design goals cfs 
section describes chord location protocol 
section presents detailed design cfs 
section describes implementation details section presents experimental results 
section discusses open issues 
section summarizes findings concludes 

related cfs inspired napster gnutella particularly freenet :10.1.1.10.4919
cfs uses peer peer distributed hashing similar spirit number ongoing research projects :10.1.1.110.5867:10.1.1.111.1818:10.1.1.140.3129
comparison existing peer peer file sharing systems cfs offers simplicity implementation high performance compromising correctness 
cfs balances server load finds data quickly clients guarantees data availability face server failures high probability 
cfs complete system individual aspects common existing systems 
major relationships summarized 
naming authentication cfs authenticates data naming public keys distributed storage systems :10.1.1.20.417:10.1.1.110.5867:10.1.1.125.3017:10.1.1.115.4299
content hashes securely link different pieces data due merkle public keys name data due sfs system 
cfs adopts naming authentication file system structure ideas implements secure distributed readonly file system file system files modified owner complete replacement file 
cfs significant differences architectural mechanism levels 
defines protocols authentication mechanisms client retrieve data server 
cfs adds ability dynamically find server currently holding desired data chord location service 
increases robustness availability cfs changes set servers transparent clients 
peer peer search napster gnutella arguably widely peer peer file systems today 
keyword search interface clients retrieving uniquely identified data 
result search engines distributed hash tables trade scalability power gnutella broadcasts search queries machines napster performs searches central facility 
cfs described doesn provide search developing scalable distributed search engine cfs 
mojo nation broadcast query peer peer storage system divides files blocks uses secret sharing algorithm distribute blocks number hosts 
cfs divides files blocks secret sharing 
anonymous storage freenet uses probabilistic routing preserve anonymity clients publishers servers 
anonymity requirement limits freenet reliability performance 
freenet avoids associating document predictable server avoids forming globally coherent topology servers 
means unpopular documents may simply disappear system server responsibility maintaining replicas 
means search may need visit large fraction freenet network 
example hong shows network servers lookup path length exceed hops 
means hop count limited lookup may fail document available 
cfs try provide anonymity guarantee tighter bounds lookup cost example node system lookups essentially exceed hops 
cfs caching scheme similar freenet sense leave cached copies data query path client data 
cfs finds data significantly fewer hops freenet cfs structured lookup paths overlap freenet cfs better quantity cache space 
freenet publius focuses anonymity achieves encryption secret sharing routing :10.1.1.125.3017
publius requires static globally known list servers stores share fixed location predictable file name 
free haven uses cryptography routing re provide anonymity gnutella free haven finds data global search 
cfs attempt provide anonymity focusing efficiency robustness 
believe intertwining anonymity basic data lookup mechanism interferes correctness performance 
hand robust location storage layer anonymous client access cfs provided separate anonymizing proxies techniques similar proposed chaum reiter rubin 
peer peer hash systems cfs layers storage top eff cient distributed hash lookup algorithm 
number peer peer systems similar approaches similar scalability performance including past oceanstore :10.1.1.110.5867:10.1.1.111.1818:10.1.1.115.4299
detailed comparison algorithms :10.1.1.105.3673
past storage system differs cfs approach load balance 
past server stores files server disk space store large file system sufficient free space 
past server solves offloading files responsible servers spare disk space 
past handles load serving popular files caching lookup path 
cfs stores blocks files spreads blocks evenly available servers prevents large files causing unbalanced storage 
cfs solves related problem different servers having different amounts storage space mechanism called virtual servers gives server managers control disk space consumption 
cfs block storage helps handle load serving popular large files serving load spread servers blocks 
dhash dhash dhash directory inode data block block block rtt root block oy 

cfs client cfs cfs server cfs software structure 
vertical links local apis horizontal links rpc apis 
space efficient large files file caching 
cfs relies caching files small distributing blocks effective 
evaluating performance impact block storage granularity purposes 
oceanstore aims build global persistent storage utility 
provides data privacy allows client updates guarantees durable storage 
features come price complexity 
example oceanstore uses byzantine agreement protocol conflict resolution complex protocol plaxton trees implement location service 
oceanstore assumes core system maintained commercial providers 
uses consistent hashing map files keyword queries servers freenet routing algorithm locate files 
result shares weaknesses freenet 
web caches content distribution networks cdns handle high demand data distributing replicas multiple servers 
cdns typically managed central entity cfs built resources shared owned cooperative group users 
proposed scalable cooperative web caches 
locate data systems multicast queries require servers know servers 
result proposed methods highly scalable robust 
addition load balance hard achieve content cache depends heavily query pattern 
cache resolver cfs uses consistent hashing evenly map stored data servers :10.1.1.147.1879
cache resolver assumes clients know entire set servers maintaining date server list difficult large peer peer system servers join depart unpredictable times 

design overview cfs provides distributed read file storage 
structured collection servers provide block level storage 
publishers producers data clients consumers data layer file system semantics top block store ordinary file system layered top disk 
unrelated publishers may store separate file systems single cfs system cfs design intended support possibility single world wide system consisting millions servers 
system structure illustrates structure cfs software 
cfs client contains software layers file system client dhash simple cfs file system structure example 
root block identified public key signed corresponding private key 
blocks identified cryptographic hashes contents 
storage layer chord lookup layer 
client file system uses dhash layer retrieve blocks 
client dhash layer uses client chord layer locate servers hold desired blocks 
cfs server software layers dhash storage layer chord layer 
server dhash layer responsible storing keyed blocks maintaining proper levels replication servers come go caching popular blocks 
server dhash chord layers interact order integrate looking block identifier checking cached copies block 
cfs servers oblivious file system semantics simply provide distributed block store 
cfs clients interpret dhash blocks file system format adopted format similar unix file system uses dhash blocks block identifiers place disk blocks disk addresses 
shown block piece file piece file system meta data directory 
maximum size block order tens kilobytes 
parent block contains identifiers children 
publisher inserts file system blocks cfs system hash block content content hash identifier 
publisher signs root block private key inserts root block cfs corresponding public key root block identifier 
clients name file system public key check integrity root block key integrity blocks lower tree content hash identifiers refer blocks 
approach guarantees clients see authentic internally consistent view file system circumstances client may see old version updated file system 
cfs file system read far clients concerned 
file system may updated publisher 
involves updating file system root block place point new data 
cfs authenticates updates root blocks checking new block signed key old block 
timestamp prevents replays old updates 
cfs allows file systems updated changing root block identifier external data need changed data updated 
cfs stores data agreed finite interval 
publishers want indefinite storage periods periodically ask cfs extension cfs server may discard data guaranteed period expired 
cfs explicit delete operation publisher simply asking extensions 
area replication caching policies cfs relies assumption large amounts spare disk space available 
cfs properties cfs provides consistency integrity file systems adopting file system format 
cfs extends ing desirable desirable properties decentralized control 
cfs servers need share administrative relationship publishers 
cfs servers ordinary internet hosts owners volunteer spare storage network resources 
scalability 
cfs lookup operations space messages logarithmic number servers 
availability 
client retrieve data long trapped small partition underlying network long data replicas reachable underlying network 
true servers constantly joining leaving cfs system 
cfs places replicas servers unrelated network locations ensure independent failure 
load balance 
cfs ensures burden storing serving data divided servers rough proportion capacity 
maintains load balance data far popular combination caching spreading file data servers 
persistence 
cfs commits storing data keeps available agreed interval 
quotas 
cfs limits amount data particular ip address insert system 
provides degree protection malicious attempts exhaust system storage 
efficiency 
clients fetch cfs data delay comparable fte due cfs efficient lookup algorithms caching pre fetching server selection 
sections chord dhash provide properties 

chord layer cfs uses chord protocol locate blocks :10.1.1.105.3673
chord supports just operation key determine node responsible key 
chord store keys values provides primitives allow higher layer software build wide variety storage systems cfs chord primitive 
rest section summarizes chord describes new algorithms robustness server selection support applications cfs 
consistent hashing chord node unique bit node identifier id obtained hashing node ip address virtual node index 
chord views ids occupying circular identifier space 
keys mapped id space hashing bit key ids 
chord defines node responsible key successor key id successor id node smallest id greater equal wrap consistent hashing :10.1.1.147.1879
consistent hashing lets nodes enter leave network minimal movement keys 
maintain correct successor mappings node joins network certain keys previously assigned successor assigned node leaves network ofn assigned keys reassigned successor 
changes assignment keys nodes need occur 
consistent hashing straightforward implement constant time lookups nodes date list nodes 
system scale chord provides scalable distributed version consistent hashing 
chord lookup algorithm chord node uses data structures perform lookups successor list finger table 
successor list required correctness chord careful maintain accuracy 
finger table accelerates lookups need accurate chord aggressive maintaining 
discussion describes perform correct slow successor list describes accelerate finger table 
discussion assumes malicious participants chord protocol believe possible nodes verify routing information chord participants send algorithms left 
chord node maintains list identities ip addresses immediate successors chord ring 
fact node knows successor means node process lookup correctly desired key node successor node key successor lookup forwarded successor moves lookup strictly closer destination 
new node learns successors joins chord ring asking existing node perform lookup successor asks successor successor list 
entries list provide fault tolerance node immediate successor respond node substitute second entry successor list 
successors simultaneously fail order disrupt chord ring event improbable modest values implementation fixed chosen log foreseeable maximum number nodes main complexity involved successor lists notifying existing node new node successor 
stabilization procedure described way guarantees preserve connectivity chord ring successor pointers :10.1.1.105.3673:10.1.1.105.3673
lookups performed successor lists require average message exchanges number servers 
reduce number messages required log node maintains finger table table entries 
th entry table node contains identity node succeeds id circle 
node knows identities nodes power intervals id circle position 
new node initializes finger table querying existing node 
existing nodes finger table successor list entries refer new node find periodic lookups 
shows pseudo code look successor identifier id main loop find predecessor sends preceding node list rpcs succession nodes rpc searches tables node nodes closer id finger table entries point nodes power oftwo intervals id ring iteration set node halfway id ring current id preceding node list returns id greater id process overshoot correct successor 
may shoot especially new node joined id just id case check id successor ensures ask node find id successor finds id predecessor asks predecessor successor 
successor id find predecessor id return successor ask node find id predecessor 
predecessor id id nt preceding node list id maxn return ask node list nodes finger table successor list precede id preceding node list id return fingers li successors mid pseudo code find successor node identifier id remote procedure calls preceded remote node 
find predecessor persists finds pair nodes straddle id aspects lookup algorithm robust 
rpc preceding node list node rt returns list nodes believes desired id progress successor unresponsive lookup fall 
second loop ensures find predecessor keep trying long find node closer id long nodes careful maintain correct successor pointers find predecessor eventually succeed 
usual case nodes correct finger table information iteration loop eliminates half remaining distance target 
means hops early lookup travel long distances id space hops travel small distances 
efficacy caching mechanism described section depends observation 
theorems proved accompanying technical report show success performance chord lookups affected massive simultaneous failures 
theorems assume successor list length log 
chord ring stable node successor list correct 
theorem 
network initially stable node fails probability high probability find successor returns closest living successor query key 
theorem 
network initially stable node fails probability expected time execute find successor log 
evaluation section validates theorems experimentally 
server selection chord reduces lookup latency preferentially contacting nodes nearby underlying network 
server selection added chord part cfs part chord originally published 
step find predecessor id node doing lookup choose hop set nodes 
initially set contents routing tables subsequently set list nodes returned preceding node list rpc previous hop 
node tells measured latency node set collected latencies acquired finger table entries 
different choices hop node take query different distances id ring impose different rpc latencies calculation seeks pick best combination 
chord estimates total cost ni node set potential hops ni di ni ones id logn ni estimate number chord hops remain contacting ni 
node estimate total number chord nodes system density nodes nearby id ring 
log estimate number significant high bits id id successor agree bits significant bits 
ni id log yields just significant bits id space distance ni target key id ones function counts bits set difference approximately number chord hops ni id multiplying average latency rpcs node issued estimates time take send rpcs hops 
adding di latency node ni reported node produces complete cost estimate 
chord uses node minimum ni hop 
benefit server selection method extra measurements necessary decide node closest decisions latencies observed building finger tables 
nodes rely latency measurements taken nodes 
works low latencies nodes mean latency low measurements internet test bed suggest true 
node id authentication chord nodes arbitrary ids attacker destroy chosen data choosing node id just dam id control successor attacker node effectively delete block denying block existed 
limit opportunity attack chord node id form sha hash function node ip address concatenated virtual node index 
virtual node index fall small maximum 
result node easily control choice chord id new node joins system existing nodes may decide add finger tables 
part process existing node sends message claimed ip address containing nonce 
node ip address admits having id claimed ip address virtual node index hash id existing node accepts defense place attacker control roughly ip addresses total nodes chord system order chance targeting arbitrary blocks 
owners large blocks ip address space tend easily identifiable malicious individuals 

dhash layer cfs dhash layer stores retrieves uniquely identified blocks handles distribution replication caching blocks 
dhash uses chord help locate blocks 
dhash reflects key cfs design decision split file system file blocks distribute blocks servers 
arrangement balances load serving popular files servers 
increases number messages required fetch file client look block separately 
network bandwidth consumed lookup small compared bandwidth required deliver block 
addition cfs hides block lookup latency prefetching blocks 
systems freenet past store files :10.1.1.10.4919
results lower lookup costs cfs lookup file block requires achieve load balance 
servers unlucky responsible storing large files may run disk space system sufficient free space 
balancing load serving files typically involves adaptive caching 
may awkward large files popular file stored entirety caching server 
dhash uses caching depends small files 
dhash block granularity particularly suited serving large popular files software distributions 
example server system file small megabytes produce reasonably balanced serving load kbyte blocks 
system balances load caching files require case times total storage achieve load balance 
hand dhash efficient file scheme large unpopular files experiments section show provide competitive download speeds 
dhash block granularity affect better worse performance load balance small files 
files dhash depends caching server selection block replicas described section 
table shows api dhash layer exposes 
cfs file system client layer uses gee implement application requests open files read files navigate directories publishers data special application inserts updates cfs file system put put calls 
replication dhash replicates block cfs servers increase availability maintains replicas automatically servers come go places replicas way clients easily find 
dhash places block replicas servers immediately block successor chord ring see 
dhash easily find identities servers chord entry successor list 
cfs configured placement replicas means block successor server fails block immediately available block new successor 
dhash software block successor server manages replication block making sure successor servers copy block times 
successor server fails block new successor assumes responsibility block 
value replication scheme depends part independence failure unreachability block replica servers 
servers close id ring physically close server id hash ip address 
provides desired independence failure 
placement example block replicas cached copies chord identifier ring 
block id shown tick mark 
block stored successor id server denoted square 
block replicated successor immediate successors circles 
hops typical lookup path block shown arrows block cached servers lookup path triangles 
cfs save space storing coded pieces blocks block replicas algorithm ida 
cfs doesn coding storage space expected highly constrained resource 
placement block replicas easy client select replica fastest download 
result chord lookup block id identity server immediately precedes id client asks predecessor successor list include identities servers holding replicas block id latency measurements predecessor servers 
client fetches block replica lowest reported latency 
chord server selection described section approach works best proximity underlying network transitive 
fetching lowest latency replica side effect spreading load serving block replicas 
caching dhash caches blocks avoid overloading servers hold popular data 
dhash layer sets aside fixed amount disk storage cache 
cfs client looks block key performs chord lookup visiting intermediate cfs servers ids successively closer key successor see 
step client asks intermediate server desired block cached 
eventually client arrives key successor intermediate server cached copy 
client sends copy block servers contacted lookup path 
versions dhash nd copy just second server path traversed lookup reduce amount network traffic required lookup significantly decreasing caching effectiveness 
chord lookup takes shorter shorter hops id space gets closer target lookups different clients block tend visit servers late lookup 
result policy caching blocks lookup path effective 
dhash replaces cached blocks order 
copies block servers ids far block function put block put block pubkey get key description computes block key hashing contents sends key successor server storage 
stores updates signed block root blocks 
block signed public key 
block chord key hash pubkey 
fetches returns block associated specified chord key 
table dhash client api exposed client file system software 
sor discarded clients stumble 
effect preserving cached copies close successor expands contracts degree caching block popularity 
caching replication conceptually similar dhash provides distinct mechanisms 
dhash stores replicas predictable places ensure replicas exist 
contrast number cached copies easily counted fall zero 
fault tolerance achieved solely cached copies unpopular block simply disappear cached copy 
cfs avoids cache consistency problems blocks keyed content hashes 
root blocks public keys identifiers publisher change root block inserting new signed corresponding private key 
means cached root blocks may stale causing clients see old internally consistent file system 
client check freshness cached root block decide look newer version 
non root blocks longer eventually eliminated caches lru replacement 
load balance dhash spreads blocks evenly id space content hash function uniformly distributes block ids 
cfs server id fact ids uniformly distributed mean server carry roughly storage burden 
desirable different servers may different storage network capacities 
addition uniform distribution doesn produce perfect load balance maximum storage burden log times average due irregular spacing server ids :10.1.1.147.1879
accommodate heterogeneous server capacities cfs uses notion real server acting multiple virtual servers :10.1.1.147.1879
cfs protocol operates virtual server level 
virtual server uses chord id derived hashing real server ip address index virtual server real server 
cfs server administrator configures server number virtual servers rough proportion server storage network capacity 
number adjusted time time reflect observed load levels 
virtual servers potentially increase number hops chord lookup 
cfs avoids expense allowing virtual servers physical server examine tables fact virtual servers take short cuts routing tables exactly compensates increased number servers 
cfs potentially vary number virtual servers real server adaptively current load 
high load real server delete virtual servers low load server create additional virtual servers 
algorithm need designed stability high load 
server overloaded cfs system overloaded automatically deleting virtual servers cause cascade deletions 
quotas damaging technical form abuse cfs encounter malicious injection large quantities data 
aim attack disk space cfs servers leaving available legitimate data 
user cause kind problem accident 
ideally cfs impose publisher quotas reliable identification publishers done past system 
reliable identification usually requires form centralized administration certificate authority 
decentralized approximation cfs bases quotas ip address publisher 
example cfs server limits ip address storage attacker mount attack machines successful 
mechanism limits storage legitimate publisher just assuming publisher uses just ip address 
limit easy subvert simple forging ip addresses cfs servers require publishers respond confirmation request includes random nonce described section 
approach weaker requires publishers unforgeable identities requires centralized administrative mechanisms 
cfs server imposes fixed lp address quota total amount storage ip address consume grow linearly total number cfs servers 
may prove desirable enforce fixed quota total storage require quota imposed server decrease proportion total number servers 
adaptive limit form possible estimate total number servers chord software maintains 
updates deletion cfs allows updates way allows publisher file system modify 
cfs server accept request store block conditions 
block marked content hash block server accept block supplied key equal sha hash block content 
block marked signed block block signed public key sha hash block cfs key 
low probability finding blocks sha hash prevents attacker changing block associated content hash key explicit protection required file system blocks 
sensitive block file system root block signed safety depends publisher avoiding disclosure private key 
cfs support explicit delete operation 
publishers periodically refresh blocks wish cfs continue store 
cfs server may delete blocks refreshed 
benefit cfs implicit deletion automatically recovers malicious insertions large quantities data 
attacker stops inserting refreshing data cfs gradually delete 

implementation cfs implemented lines including line chord implementation 
consists number separate programs run user level 
programs communicate udp rpc package provided sfs toolkit 
busy cfs server may exchange short messages large number servers making overhead tcp connection setup unattractive compared udp 
internal structure program asynchronous events callbacks threads 
software layer implemented library interface 
cfs runs linux openbsd freebsd 
chord implementation chord library maintains routing tables described section 
exports tables dhash layer implements integrated version chord lookup algorithm 
implementation uses sha cryptographic hash function produce cfs block identifiers block contents 
means block server identifiers bits wide 
chord implementation maintains running estimate total number chord servers server selection algorithm described section 
server computes fraction id ring nodes successor list cover fraction estimated total number servers system dhash implementation dhash implemented library depends chord 
dhash instance associated chord virtual server communicates virtual server function call interface 
dhash instances different servers communicate rpc 
dhash implementation chord lookup algorithm relies chord layer maintain routing tables 
integrating block lookup dhash increases efficiency 
dhash called chord find successor routine awkward dhash check server lookup path cached copies desired block 
cost unneeded round trip time chord dhash separately contacting block successor server 
pseudo code dhash implementation lookup key shown dhash version chord code shown 
function lookup key returns data associated key error 
function lookup operates repeatedly invoking remote procedure lookup step key possible values 
called server stores caches data associated key step key returns data 
store data step key returns closest predecessor key determined consulting local routing tables 
lookup step returns error true successor key store associated data 
lookup tries contact failed server rpc machinery return rpc failure 
function lookup backtracks previously contacted server tells failed server alert asks best predecessor 
point lookup key contacted pair servers side rpc handler server returns block id key best server talk 
lookup step key key stored cached replicated return complete key key predecessor return nonexistent key myid live successor hop live successor find highest server key finger table successor list 
hop lookup pred key list fingers successors hop return continue hop succ list rpc handler ask chord software delete server finger list successor list 
alert id return block associated key error 
runs server invokes lookup 
lookup key push stack accumulate path 
status res lookup step key repeat status complete return res status continue res hop top tip top knew server return nonexistent key top top top block 
return nonexistent explore hop push res hop status res res hop lookup step key status rpc failure try previous hop 
failed pop top alert failed status res lookup step key return nonexistent procedure dhash locate block 
key 
server failures second server pair may key original successor 
second server live successor hold replica key assuming replicas failed 
pseudo code show virtual servers physical server look routing tables block stores 
allows lookups progress faster ring increases chances encountering cached block 
client implementation cfs client software layers file system top dhash 
cfs exports ordinary unix file system interface acting local nfs server sfs user level file system toolkit 
cfs client runs machine cfs server client communicates local server unix domain socket uses proxy send queries non local cfs servers 
dhash back sufficiently flexible support number different client interfaces 
instance currently implementing client acts web proxy order layer name space top name space world wide web 

experimental results order demonstrate practicality cfs design sets tests 
explores cfs performance modest number servers distributed internet focuses real world client perceived performance 
second involves larger numbers servers running single machine focuses scalability robustness 
quotas section implemented tested software 
cryptographic verification updates section server id authentication section implemented enabled 
effect results 
noted tests run caching turned replication just virtual server physical server server selection turned 
defaults allow effects features individually illustrated 
experiments involve block level dhash operations file system client software driving experiments fetches file fetching specified list block identifiers 
server maintains successor list entries mentioned section help maintain ring connectivity 
cfs automatically adjust successor list length match number servers robustness sensitive exact value 
real life tests described section cfs servers running testbed machines scattered internet 
machines part ron testbed sites spread united states netherlands sweden south korea 
servers held megabyte cfs file split blocks 
test download speed client software machine fetched entire file 
machines fetched file time 
rpcs average required fetch block experiments 
client software uses pre fetch overlap lookup fetching blocks 
client initially issues window number parallel block fetches fetch completes client starts new 
shows average download speeds range prefetch window sizes server selection 
block fetch server selection averages milliseconds explains download speed kbytes second fetching kbyte block time 
increasing amount pre fetch increases speed example fetching blocks time yields average speed kbytes second 
large amounts pre fetch counter productive client server network connection 
server selection increases download speeds substantially small amounts pre fetch doubling download speed pre fetch 
improvement dramatic larger amounts pre fetch partially concentrating block fetches just nearest servers may overload servers links 
data shown obtained flawed version server selection algorithm correct algorithm yield better download speeds 
shows distribution speeds seen downloads different machines different pre fetch windows server selection 
distributions speeds server selection fairly narrow download require blocks server downloads see similar mix block times 
best download speeds machine new york university connections multiple server selection server selection 



prefetch window kbytes download speeds achieved fetching file cfs internet testbed range pre fetch window sizes 
point average times seen testbed machine 
curve includes server selection 
kbytes nos kbytes kbytes kbytes kbytes 
rs speed bytes second cumulative distribution download speeds plotted various pre fetch windows 
results server selection marked respectively 
kbytes 
kbytes 
kbytes speed kbytes sec distribution download speeds achieved ordinary tcp pair hosts internet testbed file sizes 
backbones worst download speeds small pre fetch windows sites outside united states high latency servers 
worst speeds large amounts pre fetch fetches cable modem sites united states limited link capacity 
speed distributions server selection skewed 
time server selection improves download speeds modest amount 
improves substantially usually downloads initiated connected sites 
server selection download speeds worse usually downloads initiated sites outside united states 
show cfs download speeds competitive file access protocols files various sizes transferred pair testbed machines ordinary tcp 
files transferred time file tcp connection 
shows cumulative distribution transfer speeds various machine pairs kbyte kbyte mbyte files 
wide distributions reflect wide range propagation delays link capacities different pairs machines 
best speeds connected sites east coast united states 
worst speeds kbyte transfers occur points outside united states worst megabyte transfers occur endpoint outside united states cable modem combining high latency limited link speed 
cfs kbyte pre fetch window achieves speeds competitive tcp average 
cfs speeds generally distribution narrower tcp 
means users see performance fetching files cfs fetching files example ftp servers 
controlled experiments remainder results obtained set cfs servers running single machine local loopback network interface communicate 
servers act just different machines 
arrangement 


number cfs servers average number rpcs client issue find block function total number servers 
error bars reflect standard deviation 
experimental data linear log plot fits expectation logarithmic growth 
appropriate controlled evaluation cfs scalability tolerance failure 
lookup cost looking block data expected require log rpcs 
experiment verifies expectation 
range numbers servers blocks inserted system 
lookups done single server randomly selected blocks 
number rpcs required lookup recorded 
averages plotted error bars showing standard deviation 
results linear log plot fit expectation logarithmic growth 
actual values loga example servers lookups averaged rpcs 
number rpcs required determined number bits originating server id desired block id differ average half bits accounts :10.1.1.105.3673
load balance main goals cfs balance load servers 
cfs achieves load balanced storage breaking file sys tems blocks distributing blocks servers 
balances storage placing multiple virtual servers physical server virtual server id expect log virtual servers physical server sufficient balance load reasonably :10.1.1.105.3673
shows typical distributions id space physical servers virtual servers physical server 
crosses represent actual distribution blocks physical servers virtual servers 
desired result server fraction 
virtual server server virtual servers servers store blocks store times average 


fraction id space physical node real 
representative cumulative distributions fraction key space server responsible 
servers simulated virtual servers 
data marked real derived distribution blocks servers virtual servers 
multiple virtual servers server sum parts id space server virtual servers responsible tightly clustered average 
fact cfs spreads storage blocks servers means cases burden serving blocks evenly spread 
large files true files popular file blocks widely spread 
popular data consists blocks servers happen blocks successors experience high load 
section describes caching helps balance serving load small files 
caching cfs caches blocks lookup path 
initiating server contacts successive servers checks desired block cached 
initiating server block sends copy servers contacted lookup servers add block caches 
scheme expected produce high cache hit rates lookup paths block different sources tend intersect get closer block successor server 
illustrates caching works 
single block inserted server system 
sequence randomly chosen servers fetch block 
graph shows number rpcs required fetch block decreases number cumulative fetches due block cached places 
plotted point average sequential fetches 
quirk implementation prevents originating server checking cache fetches rpc count zero expected rpc counts decrease servers block cached 
rpc counts decrease significantly just lookups 
shows lookups caching server system require average rpcs 




cumulative number lookups impact caching successive client fetches block 
point average number rpcs successive fetches randomly chosen servers error bars indicate standard deviation 
system servers 
lookups caching average hops required 
net effect improve client perceived performance spread load serving small files 
storage space control varying number virtual servers physical server server owner control amount data cfs stores server amount server serve 
shows effective experiment involves physical servers virtual servers respectively 
blocks inserted system relationship virtual servers physical server blocks store plotted 
example physical server virtual server stores blocks total virtual servers close expected value relationship blocks virtual servers linear administrator easily adjust cfs server storage consumption 
little memory overhead running virtual servers achieve fine grained control load 
virtual server requires finger table successor list accounting structures block store cache total memory footprint structures unoptimized implementation kbytes 
effect failure cfs server falls time pass remaining servers react failure correcting finger tables successor pointers copying blocks maintain desired level replication 
theorems suggest cfs able perform lookups correctly efficiently recovery process starts face massive failure 
test blocks inserted server system 



number virtual nodes impact number virtual servers physical server total amount data physical server store 






failed nodes fraction average lookup rpc count function fraction cfs servers fail 
servers failures 
data point average experiments error bars indicate minimum maximum results 

failed nodes fraction 
fraction block request failures function fraction cfs servers fail 
data point average experiments involving block error bars indicate minimum maximum results 
block replicas including main copy stored direct successor 
insertions fraction servers fail warning 
chord starts rebuilding routing tables fetches randomly selected blocks attempted single server 
shows fraction lookups fail shows average rpc count lookups 
lookups fail fewer servers fail fail 
reason server finger tables successor lists provide potential paths carry query chord id ring desirable finger table entry points failed server cfs uses entry points far ring 
lookups start fail servers fail blocks lose copies 
example servers fail probability losing block replicas close value shown 
lookup failures encountered experiment due block replicas failing cfs able find copy block available 
shows lookups take rpc longer result servers failing 
rpc counts include attempts contact failed servers 
lookups take longer failures finger table entries required fast lookups point failed servers 
half finger table entries valid rpc half progress expected extra rpc fully corrects 
shows number attempts contact failed servers occur lookup averaged block lookups 
time server decides timeout finger table successor list entry points failed server server stabilized 
massive failures little effect availability data number rpcs lookup users perceive failures rpc timeouts lookups 
shows typical block lookup shortly failure expect 





failed nodes fraction number rpc timeouts incurred lookups function fraction cfs servers fail 
servers failures 
data point average experiments error bars indicate minimum maximum results 
timeout way retrieving desired block 
experiments demonstrate large fraction cfs servers fail significantly affecting data availability performance 

research cfs benefit keyword search system 
way provide adopt existing centralized search engine 
ambitious approach store required index files cfs idea pursuing 
cfs robust case fail failures specifically defend malicious participants 
group malicious nodes form internally consistent cfs system system falsify documents cfs verify authenticity incorrectly claim document exist 
versions cfs defend periodically checking consistency chord tables randomly chosen nodes 
cfs copy blocks servers node joins leaves system order maintain desired level replication 
nodes join system short time leaving copies wasted 
cfs allow lazy replica copying 
cfs nat 
special arrangements possible pool global servers acting proxies hosts nats 
current cfs client uses fixed pre fetch window 
size give best performance situations right size depends round trip time available network bandwidth 
pre fetch window serves purpose similar tcp congestion window analogous adaptive algorithms 

cfs highly scalable available secure read file system 
presents stored data applications ordinary file system interface 
servers store uninterpreted blocks data unique identifiers 
clients retrieve blocks servers interpret file systems 
cfs uses peer peer chord lookup protocol map blocks servers 
mapping dynamic implicit 
result directory information updated underlying network changes 
cfs robust scalable 
cfs uses replication caching achieve availability load balance 
cfs replicates block consecutive servers identifier space 
caches block lookup path starting block server 
cfs provides simple effective protection single attacker inserting large amounts data 
prototype implementation cfs implemented evaluated controlled intemet wide test bed 
operational deployment uncover opportunities improvement current results indicate cfs viable large scale peer topeer system 
acknowledgments grateful balakrishnan john zahorjan anonymous reviewers helpful comments david andersen managing ron testbed letting 
akamai technologies www akamai com 
cambridge ma 
andersen balakrishnan kaashoek morris resilient overlay networks 
proceedings th acm symposium operating systems principles oct 
chankhunthod danzig neerdaels schwartz worrell hierarchical object cache 
proc 
usenix technical conference jan pp 

chaum untraceable electronic mail return addresses digital pseudonyms 
communications acm feb 
clarke distributed decentralised information storage retrieval system 
master thesis university edinburgh 
clarke sandberg wiley hong freenet distributed anonymous information storage retrieval system :10.1.1.10.4919
proceedings workshop design issues anonymity unobservability july pp 

dingledine freedman molnar free haven project distributed anonymous storage service 
proceedings workshop design issues anonymity unobservability july pp 

fan cao almeida broder summary cache scalable wide area web cache sharing protocol 
tech 
rep computer science department university wisconsin madison feb 
fu kaashoek mazi res fast secure distributed read file system 
proceedings th usenix symposium operating systems design implementation osdi october pp 

gadde chase rabinovich taste squid 
workshop server performance june pp 

gnutella website gnutella wego com 
karger lehman leighton levine lewin panigrahy consistent hashing random trees distributed caching protocols relieving hot spots world wide web :10.1.1.147.1879
proceedings th annual acm symposium theory computing may pp 

kubiatowicz bindel chen czerwinski eaton geels gummadi rhea weatherspoon weimer wells zhao oceanstore architecture global scale persistent storage 
proceeedings ninth international conference architectural support programming languages operating systems asplos november pp 

consistent hashing random trees algorithms caching distributed networks 
master thesis mit 
lorch berger making world wide web caching servers cooperate 
fourth international world wide web conference pp 

res toolkit user level file systems 
proc 
usenix technical conference june pp 

mazi res kaminsky kaashoek witchel separating key management file system security 
proceedings th acm symposium operating systems principles sosp dec pp 

merkle digital signature conventional encryption function 
advances cryptology crypto berlin ed vol 
lecture notes computer science springer verlag pp 

mojo nation documentation www 
napster 
www napster com 
ng stoica zhang 
waypoint service approach connect heterogeneous internet address spaces 
proc 
usenix technical conference june pp 


www com design html june application longer available 
oram ed 
peer peer harnessing power disruptive computation 
reilly associates 
plaxton rajaraman richa accessing nearby copies replicated objects distributed environment 
proceedings acm spaa june pp 

rabin efficient dispersal information security load balancing fault tolerance 
journal acm 
ratnasamy francis handley karp shenker scalable content addressable network 
proc 
acm sigcomm san diego 
reiter rubin crowds anonymity web transactions 
acm transactions information system security nov 
rowstron druschel pastry distributed object location routing large scale peer peer systems 
proceedings th ifip acm international conference distributed systems platforms middleware nov 
rowstron druschel storage management caching past large scale persistent peer peer storage utility 
proceedings th acm symposium operating systems principles oct 
sherman karger kim web caching consistent hashing 
computer networks may 
stoica morris karger kaashoek balakrishnan chord scalable peer peer lookup service interact applications :10.1.1.105.3673
proc 
acm sigcomm san diego 
stoica morris karger kaashoek balakrishnan chord scalable peer peer lookup service internet applications 
tech 
rep tr mit cambridge ma march 
case study server selection 
master thesis mit sept 
waldman robin cranor publius robust tamper evident censorship resistant web publishing system :10.1.1.125.3017
proc 
th usenix security symposium august pp 

zhao kubiatowicz joseph tapestry infrastructure fault tolerant wide area location routing 
tech 
rep ucb csd computer science division berkeley apr 

