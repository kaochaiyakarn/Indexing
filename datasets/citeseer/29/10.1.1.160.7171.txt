kernel methods dave krebs cs fall sources herman hilbert spaces 
pennsylvania state university 
june 
bousquet perez cruz 
kernel methods potential signal processing 
ieee signal processing magazine 
may 
burges christopher 
tutorial support vector machines pattern recognition 
cristianini shawe taylor 
kernel methods paradigm pattern analysis 
kernel methods signal image processing 

sch lkopf bernhard 
statistical learning kernel methods 
sch lkopf bernhard 
kernel trick distances 
outline motivation kernel basics definition example application modularity creating complicated kernels mercer condition definitions constructing feature space hilbert spaces kernels generalized distances gaussian kernel choosing best feature space motivation set vectors tools available detect linear relations data ridge regression support vector machines svm principal component analysis pca relations non linear original space 
motivation solution map data possibly highdimensional vector space linear relations exist data apply linear algorithm space motivation problem representing data highdimensional space computationally difficult alternative solution original problem calculate similarity measure feature space coordinates vectors apply algorithms need value measure dot product similarity measure kernel definition function takes inputs vectors original space returns dot product vectors feature space called kernel function formally data map kernel function important point kernels need embed data space explicitly number algorithms require inner products image vectors 
need coordinates data feature space 
kernel example consider dimensional input space feature map consider inner product feature space kernel example continued kernel example continued kernel computes inner product map shows feature space unique kernel function kernel application support vector machines solution dual problem gives decision boundary sv decision sign sv mapping feature space decision sign sv modularity basic approach kernel methods choose algorithm uses inner products inputs combine algorithm kernel function calculates inner products input images feature space kernels algorithm implemented high dimensional space nice property kernels modularity kernel reused adapted different real world problems kernels combined form complex learning systems creating complicated kernels 

real valued function kernel trick want map patterns high dimensional feature space compare dot product avoid working space choose feature space dot product evaluated directly nonlinear function input space called kernel trick mercer condition kernels exist pair possibly infinite dimensional euclidean space mapping 
mercer condition tells prospective kernel dot product space stated follows exists mapping expansion mercer condition continued finite dxdy dx shown condition satisfied positive integral powers dot product kernel cp positive real coefficients series uniformly convergent satisfies mercer condition definitions gram matrix xm ij kernel positive matrix matrix kij satisfying input patterns cic ij ci definitions continued positive definite pd kernel mercer kernel support vector kernel function xi gives rise positive gram matrix property implies positivity diagonal real coefficients kernel symmetric ci xi require definitions continued conditionally positive matrix matrix ij satisfying ij ci conditionally positive definite kernel function xi gives rise conditionally positive gram matrix constructing feature space want kernel function satisfies construct feature space 
define feature map denotes function assigns value pattern function domain constructing feature space continued 
turn linear space ik xi jk constructing feature space continued 
endow dot product jk turn hilbert space note kernel trick hilbert spaces vector space set endowed addition operation scalar multiplication operation operations satisfy certain rules trivial verify euclidean space real vector space inner product real vector space real function cx hilbert spaces continued norm vector space mapping scalars 


triangular inequality norm defines metric thought function measures distance elements hilbert spaces continued cauchy sequence sequence elements xn metric space space endowed metric metric 
exists hilbert space vector space endowed inner product associated norm metric cauchy sequence limit hilbert spaces continued example hilbert space euclidean space vector space inner product dot product norm metric feature space larger number training examples sufficient data linear algorithms perform applied linear problems nonlinear problems dimensionality feature space larger number input samples dimensionality infinite rely data obtain results find best solution minimize empirical error control complexity feature space larger number training examples continued svm maximum margin principle choose simplest solution linear solution largest margin set solutions smallest error rationale expect simple solution perform better unseen patterns problem dot product patterns translated changes suitable algorithms learning process translation invariant pca dissimilarity measure translation invariant expressed kernel trick kernels generalized distances dot product translation invariant need fix origin going distance measure dot product write dot product terms distances kernel trick works pd kernel exists larger class kernels generalized distances kernels generalized distances continued kernel pd cpd cpd property imply pd property cpd kernels comprise larger class pd kernels example kernel cpd pd fact cpd real valued cpd kernel satisfying exists hilbert space realvalued functions mapping kernels generalized distances continued result cpd kernels generalize algorithms distances corresponding algorithms operating feature spaces kernel trick distances gaussian kernel defined exp widely kernel universal linear combinations kernel approximate continuous function corresponding feature space infinite dimensional labeled data set exists linear hyperplane correctly separates data gaussian feature space expression power basically unlimited careful overfit 
choose best feature space problem choosing optimal feature space problem non trivial know feature space kernel problem reduces choosing best kernel learning performance kernel algorithm highly dependent kernel best kernel depends specific problem choosing best kernel prior knowledge problem significantly improve performance shape solution kernel appropriate problem needs tune kernel performance problem dependent universally algorithm exists bad news need prior knowledge news knowledge extracted data approaches choosing best kernel approach tune hyper parameter family functions gaussian kernel exp set kernel width non vectorial data strings approach popular kernels people devised kernel problems protein classification approaches choosing best kernel continued approach learn kernel matrix directly data approach promising goals approach restricted family kernels may appropriate problem tuning hyper parameters derive way learn kernel matrix setting parameters learning kernel matrix problems learning kernel matrix clear best criterion optimize current procedures computationally efficient choice class kernel matrices considered important implementation issues selection kernel problem difficult may possible store entire kernel matrix large data sets may need re compute kernel function entries needed advantages kernels possible look linear relations high dimensional spaces low computational cost inner products input images feature space calculated original space modularity require data real vectors example strings time series questions 
