multivariate information bottleneck nir friedman ori naftali tishby school computer science engineering hebrew university jerusalem israel nir tishby cs ac il information bottleneck method unsupervised non parametric data organization technique 
joint distribution method constructs new variable extracts partitions clusters values informative information bottleneck applied document classification gene expression neural code spectral analysis 
introduce general principled framework multivariate extensions information bottleneck method 
allows consider multiple systems data partitions inter related 
approach utilizes bayesian networks specifying systems clusters information captures 
show construction provides insight bottleneck variations enables characterize solutions variations 
general framework iterative algorithms constructing solutions apply examples 
clustering data partitioning common data analysis paradigm 
central question understanding general underlying principles clustering 
information theoretic approach clustering require clusters capture relevant information data relevance explicitly determined various components data 
common data type calls principle occurrence data verbs direct objects sentences words documents tissues gene expression patterns galaxies spectral components cases objects discrete obvious correct measure similarity exists :10.1.1.42.7488:10.1.1.21.3062
rely purely joint statistics occurrences organize data relevant information variables captured best possible way 
formally quantify relevance variable respect terms mutual information known quantity defined symmetric non negative equals zero variables independent 
measures bits needed average convey information vice versa 
aim information theoretic clustering find soft partitions values informative requires balancing goals want lose irrelevant distinctions time maintain relevant ones 
capital letters random variable names lowercase letters denote specific values taken variables 
sets variables denoted boldface capital letters assignments values variables sets denoted boldface lowercase letters statement shorthand tishby considered variables assumed joint distribution variable try compress respect relevant variable seek soft partition auxiliary variable probabilistic mapping mutual information minimized maximum compression relevant information maximized 
dependency relations variables described relations independent hand want predict introducing positive lagrange multiplier tishby formulate tradeoff minimizing lagrangian take variation derivative finite case proper normalization con straints tishby show optimal partition satisfies gence 
equation satisfied self consistently 
practical solution equations done repeated iterations self consistent equations value similar clustering deterministic annealing :10.1.1.33.3047
convergence iterations generally local optimum proven 
bayesian networks multi information familiar kl bayesian network structure set random variables dag vertices names random variables 
variable denote potentially empty set parents say distribution consistent factored form notation denote 
main issues deal amount information variables contain 
theorem asynchronous iterations equations converge stationary point optimization problem 
see appendix proof case convergence proof general case involved appear full version 
current stage proof convergence synchronous case experiments synchronous updates converge 
key question initialize procedure different initialization points lead different solutions 
describe deterministic annealing procedure :10.1.1.33.3047
procedure works iteratively increasing parameter adapting solution previous value new 
allows algorithm track changes solution system shifts preferences compression prediction recall optimization problem tends independent parents 
point solution consists essentially cluster predictive variable 
increase suddenly reach point values diverge show different behaviors 
real structure data 
interestingly due split information preserve increased split reason course predicts split increases hand split step remains unchanged 
splits practically overfitting effects accordingly real information gain due splits 
realistic example standard newsgroups corpus 
natural language corpus contains articles evenly distributed usenet discussion groups employed evaluating text classification techniques :10.1.1.22.6286:10.1.1.107.7109
groups similar topics 
groups discuss different issues concerning computers groups discuss religion issues inherent hierarchy groups 
model domain setting introduce random variables 
denote words denote category newsgroup 
hofmann 
probabilistic latent semantic indexing 
acm sigir pages 

lang :10.1.1.22.6286
learning filter netnews 
th int 
conf 
machine learning pages 
pereira tishby lee 
distributional clustering english words 
th annual meeting association computational linguistics pages 

rose :10.1.1.33.3047
deterministic annealing clustering compression classification regression related optimization problems 
proceedings ieee 
tishby 
agglomerative information bottleneck 
tishby 
document clustering word clusters information bottleneck method 
acm si gir pages 

tishby :10.1.1.107.7109
power word clusters text classification 
rd european colloquium information retrieval research 

tishby pereira 
