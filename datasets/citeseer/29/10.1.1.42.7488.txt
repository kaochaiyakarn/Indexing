distributional clustering words text classification douglas baker yz www cs cmu edu school computer science carnegie mellon university pittsburgh pa andrew mccallum www cs cmu edu mccallum just research henry street pittsburgh pa describes application distributional clustering document classification :10.1.1.14.1729
approach clusters words groups distribution class labels associated word 
unsupervised techniques latent semantic indexing able compress feature space aggressively maintaining high document classification accuracy 
experimental results obtained real world data sets show reduce feature dimensionality orders magnitude lose accuracy significantly better latent semantic indexing class clustering feature selection mutual information markov blanket feature selection :10.1.1.155.2293:10.1.1.13.9919:10.1.1.108.8490
show aggressive clustering results improved classification accuracy classification clustering 
popularity internet caused exponential increase amount line text number people create text 
experimental results obtained real world data sets show reduce feature dimensionality orders magnitude lose accuracy significantly better latent semantic indexing class clustering feature selection mutual information markov blanket feature selection :10.1.1.155.2293:10.1.1.13.9919:10.1.1.108.8490
show aggressive clustering results improved classification accuracy classification clustering 
popularity internet caused exponential increase amount line text number people create text 
amount documents number users rise automatic document categorization increasingly important tool helping people organize vast amount data 
statistical document classification algorithms applied categorizing classifying web pages sorting electronic mail learning interests users :10.1.1.21.7950:10.1.1.48.284:10.1.1.35.6633
cluster words groups specifically benefit document classification 
study devoted word clustering language modeling word occurrence little done word clustering document classification :10.1.1.14.1729:10.1.1.13.9919
underlying clustering method apply distributional clustering information theoretic approach shown performance language modeling :10.1.1.14.1729
word clustering methods create new reduced size event spaces joining similar words groups 
popularity internet caused exponential increase amount line text number people create text 
amount documents number users rise automatic document categorization increasingly important tool helping people organize vast amount data 
statistical document classification algorithms applied categorizing classifying web pages sorting electronic mail learning interests users :10.1.1.21.7950:10.1.1.48.284:10.1.1.35.6633
cluster words groups specifically benefit document classification 
study devoted word clustering language modeling word occurrence little done word clustering document classification :10.1.1.14.1729:10.1.1.13.9919
underlying clustering method apply distributional clustering information theoretic approach shown performance language modeling :10.1.1.14.1729
word clustering methods create new reduced size event spaces joining similar words groups 
distributional clustering joining words digital hard copy part personal classroom granted fee provided copies distributed profit commercial advantage copyright notice title publication date appear notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee 
sigir melbourne australia fl acm 
amount documents number users rise automatic document categorization increasingly important tool helping people organize vast amount data 
statistical document classification algorithms applied categorizing classifying web pages sorting electronic mail learning interests users :10.1.1.21.7950:10.1.1.48.284:10.1.1.35.6633
cluster words groups specifically benefit document classification 
study devoted word clustering language modeling word occurrence little done word clustering document classification :10.1.1.14.1729:10.1.1.13.9919
underlying clustering method apply distributional clustering information theoretic approach shown performance language modeling :10.1.1.14.1729
word clustering methods create new reduced size event spaces joining similar words groups 
distributional clustering joining words digital hard copy part personal classroom granted fee provided copies distributed profit commercial advantage copyright notice title publication date appear notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee 
sigir melbourne australia fl acm 
duce similar probability distributions target features occur words question 
derive naive bayes explain assumptions discuss close ties cross entropy 
describe distributional clustering show distributional clustering clusters features minimize errors cross entropy 
experimental results real world text corpora including newswire stories usenet articles web pages 
results show distributional clustering reduce feature dimensionality orders magnitude lose accuracy 
performance significantly better class clustering mutual information clustering latent semantic indexing feature selection information gain feature selection markov blanket :10.1.1.155.2293:10.1.1.13.9919:10.1.1.108.8490
data sets show clustering increases classification accuracy 
hypothesize happen cases discuss possible improvements 
clustering words class distributions section introduces probabilistic framework derives naive bayes classifier explains distributional clustering 
previous distributional clustering form kullback leibler divergence mean :10.1.1.14.1729:10.1.1.14.1729
performance significantly better class clustering mutual information clustering latent semantic indexing feature selection information gain feature selection markov blanket :10.1.1.155.2293:10.1.1.13.9919:10.1.1.108.8490
data sets show clustering increases classification accuracy 
hypothesize happen cases discuss possible improvements 
clustering words class distributions section introduces probabilistic framework derives naive bayes classifier explains distributional clustering 
previous distributional clustering form kullback leibler divergence mean :10.1.1.14.1729:10.1.1.14.1729
weighted average simple average hard clustering soft greedy agglomerative method divisive entropy method 
probabilistic framework naive bayes approach text classification bayesian learning framework 
assume text data generated parametric model training data calculate bayes optimal estimates model parameters 
equipped estimates classify new test documents bayes rule turn generative model calculate probability class generated test document question 
calculate estimates written parameters training data 
jc estimates consist straightforward counting events supplemented smoothing prior primes estimate count 
define count number times word occurs document define jd document class label estimate probability word class jc jdj jd jv jv jdj ws jd class prior parameters estimated maximum likelihood estimate fraction documents class corpus jdj jd jdj estimates parameters calculated training documents classification performed test documents calculating probability class evidence test document selecting class highest probability 
formulate applying bayes rule substituting jc equations 
jd jc jd wd ik jc jcj cr jd wd ik mixture model word independence assumptions violated practice real world data empirical evidence naive bayes performs spite violations :10.1.1.35.6633:10.1.1.49.860
friedman domingos pazzani discuss violation word independence assumption little damage classification accuracy :10.1.1.144.7475
measuring word similarity distributional clustering address question cluster words context generative model naive bayes 
word clustering algorithms define similarity measure words collapse similar word single events longer distinguish constituent words 
typically parameters cluster weighted average parameters constituent words 
jc estimates consist straightforward counting events supplemented smoothing prior primes estimate count 
define count number times word occurs document define jd document class label estimate probability word class jc jdj jd jv jv jdj ws jd class prior parameters estimated maximum likelihood estimate fraction documents class corpus jdj jd jdj estimates parameters calculated training documents classification performed test documents calculating probability class evidence test document selecting class highest probability 
formulate applying bayes rule substituting jc equations 
jd jc jd wd ik jc jcj cr jd wd ik mixture model word independence assumptions violated practice real world data empirical evidence naive bayes performs spite violations :10.1.1.35.6633:10.1.1.49.860
friedman domingos pazzani discuss violation word independence assumption little damage classification accuracy :10.1.1.144.7475
measuring word similarity distributional clustering address question cluster words context generative model naive bayes 
word clustering algorithms define similarity measure words collapse similar word single events longer distinguish constituent words 
typically parameters cluster weighted average parameters constituent words 
consider example random variable classes distribution particular word write distribution cjw 
kl divergence odd properties 
symmetric infinite event non zero probability distribution zero probability second distribution 
distributional clustering related measure problems 
average kl divergence distribution mean distribution called kl divergence mean 
earlier weighted average simple average :10.1.1.14.1729:10.1.1.14.1729:10.1.1.14.1729
delta cjw cjw ws ws delta cjw ws metric understood expected amount inefficiency incurred compressing distributions optimally code code optimal mean 
explanation clear metric fit clustering distance metric 
describes perfectly effect clustering events generated individual statistics clustered generate combined statistics 
sort vocabulary mutual information class variable 
agglomerative hard clustering algorithm uses statistic similarity metric 
tried experiments kl divergence average yields better performance 
chi extension feature selector numeric attributes 
liu setiono observe values attribute clustered value irrelevant classification task removed 
class clustering uses agglomerative hard clustering algorithm clustering criterion designed maximize average mutual information clusters class variable :10.1.1.13.9919
criterion implicitly measures similarity distributions cjw similarity distributions cj cj ws features 
find average mutual information clustering criterion text classification multinomial naive bayes model considers information class label indicated presence absence word document classifier considers words document 
clustering criterion class distributions words appear better suited multinomial naive bayes classifier 
argue kl divergence choice criteria 
argue kl divergence choice criteria 
clustering reduces dimensionality feature space seen form feature selection remove features 
previous study feature selection mutual information class label best text common time space efficient methods 
mutual information words classes capture dependencies words 
koller sahami markov blanket feature selection algorithm aims address exactly :10.1.1.155.2293
technique principles distributional clustering examines cjw tries preserve proper distribution 
latent semantic indexing unsupervised dimensionality reduction technique information retrieval explicitly accounts dependencies words :10.1.1.108.8490
brief applies principle component analysis pca documents represented word vectors 
dumais applies text classification representing class centroid vector sum feature vectors documents class 
previous study feature selection mutual information class label best text common time space efficient methods 
mutual information words classes capture dependencies words 
koller sahami markov blanket feature selection algorithm aims address exactly :10.1.1.155.2293
technique principles distributional clustering examines cjw tries preserve proper distribution 
latent semantic indexing unsupervised dimensionality reduction technique information retrieval explicitly accounts dependencies words :10.1.1.108.8490
brief applies principle component analysis pca documents represented word vectors 
dumais applies text classification representing class centroid vector sum feature vectors documents class 
new document labelled class centroid feature vector closest measured cosine similarity vectors 
linear squares fit llsf method classification algorithm pca equivalent dumais lsi classification llsf uses dot product compute similarity cosine sensitive length vectors compared 
dumais applies text classification representing class centroid vector sum feature vectors documents class 
new document labelled class centroid feature vector closest measured cosine similarity vectors 
linear squares fit llsf method classification algorithm pca equivalent dumais lsi classification llsf uses dot product compute similarity cosine sensitive length vectors compared 
experimental results section provides empirical evidence distributional clustering able aggressively reduce number features maintaining high classification accuracy 
equal feature dimensionality achieves significantly higher accuracy feature clustering feature selection algorithms supervised latent semantic indexing class clustering feature selection mutual information class variable feature selection markov blanket method :10.1.1.155.2293:10.1.1.13.9919
newsgroups data set collected ken lang contains articles evenly divided usenet discussion groups :10.1.1.21.7950
topic classes quite confusable computers discuss religion 
tokenizing data skipped usenet headers stoplist stem hurt accuracy 
resulting vocabulary removing words occur words 
new document labelled class centroid feature vector closest measured cosine similarity vectors 
linear squares fit llsf method classification algorithm pca equivalent dumais lsi classification llsf uses dot product compute similarity cosine sensitive length vectors compared 
experimental results section provides empirical evidence distributional clustering able aggressively reduce number features maintaining high classification accuracy 
equal feature dimensionality achieves significantly higher accuracy feature clustering feature selection algorithms supervised latent semantic indexing class clustering feature selection mutual information class variable feature selection markov blanket method :10.1.1.155.2293:10.1.1.13.9919
newsgroups data set collected ken lang contains articles evenly divided usenet discussion groups :10.1.1.21.7950
topic classes quite confusable computers discuss religion 
tokenizing data skipped usenet headers stoplist stem hurt accuracy 
resulting vocabulary removing words occur words 
test train split reuters data set www research att com lewis contains training documents testing documents gathered reuters newswire 
acknowledgments grateful sahami markov blanket feature selection code susan dumais lsi code friendly responsive help making run system 
thorsten joachims provided useful advice formatting reuters data 
answered questions regarding linear discriminant analysis 
yahoo permission data 
brown mercer della pietra lai :10.1.1.13.9919
class gram models natural language 
computational linguistics 
thomas cover peter hart 
nearest neighbor pattern classification 
ieee transactions information theory 
thomas cover joy thomas 
elements information theory 
john wiley 
mark craven daniel dipasquo dayne freitag andrew mccallum tom mitchell kamal nigam sean slattery :10.1.1.35.6633
learning extract symbolic knowledge world wide web 
proceedings fifteenth national conference artificial intelligence aaai 
ido dagan fernando pereira lee 
similarity estimation word cooccurrence probabilities 
proceedings fifteenth national conference artificial intelligence aaai 
ido dagan fernando pereira lee 
similarity estimation word cooccurrence probabilities 
proceedings nd annual meeting association computational linguistics 
deerwester dumais landauer furnas harshman :10.1.1.108.8490
indexing latent semantic analysis 
journal american society information science 
domingos pazzani 
independence conditions optimality simple bayesian classifier 
technical report national institute standards technology 
jerome friedman 
bias variance loss curse dimensionality 
data mining knowledge discovery 
thorsten joachims :10.1.1.21.7950
probabilistic analysis rocchio algorithm tfidf text categorization 
international conference machine learning icml 

applied multivariate data analysis volume ii categorical multivariate methods 
springer verlag 

discretization numeric attributes 
proceedings tenth national conference artificial intelligence aaai 
koller sahami :10.1.1.155.2293
optimal feature selection 
proceedings thirteenth international conference machine learning icml 
ken lang 
newsweeder learning filter netnews 
technical report tr 
david lewis marc ringuette 
comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pages 
david lewis knowles :10.1.1.48.284
threading electronic mail preliminary study 
information processing management 
liu setiono 
chi feature selection discretization numeric attributes 
andrew mccallum kamal nigam 
comparison event models naive bayes text classification 
aaai workshop learning text categorization 
www cs cmu edu mccallum 
fernando pereira naftali tishby lee :10.1.1.14.1729
distributional clustering english words 
proceedings st annual meeting association computational linguistics pages 

www com 
