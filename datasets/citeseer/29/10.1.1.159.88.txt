fahiem bacchus dept computer science university waterloo waterloo ontario canada logos uwaterloo ca rewarding behaviors craig boutilier dept computer science university british columbia vancouver canada cs ubc cs adam grove nec research institute independence way princeton nj usa grove research nj nec com markov decision processes mdps popular tool decision theoretic planning dtp partly expressive theory includes effective solution techniques 
markov assumption dynamics rewards depend current state history inappropriate 
especially true rewards frequently wish associate rewards behaviors extend time 
course reward processes encoded mdp rich state space states encode history 
difficult hand craft suitable state spaces encode appropriate amount history 
consider problem case non markovian rewards encoded assigning values formulas temporal logic 
formulas characterize value temporally extended behaviors 
argue allows natural representation commonly encountered non markovian rewards 
main result algorithm decision process non markovian rewards expressed manner automatically constructs equivalent mdp markovian reward structure allowing optimal policy construction standard techniques 
years seen tremendous interest extending classical planning paradigm deal domains involving uncertain information actions uncertain effects problems competing objectives 
decision theoretic planning dtp generally aimed addressing issues adopted theory markov decision processes mdps underlying conceptual computational model tr bd bdg 
mdps allow formulate problems agent involved going process oriented interaction environment receives rewards various system states 
generalizes classical goal oriented view planning bp 
classical plans considers flexible concept policy mapping state action executed state 
effective optimization methods exist computing policies agent executing policy maximize accumulated reward time put 
fundamental assumption formulation planning problem mdp system dynamics rewards markovian 
manner system behaves action executed rewards received depend system current state states previously visited 
example wish control robot usually difficult find state space robot actions described markovian stochastic state transitions 
fact natural way represent effects actions 
assigning natural markovian rewards problematic 
easy associate rewards individual states navigation problem rewards associated locations reward naturally assigned behavior occurs extended period 
cases difficult encode reward function state 
instance may reward agent states coffee just delivered state preceded state coffee request issued withholding reward spurious delivery 
reward properly function system trajectory history state 
typical forms desirable temporally extended behaviors include response requests bounded response lack response maintaining safety constraints 
temporally extended goals nature examined extent literature hh kab gk context generating effective policies 
key difficulty non markovian rewards standard optimization techniques bellman bel dynamic programming principle 
way dealing formulate equivalent decision problem rewards markovian 
particular augment state space underlying system adding variables keep track history relevant reward function 
instance boutilier puterman bp suggest straightforward ways encoding reward functions involve simple requests 
approach advantage existing optimization methods mdps 
unfortunately general finding way augment state space requires considerable cleverness especially concerned minimizing size resulting augmented space computational reasons 
examine problem rewarding temporally extended behaviors 
provide natural quite expressive means specifying rewards attached behaviors extended time 
furthermore solve problem computing policies face non markovian rewards developing algorithm automatically constructs markovian reward process associated mdp 
algorithm automates process generating appropriate augmentation state space coupled traditional policy construction techniques provides way computing policies richer range reward functions 
section introduce essentially mdps non markovian reward 
system dynamics specified mdps rewards associated formulas suitable temporal logic 
define temporally extended reward functions requiring reward associated formula state formula satisfied 
note decision reward agent state depend past states states 
reason natural encode reward formulas past backward looking temporal logic usual forward logics ltl ctl eme mtl ah 
section describe number interesting useful classes target behaviors show encoded 
section consider problem constructing optimal policies 
mentioned dynamic programming construct policies setting 
nominally requires resort optimization policy space maps histories states actions process incur great computational expense 
procedure expands original state space attaching temporal formula state 
formula keeps track appropriate amount relevant history 
constructing state markovian reward function extended state space convert nmrdp equivalent mdp particular optimal policies mdp determine optimal policies original nmrdp natural way 
way obtain compact representation required history dependent policy considering relevant history produce policy computationally effective mdp algorithms 
non markovian rewards pr ors markov decision processes dtp considers planning problems modeled completely observable markov decision processes put 
model assume finite set system set reward 
effects actions predicted certainty write denote thats reached actiona performed states 
complete observability entails agent knows state 
assume state space characterized set features logical propositions 
allows actions described compactly probabilistic strips rules bd bayes nets dk bdg action representations 
real valued reward objectives tasks goals accomplished agent denoting immediate utility states 
purposes mdp consists ofs rand set transition ag 
stationary markovian policy mapping denotes action agent perform states 
think policies reactive universal plans sch 
mdp agent ought adopt policy maximizes expected value potentially infinite trajectory state space 
common value criterion dtp infinite horizon problems discounted total reward current value rewards discounted factor xt spr maximize expected accumulated discounted rewards infinite time period 
expected value fixed policy shown satisfy value initial computed solving system linear equations 
policy optimal ifv alls sand policies 
techniques constructing optimal policies case discounted rewards studied include algorithms value iteration bel policy iteration 
noted algorithms exploits markovian nature reward process 
refer put excellent treatment mdps associated computational methods 
temporal logic past reward agents temporally extended behaviors opposed simply reaching certain states need means specify rewards specific trajectories state space 
generally want associate rewards properties trajectories rewarding individual trajectories 
example reward agent achieved regard particular trajectory agent traversing 
associate rewards penalties desirable undesirable formulas suitable temporal logic describes trajectory properties 
logic consider backward past looking 
truth temporal formula depends prior states happen 
accords view reward processes contexts rewards earned happened 
past version ltl eme called pltl 
assume underlying finite set propositional constants usual truth functional connectives temporal operators past past previously 
backward analogs ltl operators formulas formed 
oft hs sni lett denote truth falsity respectively 
semantics pltl described respect models hs sni state valuation si 
atis called finite trajectory partial history 
hs sni denote tj forp initial hs sii 
intuitively temporal formula true true current state respect history alli reflected trajectory 
define truth formulas inductively follows tj sn tj tj tj iff andt intuitively true time held tj iff true point past tj iff true point past true previous state notable consequence semantics fact gis unsatisfiable gis satisfiable model 
known modalities ltl decomposed components eme 
similarly modalities pltl decomposed past components 
example equivalent 
true iff true current state true previous state 
equivalences determine formula true previous state order true 
call regression current state 
note current component falsified current state previous state true 
case regression 
definition regression throughs denoted regr formula pltl final states regr computed recursively regr regr regr regr regr regr eventually respectively 
modality stand disjunction ofi ik 
tj regr regr regr regr regr regr regr regr regr define useful notation 
mdp nmrdp transition probabilities pr feasible iff actions pr si ai si 
pltl formulas determines iff hold 
pltl formula define subformulas set subformulas including 
note 
rewarding temporally extended behaviors reward behaviors adopt generalization mdps allows reward stage process depend past history 
decision process non markovian reward nmrdp similar mdp exception reward domain histories 
intuitively agent receives hs sni process passed 
clearly explicit specification reward function impossible infinite number different histories 
hs sni hs ig assume reward function nmrdp specified compactly 
particular assume reward function defined finite set reward formulas expressed pltl realvalued write ri 
temporally extended reward function ris defined follows formulation gives reward state satisfies nontrivial temporal component reward history dependent 
reward formulas expressed pltl rewards depend past states unambiguously evaluated stage process 
consideration restricted markovian policies dealing 
value choice action stage may depend history 
take policies mappings histories actions 
usual value policy taken expectation discounted accumulated reward ef xn nr hs sni jg finitely specified find ways encoding computing optimal policies see section 
examine expressive power 
assumed additive independent restrictive 
history independent mdp expressed way restricting contain temporal operators 
encoding typical forms behavior demonstrate provide appropriate useful language specify rewards examine common examples see encoded pltl 
claim interesting rewards encoded way evidence suggests pltl capture large useful class reward functions 
common types behaviors simple goal achievement retained special place classical planning 
process oriented model mdp nmrdp number subtleties arise giving goal achievement precise interpretation 
describe possibilities 
assume goal wish agent reach state reward 
simplest reward formula goal isg 
rewards agent state agent highly rewarded roughly larger fraction time spends ing states 
provides incentive agent constantly greater rewards may receive behaviors 
cases intended effect specifying 
care achieved different interpretations provided 
strictest offers state holds generous formula rewards state follows achievement may encourage constant maintenance ofg kg states occurred stages state 
option rewards state occurs stages state allowing rewards addition pltl allows formulate temporally extended goal sequences 
instance agent rewarded followed immediately reward formula 
periodic reward behavior similar behavior steps allowed intervene andi prescribed straightforward fashion 
formulations assume goal constantly desirable classical interpretation goals 
behaviors suited background maintenance goals 
process oriented setting want agent respond requests commands bring goal 
settings goals constant arise periodically fulfilled forgotten preempted expire 
model pltl response formulas specify relation rewarded goal 
basic response formula eventual response agent rewarded state follows ac state command outstanding 
usual may wish reward state command case gsc suffices 
requests achieved timely fashion 
immediate response formulas rewarding goal achieved state command 
generally bounded response formulas typeg reward goal achievement steps request 
formula preclude multiple rewards single request kc gsc rewards goal state 
graded reward faster achievement ofg limits 
instance set rewards goal achievement step steps steps 
longer version describe additional types behaviors possibility logics express different kinds reward 
modeling mdps pointed constructing optimal policies settings non markovian reward computationally prohibitive 
section describe method statespace expansion determines aspects history relevant nmrdp recorded verify truth temporal reward formulas encodes history state 
straightforward transformation reward function rewards attached extended states trajectories restores markovian reward property 
adjustment action descriptions deal new state space fully observable mdp accurately reflects nmrdp solved standard relatively efficient methods 
discussing basic properties transformation satisfy specialize case rewards 
markovian transformations transform nmrdp equivalent mdp requires expand state nmrdp new state expanded state space es carries just original state information additional information required render reward ascription independent history 
shall see think expanded states consisting base state annotated label summarizes relevant history 
nmrdp question wish produce es res expanded space es 
agent remain unchanged aim produce policy suitable original nmrdp reward markovian assigns rewards expanded states 
new mdp useful expect bear strong relationship nmrdp constructed 
particular define strong correspondence follows concerned reward ascription system dynamics markovian 
es 
definition es res expansion functions sand es 
alls 
alls sand es es pr es unique es es pr es es 
feasible hs sni res esn 
intuitively es base state es state ins extended es 
reason speak extended states labeled annotated extended state wheres sis base state lis label distinguishes es extensions ofs 
extensions ofs pick unique es start state corresponding tos 
words thought annotation empty history corresponding occurrence start trajectory 
sn pn sn see important distinguish extension extensions 
important parts definition clauses assert equivalent respect base states dynamics esn pn reward structure 
particular clause ensures trajectory ings extended state es base states trajectory similar structure esn esi alli 
hs corresponding trajectories case 
clause imposes strong requirements reward assigned individual states 
particular hes weakly corresponding es es start state say trajectories strongly corresponding 
hard see relationship unique strongly corresponding trajectory unique strongly corresponding trajectory iff es start state 
clause requires assign rewards extended states manner strongly corresponding trajectories receive reward 
need case weakly corresponding trajectories intuitively different annotations extensions ofs correspond different possible histories 
produce expansion specified defn 
find optimal policies 
hes esi es es es definition policy 
corresponding policy defined hs sni esn strongly corresponding trajectory sni 
proposition policy corresponding policy ands 
corollary optimal policy 
corresponding policy optimal 
suitable expanded mdp optimal policy produce optimal policy nmrdp quite easily 
practice agent need construct explicitly 
run expanded mdp 
agent knows base state starts determines corresponding extended state function furthermore dynamics expanded mdp ensures keep track current extended state simply observing base state transition 
consider size expanded mdp 
fulfill requirements defn 
trivial mdp states encoding complete trajectory information finite horizon 
expanded space grows exponentially horizon 
furthermore simple rewards require item history bit indicating passed require infinite amount complete trajectory history naive approach 
possible want encode relevant history find mdp states possible subject defn 

note statespace size tends dominant complexity determining factor standard mdp solution techniques especially applied planning problems 
transformations problem finding small mdp expands nmrdp easier rewards 
case natural label states pltl formulas summarize history 
precisely new state space es consists annotated states formula pltl 
annotations meaningful correct assertions history sense precise 
give algorithm constructs expansion state space producing labelings states sufficient determine reward 
simple example illustrate essential ideas 
consider single reward es recall goal encode relevant history state annotation 
possibly true need distinct labels implying truth falsity 
imagine extended state true 
es implies true 
suppose reachable states transition nmrdp 
tos 
ensure es label correct assertion history expanded mdp transition extended version ofs es say es satisfy see complexity solving mdps :10.1.1.108.2266
generally state space problematic planning problems grows exponentially number atomic propositions 
adding history naively domain exacerbates problem considerably 
regr historical constraints imposed example transition es es case es es satisfy 
general regression operator determine true earlier states 
reward formula true trajectory terminating es iff holds es predecessor 
formula regr stronger formula implying regr part label attached states reach es sin step 
process naturally repeated states reaching es 
quickly summarize example suppose state reachable propositions exactly base states 
extended states necessary 
base state false need extension labeled pand 
base states true need extended states note extended state property easily tell reward es es pis true 
furthermore regression constraints discussed hold 
example consider transition base necessary labeling ofs regr bep implied es 
take note able find mean es label encode history unable determine correct subsequent label move tos 
algorithm constructs set extended phases somewhat indirectly phase approach 
phase algorithm constructs label sets state ls containing pltl formulas relevant reward 
elements ls necessarily labels ingredients labels constructed 
certain sense discussed section matter add strong formulas ls fact distinct implementations phase just seen regression impose constraints label sets 
ls part label extension es regr implied annotation state es es reachable 
atoms phase correct finds formulas relevant restrict attention extended states labels combinations formulas ls asserting true false 
formally definition set pltl formulas atoms denoted atoms set conjunctions formed members negations 
fq pg pg 
labels belong atoms ls 
general atoms inconsistent simply reachable set feasible trajectories original nmrdp 
performing theorem fq ls proving check consistency generate extended states require constructive fashion explicitly considering states reachable start state 
phase ii algorithm 
illustrate suppose determined pg thats atom ls true ats length includes fin es 
note thats logically simplified tos extended state consider successor states ofs atom label set true 
unique instance ifs succeed obtain new extended states simplified case tos 
pr assert pr adding extended states es way add extended states reachable history meaningful 
instance see pis ls label true ats reachable recalls 
effectively eliminates consideration ats 
algorithm described 
defer discussion phases iii section 
note easy implementation phase set ls equal subformulas set reward formulas 
results section apply suitable choice ls including 
generated algorithm expansion 
show useful define general concept instance definition ges es res hs fn sound annotation gs state es es sand pltl 
fixing sf exists es clauses definition hold 


definition similar definition expansion defn 
give extended states particular form annotations pltl formulas 
furthermore requiring annotations summarize history purposes determining rewards longer care annotations insist history recorded annotations accurate 
generality notion sound annotation may applications 
purposes assumption labels informative determine rewards 
definition ges determines rewards set reward formulas iff es sf es alli 
regr phase find label sets choose ls subsets pltl alls atoms ls formulas set reward formulas ls pr note 
see text discussion phase 
subformulas suitable 
phase ii 
alls atoms ls note 
atom exists unique 
es 
note 
start state corresponding tos 


exists unvisited state es es es alls pr atoms ls regr 
ii 
ifs es adds es mark unvisited 
iii 
set pr sf equal pr 

es es es fri fj ig 

set pr sf transition probabilities previously assigned 
phase iii minimization see section discussion 
andres sf note 
phase necessary 
algorithm find annotated expansion proposition es res sound annotation pi fri fj ig determines rewards expansion 
key understanding algorithm realizing designed generate mdp satisfies conditions proposition 
results section succeeded goal finding equivalent rewards 
particular key result theorem nmrdp reward function set formulas 
expansion algorithm constructs expansion 
constructed optimal computed standard techniques 
correspondence section shows agent executing policy behave optimally respect original nmrdp 
note labels determine history kept track execution 
particular suppose policy defined extended space apply nmrdp process starts states 
take extended state unique start state es perform es observation resulting states 
dynamics extended mdp ensure unique es extending reachable es actiona 
execute action es proceed 
note keep track extended state currently get directly observe base states 
properties algorithm section briefly discuss interesting issues raised expansion algorithm 
examining phase noted possible implementation ls label sets consisting subformulas 
advantage choice phase trivial pi length bound number subformulas generate 
furthermore bound size 
ls base state receive number distinct labels 
factor larger phase ii usually generate conceivable labels 
exponential may discouraging simple natural examples number historical distinctions required implementing optimal policy 
instance reward need keep track true leading annotations 
main disadvantage lead unnecessarily fine distinctions histories produced phase ii guaranteed minimal sense having states possible valid expansions 
minimality important separate step phase ii required 
fortunately performed variant standard algorithms minimizing finite state automata hu 
defer discussion full note complexity doing polynomial size 
long produced phase ii manageable size minimization fairly straightforward 
second implementation phase constructs label sets lsw weaker formulas subject stated requirements 
precisely initially set lsw alls 
long finds thats reachable lsw lsw lsw gto lsw 
iterate terminates long careful add different logically equivalent formulas twice lsw 
procedure ensures necessary properties ls natural examples reward formula process terminates quickly generating small label sets 
major reason considering lsw constructed subsequently phase ii guaranteed minimal size 
lsw serious drawback phase potentially complex 
number iterations termination exponential size reward formulas size label sets grow double exponentially 
optimal strategy implement phase lsw reward larger necessary phase ii complexity cause difficulties 
formula proves troublesome revert subformula technique point 
conclude noting phase ii comparison unproblematic 
extended state visited exactly complexity linear size final answer size 
furthermore operations performed phase ii difficult 
steps appear involve theorem proving misleading 
step just model checking furthermore short trajectory particular case done time proportional top ls length 
step performed quickly details depend exactly phase implemented general particular proposals discussed book keeping information recorded phase performed time proportional space limitations prevent providing details 
annotation algorithm appears quite practical 
potential exists exponential relative size generally case exactly really need store lot history necessarily large 
concluding remarks mdps provide useful framework dtp necessary assumptions quite restrictive requiring planning problems encoded unnatural way 
technique weakens impact assumptions requirement markovian state reward 
main contributions methodology natural specification temporally extended rewards algorithm automatically constructs equivalent mdp allowing standard mdp solution techniques construct optimal policies 
number interesting directions extended 
similar techniques cope non markovian dynamics partially observable processes 
addition temporal logics standard forward looking logics process logics potentially similar fashion specify different classes behaviors 
interesting idea compact representations mdps obviate need computation involving individual states 
instance bayes net representations specify actions mdps bdg exploited policy construction 
nmrdp specified way produce new bayes net action descriptions involving variables propositions render underlying reward process markovian expanding states explicitly 
technique expanded mdp large may case lot history necessary note inherent formulating problem mdp automatically constructed 
complexity policy construction typically dominated size state space 
important direction combine policy construction state space expansion 
hope avoid generating expanded states dominance arguments particular reward structure nmrdp 
fahiem bacchus craig boutilier supported canadian government nserc iris programs 
anonymous referees thoughtful reviews 
unfortunate space limitations prevented responding valuable suggestions 
ah alur henzinger 
real time logics complexity expressiveness 
lics philadelphia 
bd boutilier dearden 
abstractions decision theoretic planning time constraints 
aaai pp seattle 
bdg boutilier dearden goldszmidt 
exploiting structure policy construction 
ijcai pp montreal 
bel bellman 
dynamic programming 
princeton university press princeton 
bp boutilier puterman 
process oriented planning average reward optimality 
ijcai pp montreal 
dk dean kanazawa 
model reasoning persistence causation 
comp 
intel 
dean kaelbling kirman nicholson 
planning deadlines stochastic domains 
aaai pp washington 
drummond 
situated control rules 
kr pp toronto 
eme emerson 
temporal modal logic 
van leeuwen ed handbook theor 
comp 
sci vol pp 
gk godefroid kabanza 
efficient reactive planner synthesizing reactive plans 
aaai pp 
hh haddawy hanks 
representations decisiontheoretic planning utility functions deadline goals 
kr pp cambridge 
howard 
dynamic programming markov processes 
mit press cambridge 
hu hopcroft ullman 
automata theory languages computation 
addison wesley 
kab kabanza 
synthesis reactive plans multi path environments 
aaai pp 
kushmerick hanks weld 
algorithm probabilistic commitment planning 
aaai pp seattle 
littman dean kaelbling :10.1.1.108.2266
complexity solving markov decision problems 
uai pp montreal 
put puterman 
markov decision processes discrete stochastic dynamic programming 
wiley new york 
sch schoppers 
universal plans reactive robots unpredictable environments 
ijcai milan 
tr russell 
control strategies stochastic planner 
aaai seattle 
