continuous valued learning vision guided behavior acquisition takahashi takeda minoru asada dept adaptive machine systems graduate school engineering osaka university osaka japan takeda asada er ams eng osaka ac jp learning widely reinforcement learning method normally needs defined quantized state action spaces converge 
difficult applied real robot tasks poor performance learned behavior new problem state space construction 
proposes continuous valued learning real robot applications calculates contribution values estimate continuous action value order motion smooth effective 
proposed method obtained better performance desired behavior conventional real valued learning method roughly quantized state action 
show validity method applied method vision guided mobile robot task chase ball 
task simple performance quite impressive 
task simple performance quite impressive 
improvement discussed 
reinforcement learning receiving increased attention method little priori knowledge higher capability reactive adaptive behaviors interactions 
asada series works soccer robot agents chase shoot ball goal pass agent 
reinforcement learning methods state action spaces quantized designer constructed learning process order learning widely reinforcement learning method applicable :10.1.1.47.6702
defined quantized state action spaces needed apply learning real robot tasks 
causes kinds problems performance robot behavior smooth due quantized action commands forward left turn 
state space construction satisfies markovian assumption new problem noted propose continuous valued qlearning real robot applications 
related works far 
time update action values problems rough quantization state space action corresponds state transition 
example contribution value representative state time update wx sampling time case mobile robot tv camera mounted physical action ex 
forward motion command may result small change camera image means action sufficient cause state transition 
value updating just physical actions state transition causes underestimate value state action pair learner acquire appropriate policy 
asada called state action deviation problem re defined action series kind physical action primitives causes state transition :10.1.1.47.6702
physical action primitive repeated state transition 
avoid problem update value eq physical fixed time step sampled time step considering time update 
specify time update value follows time update moment contribution value representative state maximum value reaches maximum see fig moment situation doesn change specific period 
manner policy state changed learner update action value function :10.1.1.47.6702
asada called state action deviation problem re defined action series kind physical action primitives causes state transition :10.1.1.47.6702
physical action primitive repeated state transition 
avoid problem update value eq physical fixed time step sampled time step considering time update 
specify time update value follows time update moment contribution value representative state maximum value reaches maximum see fig moment situation doesn change specific period 
manner policy state changed learner update action value function :10.1.1.47.6702
experimental results task real mobile robot order show validity proposed method apply method mobile robot task chase ball vision guided behavior acquisition 
fig shows picture mobile robot designed built ball chase shown 
simple color image processing hitachi ip applied detect ball area image real time ms 
robot tv camera visual angles horizontal vertical directions respectively 
research supported japan society promotion science research program titled cooperative distributed vision dynamic dimensional scene understanding image sequence robot chasing ball 
mahadevan editors 
robot learning 
kluwer academic publishers 
asada noda hosoda :10.1.1.47.6702
purposive behavior acquisition real robot vision reinforcement learning 
machine learning 
uchibe asada hosoda 
behavior coordination mobile robot modular reinforcement learning 
