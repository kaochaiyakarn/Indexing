distal inter pattern distance constructive learning algorithm yang parekh vasant honavar ai research group department computer science hall iowa state university ames ia 
multi layer networks threshold logic units offer attractive framework design pattern classification systems 
new constructive neural network learning algorithm distal inter pattern distance introduced 
distal constructs single hidden layer spherical threshold neurons 
neuron designed exclude cluster training patterns belonging class 
weights thresholds hidden neurons determined directly comparing inter pattern distances training patterns 
offers significant advantage constructive learning algorithms iterative time consuming weight modification strategy train individual neurons 
individual clusters represented hidden neurons combined single output layer threshold neurons 
speed distal candidate datamining knowledge acquisition large datasets 
results experiments artificial real world datasets show distal compares favorably neural network learning algorithms pattern classification 
keywords constructive neural network learning algorithm inter pattern distance layer networks threshold logic units tlu offer attractive framework design trainable pattern classification systems number reasons including potential parallelism fault noise tolerance significant representational computational efficiency disjunctive normal form dnf expressions decision trees simpler digital hardware implementations continuous counterparts sigmoid neurons networks trained error backpropagation algorithm :10.1.1.16.9105
tlu implements dimensional hyperplane partitions dimensional euclidean pattern space regions 
single tlu sufficient classify patterns classes linearly separable 
number learning algorithms guaranteed find tlu weight setting correctly classifies linearly separable pattern set proposed literature 
set patterns linearly separable multi layer network needed learn complex decision boundary necessary correctly classify training examples 
number hidden neurons hidden layer connections neuron defined priori classification task 
done basis problem specific knowledge available ad hoc fashion requiring process trial error 
ffl adaptive topology networks topology target network determined dynamically introducing new neurons layers connections controlled fashion generative constructive learning algorithms 
cases pruning mechanisms discard redundant neurons connections conjunction network construction mechanisms 
constructive algorithms offer advantages conventional backpropagation style learning approaches :10.1.1.16.9105:10.1.1.31.6882
ffl obviate need ad hoc priori choice network topology 
ffl guaranteed converge zero classification errors finite non contradictory datasets 
ffl elementary threshold logic units tlu trained perceptron style weight update rules 
ffl involve extensive parameter fine tuning 
ffl obviate need ad hoc priori choice network topology 
ffl guaranteed converge zero classification errors finite non contradictory datasets 
ffl elementary threshold logic units tlu trained perceptron style weight update rules 
ffl involve extensive parameter fine tuning 
ffl provide natural framework exploiting problem specific knowledge initial network configuration heuristic knowledge network construction algorithm :10.1.1.51.4461
constructive algorithms incrementally construct networks threshold neurons category pattern classification tasks proposed literature 
include tower pyramid tiling perceptron cascade sequential 
provably correct extensions algorithms handle multiple output classes real valued pattern attributes proposed see :10.1.1.16.9105
exception sequential learning algorithm constructive learning algorithms idea transforming hard task determining necessary network topology weights subtasks ffl incremental addition threshold neurons network existing network topology fails achieve desired classification accuracy training set 
ffl involve extensive parameter fine tuning 
ffl provide natural framework exploiting problem specific knowledge initial network configuration heuristic knowledge network construction algorithm :10.1.1.51.4461
constructive algorithms incrementally construct networks threshold neurons category pattern classification tasks proposed literature 
include tower pyramid tiling perceptron cascade sequential 
provably correct extensions algorithms handle multiple output classes real valued pattern attributes proposed see :10.1.1.16.9105
exception sequential learning algorithm constructive learning algorithms idea transforming hard task determining necessary network topology weights subtasks ffl incremental addition threshold neurons network existing network topology fails achieve desired classification accuracy training set 
ffl training added threshold neuron variant perceptron training algorithm pocket algorithm improve classification accuracy network 
case sequential learning algorithm hidden neurons added trained appropriate weight training rule exclude patterns belonging class rest pattern set 
timeconsuming iterative nature perceptron training algorithm considerably faster corresponding error guided backpropagation training algorithms impractical large datasets largescale datamining knowledge acquisition tasks especially applications reasonably accurate classifiers learned real time 
exception sequential learning algorithm constructive learning algorithms idea transforming hard task determining necessary network topology weights subtasks ffl incremental addition threshold neurons network existing network topology fails achieve desired classification accuracy training set 
ffl training added threshold neuron variant perceptron training algorithm pocket algorithm improve classification accuracy network 
case sequential learning algorithm hidden neurons added trained appropriate weight training rule exclude patterns belonging class rest pattern set 
timeconsuming iterative nature perceptron training algorithm considerably faster corresponding error guided backpropagation training algorithms impractical large datasets largescale datamining knowledge acquisition tasks especially applications reasonably accurate classifiers learned real time 
similarly hybrid learning systems neural network learning inner loop complex optimization process feature subset selection genetic algorithm evaluation fitness solution requires training neural network subset input features represented solution evaluating classification accuracy call fast neural network training algorithm :10.1.1.30.3522
instance learning ibl approach learning learning algorithm typically stores training examples prototypes :10.1.1.37.4690:10.1.1.138.635
prototype stored ordered pair pattern represented chosen instance language typically form vector attribute values class belongs 
system classify new pattern uses distance function euclidean distance case real valued patterns computes distance stored prototype predicts classification known classification nearest prototype prototypes 
algorithms referred nearest neighbor techniques investigated researchers pattern recognition case reasoning artificial neural networks cognitive psychology text classification 
ffl training added threshold neuron variant perceptron training algorithm pocket algorithm improve classification accuracy network 
case sequential learning algorithm hidden neurons added trained appropriate weight training rule exclude patterns belonging class rest pattern set 
timeconsuming iterative nature perceptron training algorithm considerably faster corresponding error guided backpropagation training algorithms impractical large datasets largescale datamining knowledge acquisition tasks especially applications reasonably accurate classifiers learned real time 
similarly hybrid learning systems neural network learning inner loop complex optimization process feature subset selection genetic algorithm evaluation fitness solution requires training neural network subset input features represented solution evaluating classification accuracy call fast neural network training algorithm :10.1.1.30.3522
instance learning ibl approach learning learning algorithm typically stores training examples prototypes :10.1.1.37.4690:10.1.1.138.635
prototype stored ordered pair pattern represented chosen instance language typically form vector attribute values class belongs 
system classify new pattern uses distance function euclidean distance case real valued patterns computes distance stored prototype predicts classification known classification nearest prototype prototypes 
algorithms referred nearest neighbor techniques investigated researchers pattern recognition case reasoning artificial neural networks cognitive psychology text classification 
distance techniques related radial basis function networks 
weight vector neuron defines center regions thresholds determine boundaries regions relative choice distance metric 
choice appropriate distance metric hidden layer neurons critical achieving clustering performance 
different distance metrics represent different notions distance pattern space 
impose different inductive biases learning algorithm 
consequently researchers investigated alternative distance functions instance learning :10.1.1.48.9623
number distribution clusters result specific choices distance functions function distribution patterns clustering strategy 
difficult identify best distance metric absence knowledge distribution patterns pattern space chose explore number different distance metrics proposed literature 
distance patterns skewed attributes high values 
normalization individual attributes overcomes problem distance computation 
number distribution clusters result specific choices distance functions function distribution patterns clustering strategy 
difficult identify best distance metric absence knowledge distribution patterns pattern space chose explore number different distance metrics proposed literature 
distance patterns skewed attributes high values 
normalization individual attributes overcomes problem distance computation 
normalization achieved dividing pattern attribute range possible values attribute times standard deviation attribute :10.1.1.48.9623
normalization allows attributes nominal missing values considered distance computation 
distance attributes nominal values say attribute values computed follows ffl overlap ol :10.1.1.48.9623
ffl value difference vd gamma number patterns training set value attribute number patterns training set value attribute output class number output classes constant euclidean manhattan missing value patterns distance component entire pattern vector taken 
delta delta delta delta delta delta pattern vectors 
distance patterns skewed attributes high values 
normalization individual attributes overcomes problem distance computation 
normalization achieved dividing pattern attribute range possible values attribute times standard deviation attribute :10.1.1.48.9623
normalization allows attributes nominal missing values considered distance computation 
distance attributes nominal values say attribute values computed follows ffl overlap ol :10.1.1.48.9623
ffl value difference vd gamma number patterns training set value attribute number patterns training set value attribute output class number output classes constant euclidean manhattan missing value patterns distance component entire pattern vector taken 
delta delta delta delta delta delta pattern vectors 
max min oe maximum minimum standard deviation values ith attribute patterns dataset respectively 
distance different choices distance metric defined follows 
network allowed train achieved classification accuracy training set 
best generalization process training reported 
details experimental set datasets see 
thorough comparison algorithm existing pattern classification algorithms scope project 
follows compare performance distal results nearest neighbor algorithm reported :10.1.1.48.9623
see table ii distal gave comparable results algorithms datasets vowel despite fast execution 
case vowel dataset nearest neighbor algorithm reports higher accuracy distal :10.1.1.48.9623
best results reported literature vowel dataset 
table ii comparison generalization accuracy various algorithms 
details experimental set datasets see 
thorough comparison algorithm existing pattern classification algorithms scope project 
follows compare performance distal results nearest neighbor algorithm reported :10.1.1.48.9623
see table ii distal gave comparable results algorithms datasets vowel despite fast execution 
case vowel dataset nearest neighbor algorithm reports higher accuracy distal :10.1.1.48.9623
best results reported literature vowel dataset 
table ii comparison generalization accuracy various algorithms 
distal results approach nn best results :10.1.1.48.9623
dataset distal nn annealing audiology bridge cancer credit flag glass heart heart cleveland heart hungary heart heart switzerland hepatitis horse ionosphere iris liver pima promoters sonar soybean large soybean small vehicle votes vowel wine zoo iv 
see table ii distal gave comparable results algorithms datasets vowel despite fast execution 
case vowel dataset nearest neighbor algorithm reports higher accuracy distal :10.1.1.48.9623
best results reported literature vowel dataset 
table ii comparison generalization accuracy various algorithms 
distal results approach nn best results :10.1.1.48.9623
dataset distal nn annealing audiology bridge cancer credit flag glass heart heart cleveland heart hungary heart heart switzerland hepatitis horse ionosphere iris liver pima promoters sonar soybean large soybean small vehicle votes vowel wine zoo iv 
summary discussion fast inter pattern distance constructive learning algorithm distal introduced performance number datasets demonstrated 
despite simplicity distal performance artificial real world datasets considered 
simplicity distal rapid convergence large datasets suitable practical pattern classification tasks encountered largescale datamining knowledge acquisition involve large datasets 
dataset distal nn annealing audiology bridge cancer credit flag glass heart heart cleveland heart hungary heart heart switzerland hepatitis horse ionosphere iris liver pima promoters sonar soybean large soybean small vehicle votes vowel wine zoo iv 
summary discussion fast inter pattern distance constructive learning algorithm distal introduced performance number datasets demonstrated 
despite simplicity distal performance artificial real world datasets considered 
simplicity distal rapid convergence large datasets suitable practical pattern classification tasks encountered largescale datamining knowledge acquisition involve large datasets 
applied distal task feature subset selection obtaining optimal feature subset entire set input attributes genetic algorithms :10.1.1.30.3522
distal worked fairly terms speed generalization feature subset selection task 
potential disadvantage distal need maintaining inter pattern distance matrix learning 
memory needed store matrix grows quadratically size training set 
problem mitigated freeing memory patterns excluded new hidden neuron learning progresses 
potential disadvantage distal need maintaining inter pattern distance matrix learning 
memory needed store matrix grows quadratically size training set 
problem mitigated freeing memory patterns excluded new hidden neuron learning progresses 
interesting explore variants distal avoid need maintaining entire inter pattern distance matrix learning 
constructive algorithms general provide natural framework exploration cumulative life long learning knowledge theory refinement :10.1.1.51.4461
interesting direction research explore distal task real world datasets genome data :10.1.1.51.4461
gallant neural network learning expert systems mit press cambridge ma 

chen parekh yang balakrishnan honavar analysis decision boundaries generated constructive neural network learning algorithms proceedings july washington vol 
memory needed store matrix grows quadratically size training set 
problem mitigated freeing memory patterns excluded new hidden neuron learning progresses 
interesting explore variants distal avoid need maintaining entire inter pattern distance matrix learning 
constructive algorithms general provide natural framework exploration cumulative life long learning knowledge theory refinement :10.1.1.51.4461
interesting direction research explore distal task real world datasets genome data :10.1.1.51.4461
gallant neural network learning expert systems mit press cambridge ma 

chen parekh yang balakrishnan honavar analysis decision boundaries generated constructive neural network learning algorithms proceedings july washington vol 
pp 

chen parekh yang balakrishnan honavar analysis decision boundaries generated constructive neural network learning algorithms proceedings july washington vol 
pp 

parekh yang honavar constructive neural network learning algorithms multi category real valued pattern classification tech :10.1.1.16.9105
rep cs tr department computer science iowa state university 
rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations microstructure cognition vol 
foundations 
mit press cambridge massachusetts 
pp 

parekh yang honavar pruning strategies constructive neural network learning algorithms proceedings ieee international conference neural networks icnn pp 

honavar generative learning structures processes generalized connectionist networks ph thesis university wisconsin madison :10.1.1.31.6882
honavar uhr generative learning structures generalized connectionist networks information sciences vol 
pp 

parekh honavar constructive theory refinement knowledge neural networks proceedings international joint conference neural networks appear :10.1.1.51.4461
honavar generative learning structures processes generalized connectionist networks ph thesis university wisconsin madison :10.1.1.31.6882
honavar uhr generative learning structures generalized connectionist networks information sciences vol 
pp 

parekh honavar constructive theory refinement knowledge neural networks proceedings international joint conference neural networks appear :10.1.1.51.4461
nadal study growth algorithm feedforward network international journal neural systems vol 
pp 

gallant perceptron learning algorithms ieee transactions neural networks vol 

convergence theorem sequential learning layer perceptrons letters vol 
pp 

yang honavar feature subset selection genetic algorithm feature extraction construction selection data mining perspective :10.1.1.30.3522
kluwer academic ny appear 
aha kibler albert instance learning algorithms machine learning vol :10.1.1.138.635
pp 

pp 

yang honavar feature subset selection genetic algorithm feature extraction construction selection data mining perspective :10.1.1.30.3522
kluwer academic ny appear 
aha kibler albert instance learning algorithms machine learning vol :10.1.1.138.635
pp 

turney theoretical analyses cross validation error voting instance learning journal experimental theoretical artificial intelligence pp :10.1.1.37.4690

kluwer academic ny appear 
aha kibler albert instance learning algorithms machine learning vol :10.1.1.138.635
pp 

turney theoretical analyses cross validation error voting instance learning journal experimental theoretical artificial intelligence pp :10.1.1.37.4690

domingos rule induction instance learning unified approach proceedings international joint conference artificial intelligence ijcai 
cover hart nearest neighbor pattern classification ieee transactions information theory vol 
pp 

langley elements machine learning morgan kaufmann palo alto ca 
mitchell machine learning mcgraw hill new york 
duda hart pattern classification scene analysis wiley new york 
wilson martinez improved heterogeneous distance functions journal artificial intelligence research vol :10.1.1.48.9623
pp 

andersen constructive algorithm training multilayer perceptron genetic algorithm complex systems vol 
pp 
