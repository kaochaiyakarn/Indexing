supervised learning incomplete data em approach zoubin ghahramani michael jordan department brain cognitive sciences massachusetts institute technology cambridge ma cowan tesauro alspector 
eds 
advances neural information processing systems 
morgan kaufmann publishers san francisco ca 
email zoubin mit edu 
real world learning tasks may involve high dimensional data sets arbitrary patterns missing data 
morgan kaufmann publishers san francisco ca 
email zoubin mit edu 
real world learning tasks may involve high dimensional data sets arbitrary patterns missing data 
framework maximum likelihood density estimation learning data sets 
mixture models density estimates distinct appeals em principle dempster deriving learning algorithm em estimation mixture components coping missing data :10.1.1.133.4884
resulting algorithm applicable wide range supervised unsupervised learning problems 
results classification benchmark iris data set 
adaptive systems generally operate environments imperfections cope imperfections learn extract relevant information needed particular goals 
form incompleteness sensing information 
estimate inverse gamma relation subsets elements concatenated vector 
second density approach applicable supervised learning unsupervised learning exactly way 
distinction supervised unsupervised learning framework portion data vector denoted input portion target 
third discuss density approach deals naturally incomplete data missing values data set 
problem estimating mixture densities viewed missing data problem labels component densities missing expectation maximization em algorithm dempster developed handle kinds missing data :10.1.1.133.4884
density estimation em section outlines basic learning algorithm finding maximum likelihood parameters mixture model dempster duda hart nowlan :10.1.1.133.4884
assume data fx xn generated independently mixture density 

component mixture denoted parametrized equation independence assumption see log likelihood parameters data set jx log 
second density approach applicable supervised learning unsupervised learning exactly way 
distinction supervised unsupervised learning framework portion data vector denoted input portion target 
third discuss density approach deals naturally incomplete data missing values data set 
problem estimating mixture densities viewed missing data problem labels component densities missing expectation maximization em algorithm dempster developed handle kinds missing data :10.1.1.133.4884
density estimation em section outlines basic learning algorithm finding maximum likelihood parameters mixture model dempster duda hart nowlan :10.1.1.133.4884
assume data fx xn generated independently mixture density 

component mixture denoted parametrized equation independence assumption see log likelihood parameters data set jx log 

em algorithm mixture models iterative method solving credit assignment problem 
intuition access hidden random variable indicated data point generated component maximization problem decouple set simple 
indicator variable complete data log likelihood function written jx ij log jz involve log summation 
unknown utilized directly expectation denoted 
shown dempster jx maximized iterating steps step jx jx step arg max expectation step computes expected complete data log likelihood maximization step finds parameters maximize likelihood :10.1.1.133.4884
steps form basis em algorithm sections outline real discrete density estimation 
real valued data mixture gaussians real valued data modeled mixture gaussians 
model step simplifies computing ij ij jx probability gaussian defined parameters estimated time step generated data point ij sigma gamma expf gamma gamma sigma gamma gamma sigma gamma expf gamma gamma sigma gamma gamma step re estimates means covariances gaussians data set weighted ij ij ij sigma ij gamma gamma ij derivation assumes equal priors gaussians priors viewed mixing parameters learned maximization step 
discrete valued data mixture dimensional binary data xd modeled mixture bernoulli densities 
xj 
xd jd gamma jd gammax model step involves computing ij id jd gamma jd gammax id id ld gamma ld gammax id step re estimates parameters ij ij generally discrete categorical data modeled generated mixture multinomial densities similar derivations learning algorithm applied 
extension data mixed real binary categorical dimensions readily derived assuming joint density mixed components types 
learning incomplete data previous section aspect em algorithm learning mixture models 
important application em learning data sets missing values little rubin dempster :10.1.1.133.4884
application pursued statistics literature non mixture density estimation problems combine application em learning mixture parameters 
assume data set fx xn divided observed component missing component similarly data vector divided data vector different missing components denoted superscript simplified notation sake clarity 
handle missing data rewrite em algorithm follows step jx jx step arg max comparing equation see aside indicator variables added second form incomplete data corresponding missing values data set 
step algorithm estimates forms missing information essence uses current estimate data density complete missing values 
