machine learning 
improved boosting algorithms confidence rated predictions schapire research att com labs shannon laboratory park avenue room florham park nj singer research att com labs shannon laboratory park avenue room florham park nj 
describe improvements freund schapire adaboost boosting algorithm particularly setting hypotheses may assign confidences predictions 
give simplified analysis adaboost setting show analysis find improved parameter settings refined criterion training weak hypotheses 
give specific method assigning confidences predictions decision trees method closely related quinlan 
method suggests technique growing decision trees turns identical proposed kearns mansour 
focus apply new boosting algorithms multiclass classification problems particularly multi label case example may belong class 
give boosting methods problem plus third method output coding 
leads new method handling single label case simpler effective techniques suggested freund schapire 
give experimental results comparing algorithms discussed 
keywords boosting algorithms multiclass classification output coding decision trees 
boosting method finding highly accurate hypothesis classification rule combining weak hypotheses moderately accurate 
typically weak hypothesis simple rule generate predicted classification instance 
study boosting extended framework weak hypothesis generates predicted classifications self rated confidence scores estimate reliability predictions 
essential questions arise studying problem boosting paradigm 
modify known boosting algorithms designed handle simple predictions confidence rated predictions effective manner possible 
second design weak learners predictions confidence rated manner described 
give answers questions 
result powerful set boosting methods handling expressive weak hypotheses advanced methodology designing weak learners appropriate boosting algorithms 
base freund schapire adaboost algorithm received extensive empirical theoretical study bauer kohavi appear breiman dietterich appear dietterich bakiri drucker cortes freund schapire maclin opitz dietterich quinlan schapire schapire freund bartlett lee schwenk bengio :10.1.1.32.8860:10.1.1.105.6964:10.1.1.49.2457:10.1.1.72.7289:10.1.1.133.1040:10.1.1.31.2869
boost confidence rated predictions propose generalization adaboost main parameters tuned number methods describe detail 
intuitively control influence weak hypotheses 
determine proper tuning parameters presenting streamlined version freund schapire analysis provides clean upper bound training error adaboost parameters left unspecified 
purposes minimizing training error analysis provides immediate clarification criterion setting discussed analysis provides criterion weak learner formulating weak hypotheses 
analysis give number methods choosing show optimal tuning respect criterion numerically general give exact methods setting special cases 
freund schapire considered case individual predictions weak hypotheses allowed carry confidence 
show setting approximation optimal tuning techniques 
discuss methods designing weak learners confidence rated predictions criterion provided analysis 
weak hypotheses partition instance space small number equivalent prediction regions decision trees analyze simple method automatically assigning level confidence predictions region 
method turns closely related heuristic method proposed quinlan boosting decision trees 
analysis viewed partial theoretical justification experimentally successful method 
technique leads modified criterion selecting domain partitioning weak hypotheses 
words weak learner simply choosing weak hypothesis low training error usually done past show theoretically methods best combined weak learner minimizes alternative measure badness growing decision trees measure turns identical earlier proposed kearns mansour 
primarily focus minimizing training error outline methods analyze generalization error 
show extend methods described binary classification problems multiclass case generally multi label case example may belong class 
problems arise naturally instance text categorization problems document say news article may easily relevant topic politics sports 
freund schapire gave algorithms boosting multiclass problems designed handle multi label case 
new extensions adaboost multi label problems 
cases show apply results half new extensions 
extension learned hypothesis evaluated terms ability predict approximation set labels associated instance 
special case obtain novel boosting algorithm multiclass problems conventional single label case 
algorithm simpler apparently effective methods freund schapire 
addition propose analyze modification method combines techniques dietterich bakiri method 
method combining boosting output coding proposed schapire 
superficially similar method fact quite different 
second extension multi label problems learned hypothesis predicts instance ranking labels evaluated ability place correct labels high ranking 
freund schapire adaboost special case method single label problems 
primary focus theoretical issues give experimental results comparing new algorithms 
obtain especially dramatic improvements performance fairly large amount data available large text categorization problems 

generalized analysis adaboost sequence training examples instance belongs domain instance space label belongs finite label space focus binary classification problems assume access weak base learning algorithm accepts input sequence training examples distribution 
indices input weak learner computes weak base hypothesis predicted label general form interpret sign assigned instance magnitude gf confidence prediction 
close far zero interpreted low high confidence prediction 
range may generally include real numbers restrict range 
idea boosting weak learner form highly accurate prediction rule calling weak learner repeatedly different distributions training examples 
slightly generalized version freund schapire adaboost algorithm shown 
main effect adaboost update rule assuming decrease increase weight training examples classified correctly incorrectly examples agree disagree sign 
version differs freund schapire weak hypotheses range restricted mn range assumed freund schapire freund schapire prescribe specific choice leave choice unspecified discuss various 
despite differences continue refer algorithm adaboost discussed range restricted mp choose appropriately obtain freund schapire original adaboost algorithm ignoring superficial differences notation 
give simplified analysis algorithm left unspecified 
analysis yields improved general method choosing fn proof unraveling update rule 
initialize 
train weak learner distribution get weak hypothesis choose update normalization factor chosen distribution 
output final hypothesis 
generalized version adaboost 
predicate mm holds 
prove bound training error theorem assuming notation bound holds training error 
implying mm oo 
deriving freund schapire choice showing freund schapire version adaboost derived special case new version 
weak hypotheses range mp choice obtained approximating follows upper bound valid mn fact exact range 
proof bound follows immediately convexity constant analytically choose minimize right hand side eq 
giving plugging eq 
choice gives upper bound proved corollary theorem equivalent freund schapire theorem corollary freund schapire notation assume mn range choose training error setting reasonable try find maximizes round boosting 
quantity natural measure correlation predictions labels respect distribution closely related ordinary error range mp mn maximizing equivalent minimizing error 
generally range equivalent definition error freund schapire notation 
approximation eq 
essentially linear upper bound function range clearly upper bounds give tighter approximation quadratic piecewise linear approximation 

numerical method general case give general numerical method exactly minimizing respect recall goal find minimizes derivative definition formed value minimizes words means respect distribution weak hypothesis exactly uncorrelated labels easily verified strictly positive ignoring trivial case 
zero 
see appendix 
exists similarly root degenerate case non zero sign 
furthermore strictly increasing numerically find unique minimum simple binary search sophisticated numerical methods 
summarizing argued theorem means 
assume set 
includes positive negative values 
exists unique choice minimizes 
choice oa 
analytic method weak hypotheses abstain consider natural special case choice computed analytically numerically 
suppose range weak hypothesis restricted words weak hypothesis definitive prediction label abstain predicting levels confidence allowed 
allowing weak hypothesis effectively say don know introduce model analogous specialist model blum studied freund 

fixed defined continue omit subscript clear context 
readability notation abbreviate subscripts symbols written written calculate easily verified minimized setting case freund schapire original adaboost algorithm conservative choice giving value necessarily inferior eq 
freund schapire able upper bound range choices resulting values identical 

criterion finding weak hypotheses far discussed theorem choose general theorem applied broadly guide design weak learning algorithms combined boosting 
past assumed goal weak learning algorithm find weak hypothesis small number errors respect distribution training samples 
results suggest different criterion 
particular attempt greedily minimize upper bound training error theorem minimizing round 
weak learner attempt find weak hypothesis minimizes expression simplified folding words assuming loss generality weak learner freely scale weak hypothesis constant factor omitting subscripts weak learner goal minimize algorithms may possible appropriate modifications handle loss function directly 
instance gradient algorithms backprop easily modified minimize eq 
traditional mean squared error 
show decision tree algorithms modified new criterion finding weak hypotheses 

domain partitioning weak hypotheses focus weak hypotheses predictions partitioning domain specific weak hypothesis associated partition disjoint blocks cover words prediction depends block instance falls 
prime example hypothesis decision 
tree leaves define partition domain 
suppose partition space 
predictions block partition 
words find function respects partition minimizes eq 

goal find appropriate choices weighted fraction examples fall block rewritten label eq 
standard calculus see minimized plugging eq 
choice gives note sign equal weighted majority class block close zero low confidence prediction roughly equal split positive negative examples block likewise far zero label strongly 
similar scheme previously proposed quinlan assigning confidences predictions leaves decision tree 
scheme differed details feel new theory provides partial justification method 
criterion eq 
splitting criterion growing decision tree gini index entropic function 
words decision tree built greedily choosing split causes greatest drop value function eq 

fact exactly splitting criterion proposed kearns mansour 
furthermore wants boost decision tree tree built splitting criterion eq 
predictions leaves boosted trees eq 


smoothing predictions scheme requires predict eq 
block may happen small zero case large infinite magnitude 
practice large predictions may cause numerical problems 
addition may theoretical reasons suspect large overly confident predictions increase tendency overfit 
limit magnitudes predictions suggest smoothed values appropriately small positive value bounded effect bounding smoothing slightly weakens value plugging eq 
gives second inequality inequality nonnegative inequality fact implies recall number blocks partition 
comparing eqs 
see greatly degraded smoothing choose experiments typically order number training examples 

generalization error far focused training error primary objective achieve low generalization error 
methods analyzing generalization error adaboost proposed 
freund schapire uses standard vc theory bound generalization error final hypothesis terms training error additional term function vc dimension final hypothesis class number training examples 
vc dimension final hypothesis class computed methods baum haussler 
derived upper bound qualitative prediction behavior analysis suggests adaboost overfit run rounds 
schapire 
proposed alternative analysis explain adaboost empirically observed resistance overfitting 
bartlett method margins achieved final hypothesis training examples 
margin measure confidence prediction 
schapire show larger margins imply lower generalization error regardless number rounds 
show adaboost tends increase margins training examples 
large extent analysis carried current context focus section 
step applying theory assume weak hypothesis bounded range 
recall final hypothesis form bounded care sign rescale normalize allowing assume loss generality assume mp belongs hypothesis space schapire define margin labeled example margin mn positive correct prediction example 
regard magnitude margin measure confidence prediction 
schapire results applied directly context special case range case interest focus weak hypotheses real valued predictions 
extend margins theory define definitions see instance haussler 
method sketched section schapire haussler long lemma prove upper bound generalization error holds probability form denotes probability respect choosing example uniformly random training set 
term fraction training examples margin proof outline bound communicated peter bartlett provided appendix note mentioned section margin analysis suggests may bad idea allow weak hypotheses predictions large magnitude 
gf large rescaling leads large coefficient turn may overwhelm coefficients may dramatically reduce margins training examples 
turn theory detrimental effect generalization error 
remains seen theoretical effect observed practice alternatively improved theory developed 

multiclass multi label classification problems class show methods extended multiclass case may possible labels classes 
consider general multi label case single example may belong number classes 
formally finite set labels classes traditional classification setting example assigned single possibly stochastic process labeled examples pairs goal typically instance may belong multiple labels find hypothesis minimizes probability newly observed example multi label case labeled example pair set labels assigned single label case clearly special case observations 
unclear setting precisely formalize goal learning algorithm general right formalization may depend problem hand 
possibility seek hypothesis attempts predict just labels assigned example 
words goal find minimizes probability new observation call measure hypothesis measures probability getting labels correct 
denote error hypothesis respect distribution observations err err note single label classification problems error identical ordinary error 
sections introduce loss measures multi label setting hamming loss ranking loss 
discuss modifications adaboost appropriate case 

hamming loss multiclass problems suppose goal predict correct labels 
words learning algorithm generates hypothesis predicts sets labels loss depends predicted set differs observed 
respect distribution loss 
lg initialize 
train weak learner distribution get weak hypothesis choose update lg lg normalization factor chosen distribution 
output final hypothesis 
adaboost mh multiclass multi label version adaboost hamming loss 
denotes symmetric difference 
leading meant merely ensure value call measure hamming loss denote minimize hamming loss natural way decompose problem orthogonal binary classification problems 
think specifying binary labels depending label included 
similarly viewed binary predictions 
hamming loss regarded average error rate binary problems 
define oa simplify notation identify function corresponding argument function defined reduction binary classification mind straightforward see boosting minimize hamming loss 
main idea reduction simply replace training example examples result boosting algorithm called adaboost mh shown maintains distribution examples labels round weak learner accepts distribution training set generates weak hypothesis 
reduction leads choice final hypothesis shown 
reduction derive algorithm combined theorem immediately implies bound hamming loss final hypothesis theorem assuming notation bound holds hamming loss training data apply ideas preceding sections binary classification problem 
goal minimize lg round 
understood sum examples indexed labels section require range choose lg gm gives goal weak learner maximization note equal thought weighted hamming loss respect example 
example maximize suppose goal find oblivious weak hypothesis ignores instance predicts basis label omit argument write omit subscripts 
symmetry minimizing find maximizes equivalent maximizing need lg clearly maximized setting lg 
domain partitioning weak hypotheses section 

combine ideas section domain partitioning weak space natural create partitions form consisting sets appropriate hypothesis hypotheses 
suppose associated partition formed predicts results section choose lg gmm oa oo gives 
relation error single label classification algorithms goal minimize error 
natural way set predict label predicted weak hypotheses 
simple theorem relates error hamming loss theorem respect distribution observations err proof assume suppose argue implies maximum eq 
positive maximum nonpositive case mm oo expectations implies theorem 
particular means adaboost mh applied single label multiclass classification problems 
resulting bound training error final hypothesis eq 

fact results section imply better bound leading constant improved somewhat assuming loss generality prior examining data th weak hypothesis chosen weak hypothesis minimized setting gives plugging bound eq 
get improved bound hack equivalent modifying algorithm manner correct label lg unaffected 
initialized 
specifically chosen note 
output coding multiclass problems method maps single label problem multi label problem simplest obvious way mapping single label observation multi label observation may effective sophisticated mapping 
general define mapping map observation note maps subsets unspecified label set need desirable choose function maps different labels sets far say terms symmetric difference 
essentially approach advocated dietterich bakiri somewhat different setting suggested error correcting codes designed exactly property 
alternatively small expect get similar effect choosing entirely random include include equal probability 
function directly transformed training data classify new instance chosen apply adaboost mh direct dietterich bakiri label mapped output code shortest hamming distance choose approach evaluate obtain set choose gfp weakness approach ignores confidence label included included alternative approach predict label paired training set caused smallest weight final distribution 
words suggest predicting label 
mapping 
variant gm run adaboost mh relabeled 
data get back final hypothesis form output modified final hypothesis variant 
adaboost mo multiclass version adaboost output codes 
call version boosting output codes adaboost mo pseudocode 
theorem formalizes intuitions giving bound training error terms quality code measured minimum distance pair code words theorem assuming notation viewed subroutine run choice training error adaboost mo upper bounded fp variant variant 
proof start variant 
suppose modified output hypothesis variant mistake example means fp mistake occurs gm implies second inequality uses fact sets case error hand hamming error adaboost mh training set definition 
theorem 
number training mistakes gf implies stated bound 
variant suppose error example fixing define note gm eq 
implies oa 
implies third inequality uses fact shown number training errors variant means equality uses main argument proof theorem combined reduction binary classification described just prior theorem 
immediately implies stated bound 
code chosen random uniformly possible codes large expect approach case leading coefficients bounds theorem approach variant variant independent number classes original label set theorem improve bound eq 
adaboost mh eq 

apply theorem code defined clearly case 
claim defined eq 
produces identical predictions generated variant adaboost mo clearly minimum eq 
attained maximized 
applying theorem gives bound eq 


ranking loss multiclass problems section looked problem finding hypothesis exactly identifies labels associated instance 
section consider different variation problem goal find hypothesis ranks labels hope correct labels receive highest ranks 
approach described closely related freund 
boosting general ranking problems 
formal seek hypothesis form interpretation instance labels ordered label considered ranked higher respect observation care relative ordering crucial pairs say crucial pair fails rank goal find function small number labels ranked labels goal minimize expected fraction crucial pairs 
quantity called ranking loss respect distribution observations defined denote measure note assume empty equal observation ranking problem solved case 

gf initialize lg 
train weak learner distribution get weak hypothesis choose update 
lg normalization factor chosen distribution 
lg output final hypothesis 
adaboost multiclass multi label version adaboost ranking loss 

lg version adaboost ranking loss called adaboost shown 
maintain distribution distribution zero relevant triples crucial pair relative weak hypotheses form think providing ranking labels described 
update rule bit new 
crucial pair relative recall zero cases 
assuming momentarily rule decreases weight gives correct ranking increases weight 
prove theorem analogous theorem ranking loss theorem assuming notation bound holds ranking loss training data proof proof similar theorem 
unraveling update rule lg lg ranking loss training set lg mm oo lg sums example indices pairs labels completes theorem 
goal round try minimize 
usual omit subscripts 
apply methods described previous sections 
starting exact methods finding suppose hy appropriate modifications method section find numerically 
alternatively special case range method section choose exactly lg gmm oo case 
find weak hypothesis minimize expression 
simplest case try find best oblivious weak hypothesis 
interesting open problem distribution find oblivious hypothesis 
minimizes defined eqs 

suspect problem may np complete size fixed 
know analytically find best oblivious hypothesis restrict range numerical methods may reasonable 
note finding best oblivious hypothesis simplest case natural extension technique section ranking loss 
folding minimize find section problem rewritten lg appendix show expressions form eq 
convex discuss minimize expressions 
see expression eq 
general form eq 
identify eq 
exact analytic solutions hard come ranking loss consider approximations section 
assuming weak hypotheses range mp approximation eq 
yields lg right hand side eq 
minimized gives reasonable tractable goal weak learner try maximize example 
find oblivious weak hypothesis 
maximizes note rearranging sums lg lg clearly maximized set note approximation find weak hypothesis weak hypothesis computed weak learner methods choose outlined 
lg lg 
initialize lg 
train weak learner distribution defined eq 
get weak hypothesis choose update lg output final hypothesis 
efficient version adaboost 

efficient implementation method described may time space inefficient labels 
particular naively need maintain weights training example weight updated round 
space complexity time round complexity bad fact algorithm implemented space time round 
nature updates show need maintain weights crucial pair relative maintain condition lg lg lg times 
recall zero triples pseudocode implementation shown 
eq 
proved induction 
clearly holds initially 
inductive hypothesis straightforward expand computation see equivalent computation 
show eq 
holds round crucial pair lg lg lg lg note space requirements round computations possible exception call weak learner 
want weak weights learner maximize eq 
need pass weak learner computed time 
omitting subscripts rewrite lg gm lg lg lg gm lg gm lg lg lg lg weights computed time computing sums appear equation possible cases need pass weights weak learner case full distribution size note eq 
exactly form eq 
means setting weak learner hamming loss ranking loss 

relation error section ranking loss method minimizing error single label problems 
freund schapire pseudoloss algorithm adaboost special case ranking loss data single labeled weak learner attempts maximize eq 
set eq 

natural prediction rule words choose highest ranked label instance show theorem respect distribution observations empty equal err proof suppose respect observation occur pairs expectations gives mm oo proves theorem 

experiments section describe experiments ran boosting algorithms described 
set experiments compares algorithms set learning benchmark problems uci repository 
second experiment comparison large text categorization task 
details text categorization experiments appear companion schapire singer appear 
multiclass problems compared boosting algorithms discrete adaboost mh version adaboost mh require weak hypotheses range described section set eq 

goal weak learner case maximize defined eq 

real adaboost mh version adaboost mh restrict range weak hypotheses 
experiments involve domain partitioning weak hypotheses set confidence ratings section eliminating need choose 
goal weak learner case minimize defined eq 

smoothed predictions sec 
discrete adaboost version adaboost require weak hypotheses range approximation eq 
set eq 
corresponding goal weak learner maxi defined eq 

note single label case algorithm identical freund schapire adaboost algorithm 
test error rounds training error rounds discrete adaboost mh discrete adaboost mh discrete adaboost discrete adaboost test error rounds training error rounds discrete adaboost mh discrete adaboost mh discrete adaboost discrete adaboost test error rounds training error rounds discrete adaboost mh discrete adaboost mh discrete adaboost discrete adaboost 
comparison discrete adaboost mh discrete adaboost multiclass benchmark problems uci repository 
point scatterplot shows error rate competing algorithms single benchmark 
top bottom rows give training test errors respectively rounds boosting 
benchmark dataset error rates fell outside range rounds boosting 
test error rounds training error rounds discrete adaboost mh discrete adaboost mh real adaboost mh real adaboost mh test error rounds training error rounds discrete adaboost mh discrete adaboost mh real adaboost mh real adaboost mh test error rounds training error rounds discrete adaboost mh discrete adaboost mh real adaboost mh real adaboost mh 
comparison discrete real adaboost mh uci repository 
see 
binary multiclass benchmark problems algorithms class multiclass problems alike 
note discrete adaboost discrete adaboost mh equivalent algorithms problems 
compared algorithms collection benchmark problems available repository university california irvine merz murphy 
experimental set freund schapire 
test set provided experiments run times results averaged learning algorithms may randomized 
test set provided fold cross validation rerun times total runs algorithm 
tested set benchmarks dropped vowel dataset 
version adaboost run rounds 
simplest weak learners tested freund schapire 
weak learner finds weak hypothesis prediction result single test comparing attributes possible values 
discrete attributes equality tested continuous attributes threshold value compared 
hypothesis viewed level decision tree called decision stump 
best hypothesis form optimizes appropriate learning criterion listed direct efficient search methods described 
compares relative performance freund schapire adaboost algorithm called discrete adaboost new algorithm discrete adaboost mh 
point scatterplot gives averaged error rates methods single benchmark problem coordinate point gives error rate discrete adaboost coordinate gives error rate discrete adaboost mh 
methods equivalent class problems give results multiclass benchmarks 
provided scatterplots rounds boosting test train error rates 
clear figures methods generally quite evenly matched possible slight advantage adaboost mh 
problems hamming loss methodology gives comparable results freund schapire method advantage conceptually simpler 
assess value weak hypotheses give confidence rated predictions 
shows similar scatterplots comparing real adaboost mh discrete adaboost mh 
scatterplots show real version confidences effective driving training error advantage test error rate especially relatively small number rounds 
rounds differences largely disappear 
figures give details behavior different versions ada boost 
compare discrete real adaboost mh different problems uci repository 
problem plot method training test error function number rounds boosting 
similarly compare discrete adaboost discrete adaboost mh real adaboost mh multiclass problems 
examining behavior various error curves potential improvement adaboost real valued predictions greatest larger problems 
noticeable case letter recognition task largest uci problem suite 
labor labor hepatitis hepatitis sonar sonar promoters promoters cleve cleve ionosphere ionosphere house votes house votes votes votes crx crx breast cancer wisconsin breast cancer wisconsin pima indians diabetes pima indians diabetes german german hypothyroid hypothyroid sick euthyroid sick euthyroid kr vs kp kr vs kp 
comparison discrete real adaboost mh binary problems uci repository 
problem show training left test right errors function number rounds boosting 
soybean small soybean small glass glass audiology audiology soybean large soybean large vehicle vehicle vowel vowel segmentation segmentation splice splice satimage satimage letter recognition letter recognition iris real adaboost mh discrete adaboost discrete adaboost mh iris 
comparison discrete adaboost discrete adaboost mh real adaboost mh multiclass problems uci repository 
problem show training left test right errors function number rounds boosting 
discrete adaboost discrete adaboost mh real adaboost mh discrete adaboost discrete adaboost mh real adaboost mh error error number rounds number rounds 
comparison training left test right error boosting methods class text classification problem trec ap collection 
class problem training examples test examples 
problem training error rounds discrete adaboost discrete adaboost mh real adaboost mh 
test error rates rounds respectively 
rounds gap test error narrowed somewhat 
give results large text categorization problem 
details text categorization experiments described companion schapire singer appear 
problem classes domestic entertainment cial international political washington 
goal assign document classes 
weak learner appropriately modified text specifically weak hypotheses predictions tests check presence absence phrase document 
training documents test documents 
compare performance discrete adaboost discrete ada boost mh real adaboost mh 
shows training test error function number rounds 
axis shows number rounds logarithmic scale axis training test error 
real adaboost mh dramatically outperforms methods behavior typical large text categorization tasks 
example reach test error discrete adaboost mh takes rounds discrete adaboost takes rounds 
comparison real adaboost mh takes rounds fold speed best methods 
happened example discrete adaboost mh consistently outperform discrete adaboost similar problems 
partially due inferior choice approximation leading eq 
exact method gives choice eq 


concluding remarks described improvements freund schapire adaboost algorithm 
new framework weak hypotheses may assign confidences predictions 
described generalizations multiclass problems 
experimental results improved boosting algorithms show dramatic improvements training error possible fairly large amount data available 
small noisy datasets rapid decrease training error accompanied overfitting results poor generalization error 
important research goal control directly indirectly complexity strong hypotheses constructed boosting 
applications improved boosting algorithms 
implemented system called boostexter multiclass multi label text speech categorization performed extensive set experiments system schapire singer appear 
new boosting framework devising efficient ranking algorithms freund 
domains may new framework boosting 
instance possible train non linear classifiers neural networks objective function 
mentioned open problems finding oblivious hypothesis minimizes adaboost interesting connections boosting models learning algorithms generalized additive models friedman maximum entropy methods csisz dy form new exciting research arena :10.1.1.30.3515
acknowledgments yoav freund raj iyer helpful discussions 
peter bartlett showing bound generalization error section roland freund tommi jaakkola useful comments numerical methods 
bartlett 

sample complexity pattern classification neural networks size weights important size network 
ieee transactions information theory 
bauer kohavi 
appear 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
baum haussler 

size net gives valid generalization 
neural computation 
blum 

empirical support winnow weighted majority algorithms results calendar scheduling domain 
machine learning 
breiman 

arcing classifiers 
annals statistics 
csisz dy 

information geometry minimization procedures 
statistics decisions supplement issue 
dietterich 
appear 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 
dietterich bakiri 

solving multiclass learning problems errorcorrecting output codes 
journal artificial intelligence research 
drucker cortes 

boosting decision trees 
advances neural information processing systems pp 

fletcher 

practical methods optimization second edition 
john wiley 
freund iyer schapire singer 

efficient boosting algorithm combining preferences 
machine learning proceedings fifteenth international conference 
freund schapire 

experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pp 

freund schapire 

decision theoretic generalization line learning application boosting 
journal computer system sciences 
freund schapire singer warmuth 

combining predictors specialize 
proceedings ninth annual acm symposium theory computing pp 

friedman hastie tibshirani 

additive logistic regression statistical view boosting 
technical report 
haussler 

decision theoretic generalizations pac model neural net learning applications 
information computation 
haussler long 

generalization sauer lemma 
journal combinatorial theory series 
kearns mansour 

boosting ability top decision tree learning algorithms 
proceedings eighth annual acm symposium theory computing 
maclin opitz 

empirical evaluation bagging boosting 
proceedings fourteenth national conference artificial intelligence pp 

dietterich 

pruning adaptive boosting 
machine learning proceedings fourteenth international conference pp 

merz murphy 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
quinlan 

bagging boosting 
proceedings thirteenth national conference artificial intelligence pp 

schapire 

output codes boost multiclass learning problems 
machine learning proceedings fourteenth international conference pp 

schapire freund bartlett lee 

boosting margin new explanation effectiveness voting methods 
annals statistics 
schapire singer 
appear 
boostexter boosting system text categorization 
machine learning 
schwenk bengio 

training methods adaptive boosting neural networks 
advances neural information processing systems pp 

appendix properties appendix show function defined eq 
convex function parameters 
describe numerical procedure newton method find parameters minimize 
simplify notation analyze slightly general form eq 
note cases discussed form eq 

refer brevity function eq 
second order derivatives respect 
denoting 
rewrite oa vector positive semidefinite implies convex respect unique minimum exception pathological cases 
find values minimize iterative methods 
newton method 
short newton method new set parameters updated current set follows 
denote substituting values eqs 
eq 
get newton parameter update typically update result new set parameters attains smaller value current set 
decrease guaranteed 
iteration augmented test value line search direction case increase value 
details see instance fletcher 
appendix bounding generalization error appendix prove bound generalization error combined hypothesis produced adaboost terms margins training examples 
outline proof communicated peter bartlett 
uses techniques developed bartlett schapire 

set real valued functions domain denote convex hull understood sums finite subset hypotheses assume weights hypotheses nonnegative 
result generalized handle negative weights simply adding hypotheses main result appendix theorem 
theorem identical schapire theorem allow weak hypotheses realvalued binary 
denote probability event example chosen denote probability respect choosing example uniformly random training set 
clear context abbreviate denote expected value similar manner 
prove theorem need define notion sloppy cover 
class real valued functions training set size real numbers say function class sloppy cover respect exists gf denote maximum training sets cover respect size size smallest sloppy theorem distribution sample examples chosen independently random suppose weak hypothesis space mn valued functions assume probability random choice training set weighted average function satisfies generalization error bound proof techniques bartlett schapire 
theorem give theorem states probability random choice training set exists function prove theorem applying result 
need construct sloppy covers haussler long lemma prove fix set size result means exists cardinality exists gf set unweighted averages elements show sloppy cover write chosen satisfy eq 

gf define distribution functions function selected choosing independently random distribution defined coefficients setting note fixed chernoff bounds show gf gf exists gf gf combined eq 
means sloppy cover shown setting implies eq 
logarithm eq 
inequality fact second inequality note increasing function upper bounded choice eq 
bound eq 
proved bound theorem single choice high probability 
prove high probability bound holds simultaneously choice eq 
regarding parameters fixed 
shown probability 
union bound implies probability fixed eq 
holds probability probability fails hold assume high probability case eq 
holds choose completes proof 

