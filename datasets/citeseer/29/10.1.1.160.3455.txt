appears proceedings fifth nasa goddard space flight center conference mass storage systems technologies sept college park md global file system steven thomas matthew keefe department electrical engineering laboratory computational science engineering university minnesota ee cs building union street minneapolis mn ee umn edu tel fax global file system gfs prototype design distributed file system cluster nodes physically share storage devices connected network fibre channel 
networks network attached storage devices advanced level performance extensibility believed disadvantages shared disk architectures longer valid 
shared storage architecture attempts exploit sophistication device technologies client server architecture diminishes device role simple components 
gfs distributes file system responsibilities processing nodes storage devices file system resources entire storage pool 
gfs caches data storage devices main memories machines 
consistency established locking mechanism maintained storage device controllers facilitate atomic read modify write operations 
locking mechanism prototyped seagate disks drives disk arrays 
gfs implemented silicon graphics irix operating system accessed standard unix commands utilities 
supported office naval research 
national science foundation 
asc university space research association administered nasa center excellence space data information sciences nasa goddard space flight center 
distributed systems evaluated factors performance availability extensibility 
performance characterized measurements response time throughput 
distributed systems achieve availability allowing working components act replacements failed components 
extensibility combination portability scalability 
obvious influences scalability things addressing limitations network ports subtle bottlenecks hardware software may arise 
factors influenced architecture distributed parallel systems 
architectures categorized message shared shared storage shared disk 
message architectures share data communication machines network data stored locally devices machine 
machines shared storage architecture access storage devices directly 
figures show examples message shared storage architectures 
client client client node node node cpu memory disk cpu memory disk cpu memory disk cpu memory disk cpu memory disk cpu memory disk network network bus cpu memory disk cpu memory disk disk disk server server storage controller storage controller message shared storage advocates architectures claim theirs better respect factors 
techno religious war solve time soon analyzing existing systems gives perspective strengths weaknesses architecture 
section summarizes number distributed file systems data sharing approaches 
message distributed file systems sun network file system sun network file system nfs designed sun microsystems 
design goals system independence name transparency preservation unix file system semantics 
nfs uses client server approach 
server stateless writes modified data stable storage returning results 
server able cache data system memory improve performance 
clients requests server information necessary complete operation 
clients servers communicate network remote procedure calls rpc 
rpc high level protocol built user datagram protocol udp internet protocol ip 
server eases crash recovery 
client goes effect operations server clients 
server fails need reboot 
clients resend requests server completed requests time 
clients perceive server slow rebooted 
sprite file system sprite distributed operating system networked workstations developed symbolic processing spur research project 
sun nfs sprite uses remote procedure calls communicate network 
sprite file system distributed multiple servers clients 
primary goal provide name transparency providing adequate performance 
device special files accessible process network 
sprite file system maintains cache consistency server initiated approach 
server tracks open files 
files non write shared clients may cache portions file local memories 
files moves non write shared write shared server performs call back operation disables client caching 
andrew coda file systems carnegie mellon university coda distributed file system descended andrew file system joint research project ibm cmu 
file systems designed operate distributed network workstations scaling machines 
coda constant data availability designed improve availability andrew 
client able cache entire files locally memory disks 
furthermore multiple copies file may exist servers 
server failure may little impact availability 
approach allows clients run disconnected operation files cached locally 
client reconnect network synchronize cache rest system 
sprite file system coda servers maintain state concerning file accesses 
servers responsible performing call backs clients cached data modified client 
file sharing client guaranteed consistency described unix file sharing semantics 
files shared different systems see consistency granularity entire file 
xfs serverless network file system xfs file system part berkeley network workstations project 
follow research sprite project 
uses log structured approach sprite log structured file system lfs zebra striping technique simplify failure recovery provide high throughput transfers 
xfs workstations connected fast switched network 
xfs said serverless storage server functionality placed machines clients 
system manage control metadata real data 
advantages load balancing scalability high availability 
sprite system supports data caching clients 
client requests data manager 
manager tries satisfy request client cache directs client appropriate storage device 
xfs uses token cache consistency mechanism 
client acquire token file system block wishes modify 
managers notify clients invalidate stale copies data forward requests new owner token 
shared storage distributed file systems digital vms vaxcluster closely coupled structure vax computing storage nodes operates single system 
system cpu memory combinations connected message interconnect 
processor runs copy distributed vax vms operating system 
interconnection network topologies high performance star hub supported maximum devices low cost ethernet network 
storage devices connected system hierarchical storage controller hsc 
high reliability hsc placed dual ported storage devices star adding redundant path cpus storage devices 
operating system allows files shared cluster distributed lock manager 
lock requests particular access mode exclusive access protected read concurrent read concurrent write 
requests resources queued resource unlocked 
system falls shared storage architecture file requests serviced shared 
hsc support disks 
vms allows caching data file system resources 
coherence maintained cpu local memories sequence numbers files synchronization locks 
file system modifies block increments sequence number file lock 
system block cached system find sequence number old 
block refreshed hsc 
cray serverless file system cray research serverless file system sfs file system incorporated operating system 
file system uses disk array shared storage device 
disk array connected machines switch 
machines nodes act peers server machine 
arbitration disk array performed sun sparc workstation connected switch 
workstation call responsible maintaining semaphores mutual exclusion data stored disk array 
error recovery functions 
cray sfs supports types file operations multiple readers single writer 
sfs provides consistency semaphores facilitate read modify write operations limiting open file states 
nodes able cache data local file system constraints limited parallel file operations 
message base versus shared storage message architecture strength lies extensibility 
approach portable network protocol connecting machines potentially scalable large numbers machines 
best example message base portability nfs 
file system dominates industry available platform 
file systems coda shown message approach scales thousands machines 
message systems may perform data access balanced machines 
load balancing difficult machine capacities usage differ dynamically system 
locality difficult maintain resources shared nodes system 
redundant copies maintained cost coherence overheads 
furthermore performance benefit high speed devices disk arrays negated bandwidth machine limited network 
summarize achieving performance message systems easy task 
server device failures challenging problem facing message approach server failure may result data 
fault tolerance may maintained disk array devices node redundancy extended machines 
software redundancy schemes built file system maintain fault tolerance 
shared storage approach advantage machine nearly uniform access storage devices freedom servicing data requests machines 
approach similar traditional uniprocessor model machines act independently 
failures machine little effect rest systems expect possibly increasing load 
storage device availability solved hardware raid 
shared storage architecture takes advantage properties underlying storage hardware 
node uniform access devices bandwidth produced disk arrays disk striping utilized nodes 
devices capable command queuing optimize head seeks provide high throughput 
traditional shared storage system extensibility cost 
systems digital vaxcluster cray sfs proprietary networks hardware 
proprietary hardware benefit market competition remains costly 
furthermore systems scale number nodes 
examples storage device machines attached system 
far architecture fully satisfies factors performance availability extensibility new network technologies change 
instance fibre channel fc emerging ansi international standards organization iso standard network architecture supports network attached storage including scsi channel protocol 
fibre channel provides topologies network attached storage switched arbitrated loop 
high speed network fibre channel improve performance shared storage message architectures little improve extensibility availability message approach 
providing network attachment storage devices greatly enhances extensibility shared storage 
network fibre channel system non portable built shared storage architecture 
existing shared storage systems redesigned exploit properties networks devices 
assumptions traditional shared storage file systems respect data caching coherence resource management obsolete 
instance cray sfs caches data locally nodes exploit high bandwidth low latency memories 
caching comes price allowing non write shared file operations 
file opened readers writer access readers close file 
coherence mechanism lead large latencies starvation 
global file system global file system prototype design distributed file system 
network attached storage devices physically shared cluster nodes 
gfs prototype implemented silicon graphics irix operating system vfs interface accessed standard unix commands utilities 
machines storage devices connected fibre channel network 
gfs views storage network storage pool nsp collection network attached storage devices logically grouped provide node machines unified storage space 
storage pools owned controlled machine act shared storage machines devices network 
divided subpool takes attributes underlying hardware 
gfs targets environments require large storage capacities bandwidth multimedia scientific computing visualization 
large capacities influence tradeoffs caching metadata structure associated design file system 
chip integration transformed storage devices sophisticated units capable replacing functions performed server machine client server environment 
devices schedule accesses media queuing multiple requests 
possess caches megabytes read write caching prefetching 
gfs caches data nodes main memories request processing 
request satisfied data released written back storage devices 
exploit locality gfs caches data storage devices 
gfs informs devices request data appropriate cache metadata accessed repetitively small files directories frequently accessed 
consistency established locking mechanism maintained storage device controllers facilitate atomic read modify write operations 
form locking simplicity centralized mechanism distributed large number devices 
represents example gfs distributed environment 
nodes attached network top storage pool bottom 
connecting nodes devices fibre channel network may consist switches loops hubs 
example different exist single single disk wide striping disks fast disk array 
left tape device directly connected network 
tape drive may data backup hierarchical storage management 
node initiate third party transfers disk devices tape drive 
shows gfs host act nfs server 
ability allows machines gfs capabilities access gfs data nfs exported file system 
operating systems vfs interface handles translation gfs nfs 
gfs node gfs node cpus memory disks cpu memory disk gfs node nfs server cpu memory disk backup fibre channel network switches hubs loops nfs client nfs client network tape drive nfs client nfs client single wide fast network storage pool gfs distributed environment file system structure gfs file system divided resource groups rg 
resource groups designed distribute file system resources entire storage subpool 
shows logical structure file system 
multiple rgs exist device striped devices 
resource groups essentially mini file systems 
group rg block data bitmaps dinode bitmaps dinode free list data blocks 
rg block contains information similar traditional superblocks maintain number free number free data blocks access times rg 
file data metadata may span multiple groups 
gfs superblock contains information distributed resource groups 
information includes number nodes mounted file system bitmaps calculate unique identifiers node device file system mounted file system block size 
superblock contains static index rgs 
rg index describes location group attributes configuration 
device lock super block resource index device lock device lock device lock resource block resource block resource block dinode bitmaps dinode bitmaps dinode bitmaps data block bitmaps data block bitmaps data block bitmaps device lock dinode dinode device lock dinode device lock device lock dinode dinode device lock dinode device lock data block data block data block data block data block data block resource group resource group resource group gfs structure gfs dinode takes entire file system block 
dinode divided header section contains standard dinode fields section pointers 
number pointers dinode determined equation number pointers indirect block equation 
pointers dinode pointers indirect block file system block size size dinode header size block address file system block size size block address pointers form tree structure data blocks tree leaves having height 
structure accesses require number indirect accesses data blocks regardless offset file 
structure differs traditional unix file system ufs data blocks different heights 
ufs tree simpler design require additional level indirection 
furthermore ufs places multiple file system block 
entire block gfs hundreds direct pointers dinode just ufs dinode 
illustrates gfs dinode level indirection referencing data blocks 
gfs dinode indirect blocks data blocks dinode number resource group number file type mode owner group number links access times bookkeeping info gfs dinode device locks device locks mechanisms node machines maintain mutual exclusion file system data 
implemented storage devices accessed single scsi command 
command instructs devices perform primitive operations locks test set clear 
implementation device locks device limited constraints 
device lock commands independent scsi commands 

devices supporting device locks awareness nature data locked mutual exclusion 

lock requires minimal amounts disk controller memory little byte lock 
lock states state lock described bit 
bit set lock acquired owned machine node 
bit lock available acquired node 
command action test set determines lock value 
value command returns status indicating lock acquired 
value sets lock returns status initiator 
command clear simply sets lock bit 
clocks associated lock clock 
clocks logical clocks sense relate time keep ordering events lock 
clocks incremented successful action performed 
clocks monitor lock accessed times lock set cleared 
clock gives insight load balancing hot spots 
occur locks accessed 
importantly clocks useful error recovery 
clocks implemented minimal amount memory typically bits 
initiators aware clock values periodically roll maximum value zero 
may happen times second highly accessed lock care taken initiator assume clock value slowly growing 
clock value returned command 
device failures device locks accompanying clocks stored volatile memory device locks held scsi resets 
device powered failure occurs results locks cleared device notifies nodes setting unit attention 
finding unit attention node checks see locks valid 
proceeding reacquire locks may lost 
node failures node fails leave device locks locked state indefinitely 
locks remain state node clears 
node attempting acquire lock owned failed node identify lock untouched checking activity lock clock 
clock remain unchanged extended time period node identify case clear lock 
care taken node clearing lock 
true owner may failed may hung state eventually return believing owns lock 
furthermore separate nodes may simultaneously identify lock cleared send resets 
may possible node clears lock sets lock command 
second node decided clear lock sends command lock acquired node 
second clear request ignored 
node wishes clear lock failure recovery device compares current clock input clock node 
test ensures lock cleared node identify current value clock 
deadlocks starvation deadlocks avoided file system 
file system acquires locks increasing order 
circular dependencies avoided 
starvation handled file system device drivers 
file system hold locks requests storage 
node device drivers test starvation checking activity lock clock values 
node increase rate lock requests performed attempt feed starvation 
consistency caching consistency maintained atomic operations guaranteed device locks modifying data 
limited number practical device locks device order individual locks assigned file 
lock assigned super block lock assigned resource group remaining locks divided 
shows device locks associated superblock resource groups 
device locks generally implemented storage device scsi commands reserve release perform atomic operations data 
commands provide exclusive access entire device node servicing requests nodes 
commands guarantee exclusive access provide parallelism 
reservation device non conflicting requests wait storage device released 
distributed environment limited access decreases system throughput response times 
scsi protocol describes optional commands reserve release extents 
commands allow initiators reserve exclusive access data blocks may need 
commands decrease granularity exclusion device level block level 
potentially increasing throughput distributed system reserve release extent commands require devices maintain complicated states access permissions 
reason commands implemented majority device manufacturers 
device locks mutual exclusion mechanism highly parallel low device overheads recovers failures gracefully 
command prototyped seagate disks drives disk arrays 
preliminary results preliminary measurements taken parallel scsi hardware fibre channel 
fibre channel hardware available parallel scsi results 
hope fibre channel measurements time 
results tests scsi reserve release maintain consistency device locks 
command development testing seagate 
test configuration tests conducted sgi running irix operating system 
indy mhz processor mhz processors 
machines mbytes main memory 
seagate lp shared storage device 
lp gigabyte fast scsi drive 
millisecond maximum seek time rpm spindle speed 
tests measured time reserve release commands approximately milliseconds 
disk system disks share common bus 
disk caching parameters set default values 
configuration shown representation test environment 
overcome difficulties system disk node attached internal scsi bus node node controller directly connected internal bus node 
devices controllers electrically connected 
node system disks system disk node nodes node scsi id scsi id node scsi controller scsi id scsi controller scsi id scsi controller scsi id fast scsi bus shared disks scsi id scsi id test environment benchmarks small benchmark programs run machines study effectively scsi bus disk shared 
tests run file system running user space opposed running vfs interface kernel 
user level file system allows easy tracing performance measurements 
user level file system adds various overheads system believe vfs file system perform better 
benchmarks chosen involve tests creating writing reading files 
file sizes range mbyte mbytes 
files written read making requests file 
delay placed request ranging zero second 
delay represents time real application perform computation wait reason tests run times median values evaluate performance 
table summarizes benchmarks 
benchmarks chosen tests attempt match performance capabilities single shared disk mb sec scsi bus 
benchmarks scaled appropriately performing tests fibre channel hardware multiple devices 
parameter range number nodes types create write write read file sizes mb mb number request file delay requests ms ms ms ms table benchmark parameters parallel scsi performance figures show speedups machine creating reading mb file kb requests respectively 
figures show speedups machines create read mb file mb requests respectively 
results scaled reflect equal amounts mb mb respectively 
time machine multiplied time multiplied machines 
times normalized machine test dividing machine time 
curves delay ms ms ms delays 
write tests show trends similar read tests 
figures plots mb creates varying request sizes 
figures plots test read requests 
request axis number requests delay ms delay ms delay ms delay ms delay ms delay ms delay ms delay ms speedup speedup machines machines gfs speedup mb files created kb requests gfs speedup mb files read kb requests delay ms delay ms delay ms delay ms delay ms delay ms delay ms delay ms time time machines machines gfs speedup mb files created mb requests gfs speedup mb files read mb requests needed access entire file 
curves plot time machine time slowest machine machines running simultaneously times machine time 
curve constant workload comparison machine case 
time time requests requests gfs creates mb files delay gfs creates mb files ms delay time time requests requests gfs creates mb files ms delay gfs creates mb files ms delay figures plots number conflicts encountered machines running mb create read tests delay requests 
figures show tests ms delay 
conflicts counted device driver reserve command failed device reserved 
conflict plots show obvious interesting trends single machine tests conflicts machines tests conflicts machines test delay requests decreased conflicts creates conflicts reads 
trends explained argument increasing numbers rate requests increases chances having conflicts 
tests conflicts issued requests time time requests requests gfs reads mb files delay gfs reads mb files ms delay time time requests requests gfs reads mb files ms delay gfs reads mb files ms delay time period 
machine machines machines machine machines machines number conflicts number conflicts gfs creates mb files delay gfs reads mb files delay machine machines machines machine machines machines number conflicts number conflicts gfs mb files ms delay gfs reads mb files ms delay results promising considering nature tests 
delay case represents nearly constant access device single machine case 
parallelism exploited adding machines device fully utilized 
slowdown seen plots result contention shared device 
file creates slower reads creates require additional requests allocate allocate data blocks build metadata tree 
seen plots number requests axis number requests indirectly proportional performance 
request overhead greater milliseconds 
request possibility reservation conflict slows request milliseconds 
fibre channel device locks overheads substantially reduced 
ms delay tests allow parallelism realized 
ms delay parallelism exploited machines 
ms delay may represent best speedup configuration ms delay test distinguish machines running simultaneously 
striping file system multiple devices may effect similar increasing delay file system requests 
assuming mb sec scsi bus system bottleneck adding device configuration may remove percent burden single disk 
nfs comparison benchmarks run configuration nfs comparison distributed file systems running hardware 
configuration consists dedicated server machine clients 
workstations disk server machine mhz indy 
connecting machines mbit sec ethernet network 
server file system sgi xfs 
figures nfs equivalent speedup curves 
figures time comparison nfs gfs read tests 
differences noticed gfs nfs tests 
file system speedup larger delays nfs slowdown 
nfs tests dedicated server machine gfs test configuration 
speedup values take account 
second gfs times smaller tens times faster 
cases request sizes large gfs exceeded transfer speeds mb sec 
delay ms delay ms delay ms delay ms delay ms delay ms delay ms delay ms speedup speedup machines machines nfs speedup mb files created kb requests nfs speedup mb files read kb requests delay ms delay ms delay ms delay ms delay ms delay ms delay ms delay ms speedup speedup machines machines nfs speedup mb files created mb requests nfs speedup mb files read mb requests time time requests requests nfs creates mb files delay nfs creates mb files ms delay time time requests requests nfs reads mb files delay nfs read mb files ms delay gfs project early phase 
fibre channel hardware device drivers available expanded set performance tests performed 
tests performed file systems different configurations file system block sizes resource group layouts device lock mechanisms studied accommodate failure recovery 
polling rates locking retry requests tuned provide compromise low latencies node high system throughput 
associated rates delay retry algorithm constant delay versus variable delay 
time node waits resetting device lock owned node investigated 
time handle failures short period time resetting prematurely 
premature resets cause previous owner locks owned 
acceptable occasionally kept minimum 
currently gfs stores data linear layout storage devices 
layout allows various form striping data partitioning plan generalize layout subpool architecture 
subpool attributes reflected underlying hardware configuration 
translation layer placed file system device drivers 
translation layer convert linear block addresses file system proper devices block numbers padding requests appropriate 
begun study caching algorithms cache configurations storage devices 
hardware simulations attempting determine benefit large caches 
replacement policies studied 
gfs approach distributed file system shared storage devices promising high bandwidth natures new networks increasing sophistication devices 
architecture places responsibilities storage devices message architectures 
modern devices able cache perform mutual exclusion schedule requests freeing burdens node machines 
results preliminary performance measurements indicate sufficient delay requests shared device device parallelism exploited system 
delay may take form machines performing file system requests lengthy computation low activity 
striping file system multiple devices may effect similar increasing delay requests 
believe mb sec fibre channel multiple storage devices shared storage scheme scale machines large workloads 
furthermore fine grain mutual exclusion implemented device locks decrease conflicts increase performance node system 
ben university minnesota help device driver development performance evaluations 
contributed lot preliminary results portion 
aaron university minnesota advice experience building parallel scsi test configuration development preliminary tests 
scsi device locks represent input number individuals 
design definition result dialogues seagate 
raymond steven hansen edward jim gerald michael miller seagate 
carl brad silicon graphics granting access device driver code aided development gfs 
valduriez parallel database systems case shared proceedings ninth international conference data engineering pp 

pfister search clusters 
upper saddle river nj prentice hall 
sandberg goldberg kleiman walsh lyon design implementation sun network file system proceedings summer usenix conference pp 

ousterhout douglis nelson welch sprite network operating system ieee computer pp 
february 
satyanarayanan scalable secure highly available distributed file access ieee computer pp 
may 
satyanarayanan coda highly available file system distributed workstation environment proceedings second ieee workshop workstation operating systems september 
anderson dahlin neefe patterson roselli wang serverless network file system acm operating systems review vol 
december 
hartman ousterhout zebra striped network file system usenix workshop file systems may 
dahlin wang anderson patterson quantitative analysis cache policies scalable network file systems proceedings sigmetrics conference may 
digital technical journal vaxcluster systems september 
special issue number 
matthews implementing shared file system disk array fourteenth ieee symposium mass storage systems pp 

katz gibson patterson disk system architectures high performance computing proceedings ieee vol 
pp 

fibre channel connection ieee computer pp 
august 
ansi fc direct attach disk profile private loop june 
version 
silicon graphics mountain view ca programming silicon graphics systems overview 
document number 
silicon graphics mountain view ca irix device driver programming guide 
document number 
unix internals new frontiers 
upper saddle river nj prentice hall 
bach design unix operating system 
englewood cliffs nj prentice hall 
cox magic garden explained 
englewood cliffs nj prentice hall 
woodward interactive scientific visualization fluid flow ieee computer pp 
october 
reddy wyllie issues multimedia system ieee computer pp 
march 
silicon graphics mountain view ca irix admin disks filesystems 
document number 

