parallel crawlers junghoo cho hector garcia molina stanford university cho hector cs stanford edu study design effective parallel crawler 
size web grows imperative parallelize crawling process order finish downloading pages reasonable amount time 
propose multiple architectures parallel crawler identify fundamental issues related parallel crawling 
understanding propose metrics evaluate parallel crawler compare proposed architectures pages collected web 
results clarify relative merits architecture provide guideline adopt architecture 
crawler program downloads stores web pages web search engine 
roughly crawler starts placing initial set urls queue urls retrieved kept prioritized 
queue crawler gets url order downloads page extracts urls downloaded page puts new urls queue 
process repeated crawler decides 
collected pages applications web search engine web cache 
size web grows difficult retrieve significant portion web single process 
search engines run multiple processes parallel perform task download rate maximized 
refer type crawler parallel crawler 
study design parallel crawler maximize performance download rate minimizing overhead parallelization 
believe existing search engines sort parallelization little scientific research conducted topic 
little known tradeoffs various design choices parallel crawler 
particular believe issues study parallel crawler challenging interesting overlap multiple processes run parallel download pages possible different processes download page multiple times 
process may aware process downloaded page 
clearly multiple downloads minimized save network bandwidth increase crawler effectiveness 
coordinate processes prevent overlap 
quality crawler wants download important pages order maximize quality downloaded collection 
parallel crawler process may aware image web collectively downloaded far 
reason process may crawling decision solely image web downloaded poor crawling decision 
sure quality downloaded pages parallel crawler centralized 
communication bandwidth order prevent overlap improve quality downloaded pages crawling processes need periodically communicate coordinate 
communication may grow significantly number crawling processes increases 
exactly need communicate significant overhead 
minimize communication overhead maintaining effectiveness crawler 
challenging implement believe parallel crawler important advantages compared single process crawler scalability due enormous size web imperative run parallel crawler 
crawler simply achieve required download rate certain cases 
network load dispersion multiple crawling processes parallel crawler may run geographically distant locations downloading geographically adjacent pages 
example process germany may download european pages japan crawls asian pages 
way disperse network load multiple regions 
particular dispersion necessary single network handle heavy load large scale crawl 
network load reduction addition load parallel crawler may reduce network load 
example assume crawler north america retrieves page europe 
downloaded crawler page go network europe north america inter continental network network north america 
crawling process europe collects european pages process north america crawls north american pages network load reduced pages go local networks 
note downloaded pages may need transferred central location central index built 
case believe transfer significantly smaller original page download traffic methods compression pages collected stored easy compress data sending central location 
difference sending entire image downloaded pages may take difference previous image current send difference 
pages static change scheme significantly reduce network traffic 
summarization certain cases may need central index original pages 
case may extract necessary information index construction postings list transfer data 
build effective web crawler clearly need address challenges just parallelization 
example crawler needs page changes revisit page order maintain page date 
sure particular web site flooded requests crawl 
addition carefully select page download store limited storage space order best stored collection pages :10.1.1.43.1111
issues important focus crawler parallelization problem paid significantly attention 
summary believe parallel crawler advantages poses interesting challenges 
particular believe contributions identify major issues problems related parallel crawler discuss solve problems 
multiple architectures techniques parallel crawler discuss advantages disadvantages 
far know techniques described open literature 
little known internals crawlers closely guarded secrets 
large dataset web pages collected web experimentally compare design choices study tradeoffs quantitatively 
propose various optimization techniques minimize coordination effort crawling processes operate independently maximizing effectiveness 
related web crawlers studied advent web :10.1.1.43.1111:10.1.1.109.4049
studies roughly categorized topics general architecture category describes general architecture web crawler studies crawler works 
example describes architecture compaq src crawler major design goals 
studies briefly describe crawling task parallelized 
instance describes crawler distributes individual urls multiple machines download web pages parallel :10.1.1.109.4049
downloaded pages sent central machine links extracted sent back crawling machines 
contrast studies try understand fundamental issues related parallel crawler various design choices affect performance 
identify multiple architectures parallel crawler compare relative merits carefully real web data 
page selection crawlers download small subset web crawlers need carefully decide page download :10.1.1.43.1111:10.1.1.43.1111
retrieving important relevant pages early crawler may improve quality downloaded pages 
studies category explore crawler discover identify important pages early propose various algorithms achieve goal 
study parallelization affects techniques explain fix problems introduced parallelization 
page update web crawlers need update downloaded pages periodically order maintain pages date 
studies category discuss various page revisit policies maximize freshness downloaded pages 
example studies crawler adjust revisit frequencies pages pages change different rates 
believe studies orthogonal discuss 
exists significant body literature studying general problem parallel distributed computing 
studies focus design efficient parallel algorithms 
example various architectures parallel computing propose algorithms solve various problems finding maximum cliques architecture study complexity proposed algorithms 
general principles described existing solutions directly applied crawling problem 
body literature designs implements distributed operating systems process distributed resources transparently distributed memory distributed file systems :10.1.1.110.7161
clearly os level support easy build general distributed application believe simply run centralized crawler distributed os achieve parallelism 
web crawler contacts millions web sites short period time consumes extremely large network storage memory resources 
loads push limit existing task carefully partitioned processes carefully coordinated 
general purpose distributed operating system understand semantics web crawling lead unacceptably poor performance 
example may consider proposed solution variation divide conquer approach partition assign web multiple processes 
architecture parallel crawler illustrate general architecture parallel crawler 
parallel crawler consists multiple crawling processes refer proc proc performs basic tasks single process crawler conducts 
downloads pages web stores downloaded pages locally extracts urls downloaded pages follows links 
depending proc split download task extracted links may sent proc proc performing tasks may distributed local network geographically distant locations 
intra site parallel crawler proc run local network communicate high speed interconnect lan call intra site parallel crawler 
scenario corresponds case proc run local network top 
case proc local network download pages remote web sites 
network load proc centralized single location operate 
distributed crawler proc run geographically distant locations connected internet wide area network call distributed crawler 
example proc may run crawling pages proc may run france crawling european pages 
discussed distributed crawler disperse reduce network load network 
proc run distant locations communicate internet important proc need communicate 
bandwidth proc may limited unavailable case internet 
multiple proc download pages parallel different proc may download page multiple times 
order avoid overlap proc need coordinate pages download 
coordination ways independent extreme proc may download pages totally independently coordination 
proc starts set seed urls follows links consulting proc scenario downloaded pages may overlap may hope overlap significant proc start different seed urls 
scheme minimal coordination overhead scalable directly study option due overlap problem 
consider improved version option significantly reduces overlap 
dynamic assignment exists central coordinator logically divides web small partitions certain partitioning function dynamically assigns partition proc inter net proc 
local connect proc proc 
proc local connect collected pages queues urls visit general architecture parallel crawler site crawled site crawled download call dynamic assignment 
example assume central coordinator partitions web site name url 
pages site cnn com top html cnn com content 
html belong partition pages different sites belong different partitions 
crawl central coordinator constantly decides partition crawl site cnn com sends urls partition discovered far proc seed urls 
request proc downloads pages extracts links 
extracted links point pages partition cnn com article html proc follows links link point page partition 
com index html proc reports link central coordinator 
coordinator uses link seed url appropriate partition 
note web partitioned various granularities 
extreme central coordinator may consider page separate partition assigns individual urls proc download 
case proc follow links different pages belong separate partitions 
simply reports extracted urls back coordinator 
communication proc central unit may vary dramatically depending granularity partitioning function 
static assignment web partitioned assigned proc start crawl call static assignment 
case proc knows proc responsible page crawl crawler need central coordinator 
shortly discuss detail proc operates scheme 
mainly focus static assignment simplicity scalability defer study dynamic assignment 
note dynamic assignment central coordinator may major bottleneck maintain large number urls reported proc constantly coordinate proc coordinator may need parallelized 
crawling modes static assignment static assignment proc responsible certain partition web download pages partition 
pages partition may links pages partition 
refer type link inter partition link 
illustrate proc may handle inter partition links example 
assume proc responsible sites respectively 
assume web partitioned sites web 
assume proc starts crawl root page site 
firewall mode mode proc downloads pages partition follow inter partition link 
inter partition links ignored thrown away 
example links ignored thrown away 
mode crawler overlap downloaded pages page downloaded proc 
crawler may download pages download pages may reachable inter partition links 
example download reached link 
proc run quite independently mode conduct run time coordination url exchanges 

cross mode primarily proc downloads pages partition runs pages partition follows inter partition links 
example consider 
process downloads pages links point runs pages follows link starts exploring 
downloading discovers link comes back downloads pages mode downloaded pages may clearly overlap pages downloaded twice crawler download pages firewall mode downloads mode 
firewall mode proc need communicate follow links discovered 

exchange mode proc periodically incrementally exchange inter partition urls say operate exchange mode 
processes follow inter partition links 
example informs page downloads page transfers url page downloads page note follow links page transfers links download page 
way crawler avoid overlap maximizing coverage 
note firewall cross modes give proc independence proc need communicate may download page multiple times may download pages 
contrast exchange mode avoids problems requires constant url exchange proc reduce url exchange crawler exchange mode may techniques 
batch communication transferring inter partition url immediately discovered proc may wait collect set urls send batch 
batching proc collects inter partition urls downloads pages 
partitions collected urls sends appropriate proc 
urls transferred proc starts collect new set urls downloaded pages 
note proc maintain list inter partition urls discovered far 
maintains list inter partition links current batch order minimize memory overhead url storage 
batch communication various advantages incremental communication 
incurs communication overhead set urls sent batch sending url message 
second absolute number exchanged urls decrease 
example consider 
link page appears twice page page transfers link downloading page needs send url downloading page contrast waits page sends urls batch needs send url 

replication known number incoming links pages web follows zipfian distribution 
small number web pages extremely large number links pointing majority pages small number incoming links 
may significantly reduce url exchanges replicate popular urls proc popular mean urls incoming links transferring proc start crawling pages identify popular urls image web collected previous crawl 
replicate urls proc proc exchange crawl 
small number web pages large number incoming links scheme may significantly reduce url exchanges proc replicate small number urls 
downloads page remember link sent 
note replicated urls may seed urls proc 
urls replicated set belong partition proc responsible proc may urls seeds starting pages 
note possible proc tries discover popular urls fly crawl identifying previous image 
believe popular urls previous crawl approximation popular urls current web popular web pages yahoo maintain popularity relatively long period time exact popularity may change slightly 
far mainly assumed web pages partitioned web sites 
clearly exist multitude ways partition web including 
url hash hash value url page assign page proc 
scheme pages site assigned different proc locality link structure reflected partition inter partition links 

site hash computing hash value entire url compute hash value site name url cnn com cnn com index html assign page proc 
scheme note pages site allocated partition 
inter site links inter partition links reduce number inter partition links quite significantly compared url hash scheme 

hierarchical hash value may partition web hierarchically urls pages 
example may divide web partitions pages com domain net domain pages allocate proc note scheme may fewer inter partition links site hash scheme pages may point pages domain 
preliminary experiments observe significant difference schemes long scheme splits web roughly number partitions 
consider url hash scheme generates large number links 
crawler uses url hash scheme proc need exchange larger number urls exchange mode coverage crawler lower firewall mode 
addition experiments mainly site hash scheme partitioning function 
chose option simpler implement captures core issues experiments links page point pages site average 
partitioning type intra site distributed independent dynamic static coordination firewall cross batch replication exchange url hash site hash hierarchical main focus discussed mode discussed summary options discussed want study 
example hierarchical scheme easy divide web equal size partitions relatively straightforward site hash scheme 
believe interpret results site hash scheme upper lower bound hierarchical scheme 
instance assuming web pages link pages domain number inter partition links lower hierarchical scheme site hash scheme confirm trend experiments 
summarize options discussed far 
right hand table shows detailed view static coordination scheme 
diagram highlight main focus dark grey 
mainly study static coordination scheme third column left hand table site hash partitioning experiments second row second table 
discussion briefly explore implications options 
instance firewall mode improved version independent coordination scheme table study firewall mode show implications independent coordination scheme 
roughly estimate performance url hash scheme row second table discuss results site hash scheme 
table crawler design space interesting see options existing search engines selected crawlers 
unfortunately information impossible obtain cases companies consider technologies proprietary want keep secret 
crawler design know prototype google crawler developed stanford :10.1.1.109.4049
time took intra site static site hash scheme ran exchange mode :10.1.1.109.4049
describes architecture compaq src parallel crawler 
parallelization options explicitly described 
sizes individual web site vary sizes partitions similar partition contains web sites average sizes similar partitions 
evaluation models section define metrics quantify advantages disadvantages different parallel crawling schemes 
metrics experiments 

overlap multiple proc downloading web pages simultaneously possible different proc download page multiple times 
multiple downloads page clearly undesirable 
precisely define overlap downloaded pages represents total number pages downloaded crawler represents number unique pages downloaded crawler 
goal parallel crawler minimize overlap 
note parallel crawler overlap problem firewall mode page item exchange mode page item 
modes proc downloads pages partition overlap zero 

coverage multiple proc run independently possible may download pages 
particular crawler firewall mode page item may problem proc follow inter partition links exchange links 
formalize notion define coverage downloaded pages represents total number pages crawler download number unique pages downloaded crawler 
example downloaded pages downloaded pages coverage crawler downloaded pages 
quality crawlers download web try download important relevant section web 
example crawler may storage space pages may want download important pages 
implement policy crawler needs notion importance pages called importance metric 
example assume crawler uses backlink count importance metric 
crawler considers page important lot pages point 
goal crawler download highly linked pages 
achieve goal single process crawler may method crawler constantly keeps track backlinks page pages downloaded visits page highest backlink count 
clearly pages downloaded way may top pages page selection entire web seen far 
may formalize notion quality downloaded pages follows assume hypothetical oracle crawler knows exact importance page certain importance metric 
assume oracle crawler downloads important pages total pn represent set pages 
represent set pages actual crawler download necessarily pn define pn pn quality downloaded pages actual crawler 
definition quality represents fraction true top pages downloaded crawler 
note quality parallel crawler may worse single process crawler importance metrics depend global structure web backlink count 
proc parallel crawler may know pages downloaded information page importance single process crawler 
hand single process crawler knows pages downloaded far 
proc parallel crawler may worse crawling decision single process crawler 
order avoid quality problem proc need periodically exchange information page importance 
example backlink count importance metric proc may periodically notify proc pages partition links pages partitions 
note backlink exchange naturally incorporated exchange mode crawler page item 
mode crawling processes exchange inter partition urls periodically proc simply count inter partition links receives proc count backlinks originating partitions 
precisely crawler uses batch communication technique page item process send message cnn com index html notify seen links page current batch 
receipt message increases backlink count page reflect inter partition links 
incorporating scheme believe quality exchange mode better firewall mode cross mode 
note quality exchange mode crawler may vary depending exchanges backlink messages 
instance crawling processes exchange backlink messages page download essentially backlink information single process crawler 
know backlink counts pages downloaded 
quality downloaded pages virtually single process crawler 
contrast proc rarely exchange backlink messages accurate backlink counts downloaded pages may poor crawling decisions resulting poor quality 
study proc exchange backlink messages order maximize quality 
proc send inter partition urls incrementally page proc send url proc simply count urls 
mode coverage overlap quality communication firewall bad bad cross bad bad exchange bad table comparison crawling modes 
communication overhead proc parallel crawler need exchange messages coordinate 
particular proc exchange mode page item swap inter partition urls periodically 
quantify communication required exchange define communication overhead average number inter partition urls exchanged downloaded page 
example parallel crawler downloaded pages total proc exchanged inter partition urls communication overhead 
note crawler firewall cross mode communication overhead exchange inter partition urls 
table compare relative merits crawling modes page items 
table means mode expected perform relatively metric bad means may perform worse compared modes 
instance firewall mode exchange inter partition urls communication downloads pages overlap may download page coverage bad 
proc exchange inter partition urls downloaded pages may low quality exchange mode crawler 
examine issues quantitatively experiments real web data 
description dataset discussed various issues related parallel crawler identified multiple alternatives architecture 
remainder quantitatively study issues experiments conducted real web data 
experiments millions web pages stanford webbase repository 
property dataset may significantly impact result experiments readers interested collected pages 
downloaded pages stanford webbase crawler december period weeks 
downloading pages webbase crawler started urls listed open directory www dmoz org followed links 
decided open directory urls seed urls pages ones considered important maintainers 
addition local webbase users interested open directory pages explicitly requested cover 
total number urls open directory time 
conceptually webbase crawler downloaded pages extracted urls downloaded pages followed links breadth manner 
webbase crawler uses various techniques expedite prioritize crawling process believe optimizations affect final dataset significantly 
dataset downloaded crawler particular way dataset may correctly represent actual web particular dataset may biased popular pages started open directory pages 
dataset cover pages accessible query interface 
example crawler download pages generated keyword search engine try guess appropriate keywords fill 
emphasize dynamically generated pages downloaded crawler 
example pages amazon web site amazon com dynamically generated download pages site links 
summary dataset may necessarily reflect actual image web believe represents image parallel crawler see crawl 
cases crawlers mainly interested downloading popular important pages download pages links data collection 
dataset relatively small pages compared full web keep mind significantly larger dataset experiments prohibitively expensive 
see graphs study multiple configurations configuration multiple crawler runs obtain statistically valid data points 
run involves simulating proc visit pages 
detailed simulations inherently time consuming 
results return dataset size issue various points discuss larger dataset changed 
case impacted run additional experiment pages understand impact growing dataset 
firewall mode coverage firewall mode crawler page item minimal communication overhead may coverage quality problems section 
section quantitatively study effectiveness firewall mode crawler pages repository 
particular estimate coverage section item firewall mode crawler employs proc parallel 
discuss quality issue parallel crawler 
experiments considered pages webbase repository entire web site hash partitioning page item 
seed urls proc random coverage number proc coverage number seeds processes processes processes processes number processes vs coverage number seed urls vs coverage urls partition seed urls total crawler 
discuss effect number seed urls shortly 
crawler ran firewall mode proc followed intra partition links inter partition links 
settings proc run ran urls 
simulated crawling measured coverage crawler 
performed experiments random seed urls repeated experiments multiple times different seed urls 
runs results essentially 
summarize results experiments 
horizontal axis represents number parallel proc vertical axis shows coverage crawler experiment 
solid line graph result page experiment 
explain dotted line 
note coverage single process 
result crawler experiment started urls actual dataset collected seed urls 
pages unreachable seed urls 
clear coverage decreases number processes increases 
trend number inter partition links increases web split smaller partitions pages reachable inter partition links 
result see may run crawler firewall mode decrease coverage fewer proc example process case coverage decreases single process case 
time see firewall mode crawler quite ineffective large number proc web downloaded proc run starting seed urls 
clearly coverage may depend number seed urls proc starts 
study issue ran experiments varying number seed urls show results 
horizontal axis graph represents total number seed urls crawler vertical axis shows coverage experiment 
example crawler total seed urls proc starting seed urls proc ran parallel 
performed experiments proc cases plotted coverage values 
observe trends large number proc run parallel total number seed urls affects coverage significantly 
example processes run parallel coverage value jumps number seed urls increases 
small number processes run parallel coverage significantly affected number seed urls 
coverage increases slightly increases improvement marginal 
results draw 
relatively small number proc running parallel crawler firewall mode provides coverage 
case crawler may start small number seed urls coverage affected number seed urls 

firewall mode choice crawler wants download single page web 
crawler may portion web particularly runs proc parallel 
results section page dataset important consider coverage change different dataset equivalently change web grows evolves 
unfortunately difficult predict web grow 
hand newly created pages connected existing pages creation site coverage increase 
hand new pages tend form disconnected groups coverage decrease 
depending web grows coverage go way 
preliminary study growth issue conducted experiments subset pages measured coverage changes 
randomly selected half sites dataset ran experiments pages sites 
roughly view smaller dataset smaller web time doubled number sites yield second dataset 
dotted line shows results page experiments 
graph see web doubles size double number proc retain roughly coverage 
new sites visited new proc significantly changing coverage obtain 
growth exclusively come new sites quite double number proc time web doubles size retain coverage 
completed crawl pages running experiments new dataset 
run takes week complete 
include results final version 
overlap number proc coverage coverage vs overlap cross mode crawler example generic search engine illustrate results guide design parallel crawler consider example 
assume operate web search engine need download pages month 
machine plan run proc mbps link internet machines want 
average size web page bytes roughly need download bytes month 
download rate corresponds mbps need machines proc obtain rate 
want conservative results page experiment estimate coverage proc scenario firewall mode may important download entire web 
example high freshness second example assume strong freshness requirement pages need revisit page week month 
new scenario requires approximately mbps page download need run proc case coverage crawler decreases 
course coverage larger conservative estimate safe probably want consider crawler mode different firewall mode 
cross mode overlap section study effectiveness cross mode crawler page item 
cross crawler may yield improved coverage web follows inter partition links proc runs urls partition 
mode incurs overlap downloaded pages section item page downloaded multiple proc crawler increases coverage expense overlap downloaded pages 
currently web estimated pages 
show relationship coverage overlap cross mode crawler obtained experiments 
partitioned pages site hash partitioning assigned proc proc random seed urls partition followed links cross mode 
experiment measured overlap crawler incurred coverage reached various points 
horizontal axis graph shows coverage particular time vertical axis shows overlap coverage 
performed experiments 
note cases overlap stays zero coverage relatively large 
example overlap zero coverage reaches 
understand result looking graph 
graph crawler proc cover web intra partition links 
cross mode crawler follow intra partition links coverage reaches point 
proc starts follow inter partition links increasing overlap 
reason believe overlap worse crawl adopted independent model page item 
applying partitioning scheme proc proc stay partition suppress overlap long possible 
crawler cross mode better independent model clear cross crawler incurs quite significant overlap 
example proc run parallel cross mode overlap obtain coverage close 
reason recommend cross mode absolutely necessary download page communication proc exchange mode communication avoid overlap coverage problems exchange mode crawler page item constantly exchanges inter partition urls proc section study communication overhead section item exchange mode crawler reduce replicating popular urls 
assume proc immediately transfers inter partition urls 
discuss batch communication discuss quality parallel crawler 
experiments split pages partitions site hash value ran proc exchange mode 
crawl measured urls exchanged crawl show results 
horizontal axis represents number parallel proc vertical axis shows communication overhead average number urls transferred page 
comparison purposes shows overhead url hash communication overhead site hash url hash number proc communication overhead processes number replicated urls number crawling processes vs number urls exchanged page number replicated urls vs number urls exchanged page scheme curve clipped top large overhead values 
explain site hash graph note average page links point pages site 
links followed local crawling process site hash partitioning 
remaining link points page different site exchanged processes 
see url exchange increases number processes 
example proc exchanged urls page processes ran exchanged urls page processes ran 
graph draw site hash partitioning scheme significantly reduces communication overhead compared url hash scheme 
need transfer link page links significantly smaller url hash scheme 
statistically run say proc url hash scheme half urls proc discovers belong proc 
half links page links page transferred url hash scheme significantly larger site hash scheme 
network bandwidth url exchange relatively small compared actual page download bandwidth 
site hash scheme url exchanged page bytes 
average size web page kb url exchange consumes total network bandwidth 
overhead url exchange system quite significant 
processes need exchange message page message go tcp ip network stack sender receiver 
copied kernel space twice incurring context url hash graph computed analytically opposed measured site hash graph 
estimation average url bytes long 
switches kernel user mode 
operations pose significant overhead message size small believe overhead important processes exchange message downloaded page 
order study reduce overhead replication page item show communication overhead replicate top popular urls 
horizontal axis shows number replicated urls vertical axis shows communication overhead 
remember real world scenario identify popular urls image web previous crawl 
experiment identified top urls pages current webbase repository experiments 
replicated urls popular ones repository 
reason real world scenario actual communication overhead slightly worse results show 
clear significantly reduce communication overhead replicating relatively small number urls 
example proc run parallel overhead reduces urls page urls page reduction replicate urls 
see reduction diminishes number replicated urls increases 
example replicate urls get reduction process case get reduction replicate urls 
result recommend replicating proc order minimize communication overhead maintaining replication overhead low 
experiments pages results similar run experiments larger dataset 
note results depend fraction links pointing top urls 
believe links experiments sample web pages downloaded web links pages created independent authors 
similar fraction links point top urls download pages equivalently web grows time 
quality batch communication discussed quality section item parallel crawler worse single process crawler proc may crawling decisions solely information collected partition 
study quality issue 
discussion study impact batch communication technique page item quality 
experiments section assume crawler uses number backlinks page importance ori 
pages web links page importance 
clearly exist ways define importance page metric variations existing search engines :10.1.1.109.4049
note metric depends global structure web 
importance metric solely depends page global structure web quality parallel crawler essentially single crawler proc parallel crawler decisions pages downloaded 
backlink metric proc experiments counted backlinks page downloaded pages visited page backlinks 
remember proc need periodically exchange messages inform inter partition backlinks 
depending exchange messages quality downloaded pages differ 
example proc exchange messages quality firewall mode crawler exchange messages downloaded page quality similar single process crawler 
study issues compared quality downloaded pages proc exchanged backlink messages various intervals show results figures 
graph shows quality achieved crawler downloaded total pages respectively 
horizontal axis graphs represents total number url exchanges crawl vertical axis shows quality experiment 
example proc exchanged backlink count information middle crawl 
case represents quality firewall mode crawler case shows quality single process crawler 
show communication overhead page item average number url backlink count pairs exchanged downloaded page 
figures observe trends number crawling processes increases quality downloaded pages worse exchange backlink messages 
example quality achieved process crawler significantly higher process crawler firewall mode 
result proc learns global backlink counts web split smaller parts 
quality firewall mode crawler significantly worse single process crawler crawler downloads relatively small fraction pages 
difference significant crawler downloads relatively large fraction 
experiments crawler downloaded pages difference negligible case 
due space limitations show graphs 
intuitively result sense quality important issue crawler downloads quality processes number url exchanges communication overhead processes number url exchanges url exchange vs quality url exchange vs communication crawlers downloaded pages quality processes number url exchanges communication overhead processes number url exchanges url exchange vs quality url exchange vs communication crawlers downloaded pages small portion web 
crawler visit pages anyway quality relevant 
communication overhead increase linearly number url exchange increases 
graphs straight lines 
result popular url appear multiple times backlink exchanges 
popular url transferred entry url backlink count exchange appeared multiple times 
reduction increases proc exchange backlink messages frequently 
need large number url exchanges achieve high quality 
multiple experiments tried identify proc exchange backlink messages achieve highest quality value 
experiments parallel crawler get highest quality values processes communicate times crawl 
example illustrate results experiments 
example medium scale search engine say plan operate medium scale search engine quality processes number url exchanges communication overhead processes number url exchanges url exchange vs quality url exchange vs communication crawlers downloaded pages want maintain web pages index 
plan refresh index month 
machines individual links mbps internet 
order update index month need mbps download bandwidth run proc machines 
download case achieve highest quality proc exchange backlink messages times crawl processes run parallel 
process case closest number 
see proc exchange messages times crawl need exchange fewer url backlink count pairs total 
total network bandwidth backlink exchange bandwidth actual page downloads 
exchange happens times crawl context switch overhead message transfers discussed page minimal 
note scenario need exchange backlink messages month message days 
connection proc unreliable sporadic exchange mode problem 
crawlers collect web data search engine caches data mining 
size web grows increasingly important parallel crawlers 
unfortunately known open literature options parallelizing crawlers performance 
addresses shortcoming presenting architectures strategies parallel crawlers studying performance 
believe offers useful guidelines crawler designers helping example select right number crawling processes select proper inter process coordination scheme 
summary main study small number crawling processes run parallel experiment firewall mode provides coverage 
firewall mode crawlers run totally independently easy implement believe option consider 
cases firewall mode appropriate 
need run crawling processes 
download small subset web quality downloaded pages important 
crawler exchange mode consumes small network bandwidth url exchanges network bandwidth 
minimize overheads adopting batch communication technique 
experiments crawler maximize quality downloaded pages exchanged backlink messages times crawl 
replicating popular urls reduce communication overhead roughly 
replicating urls reduce overhead 
thomas anderson michael dahlin neefe david patterson drew roselli randolph wang 
serverless network file systems 
proceedings th symposium operating systems principles 
barabasi albert 
emergence scaling random networks 
science 
andrei broder ravi kumar maghoul prabhakar raghavan sridhar rajagopalan raymie stata andrew tomkins janet wiener 
graph structure web 
proceedings ninth world wide web conference 
mike burner 
crawling building archive world wide web 
web techniques magazine may 
soumen chakrabarti martin van den berg byron dom 
focused crawling new approach topic specific web resource discovery 
th international world wide web conference 
junghoo cho hector garcia molina 
evolution web implications incremental crawler 
proceedings sixth international conference large databases 
junghoo cho hector garcia molina 
synchronizing database improve freshness 
proceedings acm sigmod 
junghoo cho hector garcia molina lawrence page 
efficient crawling url ordering 
computers networks isdn systems 
coffman jr zhen liu richard weber 
optimal robot scheduling web search engines 
technical report inria 
diligenti coetzee lawrence giles gori 
focused crawling context graphs 
proceedings sixth international conference large databases 
david eichmann 
spider balancing effective search web load 
proceedings world wide web conference 
google www google com 
allan heydon marc najork 
mercator scalable extensible web crawler 
proceedings eighth world wide web conference pages 
hirschberg 
parallel algorithms transitive closure connected component problem 
proceedings th annual acm symposium theory computing 
martijn koster 
robots web threat treat 
connexions april 
kai li paul hudak 
memory coherence shared virtual memory systems 
acm transactions computer systems november 
oliver mcbryan 
wwww tools taming web 
proceedings world wide web conference 
robert miller krishna bharat 
sphinx framework creating personal site specific web crawlers 
proceedings seventh world wide web conference 
milojicic fred douglis yves richard wheeler zhou 
process migration 
acm computing surveys september 
david sahni 
parallel permutation sorting algorithms new generalized connection network 
journal acm july 
tamer ozsu patrick valduriez 
principles distributed database systems 
prentice hall 
lawrence page sergey brin :10.1.1.109.4049
anatomy large scale hypertextual web search engine 
proceedings seventh world wide web conference 
brian pinkerton :10.1.1.109.4049
finding people want experiences web crawler 
proceedings second world wide web conference 
michael quinn deo 
parallel graph algorithms 
acm computing surveys september 
robots exclusion protocol 
info webcrawler com mak projects robots exclusion html 
satyanarayanan kistler kumar okasaki siegel steere 
coda highly available file system distributed workstation environment 
ieee transactions computers april 
andrew tanenbaum robbert van renesse 
distributed operating systems 
acm computing surveys december 
george zipf 
human behaviour principle effort human ecology 
addison wesley 

