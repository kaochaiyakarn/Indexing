overview reservoir computing theory applications implementations benjamin david jan van electronics information systems department ghent university belgium benjamin 
training recurrent neural networks hard 
discovered possible just construct random recurrent topology train single linear readout layer 
state ofthe art performance easily achieved setup called reservoir computing 
idea broadened stating high dimensional driven dynamic system operated correct dynamic regime temporal kernel possible solve complex tasks just linear post processing techniques 
tutorial give overview current research theory application implementations reservoir computing 
machine learning feed forward structures artificial neural networks graphical bayesian models kernel methods extensively studied processing non temporal problems 
architectures understood due feed forward non dynamic nature 
applications temporal prediction weather dynamic systems financial data system control identification adaptive filtering noise reduction robotics gait generation planning vision speech recognition processing production 
short real world tasks 
possible solve temporal problems feed forward structures 
area dynamical systems modeling takens proposed hidden state dynamical system reconstructed adequate delayed embedding 
explicit embedding converts temporal problem spatial 
disadvantages approach artificially introduced time horizon parameters needed long delay introduced natural way represent time 
possible solution problem adding recurrent connections feed forward architectures mentioned 
recurrent connections transform system potentially complex dynamical system 
ground breaking hopfield dynamics recurrent neural network rnn controlled constructing specific topologies symmetrical weights 
system general chaotic hopfield architecture depends critically presence point attractors 
autonomous offline way initial state imposed dynamic system left running ends attractors 
line research initiated back propagation time learning rule recurrent networks werbos re discovered train weights fully sparsely connected recurrent neural network 
literature describes possible applications including learning context free context sensitive languages control modeling complex dynamical systems speech recognition 
rnns shown turing equivalent common activation functions approximate arbitrary finite state automata universal approximators 
theoretically rnns powerful tools solving complex temporal machine learning tasks 
application rules real world problems feasible due high computational training costs slow convergence possible achieve state art performance reserved experts field 
significant problem called fading gradient error gradient gets distorted time steps account short examples usable training 
possible solution specially constructed long short term memory lstm architecture outperform time delayed neural networks 
early random network spiking neurons short term plasticity known dynamic synapses shown plasticity introduces slower dynamics supported recurrent topology 
supervised correlation learning rule train separate output layer demonstrate 
idea random recurrent network left untrained processed simple classification regression technique independently reinvented jaeger echo state network maass liquid state machine 
echo state network esn consists random recurrent network analog neurons driven multi dimensional time signal activations neurons linear classification regression 
esn introduced better way computational power rnns training internal weights viewpoint reservoir acts complex nonlinear dynamic filter transforms input signals high dimensional temporal map operation explicit temporal kernel function 
possible solve classification tasks input signal adding multiple single reservoir 
jaeger proposed reservoir scaled operate edge stability setting spectral radius connection matrix close 
liquid state machine lsm maass originally general framework perform real time computation temporal signals description cortical model 
structured locally connected network spiking neurons randomly created biologically inspired parameters excited external input spike trains 
responses neurons projected cortical layer magnitude largest eigenvalue 
rough measure global scaling weights case eigenvalue spread 
actual training performed 
projection usually modeled simple linear regression function description liquid state machine supports advanced readout layers parallel perceptron trained delta learning rule 
descriptions liquid state machines node types published network threshold logic gates 
lsm literature shown reservoir perform optimally dynamic regime edge chaos 
research showed current state art learning rule rnns recurrent learning weight dynamics proposed jaeger maass outputs weights trained internal weights globally scaled bit ignored weights initially scaled 
lead learning rule rnns called backpropagation decorrelation output weights trained case output nodes nonlinear training done line manner 
fact idea independently discovered times discovered theoretical study state art rnn learning rules underlines importance concept 
literature concerning ideas spread different domains esn lsm research really interact 
authors tutorial proposed ideas unified common research stream propose call reservoir computing rc 
rc field active special session ijcnn workshop nips special issue neural network appear april esann special session 
ideas reservoir computing approached grounded field recurrent neural network theory different equally interesting view reservoirs compared kernel methods 
key idea kernel methods pre process input applying form transformation input space feature space usually far higher dimensional possibly infinite dimensional 
classification regression performed feature space 
power kernel methods lies mainly fact transformation need computed explicitly costly simply impossible 
transformation higher dimensional space similar functionality reservoir exist major differences case reservoirs transformation explicitly computed second kernels equipped cope temporal signals 
theory section general rc setup learning rules reservoir creation tuning rules theoretical results concerning computational showed unification experimental validation presenting matlab toolbox supports lsms 
bias wout fig 
shows general model rc system shows exact timing setup case teacher forced operation 
inp res wout power optimal dynamic regime 
general model exist variations global setup rc system generic architecture depicted 
training teacher forced state equations res res res res bias res inp bias 
weights matrices reservoir res initialized random connections output trained 
exact timing different components equations important differently publications 
propose logical functional point view timing 
system training res connections computed output fed back reservoir 
spiking neurons lsm spikes need converted analog values processed example linear regression 
conversion usually done exponential filtering resampling spike train 
readout implemented spiking learning rule conversion filtering dropped readout function described 
node types different neuron types literature linear nodes threshold logic gates hyperbolic tangent fermi neurons extra leaky integrator spiking neurons synaptic models dynamic synapses 
clear understanding node types optimal applications 
instance network best memory capacity consists linear neurons tasks node type far optimal 
evidence spiking neurons outperform analog neurons certain tasks 
spiking neurons shown synapse models dynamic synapses improve performance 
adding node integration analog neurons possible change timescale reservoir operates 
making transition continuous time model discrete time model euler integration time constant introduced effect slowing reservoir dynamics compared input 
theoretical capabilities promising performance variety practical applications done theoretical properties reservoirs 
specifically contributions lsm esn formulate prove necessary conditions general sense reservoirs possess order reservoirs 
concept lsm strongly inspired ideas neuroscience broad formulated theoretically 
class filters defined needs obey point wise separation property quite condition linear filters delay lines obey spiking neurons dynamic synapses 
readout function introduced operates outputs filters needs universal approximation property requirement quite hard instance satisfied simple linear regression function 
conditions satisfied resulting system approximate real time immediate computation temporal inputs 
theory lsm idea computationally powerful principle solve majority difficult temporal classification regression problems 
similarly theoretical requirements esn derived 
particular necessary sufficient conditions recurrent network echo states formulated spectral radius largest singular value connection matrix 
echo state property means informally stated current state network uniquely determined network input limit initial state 
network state forgetting 
theoretical results important necessary add credibility idea reservoir computing unfortunately offer immediate indications construct reservoirs certain desirable properties certain task consensus desirable properties 
lot current research reservoirs focuses reducing randomness search reservoir offering heuristic analytical measures reservoir properties predict performance 
memory capacity mc measure information contained input extracted reservoir certain time 
see 
reservoir creation scaling reservoirs randomly created exact weight distribution sparsity limited influence reservoir performance 
esn literature reservoirs rescaled measures stability bounds 
measures literature 
jaeger proposed spectral radius slightly lower reservoir guaranteed echo state property 
tighter bound echo state property described ideas robust control theory exact special connection topologies 
lsm literature measures exist harder construct reservoirs desired dynamic regime issue harder due dynamic properties simple spiking neurons refractoriness 
shown prior knowledge problem best create reservoir uniform pole placement possible frequencies maximally covered idea originated identification linear systems kautz filters 
random connectivity give clear insight going reservoir 
reservoir constructed consisting nodes random self connection 
topology called simple esn consists effectively random filter bank shown performance tasks slightly worse full esn 
training original lsm concept stated dynamic reservoir states processed statistical classification regression technique ranging form simple linear models kernel techniques 
esn literature uses linear regression readout function 
revival linear techniques kernel methods gaussian processes 
understood adding non linear techniques reservoir unclear exact influence non linear reservoir chose focus linear processing 
readout trained line batch line learning rules 
performing line training teacher forcing output fed reservoir readout 
weight matrix minimizes aw matrix consists concatenation inputs readout reservoir states inputs outputs bias matrix consists desired outputs 
weights moore penrose pseudo inverse 
original papers jaeger added noise matrix regularization 
better ridge regression norm weights minimized regularization parameter 
regularization parameter cross validation 
line training reservoirs mean squares learning algorithm quite inefficient due poor eigenvalue spread cross correlation matrix 
recursive squares learning rule better suited shown 
reservoir adaptation original rc concept uses fixed randomly created reservoirs considered main advantage quite research done altering reservoirs improve performance application 
train thought find optimal reservoir certain task 
idea fact performance randomly chosen reservoirs form distribution 
search algorithm easy perform better average choosing right reservoir 
done genetic algorithms 
possibility start large reservoir prune away bad nodes certain task 
idea session 
purest idea reservoir adaptation techniques reservoir parameters changed unsupervised learning rule input expected output 
way dynamics tuned input explicitly specializing expected response 
idea hebbian unsupervised learning rules jaeger stated significantly improve performance 
probably simple correlation anti correlation hebbian rules limited improve reservoir dynamics 
new way adapting reservoir parameters imposing certain output distributions reservoir nodes 
achieved changing global neuron parameters gain threshold individual weights 
type learning rules called intrinsic plasticity originally discovered real neurons derived analog neurons triesch rc 
ip able robustly impose dynamic regime network parameters 
certain tasks tractable reservoirs ip shown contribution session 
tuning ip distributions parameters smaller final performance variance setting spectral radius shown contribution session 
structured reservoirs initially reservoirs uniform structures specific internal structure 
look brain obvious specific structure important 
showed creating lsm global statistical structure discovered brain performance significantly increases compared unstructured reservoir 
single reservoir able support limited number timescales 
alleviated structured reservoir 
reservoirs interconnected sparse inhibitory connections aim decorrelate different sub reservoirs 
way different timescales live sub reservoir 
measures dynamics performance reservoir system highly dependent actual dynamics system 
due high degree nonlinear feedback static analysis dynamic regime stability measures bound inaccurate 
original jaeger frequently uses spectral radius reservoir connection matrix way analyzing controlling reservoir dynamics 
linear systems spectral radius hard limit stability dynamics approximation nonlinear systems reservoirs small signal values zero total input sigmoid somewhat linear 
actual reservoir dynamics applied specific task highly dependent actual inputs reservoir system reaches steady state 
shown effective spectral radius linearized system models reservoir dependent constant bias input scaling input large inputs bias leads smaller effective spectral radius 
possible construct chaotic reservoir transiently stable driven large external signal 
considerations quest accurate measure predictor reservoir performance lead development measure reservoir dynamics takes actual task dependent input account 
application local lyapunov measure reservoir dynamics introduced usability performance predictor evaluated 
tasks considered pseudo measure offers measure task dependent reservoir dynamics links static parameters spectral radius reservoirs performing optimally certain tasks actual dynamics 
applications reservoir computing generally suited solving temporal classification regression prediction tasks performance usually attained having care specifically setting reservoir parameters 
real world applications crucial natural time scale reservoir tuned order magnitude important time scales temporal application 
successful applications reservoir computing real world engineering applications reported literature 
applications include dynamic pattern classification autonomous sine generation computation highly nonlinear functions instantaneous rates spike trains 
robotics lsms control simulated robot arm model existing robot controller perform object tracking motion prediction event detection applications robocup competitions motor control 
context reinforcement learning 
applications field digital signal processing dsp quite successful speech recognition noise modeling 
application brain machine interfacing 
reservoirs chaotic time series generation prediction reported 
area chaotic time series prediction isolated digit recognition rc techniques outperform state art approaches 
striking case demonstrated possible predict mackey glass chaotic time series orders magnitude farther time classic techniques 
implementations rc concept introduced rnn reservoir broader concept high dimensional dynamic system right dynamic properties temporal kernel basis code pre process data easily processed linear techniques 
examples real bucket water reservoir speech recognition genetic regulatory network cell actual brain cat 
software toolbox open source gpl toolbox released implements broad range rc techniques 
toolbox consists functions allow user easily set datasets reservoir topologies experiment parameters easily process results experiments 
supports analog linear sign tanh 
spiking lif neurons transparent way 
spiking neurons simulated optimized event simulator 
discussed tutorial implemented toolbox 
hardware time writing implementation rc hardware exist focus neural reservoirs 
snn reservoir implemented digital hardware eu fp facets project currently pursuing large scale implementation reservoir 
ann reservoir implemented digital hardware stochastic bit stream arithmetic 
open research questions rc relatively new research idea open issues research directions 
theoretical standpoint proper understanding reservoir dynamics measures dynamics important 
theory regularization readout reservoir dynamics currently missing 
influence hierarchical structured topologies understood 
bayesian inference rc needs investigated 
intrinsic plasticity reservoir adaptation needs investigated 
implementation standpoint rc concept extended way rnn high dimensional dynamical system operating proper dynamical freely available www 
regime theoretically quite hard define reservoir 

new results recurrent network training unifying algorithms accelerating convergence 
ieee neural networks 
young 
tighter bound echo state property 
ieee neural networks 

temporal information transformed spatial code neural network realistic properties 
science 

learning recurrent spiking neural networks applications robot control real world devices 
phd thesis graz university technology 

training networks biological realistic spiking neurons real time robot control 
proc 
eann pages lille france 
bush anderson 
modeling reward functions incomplete state representations echo state networks 
proc 
ijcnn 
dai 
genetic regulatory systems modeled recurrent neural network 
proc 
pages 
fernando 
pattern recognition bucket 
proc 
ecal pages 
eggert 
short term memory pattern matching simple echo state networks 
proc 
icann pages 

approximation dynamical systems continuous time recurrent neural networks 
neural networks 
gers schmidhuber 
lstm recurrent networks learn simple context free context sensitive languages 
ieee neural networks 
graves eck schmidhuber 
biologically plausible speech recognition lstm neural nets 
proc 
bio pages 
maass 
statistical analysis information processing properties specific cortical models 
cerebral cortex 
hammer 
perspectives learning recurrent neural networks 
proc 
esann 
hertzberg jaeger sch 
learning ground fact symbols robots 
proc 
ecai pages 
hopfield 
neural networks physical systems emergent collective computational abilities 
proc 
national academy sciences usa volume pages 
jaeger 
echo state approach analysing training recurrent neural networks 
technical report german national research center information technology 
jaeger 
short term memory echo state networks 
technical report german national research center information technology 
jaeger 
adaptive nonlinear system identification echo state networks 
proc 
nips pages 
jaeger 
reservoir echo state network research extended 
proc 
ijcnn pages august 
jaeger haas 
harnessing nonlinearity predicting chaotic systems saving energy wireless telecommunication 
science 
jaeger 
optimization applications echo state networks leaky integrator neurons 
neural networks 
appear 
jones rowe fernando 
liquid state machine bacterium escherichia coli 
ieee artificial life submitted 
joshi maass 
movement generation control generic neural 
proc 
bio audit pages 
kilian siegelmann 
dynamic universality sigmoidal neural networks 
information computation 
maass 
new directions statistical signal processing systems brain chapter dynamical system computationally powerful 
mit press 
maass 
edge chaos prediction computational performance neural models 
neural networks press 
maass 
new approach vision suggested biologically realistic neural models 
proc 
nd workshop biologically motivated computer vision 
maass ger 
real time computing stable states new framework neural computation perturbations 
neural computation 
maass ger 
model real time computation generic neural 
proc 
nips pages 
maass ger fading memory kernel properties generic cortical models 
journal physiology 
singer maass 
temporal dynamics information content carried neurons primary visual cortex 
proc 
nips 
press 
giles 
constructing deterministic finite state automata sparse recurrent neural networks 
proc 
icnn pages 
levi 
velocity control omnidirectional robocup player recurrent neural networks 
proc 
robocup symposium pages 
principe 
computing transiently stable states 
proc 
ijcnn 
xu principe 
analysis design echo state networks 
neural computation 
pl ger nther 
echo state networks mobile robot modeling control 
robocup pages 
nonlinear dynamical systems kalman filter trained recurrent networks 
ieee neural networks 
rao 
kim sanchez principe 
learning mappings brain machine interfaces echo state networks 
proc 
icassp pages 
robinson 
application recurrent nets phone probability estimation 
ieee neural networks 
rodriguez 
simple recurrent networks learn context free context sensitive languages counting 
neural computation 
rumelhart hinton williams 
parallel distributed processing chapter learning internal representations error propagation 
mit press 
pl ger 
echo state networks motor control 
proc 
icra pages 
schmidhuber hochreiter 
long short term memory 
neural computation 
schmidhuber gomez 
training recurrent networks 
neural computation 
sch lkopf smola 
learning kernels support vector machines regularization optimization 
mit press 
van 
compact hardware real time speech recognition liquid state machine 
proc 
ijcnn 
submitted 
sch rmann meier 
edge chaos computation mixed mode vlsi hard liquid 
proc 
nips 
harris 
minimum mean squared error time series classification echo state network prediction model 
proc 
iscas 

memory backpropagation decorrelation efficient online recurrent learning 
proc 
icann 

online stability backpropagation decorrelation recurrent learning 
neurocomputing 

online reservoir adaptation intrinsic plasticity echo state learning 
neural networks 
appear 

backpropagation decorrelation online recurrent learning complexity 
proc 
ijcnn volume pages 
suykens vandewalle de moor 
artificial neural networks modeling control non linear systems 
springer 
takens rand 
detecting strange attractors turbulence 
dynamical systems turbulence volume lecture notes mathematics pages 
springer verlag 
triesch 
synergies intrinsic synaptic plasticity individual model neurons 
proc 
nips volume 
van der ishii 
pl ger 
finding echo state networks control underwater robot evolutionary computations 
proc 


unifying comparison reservoir computing methods 
neural networks 
appear 

reservoir computing stochastic bitstream neurons 
proc 
workshop pages 
van 
isolated word recognition liquid state machine case study 
information processing letters 
werbos 
regression new tools prediction analysis behavioral sciences 
phd thesis harvard university boston ma 
werbos 
backpropagation time 
proc 
ieee 
xue yang haykin 
decoupled echo state networks lateral inhibition 
ieee neural networks 
zhang linden 
side experience driven changes neuronal intrinsic 
nature reviews neuroscience 
