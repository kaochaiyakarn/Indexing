chapter 
loop parallelization algorithms alain darte yves robert fr ric vivien lip ecole normale sup rieure de lyon lyon cedex france alain darte yves robert frederic vivien lip ens lyon fr summary 
chapter devoted comparative survey loop parallelization algorithms 
various algorithms literature introduced allen kennedy wolf lam darte vivien feautrier 
algorithms different mathematical tools 
rely representation data dependences 
chapter survey algorithms assess power limitations examples stating optimality results 
important contribution chapter characterize algorithm suitable representation dependences 
result practical interest provides guidance compiler dependence analysis available simplest cheapest parallelization algorithm remains optimal selected 

loop parallelization algorithms useful source source program transformations 
particularly appealing applied knowledge target architecture 
viewed machineindependent step code generation process 
loop parallelization detect parallelism transforming loops doall loops expose dependences responsible intrinsic sequentiality operations original program 
course second step code generation take machine parameters account 
determining granularity generally key efficient performance 
data distribution communication optimization important issues considered 
problems addressed stage 
step approach typical field parallelizing compilers examples general task graph scheduling software pipelining 
chapter devoted study various parallelism detection algorithms 
simple decomposition dependence graph strongly connected components allen kennedy algorithm 

unimodular loop transformations ad hoc transformations banerjee algorithm generated automatically wolf lam algorithm 

schedules mono dimensional schedules particular case hyperplane method multi dimensional schedules 
pande agrawal eds compiler optimizations scalable ps lncs pp 

springer verlag berlin heidelberg alain darte yves robert fr ric vivien loop parallelization algorithms different number reasons 
various mathematical techniques graph algorithms matrix computations linear programming 
second take different description data dependences input graph description dependence levels direction vectors description dependences polyhedra affine expressions 
algorithms identify key concepts underline discuss respective power limitations examples stating optimality results 
important contribution chapter characterize algorithm suitable representation dependences 
need sophisticated dependence analysis algorithm parallelization algorithm take advantage precision result 
conversely need sophisticated parallelization algorithm dependence representation precise 
rest chapter organized follows 
section devoted brief summary loop parallelization algorithms 
section review major dependences abstractions dependence levels directions vectors dependence polyhedra 
allen kennedy algorithm section wolf lam algorithm section 
shown algorithms optimal class parallelization algorithms dependence abstraction input dependence levels allen kennedy direction vectors wolf lam 
section move new algorithm subsumes previous algorithms 
algorithm generalization direction vectors dependence polyhedra 
section briefly survey feautrier algorithm relies exact affine dependences 
state section 
input output parallelization algorithms nested loops enable describe set computations size larger corresponding program size 
example consider nested loops loop counters describe cube size encapsulate set computations size furthermore happens loop nests contain non trivial degree parallelism set independent computations size forr 
parallelization nested loops challenging problem compiler able detect possible non trivial degree parallelism compilation time proportional sequential execution time loops 
possible efficient parallelization algorithms proposed complexity size output size depend certainly depend size sequential code number computations described 
loop parallelization algorithms input parallelization algorithms description dependences link different computations 
output description equivalent code explicit parallelism 
input dependence graph statement loop nest surrounded loops 
iteration loops defines particular execution statement called operation 
dependences operations represented directed acyclic graph expanded dependence graph edg 
vertices edg operations loop nest 
executing operations loop nest respecting partial order specified edg guarantees correct result loop nest preserved 
detecting parallelism loop nest amounts detecting anti chains edg 
illustrate notion expanded dependence graph example 
edg corresponding code depicted 
example 
enddo enddo fig 

example edg 
unfortunately edg input parallelization algorithms usually large may described exactly compile time 
reduced dependence graph rdg 
rdg condensed approximated representation edg 
approximation superset edg order preserve dependence relations 
rdg vertex statement loop nest edges labeled chosen approximation dependences see section details 
presents possible example corresponding different approximations dependences 
input rdg edg parallelization algorithm able distinguish different rdg 
parallelism detected parallelism contained alain darte yves robert fr ric vivien fig 

rdg dependence levels direction vectors 
rdg 
quality parallelization algorithm studied respect dependence analysis 
example example example rdg dependence levels 
parallelization algorithm takes input dependence levels distinguish codes 
example contains degree parallelism example intrinsically sequential 
example 
enddo enddo output nested loops size parallelized code noticed depend number operations described 
reason output parallelization algorithm described set loops ways define new order operations loop nest ways define output parallelization algorithm terms nested loops 
elementary loop transformations basic steps algorithm loop distribution allen kennedy algorithm loop interchange loop skewing banerjee algorithm 
apply linear change basis iteration domain apply unimodular transformation iteration vectors wolf lam algorithm 

define dimensional schedule apply affine transformation interpret transformation multi dimensional timing function 
component correspond sequential loop loops arbitrarily complicated long complexity depends size initial code 
obviously simpler result better 
context meaning simple clear depends optimizations may follow 
consider structural simplicity preferable discussed 
loop parallelization algorithms missing dimensions correspond doall loops feautrier algorithm darte vivien algorithm 
output transformation schemes described loop nests complicated rewriting processes see 
discuss rewriting process 
focus link representation dependences input loop transformations involved parallelization algorithm output 
goal characterize algorithm optimal representation dependences 
optimal means algorithm succeeds exhibiting maximal number parallel loops 

dependence abstractions sake clarity restrict case perfectly nested loops affine loop bounds 
restriction permits identify iterations nested loops called depth loop nest vectors called iteration vectors contained finite convex polyhedron called iteration domain bounded loop bounds 
th component iteration vector value th loop counter nest counting outermost innermost loop 
sequential code iterations executed lexicographic order iteration vectors 
sections denote polyhedral iteration domain jn dimensional iteration vectors th statement loop nest lexicographically greater section recalls different concepts dependence graphs introduced informal discussion section expanded dependence graphs edg reduced dependence graphs rdg apparent dependence graphs adg notion distance sets 
section formally define call polyhedral reduced dependence graphs reduced dependence graphs edges labeled polyhedra 
section show model generalizes classical dependence abstractions distance sets dependence levels direction vectors 
dependence graphs distance sets dependence relations operations defined bernstein conditions 
briefly speaking operations considered dependent operations access memory location accesses write 
dependence directed sequential order executed operation 
depending order write read dependence corresponds called alain darte yves robert fr ric vivien flow dependence anti dependence output dependence 
si sj statement sj iteration depends statement si iteration partial order defined describes expanded dependence graph edg 
lexicographically nonnegative si sj 
general edg computed compile time information missing values size parameters worse precise memory accesses generating graph expensive see survey dependence tests gcd test power test omega test lambda test details exact dependence analysis 
dependences captured smaller cyclic directed graph vertices statements called reduced dependence graph rdg statement level dependence graph 
rdg compression edg 
rdg statements si sj said dependent write si sj exists pair si sj 
furthermore edge si sj rdg labeled set si sj approximation de contains set 
precision representation approximation power dependence analysis 
words rdg describes condensed manner iteration level dependence graph called maximal apparent dependence graph adg superset edg 
adg edg vertices adg edges defined si sj si sj suchthat de 
certain class nested loops possible express exactly set pairs see affine function particular cases involving floor ceiling functions fi varies polyhedron pi si sj fi pi dependence analysis algorithms set pairs computes set ei possible values 
ei called set distance vectors set ei si sj exact dependence analysis feasible equation shows set distance vectors projection integer points polyhedron 
set approximated convex hull accurate edge pair memory accesses induces dependence si sj 
loop parallelization algorithms description larger polyhedron finite union polyhedra 
set distance vectors represented finite union corresponding dependence edge rdg decomposed multi edges 
note representation distance vectors equivalent representation pairs equation information concerning location edg distance vector lost 
may cause loss parallelism seen example 
representation remains important especially exact dependence analysis expensive feasible 
classical representations distance sets increasing precision level dependence introduced allen kennedy parallelizing algorithm 
direction vector introduced lamport wolfe wolf lam parallelizing algorithm 
dependence polyhedron introduced irigoin supernode partitioning algorithm 
refer pips software details dependence polyhedra 
formally define reduced dependence graphs edges labeled dependence polyhedra 
show representation subsumes representations dependence levels direction vectors 
polyhedral reduced dependence graphs recall mathematical definition polyhedron decomposed vertices rays lines 
definition polyhedron polytope 
vectors called convex polyhedron exists integral matrix integral vector polytope bounded polyhedron 
ax polyhedron decomposed sum polytope polyhedral cone details see 
polytope defined vertices point polytope non negative barycentric combination polytope vertices 
polyhedral cone finitely generated defined rays lines 
point polyhedral cone sum nonnegative combination rays combination lines 
dependence polyhedron equivalently defined set vertices denoted set rays denoted alain darte yves robert fr ric vivien set lines denoted 
set vectors ivi iri ili 
define call polyhedral reduced dependence graph reduced dependence graph labeled dependence polyhedra 
interested integral vectors belong dependence polyhedra dependence distance integral vectors 
definition 
polyhedral reduced dependence graph rdg edge si sj labeled dependence polyhedron approximates set distance vectors associated adg contains edge instance node si instance node sj 
explore section representation dependences 
sight reader see dependence polyhedra generalization direction vectors 
definition simulation classical dependence representations come back classical dependence abstractions level dependence direction vector 
recall definition show labeled direction vectors dependence levels particular cases polyhedral reduced dependence graphs 
direction vectors set distance vectors singleton dependence said uniform unique distance vector called uniform dependence vector 
set distance vectors represented dimensional vector called direction vector components belong 
th component approximation th components possible distance vectors equal resp th components greater resp 
smaller equal th component may take value dependence uniform dimension unique value general resp 
shorthand resp 

denote ei th canonical vector dimensional vector components null th component equal 
direction vector approximation polyhedron single vertex rays lines canonical vectors 
consider edge labeled direction vector denote respectively equal loop parallelization algorithms integer 
denote dz dimensional vector th component equal th component equal 
definition symbols direction vector represents exactly dimensional vectors exist integers inn dz iei iei iei words direction vector represents integer points belong polyhedron defined single vertex dz rays ei lines ei example direction vector defines polyhedron vertex rays line 
dependence levels representation level accurate dependence abstraction 
loop nest nested loops set distance vectors approximated integer defined largest integer components distance vectors zero 
dependence level means dependence occurs depth loop nest iteration case says dependence loop carried dependence level ifl dependence occurs inside loop body different statements called loop independent dependence 
reduced dependence graph edges labeled dependence levels called reduced leveled dependence graph 
consider edge level definition level non zero component distance vectors th component possibly take positive integer value 
furthermore information remaining components 
edge level equivalent direction vector edge level corresponds null dependence vector 
direction vector admits equivalent polyhedron representation level 
example level dependence dimensional loop nest means direction vector corresponds polyhedron vertex ray line 

allen kennedy algorithm allen kennedy algorithm designed vectorizing loops 
extended maximize number parallel loops minimize number synchronizations transformed code 
input algorithm 
alain darte yves robert fr ric vivien allen kennedy algorithm facts 
loop parallel loop carried dependence dependence level equal depth loop concerns statement surrounded loop 

iterations statement carried iteration statement dependence 
property allows mark loop doall loop property suggests parallelism detection independently conducted strongly connected component 
parallelism extraction done loop distribution 
algorithm dependence graph subgraph dependences level strictly smaller removed 
sketch algorithm basic formulation 
initial call allen kennedy 
allen kennedy 

decompose strongly connected components gi sort topologically 
different loop nest level order gi preserved distribution loops level 
gi mark loop level doall loop gi edge level mark loop loop 
gi kennedy gi 
illustrate allen kennedy algorithm code example 
enddo enddo enddo dependence graph drawn strongly connected component edge level call finds outermost loop sequential 
level edge level longer considered strongly connected components iterations statement carried loop parallelization algorithms fig 

example 
iteration statement 
loop distribution performed 
strongly connected component including contains edge level edge level 
second loop surrounding marked third doall 
strongly connected component including contains edge level edge level 
second loop surrounding marked doall third 
get doall enddo enddo doall enddo enddo enddo power limitations shown statement initial code surrounding loops possible detected parallel loops allen kennedy algorithm 
precisely consider statement initial code li surrounding loops 
li marked parallel dependence level instances result proves algorithm optimal parallelization algorithms describe transformed code instances exactly loops initial code 
fact stronger result proved theorem 
algorithm allen kennedy optimal parallelism detection algorithms input reduced leveled dependence graph 
proved loop nest exists loop nest statement surrounded parallelization ds sequential loops exists alain darte yves robert fr ric vivien exact dependence graph dependence path includes ds instances statement words allen kennedy algorithm distinguishes parallelization algorithm optimal strongest sense reaches statement upper bound parallelism defined longest dependence paths edg 
proves long information available possible find parallelism allen kennedy algorithm 
words algorithm allen kennedy adapted representation dependences dependence levels 
detect parallelism algorithm allen kennedy information dependences required 
classical examples possible overcome algorithm allen kennedy example simple interchange reveals parallelism example simple skew interchange sufficient 
example 
enddo enddo fig 

example code rdg 
example 
enddo enddo fig 

example code rdg 

wolf lam algorithm examples contain parallelism detected allen kennedy algorithm 
shown theorem loop parallelization algorithms parallelism extracted dependences represented dependence levels 
overcome limitation wolf lam proposed algorithm uses direction vectors input 
unifies previous algorithms elementary matrix operations loop skewing loop interchange loop reversal unique framework framework valid unimodular transformations 
purpose wolf lam aim building sets fully permutable loop nests 
fully permutable loops basis tiling techniques 
tiling expose medium grain coarse grain parallelism 
furthermore set fully permutable loops rewritten single sequential loop parallel loops 
method express fine grain parallelism 
wolf lam algorithm builds largest set outermost fully permutable loops 
looks recursively remaining dimensions dependences satisfied loops 
version builds set loops case analysis simple examples relies heuristic loop nests depth greater equal 
rest section explain algorithm theoretical perspective provide general version algorithm 
theoretical interpretation unimodular transformations main advantages linearity invertibility 
unimodular transformation linearity allows easily check valid transformation 
valid td non zero distance vectors invertibility enables rewrite easily code transformation simple change basis general td checked distance vectors 
tries guarantee td non zero direction vectors usual arithmetic conventions 
consider non zero direction vectors known lexicographically positive see section 
denote 
rows closure cone generated direction vectors 
direction vector td kd kd kd andt kd 
means dependences represented carried loop level kd 
direction vectors dependences carried loop inner loops doall loops 
called th th loops permutable th th components distance vector depth nonnegative 
alain darte yves robert fr ric vivien timing vector separating hyperplane 
timing vector exists pointed contains linear space 
equivalent fact cone defined full dimensional see details cones related notions 
building linearly independent vectors permits transform loops fully permutable loops 
notion timing vector heart hyperplane method variants see particularly interesting exposing finegrain parallelism notion fully permutable loops basis tiling techniques 
said formulations strongly linked cone pointed dimension dimension space linearly independent vectors transform loop nest outermost loops fully permutable 
recursively apply technique transform innermost loops considering direction vectors carried outermost loops considering direction vectors included space thisis general idea wolf lam algorithm obviously express terms 
general algorithm discussion summarized algorithm wolf lam 
algorithm wolf lam takes input set direction vectors sequence linearly independent vectors initialized void transformation matrix built wolf lam 
define closure cone generated direction vectors define dimension 
complete set linearly independent vectors construction 
subset defined lin space 
call wolf lam 
process may lead non unimodular matrix 
building desired unimodular matrix done follows set direction vectors 
set call wolf lam 
build non singular matrix rows vectors order 
pt chosen integral matrix 
compute left hermite form qh nonnegative lower triangular unimodular 
desired transformation matrix pq ht 
loop parallelization algorithms illustrate algorithm example example 
enddo enddo enddo fig 

example code rdg 
set direction vectors see 
space dimensional generated 
dimensional generated 
pointed 
complete vectors example 
particular example transformation matrix rows unimodular corresponds simple loop skewing 
exposing doall loops choose vector relative interior example 
terms loops transformations amounts skewing loop factor interchanging loops doall max min enddo enddo enddo power limitations wolf lam showed methodology optimal theorem 
algorithm finds maximum coarse grain parallelism recursively calls inner loops produces maximum degree parallelism possible 
strangely gave hypothesis theorem 
theorem understood respect dependence analysis direction vectors information structure dependence graph 
correct formulation alain darte yves robert fr ric vivien theorem 
algorithm wolf lam optimal parallelism detection algorithms input set direction vectors implicitly considers loop nest statement statements form atomic block 
algorithm allen kennedy sub optimality algorithm wolf lam general case algorithm methodology weakness input fact structure rdg exploited may result loss parallelism 
example contrarily algorithm allen kennedy algorithm wolf lam finds parallelism example rdg typical structure direction vectors 
fig 

reduced dependence graph direction vectors example 

darte vivien algorithm section introduce third parallelization algorithm takes input polyhedral reduced dependence graphs 
explain motivation section proceed step step presentation algorithm 
examples 
algorithm needed seen parallelization algorithms far 
algorithm may output pure sequential code examples algorithm find parallelism 
motivates search new algorithm subsuming algorithms wolf lam allen kennedy 
reach goal imagine combine algorithms simultaneously exploit structure rdg structure direction vectors compute cone generated direction vectors transform loop nest expose largest outermost fully permutable loop nest consider subgraph rdg formed direction vectors carried loop parallelization algorithms outermost loops compute strongly connected components apply loop distribution order separate components recursively apply technique component 
strategy enables expose parallelism combining unimodular transformations loop distribution 
optimal example illustrates 
example combining algorithms allen kennedy wolf lam proposed enables find degree parallelism second phase rdg remains strongly connected 
better basic algorithm allen kennedy 
find degrees parallelism example scheduling time step time step 
example 
enddo enddo enddo fig 

example code rdg 
consequently single parallelization algorithm finds parallelism allen kennedy wolf lam 
obvious solution try allen kennedy lam combination algorithms report best answer 
naive approach powerful uses dependence graph structure allen kennedy direction vectors wolf lam benefits step 
example proposed combination algorithms dependence graph structure computation maximal set fully permutable loops computation 
claim information graph structure direction vectors simultaneously 
key concept scheduling cone generated direction vectors weights edges rdg turns cone generated weights cycles rdg 
motivation multi dimensional scheduling algorithm 
seen combination unimodular transformations loop distribution index shift method 
algorithm subsumes algorithms allen kennedy wolf lam 
motivate alain darte yves robert fr ric vivien choice representation dependences algorithm works 
polyhedral dependences motivating example section example contains parallelism detected dependences represented levels direction vectors 
need exact representation dependences find parallelism loop nest 
representation dependences dependence polyhedra enables parallelize code 
example 
enddo enddo flow flow anti fig 

example source code exact dependence relations consider example 
exact dependences listed shows corresponding reduced dependence graphs dependence edges labeled respectively levels direction vectors 
output favorite parallelization algorithms 
fig 

rdg example levels direction vectors 
allen kennedy 
levels dependences respectively 
dependence cycle depth depth 
parallelism detected 
wolf lam 
dependence vectors respectively 
second dimension prevent detect fully permutable loops 
code remains unchanged parallelism detected 
loop parallelization algorithms feautrier 
algorithm described section 
takes input exact dependences 
leads valid schedule 
level parallelism detected 
particular example representation dependences levels direction vectors accurate reveal parallelism 
reason allen kennedy wolf lam able detect parallelism 
exact dependence analysis associated linear programming methods require solve large parametric linear programs feautrier algorithm enables reveal degree parallelism 
corresponding parallelized code max min enddo enddo example exact representation dependences necessary reveal parallelism 
notice uniform dependence set distance vectors approximated set 
polyhedron vertex ray 
suppose looking linear schedule 
letx 
valid schedule look xd dependence vector inequality equal equivalent xv 
just solve inequalities xu xv xr leads feautrier 
example approximation dependences levels direction vectors sufficient detection parallelism 
approximation dependences polyhedra find parallelism exact dependence analysis solving simpler set inequalities 
important enables go inequality set uniform inequalities affine constraints disappear need affine form farkas lemma anymore feautrier algorithm number inequalities variables related number constraints define validity domain dependence relation 
alain darte yves robert fr ric vivien see section 
better understand principle think terms dependence path 
idea consider edge statement statement labeled distance vector uses uniform dependence vector times uniform dependence vector simulation summarized introduce new node enables simulate null weight edge go back initial node principle underlying idea loop parallelization algorithm described section 
fig 

simulation edge labeled polyhedron vertex ray 
dependences fact constraints transformed underlying affine scheduling problem simple scheduling problem dependences uniform andr 
fundamental differences framework classical framework uniform loop nests uniform dependence vectors necessarily lexico positive example ray equal 
scheduling problem difficult 
solved techniques similar solve problem computability systems uniform recurrence equations 
constraint imposed ray weaker classical constraint constraint xr xr 
freedom taken account parallelization algorithm 
illustrating example example assuming reduced dependence graph edges labeled direction vectors 
dependence graph depicted built dependence analyzer tiny 
reader check allen kennedy lam able find full parallelism code third statement purely sequential 
parallelism detection algorithm propose sections able build multi dimensional schedule statement second statement loop parallelization algorithms example 
enddo enddo enddo fig 

example code rdg 
third statement 
schedule corresponds code explicit parallelism effort loop peeling remove tests 
statement level parallelism detected 
enddo enddo enddo endif enddo endif enddo enddo code generated schedule procedure codegen omega calculator delivered petit 
point code proposed virtual code sense reveals hidden parallelism 
claim implemented 
omega calculator framework compute dependences check validity program transformations transform programs transformation 
alain darte yves robert fr ric vivien step show polyhedral reduced dependence graphs captured equivalent simpler manipulate structure structure uniform dependence graphs graphs edges labeled constant dependence vectors 
scheme achieved translation algorithm 
avoid possible confusions vertices dependence graph vertices dependence polyhedron call nodes vertices 
furthermore initial describes dependences code parallelized called original graph denoted go 
uniform rdg equivalent go built translation algorithm called uniform graph translated go denoted gu 
translation algorithm builds gu scanning edges go 
gu edge adds gu new nodes new edges depending polyhedron 
call virtual nodes nodes created opposed actual nodes correspond nodes go 
edge ye respectively tail head nodes respectively leaves enters xe ye 
definition generalized paths head resp 
tail path head resp 
tail resp 
edge 
follow notations introduced section denote number vertices vi lines li polyhedron 
translation algorithm 
add add edges weights directed xe ne add self loops ne weights add self loops ne weights add self loops ne weights add null weight edge directed ne ye 
back example 
example drawn 
shows uniform dependence graph associated 
new nodes gray virtual nodes correspond symbol symbols initial direction vectors 
scheduling step scheduling step takes input translated dependence graph gu builds multi dimensional schedule actual node node loop parallelization algorithms fig 

translated uniform reduced dependence graph 
gu corresponds node go 
gu assumed strongly connected algorithm called strongly connected component gu 
recursive algorithm 
step recursion builds particular subgraph current graph processed 
built set linear constraints derived valid schedule satisfies dependence edges computed 
algorithm keeps working remaining edges edges precisely additional edges see 
defined subgraph generated edges belong multi cycle null weight 
multi cycle union cycles necessarily connected weight union cycles sum weights constitutive cycles 
built resolution linear program see section 
scheduling step summarized recursive algorithm 
initial call darte vivien gu 
algorithm builds actual node gu sequence vectors ds xs sequence constants ds define valid multi dimensional schedule 
darte vivien 

build subgraph generated edges belong null weight multi cycle 
add ye self loops ye xe ye edge virtual node ye 

select vector constant node xe ye xe virtual node xw ye xe xe ye xe actual node xw ye xe actual nodes alain darte yves robert fr ric vivien 
empty virtual nodes return 

strongly connected actual node computable initial go consistent return 

decompose strongly connected components gi call darte vivien gi subgraph gi actual node 
remarks step necessary general example removed labeled direction vectors details see 
case resolution single linear program simultaneously solve step step 
step specify purpose vector constants selected allow various selection criteria 
example maximal set linearly independent vectors selected goal derive fully permutable loops see details 
back example consider uniform dependence graph 
elementary cycles weights weights twice 
edges edges belong cycle weight belong multi cycle null weight 
subgraph drawn 
fig 

subgraph null weight multi cycles example 
constraints coming edges impose orthogonal weight cycles considering constraints find solution ing remain strongly connected loop parallelization algorithms components considered virtual nodes 
components null weight multi cycles 
strongly connected component single node scheduled vector studying strongly connected component leads solutions 
summarizing results find claimed section dimensional schedules fors fors 
schematic explanations gu correspond rdg loop nest dependence vectors necessarily lexicographically nonnegative 
fact forgets nodes virtual gu reduced dependence graph system uniform recurrence equations sure introduced karp miller winograd 
karp miller winograd study problem computability sure show computability linked problem detecting cycles null weight rdg done recursive decomposition graph detection multi cycles null weight 
key structure algorithm subgraph generated edges belong multi cycle null weight 
efficiently built resolution simple linear program program dual program 
resolution enables design parallelization algorithm principle dual karp miller winograd algorithm min bq max ze ze xw ye xe ze dependence vector associated edge cw connection matrix matrix dependence vectors 
entering details dimensional vector variable vertex rdg variable edge rdg 
edges resp 
xe ye ze resp ze optimal solution dual program equivalently resp primal program 
summing inequalities xw ye xe ze cycle finds xw ifc cycle xw number edges 
see link algorithm wolf lam considering cone generated weights cycles weights edges alain darte yves robert fr ric vivien subgraph cycle weights generate space vector relative interior need build effectively build interest linear programs 
outlined main ideas algorithm darte vivien 
technical modifications needed distinguish virtual actual nodes take account nature edges vertices rays lines dependence polyhedron see full details 
power limitations multi dimensional schedule prove optimality terms degree parallelism 
show statement node go number instances sequentialized order number instances inherently sequentialized dependences 
theorem 
scheduling algorithm nearly optimal iteration domain contains resp 
contained full dimensional cube size resp 
depth number nested recursive calls algorithm latency schedule length longest dependence path 
precisely code generation statement surrounded exactly ds sequential loops loops considered inherently sequential dependence analysis 
algorithm optimal respect dependence analysis 
consider example 
example 
enddo enddo fig 

example code rdg 
dependences described distance vectors rdg self dependences edges labeled polyhedra vertex ray respectively 
exists multi cycle null weight 
furthermore actual vertices belong depth algorithm darte vivien parallelism 
loop parallelization algorithms computing iteration statement resp 
second statement step resp 
leads valid schedule exposes degree parallelism darte vivien able find parallelism example approximation dependences lost parallelism 
technique detect parallel loops consists looking multi dimensional schedules linear parts vectors different statements belong strongly connected component 
base feautrier algorithm fundamental mathematical tool affine form farkas lemma 
theorem shows need look different linear parts construction expensive lead complicated rewriting processes strongly connected component current subgraph dependences distances vectors 
hand example shows refinement may useful accurate dependence analysis available 

feautrier algorithm paul feautrier proposed algorithm schedule static control programs affine dependences 
algorithm exact dependence analysis feasible programs 
contrasted previous algorithms allen kennedy wolf lam vivien approximation dependences 
feautrier algorithm takes input reduced dependence graph edge si sj labeled set pairs sj depends si 
algorithm builds recursively multi dimensional affine schedule statement loop nest feautrier 
decompose strongly connected component gi sort topologically 
strongly connected component gi find affine schedule statement induces non negative delay dependences satisfies dependences possible 
build set unsatisfied edges 

algorithm similar darte vivien structure output linear programs build affine schedules 
main points comparison algorithms schedules minimize latency code complicated write 
alain darte yves robert fr ric vivien darte vivien able schedule programs dependence analysis feasible rdg polyhedral dependences 
feautrier able process static control programs affine dependences 
sense algorithm powerful 
note attempts generalize feautrier approach weakening constraints input fuzzy array dataflow analysis 
dependence analysis feasible feautrier powerful 
algorithm able process set loops describe polyhedra loops perfectly nested 
darte vivien process non perfectly nested loops considering block perfectly nested loops separately fusing artificially non perfectly nested loops 
theory natural powerful 
darte vivien resolution linear programs similar solved feautrier 
fundamental difference looks general affine transformations 
static control programs affine dependences feautrier find parallelism darte vivien cf 
example 
despite difference optimality result darte vivien gives hints concerning optimality cases feautrier greedy heuristic 
feautrier needs affine form farkas lemma obtain linear programs darte vivien avoids scheme 
feautrier linear programs complex 
algorithms extended fine grain medium grain parallelism detection search fully permutable loops 
darte proposed extension darte vivien mere generalization wolf lam 
lim lam proposed extension feautrier finds maximal sets fully permutable loops minimizing amount synchronizations required parallelized code 
darte vivien produces schedules regular possible order generate codes simple possible 
algorithm rewrites codes affine schedules feautrier affine schedules chosen statements possible linear part code generation viewed sequence partial unimodular transformations loop distributions 
result output codes guaranteed simpler feautrier codes 
small comparison study conducted 
examples 
expected complexity darte vivien lower feautrier 
surprisingly algorithms output result examples considered 
obviously real examples processed reach 
say complex techniques provide better results 
write able rewrite 
loop parallelization algorithms code example obviously contain parallelism parallelized parallelization algorithms surveyed example 
enddo enddo enddo fig 

example original code parallelized version 

study provides classification loop parallelization algorithms 
main results allen kennedy algorithm optimal representation dependences levels wolf lam algorithm optimal representation direction vectors loop nest statement 
subsumes uses information exploited graph structure direction vectors second 
subsumed darte vivien algorithm optimal polyhedral representation distance vectors 
feautrier algorithm subsumes darte vivien algorithm dependences represented affine dependences characterization optimality remains open 
believe classification loop parallelization algorithms practical interest 
provides guidance compiler order choose suitable algorithm dependence analysis available simplest cheapest parallelization algorithm remains optimal selected 
algorithm appropriate available representation dependences 

allen kennedy 
pfc program convert programs parallel form 
technical report dept math 
sciences rice university tx march 

allen kennedy 
automatic translations fortran programs vector form 
acm toplas 

banerjee 
theory loop permutations 
gelernter nicolau padua editors languages compilers parallel computing 
mit press 
alain darte yves robert fr ric vivien 
bernstein 
analysis programs parallel processing 
ieee trans 
el 
computers ec 

pierre alain darte yves robert 
pen ultimate tiling 
integration vlsi journal 

callahan 
global approach detection parallelism 
phd thesis dept computer science rice university houston tx 


feautrier 
fuzzy array dataflow analysis 
proceedings th acm sigplan symp 
principles practice parallel programming santa barbara ca july 

jean fran ois 
code generation automatic 
claude girault editor proc 
int 
conf 
application parallel distributed computing 
ifip wg pages 
north holland april 

jean fran ois paul feautrier 
construction loops systems affine constraints 
parallel processing letters september 

alain darte leonid khachiyan yves robert 
linear scheduling nearly optimal 
parallel processing letters 

alain darte yves robert 
mapping uniform loop nests distributed memory architectures 
parallel computing 

alain darte yves robert 
affine statement scheduling uniform affine loop nests parametric domains 
parallel distributed computing 

alain darte georges andr fr ric vivien 
combining retiming scheduling techniques loop parallelization loop tiling 
technical report lip ens lyon france november 

alain darte fr ric vivien 
automatic parallelization multidimensional scheduling 
technical report lip ens lyon france september 

alain darte fr ric vivien 
optimal fine medium grain parallelism detection polyhedral reduced dependence graphs 
proceedings pact boston ma october 
ieee computer society press 

alain darte fr ric vivien 
optimal fine medium grain parallelism detection polyhedral reduced dependence graphs 
technical report lip ens lyon france april 

alain darte fr ric vivien 
optimality allen kennedy algorithm parallelism extraction nested loops 
journal parallel algorithms applications 
special issue optimizing compilers parallel languages 

paul feautrier 
dataflow analysis array scalar 
int 
parallel programming 

paul feautrier 
efficient solutions affine scheduling problem part dimensional time 
int 
parallel programming october 

paul feautrier 
efficient solutions affine scheduling problem part ii multi dimensional time 
int 
parallel programming december 

irigoin jouvelot 
semantical interprocedural parallelization overview pips project 
proceedings acm international conference supercomputing cologne germany june 

irigoin 
computing dependence direction vectors dependence cones linear systems 
technical report cai ecole des mines de paris france 
loop parallelization algorithms 
irigoin 
supernode partitioning 
proc 
th annual acm symp 
principles programming languages pages san diego ca january 

karp miller winograd 
organization computations uniform recurrence equations 
journal acm july 

kelly maslov pugh rosser wonnacott 
new user interface petit interfaces user guide 
university maryland june 

leslie lamport 
parallel execution loops 
communications acm february 

amy lim monica lam 
maximizing parallelism minimizing synchronization affine transforms 
proceedings th annual acm sigplan sigact symposium principles programming languages january 

wolfgang 
practical methods scheduling allocation polytope model 
world wide web document url brahms fmi uni passau de cl doc 

schreiber jack dongarra 
automatic blocking nested loops 
technical report university tennessee knoxville tn august 

alexander schrijver 
theory linear integer programming 
john wiley sons new york 

michael wolf monica lam 
loop transformation theory algorithm maximize parallelism 
ieee trans 
parallel distributed systems october 

wolfe 
optimizing supercompilers supercomputers 
phd thesis dept computer science university illinois urbana champaign october 

michael wolfe 
optimizing supercompilers supercomputers 
mit press cambridge ma 

michael wolfe 
tiny loop restructuring research tool 
oregon graduate institute science technology december 

michael wolfe 
high performance compilers parallel computing 
addison wesley publishing 

xue 
automatic non unimodular transformations loop nests 
parallel computing may 

hans zima barbara chapman 
supercompilers parallel vector computers 
acm press 
