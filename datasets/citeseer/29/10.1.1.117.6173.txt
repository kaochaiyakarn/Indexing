learning act real time dynamic programming lambda andrew barto steven bradtke singh department computer science university massachusetts amherst ma march lambda authors rich yee vijay brian jonathan helping clarify relationships heuristic search control 
rich sutton chris watkins paul werbos ron williams sharing fundamental insights subject numerous discussions rich sutton making aware korf research comments manuscript 
grateful bertsekas steven sullivan independently pointing error earlier version article 
harry klopf insight persistence encouraged interest class learning problems 
research supported barto national science foundation ecs ecs air force office scientific research afb afosr 
learning methods dynamic programming dp receiving increasing attention artificial intelligence 
increasing interest artificial intelligence ai researchers systems embedded environments demanding real time performance narrowing gulf problem solving control engineering 
similarly machine learning techniques suited embedded systems comparable methods adaptive control dynamic systems 
growing number researchers investigating learning systems dynamic programming dp algorithms solving stochastic optimal control problems arguing dp provides appropriate basis compiling planning results reactive strategies real time control learning strategies system controlled incompletely known 
learning algorithms dp employ novel means improving computational efficiency conventional dp algorithms 
werbos watkins proposed incremental versions dp learning algorithms sutton dyna architecture learning planning reacting principles :10.1.1.48.6005
key issue addressed dp learning tradeoff short long term performance agent learn improve long term performance may require sacrificing short term performance 
dp learning examples reinforcement learning methods autonomous agents improve skills environments contain explicit teachers 
article introduce learning algorithm dp call real time dynamic programming rtdp embedded problem solving system improve long term performance experience prove results behavior different types problems 
rtdp result recognizing korf learning realtime lrta algorithm closely related form dp known asynchronous dp 
attempting score calculated current board position look calculated terminal board position chain moves probably occur actual play 
samuel 
result process backing board evaluations evaluation function improve ability evaluate long term consequences moves 
version algorithm samuel represented evaluation function weighted sum numerical features adjusted weights error derived comparing evaluations current predicted board positions 
compatibility connectionist learning algorithms approach refined extended sutton heuristically number single agent problem solving tasks barto sutton anderson anderson sutton :10.1.1.48.6005:10.1.1.132.7760
algorithm implemented neuron connectionist element called adaptive critic element 
sutton called algorithms temporal difference td methods obtained theoretical results convergence :10.1.1.132.7760
proposals klopf sutton barto developed methods models animal learning 
minsky discussed similar ideas context credit assignment problem reinforcement learning systems independently developed ideas related animal behavior christensen korf experimented samuel method updating evaluation function coefficients linear regression holland bucket brigade algorithm assigning credit classifier systems closely related samuel method 
result process backing board evaluations evaluation function improve ability evaluate long term consequences moves 
version algorithm samuel represented evaluation function weighted sum numerical features adjusted weights error derived comparing evaluations current predicted board positions 
compatibility connectionist learning algorithms approach refined extended sutton heuristically number single agent problem solving tasks barto sutton anderson anderson sutton :10.1.1.48.6005:10.1.1.132.7760
algorithm implemented neuron connectionist element called adaptive critic element 
sutton called algorithms temporal difference td methods obtained theoretical results convergence :10.1.1.132.7760
proposals klopf sutton barto developed methods models animal learning 
minsky discussed similar ideas context credit assignment problem reinforcement learning systems independently developed ideas related animal behavior christensen korf experimented samuel method updating evaluation function coefficients linear regression holland bucket brigade algorithm assigning credit classifier systems closely related samuel method 
tesauro td gammon program td method connectionist network improve performance playing backgammon achieved remarkable success :10.1.1.20.3760
independently approaches inspired samuel checkers player researchers suggested similar algorithms theory optimal control dp provides important solution methods 
algorithm implemented neuron connectionist element called adaptive critic element 
sutton called algorithms temporal difference td methods obtained theoretical results convergence :10.1.1.132.7760
proposals klopf sutton barto developed methods models animal learning 
minsky discussed similar ideas context credit assignment problem reinforcement learning systems independently developed ideas related animal behavior christensen korf experimented samuel method updating evaluation function coefficients linear regression holland bucket brigade algorithm assigning credit classifier systems closely related samuel method 
tesauro td gammon program td method connectionist network improve performance playing backgammon achieved remarkable success :10.1.1.20.3760
independently approaches inspired samuel checkers player researchers suggested similar algorithms theory optimal control dp provides important solution methods 
applied control problems dp term introduced bellman consists methods successively approximating optimal evaluation functions decision rules deterministic stochastic problems 
general form dp applies optimization problems costs objects search space compositional structure exploited find object globally minimum cost performing exhaustive search 
kumar kanal discuss dp level generality 
come ferguson independently proposed method similar adaptive rtdp 
sutton barto williams discussed reinforcement learning perspective dp adaptive control white jordan barto provide additional background extensive current research 
aspects approach apply problems involving continuous time state action spaces restrict attention discrete time problems finite sets states actions relative simplicity closer relationship non numeric problems usually studied ai 
excludes various differential approaches optimization algorithms related connectionist algorithm jacobson jordan jacobs werbos white jordan 
relevance dp planning learning ai articulated sutton dyna architecture :10.1.1.48.6005
key idea dyna perform computational steps dp algorithm information obtained state transitions taken exceptions heuristic search literature algorithms proposed er 
algorithms dp backups update heuristic evaluation functions developed independently dp 
system controlled hypothetical state transitions simulated model system 
satisfy time constraints approach interleaves phases acting planning performed hypothetical state transitions 
conditions hold problems studied ai true realistic control problems 
uncertainty behavior physical system process modeling system implies closed loop control produce better performance 
control closed loop action depends current observations real system past observations information internal controller 
closed loop control policy called closed loop control rule law strategy rule specifying action function current possibly past information behavior controlled system 
closely corresponds universal plan discussed example chapman ginsberg schoppers :10.1.1.49.6261
control theory closed loop control policy usually specifies action function controlled system current state just current values observable variables distinction significance universal planning discussed chapman 
control closely associated negative feedback deviations desired system behavior negative feedback control merely special case closed loop control 
uncertainty closed loop control principle competent open loop control 
deterministic system disturbances policy initial state exists open loop policy produces exactly system behavior open loop policy generated running system simulating perfect model control closed loop policy 
section discuss adaptive case complete accurate model decision problem available 
model decision problem concurrent execution dp control carried simulation mode model surrogate actual system underlying decision problem 
result novel line dp computation computational advantages conventional line dp due ability focus relevant parts state set 
despite real time computation regard concurrent execution dp control simulation mode form learning 
fact learning accomplished programs samuel tesauro :10.1.1.20.3760
learning occurred simulated games learning systems competed 
emphasize real time dp learning algorithms reader aware discussion applies algorithms simulation mode 
describe concurrent execution dp control think time steps discrete time formulation markovian decision problem indices sequence instants real time controller execute control actions 
st state observed time kt total number asynchronous dp stages completed time latest estimate optimal evaluation function available controller select action ut st 
indirect direct methods central issue conflict controlling system exploring behavior order discover control better 
called conflict identification control appears indirect methods conflict conducting exploration achieve model convergence objective eventually optimal policy 
direct methods require exploration involve issues 
adaptive optimal control algorithms require mechanisms resolving problems mechanism universally favored 
approaches rigorous theoretical results available reviewed kumar variety heuristic approaches studied barto singh kaelbling moore schmidhuber sutton watkins thrun thrun :10.1.1.48.6005
subsections describe non bayesian methods solving markovian decision problems incomplete information 
methods form basis algorithms proved converge optimal policies describe exploration mechanisms rigor developing theory direction 
call method generic indirect method 
system identification algorithm updates system model time step control conventional dp algorithm executed time step current system model 
easy generate examples current certainty equivalence optimal policy prevents convergence true optimal policy due lack exploration see example kumar 
simplest ways induce exploratory behavior controller randomized policies actions chosen probabilities depend current evaluation function 
action nonzero probability executed current certainty equivalence optimal action having highest probability 
problems immediate cost random function current state action maximum likelihood estimate immediate cost observed average immediate cost state action 
facilitate comparison algorithms simulations described section adopt action selection method boltzmann distribution watkins lin sutton :10.1.1.48.6005
method assigns execution probability admissible action current state probability determined rating action utility 
compute rating action st follows qf lambda st transform ratings negative sum probability mass function admissible actions boltzmann distribution time step probability controller executes action st prob gamma tp st gamma positive parameter controlling sharply probabilities peak certainty equivalence optimal action lambda st 
increases probabilities uniform decreases probability executing lambda st approaches probabilities actions approach zero 
acts kind computational temperature simulated annealing decreases time :10.1.1.123.7607:10.1.1.123.7607
facilitate comparison algorithms simulations described section adopt action selection method boltzmann distribution watkins lin sutton :10.1.1.48.6005
method assigns execution probability admissible action current state probability determined rating action utility 
compute rating action st follows qf lambda st transform ratings negative sum probability mass function admissible actions boltzmann distribution time step probability controller executes action st prob gamma tp st gamma positive parameter controlling sharply probabilities peak certainty equivalence optimal action lambda st 
increases probabilities uniform decreases probability executing lambda st approaches probabilities actions approach zero 
acts kind computational temperature simulated annealing decreases time :10.1.1.123.7607:10.1.1.123.7607
controls necessary tradeoff identification control 
zero temperature exploration randomized policy equals certainty equivalence optimal policy infinite temperature attempt control 
simulations described section introduced exploratory behavior method just described generating randomized policies decrease time pre selected minimum value learning progressed 
choice method dictated simplicity desire illustrate algorithms generic possible 
adaptive real time dynamic programming generic indirect method just relies executing non real time dp algorithm convergence time step 
straightforward substitute rtdp resulting indirect method call adaptive rtdp 
method exactly rtdp described section system model updated line system identification method maximum likelihood method equation current system model performing stages rtdp true system model action time step determined randomized policy equation method balances identification control objectives 
adaptive rtdp related number algorithms investigated 
sutton dyna architecture focuses learning methods policy iteration section encompasses algorithms adaptive rtdp discusses ref :10.1.1.48.6005

lin discusses methods closely related adaptive rtdp 
engineering literature ferguson describe algorithm similar adaptive rtdp focus markovian decision problems performance measured average cost time step discounted cost discussed 
performing rtdp concurrently system identification adaptive rtdp provides opportunity progress identification influence selection states backup operation applied 

lin discusses methods closely related adaptive rtdp 
engineering literature ferguson describe algorithm similar adaptive rtdp focus markovian decision problems performance measured average cost time step discounted cost discussed 
performing rtdp concurrently system identification adaptive rtdp provides opportunity progress identification influence selection states backup operation applied 
sutton suggested advantageous back costs states confidence accuracy estimated state transition probabilities :10.1.1.48.6005
devise various measures confidence estimates direct algorithm states cost backups reliable state transition information confidence measure 
time possible confidence measure direct selection actions controller tends visit regions state space confidence low improve model regions 
strategy produces exploration aids identification conflict control 
kaelbling lin moore schmidhuber sutton thrun thrun discuss possibilities :10.1.1.48.6005
sutton suggested advantageous back costs states confidence accuracy estimated state transition probabilities :10.1.1.48.6005
devise various measures confidence estimates direct algorithm states cost backups reliable state transition information confidence measure 
time possible confidence measure direct selection actions controller tends visit regions state space confidence low improve model regions 
strategy produces exploration aids identification conflict control 
kaelbling lin moore schmidhuber sutton thrun thrun discuss possibilities :10.1.1.48.6005
learning learning method proposed watkins solving markovian decision problems incomplete information indirect adaptive methods discussed direct method explicit model dynamic system underlying decision problem 
directly estimates optimal values pairs states admissible actions call admissible state action pairs 
recall equation lambda optimal value state action cost generating action state optimal policy 
policy selecting actions greedy respect optimal values optimal policy 
watkins proposed family learning methods call learning article simplest case called step learning 
observed learning methods simple idea suggested previously far knew 
observed problems intensively studied years surprising studied earlier 
idea assigning values state action pairs formed basis approach dp seen algorithms learning estimating values watkins dissertation 
depart somewhat presentation view taken watkins sutton barto singh learning method adaptive line control :10.1.1.48.6005
emphasize learning relationship asynchronous dp basic learning algorithm line asynchronous dp method unique requiring direct access state transition probabilities decision problem 
describe usual line view learning 
line learning maintaining explicit estimate optimal evaluation function done methods described learning maintains estimates optimal values admissible state action pair 
state action qk estimate lambda available stage computation 
values admissible state action pairs remain qt qt admissible st ut 
process repeats time step 
far convergence concerned real time learning special case offline learning set state action pairs values backed step stage st ut sequence values generated realtime learning converges true values lambda conditions required convergence line learning 
means admissible action performed state infinitely infinite number control steps 
noteworthy pointed dayan admissible action state real time learning reduces td algorithm investigated sutton :10.1.1.132.7760
define complete adaptive control algorithm making real time learning necessary specify action selected current values 
convergence optimal policy requires kind exploration required indirect methods facilitate system identification discussed 
method selecting action current evaluation function randomized method described equation method leads convergence indirect method leads convergence corresponding direct method real time learning 
learning methods real time learning real system underlying decision problem plays role successor function 
method selecting action current evaluation function randomized method described equation method leads convergence indirect method leads convergence corresponding direct method real time learning 
learning methods real time learning real system underlying decision problem plays role successor function 
possible define successor function real system system model 
state action pairs experienced control real system provides successor function state action pairs system model provides approximate successor function 
sutton studied approach algorithm called dyna performs basic learning backup actual state transitions hypothetical state transitions simulated system model :10.1.1.48.6005
performing learning backup hypothetical state transitions amounts running multiple stages line learning intervals times controller executes actions 
step real time learning performed actual state transition 
obviously possible ways combine direct indirect adaptive methods emphasized sutton discussion general dyna learning architecture :10.1.1.48.6005
possible modify basic learning method variety ways order enhance efficiency 
state action pairs experienced control real system provides successor function state action pairs system model provides approximate successor function 
sutton studied approach algorithm called dyna performs basic learning backup actual state transitions hypothetical state transitions simulated system model :10.1.1.48.6005
performing learning backup hypothetical state transitions amounts running multiple stages line learning intervals times controller executes actions 
step real time learning performed actual state transition 
obviously possible ways combine direct indirect adaptive methods emphasized sutton discussion general dyna learning architecture :10.1.1.48.6005
possible modify basic learning method variety ways order enhance efficiency 
example lin studied method real time learning augmented model line learning action clearly stand preferable current values 
case line learning carried backup values admissible actions promising latest values current state 
watkins describes family learning methods values backed information gained sequences state transitions 
possible modify basic learning method variety ways order enhance efficiency 
example lin studied method real time learning augmented model line learning action clearly stand preferable current values 
case line learning carried backup values admissible actions promising latest values current state 
watkins describes family learning methods values backed information gained sequences state transitions 
way implement kind extension eligibility trace idea back values state action pairs experienced past magnitudes backups decreasing zero increasing time past :10.1.1.132.7760
sutton td algorithms illustrate idea :10.1.1.132.7760
attempting combinations variations learning methods described scope article 
barto singh dayan lin moore sutton comparative empirical studies adaptive algorithms learning :10.1.1.48.6005
methods explicit policy representations dp learning algorithms described non adaptive adaptive cases explicit representation evaluation function function giving values admissible state action pairs 
example lin studied method real time learning augmented model line learning action clearly stand preferable current values 
case line learning carried backup values admissible actions promising latest values current state 
watkins describes family learning methods values backed information gained sequences state transitions 
way implement kind extension eligibility trace idea back values state action pairs experienced past magnitudes backups decreasing zero increasing time past :10.1.1.132.7760
sutton td algorithms illustrate idea :10.1.1.132.7760
attempting combinations variations learning methods described scope article 
barto singh dayan lin moore sutton comparative empirical studies adaptive algorithms learning :10.1.1.48.6005
methods explicit policy representations dp learning algorithms described non adaptive adaptive cases explicit representation evaluation function function giving values admissible state action pairs 
functions computing action time step policy defined explicitly stored 
watkins describes family learning methods values backed information gained sequences state transitions 
way implement kind extension eligibility trace idea back values state action pairs experienced past magnitudes backups decreasing zero increasing time past :10.1.1.132.7760
sutton td algorithms illustrate idea :10.1.1.132.7760
attempting combinations variations learning methods described scope article 
barto singh dayan lin moore sutton comparative empirical studies adaptive algorithms learning :10.1.1.48.6005
methods explicit policy representations dp learning algorithms described non adaptive adaptive cases explicit representation evaluation function function giving values admissible state action pairs 
functions computing action time step policy defined explicitly stored 
number real time learning control methods dp policies evaluation functions stored updated time step control 
methods addressed article methods closely related policy iteration dp algorithm value iteration algorithms discussed section 
policy evaluation require repeated minimizing admissible actions require computation practical large state sets 
feasible modified policy policy iteration policy evaluation phase executed completion policy improvement phase 
real time algorithms policy iteration effectively executing asynchronous form modified policy iteration concurrently control 
examples methods appear pole balancing system barto sutton anderson refs 
dyna pi method sutton pi means policy iteration :10.1.1.48.6005
barto sutton watkins discuss connection methods policy iteration detail 
article discuss learning algorithms policy iteration theory understood theory learning algorithms asynchronous value iteration 
williams baird valuable contribution theory addressing dp algorithms asynchronous grain finer dp learning 
algorithms include value iteration policy iteration modified policy iteration special cases 
state costs 
weights adjusted reduce discrepancy current cost state backed cost 
approach inspired variety studies parameterized function approximations 
discrepancy supplies error error correction procedure approximates functions training set function samples 
form supervised learning learning examples provides natural way connectionist networks shown example anderson tesauro :10.1.1.20.3760
parametric approximations evaluation functions useful generalize training data supply cost estimates states visited important factor large state sets 
fact supervised learning method associated manner representing hypotheses adapted approximating evaluation functions 
includes symbolic methods learning examples 
methods generalize training information derived back operations various dp algorithms 
extensive literature function approximation methods dp methods methods splines orthogonal polynomials bellman dreyfus bellman daniel 
literature devoted line algorithms cases complete model decision problem 
adapting techniques literature produce approximation methods rtdp dp learning algorithms challenge research 
best knowledge theoretical results directly address generalizing methods dp learning algorithms 
results sutton dayan concern td methods evaluate policy linear combination complete set linearly independent basis vectors :10.1.1.132.7760
unfortunately results address problem representing evaluation function compactly represented lookup table 
bradtke addresses problem learning values quadratic functions continuous state results restricted linear quadratic regulation problems 
singh yee point discounted case small errors approximating evaluation function function giving values lead worst small decrements performance controller approximate evaluation function basis control :10.1.1.132.7760
result plausible small evaluation errors drastically control performance condition true raise concerns combining dp learning function approximation 
best knowledge theoretical results directly address generalizing methods dp learning algorithms 
results sutton dayan concern td methods evaluate policy linear combination complete set linearly independent basis vectors :10.1.1.132.7760
unfortunately results address problem representing evaluation function compactly represented lookup table 
bradtke addresses problem learning values quadratic functions continuous state results restricted linear quadratic regulation problems 
singh yee point discounted case small errors approximating evaluation function function giving values lead worst small decrements performance controller approximate evaluation function basis control :10.1.1.132.7760
result plausible small evaluation errors drastically control performance condition true raise concerns combining dp learning function approximation 
research needed provide better understanding function approximation methods effectively algorithms described article 
illustrations dp learning race track problem described section illustrate compare conventional dp rtdp adaptive rtdp real time learning race tracks shown 
small race track shown panel start states goal states states reachable start states policy 
algorithms scale larger problems adequately addressed simulations 
results small larger race track give indication algorithms scale collection problems adequate studying issue 
variability algorithm performance function problem details size state action sets difficult extrapolate performance just problems 
proceeding larger problems hampered large space requirements algorithms continue lookup tables storing evaluation functions 
tesauro td gammon system encouraging data point dp learning conjunction function approximation methods problems larger described continued theoretical research necessary address computational complexity real time dp algorithms :10.1.1.20.3760
clear simulations real time dp algorithms confer significant computational advantages conventional line dp algorithms 
concluding discussion race track problem point misleading think application dp learning algorithms problem productive way apply realistic robot navigation tasks 
example learning applied formulation race track problem refines skill racing specific track 
skill transfer tracks due specificity track represented 
suitability asynchronous dp implementation multi processor systems motivated theory novel results 
applying results especially results stochastic shortest path problems rtdp provides new theoretical basis learning algorithms 
convergence theorems asynchronous dp imply rtdp retains competence conventional synchronous gauss seidel dp algorithms extension korf lrta convergence theorem framework provides conditions rtdp avoids exhaustive nature line dp algorithms ultimately yielding optimal behavior 
term simulation mode refer execution rtdp related algorithms simulated control actual control 
dp learning simulation mode illustrated samuel checkers playing system tesauro backgammon playing system illustrations rtdp race track problem :10.1.1.20.3760
despite fact dp learning algorithms executed simulation mode line algorithms treat learning algorithms incrementally improve control performance simulated experience solely application computational methods 
algorithms rtdp require accurate model decision problem simulation mode option obvious advantages due large number trials required 
applying rtdp actual control sense time compute satisfactory policy line method actual control 
applied actual control simulation mode rtdp significant advantages conventional dp algorithms 
kaelbling 
learning embedded systems 
mit press cambridge ma 
revised version research tr june 
kirkpatrick gelatt vecchi :10.1.1.123.7607
optimization simulated annealing 
science 
klopf 
brain function adaptive systems theory 
schmidhuber 
adaptive confidence adaptive curiosity 
technical report institut ur informatik technische universit unchen 
unchen germany 
schoppers :10.1.1.49.6261
universal plans reactive robots unpredictable environments 
proceedings tenth international joint conference artificial intelligence pages menlo park ca 
schoppers 
defense reaction plans caches 
proceedings tenth international joint conference artificial intelligence pages menlo park ca 
schoppers 
defense reaction plans caches 
ai magazine 
singh yee :10.1.1.132.7760
upper bound loss approximate optimal value functions 
technical report 
submitted technical note machine learning 
sutton 
submitted technical note machine learning 
sutton 
temporal credit assignment reinforcement learning 
phd thesis university massachusetts amherst ma 
sutton :10.1.1.132.7760
learning predict method temporal differences 
machine learning 
sutton :10.1.1.48.6005
integrated architectures learning planning reacting approximating dynamic programming 
phd thesis university massachusetts amherst ma 
sutton :10.1.1.132.7760
learning predict method temporal differences 
machine learning 
sutton :10.1.1.48.6005
integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning pages san mateo ca 
morgan kaufmann 
sutton 
tan 
learning cost sensitive internal representation reinforcement learning 
birnbaum collins editors learning proceedings eighth international workshop pages san mateo ca 
morgan kaufmann 
tesauro :10.1.1.20.3760
practical issues temporal difference learning 
machine learning 
thrun 
active exploration dynamic environments 
