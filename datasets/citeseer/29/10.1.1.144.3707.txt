exploiting order regression inductive policy selection charles thi baux national ict australia computer sciences laboratory australian national university canberra act australia csl anu edu au consider problem computing optimal generalised policies relational markov decision processes 
describe approach combining benefits purely inductive techniques symbolic dynamic programming methods 
reason optimal value function order decisiontheoretic regression formula rewriting provided suitable hypotheses language capable generalising value functions policies small instances 
idea reasoning particular classical order regression automatically generate hypotheses language dedicated domain hand input inductive solver 
approach avoids complex reasoning symbolic dynamic programming focusing inductive solver attention concepts specifically relevant optimal value function domain considered 
planning domains exhibit strong relational structure traditionally represented firstorder languages supporting declaration objects relations quantification objects 
markov decision processes mdps widely accepted preferred model decision theoretic planning state art mdp algorithms operate state propositionally factored representations failing exploit relational structure planning domains :10.1.1.111.5793:10.1.1.75.4460
due size representations approaches scale number objects increases 
furthermore little way addressing longstanding goal generating generalised policies applicable arbitrary number objects 
mdp planners usually replan scratch computing policy instance marginally fewer states 
research relational mdps started address issues 
relational approaches fall mainly classes 
approaches class extend dynamic programming methods operate directly order domain value function descriptions :10.1.1.111.9112
stage value function represented mapping set order formulae partitioning state space reals obtained pure logical reasoning 
involves particular reasoning domain dynamics firstorder version decision theoretic regression reasoning maximisation formula rewriting :10.1.1.111.9112:10.1.1.158.8490
approach theoretically attractive difficult challenge implement effective formula simplification rules theorem proving techniques keep formulae consistent manageable size 
unaware existing implementations successfully address challenge 
approaches second class avoid problems employing inductive learning techniques generalise policies value functions instances small number objects get useful generalised policy 
order address domains small instances representative general case induce policies generalise instance training data form policy trajectory list propositional state action pairs generated approximate policy iteration :10.1.1.13.8448
inductive learning proposals reason domain dynamics generation training data 
contrast dynamic programming approach explicitly seek optimality cases correctness 
feature motivated fact domains arise practical representation optimal generalised value function policy exists 
keep search space manageable inductive approaches require suitable hypotheses language sufficiently rich describe control strategies interest wasting learner time irrelevant planning concepts 
take form support predicates express key features domain terms basic relations position blocks world domain independent language bias im portant features discovered scratch example concept language description taxonomic logics appears suited blocks world logistics benchmarks :10.1.1.13.8448:10.1.1.13.8448
main weakness inductive approaches reliance suitable hypotheses language 
question fact explicitly reason known domain dynamics exploit generation training data 
flexible practical decision theoretic regression approach may seen failure exploit useful information 
consider problem computing optimal generalised policies order domain reward descriptions 
investigate approach aimed combining strengths dynamic programming inductive techniques 
idea automatically generate suitable hypotheses language domain hand reasoning dynamics domain order regression 
language guaranteed cover concepts relevant optimal go value function input inductive solver 
explicitly repeatedly apply classical order regression see order formulae involved reward description generate candidate formulae inclusion stage generalised value function 
inductive solver selects formulae build decision tree generalising small value functions generated state art mdp solver 
avoid expensive reasoning performed dynamic programming approaches able retain acceptable performance 
hypotheses language targeted domain interest able obtain optimal generalised policies training examples 
organised follows 
start background material mdps relational mdps order regression previous approaches 
follow description approach discussion strengths weaknesses 
experimental results concluding remarks related 
background mdps take markov decision process tuple pr possibly infinite set fully observable states possibly infinite set ground actions denotes subset actions applicable pr family probability distributions pr probability state performing action state ir reward function immediate reward state stationary policy mdp function action executed state value state policy sum expected rewards discounted far occur lim discounting factor controlling contribution distant rewards state time policy optimal iff policies 
relational mdps state definition mdps suitable general mathematical model fails emphasise relational structure planning problems 
reason research focused relational mdps structure explicit open way algorithms capable exploiting 
relational model mdps represented order formalism supporting relations functions quantification objects 
famous formalisms purpose order probabilistic strips variants situation calculus :10.1.1.111.9112
presentation uses situation calculus believe provides clear logical foundations approach 
situation calculus disjoint sorts actions situations objects 
alphabet includes variables sort function predicate symbols sort object respectively denote functions relations usual connectives quantifiers usual abbreviations elements language include 
actions order terms built action function symbol sort object action arguments 
instance move denotes action moving object object arguments ground speak ground action 
follows shall distinction actions ground actions distinction matters 
situation terms built symbols constant symbol denoting initial situation function symbol action situation situation interpretation denotes situation resulting performing deterministic action situation relations truth values vary situation situation built predicate symbols sort situation called relational fluent symbols 
instance relational fluent meaning object object situation additionally predicate symbol poss sort action situation 
intended interpretation poss possible perform deterministic action situation situation calculus views stochastic actions probability distributions deterministic actions 
executing stochastic action amounts letting nature choose deterministic action executed choice governed probabilities 
describing stochastic actions requires predicate symbol choice action action choice da sa denotes executing deterministic action da possible nature choice executing stochastic action sa function symbol prob action action situation ir prob da sa denotes probability choice situation function symbol situation ir denote immediate reward received situation 
notion state formula free variables non situation variables situation variable situation term occurs :10.1.1.111.9112
intuitively state formula refers properties situation say mdp state models state formula free variable situation iff properties described formula hold state 
set state formulae fi partitions state space iff ifi fi fj modelling relational mdp situation calculus involves writing axioms 

reward axiom rewards current situation conveniently expressed statement form case 
rn ris reals state formulae partitioning state space notation case 
fn tn abbreviates fi ti 
instance consider blocks world domain get rewarded blocks goal position case ong ong ong situation independent relation representing goal configuration 

nature choice probability axioms stochastic action specify deter extended depend current action 
formulae said uniform assume state formulae contain statements involving predicates poss choice functions prob able say mdp states order models state formulae strictly accurate presence situation variables fluents 
strip state variables reduce arity relations 
spelling formally waste space 
actions 
dk available nature choose axiom choice dj 
define probabilities choices current situation axioms form prob dj case 
mj ij state formulae partitioning state space pi js probabilities 
instance suppose blocks world domain move action stochastic behaves deterministic moves action succeeds moving behaves deterministic action fails change 
suppose furthermore probability successful move weather fine rainy get axioms choice move moves prob moves move case rain rain prob move case rain rain 
action precondition axioms deterministic action need write axiom form poss state formula characterising preconditions action 
poss moves poss table table 
successor states axioms means deterministic dynamics system described 
relational fluent axiom form state formula characterising truth value situation resulting performing instance moves moves rain rain 
pair distinct actions need unique name axiom form action different axiom form completes description relational mdps situation calculus framework 
emphasise modelling retains classical situation calculus machinery deterministic domains stochastic actions appear extra layer top machinery axioms restrict domain pre specified finite set objects solution mdp axiomatised way generalised policy applying arbitrary object universe 
generalised value functions policies conveniently represented situation calculus case statement involving state formulae partitioning state space real action terms respectively 
order regression action formalisms regression corner stone reasoning dynamics deterministic domain situation calculus 
usual regression formula deterministic action formula holds executed holds execution 
situation calculus regression takes form 
consider state formula action term 
holds situation resulting executing iff regr holds regr defined follows regr successor state axiom regr regr regr regr regr regr regr regr cases regressing formula ong reward description action moves yields table table ong meaning goal achieved move move executable subgoal ong move achieves subgoal true move destroy 
order dynamic programming approaches solving relational mdps order symbolic dynamic programming :10.1.1.111.9112
value iteration approach directly operates symbolic representation generalised value function case statement 
relies order decisiontheoretic regression extension regression defined stochastic actions 
stochastic action logical description generalised stage go value function order decision theoretic regression able compute logical description generalised stage go function 
value iteration step functions computed various actions formula built expressing quantifiers quantified variable renamed needed different free variables 
similar applies quantifiers substituting maximum functions actions 
drawback order dynamic programming practicality retaining manageable case expressions value functions length number formulae included case statements rapidly impractically large 
especially exacerbated symbolic maximisation functions requires combining complex formulae obtained firstorder decision theoretic regression 
complication need detecting inconsistent expressions may form result combinations 
implementing logical simplification rules theorem prover otter able significantly reduce case bloat eliminate forms redundancies contradictions 
unfortunately comes cost 
essentially find dynamic programming remains impractical little value iteration steps standard planning benchmarks blocks world logistics 
approach having examined shortcomings dynamic programming approach seek extract apply essence reasoning order regression different context inductive learning 
briefly approach uses classical order regression defined section generate hypotheses language inductive learner 
language consists state formulae inductive learner selects build decision tree generalising small instances generated conventional mdp solver 
hypotheses language suppose state formula situation calculus free variable sort situation powers example indicate post action formula related application regression formula explicitly regr 
step go derivation corresponds start formula trajectory length leading consider set consisting state formulae reward axiom case statement 
compute regressing domain deterministic actions 
subset mdp states action application rewarding state model usefully state formula characterising pre abstain making situation variable explicit doing superfluous 
action states stochastic action formed considering disjunctions 
similar fashion encapsulate longer trajectories facilitated stochastic actions computing larger 
specifically set state formulae sufficient encapsulate trajectories members set follows shall able induce classification state space regions value policy state formulae computed regression 
course provided functions finite range provision violated remain able consider relational mdps object universe finitely bounded 
approach fact cheaper compute perform order dynamic programming steps 
instance blocks world able compute minute 
typically state formulae interesting 
relevant optimal useful value function 
propose useful identified inductive learning 
inductive learning follows provide details inductive algorithm supposing learn generalised policy value function single object 
learn policy record deterministic action derived aggregate stochastic action deterministic action forms part 
starting point assume set training examples 
triple mdp state optimal value optimal ground stochastic action value order policy prescribed induced function agree value policy entry training examples 
learning algorithm need notion example satisfying form regr 
say training example satisfies iff composite stochastic action symbol recorded models 
captures intuition state ground action example match constraints expressed formula 
initially generic inductive logic programming utility learns comprehensive theories noisy structured data 
takes input hypotheses language described higher order logic set examples able greedily induce decision tree classifying examples 
preliminary experiments demonstrated inductive technique value functions policies relational mdps may infinite range require infinite representations 
box bin act na val box act unload val box city act drive val box city bin act load val box city bin act drive val table decision tree representation policy value function logistics mild simplifications 
situation variables omitted 
holds promise 
able generate encoding input language produce decision tree representation order value function 
example consider logistics domain described reward package sydney :10.1.1.111.9112
provided hypotheses language training examples able induce optimal generalised value function shown table matter seconds 
due greedy nature search self imposed incomplete hypotheses space form unable build generalised value function domains experimented provided perfectly sufficient purpose 
coupled redundancy required planner led develop learner specific planning context 
algorithm provides listing pseudo code learner 
computes binary tree representation value policy functions nodes correspond regressed formulae connected children negative positive arc build formula tracing root node leaf conjoining formula resp 
negated formulae node depending take positive resp 
negative arc node successors 
states resultant leaf model main aspects algorithm 
selection formula inclusion decision tree lines second generation hypotheses space lines third actual construction tree representing order policy value function lines 
aspects requires comment 
rest straightforward description authors currently adding support complete search 
algorithm inductive policy construction 
initialise max compute set examples call build tree function build tree integer examples pure return success leaf classifier null exists null max return failure leaf update hypotheses space return build tree positive satisfies negative positive positive tree build tree positive negative tree build tree negative return tree positive tree negative tree gorithm 
algorithm starts checking examples ek vk bk tk current tree node pure lines prescribe stochastic action bk value vk 
examples pure select state formulae generated point best discriminates examples line 
description algorithm remains voluntarily non concrete implementation choice 
possibilities include accepting yields sufficient information gain expected reduction entropy 
alternatively step yield null max point information gain weaker measure pick max tradeoff prematurely adding redundant nodes tree needlessly generating candidate concepts 
case process selection acceptable prune formulae satisfied example avoid regressing 
general may lead removing formula trajectories necessary correctly classify training data graphically connected 
pruning admissible training examples correspond state trajectories policies domain instances 
implementation inadmissible heuristic number elements satisfying divided number distinct state values subset satisfy 
discussion learning approaches operates training examples form state trajectories 
due nature state formulae able learn far fewer state trajectories learning proposals date 
see important strength approach 
extreme take case domain training data form single state trajectory approach able induce optimal policy generalises problem states values training 
instance consider deterministic blocks world single action move seek minimize number moves goal 
reward case statement consists formulae 
say learner consists formulae single trajectory length instance trajectory changes bottom block tower blocks 
suppose states trajectory values ranging 
learner able learn optimal policy covering instances blocks world distance goal 
particular includes blocks instances length longest optimal blocks plan 
intuitively policy generated says distance goal find arguments move bring state distance distance goal find arguments move bring state distance 
method induce policy optimal domain instances explicit implicit universal quantification reward axiom 
domains rewards optimal generalised value functions finite range 
instance blocks world goal blocks table free blocks training data need comprise longest optimal trajectory goal length hypotheses language considered need important note generalised policies computed algorithm maximise expected utility making general simple goal driven learning 
pointed case range value function increases number objects domain yielding generalised value functions infinite range 
typical domains reward relevant optimal policy received objects type satisfy condition consider blocks world logistics domains ask blocks goal position packages reach destination 
negative side approach value function driven suited infer completely generalised policies range generalised value function infinite 
learner unable induce policy states values represented training examples 
note approach suited generating optimal policies conceive approximately optimal policies gn policies blocks world 
generating optimal policies may impractical suboptimal solutions sought 
compared purely inductive methods hybrid approach requires axiomatisation actions domain 
feel excessive demand especially comparison pure reasoning approach order symbolic dynamic programming require chance practical 
recall performance symbolic dynamic programming relies ability making theorem prover prune inconsistent formulae generalised value function description 
theorem prover possibly effective provided domain state constraints constraints stating object places box truck 
experience static constrains time consuming domain dynamics 
main motivations approach avoid complex reasoning performed dynamic programming 
succeeded formula rewriting theorem proving loop need model checking match examples state formulae satisfy 
model checking substantially faster automated theorem proving free 
number formulae remaining increases rapidly approach impractical 
shown experiments find model checking bottleneck approach 
results approach substantially different generates optimal policies unorthodox combination knowledge representation reasoning form order regression induction noise free training examples 
papers reporting results relational mdps typically focus evaluating coverage suboptimal generalised policy discovered inductive method find interesting study factors affecting performance approach number domains 
approach implemented 
results reported obtained pentium ghz gnu linux machine mb ram 
interesting characteristics approach experimentation apparent stochastic deterministic domains 
results deterministic stochastic variants logistics blocks world domains lg lg ex bw bw ex lg alls lg exs bw alls bw exs respectively 
lg deterministic version logistics domain reward received packages sydney :10.1.1.111.9112
lg ex similar lg reward received package sydney 
stochastic versions called lg exs lg alls additional property trucks chance mistakenly driving wrong city 
bw standard blocks world domain single move operator reward received blocks goal position 
bw ex blocks world reward received blocks table 
stochastic bw domains bw alls bw exs faulty move action chance dropping box table regardless intended destination 
difference domains deterministic probabilistic international planning competitions absence planes single wrong city drive action logistics single move action pickup putdown blocks world 
tables report results obtained experimentation deterministic stochastic domains respectively 
entries contain name domain maximal depth max regression set number distinct values appearing training data size number blocks number boxes trucks cities instances training data number examples type training data type indicates examples extracted optimal policy covering states size policies generated system type indicates examples extracted optimal plans trajectories returned optimal blocks world solver selected instances size 
column labelled time reports time seconds takes implementation induce policy 
column annotated pruning interrupted algorithm termination 
column reports scope applicability induced policy 
possible entries algorithm induced completely generalised policy integer case policy generalised states optimal value largest values training set 
rows tables demonstrate best case behaviour approach 
domains completely generalised policy order policy optimal domain instances computed training examples correspond complete mdp policy 
examples small size examples size sufficient induce generalised domain max size type time scope lg ex lg ex bw ex bw ex bw ex bw bw bw bw lg lg lg lg lg table experimental results deterministic policy results progressively larger sizes training sets study cost modelchecking 
impressive speed algorithm induces completely generalised policy due small size required speed satisfiability elements set verified training examples 
observe cost model checking increases faster blocks world logistics 
blocks world formulae state model descriptions training data longer 
results tabulated bw bw alls validate comments discussion 
know complete generalised policy question explicit cost matching training examples state formulae satisfy 
results show size state models training data complexity state formulae considered length trajectories considered greatly effect practicality model checking 
take deterministic case instance due cost model checking achieve better time performance examples size complete blocks policy fixed goal state single blocks trajectory length 
true lg case growth addition time consuming model checking contributes learning time 
instance takes lot longer infer policy larger scope examples size max examples size max 
conclude draw reader attention entries tables 
particular time algorithm takes induce policy pruning 
prune formulae satisfied example algorithm takes considerably longer induce optimal stage go policy lg 
growth pruning logistic domain actions lenient preconditions fast model checking keep 
domain max size type time scope lg exs lg exs bw exs bw exs bw exs bw alls bw alls bw alls bw alls lg alls lg alls lg alls lg alls lg alls table experimental results stochastic related previous relational reinforcement learning considered exact calculation optimal order value function symbolic dynamic programming machine learning hand coded background knowledge general purpose hypotheses space representing policies optimisation value function approximation combining object local value functions assuming object local behaviour depends class :10.1.1.118.5564:10.1.1.111.9112:10.1.1.13.8448
types approaches directly relevant :10.1.1.118.5564
language express regularities value function rich comparison relational approaches hypotheses space supported learning algorithm constrained comparison learning methods 
domain specification exploited extent adoption reasoning permits 
substantially different firstorder dynamic programming inductive approach propose 
proposal closely related techniques 
inspiration order dynamic programming able exploit order regression generate hypotheses language sufficient encode optimal generalised stage go policies relational mdps 
proposed experimented method inducing policies training examples drawn domain instances 
expensive reasoning required order symbolic dynamic programming avoided doing 
observed drawbacks parent particular growth number length regressed formulae carry technique lesser degree 
pitfalls remain difficulty generalising value functions 
demonstrated cases approach able succeed excel contexts previous proposals 
contrast previous inductive approaches reasoning domain dynamics confined generation suitable training data reasoning generate hypotheses space suitable represent optimal stage go policy 
approach achieves middle ground learning techniques rely general purpose hypotheses language representing policies relying hand coded background knowledge :10.1.1.13.8448
human intervention required define hypotheses space resulting space targeted domain interest 
finding general purpose hypotheses languages expressive represent policies restricting attention learner relevant concepts important challenge 
see step addressing issue 
majority approaches proposal learns domain instances small number objects get useful generalised policy value function 
techniques suitable practical representation optimal generalised value function policy exists 
pressing item investigate ways mitigating poor performance presence universal quantification reward axiom purely value driven framework 
consideration control knowledge prune quickly regression 
experiment practicability concatenating optimal stage go policies induced approach solve problems requiring longer planning horizons 
doug aberdeen joshua cole bob givan john lloyd kee ng john slaney useful discussions anonymous reviewers suggestions improve 
acknowledge support national ict australia 
funded australian government backing australia ability initiative part australian research council 
blum langford 
probabilistic planning graphplan framework 
proc 
ecp 
bonet geffner 
labeled rtdp improving convergence real time dynamic programming 
proc 

boutilier dearden goldszmidt :10.1.1.111.9112:10.1.1.158.8490
stochastic dynamic programming factored representations 
artificial intelligence 
boutilier reiter price :10.1.1.111.9112
symbolic dynamic programming order mdps 
proc 
ijcai 
cole lloyd ng 
symbolic learning adaptive agents 
proc 
annual partner conference smart internet technology cooperative research centre 
csl anu edu au crc pdf guestrin koller generalizing plans new environments relational mdps :10.1.1.118.5564
proc 
ijcai 
dzeroski de raedt driessens 
relational reinforcement learning 
machine learning 
feng hansen 
symbolic lao search factored markov decision processes 
proc 
aaai 
kaelbling 
envelope planning relational mdps 
proc 
nips 
fern yoon givan approximate policy iteration policy language bias 
proc 
nips 
fern yoon givan learning domain specific knowledge random walks 
proc 

ghallab nau traverso 
automated planning theory practice 
kaufmann 
charles david price thi baux 
implementation comparison solution methods decision processes non markovian rewards 
proc 
uai 
hansen zilberstein 
lao heuristic search algorithm finds solutions loops 
artificial intelligence 
hoey st aubin hu boutilier 
spudd stochastic planning decision diagrams 
proc 
uai 
khardon 
learning action strategies planning domains 
artificial intelligence 
lloyd 
logic learning learning comprehensible theories structured data 
springer 
martin geffner 
learning generalized policies planning concept languages 
proc 
kr 
weld 
solving relational mdps order machine learning 
proc 
workshop planning uncertainty incomplete information 
mccune 
otter manual 
technical report anl mcs tm argonne national laboratory illinois 
reiter 
knowledge action logical foundations specifying implementing dynamical systems 
mit press 
slaney thi baux 
blocks world revisited 
artificial intelligence 
yoon fern givan 
inductive policy selection order mdps 
proc 
uai 
younes littman 
extension pddl expressing planning domains probabilistic effects 
www cs cmu edu papers pdf 
