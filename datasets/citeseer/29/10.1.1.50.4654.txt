sample complexity pattern classification neural networks size weights important size network peter bartlett member ieee april revised may sample complexity results computational learning theory applied neural network learning pattern classification problems suggest generalization performance number training examples grow linearly number adjustable parameters network 
results show large neural network pattern classification problem learning algorithm finds network small weights small squared error training patterns generalization performance depends size weights number weights 
example consider layer feedforward network sigmoid units sum magnitudes weights associated unit bounded input dimension show misclassification probability certain error estimate related squared error training set plus log ignoring log log factors number training patterns 
may explain generalization performance neural networks particularly number training department systems engineering research school information sciences engineering australian national university canberra australia 
peter bartlett anu edu au examples considerably smaller number weights 
supports heuristics weight decay early stopping attempt keep weights small training 
define misclassification probability hypothesis probability random pair er yg training data sequence elements theta gamma generated independently probability distribution training data sequence xm ym length real number fl define error estimate er fl jfi estimate counts proportion examples correctly classified margin fl 
class real valued functions defined fl sequence xm points said fl shattered gamma satisfying gamma fl 
define fat shattering dimension function fat fl fl fat shattering dimension introduced kearns schapire 
theorem gives generalization error bound hypothesis mistakes training examples value bounded away zero 
result essentially main result observed similar slightly weaker result follows trivially main result :10.1.1.33.8995
proof theorem similar proof closely followed proofs vapnik chervonenkis pollard 
theorem follows assume set class real valued functions defined probability distribution theta gamma ffi fl 
theorem suppose xm ym chosen independent draws probability gamma ffi er fl er log em log log ffi fat fl :10.1.1.33.8995
theorem main technical results 
theorem gives generalization error bound hypothesis mistakes training examples value bounded away zero 
result essentially main result observed similar slightly weaker result follows trivially main result :10.1.1.33.8995
proof theorem similar proof closely followed proofs vapnik chervonenkis pollard 
theorem follows assume set class real valued functions defined probability distribution theta gamma ffi fl 
theorem suppose xm ym chosen independent draws probability gamma ffi er fl er log em log log ffi fat fl :10.1.1.33.8995
theorem main technical results 
gives generalization error bounds hypothesis classifies significant proportion training examples correctly value bounded away zero points 
case may possible get better generalization error bound excluding examples hypothesis takes value close zero examples correctly classified 
theorem suppose xm ym chosen independent draws probability gamma ffi er er fl ln em log ln ffi fat fl 
giving bounds generalization error results restricted bounds misclassification probability fixed test sample advance 
problem investigated 
gave proof vapnik result linear case extended give bounds misclassification probability 
theorem generalizes result arbitrary function classes 
gave result provides generalization error bounds terms hypothesis performance estimator luckiness function satisfies properties roughly consistent large values function unusual :10.1.1.33.8995
applications described :10.1.1.33.8995
er obtained bounds misclassification probability terms properties regression functions 
bounds improve vc bounds information behaviour true regression function conditional expectation 
specifically show error skeleton estimator depends certain covering numbers respect unusual class possible regression functions vc dimension corresponding class bayes classifiers 
problem investigated 
gave proof vapnik result linear case extended give bounds misclassification probability 
theorem generalizes result arbitrary function classes 
gave result provides generalization error bounds terms hypothesis performance estimator luckiness function satisfies properties roughly consistent large values function unusual :10.1.1.33.8995
applications described :10.1.1.33.8995
er obtained bounds misclassification probability terms properties regression functions 
bounds improve vc bounds information behaviour true regression function conditional expectation 
specifically show error skeleton estimator depends certain covering numbers respect unusual class possible regression functions vc dimension corresponding class bayes classifiers 
show covering numbers bounded terms scale sensitive dimension closely related fat shattering dimension version function class see definition 
shawe taylor bartlett williamson anthony 
framework structural risk minimisation 
proceedings th annual conference computational learning theory pages 
acm press new york ny 
shawe taylor bartlett williamson anthony :10.1.1.33.8995
structural risk minimization data dependent hierarchies september 
technical report 

feedforward nets interpolation classification 
