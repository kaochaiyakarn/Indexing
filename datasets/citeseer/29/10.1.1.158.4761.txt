appeared ieee international conference systems man cybernetics ieee smc pages tokyo japan october 
amount quality bias reinforcement learning sommer christian university department cognitive systems kiel germany reinforcement learning widely regarded elegant theory hopelessly slow practice 
studied assumption little prior information task hand 
assumption defining characteristic learning 
learning involves incorporation prior knowledge bias greatly accelerates improves learning process 
address influence amount quality bias speed reinforcement learning 
chosen class learning problem different forms biases initially identified 
bias extracted knowledge environment task 
belief matrices reset tables learning commences encode biases 
average number interactions agent environment quantify biases 
performance measure biases graded new results reported 
addition compares continual learning learning scratch presents results clearly demonstrate advantages 
key words learning bias continual learning 
bias context learning term describe learning system pre disposition learning expense 
having varying degree built structure learning systems fall continuum unbiased highly biased memory systems 
lookup tables near unbiased continuum impose constraints certain quantization data store 
memory systems near highly biased assume specific functional relationship inputs outputs 
highly biased memory systems degrees freedom form bias enables generalize data direct experience 
biasing regarded cheating machine learning community understood accepted necessary part designing useful learning systems 
past different kinds biases speed rl 
mahadevan decomposed task sets simple sub tasks applicability predicate 
mat ric minimized state space transforming stateaction pairs condition behavior pairs maximized learning designing reward rich heterogeneous reinforcement 
mill tremendously accelerated rl integrating reflex rules focus exploration needed 
embedded environment knowledge ease state space construction 
common characteristic examples basic rl algorithm endowed built knowledge 
case built knowledge different form different purpose break arbiter tasks design rich reward focus exploration mitigate dimensionality problem crafting key states hand 
agreed biasing necessary crucial step rl clear quality learning system biased 
intuitively human effort insight time required converge 
limit system autonomous non interesting 
challenge amount quality way expressing bias systematic way give inductive leap learning system 
main issue shed light amount quality bias needed cut learning time rl system 
labyrinth world study influence amount quality bias reinforcement learning deterministic world denumerable states considered 
world agent assumed point robot simplified motor actions move square turn degrees 
robot world configurations called labyrinth worlds 
clearly labyrinth world highly simplified scenario real robot world 
unrealistic think dimensionless robot denumerable world states 
similarly impossible throw away details low level control deal simplified motor actions 
despite unrealistic assumptions experiment labyrinth world justifiable reasons 
influence different types biases learning speed investigated rl experiment set times number biases available 
rl set requires large number expensive learning trials 
expensive ways wall clock time danger robot power consumption techniques dyna experience replay transition proximity learning asynchronous dynamic programming examples efforts cut expensive learning trial substituting world experience storage computation 
clearly inefficient carry rl experiment real robot bias introduced learning system 
second main goal identify biases significantly enhance learning 
clearly problem domain enables vary strength bias qualify variation important consideration 
shall see shortly external inductive biases hard manipulate real domains easily manipulated labyrinth domains 
third decided undertake experiment real robot danger come incorrect 
varying bias studying learning performance physical agent notoriously difficult noise error certain parts agent policy fluctuate 
learning system appropriately biased due noise error may exhibit bad performance smoothed averaging large set experiments 
efficient inexpensive method perform experiment artificial world requires experimental effort running real domain come domain free biasing scheme suggest best way biasing rl system 
robot world grid world consisting states identified goal state state chosen start state 
assumed states distinct completely distinguishable 
furthermore possible actions turn left forward turn right agent choose actions tried states 
defines state transitions function state action 
previously mat ric world study compare performance learning bucket brigade algorithms 
domain task agent reach goal state shortest steps 
reward zero transitions goal state case 
entering goal state system instantly transported back start state trial 
attempting action world boundary change state 
structure dynamics known unbiased learning system priori 
dimensional labyrinth problem point robot find shortest path start state goal state numbered bold 
state transitions table governs motion robot 
belief matrices discussed section biases come different forms shape different parts reinforcement learning components 
example domain rich heterogeneous reinforcement fundamentally ease problem temporary credit assignment 
likewise reflex primarily focus exploration 
belief matrices version reflex discrete state action space restricts set possible hypothesis putting strong belief hypothesis strong negative belief needed eliminate hypothesis consideration 
hypothesis pairing state action associated belief value represents appropriateness pairing 
short belief values eliminate put preference set possible actions tried state encoding domain knowledge belief matrix equation 
general statei pg qg bij bik 
bp bp bp bpq ca bias design task described considered different types biases unbiased environment insight goal 
choice biases primary guided particular task hand 
case tasks requiring reach destination biases goal useful available 
biases environment generic applied tasks 
unbiased case agent know hand nature problem action selection strategy optimism face uncertainty 
entries belief matrices unused blackboard learning proceeds scratch 
environment bias type bias part environment knowledge inform agent stay away collision boundary world obstacles encoded belief matrix 
category forms biases identified 
excludes actions immediate consequence 
referring actions state action forward immediate consequence collision world boundary 
belief matrix action discriminated putting high negative value chosen action selection mechanism 
second general case looks step ahead puts preference actions belief matrix 
means actions potential consequence preferred actions consequence 
referring forward action state excluded bias 
addition action left potentially results collision world boundary forward action chosen transition taken place 
action right safe bound robot world boundary 
form bias places state belief matrix higher preference right action left 
insight bias type bias tries exploit unique characteristics task 
know fact task unique characteristics great help discovered solving task 
task described goal placed third column main strategy agent arrive column heading right destination state 
close look task reveals states choice actions agent choose leading effect 
instance agent state immediate strategy reach state choose forward action leave column 
accomplished choosing sequence actions left left right right 
agent need execute actions result 
sequence eliminated putting negative value belief matrix 
worth noting leaving insight bias letting rl algorithm discover redundant state action pairs enormously learner 
goal bias goal directed bias 
destination known possible bias learner vector fields ultimately lead agent goal 
vector fields supplied left agent learn 
similar environment bias identified goal directed biases near far biases 
near bias case right vector fields supplied states near goal remaining states left agent discover right vectors rl 
far bias case dual near bias far vector fields supplied agent learns near vector fields 
note word far near literal meaning represent spatial distance 
far carry semantic meanings represent reachability state 
reachability state defined minimum sequence actions required reach specified goal starting state 
agent near goal spatially may require series actions reaches goal robot position goal near separated say wall 
task described state spatially nearer goal state agent starts state requires actions reach goal starts state needs action left 
learning learning rl algorithm explicit model sys pair max tem cost structure 
algorithm works maintaining estimate expected reinforcement state action pair called values adjusting values actions taken rewards received 
done difference immediate reward received plus discounted value state value current state action uq state system applying 
choice key parameters learning affects efficiency learner 
parameter determines learning rate results update rule disregards history accumulated current value 
resets value current sum received expected reward time step usually causes algorithm oscillate 
parameter discount factor reward 
ideally close general case 
values learning parameters initial values shown table 
world deterministic unity discount factor chosen relevance reward maximized 
bias values initialized belief matrix bias 
initial table learning parameters initialization 
action selection learning process opposing objectives combined 
hand environment sufficiently pv explored order find sub optimal controller 
hand exploited minimize cost learning 
simplest way balancing exploration exploitation take action best estimated expected reward choosing action random 
simple strategy mechanism distinguish promising action hopeless actions exploration 
choose boltzmann exploration strategy slightly complex strategy actions chosen probabilities depend current evaluation 
probability controller executes decreasing positive valued function referred computational temperature controls exploration adjusting sharply probability peaks greedy policy 
experimental results test learning algorithm different types biases particular state labyrinth problem state chosen start state agent 
specific task agent find shortest path state state 
start goal states known optimal number actions required reach goal computed hand case 
optimal policies takes agent goal state shortest path 
policies example left forward left left forward right 
learning algorithm seeks find policies 
bias type involves learning experiment total number biases learning experiments conducted figures show respective learning curves 
purpose comparison learning curves plotted unbiased light curve 
furthermore learning curve average episodes runs 
episode turn consists trials 
vertical axes figures depict number actions agent required reach goal trials 
actions trials environment biases light curve unbiased performance dark heavy dark curves performances ofb andb respectively 
curves performance learning time defined different ways 
method equate learning time number trials agent required reaching optimal performance 
measure misleading actions trials insight bias dark curve performance agent 
actions trials goal biases dark heavy dark curves performance andb biases respectively 
take bi account actual time elapsed trial 
accurate performance measure considers number trials time trial trt dt wheref function plot graphs trial 
equation represents average number actions agent taken convergence 
discrete states reformulated number trials convergence andf number actions taken 
table shows performance indices various biases computed equation 
equation measures reduction number actions gained relative unbiased performance 
incorporating environment reduced average number action respectively 
looks step ahead put preference actions profitable 
shown bias types indices unbiased environment environment ii insight goal goal ii table performance index different types biases 
final learned policy ofb poorer unbiased performance 
paradox explained asfollows 
general biased reinforcement takes advantage cleverness designer reduce state space manually 
process irrelevant inputs eliminated potentially useful ones overlooked resulting incomplete state space suboptimal solution 
hand unbiased reinforcement complete state space guarantees agent sufficient time reinforcement produce complete optimal policy 
reassuring quality useless practical terms 
agent quickly reaches plateau optimality may applications preferable agent guarantees eventual convergence sluggish early learning 
insight bias reduced average action 
astonishing result search space 
large search space insight bias enabled agent learn faster 
signifies fact biases derived problem insight stronger environment biases 
unfortunately environment insight bias sufficient real complex systems 
simple defined problem best bias reduced average action half 
suggests system needs efficient task oriented bias leverage learning significantly 
shows learning curve andb employed 
seen near goal bias performs worse worth discussing 
far goal bias average action convergence reduced unbiased 
biases introduced produced significant leap learning 
continual learning reinforcement learning works start scratch adapt single policy 
learning task independently scratch continual learning useful 
carried experiment investigate agent able previously learned knowledge speed learning entirely new policy 
procedure followed follows 
system trained previously described task initial state goal state 
training unbiased final learned values stored designate values 
new task constructed goal state altered state corresponds complete shift relative goal location right left 
new task rl experiments carried 
experiment values initialized identical value corresponds learning scratch 
second experiment values initialized stored values previous learned task shows learning curves experiments 
note optimum number actions new task ways initializations arrived optimum value 
continual learning agent accumulates learned previous task performed far better learning task independently 
continual learning scheme brought advantages number actions taken trials significantly reduced second trials required arrive optimum action 
performance index equation average number actions agent required continual learning scheme adapt new policy lowered learning scratch 
order check enhancement dependent new task carried experiment various goal states labyrinth grid 
surprisingly instance earlier training interfered caused continual learning perform poorer 
actions trials performance continual learning dark curve learning scratch light curve 
different types biases considered effect learning time rl studied 
generally results indicate biases similar influence learning speed biases aid learning intensively aid 
bias seen mitigating learning process care taken constructing bias 
important ensure introduced bias harmless final learning performance 
widely accepted method biasing learning system cutting search space may best choice 
certain biases particularly derived unique characteristics problem perform better spite large state space 
continual learning agents learn new tasks faster agents learn tasks independently 
andrew barto 
connectionist learning control 
neural networks control pages 
andrew barto steven bradtke satinder singh 
learning act real time dynamic programming 
artificial intelligence 
gerald sommer 
embedding knowledge reinforcement learning 
international conference artificial neural network icann pages sk vde sweden 
leslie kaelbling michael littman andrew moore 
reinforcement learning survey 
artificial intelligence research 
long ji lin 
reinforcement learning robots neural networks 
phd thesis carnegie mellon univeristy department computer science 
sridhar mahadevan jonathan connell 
automatic programming behavior robots reinforcement learning 
artificial intelligence 
maja mat ric 
comparative analysis reinforcement learning methods 
technical report mit artificial intelligent laboratory 
maja mat ric 
reward functions accelerated learning 
proceedings eleventh international conference machine 
andrew mccallum 
transitional proximity faster reinforcement learning 
machine learning proceedings ninth international workshop ml pages aberdeen 
jos mill rapid safe incremental learning navigation 
ieee transactions systems man cybernetics 
richard sutton 
learning predict methods temporal differences 
machine learning 
richard sutton 
result dyna integrated architecture learning planning reacting 
neural networks control 
bradford book 
christopher watkins 
learning delayed rewards 
phd thesis kings college cambridge uk 
