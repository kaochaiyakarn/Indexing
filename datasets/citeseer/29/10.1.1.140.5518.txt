decades statistical language modeling go 
statistical language models estimate distribution various natural language phenomena purpose speech recognition language technologies 
significant model proposed attempts improve state art 
review point promising directions argue bayesian approach integration linguistic theories data 

outline statistical language modeling attempt capture regularities natural language purpose improving performance various natural language applications 
various smoothing techniques developed 
include discounting ml estimates recursively backing lower order grams linearly interpolating grams different order 
approaches include variable length gram lattice approach 
done compare perfect smoothing techniques various conditions 
analysis :10.1.1.131.5458
addition toolkits implementing various techniques disseminated 
way battle sparseness vocabulary clustering 
class word assigned 
model structures 
class word assigned 
model structures 
example trigram pr pr pr pr pr pr pr pr pr pr pr quality resulting model depends course clustering narrow discourse domains atis results achieved manual clustering semantic categories 
constrained domains manual clustering linguistic categories parts speech usually improve model 
automatic iterative clustering information theoretic criteria applied large corpora reduce perplexity model interpolated word counterpart :10.1.1.13.9919

decision tree models decision trees cart style algorithms applied language modeling 
decision tree arbitrarily partition space histories asking arbitrary binary questions history internal nodes 
training data leaf con struct probability distribution pr reduce variance estimate leaf distribution interpolated internal node distributions path root 
strong bias introduced restricting class questions considered greedy search algorithms 
support optimal single word questions node algorithms developed rapid optimal binary partitioning vocabulary 
attempt cart style lm history window words restricted questions individual words allowed complicated questions consisting composites simple questions 
took months train result fell short expectations reduction perplexity baseline trigram reduction interpolated 
second attempt stronger bias introduced vocabulary clustered binary hierarchy word assigned bit string representing path leading root :10.1.1.13.9919
tree questions restricted identity significant bit word history 
reduced candidate set handful questions node 
unfortunately results disappointing approach largely abandoned 
theoretically decision trees represent ultimate partition models 
apparent decision trees tree grows leaves contain fewer fewer data points 
fragmentation avoided exponential model form parameters normalizing term features arbitrary functions pair 
training corpus ml estimate shown satisfy constraints empirical distribution training corpus 
ml estimate shown coincide maximum entropy distribution highest entropy distributions satisfying equation 
unique ml solution iterative procedure :10.1.1.43.7345
paradigm general framework suggested language modeling seen considerable success :10.1.1.103.7637:10.1.1.103.7637:10.1.1.40.180:10.1.1.40.180
strength lies incorporating arbitrary knowledge sources avoiding fragmentation 
example conventional distance long distance word pairs triggers encoded features resulted perplexity reduction speech recognition word error rate reduction trigram baseline 
modeling elegant general weaknesses 
fragmentation avoided exponential model form parameters normalizing term features arbitrary functions pair 
training corpus ml estimate shown satisfy constraints empirical distribution training corpus 
ml estimate shown coincide maximum entropy distribution highest entropy distributions satisfying equation 
unique ml solution iterative procedure :10.1.1.43.7345
paradigm general framework suggested language modeling seen considerable success :10.1.1.103.7637:10.1.1.103.7637:10.1.1.40.180:10.1.1.40.180
strength lies incorporating arbitrary knowledge sources avoiding fragmentation 
example conventional distance long distance word pairs triggers encoded features resulted perplexity reduction speech recognition word error rate reduction trigram baseline 
modeling elegant general weaknesses 
training model computationally challenging altogether infeasible 
probabilistic dependency grammars particularly suited gram style modeling word predicted small number words 
main difference conventional gram structure model predetermined word predicted words immediately preceded 
dg words serve predictors depends dependency graph hidden variable 
typical implementation parse sentence generate dependency graphs attendant probabilities compute generation probability gram style model estimate complete sentence probability approximate derived sentence approximated single best scoring parse 
example model uses parser generate candidate parses trains parameters maximum entropy :10.1.1.14.4552
probabilistic link grammar mentioned section falls roughly category 
employed parser probabilistic parameterization pushdown automata em type algorithm training encouraging results recognition word error rate reduction notoriously difficult corpus 
method combining hidden linguistic structure chain rule parameterization yield linguistically grounded computationally tractable model 

native speakers feel strongly language deep structure 
sure articulate structure encode probabilistic framework 
established linguistic theories surprisingly little help probably goal draw line properly language isn goals quite different 
example consider problem clustering vocabulary words discussed section 
mentioned automatic iterative methods proposed :10.1.1.13.9919
table lists example word classes derived method 
words placement appear satisfactory words place 
surprisingly words count corpus insufficient reliable assignment 
ironically exactly words stood benefit clustering 
ieee transactions speech audio processing 
pierre ronald rosenfeld 
lattice language models 
technical report cmu cs carnegie mellon university department computer science september 
stanley chen joshua goodman :10.1.1.131.5458
empirical study smoothing techniques language modeling 
proceedings th annual meeting acl pages santa cruz california june 
ronald rosenfeld 
cmu statistical language modeling toolkit arpa csr evaluation 
proceedings darpa speech natural language workshop june 
wayne ward 
cmu air travel information service understanding spontaneous speech 
proceedings darpa speech natural language workshop pages june 
peter brown vincent della pietra peter jennifer lai robert mercer :10.1.1.13.9919
gram models natural language 
computational linguistics december 
kneser hermann ney 
improved clustering techniques class statistical language modeling 
david frederick jelinek victor harry eric ronald rosenfeld andreas stolcke wu 
structure performance dependency language model 
proceedings european conference speech communication technology eurospeech pages 
volume 
michael collins :10.1.1.14.4552
new statistical parser bigram lexical dependencies 
proceedings th annual meeting association computational linguistics pages may 
fred jelinek 
recognition performance structured language model 
