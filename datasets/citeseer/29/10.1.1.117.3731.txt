kluwer academic publishers boston 
manufactured netherlands 
tutorial support vector machines pattern recognition christopher burges burges lucent com bell laboratories lucent technologies editor usama fayyad 
tutorial starts overview concepts vc dimension structural risk minimization 
describe linear support vector machines svms separable non separable data working non trivial example detail 
describe mechanical analogy discuss svm solutions unique global 
proofs versions ihave placed strong emphasis clear self contained material accessible possible 
done expense elegance generality generality easily added basic ideas clear 
longer proofs collected appendix 
way motivation alert reader literature summarize applications extensions support vector machines 
pattern recognition case svms isolated handwritten digit recognition cortes vapnik scholkopf burges vapnik scholkopf burges vapnik burges scholkopf object recognition speaker identi cation schmidt detection face detection images osuna freund girosi text categorization joachims :10.1.1.15.9362
regression estimation case svms compared benchmark time series prediction tests muller mukherjee osuna girosi boston housing problem drucker arti cial data pet operator inversion problem vapnik smola 
cases svm generalization performance error rates test sets matches signi cantly better competing methods 
svms density estimation weston anova decomposition studied 
regarding extensions basic svms contain prior knowledge problem example large class svms image recognition problem give results pixels rst permuted randomly image su ering permutation act leave best performing neural networks severely handicapped done incorporating prior knowledge svms scholkopf burges vapnik scholkopf burges 

non separable case algorithm separable data applied non separable data nd feasible solution evidenced objective function dual lagrangian growing arbitrarily large 
extend ideas handle non separable data 
relax constraints necessary cost increase primal objective function doing 
done introducing positive slack variables constraints cortes vapnik xi yi xi yi error occur corresponding exceed unity upper bound number training errors :10.1.1.15.9362
natural way assign extra cost errors change objective function minimized kwk parameter chosen user larger corresponding assigning higher penalty errors 
stands convex programming problem positive integer quadratic programming problem choice advantage lagrange multipliers appear wolfe dual problem maximize ld subject xj solution ns ns number support vectors 
di erence optimal hyperplane case upper bound situation summarized schematically 
need kuhn tucker conditions primal problem 
specify fall margin labeled 
simply assigns xed class say vc dimension higher bound derived theorem 
true labels errors see appendix 
labels correct arrives gap tolerant classi ers 
hand known structural risk minimization systems structure depend data shawe taylor shawe taylor :10.1.1.33.8995
unfortunately resulting bounds looser vc bounds loose examine typical case bound factor higher measured test error 
moment structural risk minimization provide rigorous explanation svms generalization performance 
arguments strongly suggest algorithms minimize expected give better generalization performance 
evidence theorem vapnik quote proof theorem optimal hyperplanes passing origin error error probability error test set expectation left training sets size expectation right training sets size order observations useful real problems compute diameter minimal enclosing sphere described number training points kernel mapping 
