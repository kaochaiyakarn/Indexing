line algorithms machine learning avrim blum carnegie mellon university pittsburgh pa 
email avrim cs cmu edu 
areas line algorithms machine learning concerned problems making decisions knowledge past 
areas di er terms emphasis problems typically studied collection results computational learning theory nicely line algorithms framework 
survey article discusses results models open problems computational learning theory particularly interesting point view line algorithms 
emphasis article describing simpler intuitive results proofs 
pointers literature sophisticated versions algorithms 
areas line algorithms machine learning concerned problems making decisions limited information 
di er terms emphasis problems typically studied collection results computational learning theory nicely line algorithms framework 
survey article discusses results models open problems computational learning theory particularly interesting point view line algorithms 
article meant comprehensive 
goal give reader sense interesting ideas problems area line algorithms feel 
describing problem predicting expert advice studied extensively theoretical machine learning literature 
algorithms developed achieve quite tight bounds terms competitive ratio type measure 
broaden discussion consider standard models line learning examples examine key issues involved 
describe interesting algorithms line learning including winnow algorithm algorithm learning decision lists discuss issues attribute cient learning nite attribute model learning target functions change time 
list important open problems area discussion ideas computational learning theory line algorithms fruitfully combined 
aid ow text discussions history placed special history subsections article 
predicting expert advice simple intuitive problem 
learning algorithm task day predicting rain day 
prediction algorithm input advice experts 
day expert predicts learning algorithm information order prediction algorithm input bits produced experts 
making prediction algorithm told fact day 
suppose assumptions quality independence experts hope achieve absolute level quality predictions 
case natural goal perform nearly best expert far guarantee time algorithm performed worse whichever expert fewest mistakes date 
language competitive analysis goal competitive respect best single expert 
call sequence events algorithm receives predictions experts prediction told correct answer trial 
discussion assume predictions belong set consider general sorts predictions valued real valued 
simple algorithm problem described basic version problem predicting expert advice extensions predictions probabilities general sorts suggestions described section 
describe simple algorithm called weighted majority algorithm 
algorithm maintains list weights wn expert predicts weighted majority vote expert opinions 
weighted majority algorithm simple version 
initialize weights wn experts 
set predictions fx xng experts output prediction highest total weight 
output wi output 

correct answer received penalize mistaken expert multiplying weight 
xi wi xi wi modi ed 
goto 
theorem 
number mistakes weighted majority algorithm described lgn number mistakes best expert far 
wi proof 
denote total weight experts initially algorithm mistake means half total experts predicted incorrectly step total weight reduced factor 
algorithm mistakes hand best expert mistakes weight clearly combining yields lgn lg lgn ut better algorithm achieve better bound described modifying algorithm ways 
rst randomizing 
predicting outcome highest total weight view weights probabilities predict outcome probability proportional weight 
second change replace multiply multiply determined 
intuitively advantage randomized approach worst case 
previously worst case slightly half total weight predicted incorrectly causing algorithm mistake reduce total weight 
roughly chance algorithm predict correctly case generally probability algorithm mistake tied amount thatthe weight reduced 
second advantage randomized approach selecting expert probability proportional weight 
algorithm naturally applied predictions strategies sorts things easily combined 
experts programs run functions evaluated view speeds prediction expert needs examined order produce algorithm prediction experts examined order 
describe algorithm analysis 
weighted majority algorithm randomized version 
initialize weights wn experts 
set predictions fx xng experts output xi probability wi wi 

correct answer received penalize mistaken expert multiplying weight goto 
theorem 
sequence trials expected number mistakes randomized weighted majority algorithm described satis es ln lnn number mistakes best 
instance get expected number mistakes lnn get expected number mistakes lnn 
thatis adjusting competitive ratio algorithm close desired expense increase additive constant 
fact adjusting dynamically typical guess double approach achieve corollary 
sequence trials expected number mistakes modi ed version randomized weighted majority algorithm described satis es lnn ln number mistakes best 
proof theorem 
de ne fi fraction total weight wrong answers ith trial 
say wehave 
expected number mistakes far pt fi 
ith example total weight changes fi multiply weights experts mistake fi fraction weight experts 
nal weight ty fi best expert far 
fact total weight large weight expert ty fi natural log sides get ln ln tx tx ln fi ln ln fi ln ln tx fi ln ln lnn get third line noting ln fourth fi 
ut history extensions computational learning theory community problem predicting expert advice rst studied littlestone warmuth markowsky wegman vovk 
algorithms described aswell theorems littlestone warmuth corollary number re nements cesa bianchi 
key lessons comparison statistical nature remove statistical assumptions data achieve extremely tight bounds see freund :10.1.1.51.3156
problem variations extensions addressed number di erent communities names sequential compound decision problem universal prediction universal coding universal portfolios prediction individual sequences notion competitiveness called min max regret algorithm :10.1.1.56.1067
web page uniting communities discussion general problem exists www stat wharton upenn edu seq 
large variety extensions problem described studied 
example suppose expert provides real number prediction interpret real number expert belief probability rain suppose algorithm may produce real number 
case specify loss function penalty predicting outcome 
common loss functions appropriate di erent settings absolute loss jp xj square loss log loss ln ln 
papers vovk cesa bianchi foster vohra describe optimal algorithms speci loss functions wide variety general loss functions 
second extension framework broaden class algorithms algorithm competitive 
instance littlestone long warmuth modi cations algorithms described constant competitive respect best linear combination experts squared loss measure 
merhav competitive respect best line strategy implemented nite state machine 
variation problem remove semantics associated speci predictions experts simply talk losses 
variation learning algorithm required iteration select expert go 
instance suppose playing player matrix game row player 
viewed expert 
game probabilistically select expert row game done nd loss expert 
playing repeatedly adversary get opportunity probabilistically select expert forth 
freund schapire show extensions randomized weighted majority algorithm discussed nicely scenario see classic blackwell 
scenario tting framework case page replacement algorithm operating system needs decide algorithm 
periodically operating system computes losses various algorithms information decides algorithm 
ordentlich cover describe strategies related randomized weighted majority algorithm problem line portfolio selection 
give line algorithm optimally competitive best constant rebalanced portfolio crp 
algorithm viewed creating expert crp allocating resources 
setting nice property market automatically adjusts weights algorithm just initially divides funds equally nitely lets sit 
simple analysis algorithm extensions transaction costs :10.1.1.44.5760
line learning examples previous section considered problem learning expert advice 
focus consider general scenario line learning examples 
setting example space typically gn learning proceeds sequence trials 
trial example xis learning algorithm 
algorithm predicts example positive negative nally algorithm told true label 
algorithm penalized mistake prediction di ers 
goal mistakes possible 
typically presentation examples assumed control adversary 
setting broadly called mistake bound learning model 
scenario described far di erent standard framework line algorithms line sequence tasks want penalty larger best line algorithm 
task predicting labels best hidden information zero mistakes line algorithm better mistakes half time labels chosen randomly restriction problem necessary order nontrivial statements algorithms 
natural restrictions restrict labels determined reasonable function examples restrict line algorithms compared reasonable class algorithms restrict adversary having sort randomness behavior 
restrictions corresponds standard model studied computational learning theory describe detail 
describe models need notion concept class 
class simply set boolean functions domain boolean function called concept associated representation functions 
instance class disjunctions class functions described disjunction variables fx xng 
class dnf formulas contains boolean functions description length equal size minimum dnf formula representation 
discussion denote description length examples size denote description length concept 
describe standard learning problems 
learning concept class mistake bound model setting assume labels attached examples generated unknown target concept 
hidden concept belonging class trial label example equal 
goal learning algorithm mistakes possible assuming choice target concept choice examples control adversary 
speci cally algorithm property target concept cit poly size mistakes sequence examples running time trial poly size algorithm learns class mistake bound model 
furthermore number mistakes poly size polylog algorithm robust presence additional irrelevant variables algorithm said attribute cient 
algorithms learning variety concept classes mistake bound model disjunctions dnf formulas decision lists linear threshold functions 
describe elegant practical algorithm called winnow algorithm learns disjunctions mistake bound model log mistakes number variables appear target disjunction 
winnow attribute cient 
algorithm property track target concept changes time describe sense algorithm viewed log competitive task 
discuss general results attribute cient learning model known nite attribute model 
agnostic learning competitive class model assumptions existence relationship labels examples 
simply set goal performing nearly best concept called agnostic learning model viewed problem learning concept class presence adversarial noise 
article terminology line algorithms call goal competitive respect best concept speci cally algorithm competitive respect exists polynomial sequence examples concept number mistakes algorithm mc size mc concept algorithm running time trial polynomial size best concept data seen far 
consider class single variable concepts consists concepts fc cng ci xi really problem learning expert advice discussed section just think example list predictions experts algorithms section show achieve competitiveness respect best concept class 
worth noting care computational complexity remove restriction algorithm run polynomial time trial achieve competitiveness concept class speci cally wehave theorem 
theorem 
concept class non polynomial time algorithm sequence examples mc size mistakes 
proof 
simply associate expert concept randomized weighted majority algorithm described section modi cation initial weight concept size 
assignment initial weights means initially total weight 
inequality replaced statement trials ty fi size mc solving inequality proof theorem yields guarantee total number mistakes algorithm satis es mc ln size hand algorithm clearly run polynomial time interesting concept classes requires enumerating possible concepts 
ut second fact worth noting cases np hardness results require learning algorithm representations class instance np hard set labeled examples nd disjunction minimizes number disagreements sample 
necessarily imply np hard achieve competitive ratio approaching learning respect class disjunctions hypothesis learning algorithm need disjunction 
mentioned open problems section unknown possible achieve competitive ratio respect class disjunctions polynomial time algorithm 
learning presence random noise model lies somewhat models discussed far 
model assume target concept standard mistake bound model 
example learning algorithm adversary ips coin probability gives algorithm wrong label 
example correct label seen probability incorrect label seen probability independently example 
usually model considered case adversary restricted selecting examples xed unknown distribution instance space 
elaborate model article results line algorithms feel say nice theory developed learning setting intriguing open problems including list section 
nal point worth mentioning collection simple reductions standard concept classes 
instance algorithm learn class monotone disjunctions functions asx learn non monotone disjunctions conjunctions cnf formulas constant dnf formulas constant transformation input space 
classes related way need discuss simplest 
pac learning setting similar simpler fact learn concept class malicious noise simply nding concept fewest disagreements sample 
simple algorithms example learning class mistake bound model consider simple algorithm learning monotone disjunctions 
hypothesis xn 
time mistake negative example simply remove variables set notice remove variables guaranteed target function mistake positive example 
mistake removes variable algorithm mistakes 
concept class class decision lists 
decision list function form bm literal variable negation bi instance possible decision list rule positive negative positive 
decision lists natural representation language settings shown collection useful theoretical properties 
algorithm learns decision lists making rn mistakes target function relevant variables length 
hypotheses algorithm slight generalization decision lists allow rules exist level conditions level satis ed just arbitrarily choose follow 

initialize level list level contains possible rules includes possible rules 

example look rst level contains rule condition satis ed rule prediction choices choose arbitrarily 

prediction mistaken move rule level 

return step 
algorithm property rule moves level lower mistake 
notice rst rule target concept moved inductively rule move ith level rule fall levels length algorithm nl nr mistakes 
winnow algorithm describe sophisticated algorithm learning class monotone disjunctions previous section 
algorithm called winnow algorithm designed learning especially mistakes number relevant variables total number variables particular data consistent disjunction variables algorithm log mistakes 
describing result show winnow algorithm achieve essence log competitive ratio learning disjunction changes time 
discuss behavior winnow agnostic setting 
variations algorithm learn boolean threshold functions stick problem learning disjunctions keep analysis simpler 
majority algorithm discussed earlier winnow algorithm maintains set weights variable 
winnow algorithm simple version 
initialize weights wn variables 
example fx xng output output 

algorithm mistake algorithm predicts negative positive example xi equal double value wi 
algorithm predicts positive negative example xi equal cut value wi half 

goto 
theorem 
winnow algorithm learns class disjunctions mistake bound model making lg mistakes target concept disjunction variables 
proof 
rst bound number mistakes positive examples 
mistake positive example double weights target function relevant weights mistake negative example halve de nition disjunction 
furthermore weights doubled lg times weights doubled 
winnow lg mistakes positive examples 
bound number mistakes negative examples 
total weight summed variables initially positive example increases total doubling 
hand negative example decreases total weight halving 
total weight drops zero 
number mistakes negative examples twice number mistakes positive examples plus 
lgn 
adding bound number mistakes positive examples yields theorem 
ut examples necessarily consistent target disjunction 
disjunction de ne mc number mistakes concept ac number attribute errors data respect de ne follows 
example labeled positive satis es relevant variables add ac example labeled negative satis es relevant variables add ac 
concept disjunction variables mc ac rmc 
hard show winnow behavior agnostic learning disjunctions 
theorem 
sequence examples disjunction number mistakes winnow ac log number relevant variables ac rmc means winnow competitive respect best disjunction variables 
fact randomizing tuning winnow algorithm speci value achieve stronger statement 
theorem 
tune randomized winnow algorithm sequence examples disjunction variables expected number mistakes algorithm ac ln 
ac acr ln kinds theorems viewed results generalization experts scenario discussed section 
speci cally consider algorithm access specialists 
trial specialist may choose prediction may choose abstain experts scenario expert prediction trial 
think specialists making prediction situation ts specialty 
proof prove theorem show thata version winnow algorithm constant competitive respect best set specialists charge set unit mistake specialist set unit specialists set abstain 
learning drifting disjunctions problem learning static target concept noise data real notion competitiveness 
algorithm just xed upper bounded number mistakes 
natural variation scenario relevant practice imagine target function static changes time 
instance case learning disjunction imagine time time variables added removed target function 
case natural measure adversary cost number additions deletions target function obvious goal anumber mistakes larger adversary cost 
speci cally consider game played adversary 
variables target concept initially disjunction zero says negative 
round game proceeds follows 
adversary turn adversary target concept removing variables target disjunction 
adversary pays cost added 
number variables removed time bounded number added time removing variables free 
adversary presents example learning algorithm 
learner turn learning algorithm prediction example told correct answer current target concept 
algorithm charged cost mistake 
consider variation winnow algorithm allows decrease mistake weights value cut half 
surprisingly winnow variant guarantees cost log times adversary cost 
sense log competitive problem 
note theorem viewed special case rst move adversary adds variables target function changes 
theorem 
winnow variant described sequence examples ca log mistakes ca adversary total cost far 
proof 
consider total weight 
total weight initially positive example increases total weight mistake negative example decreases total weight sum come weights equal sum gets cut half 
number mistakes negative examples bounded mp mp number mistakes positive examples 
need bound positives 
denote set variables current target function currently relevant variables jrj 
consider potential function log lg wi consider potential function change 
mistake positive example decreases 
time mistake negative example change 
time adversary adds new relevant variable increases log log increase possibility new weight wi equals lg wi 
time adversary removes relevant variable increase may decrease variable removed weight 
summary way increase adding new relevant variable mistake positive example decreases furthermore initially zero non negative 
number mistakes positive examples bounded log times adversary cost proving theorem 
ut learning string valued attributes nite attribute model discussion far focused learning instance space gn examples boolean valued attributes 
common setting attributes string valued instance attribute represent object color texture number choices attribute small just convert boolean case instance letting red boolean variable true false example 
attribute large unknown apriori conversion may blow number variables 
issue motivates nite attribute learning model 
model nitely boolean variables example satis es nite number 
example speci ed listing variables satis ed instance typical example meaning variables true rest false example 
size number variables satis ed largest example seen far 
goal algorithm setting anumber mistakes polynomial size target function independent total number variables nite 
running time trial polynomial size target function description length longest example seen far 
hard see model situation learning algorithms standard boolean attribute setting fail nite attribute model 
instance listing variables crossing ones irrelevant simple disjunction learning algorithm section clearly 
decision list algorithm fails fact known polynomial time algorithm learning decision lists setting see open problems section 
hand algorithms adapted straightforward way succeed nite attribute model 
generally theorem known 
theorem 
embedding closed concept class attribute cient algorithm learning gn learned nite attribute model 
just reasonableness condition saying take concept de ned variables embed space variables stay class reverse direction values variables legal concept 
see details :10.1.1.28.5776
history winnow algorithm developed littlestone seminal extensions introduces mistake bound learning model :10.1.1.130.9013
mistake bound model equivalent extended equivalence query model angluin known strictly harder polynomialtime algorithms pac learning model valiant inwhich di erences adversary required select examples xed distribution 
agnostic learning 
littlestone gives variety results behavior winnow inthe presence various kinds noise 
improved bounds theorem auer warmuth 
winnow learning changing concepts folklore homework problem auer warmuth provide sophisticated algorithm analysis achieving stronger result theorem style theorem 
winnow algorithm shown quite successful practical tasks predicting links followed users web calendar scheduling application 
algorithm learning decision lists rivest algorithm pac model adapted mistake bound model littlestone helmbold sloan warmuth 
nite attribute model de ned blum theorem blum hellerstein littlestone :10.1.1.28.5776
open problems 
bounds corollary achieved improved smooth algorithm 
bound corollary achieved guess double algorithm periodically throws learned far restarts new value natural better practice just smoothly adjust go restarting scratch 
algorithm form shown achieve bound preferably better constants 
see precise constants 

decision lists learned attribute ciently 
recall section decision list function form elseif bm literal variable negation bi 
section decision lists relevant variables learned rn mistakes mistake bound model 
alternative approach winnow algorithm log mistakes 
decision lists learned attribute ciently 
mistake bound poly polylog 

parity functions learned attribute ciently 
denote class functions compute parity subset variables 
instance typical function 
easy learn mistake bound model making mistakes viewing labeled example linear equality modulo new example linearly dependent previous set label deduced provides new linearly independent vector 
learned attribute ciently 

decision lists parity functions learned nite class decision lists class parity functions learned nite attribute model 
case decision lists may assume wish literals negations variables 

converse theorem 

tolerance random noise boosted 
suppose concept class xed constant noise rate exists polynomial time algorithm property target concept distribution examples achieves expected mistake rate polynomial seeing polynomially examples 
imply exist polynomial time algorithm succeeds sense constant noise rates 
see kearns related issues 

competitive ratio achieved learning respect best disjunction 
polynomial time algorithm sequence examples number mistakes number mistakes best disjunction constant polynomial 
number relevant variables best disjunction 
making mistakes easy standard disjunction learning algorithms saw winnow algorithm mistakes 

disjunctions weak learned presence adversarial noise 
polynomial constant exist algorithm guarantee sequence examples gn fraction examples consistent disjunction gn algorithm mistakes expectation algorithm randomized 
exists disjunction nearly correct say data algorithm achieve performance slightly poly better guessing 
algorithm may require polynomial 
linear threshold functions weak learned presence adversarial noise 
question replace disjunctions linear threshold functions 
question yield quasi polynomial time algorithm learning dnf formulas generally learning ac functions pac learning model 
implication follows standard complexity theory results show ac approximated low degree polynomials 
article surveyed collection problems models algorithms computational learning theory look particularly interesting point view line algorithms 
include algorithms combining advice experts model line agnostic learning learning presence worst case noise problem learning drifting target concept 
clear crossover ideas computational learning theory line algorithms possible 
listed respective strengths weaknesses areas crossover may prove especially fruitful 
notion state 
notion algorithm having state cost associated changing state central area line algorithms 
allows study problems decisions algorithm involve doing just predicting decisions rent ect costs algorithm pay 
issue virtually ignored computational learning theory literature literature tended focus prediction problems 
prediction problems state algorithm usually just current hypothesis natural penalty changing state 
computational learning theory moves analyze general sorts learning problems inevitable notion state play larger role ideas line algorithms crucial 
direction appears 
limiting power adversary 
line algorithms literature usually assumed adversary unlimited power choose worst case sequence algorithm 
machine learning setting natural assume sort regularity world world completely random learn 
assumes world produces labels function limited concept class examples drawn xed distribution xed distribution simple type 
parametrize results function adversary power producing especially bounds adversary relatively simple 
sort approach line algorithms fact achieving pessimistic sorts bounds problems commonly studied 
limiting class line algorithms compared 
typical machine learning setup restrict adversary achieve non trivial bound limit class line algorithms competing 
sort approach may useful line algorithms achieving reasonable bounds 
iwould helpful discussions pointers 
supported part nsf national young investigator ccr sloan foundation research fellowship 

angluin 
queries concept learning 
machine learning 

armstrong freitag joachims mitchell 
webwatcher learning apprentice world wide web 
aaai spring symposium information gathering heterogeneous distributed environments march 

auer warmuth 
tracking best disjunction 
proceedings th annual symposium foundations computer science pages 

blackwell 
analog minimax theorem vector payo paci math 

blum 
learning boolean functions nite attribute space 
machine learning 

blum 
separating distribution free mistake bound learning models boolean domain 
siam computing october 

blum 
empirical support winnow weighted majority algorithms results calendar scheduling domain 
proceedings twelfth international conference machine learning pages july 

blum burch 
line learning metrical task system problem 
proceedings th annual conference computational learning theory pages 

blum hellerstein littlestone 
learning presence nitely nitely irrelevant attributes 
comp 
syst 
sci 

blum kalai 
universal portfolios transaction costs 
proceedings th annual conference computational learning theory pages 

cesa bianchi freund helmbold warmuth 
line prediction conversion strategies 
computational learning theory eurocolt volume new series number institute mathematics applications conference series pages oxford 
oxford university press 

cesa bianchi freund helmbold haussler schapire warmuth 
expert advice 
annual acm symposium theory computing pages 

cover 
universal portfolios 
mathematical finance january 

cover ordentlich 
universal portfolios side information 
ieee transactions information theory march 

markowsky wegman 
learning probabilistic prediction functions 
proceedings th ieee symposium foundations computer science pages oct 

feder merhav gutman 
universal prediction individual sequences 
ieee transactions information theory 

foster vohra 
randomization rule selecting forecasts 
operations research 

freund 
predicting binary sequence optimal biased coin 
proceedings th annual conference computational learning theory pages 

freund schapire 
game theory line prediction boosting 
proceedings th annual conference computational learning theory pages 

helmbold sloan warmuth 
learning nested di erences intersection closed concept classes 
machine learning 

kearns 
cient noise tolerant learning statistical queries 
proceedings fifth annual acm symposium theory computing pages 

kearns li pitt valiant 
learnability boolean formulae 
proceedings nineteenth annual acm symposium theory computing pages new york new york may 

kearns schapire sellie 
cient agnostic learning 
machine learning 

littlestone 
learning quickly irrelevant attributes abound new algorithm 
machine learning 

littlestone 
personal communication mistake bound version rivest decision list algorithm 


littlestone 
redundant noisy attributes attribute errors linear threshold learning winnow 
proceedings fourth annual workshop computational learning theory pages santa cruz california 
morgan kaufmann 

littlestone long warmuth 
line learning linear functions 
proc 
rd symposium theory computing pages 
acm press new york ny 
see ucsc crl 

littlestone warmuth 
weighted majority algorithm 
information computation 

merhav feder 
universal sequential learning decisions individual data sequences 
proc 
th annu 
workshop comput 
learning theory pages 
acm press new york ny 

ordentlich cover 
line portfolio selection 
colt pages 
journal version submitted mathematics operations research 

rivest 
learning decision lists 
machine learning 

robbins 
asymptotically solutions compound statistical decision problems 
proc 
nd berkeley symp 
math 
statist 
prob pages 

shtarkov 
universal sequential coding single measures 
problems information transmission pages 

valiant 
theory learnable 
comm 
acm november 

vovk 
aggregating strategies 
proceedings third annual workshop computational learning theory pages 
morgan kaufmann 

vovk 
game prediction expert advice 
proceedings th annual conference computational learning theory pages 
acm press new york ny 
article processed lat macro package llncs style 
