text classification bootstrapping keywords em shrinkage andrew mccallum mccallum justresearch com just research henry street pittsburgh pa applying text classification complex tasks tedious expensive hand label large amounts training data necessary performance 
presents alternative approach text classification requires labeled documents uses small set keywords class class hierarchy large quantity unlabeled documents 
keywords assign approximate labels unlabeled documents 
preliminary labels starting point bootstrapping process learns naive bayes classifier expectation maximization hierarchical shrinkage 
classifying complex data set computer science research papers leaf topic hierarchy keywords provide accuracy 
classifier learned bootstrapping reaches accuracy level close human agreement 
keywords assign approximate labels unlabeled documents 
preliminary labels starting point bootstrapping process learns naive bayes classifier expectation maximization hierarchical shrinkage 
classifying complex data set computer science research papers leaf topic hierarchy keywords provide accuracy 
classifier learned bootstrapping reaches accuracy level close human agreement 
provided labeled training examples variety text classification algorithms learn reasonably accurate classifiers lewis joachims yang cohen singer :10.1.1.11.8264
applied complex domains classes algorithms require extremely large training sets provide useful classification accuracy 
creating sets labeled data tedious expensive typically labeled person 
leads consider learning algorithms require large amounts labeled data 
kamal nigam cs cmu edu school computer science carnegie mellon university pittsburgh pa labeled data difficult obtain unlabeled data readily available plentiful 
adding remaining unlabeled data running em helps counter second reasons 
adding hierarchical shrinkage naive bayes helps counter third reasons 
detailed description bootstrapping algorithm short overview standard naive bayes text classification proceed adding em incorporate unlabeled data conclude explaining hierarchical shrinkage 
outline entire algorithm table 
naive bayes framework build framework multinomial naive bayes text classification lewis mccallum nigam :10.1.1.46.1529:10.1.1.11.8264
useful think naive bayes estimating parameters probabilistic generative model text documents 
model class document selected 
words document generated parameters class specific multinomial unigram model 
classifier parameters consist class prior probabilities class conditioned word probabilities 
wt di count number times word wt occurs document di define cj di document class label 
estimate probability word wt class cj wt cj di wt di cj di di ws di cj di class prior probability parameters set way indicates number classes cj di cj di 
unlabeled document classifier determine probability document belongs class cj bayes rule naive bayes assumption words document occur independently class 
denote kth word document di classification cj di cj di cj cj di cj 
empirically large number training documents naive bayes job classifying text documents lewis :10.1.1.11.8264
complete presentations naive bayes text classification provided mitchell mccallum nigam 
adding unlabeled data em standard supervised setting document comes label 
bootstrapping scenario preliminary keyword labels incomplete inaccurate keyword matching leaves documents unlabeled labels incorrectly 
order entire data set naive bayes classifier expectation maximization em algorithm generate probabilistically weighted class labels documents 
