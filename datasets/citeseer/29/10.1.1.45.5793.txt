shared distribution hidden markov models speech recognition mei hwang huang april cmu cs school computer science carnegie mellon university pittsburgh pa parameter sharing plays important role statistical modeling training data usually limited 
hand models detailed possible 
hand models detailed longer reliably estimate parameters 
triphone generalization may force models merged parts model output distributions similar rest output distributions different 
problem avoided clustering carried distribution level 
shared distribution model proposed replace generalized triphone models speaker independent continuous speech recognition 
associated state output distribution representing distinct acoustic event unit speech 
state transition probabilities ij specify time varying properties related different acoustic events 
output probability mixture continuous probability density functions discrete semi continuous output probability distribution 
forward backward algorithm generally iteratively output transition probabilities viterbi beam search algorithm recognition find word sequence 
interested readers referred detailed treatments hmms speech recognition :10.1.1.45.5793
improve structure stochastic model speech complexity dimensionality model usually needs increased leads increased effective number freedom models 
large vocabulary speech recognition hidden markov modeling words difficult repetitions needed train single word hmm 
subword units context dependent phonetic models introduced consistent trainable units 
triphone phone takes consideration left right phonetic contexts 
alternatively reduce amount free parameters techniques parameter sharing applied successfully speech recognition systems 
models group similar poorly trained close information theoretic distance measure 
similar approaches number training tokens 
parameter sharing output distributions may associated arcs states 
example semi continuous tied mixture hmm :10.1.1.45.5793
different continuous probability density functions shared different phonetic models 
comparison discrete hmm multiple codewords best codeword vector quantization vq leads relative trained parameter sets 
sphinx system generalized merge similar order reduce amount parameters clustering hmm level may provide accurate models 
clustering entire models may force output distributions quite different shapes merged parts models exhibit close resemblance 
context independent models estimate generalized 
assume codeword vq codebook represented continuous probability density function acoustic vector 
discrete output distribution replaced semi continuous function codebook size 
practice significant adequate 
assume gaussian density function mean diagonal covariance sigma means covariance matrices formula sigma gamma gamma probability time vq symbol emitted state substantial difference male female speakers sex dependent employed enhance performance :10.1.1.45.5793:10.1.1.45.5793
summarize configuration improved sphinx system ffl codebooks ffl generalized word word triphone models ffl sex dependent 

recognition recognition language network pre compiled represent search space 
shows part language network illustrates connections 
context independent discrete models estimated uniform distribution 
shared distributions employed context independent models sufficient training data 
contextdependent shared distributions estimated 
apply forward backward algorithm shared distribution model parameter counts shared distributions need accumulated baum welch reestimation 
manner affect maximum likelihood estimation criterion :10.1.1.45.5793
function modified prove :10.1.1.45.5793
initialize shared distribution context dependent models component distribution randomly chosen corresponding distribution cluster 
context independent counterpart initial shared distribution 
example table shows members th shared distribution distributions merged clusters 
shared distributions employed context independent models sufficient training data 
contextdependent shared distributions estimated 
apply forward backward algorithm shared distribution model parameter counts shared distributions need accumulated baum welch reestimation 
manner affect maximum likelihood estimation criterion :10.1.1.45.5793
function modified prove :10.1.1.45.5793
initialize shared distribution context dependent models component distribution randomly chosen corresponding distribution cluster 
context independent counterpart initial shared distribution 
example table shows members th shared distribution distributions merged clusters 
chance distribution context dependent model ae selected distribution context independent model copied initial value th shared distribution 
huang hwang lee improved hidden markov modeling speaker independent continuous speech recognition 
darpa speech language workshop 
morgan kaufmann publishers san mateo ca pp 

huang jack hidden markov models speech recognition :10.1.1.45.5793
edinburgh university press edinburgh 
huang jack semi continuous hidden markov models speech signals 
computer speech language vol 
pp 
