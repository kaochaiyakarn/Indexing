modeling text generalizable gaussian mixtures lars kai hansen thomas nielsen jan larsen department mathematical modelling technical university denmark dk denmark email fn uk jl imm dk apply discuss generalizable gaussian mixture ggm models 
model automatically adapts model complexity text representation 
show models depends dimensionality representation sample size 
discuss relation supervised unsupervised learning text data 
implement novelty detector density model 

show models depends dimensionality representation sample size 
discuss relation supervised unsupervised learning text data 
implement novelty detector density model 

information retrieval active research field starting adapt advanced machine learning techniques solving hard real world problems :10.1.1.1.5684
pattern recognition text data categorize text topic spot new topics broader sense create intelligent searches www search engines 
proceeds pattern recognition text features typically document summary statistics 
numerous highlevel language models extraction text features simple summary statistics preferred adapted automatically continually costly manual intervention language expertise 
face limited sets labeled data pattern recognition algorithms typically fail generalize high dimensions need efficient robust means data reduction feature extraction 
pattern recognition text data categorize text topic spot new topics broader sense create intelligent searches www search engines 
proceeds pattern recognition text features typically document summary statistics 
numerous highlevel language models extraction text features simple summary statistics preferred adapted automatically continually costly manual intervention language expertise 
face limited sets labeled data pattern recognition algorithms typically fail generalize high dimensions need efficient robust means data reduction feature extraction 
able generalize high dimensions choose apply biased architecture called naive bayes classifier demonstrate schemes adaptive bias :10.1.1.1.5684
latent semantic indexing lsi approach defined 
lsi summary term document matrix count set terms occur set documents analysis 
list terms adaptive derived words occur certain minimum frequency documents possibly list simple high frequency words 
research supported danish research danish computational neural network center connect thor center center multimedia 
class conditioned density marginal class probabilities 
labeled data set design classifier adapting ggm class separately 
joint density written kc xjk kc component frequencies number components class labels assigned new data point accordance optimal bayes classification rule selecting maximum posterior probability 

generalizable models text demonstrate viability ggm text modeling apply text database results alternative strategy called naive bayes classifier see :10.1.1.1.5684
primary objective show evidence possible enhance learning process mixing unlabeled data authors results learning labeled data :10.1.1.1.5684
produce learning curves generalization error function training set size compare :10.1.1.1.5684
available labeled documents downloaded repository 
web pages labeled categories course faculty project student 
labeled data set design classifier adapting ggm class separately 
joint density written kc xjk kc component frequencies number components class labels assigned new data point accordance optimal bayes classification rule selecting maximum posterior probability 

generalizable models text demonstrate viability ggm text modeling apply text database results alternative strategy called naive bayes classifier see :10.1.1.1.5684
primary objective show evidence possible enhance learning process mixing unlabeled data authors results learning labeled data :10.1.1.1.5684
produce learning curves generalization error function training set size compare :10.1.1.1.5684
available labeled documents downloaded repository 
web pages labeled categories course faculty project student 
term list words occurred documents defined screening words 
joint density written kc xjk kc component frequencies number components class labels assigned new data point accordance optimal bayes classification rule selecting maximum posterior probability 

generalizable models text demonstrate viability ggm text modeling apply text database results alternative strategy called naive bayes classifier see :10.1.1.1.5684
primary objective show evidence possible enhance learning process mixing unlabeled data authors results learning labeled data :10.1.1.1.5684
produce learning curves generalization error function training set size compare :10.1.1.1.5684
available labeled documents downloaded repository 
web pages labeled categories course faculty project student 
term list words occurred documents defined screening words 
term frequency histograms normalized unity computed documents 
learning curves ggm classifier estimated cross validation 
data randomly split times test set test training sets increasing sizes train gamma 
learning curves estimated averaged test error function pca dimension 
find generalization cross function dimension larger dimensional representations need samples generalization 
interplay supervised unsupervised learning discussed :10.1.1.1.5684
estimate role labels ggm model carried similar learning curve experiment unsupervised supervised gaussian mixture model 
experiment estimate ggm input density xjk training data 
estimate voting pattern component mixture normalize frequencies cjk 
unsupervised supervised classifier operates conditional probabilities cjk xjk number labeled documents classification error comp 
course faculty project student course faculty project student learning curves supervised learning generalizable gaussian mixture ggm classifier 
learning curves indexed dimension input representation 
confusion matrix train refers estimated class shows main confusion appears faculty student groups 
learning curve ggm classifier compared 
proposed ggm classifier achieves classification rates learning curves comparable :10.1.1.1.5684
ggm model achieves performance full dimensional term histograms showing strength latent semantic analysis representation 
allows handling complex problems avoiding selection terms :10.1.1.1.5684
learning efficient supervised supervised classifier indicating significant class overlap problem 

confusion matrix train refers estimated class shows main confusion appears faculty student groups 
learning curve ggm classifier compared 
proposed ggm classifier achieves classification rates learning curves comparable :10.1.1.1.5684
ggm model achieves performance full dimensional term histograms showing strength latent semantic analysis representation 
allows handling complex problems avoiding selection terms :10.1.1.1.5684
learning efficient supervised supervised classifier indicating significant class overlap problem 

novelty detection deploying machine learning scheme need address confidence problem predictions 
ggm classifier produces conditional probabilities obtain way clue internal confidence 
conf 
artificial neural networks pp 

ripley pattern recognition neural networks cambridge university press 
nigam mccallum thrun mitchell text classification labeled unlabeled documents em machine learning appear :10.1.1.1.5684
weigend wiener pedersen exploiting hierarchy text categorization 
working stern school business ny univ www stern nyu edu research papers hierarchy ps 
