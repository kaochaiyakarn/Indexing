learning bayesian networks combination knowledge statistical data david heckerman dan geiger microsoft research bldg redmond wa david chickering microsoft com dang cs technion ac il dmax cs ucla edu describe scoring metrics learning bayesian networks combination user knowledge statistical data 
identify important properties metrics call event equivalence parameter modularity 
properties ignored combined greatly simplify encoding user prior knowledge 
particular user express knowledge part single prior bayesian network domain 
fields artificial intelligence statistics share common goal modeling real world phenomena 
ai researchers emphasized knowledgebased approach achieving goal statisticians traditionally emphasized data approach 
unification approaches 
particular develop algorithms bayesian principles take input user prior knowledge expressed part prior bayesian network statistical data returns improved bayesian networks 
researchers examined methods learning bayesian networks data including cooper herskovits buntine spiegelhalter 
referred ch buntine respectively 
methods basic components scoring metric search procedure 
metric computes score proportional posterior probability network structure data user prior knowledge 
search procedure generates networks author primary affiliation computer science department technion haifa israel 
evaluation scoring metric 
methods components identify network set networks high posterior probabilities networks predict events 
concentrate scoring metrics 
restrict domains containing discrete variables show geiger heckerman metrics extended domains containing continuous variables 
major contribution develop metrics set consistent properties assumptions 
called parameter modularity event equivalence ignored part combined ramifications explored 
assumption parameter modularity implicitly ch buntine addresses relationship prior distributions parameters different bayesian network structures 
property event equivalence says structures represent set independence assertions correspond event receive score 
provide justifications assumptions show combined assumptions learning bayesian networks previously obtain straightforward method combining user knowledge statistical data prior network 
approach contrasted ch buntine prior network ch satisfy property event equivalence 
identification principle event equivalence arises subtle distinction types bayesian networks 
type called belief networks represents assertions conditional independence 
second type called causal networks represents assertions cause effect assertions independence 
argue metrics belief networks satisfy event equivalence metrics causal networks need 
score equivalent metric belief networks similar metrics described york dawid lauritzen madigan metric scores directed networks metrics score undirected networks 
concentrate directed models undirected models believe users find easier build interpret 
belief networks notation consider domain discrete variables xn 
lower case letters refer variables upper case letters refer sets variables 
write xi observe variable xi state denote probability person background knowledge observation observation observe state variable set call set observations instance denote set probabilities possible observations possible observations joint space set instances joint probability distribution probability distribution joint space belief network types bayesian networks consider represents joint probability distribution encoding assertions conditional independence collection probability distributions 
chain rule probability know xn xi 
xi variable xi xi set variables renders xi xi conditionally independent 
xi 
xi xi belief network pair bs bp bs belief network structure encodes assertions conditional independence equation bp set probability distributions corresponding structure 
particular bs directed acyclic graph variable corresponds node bs parents node corresponding xi nodes corresponding variables 
remainder xi refer variable corresponding node graph 
associated node xi bs probability distributions xi 
bp union distributions 
combining equations see belief network uniquely determines joint probability distribution 
xn xi minimal belief network belief network equation violated arc removed 
minimal belief network represents assertions independence assertions dependence 
metrics belief networks previous section summarize previous example ch buntine computation score belief network structure bs set cases cm 
case ci observation variables refer database 
bayesian measure goodness belief network structure posterior probability database bs bs bs bs bs bs normalization constant 
small domains network structures sum order determine constant 
researchers bs bs bs network structure score 
note metric treats variables equally important generalized spiegelhalter 
compute bs closed form researchers typically assumptions explicate 
assumption database multinomial sample belief network bs bp 
assumptions implicit assumption 
variables discrete 
modify assumption proceedings geiger heckerman 
assumption user may uncertain belief network structure generating data 
uncertainty encoded prior probabilities network structure bs 
implicit data comes particular network structure user may uncertain probabilities structure 
probabilities thought long run fractions see large database called parameters statistical literature 
note assumption implies processes generating data change time 
case case illustration assumptions network structure binary 
assumption represented belief network 
illustrates assumption network structure binary variables 
shall variable domain illustrate points 
parameter represents long run fraction cases observed true 
observations case independent 
parameters represent long run fraction cases observed true cases observed true false respectively 
parameters known observations cases independent provided observed cases 
general belief network structure bs 
xn ri denote number states variable xi qi xl rl denote number instances integer index instances 
write denote observation jth instance parents xi 
ijk denote long run fraction cases xi cases ij denote union ijk bs denote union ij instances variables xi 
set bs corresponds parameter set bp belief network structure bs defined section 
parameters long run fractions values uncertain 
denote probability density continuous variable set variables 
example ij bs denotes probability density set parameters ij bs 
assumption call parameter independence says parameters associated belief network structure independent obvious dependence parameters variable sum 
assumption parameter independence belief network structures bs bs bs ij bs assumption illustrated network structure variables case observed say case complete 
cases database complete say database complete 
assumption databases complete 
note spiegelhalter 
provide excellent survey approximations circumvent assumption 
general metric follows 
applying chain rule obtain bs cl 
cl bs ci ith case database 
assumption follows parameters remain independent cases observed 
easily seen simple example 
conditioning parameters belief network structure bs cl 
cl bs cl bs bs ij 
cl bs bs case complete cl bs bs ijk xi case cl 
plugging equation equation result equation yields bs bs ijk cl bs ijk denotes expectation ijk respect ij 
difficulty applying equation user provide prior distributions parameter set ij associated structure bs 
reduce number prior distributions assumption 
general variable observed belief network may delete arcs emanating retain valid belief network 
assumption parameter modularity xi parents belief network structures bs bs qi ij bs ij bs call property parameter modularity says densities parameters ij depend structure belief network local variable xi ij depends parents xi 
example variable domain bs network arc pointing bs network arc bs bs parents belief networks 
note ch buntine implicitly assumption parameter modularity cooper herskovits equation buntine spiegelhalter pp 

context causal networks assumption compelling justification see section 
knowledge researchers assumption explicit 
see section assumption important ramifications 
assumption parameter modularity holds cases observed 
consequently drop conditioning event bs equation yield bs bs ijk cl heckerman 
provide greater detail general metric 
concentrate special case parameter set ij dirichlet distribution 
important concept remaining presentation complete belief network 
complete belief network missing edges represents assertions conditional independence 
assumption complete belief network structure bsc ij bs ij bsc dirichlet distribution 
exists exponents ijk ij bsc ijk ijk assumption assumption parameter modularity follows belief network structure bs ij bs ij bs dirichlet distribution 
parameter set bs distribution simply say bs bs dirichlet 
combining previous assumptions consequence assumption obtain ij bs ijk nijk nijk number cases xi prior distribution ij dirichlet distribution posterior distribution ij 
say dirichlet distribution closed multinomial sampling dirichlet distribution conjugate family distributions multinomial sampling 
family ijk ijk nijk nij ij nij ri nijk ij ri ijk substituting equation term equation performing sum obtain ijk qi ij ij ri ijk nijk ijk nij gamma function satisfies 
shall refer equation bd metric bayesian metric dirichlet priors emphasize metric new 
inclusion assumption parameter modularity application metric difficult requires user specify dirichlet exponents ijk complete belief network structure 
section introduce property belief network metrics called event equivalence 
subsequent section show property leads dramatic simplification assessment dirichlet exponents 
event equivalence score equivalence previous section bs argument probabilities probability densities 
bs belief network structure event 
situations event corresponds structure bs superscript stands event 
section provide form 
ch buntine express assumption definition explicate important property definition 
simple definition implicit assumption 
particular assumption says database multinomial sample joint space holds true iff multinomial parameters satisfy independence assertions bs 
example variable domain condition corresponds assertion database multinomial sample joint space xy xy 
bs network structure arc condition says event corresponds assertion xy xy xy xy definition desirable property 
belief network structures represent assertions conditional independence say isomorphic 
example variable domain network structures represent assertion independent definition equivalent follows events structures bs bs isomorphic 
relation isomorphism induces equivalence class set events call property event equivalence 
problem definition 
particular events corresponding non isomorphic network structures mutually exclusive 
example event corresponding complete structure holds true implies event corresponding structure 
case general scores associated network structures useless comparison 
seemingly reasonable repair say holds true iff multinomial parameters satisfy independence dependence assertions bs bs interpreted minimal structure 
revised definition event set equalities set inequalities 
example bs structure variable domain event bs belief network structure arc event events mutually exclusive 
furthermore events corresponding equal 
repair sufficient larger domains 
property score equivalence may violated 
example variable domain events corresponding complete belief network structures different orderings equal 
may recover property including event corresponding set isomorphic network structures union inequalities associated structure second events corresponding non isomorphic structures mutually exclusive 
example events corresponding structures include situation general overlaps measure zero respect events create overlap 
set overlapping events may exclude intersection events affecting mathematical results intuitive understanding events user 
revised definition event guarantees set events corresponding set possible network structures domain mutually exclusive 
furthermore definition retains property event equivalence 
proposition event equivalence belief network structures bs bs isomorphic score network structure bs immediate consequence property event equivalence score equivalence proposition score equivalence scores isomorphic belief network structures equal 
note property event equivalence technically score belief equivalence class structure 
users find intuitive construct interpret belief networks 
consequently continue presentation terms belief networks keeping proposition mind 
easy show bd metric equation exhibit property score equivalence choices dirichlet exponents ijk property event equivalence induce constraints parameters ijk making assumptions parameter independence parameter modularity effect specified prior densities multinomial parameters terms structure belief network 
consequently possibility specification violates property score equivalence 
heckerman 
show assumptions score equivalence consistent 
prior belief network section show property event equivalence assumptions lead constraints exponents ijk see constraints strong exponents may constructed belief network reflecting user current knowledge case equivalent sample size domain 
see property event equivalence assumptions constrain prior densities bs 
moment ignore assumption densities dirichlet 
consider complete belief network structures 
property event equivalence know event associated complete belief network structure domain denote event 
sup sc pose know density multinomial parameters joint space conditioned sc may determine density parameters complete network structure simply performing change variable operation 
example consider complete belief network structure variable domain 
parameter set joint space xy xy parameter set network structure 
sets related relations xy xy density xy xy sc joint space may compute density relation sc sc xy xy sc jacobian transformation xy xy xy xy xy xy assumption parameter modularity result extends belief network structure 
heckerman 
theorem show density multinomial parameters joint space conditioned may determine density parame sc ters network structure 
understand result consider incomplete network structure containing arc variable domain 
method determining density parameters bs illustrated 
assumption parameter indepen xy xy xy bx change variable parameter modularity method obtaining density parameters bs density joint space domain 
dence may obtain densities separately 
obtain density identify complete network structure bsc parents bs bsc 
change variable procedure described previous paragraph determine density sc xy xy 
sc assumption parameter modularity obtain sc 
similar manner illustrated obtain density 
consider assumption 
similar argument part discussion know complete network structures bsc bsc may obtain density bsc sc bsc vice sc versa change variable 
assume densities bsc dirichlet may case densities bsc dirichlet 
example variable domain suppose equal constant sc dirichlet exponents equal 
change variable equation sc dirichlet 
consequently dirichlet exponents constrained 
heckerman 
theorem show bs dirichlet complete sc belief network structure bsc density parameters joint space conditioned dirichlet distribution 
com sc result previous discussion see may obtain exponents ijk structures simply assessing dirichlet density joint space conditioned sc domains containing small number variables user may assess density directly 
larger domains assessment method notion equivalent sample size described winkler 
understand method xn denote set parameters joint space xn 
denote dirichlet density parameters follows xn sc xn xn xn expectation xn respect xn equal user prior proba sc bility 
xn instance sc domain observed 
formula expectation xn dirichlet density equation obtain 
xn sc xn xn xn determine needed exponents having user assess 
xn sc user assess joint probability distribution xn constructing belief network sc call network user prior sc belief network 
constant simple interpretation equivalent number cases user seen completely ignorant domain 
winkler shows user may trained assess bde metric prior belief network constant difficult show exponents ijk determined relation ijk xi sc see heckerman derivation 
constraint simple interpretation terms equivalent sample sizes 
ij ri ijk equivalent sample size parameter set ij glance contradiction asking user construct belief network may contain assertions independence assertion true 
assertions independence prior network refer independencies case observed 
contrast assertion full dependence refers long run fractions 
parameters xi observed jth instance equation see ij equivalent sample size ij just equivalent sample size times probability see substituting equation bd metric equation obtain bde metric score equivalent metric belief networks 
note acts gain control learning smaller value quickly bde metric favor network structures differ prior belief network structure 
example bx denote structures points points respectively variable domain 
suppose user prior net gives joint distribution 
bde metric observe database containing single case true 















required bde metric exhibits property score equivalence 
causal networks people knowledge causal relationships variables addition knowledge conditional independence 
causal knowledge stronger conditional independence knowledge allows derive beliefs domain intervene 
example believe smoking causes lung cancer 
belief infer smoking decrease chances getting lung cancer 
contrast believe statistical correlation smoking lung cancer gene causes desire smoke lung note buntine derivation special case bde metric obtained letting uniform noted property score equivalence 
ch special case bd metric ijk set yielding uniform dirichlet distribution density ij 
special case exhibit property score equivalence 
cancer infer giving cigarettes decrease chances getting lung cancer 
causal networks described example spirtes 
pearl verma heckerman shachter represent causal relationships variables 
particular causal network belief network asserted nonroot node caused parents 
precise meaning cause effect important discussion 
interested reader consult previous 
formally define causal network pair cs cp cs causal network structure cp set probability distributions corresponding structure 
event ce belief network structure include event assertion nonroot node caused parents 
contrast case belief networks appropriate require properties event equivalence score equivalence 
example variable domain causal network cs points causal network cs points represent assertion dependent 
network cs addition represents assertion causes network cs represents assertion causes events ce ce equal 
reasonable assume events events associated different causal network structures mutually exclusive 
consequences event equivalence discussed section apply causal networks 
particular exponents ijk theoretical constraints may bd metric score causal networks 
practical reasons useful constrain parameters ijk describe approach 
asses prior network 
variable xi instance prior network allow user specify equivalent sample size ij assessments compute equivalent sample sizes ij network structures expansion contraction procedure 
method appealing theoretical properties computationally expensive 
ch specialization bd metric set ijk efficient ignores prior network 
explored simple approach nij equal constant 
call metric metric stands uniform equivalent sample sizes 
course bde metric may score causal networks 
note context causal networks assumption parameter modularity assumption appealing justification 
imagine causal mechanism responsible interaction node parents 
assumption parameter modularity follows assumption causal mechanisms independent 
limitations bde metric consider scoring belief networks 
method determining exponents ijk simple sense simple 
may case user knowledge variables assess different equivalent sample sizes ij different values network causal doing represents problem 
seen section doing case belief networks requires abandon score equivalence dirichlet priors ability score possible network structures 
believe important retain score equivalence possible 
furthermore computationally expensive abandon dirichlet assumption 
promise avoiding third assumption 
non score isomorphic metric accommodates variable dependent sample sizes score element equivalence class isomorphic network structures 
need method designating exactly network structure equivalence class network scored 
simple approach ask user specify complete ordering domain variables 
example ordering variable domain equivalence class corresponding conditional independence represented network structure score structure 
subtle example ordering equivalence class corresponding conditional independence represented network structures equivalence class occurs network structure 
priors network structures complete information needed compute metrics user assess prior probabilities network structures 
assessments logically independent assessment prior network limit equivalent sample size approach infinity prior network structure receive prior probability 
structures closely resemble prior network tend higher prior probabilities 
propose parametric formula prior network 
network structure bs denote number nodes symmetric difference parents xi bs parents xi prior network structure 
bs prior network differ arcs penalize bs constant factor arc set normalization constant 
formula simple requires assessment single constant 
imagine generalizing formula punishing different arc differences different weights suggested buntine 
parametric form satisfy score equivalence may recover property described previous section designating event equivalence class network structure scored 
evaluation section evaluate bde metric node alarm network domain icu management beinlich 
evaluations start network call gold standard network 
generate database network monte carlo technique 
scoring metrics local search procedure similar described lam bacchus identify high scoring network structure 
database prior knowledge populate probabilities new network called learned network 
particular set probability xi posterior mean ijk database 
compare joint distributions gold standard learned networks 
cross entropy measure comparison 
particular xi xn xi 
xn denote probability instance obtained gold standard learned networks respectively 
measure accuracy learning algorithm cross entropy xn xi xn log xi xn xi xn lower value cross entropy accurate algorithm 
heckerman 
describe method computing cross entropy networks network structures 
experiments construct prior networks adding noise gold standard network 
control amount noise parameter 
prior network identical network increases prior network diverges gold standard network 
large prior network gold standard networks unrelated 
generate prior network add arcs gold standard network creating network structure bs 
add arc copy probabilities bp maintain joint probability distribution perturb conditional probability bp noise 
particular convert probability log odds add sample normal distribution mean zero standard deviation convert result back probability renormalize probabilities 
create network structure bs deleting arcs reversing arcs reversal may create directed cycle case reversal done 
perform inference joint distribution determined network bs bp populate conditional probabilities network bs bp return prior network 
shows cross entropy learned networks respect alarm network inverse learning accuracy function deviation gold standard network user equivalent sample size bde metric 
experiment case databases generated alarm network 
value cross entropy values shown represent average learning instances instance different database prior network 
databases prior networks generated value values prior parameter function take reasonable values extremes 
reflecting complete ignorance network structures receive prior probability 
limit approaches infinity reflecting complete confidence prior network structure receives prior probability 
qualitative behavior curve reasonable 
prior network identical alarm network learning accuracy increased equivalent sample size increased 
learning accuracy decreased bde metric section medical informatics university pittsburgh 
cross entropy cooper herskovits cooper herskovits 

machine learning 
dawid lauritzen dawid lauritzen 

annals statistics 
geiger heckerman geiger heckerman 

proceedings 
heckerman heckerman geiger chickering 
march 
technical report msr tr microsoft 
evaluation results 
prior network deviated gold standard network demonstrating expected result prior knowledge useful 
addition value associated optimal accuracy 
result surprising 
large deviation true values parameters priors degrade performance 
hand small metric ignoring useful prior knowledge 
speculate results kind calibrate users assessment quantitative results encouraging 
provide scale cross entropy alarm domain note cross entropy alarm network empty network domain network variables independent marginal probabilities determined alarm network 
case database prior network significant amount noise cross entropy bde metric optimum value 
acknowledgments jack breese wray buntine greg cooper steffen lauritzen anonymous reviewers useful suggestions 
heckerman shachter heckerman shachter 

proceedings 
lam bacchus lam bacchus 

proceedings ninth conference uncertainty artificial intelligence washington dc pages 
morgan kaufmann 
madigan madigan 

journal american statistical association appear 
pearl verma pearl verma 

knowledge representation reasoning proceedings second international conference pages 
morgan kaufmann new york 
spiegelhalter spiegelhalter dawid lauritzen cowell 

statistical science 
spirtes spirtes glymour scheines 

springer verlag new york 
winkler winkler 

american statistical association journal 
york york 

phd thesis department statistics university washington seattle 
beinlich beinlich suermondt chavez cooper 

proceedings second european conference artificial intelligence medicine london 
springer verlag berlin 
cooper herskovits cooper herskovits 
january 
technical report smi 
