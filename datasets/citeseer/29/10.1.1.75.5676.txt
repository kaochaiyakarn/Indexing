machine learning kluwer academic publishers boston 
manufactured netherlands 
symbolic neural learning algorithms experimental comparison jude shavlik shavlik cs wisc edu computer sciences department university wisconsin west dayton street madison wi raymond mooney mooney cs utexas edu department computer sciences taylor hall university texas austin tx geoffrey towell towell cs wisc edu computer sciences department university wisconsin madison wi editor quinlan despite fact symbolic neural network connectionist learning algorithms address problem learning classified examples little known regarding comparative strengths weaknesses 
experiments comparing id symbolic learning algorithm perception backpropagation neural learning algorithms performed large real world data sets 
backpropagation performs slightly better algorithms terms classification accuracy new examples takes longer train 
experimental results suggest backpropagation significantly better data sets containing numerical data 
neural network literature problem frequently referred supervised associative learning 
shavlik mooney towell associative learning symbolic neural systems generally require input classified examples represented feature vectors 
addition performance type learning systems usually evaluated testing ability correctly classify novel examples 
symbolic machine learning numerous algorithms developed perform associative learning 
systems exist learning decision trees quinlan logical concept definitions mitchell michalski examples classify subsequent examples :10.1.1.167.3624
algorithms tested problems ranging soybean disease diagnosis michalski classifying chess games quinlan 
neural networks algorithms developed training network respond correctly set examples appropriately modifying connection weights rosenblatt rumelhart hinton williams hinton sejnowski 
training network classify novel examples 
neural learning algorithms tested problems ranging converting text speech sejnowski rosenberg dietterich evaluating moves backgammon tesauro sejnowski 
algorithms tested problems ranging soybean disease diagnosis michalski classifying chess games quinlan 
neural networks algorithms developed training network respond correctly set examples appropriately modifying connection weights rosenblatt rumelhart hinton williams hinton sejnowski 
training network classify novel examples 
neural learning algorithms tested problems ranging converting text speech sejnowski rosenberg dietterich evaluating moves backgammon tesauro sejnowski 
article presents results experiments comparing performance id symbolic learning algorithm quinlan perceptron rosenblatt backpropagation rumelhart neural algorithms :10.1.1.167.3624
systems tested large data sets previous symbolic connectionist experiments learning times accuracies novel examples measured 
cases backpropagation classification accuracy statistically significantly better id 
perform roughly equivalently 
cases backpropagation took longer train 
section briefly describes systems reasons choosing representatives 

id chose id simple widely symbolic algorithm learning examples 
extensively tested number large data sets quinlan catlett basis commercial rule induction systems 
addition id augmented techniques handling numerically valued features noisy data missing information quinlan :10.1.1.167.3624
experimental comparisons symbolic learning algorithms id generally performs better systems rendell cho 
id uses training data construct decision tree determining category example 
step new node added decision tree partitioning training examples value single informative attribute 
attribute chosen minimizes function comparing symbolic neural learning number values attribute number examples jth category ith value attribute total number examples si number examples ith value attribute number categories 
method terminates growth branch decision tree remaining attribute improves prediction statistically significant manner 
case leaf created labeled common category partition 
missing feature values handled techniques recommended quinlan see section details 
numerically valued features handled examining thresholds height split data partitions examples distinct splits numerical feature 
split treated information gain calculation split nominal feature quinlan :10.1.1.167.3624

perceptron known perception learning procedure incapable learning concepts linearly separable minsky papert 
despite performs quite large data sets test learning systems 
included perceptron neural network learning algorithms study example simple fast neural learning algorithm 
binary encodings natural representation neural learning algorithms 
binary encoding slightly improved classification performance id learning algorithms 
bit vectors usable systems helps standardize experimental setup 
id improved performance binary encoding may due factors 
feature values binary encoding eliminates gain criterion undesirable preference valued features quinlan :10.1.1.167.3624
second allows general branching decisions red vs non red requiring nodes branch values feature 
may help overcome irrelevant values problem cheng fayyad irani qian result general better decision trees 
binary encoded examples features id run time increased somewhat approach 

backpropagation takes long run technique value practical systems 
perceptron accuracy heart disease data declines slightly number examples increases 
attributed fact perceptron terminates decides data set linearly separable 
perceptron performance heart disease nettalk data improved having detects inseparable data set search hyperplane best separates training examples 
id tendency perform worse small training sets may related problem small disjuncts holte acker porter :10.1.1.53.9769
holte shown leaves decision tree disjuncts dnf rule cover small number training examples tend higher error rates cover large number comparing symbolic neural learning 
classification performance function amount training data 
examples 
evidence higher error rate due maximum generality bias symbolic induction systems 
quinlan presents thorough set experiments comparing various approaches 
version id uses method arguably best results experiments 
evaluating potential splitting feature system adjusts information gain distributing examples unknown shavlik mooney towell 
classification performance function amount feature category noise 
values possible values frequency quinlan :10.1.1.167.3624
considering partitioning training set particular attribute training example unknown value attribute fraction example assigned subset frequency corresponding value kononenko bratko 
comparing symbolic neural learning classifying new case unknown value attribute tested branches explored results combined reflect relative probabilities different outcomes quinlan :10.1.1.167.3624
experimental results confirm approach superior methods replacing unknown values common value discarding examples unknown values partitioning 
method leads higher accuracies novel examples drawback processing time grows exponentially function number missing feature values 
evaluating potential splitting feature system adjusts information gain distributing examples unknown shavlik mooney towell 
classification performance function amount feature category noise 
values possible values frequency quinlan :10.1.1.167.3624
considering partitioning training set particular attribute training example unknown value attribute fraction example assigned subset frequency corresponding value kononenko bratko 
comparing symbolic neural learning classifying new case unknown value attribute tested branches explored results combined reflect relative probabilities different outcomes quinlan :10.1.1.167.3624
experimental results confirm approach superior methods replacing unknown values common value discarding examples unknown values partitioning 
method leads higher accuracies novel examples drawback processing time grows exponentially function number missing feature values 
approach reducing id run time prevent splitting features create partition containing total example quinlan personal communication 
frequently decreases run time substantial amount tends reduce accuracy percentage points 
