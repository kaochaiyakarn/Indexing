stochastic dynamic programming factored craig representations richard dearden department computer science department computer science university toronto toronto canada cs toronto edu university british columbia vancouver bc canada dearden cs ubc ca goldszmidt computer science department stanford university stanford ca usa moises robotics stanford edu markov decision processes mdps proven popular models decision theoretic planning standard dynamic programming algorithms solving mdps rely explicit state specifications computations 
alleviate combinatorial problems associated methods propose new representational computational techniques mdps exploit certain types problem structure 
dynamic bayesian networks decision trees representing local families conditional probability distributions represent stochastic actions mdp decision tree representation rewards 
representation develop versions standard dynamic programming algorithms directly manipulate decision tree representations policies value functions 
generally obviates need state state computation aggregating states leaves trees requiring computations aggregate state 
key algorithms decision theoretic generalization classic regression analysis determine features relevant predicting expected value 
demonstrate method empirically planning problems parts report appeared preliminary form exploiting structure policy construction proc 
fourteenth international joint conf 
artificial intelligence ijcai montreal pp correlated action effects decision theoretic regression proc 
thirteenth conf 
uncertainty artificial intelligence uai providence pp 
author showing significant savings certain types domains 
identify certain classes problems technique fails perform suggest extensions related ideas may prove useful circumstances 
briefly describe approximation scheme approach 
keywords decision theoretic planning markov decision processes bayesian networks regression decision trees abstraction decision theoretic planning dtp attracted considerable amount ai researchers seek generalize types planning problems tackled computationally effective ways 
dtp primarily concerned problems sequential decision making conditions uncertainty exist multiple conflicting objectives desirability quantified 
markov decision processes mdps adopted model choice dtp problems provided underlying foundations reinforcement learning :10.1.1.34.3923:10.1.1.48.6957
mdps allow uncertainty effects actions modeling uncertain exogenous events presence multiple prioritized objectives solution nonterminating process oriented problems 
foundations basic computational techniques mdps understood certain cases directly dtp 
methods exploit dynamic programming principle allow mdps solved time polynomial size state action spaces planning problem 
unfortunately classical dynamic programming methods formulated require explicit state space enumeration 
ai planning systems solve mdps faced bellman called curse dimensionality number states grows exponentially number variables characterize planning domain 
impact feasibility specification solution large mdps 
curse dimensionality plagues dtp classical planning techniques 
methods developed instances circumvent problem 
classical planning typically specify actions goals explicitly underlying state space intensionally propositional variable representations 
instance strips representation action describes concisely transitions induced action large number states 
similarly classical planning techniques regression planning nonlinear planning exploit representations great effect requiring search implement shortest path dynamic programming techniques state space :10.1.1.18.4442:10.1.1.57.3126
intuitively methods aggregate states behave identically action sequence respect goal 
form uncertainty handled framework adopt specifically partial observability uncertain knowledge state system controlled 
partially observable mdps pomdps cases 
remarks pomdps article 
develop similar techniques solving certain classes large mdps 
describe representation actions stochastic effects uses bayesian networks decision trees represent required families conditional probability distributions provide type compact representation actions say strips affords deterministic settings 
decision trees represent reward functions 
representation lays bare exploits certain structural regularities transition reward functions 
describe algorithms representation compute value functions solve mdps generally requiring explicit enumeration state space 
regression classical planning focus attention variables particular action influence outcome action respect relevant variables 
addition policies value functions represented compactly decision trees structure inherent policy value function preserved large extent algorithmic operations 
certain assumptions show degree preserved structure maximal subject variable reordering trees 
decision theoretic regression key algorithms process call decision theoretic regression 
classical planning regression set weakest set conditions regr performing conditions regr ensures true 
key step backchaining subgoaling planner including commitment planners :10.1.1.18.4442:10.1.1.57.3126
sub goal regression new subgoal achievement assures simply 
decision theoretic regression generalizes process ways 
speak goal achievement mdps concern value associated certain conditions 
regress set conditions associated distinct value action 
decision theoretic regression set conditions action result new set conditions 
second stochastic actions rarely guarantee achievement particular condition producing conditions action applied lead specific target condition produce set conditions action regressed conditions true identical probability 
follows condition regressed set associated single value new conditions produced decision theoretic regression expected value actiona 
operation hand implement classical algorithms solving mdps value iteration modified policy iteration highly structured way 
structured versions algorithms cluster states stage computation estimated value optimal choice action 
partitioning state space regions represented decision trees test values specific variables 
computational advantage provided regression concept fundamental importance program synthesis verification 
approach value need computed region state 
state aggregation function approximation approach take solving large mdps specific state aggregation method 
types state aggregation techniques proposed states similar characteristics grouped 
methods reported instance vary states statically dynamically aggregated groupings states stay fixed change computation 
compact representations value functions proposed linear function representations neural networks 
techniques seek exploit regions uniformity value functions compact functions state features reflect value 
distinguished strict aggregation methods 
previous goal approximate solution large mdps 
proposal distinguished aggregation methods compact representations values functions major ways 
aggregations determined dynamically features easily extracted model 
sense intuitions underly approach closely aligned exploited classical planning 
states implicitly aggregated process abstraction removing certain variables state space description 
second methods inherently approximation techniques basic procedures produce exact solutions value functions 
describe modifications techniques allow approximate solutions constructed 
approaches state aggregation bear similarity method 
model minimization approach givan dean :10.1.1.34.3923
notion automaton minimization extended mdps analyze abstraction techniques 
closely related specific model propose current dietterich flann 
apply regression methods solution mdps consider problem context reinforcement learning addition 
original proposal restricted mdps goal regions deterministic actions represented strips operators rendering true goal regression techniques directly applicable 
extended allow stochastic actions providing stochastic generalization goal regression 
discuss models detail section 
outline section describe basic mdp model various concepts solution mdps classical algorithms solving mdps 
section define particular compact representation mdp dynamic bayesian networks special form bayesian network represent dependence variables accurately produce solutions identical standard state counterparts may optimal 
occurrence actions 
addition decision trees represent conditional probability matrices quantifying network exploit context specific independence independence particular variable assignment 
note representation somewhat related probabilistic variants strips operators introduced augmented 
describe decision tree representation reward functions value functions policies 
section describe basic decision theoretic regression operator particular treestructured value function action network regresses value function action produce new value function 
operation hand develop structured analogs classical mdp algorithms value policy iteration 
section empirical analysis methods suggest types problems possible approaches may help 
section describe extension algorithms section deal correlations action effects 
briefly describe leverages structured methods described section provide approximate solutions structured mdps 
conclude section brief discussion related extends ideas describe promising directions research 
markov decision processes mdps viewed stochastic automata actions uncertain effects inducing stochastic transitions states precise state system known certain probability 
addition expected value certain course action function transitions induces allowing rewards associated different aspects problem goal proposition 
plans optimized fixed finite period time infinite horizon suitable modeling ongoing processes 
mdps ideal models decisiontheoretic planning problems discussion desirable features mdps perspective modeling dtp problems see 
section describe basic mdp model consider classical solution procedures 
primarily reasons presentation consider action costs formulation mdps 
associated states propositions 
general cost reward models easily incorporated framework 
furthermore restrict attention finite state action spaces 
assumption full observability despite uncertainty associated action effects planning plan executing agent observe exact outcome action taken knows precise state system time 
partially observable mdps pomdps computationally demanding fully observable mdps 
remarks application techniques pomdps article 
see detailed investigations type 
refer reader material mdps 
basic model markov decision process defined ri finite set states possible worlds ais finite set actions tis state transition function reward function 
state description system interest captures information system relevant problem hand 
typical planning applications state possible world truth assignment logical system described 
agent control state system extent cause state transitions movement current state new state 
actions stochastic actual transition caused generally predicted certainty 
transition effects action state si probability distribution overs si sj probability performed 
denote quantity si sj 
require pr si sj sj psj spr si sj 
dynamics system controlled 
assume action performed state 
general models state different feasible action set crucial 
pr st st pr st states system passes actions performed correspond stages process 
system starts stage 
performed system 
fixed course action state system viewed random similarly denote action executed 
stages provide rough notion time mdps 
system markovian due nature transition function fact system fully observable means agent knows true state stage reached decisions solely knowledge 
stationary markovian policy course action adopted agent controlling system plays role plan classical planning 
agent adopting policy performs action finds states 
policies markovian sense action choice state depend previous system history stationary action choice depend stage decision problem 
problems consider optimal stationary markovian policies exist 
sense conditional universal plan specifying action model applicability conditions actions preconditions way fits framework 
prefer think actions action attempts agent execute possibly effect success state 
preconditions may useful restrict planning agent attention potentially useful actions viewed form heuristic guidance don bother considering attempting open locked door 
impact follows important ways 
perform possible circumstance 
agent policy system 
thought reactive number adopted measure value policy assume bounded real valued reward instantaneous reward agent receives occupying states 
general reward models possible introduce special complications algorithms 
common generalization random variable case expectation deterministic reward impact value policy calculations 
allows depend action taken model costs various actions 
keep presentation simple consider possibility development algorithms point minor adjustments account action costs appropriate points presentation methods 
take markov decision problem mdp specific optimality criterion 
abbreviation mdp refer specific problem process optimality criterion process context distinguishing precise meaning 
optimality criteria vary horizon process controlled manner reward valued 
focus discounted infinite horizon problems current value reward discounted 
allows simpler computational methods discounted total reward finite 
infinite horizon model important planning problem proceed infinite number stages horizon usually indefinite bounded loosely 
furthermore solving infinite horizon problem computationally tractable solving long finite horizon problem 
discounting certain attractive features encouraging plans achieve goals quickly motivated economic grounds justified modeling expected total reward setting process probability terminating agent breaks stage 
refer discussion mdps different optimality criteria 
value policy optimality criterion simply expected sum discounted rewards obtained executing value depends state process begins denote value states 
policy optimal alls sand policies 
guaranteed optimal stationary policies exist setting 
optimal value optimal policy take problem decision theoretic planning determining optimal policy approximately optimal satisficing policy 
similarly rewards depend transition take si sj expectations allow reward depend actions discuss 
methods apply directly finite horizon problems suitable modification computation average optimal policies 
pursue 
vn si si spr si si sj vn sj solution methods policy evaluation successive approximation si si spr si si sj sj fixed policy computed algorithm know successive approximation 
proceed constructing sequence stage go value functions vn 
si expected sum discounted rewards received executed starting 
si si recursively compute asn vn convergence rate error bounded 
note right hand side equation determines contraction operator algorithm converges starting equal tov vis fixed point operator 
compute value formula due howard vn si si max afx sj spr si sj vn sj find value states solving set linear 
value iteration solving mdp refer problem constructing optimal policy 
value iteration simple iterative approximation algorithm optimal policy construction proceeds successive approximation stage choose action maximizes right hand side equation computation known bellman backup 
sequence value vik jsj jaj produced value iteration converges linearly tov 
iteration value iteration computation time number iterations polynomial 
maximize right hand side equation form optimal policy value 
simple stopping criterion requires termination supremum norm 
ensures resulting value function vi optimal state induced policy optimal value ofv 
stopping criterion uses span maxfx xg xg 
similar bounds quality induced policy provided 
vn max concept useful aq function 
arbitrary value define intuitively qva denotes value performing acting manner 
particular function defined respect tov function defined respect 
manner rewrite equation policy iteration policy iteration optimal policy construction algorithm produces exact policies value functions 
proceeds follows 
policy ons si spr si sj sj si 
solving set equations equation actiona qva si si spr si sj sj qva alls si si si 
return algorithm begins arbitrary policy alternates repeatedly step evaluation phase step current policy evaluated improvement phase step local improvements policy 
continues local policy improvement possible 
algorithm converges quadratically practice tends relatively iterations compared refer detailed discussion refined stopping criteria error bounds value iteration assurances optimality optimality provided techniques action elimination 
value iteration 
evaluation step requires jsj computation naive methods solving system equations improvement step iso jsj jaj 
policy evaluation step implemented successive approximation solving linear system directly 
modified policy iteration policy iteration tends converge faster practice value iteration cost iteration high due system linear equations solved 
puterman shin observed exact value current policy typically needed check improvement 
modified policy iteration algorithm exactly policy iteration evaluation phase uses usually small number successive approximation steps exact solution method 
algorithm tends extremely practice tuned policy iteration value iteration special cases 
acceptable formal criteria exist choosing number successive approximation steps invoke quantity generally determined empirically 
bayesian network representations mdps mdp framework provides suitable semantic conceptual foundation dtp problems direct representation planning problems mdps direct implementation dynamic programming algorithms solve proves problematic due size state spaces planning problems 
generally planning problems described terms set domain features sufficient characterize state system 
unfortunately state spaces grow exponentially number features interest 
bellman called curse dimensionality specification mdp particular specification system dynamics reward function computational methods solve mdps tailored resolve difficulty 
section focus representation mdps factored feature problems 
section describe exploit proposed representations computationally dynamic programming algorithms 
illustrate representational methodology example feature stochastic sequential decision problem 
suppose robot charged task going caf buy coffee delivering coffee owner office 
may rain way case robot get wet umbrella 
umbrella kept office robot able move locations caf office buy coffee deliver hand coffee owner pick umbrella suitable conditions 
boolean characterize domain robot located office robot office caf robot wet robot umbrella raining hcr robot coffee possession hco robot owner coffee actions go moves robot opposite current location buy coffee provides robot coffee caf delc robot hands coffee user office robot picks umbrella office effect actions may noisy certain probability may intended prescribed effect 
bayesian network action representation long recognized planning community explicitly specifying effects actions terms state transitions problematic 
intuition underlying earliest representational mechanisms reasoning action planning situation calculus strips important examples actions compactly naturally specified describing effects state variables :10.1.1.85.5082:10.1.1.85.5082
example strips action representation state transitions induced actions represented implicitly describing effects actions features change value action executed 
factored representations compact individual actions affect relatively features effects exhibit certain regularities 
deal stochastic actions extend intuitions somewhat 
stating value variable takes action performed provide distribution possible values variable take conditional properties state action performed 
val val val xn exploit potential independence action effects regularities effects action performed different states adopt dynamic bayesian networks representation scheme 
note representations possible stochastic strips rules described 
see bayesian network methodology offers certain advantages 
basic graphical model formally assume system state characterized finite set random possible values take 
propositional boolean variables examples take values true false 
possible states system simply possible assignments values variables fx xng finite domain val xi hco hcr hco hcr ot ot hco hcr hcr time time matrix tree representation action network delc cpts shown just random denote state system denote value taken state timet 
example variable value depending robot coffee decision process 
bayesian network representational framework compactly representing probability distribution factored form 
networks typically represent atemporal problem domains apply techniques capture temporal distributions effects stochastic actions 
formally bayes net directed acyclic graph vertices corresponding random variables edge variables indicating direct probabilistic dependency 
network constructed reflects implicit independencies variables 
network quantified specifying probability distribution variable vertex conditioned possible values immediate parents graph 
addition network include marginal vertex parents 
independence assumptions defined graph quantification defines unique joint distribution variables network 
probability event space computed algorithms exploit independencies represented graph structure 
refer pearl details 
dynamic bayesian networks dbns represent transition probabilities associated specific action illustrated 
nodes network correspond state sets representing state system action performed representing state action executed xt arcs nodes represent direct probabilistic causal influence corresponding variables action question 
arcs directed pre action variables post action variables call diachronic arcs post action variables post action variables call synchronic arcs 
note network acyclic 
network represents effects action delc deliver coffee 
see effect action depends directly variables ot 
xt xt network quantified providing family conditional probability distributions post action xt xt xt precisely parents predecessors parents sets xt occur occur variables 
variables specify probability xt pr xt yt xt pr xt xt ijx 
family distributions usually referred conditional probability table cpt xjx tot xt distributions represented tabular form see 
write cpt xi denote family 
letx xt xt resp xt denotes resp 
instantiation semantics family conditional distributions words state performed parents furthermore state variables xt known xt iis independent variables 
illustrates points simple case delc action synchronic arcs 
family conditional distributions table instantiation variables ot probability provided 
cpt explained observing owner coffee prior action coffee action robot coffee office successfully hand coffee probability robot coffee wrong location cause owner coffee 
notice representation allows specify conditional effects stochastic action effect action specific variable vary conditions pre action state 
effect action wet shown especially simple robot wet resp 
dry wet resp 
dry action performed 
variable said persist action delc 
effects captured similar persistence relations shown 
effect delc hcr explained follows robot attempts delc isn example synchronic arcs see example occur 
boolean variables adopt usual convention specifying probability truth probability falsity minus value 
ot hcr hco hco hco hco hcr hcr pr wt ut rt ot jst time time example pr wt jst modified action network delc synchronic arc office chance take coffee robot office lose coffee certainty 
chance user getting coffee chance user getting coffee attributed 
synchronic arcs action effect state variables independent knowledge 
particular pr wt jst pr ut jst pr rt jst pr ot jst pr jst pr jst furthermore terms right rely parent variables timet pr wt wt pr wt 
easily determine state transition probabilities compact specification provided dbn 
action networks synchronic arcs calculation transition probabilities complicated slightly 
shows variant delc action synchronic arc hcr hco indicates dependence robot losing coffee owner getting coffee robot office 
specifically probability robot loses coffee depends owner successfully accepts coffee owner gets coffee robot loses owner coffee robot loses coffee probability 
synchronic dependencies reflect correlations action effect different variables 
independence variables hold dbns synchronic arcs 
determin ing probabilityof resulting state requires simple example application chain rule 
pr wt ut rt ot jst pr wt jst pr ut jst pr rt jst pr ot jst pr st pr jst pr jst pr st pr jst example write joint distribution overt variables computed slightly modified version equation notice variables hcr hco correlated remaining independencies allow computation factored respect variables 
observations representation 

normal bayes nets dbns provide marginal distribution pre action variables 
solving fully observable mdps concerned prediction resulting state distribution action knowledge current state 
action network provides schematic representation distributions instantiating variables represent straightforward computation 

specify action network action 
way action network seen compact specification transition matrix action 
convenient provide single network choice action represented variable distributions variables conditioned action node 
type representation common influence diagrams compact set individual networks action example variable value persists actions see discussion relative advantages approaches :10.1.1.41.4620
consider single network representation 

markov property need specify relationship xt knowledge irrelevant prediction values variables 
furthermore stationarity allows specify dynamics schematically dbn action characterizing effects 

typically dbn representation action considerably smaller corresponding tran si sj sition matrix 
example system states transition matrix note rationale relies basic semantics bayesian networks 
involves simple table lookup multiplication presence synchronic arcs notwithstanding 
requires specification parameters 
dbn requires specification parameters parameters 
see suitable representations cpts dbns compact 
worst case maximally connected dbn require number parameters transition matrix 
effects actions exhibit certain regularities effect variable wide variety circumstances effects subsets variables independent dbns generally compact 
see detailed discussion point 
representation augmented cpt representations described compares favorably probabilistic variants strips operators respect representation size 

certain sense dbn representation seen fall prey frame problem specify explicitly variable intuitively unaffected action persists value :10.1.1.85.5082:10.1.1.85.5082
instance arc corresponding cpt required infer value delc executed 
hard allow specification action effects focus variables change leaving distributions unaffected variables unspecified 
unspecified cpts filled default unspecified arcs dashed arcs added automatically 
frame problem dbns including aspects related variables change values conditions discussed detail :10.1.1.41.4620
structured representations conditional distributions dbn representation certain regularities transition function induced action 
specifically effect assignment values parents xt val xt val xt identical matter values taken state variables timet earlier 
representation allow exploit regularities distributions corresponding different assign 
view cpt dbn function mapping set value assignments parents val xi set distributions 
function traditionally represented tabular form explicitly lists assignment table corresponding distribution tables figures examples 
cases function compactly represented exploiting fact distribution identical elements 
instance cpt hco exploit fact probabilities sum remove entry row transition matrix row cpt done figures 
case transition matrix require entries dbns parameters respectively 
ments xt see distinct distributions mapped assignments hco parents 
suggests compact function representation mapping useful 
consider decision trees represent functions 
cpt action network represented decision tree interior nodes tree labeled parents edges tree labeled values parent variable edges emanate leaves tree labeled distributions 
semantics tree straightforward conditional parents distribution leaf node unique branch tree partial assignment parent variables consistent 
examples trees shown figures corresponding cpts 
mapping hco parents distributions hco represented compactly format usual tabular fashion 
structure tree corresponds intuitions regarding effects delc 
hco true remains true hco false true probability true hcr true remains false 
sense decision trees reflect rule structure action effects 
tree hcr relies synchronic parent 
focus decision trees familiarity ease manipulated 
furthermore quite compact describe actions 
representations may suitable compact certain circumstances 
cpts compactly represented rules decision lists boolean decision diagrams 
algorithms provide section designed exploit decision tree representation see fundamental difficulties developing similar algorithms exploit representations 
briefly point extensions described exploit decision diagram representations 
note representation viewed exploiting known context specific independence bayesian networks 
just independence variables knowledge subset variables determined graphical structure bayes net additional independence inferred certain assignments subset variables specific context 
algorithms detecting context specific independencies cpt representations decision trees decision graphs described 
related notions 
note asymmetric representations conditional distributions influence diagrams proposed investigated 
adopt convention boolean variables left edges denote right edges denote 
node synchronic parents superscripts interior node labels decision tree cpt nodes understood refer variables timet nott 
deterministic goal regression algorithms developed representations circumstances see discussion regression boolean decision diagrams 
decision theoretic techniques ideas developed section prove useful 
reward hco rew hco hco reward tree coffee example reward representation reward functions represented similarly compact fashion 
specify vector reward exploit fact reward generally determined subset system features 
represent dependence reward specific state features diagram shown reward node diamond depends values variables wand hco 
matrix represents reward function values taken variables 
see best states owner coffee robot dry worst states variables take opposite values 
note preference states robot wet owner coffee robot stays dry owner coffee delivering coffee higher priority objective robot staying dry 
reward node example related value nodes influence diagrams 
influence diagrams nodes generally represent long term value represent immediate reward note assume stationarity reward process 
cases independence reward certain state variables exploited 
influence diagrams considered reward nodes combined function summation determine value see 
action costs need modeled reward node representing chosen action included influence diagrams separate reward function specified action just specified distinct dbn action capture process dynamics 
cpts actions conditional reward function represented decision tree 
example shown decision tree compact full table instances decision tree representation considerably compact 
representation reward functions compact reward function comprised number independent components values combined simple function determine reward 
ideas common study multi attribute utility theory 
example reward function broken additive independent components component determines sub reward determined hco hco hco determines hco hco delc hcr hcr delc go go go key go examples policy tree value tree sub reward forw ifw 
reward state determined summing state 
number component reward functions specification reward terms components combination function considerably compact 
permit number independent decision trees specified algorithms exploit inherent reward specification 
simply combine component functions single decision tree representing true global reward function 
best exploit mdps general open question received attention 
discussion issues see :10.1.1.159.1021
value function policy representation clear value functions policies represented decision trees compact function representations 
exploit fact value optimal action choice may depend certain state variables may depend certain variables variables take specific values 
algorithms develop section construct tree structured representations value functions policies 
value trees policy trees internal nodes labeled state variables edges labeled corresponding variable values 
leaves value trees labeled real values denoting value state consistent labeling corresponding branch 
leaves policy trees labeled actions denoting action performed state consistent labeling corresponding branch 
tree representations policies reinforcement learning somewhat different fashion 
examples policy value tree 
implementation decision theoretic regression structured dynamic programming algorithms described section allow trees sophisticated case multi valued variables 
tree splits variable domain values require main split subsets values subset labeling edge directed corresponding interior node 
way distinctions values say fx domain val important value function policy prediction distinction andx forced create distinct subtrees 
ease presentation describe algorithms splits interior node tree exhaustive 
value function policy trees understood effecting form state aggregation state satisfying conditions labeling particular branch tree assigned value action choice 
particular form aggregation viewed state space abstraction state generally ignoring certain features value predicting say value optimal action choice state 
clear similar remarks applied dbn action representation states similar dynamics specific action grouped decision tree reward representation 
categorizing types abstraction dimensions described methods nonuniform abstraction different features ignored different parts state space 
particular decision trees capture conditional form relevance certain variables deemed relevant value function prediction certain conditions irrelevant 
compared linear function approximators neural network representations value functions representations aggregate states piecewise constant fashion 
see abstraction scheme classified adaptive aggregation states varies time algorithms progress 
main algorithm implements exact abstraction process states aggregated agree exactly quantity represented value reward optimal action transition distribution 
see section approximate variant abstraction method 
decision theoretic regression decision tree representations described previous section provide means representing value functions policies compactly straightforward table representations 
particular tree constructed represent optimal value function policy number internal nodes labeled state variables polynomial number variables representation exponentially smaller corresponding tabular representation 
structure tree say value function oracle imagine partitioning state space states structure tree performing dynamic programming assume value iteration reduced state space 
case dynamic programming iteration require bellman backup state number backups polynomial function logarithm number states 
unfortunately true value function compactly represented decision tree regions state space intermediate value function generated value iteration constant need match optimal value function 
furthermore don usually access oracles provide suitable abstractions 
need algorithms example infer proper structure sequence value functions produced value iteration perform bellman backups state structure deduced 
similar ideas regular modified policy iteration 
section develop methods just 
techniques exploit structure inherent mdp explicit dbn decision tree representations system dynamics reward function 
specifically tree structured representation value derive algorithms produce tree structured representations functions respect tov value function obtained performing bellman backup respect tov value function obtained successive respect fixed policy represented decision tree greedy policy respect tov 
algorithms infer varying degrees appropriate structure underlying value function policy performing decision theoretic calculations expected value calculations 
way operations computing expected value action computed state region state space leaf tree system state 
size trees substantially smaller size original state space computational savings substantial 
key operations mentioned computation respect value 
operation viewed decision theoretic analog regression described section 
section assume action networks describing domain contain synchronic arcs action effects distinct variables uncorrelated knowledge current state 
assumption valid domains including experimented may unrealistic 
primarily reasons exposition 
algorithms conceptually simple case correlations absent 
described section determining probability state variable certain value action performed straightforward 
action network synchronic arcs combined determine state independence combination requires simple synchronic arcs 
avoid having intuitions get lost details inference algorithms assumption uncorrelated effects 
discuss requisite decision theoretic regression algorithm correlations section 
section describe basic decision theoretic regression algorithm 
describe regression tree structured value function tree structured policy section maximization step needed bellman backups section 
section treats policy improvement step policy iteration puts pieces form tree structured version modified policy iteration 
section component algorithms implement structured value iteration 
st st st st st st st st st st tree simplified removal redundant nodes triangles denote subtrees 
sequel frequently standard operations decision trees tree simplification refers process removing redundant interior nodes tree meaningless splits 
interior nodes lying single labeled variable topmost node removed tree 
node removed retain exactly subtrees subtree consistent edge label topmost node see 
addition interior node splits tree subtrees identical interior node removed parent arc removed node redirected single copy subtree 
simplify denote tree resulting simplification 
appending trees appending refer structure oft 
new leaves added tree leaves oft labeled function label labels corresponding leaves int see 
primarily consider functions union information labels sum labels maximum labels 
denote append tree resulting process label combining function clear context 
denote append tree obtained leaf oft 
words append denotes tree branches partition state space determined intersection partitions induced byt 
usually assume resulting trees simplified explicitly mentioning fact 
merging trees merging set tng refer process producing single tree distinctions occurring trees leaves labeled function labels corresponding leaves original tree 
accomplished repeated appending successive trees merge earlier trees sequence append ordering result semantically equivalent result assuming label combination function associative commutative 
refer resulting tree merge ft tng 
usually assume resulting trees simplified 
tree simplification slightly involved multivalued variables allowed split 
case certain variable may legitimately appear branch tree continuous splits classification regression trees 
append qva si si spr si sj sj appending oft labels combined functionf 
note resulting tree simplified 
regression single action key component dynamic programming algorithms computation expected value performing respect value 
recalling equation notice computation divided phases computation expected value psj spr si sj sj discounting value addition immediate si 
represented compactly tree produce compact tree structured representation tree qva 
exploiting structure reward function tree structure action network fora structure tree 
intuitively value states reward expected discounted value 
identical reward verified easily examining tree 
focus condition 
recall branches tree correspond regions state space constant 
identical expected value respect states transition constant region ofv probability 
equivalent saying thata executed state conditions labeling tree true identical probability 
tree qva distinguish conditions branch tree true differing odds 
conditions determined examining conditions impact tree varies turn determined examining action network fora specifically tree xi 
illustrate intuitions example describing algorithm detail 
consider domain boolean reward function sufficient condition necessary condition 
reward action network tree simple action network reward function 

influence high probability true high probability true unaffected true performed 
assume reward tree simply tree shown represented compactly tree leaves table entries 
illustrates tree qv constructed 
building tree requires delineate conditions distinct expected value respect tov 
value stages go depends truth ofz expected value stage go depends conditions influence probability true false performed 
conditions directly network actiona specifically tree representing cpt 
action network tells post action probability influenced pre action truth values independent true 
branch tree corresponds set conditions state stage go lead fixed probability regions state space determined tree denoted ptree probability tree 
view regressing tree obtain conditions stage go identical effects respect conditions relevant prediction ofv 
leaf ptree labeled course dictates distribution branches tree 
compute expected value performing conditions labeling branch ptree shown 
example false true probability false probability expected value dictated conditions 
denote value tree obtained converting distributions branches expected values various intermediate trees constructed regression tree obtain tree shown 
tree tree representation ofv ptree denotes probability making variable mentioned tree true zero stages performed stage go denotes undiscounted expected value associated stage go tree tree representation ofq obtained discounting adding reward function structured tree 
zy zy zy zy zy zy ptree discounted state aggregation induced regression tree actiona 
respect tov 
ais immediate reward plus discounted expected value final tree obtained applying discount factor leaf appending reward tree resulting tree addition combine leaves 
example tree final tree illustrated grow size tree appended 
general step cause growth tree 
notice algorithm unchanged action costs involved reward function easily accommodated construction tree 
slightly direct view state aggregation induced abstraction mechanism shown 
working left see partitioning state space induced series operations described 
left original value depends 
ptree viewed new state space region partitioning contains tree partial completion regression tree obtain tree tree ptree version ptree ptree tree 
states identical probability reaching different regions ofv actiona suggested 
regions ptree state expected value discounted value shown 
final step involves adding immediate state obtain tree 
example causes state splitting variable partition reflects 
interesting aspects regression operation emerge value tree regressed structure 
imagine tree produced maximum expected value stage go state value iteration computation 
obtain tree perform steps shown 
order predict expected value predict probability making branch tree true 
know probability possibly false probability 
regress individual variables occurring tree piece appropriate conditions 
step regressed producing tree simply tree representing cpt leaves labeled distributions see 
step regress returns cpt dbn leaves labeled distributions 
tree appended tree leaf leaves certain true value irrelevant prediction ofv 
leaves appended tree labeled union original labels leaf labeled distribution see 
course tree simplified redundant node removal give ptree shown 
zy zy zy zy zy zy zy ptree ptree state aggregation induced construction ptree 
alternative perspective ptree 
regressing requires 
regression state partitioning shown left ptree sufficient determine probability half half ofv 
half determine probability false zy quarter ofv 
achieved 
conditions relevant overlayed top just created produce refined partitioning right ptree 
notice prediction relevant ptree states true move probability region ofv 
note leaf ptree induces joint product distribution 
semantics dbns assumption synchronic arcs ensures probabilities ofy true independent relevant aspects state 
specifically conditions labeling branches ptree sufficient ensure independence 
product distributions provides accurate prediction probability making conditions labeling branches tree come 
example ptree tells false andy true probability true probability 
conditions sufficient navigate tree 
expected value easily determined branch ptree giving rise shown 
discounting value tree adding immediate reward obtain final form tree shown 
example illustrates main intuitions underlying regression algorithm 
tree tree tree qva constructed stages 
important phase construction ptree qva provides tree leaves labeled set distributions subset variables occurring tree 
corresponding joint distribution simply product distribution ob general variables occur leaves 
distribution needed conditions tree qva input tree actiona output 
ptree qva tree returned tree assume tree simplified contains redundant nodes 

construct qva follows 
tree leaf joint distribution obtained product individual variable distributions 
pb tree prb 
branches tree prb probability conditions labeling branch andv value labeling bin tree 
re label 

discount qva discount factor multiply leaf label 

append simplify reward tree tree qva addition combination function 
resulting tree tree qva 
algorithm regress tree ptree qva tained individual distributions fix probability distribution branches tree values occurring value 
tree constructed regressing variables occurring tree turn piecing resulting trees 
trees individual variables taken directly dbn fora 
second phase algorithm simply involves joint leaf compute qva expected value tree respect tov 
final phase involves adding reward function discounted version qva obtain tree qva 
step may involve expanding qva appending tree doing simplification 
tree accepts input tree actiona returns tree qva detailed 
bulk tree structure produced call recursive algorithm tree produces ptree qva 
tree described 
algorithms described reasonably straightforward terms implemented slightly complicated ways minimize repeated operations tree traversals 
instance steps regress need wait qva completely constructed step 
opportunities optimization include exploiting shared substructure subtrees tree intertwining currently independent recursive calls step combining merging operations different leaves step leaves labeled distributions overlap support sets share computation 
note ways constructing ptree qva may prove useful 
follows discuss detail fine tuning tree manipulation algorithms 
soundness algorithm ensured result 
theorem ptree qva tree produced tree 
branch relevant value 
example distribution needed distribution concentrated 
specific performance different approaches tree manipulation closely tied domain specific features noise actions effects ordering variables input representation problem specific structure 
empirical experience obtained algorithms unable offer deep insights factors 
ptree qva vall fxi val prl xi input tree actiona output 
tree contains single leaf node necessarily labeled value return empty tree ptree qva 

variable labeling root tree 
tree structured cpt dbn fora leaves labeled distributions val 

val occurs positive probability distribution leaf subtree tree attached 
tree produced calling 

labeled merge xi vall union combine leaf labels distributions sets variables 
union combine leaf labels distributions label distribution 

return ptree qva 
algorithm tree fx si sj sjj xg prb ptree qva event determined edge labels assume leaf labeled xi ik 
joint product distribution induced pkg 
val corresponds unique branch tree 
assignment positive probability sufficient traverse tree unique leaf node 
letx val state 
words pr st prb 
proof prove part inductively depth tree tree 
base case tree depth tree consists single leaf labeled value immediate returns empty tree sufficient traverse tree leaf 
assume result holds value trees depth 
tree root labeled subtrees tree xi val 
subtrees tree xi depth inductive hypothesis return probability tree ptree xi capturing joint distribution variables tree xi assignment variables nonzero probability allows subtree tree xi traversed leaf 
ptree qva appending tree cpt trees ptree xi distribution constructed xt xi maintaining subsequent product resulting leaf 
resulting product distribution leaf tree xi may pr xt ct ct traverse root tree 
fact distribution include information ptree xi means event nonzero probability permits navigation subtree tree xi tree 
resulting product distribution xi traverse fact distributions ptree xi included product distribution irrelevant 
prove part ptree qva labeled distribution set fx corresponding 
construction conditions conditions labeling exactly tree cpt xi 
denote 
semantics dbn absence synchronic arcs ensures event event xt pr xt pr xt pr xt xt pr xt xt distribution corresponding product distribution independent exactly independent result follows 
follows immediately algorithm regress tree sound uses ptree qva determine values tree distribution compute expected value 
adding immediate reward discounted value straightforward 
corollary tree qva tree produced algorithm regress tree 
branch tree qva event determined edge labels assume leaf labeled 
si vb 
words tree qva accurately represents qva 
tree qv input tree tree output 
tree call regress tree produce tree qva 

leaf tree labeled bya append tree qva tree retaining leaf labels values tree qva deleting action labels tree 

return simplified resulting tree tree qv 
algorithm regress tree tree tree qv tree qa tree qv tree qa regression tree tree obtain tree qv tree tree qv 
regression policy algorithm regressing value function action generalized regress value function policy 
specifically value wish produce new value represents value executing policy step receiving terminal 
instance key step successive approximation policy evaluation vk 
tree tree tree tree representing goal produce tree tree qv 
algorithm regress tree tree conceptually similar regress tree key difference different actions performed different regions state space dictated tree 
algorithm detailed reflects straightforward approach producing regress tree tree 
algorithm regress tree applied action occurring tree resulting tree appended tree leaf see illustration 
tree suitably simplified 
quite obvious algorithm produces sound representation 
proposition tree qv tree produced algorithm regress tree tree 
tree qv event determined edge labels assume leaf labeled 
si vb 
words tree qv accurately state tree qk tree qk tree qk tree qa tree vk merging trees obtain improved value function tree vk 

previous subsection algorithm reasonably straightforward optimized somewhat efficiently certain conditions 
example tree specific conditions passed regress tree incrementally prune subtrees generated algorithm 
merging trees fundamental step value iteration policy iteration maximization step involved bellman backup 
value iteration vk computed merge ftree qk qk tree qk ais computed respect 
policy iteration improved policy constructed setting 
tree representations functions maximization steps implemented merging trees maximization leaf label combining function substituting maximizing action names values case policy improvement 
case value iteration assume actiona obtained regressing tree vk actiona 
tree vk obtained merging trees simplifying obtaining ag leaf label tree vk maximum corresponding labels trees ag 
illustrated 
mentioned earlier number ways merge operation implemented 
approach straightforward implemented repeated appending simplification 
possibility reorder trees common variable ordering ordering tree highest average value retained simultaneously traversing trees find maximizing values tree structure 
tree qva produce tree tree structured representation policy greedy respect tov 
achieved nearly identical tree qk tree qk tree qa merging trees 
set tree tree immediate reward 

termination compute tree vk regress tree vk tree 

return final tree tree tree vn 
input tree tree output tree tree vk tree qk tree obtain improved policy tree 
structured successive approximation ssa algorithm ion merging trees labeling leaves values label corresponding maximizing action names 
merged values replaced action labels trees may simplified subtree distinct maximizing value labels leaves may identical action labels leaves 
illustrates process 
note merged tree exists values replaced action labels simplified collapsing identical subtrees 
example illustrated leaves identical values intermediate merged value tree values produced different maximizing tree vk actions 
split relevant representation 
structured successive approximation tree structured implementations basic operations expected value computation maximization bellman backups implement standard dynamic programming algorithms structured fashion 
algorithm successive approximation algorithm fixed policy computes value 
assume tree structured policy represented tree 
structured successive approximation ssa described proceeds constructing sequence value functions represented tree tree vk 
initial value function simply reward function tree successive value approximations tree vk produced regressing tree 
termination determined standard criterion supremum norm span 
hco hco hco key hco hcr hcr hcr iterations ssa fixed policy delc 
implementation span 
determine convergence iteration 
merging tree vk function obtain tree vk vk 
span tree determined tree traversal find tree vk difference combination maximal minimal elements andm respectively 
termination threshold tree vk tree approximate value policy returned bounds usual formulae 
termination test clearly reproduces classical test tree structured setting follows immediately proposition tree accurately 
theorem tree tree produced ssa algorithm 
tree event determined edge labels assume leaf labeled 
wheref standard error introduced termination criterion 
words tree accurately 
note approximation error due nature successive approximation si decision tree representation 
usual error terms span supremum norm stopping criteria described section apply directly 
illustrates sequence value trees produced iterations ssa running example policy simplicity uniform application action delc state 
iterations estimated true value 
ssa algorithm discovered distinct values value function abstracted state space appropriately 
performs expected value computations backups iteration required standard state successive approximation algorithm 
thing notice immediately sequence value trees example structure stabilizes quickly 
quickly occurs depends number specific problem characteristics span semi norm generally early stopping policy accurate estimation value function 
fact value function distinct values minimal decision tree representation requires leaves 
representation decision graph add able represent value function concisely 
assured structure tree stabilizes fashion structure persists successive iterations structure change subsequent iteration 
tree vk theorem tree vk tree vk trees produced successive ssa 
tree vk identical structure identical possibly value labels leaves tree vk structure 
proof suppose tree vk tree vk structure 
algorithm regress tree vk tree produces structure tree vk structure tree vk regard values leaves 
tree vk identical structure differs tree vk leaf values algorithm produce tree vk identical structure tree vk 
simple inductive argument proves result 
significance result lies fact decision tree structure stabilizes ssa proceed exactly standard successive approximation 
specifically need recompute structure decision tree subsequent iterations 
ssa reuse decision tree simply perform expected value calculation leaf contrast standard calculation state additional overhead 
structured policy iteration policy iteration implemented way exploits tree structure simply piecing components described 
structured policy iteration spi detailed works alternating phases ssa structured policy improvement 
policy improvement implemented maximization merge described section action names replace values labeling leaves 
termination tested comparing tree tree see policies identical 
soundness component algorithms ensures spi algorithm produces trees accurately reflect optimal policy value function 
theorem tree tree trees produced spi algorithm 
tree event determined edge labels assume leaf labeled 
si wheref standard error introduced termination 
similarly policy represented tree isf optimal 
identical values collapsed subtrees structure fact simpler 
result ignores possibility 
rarely occur practice earliest stages dynamic programming 
collapsing ignored sequence iterations case practical import results remains 
usual policy iteration action chosen greedy policy policy improvement candidate actions action taken action retained 
input tree tree random initial output tree tree 

set tree tree 

repeat set tree tree 
compute tree ssa 
compute tree qva regress tree actiona 
merge trees tree qva obtain tree greedy policy 


return tree tree tree tree 
structured policy iteration spi algorithm delc hco hcr delc delc hco hcr delc hco hcr delc hco hcr delc go delc go delc delc go go delc go go go go sequence improved policies produced spi 
notice potential approximation error introduced policy iteration due fact policy evaluation obtained means successive approximation 
procedures action elimination ensure obtained policy fact optimal 
illustrates sequence policy trees produced spi running example initial policy uniformly delivers coffee state 
fifth policy tree created compared fourth identical final fourth tree optimal policy problem 
final value function shown 
notice final policy consists tree leaves showing spi capable discovering inherent structure optimal policies 
furthermore optimal value function consists distinct leaves 
policy evaluation improvement computation involves generally fewer expected value maximization computations required standard state version policy iteration 
optimality assured policy evaluation performed exactly solution corresponding linear equations 
conjecture means doing way exploits structure may possible explored possibility 
action delc essentially op domain robot doesnot coffee 
remains action selected sole action initial policy interest done 
delc incurred cost op included set actions op optimal branches delc occurs hcr oi 
hco hcr tree tree optimal value function produced spi 
input tree 
output tree tree 

set tree tree 

repeat regress tree vk actiona 
merge maximization obtain tree vk 
termination criterion holds tree vk tree vk 

set tree tree vk 

compute tree qva regress tree actiona 

merge trees tree qva obtain tree greedy policy 

return tree tree 
structured value iteration svi algorithm point modified policy iteration implemented exactly fashion 
change required spi algorithm perform fixed number steps ssa order evaluate policy implementing ssa convergence 
structured value iteration completeness describe structured value iteration svi algorithm 
results described spi svi plays important role approximation discuss section 
svi shown works repeatedly constructing trees current estimated value function performing maximization merge obtain improved estimate value function 
convergence termination criterion attained greedy policy respect value function produced maximization merge values replaced action names 
algorithm obviously sound soundness components 
related pointed number techniques solving large mdps approaches state aggregation warrant discussion due similarity method 
dietterich flann consider application regression methods mdps context reinforcement learning 
original proposal restricted mdps goal regions deterministic actions represented strips operators rendering true goal regression techniques directly applicable 
extend approach allow stochastic actions providing stochastic generalization goal regression 
key difference model deal exclusively goal problems allow general reward functions 
classify stochastic regression decision theoretic regression 
general motivation spirit proposal similar focuses different representations 
dietterich flann simply require operators actions inverted develop grid world navigation chess games examples deterministic regression 
stochastic case dietterich flann place emphasis algorithms manipulating rectangular regions grid worlds 
contrast approach deals general dbn decision tree representations discrete multi variable systems 
decision tree representation certain advantages multi variable domains see provides leverage approximation 
navigation domains take example region representation clearly superior offer little structure exploited decision tree 
approaches seen particular instances general approach regression mdps 
model minimization approach givan dean related model :10.1.1.34.3923
notion automaton minimization extended mdps analyze abstraction techniques 
technique viewed providing view type describe 
emphasis specific algorithms regression development theoretical framework abstraction methods proposed viewed minimization algorithms stochastic automata 
intuitively minimized automaton states aggregated agree certain property interest 
example solving mdp minimized discovering blocks states state block agrees reward agrees transition probabilities action respect block structure 
specifically action taken state block probability moving block necessarily state 
aggregate mdp formed way replacing states blocks solved optimally quickly due reduction state space size 
lee describe algorithm minimizing stochastic automata relies state space enumeration directed decision processes 
pointed dean givan mdp abstraction method viewed directly way explicitly minimizing mdp solving 
course minimization involve abstraction respect weaker properties value function differences 
spi algorithm viewed light dynamically constructs minimal model current estimate value function 
instance theorem pertaining stabilization value function structure interpreted confirming discovery minimal model respect value fixed policy 
analysis section describe empirical results structured dynamic programming algorithms 
focus spi algorithm show performance problems slightly different features attempt characterize types problems spi 
reasons poor performance suggest directions development mdp decomposition abstraction techniques 
describe series problems compare running time spi flat state modified policy iteration mpi 
comparisons number iterations policy evaluation termination criterion structured unstructured algorithms 
mpi algorithm optimized exploit sparseness transition matrices sparse matrix representations probability matrices sparseness avoid state enumeration zero probability states expected value computations 
sense compare spi conceptually best implementation general purpose unstructured algorithm 
describe size resulting structured policies value function terms number leaves corresponding tree contain compare state space size 
synthetic mdps best worst cases spi tested sets synthetic mdps designed illustrate performance best case worst case scenarios compared unstructured mpi enumerates state space explicitly 
worstcase behavior tested series mdps tree structured value function requires full tree 
specifically mdp designed optimal value function distinct value state 
mdp consists variables xn 
positive associated single state true reward zero assigned states 
problem discounted discount factor true preceding true effect preceding variables false 
dbn action illustrated schematically 
results obtained implementation written running linux pentium ii mhz mb memory 
xk xn xn xn xn xn dbn worst case examples worst case value function feature version mdp 
state space viewed binary number optimal policy requires choosing action set highest bit largest predecessors set 
sets predecessors false optimal policy induces path starting state enumerates binary numbers order number reached variables true 
discounting state corresponding value 
state unique value 
mdp represented dbns decision trees ino space value function space represented decision tree 
example optimal value function variables illustrated 
represents worst case spi enumerate entire state space exactly mpi pays additional overhead associated constructing trees doing expected value calculations 
compares performance spi algorithm unstructured mpi series worst case problems twelve variables states 
plot right see overhead associated spi algorithm causes algorithm run roughly times slower corresponding flat dynamic programming algorithm 
roughly constant isn say value function represented compactly way functional expression offers compact representation 
data corresponding problems described section spi web site www cs ubc ca spider dearden spi html 
running time mpi spi memory requirements mb mpi spi number variables number variables time space performance spi mpi worst case series overhead expected complete tree representing value function large corresponding flat tabular value function 
number operations required construct tree structured value function bounded size tree number operations required traverse partial trees building 
note number variables type problem increases space required compute optimal policy increases significantly greater rate spi seen plot right 
due fact tree representation value function requires storage interior nodes value tree 
overhead spi quite large worst case examples examples designed adversarial fashion illustrate worst case 
constant factor overhead computation time may serious price pay worst case arise practice long benefits best typical cases 
designed different set synthetic examples illustrate behavior 
best case examples designed optimal value function represented tree size linear number problem variables specifically variable occurs exactly tree 
best case sense problem variables play role final value function smaller 
best case mdp consists variables xn 
positive associated single state true reward zero assigned states worst case problems 
problem discounted discount factor preceding true effect succeeding false 
dbn slight separation log plots number variables increases due slight inefficiencies prototype implementation due decision theoretic regression approach 
discuss section 
course completely irrelevant variables spi recognize greater advantage unstructured algorithms discuss 
xk dbn best case examples best case value function feature version mdp 
action illustrated schematically 
state space viewed binary number optimal policy requires choosing action set highest bit largest predecessors set 
turns higher bits turn set subsequent actions 
state lowest false variable sequence ofn actions setting required reach goal value state isn 
value function contains distinct values represented tree nodes variable 
example optimal value function variables illustrated 
mdp represented dbns decision trees ino space value function space 
shows comparison spi mpi series examples ranging variables 
expected time space requirements mpi grow exponentially number variables unable solve variable problem due memory demands spi outperforms mpi considerably respect time space 
example spi solves variable problem seconds mpi requires seconds solve problem 
note largely inherent structure problems dictates differences performance spi mpi 
problems deterministic performance differences virtually identical noise various types added best worst case problems 
illustrate simple form noise similar phenomena arise noise models 
running time mpi spi memory requirements mb mpi spi number variables number variables time space performance spi mpi best case series spi noise spi noise spi noise mpi noise mpi noise mpi noise spi noise spi noise spi noise mpi noise mpi noise mpi noise log run time log run time number variables number variables solution time spi mpi various noise levels best case problems left worstcase problems right 
worst case problems ak noise level indicates ak chance set effect exactly 
best case problems ak noise level indicates ak chance false effect unchanged 
noisy variants problems structure value function optimal policy identical noise generally causes longer convergence times 
compares effect noise fork andk spi mpi best worst case examples various sizes 
noise types problems harder solve increase difficulty worse spi unstructured mpi 
problem algorithm states actions spi leaves time memory manufacturing mpi spi manufacturing mpi spi manufacturing mpi spi comparison spi mpi process planning problems process planning problems results illustrate extreme points spi performance 
test typical case performance ran spi set process planning problems synthetic manufacturing domain 
domains manufacturing problem product produced attaching finished component parts 
parts polished painted attached gluing 
types finished product high low quality policies producing quite different 
example high quality products hand painted requires skilled labour drill press supply bolts 
process expensive producing low quality products spray painted glued requiring spray gun glue 
reward function designed capture need high quality versus low quality products specifically high quality required little reward producing low quality product low quality necessary high quality production small added reward generally pay cost producing high quality 
shows comparison spi mpi problems full descriptions web site mentioned size ranging states states 
largest problems mpi unable run completion due memory limitations spi solves problem extrapolate roughly third time required mpi 
smallest problem structure value function permit tree construction overhead spi pay 
medium sized problem methods roughly comparable respect solution time 
spi ability ignore irrelevant variables new variables added problems impact optimal actions increasing state space size having trivial impact spi running time 
new boolean variable added problem effectively doubles running time mpi 
notice spi discovers considerable regularity value functions problems 
instance largest problem spi produces value function tree roughly distinct leaves discovering distinct values optimal value function states 
regularity strong impact memory requirements spi considerably lower mpi 
key feature problem class allows spi perform fact certain parts state space certain variables completely irrelevant prediction value 
instance high quality products required number variables availability glue irrelevant value function prediction 
similarly low quality products require availability skilled labour 
spi gains computational space advantages discovers conditional irrelevance effectively abstracting state space ignoring certain variables conditional variables certain values 
type irrelevance hold types domains 
example domain methods achieving various objectives chosen specific set circumstances variables relevant execution methods optimal ignored spi circumstances 
robot problems exogenous events run spi elaborate versions robot coffee delivery scenario 
report point certain problems spi dealing event 
specifically point conditions type irrelevance exploited spi exist 
problem domain robot move different locations pick delivery coffee pick deliver mail tidy lab 
penalties imposed states outstanding request coffee undelivered mail lab degrees 
problem designed irrelevant features example features relating specific objective relevant value function objective needs filled 
irrelevant details exploited spi 
feature problem problem especially difficult spi presence exogenous events 
exogenous events drive process state subset objectives needs satisfied 
objective satisfied need considered objective relevant relevant 
instance coffee request outstanding initial state coffee requests issued variables relevant coffee delivery needed predict value 
mdp optimal policy robot reach absorbing state class states objectives satisfied need considered 
realistic version problem contains exogenous events robot achieve objectives arise time 
example outstanding coffee request coffee request event occur small probability 
requires robot deliver coffee states coffee request outstanding 
similarly realistic model contains optimal policy tree smaller value function tree problems consider 
domain described detail web site mentioned 
exogenous events cause mail arrive lab 
problem domain variables valued states actions 
exogenous events spi runs completion seconds producing final value tree leaves policy tree leaves requires mb memory 
notice value tree contain significantly fewer entries flat tabular representation 
variables relevant circumstances 
comparison mpi runs seconds requires mb memory 
add exogenous events domain spi produces slightly larger value policy trees leaves respectively requires mb memory 
substantially larger exogenous events trees larger variables associated various tasks relevant states task active example state outstanding coffee requests variables relevant coffee delivery relevant predicting value exogenous coffee request event point task active speed accomplished depends status coffee delivery variables 
exogenous events tend trees larger rendering variables relevant possibilities 
worse spi takes nearly times long seconds run presence exogenous events 
final trees larger value trees produced early phases policy iteration get complex trees get larger earlier 
note exogenous events cause difficulty primarily cases variables relevant optimal performance task tasks arise time 
certain problem variables irrelevant example discover solving mdp irrelevant suboptimal unselected methods task achievement certain conditions spi discovers take advantage usual way presence exogenous events 
exogenous events increase degree stochastic connectedness mdp primary contributor difficulties faced spi domains 
fact complexity state description required predict value function optimal action choice depend variables relevant task arise 
suggests form task help alleviate difficulties return possibility section 
discussion loose sense spi viewed preserving structure value function representation possible subject certain restrictions 
example dbn tree representation value regression produce regressed tree distinctions necessary structure 
distinctions effect tree size dramatic example easy construct realistic scenarios effect considerably severe 
necessary depends large part specific values probabilities labeling leaves input structures 
spi produces output minimal size algorithm structure input trees abstraction decisions 
course trees restricts compactly certainly reward functions value functions cpts represented 
smallest tree representation value function may exponentially larger smallest representation technique ordered decision diagram handle disjunction effectively decision list set horn clauses 
variable ordering plays important role just small decision tree representation represent polynomial sized functions polynomially distinct values set variables compactly polynomial size potential blowup unavoidable 
furthermore representation universally compact instance functions best decision tree exponentially smaller best ordered decision diagram exponentially larger 
choice appropriate representation generally depend structure domain 
conjecture decision trees offer suitable choice problems 
basic conception decision theoretic regression applied representation simply needs algorithms manipulate representation region approach dietterich flann decision diagram model hoey 
empirical results suggest possible directions enhancing spi suggest conditions spi may may 
results process planning domain suggest overhead associated spi pay able eliminate equivalent roughly variables description value function 
tree representation value function times fewer leaves states system 
expect large problems reduction factor easy obtain 
note refer computation time spi offers dramatic savings memory usage time savings minimal 
time reduction estimate simple implementation described 
notice overhead worst case examples roughly constant glance 
closer examination overhead factor increasing slowly problem size 
suggests implementation tested certain inefficiencies 
suggests improved algorithms manipulating structured representations value functions policies greatly improve applicability spi 
facts confirmed subsequent extends spi algebraic decision diagrams adds 
improved structured representation implementation spudd tested problems described proven benefit decision theoretic regression substantial suggested 
example spudd solves variable worst case problem just seconds reducing worst case overhead factor factor better spi shows decrease overhead factor problem size 
largest process planning problem spudd structured value iteration modified policy iteration version modified policy iteration show better performance 
states spudd runs seconds times faster spi conjecture times faster mpi 
smaller process planning examples spi fails beat mpi spudd runs faster mpi seconds seconds compared seconds seconds mpi 
take results confirm intuition decision theoretic regression pay smaller variable reduction factors 
exogenous events addressed directly spi model 
methods deal 
form approximation 
discuss approximation spi framework simply note may suitable way handle specific problem exogenous events certain situations 
events reasonably small probability knowledge variables relevant corresponding objective small impact value 
approximation scheme outlined ignore distinctions 
problem arises robot domain largely multiple objectives may simultaneously active may active 
way deal problem treat different objectives separately construct optimal policies value functions individual objectives 
appropriate domain described objective independent contribution reward function 
deeper discussion model takes scope spi specifically 
note models mdps exploit type independence :10.1.1.159.1021
decision theoretic regression methods spi conjunction techniques great effect largely orthogonal 
extensions basic algorithm handling synchronic constraints key operation defined section decision theoretic regression operator regress tree justified assuming effects action different post action variables independent 
specifically joint distributions produced algorithm tree see product distributions 
independence assumptions valid dbns synchronic arcs arcs post action variables fact proof theorem 
unfortunately independence assumption longer holds action networks synchronic arcs 
network representing shown effect independent effect previous state 
causes distinct problems decision theoretic regression 
regression involves computing tree qva regress tree tree qva denotes value stages go stage go value 
problem occurs standard regression algorithm result fact pieces cpt trees dbn variables occuring tree 
synchronic constraints cpt trees xt xt yt zt action network synchronic arcs denoting correlation effects example value tree domain may post action variables occurring xt occurs cpt leading occurrence post action variables tree tree qva 
means tree qva longer represents value stages function state time refers properties state stages go 
fixed easily summing influence replacing dependence direct dependence parents 
second problem occurs constructing tree qva tree arises effect variables occuring tree may correlated 
example occur single branch tree probability attaining value branch actiona specified independent probabilities 
yt earlier algorithm lack synchronic arcs ensured independence keep track joint distribution explicitly constructing tree qva 
illustrate deals issues means simple examples describe intuitions needed extend decision theoretic regression algorithm deal synchronic constraints 
provide formal algorithm proof correctness refer detailed description necessary 
summing post action influences consider example value tree tree 
algorithm tree section produce tree qva obtain tree shown 
continuation algorithm lead legitimate tree involves post action 
revised algorithm establish dependence previous summing influence letting probability xt xt xt xt pr yt js yt val xt pr yt jx pr stree val xt pr yt jx yt pr stree yt yt stree stree summing influence post action parents partial tree obtained original algorithm effect conceptual view influence summed resulting partial tree 
depend directly parents 
specifically compute computation exploit tree structure follows 
replace tree representing cpt 
xt xt tree duplicated 
denote subtree replaced node corresponding xt stree xi 
stree single leftmost leaf node stree right subtree rooted 
cpt xi 
values positive probability merge trees stree xi copy atl 
specifically merge operation proceeds illustrated weight subtree stree xi xi labeling leaf cpt merge weighted subtrees addition combination operation 
resulting tree shown shows probability function previous state dependence removed completely 
completed easy see regression ofw pr yt xt val xt pr yt jx zt pr xt proceed section 
consider second example see illustrates order variables replaced tree crucial 
suppose just described depends effect correlated 
introduce tree zt appear assume appear branch cpt tree qva 
suppose depends 
case important substitute cpt substituting cpt 
compute pr yt zt xt val zt pr yt jz xt pr zt ordering variables replacement suppress mention parents zt cpt compute ordering problems 
parent 
subsequently replace occurrences approach reintroduce xt tree requiring wasted computation summing 
worse branch tree occurs computation valid independent element zt xt directly 
require regressed post action parents lie branch cpt variables cpt replaced trees order respects dependence post action variables ina network 
precisely post action ordering variables parent occurs ordering ordering goes direction synchronic arcs dbn fora 
post action variables cpt tree obtained recursive replacement post action variables replaced post action 
computing local joint distributions consider tree shown regression 
shows step regression regression 
second step regression appends cpt leaf partial tree occurs cpt cpt described previous subsection resulting tree tree structure tree shown 
xt xt yt xt yt yt pr wt xt yt yt yw yw yw yw val yt pr wt jy wt pr yt yw yw yw yw capturing correlations ptree qva stage regression final version ptree qva joint leaves alternative structure ptree qva 
yt wt leaf 
compute obtain ptree qva 
ing branches tree 
labeled leaves ptree qva yt wt computed yt wt yt edge prior state 
action network synchronic arcs distributions certain effects labeling structure proceed simply sum influence unfortunately provide accurate picture probabilityof attain probabilities correct sufficient deter independent 
synchronic arc means effect variables correlated knowl ensure correct expected values computed constructing qva maintain correlation construction ptree qva 
explicitly label leaves ptree qva joint yt wt shown 
note joint obtained simple fashion 
leaf ptree qva easy access labels yt wt conditions previous state leading leaf 
summing influence explicitly store terms pr yt wt compute 
approach explicitly representing joint probability different action effects summing influence synchronic parents allows accurately capture correlations action emphasize local joint distribution need computed represented explicitly 
factored representation storing yt wt 
fact number variables correlated generally expect approach choice 
continue speak local joint explicitly represented ease exposition 
yt yt yw yw yw yw yt wt ptree qva deciding correlations record ptree qva action network synchronic arcs stage regression final version ptree qva joint distributions labeling leaves 
effects directly impact value function 
important note need compute joint distribution relevant variables contexts correlated 
instance suppose switched locations cpt 
see depends false 
case similar structure maintain independent estimates certain leaves 
particular referring independent distributions maintained leaves independent joint distributions maintained leaves pr yt wt independent 
representing joint distribution explicitly requires val wt pr yt jw yt pr number parameters exponential number variables involved strongly impacted domain size variables maintaining independent product form joint possible important 
piece puzzle pertains decision sum variable influence synchronic descendents retain local joint representation distribution variables 
consider value tree suppose form shown notice dependence reversed action 
regressing tree stage regressed leads tree 
removing influence obtain tree shown 
ideas tempted sum influence computing look ahead see second stage regression algorithm requires regress occurs tree 
specifically detecting need parents leaf nodes attempting yt function parents 
clearly correlated yt explicitly leaving joint representation yt wt shown 
subsequently leaf yt done points 
yt leads obvious question removing post action tree produced regressing depends circumstances sum influence circumstances retain explicit joint representation zt yt 
intuitively want retain expansion terms retain joint need worry correlation 
saw notion need easily noticed variables directly involved value tree regressed explicitly afterward conditions label current branch course 
variables may needed subsequently restricted regressed directly needn part tree variables influence tree retained expanded form 
consider action value function 
obtain tree replaced cpt 
computed explicitly summing yt jz pr 
looking tree see regressed yt depends 
means ignoring specific structure cpt trees wt correlated previous states 
dependence mediated need explicitly joint yt zt determine joint yt wt 
case say zt needed sum influence 
example yt zt wt decide sum obtain reduced joint distribution pr yt wt needed 
suppose depends indirectly dependence mediated 
case sum claim needed zt effect 
effect adequately summarized yt yt zt needed yt wt independent 
considerations formalized detail 
specifically provide formal definitions algorithms needed operationalize intuitions described section 
actions correlated effects leads difficulties 
overhead tree construction increased 
essentially certain minimal probabilistic inference performed order accurately predict effects action compute expected value performing action respect value function 
second difficulty lies maintaining 
certain leaves trees labeled explicit local joint distributions 
considerations suggest may problematic practice 
possibility joint distributions factored certain ways computed needed 
second lies fact actions exhibit correlations effects correlations tend involve small number variables 
algorithm requires joint maintained variables correlated 
clear practical experience needed algorithm realistic assessment 
approximation svi advantage decision trees structure value functions ease specify approximation schemes 
tree tree representing value conditions relevant differences value different states 
distinctions may small impact value 
certain leaves tree may correspond clusters states values differ marginally 
example referring optimal value tree produced spi running example see states abstracted branches hcr ui compared total range values 
include fourth hcr wi values differ 
tree structured value functions easy detect regions similar value 
willing live certain amount approximation error value tree smaller hcr ui hcr values differ pruning tree order coalesce regions similar value 
precisely replacing subtree leaves labeled value small factor single leaf obtain approximation original value tree considerably smaller 
ways label leaves pruned value tree 
label leaf possibly weighted average values subtree replaced possibly midpoint range values replaced 
opted label leaves range values hco hco hcr hcr ranged value trees cautious pruning aggressive pruning 

specifically subtree pruned replacing leaf labeled maximum minimum values labeling leaves pruned subtree 
ranged value trees trees illustrated 
tree corresponds pruning tree removing distinctions referring removing subtrees rooted atr 
maximum range values leaf little error introduced acting optimally respect pruned value function 
second tree produced aggressive pruning subtrees rooted giving smaller tree slightly larger ranges leaves 
immediate computational utility constructing value tree pruning computational effort expended construct larger tree 
pruned tree sequence value trees constructed say structured successive approximation structured value iteration svi pruning computationally beneficial 
instance suppose svi solve structured mdp tree vk generated 
pruning tree vk obtain approximate version pruned tree smaller containing fewer interior nodes 
subsequent decisiontheoretic regression generate tree approximate version generated approximation proceed quickly due fact tree regressed smaller 
develop approximate variant svi value tree sequence produced svi pruned tree sequence constructed 
result algorithm solves say tree structured mdps approximately generally considerably efficiently exact svi spi 
value functions regressed fact ranged trees just simple value trees 
basic operations defined section extended deal value ranges backing maximum minimum values basic regress operator merging trees keep track upper lower bounds 
result algorithm produces sequence trees guarantee true value state lies range labeling appropriate leaf tree 
apart extending algorithms deal value ranges number issues dealt satisfactorily implement approximate version svi 

decide best prune ranged tree 
may opt accurate pruned tree fixed maximum size smallest pruned tree fixed minimum accuracy 
algorithm finding optimal pruning sequence tree algorithm pruning step introduces error 
allows adopt pruning criterion suitable 
problem strongly related pruning decision trees classification algorithm draws ideas bratko 
able provide error bounds algorithm way allows line anytime tradeoffs tree size solution quality 

ability prune strongly influenced variable ordering tree 
issue arises research classification 
finding smallest decision tree representing function np hard discuss certain feasible heuristics suitable reordering tree smaller amenable pruning 

termination svi requires care approximations introduced 
value iteration assured terminate due contraction property bellman backup operator property fails hold approximations introduced fact easily construct examples pruning midpoint replacement causes nontermination 
fortunately ranged value trees construct guaranteed bracket true value function adopt cautious stopping criteria closeness ranges 
refer details approximation structured decision theoretic regression operations 
discuss issues detail various algorithms describe results error bounds provide empirical evidence suggesting approximate svi provide significant computational savings svi spi standard dynamic programming techniques minimal error variety problems 
instance worst case mdps described section approximate svi provides significant savings exact svi little error levels pruning 
example variable worst case domain approximate svi cautious level pruning solves problem time required svi roughly amount time required mpi introduces average error value function 
aggressive pruning level solves problem time required svi roughly time required mpi introduces average error 
similar results obtain problems robot problems exogenous events 
concluding remarks proposed notion decision theoretic regression generalization classical regression planning deals stochastic domains conflicting objectives 
viewed form state space concerned pruning sake simplifying resulting decision tree little loss accuracy contrast pruning purpose preventing overfitting 
approximation careful avoid problems approximation described 
abstraction decision theoretic regression groups states identical value policy choice various points dynamic programming computations required solve mdp 
designed specific decision theoretic regression operator works dbns decision trees representing transition reward functions uses decision trees represent value functions policies 
operation exploits uniformity value function policy specifically fact certain variables certain conditions relevant optimal choice action prediction value 
spi algorithm shown offer significant advantages certain problems terms time space requirements compared unstructured dynamic programming 
described problems overhead spi fails pay 
generally speaking larger problem overhead tree construction associated spi compensated reduction number expected value maximization computations induced abstraction 
discussed certain problem properties benefit spi best worst case examples giving sense boundaries performance 
worst case spi overhead overwhelming 
tests suggest overhead compensated equivalent removal variables 
spi lends readily approximation offers additional computational benefits small error ability construct error bounds anytime computational decisions 
note decision theoretic regression general concept applicability restricted decision tree representations value functions 
principles apply structured representation long develop suitable regression operator representation 
wit spudd system applies decision theoretic regression techniques solution mdps value iteration algebraic decision diagrams represent inputs output 
representations compact decision trees performance spudd considerably better spi adopts general conceptualization problem described 
note spudd improved performance due optimized code 
worst case examples spudd outperforms spi twelve fold larger performance differences larger problems 
occurs despite fact decision diagram representation value functions worst case problem set exactly full decision tree 
provides evidence utility decision theoretic regression 
interesting directions described extended 
integration decision theoretic regression concepts solve mdps effectively 
includes reachability analysis abstraction methods structured value function representations support type functional decomposition value function neural networks additive structure 
prove possible structure assumed spi exploited way orthogonal types structure assumed solution methods 
example integration abstraction methods spi reachability analysis 
spi decision theoretic regression methods need tested empirically realistic domains 
testing give idea types problem structure exist naturally occurring mdps 
suggest types representations associated regression algorithms best exploit 
hope extend decision theoretic regression algorithms sophisticated forms mdps lend realistic modeling domains 
includes consideration firstorder representations stochastic decision problems allow objects relations specified 
extension crucial modeling real world planning problems 
interest extension methods semi markov hybrid continuous discrete models 
application decision theoretic regression partially observable settings important realistic modeling 
investigations application spi pomdps reported vectors corresponding usual piecewise linear representation value functions pomdps treated decision trees produced decision theoretic regression 
investigations compatible structured belief state representations needed approach practical 
christopher atkeson andrew moore stefan schaal 
locally weighted learning control 
artificial intelligence review 
iris charles gary enrico pardo fabio somenzi 
algebraic decision diagrams applications 
international conference computer aided design pages 
ieee 
richard bellman 
dynamic programming 
princeton university press princeton 
bertsekas 
adaptive aggregation infinite horizon dynamic programming 
ieee transactions automatic control 
dimitri bertsekas 
dynamic programming deterministic stochastic models 
prentice hall englewood cliffs 
dimitri bertsekas john 
tsitsiklis 
neuro dynamic programming 
athena belmont ma 
marko ivan bratko 
trading accuracy simplicity decision trees 
machine learning 
craig boutilier 
correlated action effects decision theoretic regression 
proceedings thirteenth conference uncertainty artificial intelligence pages providence ri 
craig boutilier ronen brafman christopher geib 
prioritized goal decomposition markov decision processes synthesis classical decision theoretic planning 
proceedings fifteenth international joint conference artificial intelligence pages nagoya 
craig boutilier ronen brafman christopher geib 
structured reachability analysis markov decision processes 
proceedings fourteenth conference uncertainty artificial intelligence pages madison wi 
craig boutilier thomas dean steve hanks 
decision theoretic planning structural assumptions computational leverage 
journal artificial intelligence research 
craig boutilier richard dearden 
abstractions decision theoretic planning time constraints 
proceedings twelfth national conference artificial intelligence pages seattle 
craig boutilier richard dearden 
approximating value trees structured dynamic programming 
proceedings thirteenth international conference machine learning pages bari italy 
craig boutilier nir friedman goldszmidt daphne koller 
context specific independence bayesian networks 
proceedings twelfth conference uncertainty artificial intelligence pages portland 
craig boutilier goldszmidt :10.1.1.41.4620
frame problem bayesian network action representations 
proceedingsof eleventh biennial canadian conferenceon artificial intelligence pages toronto 
craig boutilier david poole 
computing optimal policies partially observable decision processes compact representations 
proceedings thirteenth national conference artificial intelligence pages portland 
craig boutilier martin puterman 
process oriented planning average reward optimality 
proceedings fourteenth international joint conference artificial intelligence pages montreal 
justin boyan andrew moore 
generalization reinforcement learning safely approximating value function 
tesauro touretzky leen editors advances neural information processing systems 
mit press cambridge 
randal bryant 
graph algorithms boolean function manipulation 
ieee transactions computers 
burch clarke mcmillan dill hwang 
symbolic model checking states 
conference logic computer science pages 
anthony cassandra leslie pack kaelbling michael littman 
acting optimally partially observable stochastic domains 
proceedings twelfth national conference artificial intelligence pages seattle 
david chapman 
planning conjunctive goals 
artificial intelligence 
david chapman leslie pack kaelbling 
input generalization delayed reinforcement learning algorithm performance comparisons 
proceedings twelfth international joint conference artificial intelligence pages sydney 
clarke emerson sistla 
automatic verification finite state concurrent systems temporal logic specifications practical approach 
symposium principles programming languages pages 
acm 
adnan darwiche goldszmidt 
action networks framework reasoning actions change uncertainty 
proceedings tenth conference uncertainty artificial intelligence pages seattle 
thomas dean robert givan 
model minimization markov decision processes 
proceedingsof fourteenth national conference artificial intelligence pages providence 
thomas dean robert givan sonia leach 
model reduction techniques computing approximately optimal solutions markov decision processes 
proceedingsof thirteenth conference uncertainty artificial intelligence pages providence ri 
thomas dean leslie pack kaelbling jak kirman ann nicholson 
planning deadlines stochastic domains 
proceedingsof eleventh national conferenceon artificial intelligence pages washington 
thomas dean kanazawa 
model reasoning persistence causation 
computational intelligence 
richard dearden craig boutilier 
abstraction approximate decision theoretic planning 
artificial intelligence 
thomas dietterich 
maxq method learning 
proceedingsof fifteenth international conference machine learning pages madison wi 
thomas dietterich nicholas flann 
explanation learning reinforcement learning unified approach 
proceedings twelfth international conference machine learning pages lake tahoe 
thomas dietterich nicholas flann 
explanation learning reinforcement learning unified view 
machine learning 
edsger dijkstra 
guarded commands formal derivation programs 
communications acm 
jerome feldman robert sproull 
decision theory artificial intelligence ii hungry monkey 
cognitive science 
richard fikes nils nilsson 
strips new approach application theorem proving problem solving 
artificial intelligence 
zolt bor ri 
multi criteria reinforcement learning 
proceedings fifteenth international conference machine learning pages madison wi 
dan geiger david heckerman 
probabilistic reasoning 
proceedingsof seventh conference uncertainty artificial intelligence pages los angeles 
robert givan thomas dean 
model minimization regression propositional strips planning 
proceedings fourteenth international joint conference artificial intelligence pages nagoya japan 
steve hanks drew mcdermott 
modeling dynamic uncertain world symbolic probabilistic reasoning change 
artificial intelligence 
steven john hanks 
projecting plans uncertain worlds 
phd thesis yale university 
hartmanis stearns 
algebraic structure theory sequential machines 
prentice hall englewood cliffs 
robert st aubin alan hu craig boutilier 
spudd stochastic planning decision diagrams 
proceedings fifteenth conference uncertainty artificial intelligence pages stockholm 
ronald howard 
dynamic programming markov processes 
mit press cambridge 
ronald howard james matheson editors 
readings principles applications decision analysis 
strategic decision group menlo park ca 
rivest 
constructing optimal binary decision trees np complete 
information processing letters 
leslie pack kaelbling 
hierarchical reinforcement learning preliminary results 
proceedings tenth international conference machine learning pages amherst ma 
leslie pack kaelbling michael littman andrew moore 
reinforcement learning survey 
journal artificial intelligence research 
keeney raiffa 
decisions multiple objectives preferences value trade offs 
wiley new york 
nicholas kushmerick steve hanks daniel weld 
algorithm probabilistic planning 
artificial intelligence 
lee yannakakis 
online transition systems 
proceedings th annual acm symposium theory computing stoc pages victoria bc 
michael littman 
algorithms sequential decision making 
ph thesis cs brown university department computer science march 
william lovejoy 
survey algorithmic methods partially observed markov decision processes 
annals operations research 
david mcallester david rosenblitt 
systematic nonlinear planning 
proceedings ninth national conference artificial intelligence pages anaheim 
john mccarthy hayes :10.1.1.85.5082:10.1.1.85.5082
philosophical problems standpoint artificial intelligence 
machine intelligence 
nicolas meuleau hauskrecht kee kim leonid peshkin leslie pack kaelbling thomas dean craig boutilier 
solving large weakly coupled markov decision processes 
proceedings fifteenth national conference artificial intelligence pages madison wi 
judea pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san mateo 
scott penberthy daniel weld 
ucpop sound complete partial order planner adl 
proceedings third international conferenceon principles knowledge reasoning pages cambridge ma 
david poole 
probabilistic horn abduction bayesian networks 
artificial intelligence 
david poole 
logic modelling multiple agents 
artificial intelligence 
precup richard sutton satinder singh 
theoretical results reinforcement learning temporally behaviors 
proceedingsof tenth machine learning pages chemnitz germany 
martin puterman 
markov decision processes discrete stochastic dynamic programming 
wiley new york 
martin puterman shin 
modified policy iteration algorithms discounted markov decision problems 
management science 
ross quinlan 
programs learning 
morgan kaufmann san mateo 
ronald rivest 
learning decision lists 
machine learning 
earl sacerdoti 
nonlinear nature plans 
proceedings fourth international joint conference artificial intelligence pages 
schoppers 
universal plans reactive robots unpredictable environments 
proceedings tenth international joint conference artificial intelligence pages milan 
paul schweitzer martin puterman kyle 
iterative aggregation disaggregation procedures discounted semi markov reward processes 
operations research 
ross shachter 
evaluating influence diagrams 
operations research 
solomon shimony 
role relevance explanation irrelevance statistical independence 
international journal approximate reasoning 
satinder singh david cohn 
dynamically merge markov decision processes 
neural information processing systems pages 
mit press cambridge 
satinder pal singh 
transfer learning composing solutions elemental 
machine learning 
richard smallwood edward sondik 
optimal control partially observable markov processes finite horizon 
operations research 
james smith samuel james matheson 
structuring conditional relationships influence diagrams 
operations research 
edward sondik 
optimal control partially observable markov processes infinite horizon discounted costs 
operations research 
richard sutton 
integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning pages austin 
richard sutton andrew barto 
reinforcement learning 
mit press cambridge ma 
jonathan stuart russell 
control strategies stochastic planner 
proceedings twelfth national conference artificial intelligence pages seattle 
joseph ross shachter 
dynamic programming influence diagrams 
ieee transactions systems man cybernetics 
gerald tesauro 
td gammon self teaching backgammon program achieves master level play 
neural computation 
john tsitsiklis benjamin van roy 
feature methods large scale dynamic programming 
machine learning 
paul utgoff 
decision tree induction efficient tree restructuring 
technical report university massachusetts march 
richard waldinger 
achieving goals simultaneously 
editors machine intelligence machine representations knowledge pages 
ellis horwood chichester england 
christopher watkins peter dayan 
learning 
machine learning 
tom dean david poole marty puterman discussions comments various aspects research 
referees suggestions 
craig boutilier supported nserc research ogp iris phase ii project ic iris phase iii project bac 
done part boutilier university british columbia 
richard dearden supported ubc university graduate fellowship scholarship iris phase ii project ic iris phase iii project bac 
done part goldszmidt rockwell international science center supported darpa contract 

