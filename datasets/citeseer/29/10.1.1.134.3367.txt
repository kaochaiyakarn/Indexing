automatic multi lingual information extraction peng computer science department university waterloo september ph thesis proposal supervisor prof dale schuurmans committee members prof frey prof frank tompa information extraction technique explosion internet 
far systems focusing english text supervised learning framework requires large amount human labor narrow domain domain dependent 
systems difficult ported languages domains inherent shortcomings 
currently western languages english asian languages different english 
english words delimited white spaces computer easily input text string 
languages chinese japanese thai korea word boundaries words 
maxent model 
problems maxent model 
training maxent models 
problem 
iis improved iterative scaling :10.1.1.43.7345
ii maximum entropy models name entity recognition inducing features 
incorporating unlabeled data models em maxent 
training maxent 
chinese word segmentation em segmentation 
step languages word segmentation 
segment character string words separated white spaces 
done steps similar english 
problem languages related segmentation part speech pos tagging 
chinese example part speech taggers available english taggers claimed language independent publicly available pos tagger chinese :10.1.1.11.7760:10.1.1.104.8226:10.1.1.109.179
main reason lack benchmark data training testing 
release chinese treebank able port taggers chinese 
chinese segmenter pos tagger build information extraction system chinese 
problems important 
dl decision list method cotraining method 
supervised learning framework dl maximum likelihood prediction 
unsupervised learning framework name entities represented spelling context pairs 
algorithm sets seed rules label examples spelling rules induce decision list contextual rules supervised learning method label examples current contextual rules induce decision list spelling rules procedures continues rule number reaches constant 
adaboost algorithm training algorithm :10.1.1.37.5438
em method common approach unsupervised learning :10.1.1.133.4884
promising research problems main problems 
performance system behaves task precision recall portability difficult adapt system new domain 
portability problem adapt system language language easily 
supervised learning framework dl maximum likelihood prediction 
unsupervised learning framework name entities represented spelling context pairs 
algorithm sets seed rules label examples spelling rules induce decision list contextual rules supervised learning method label examples current contextual rules induce decision list spelling rules procedures continues rule number reaches constant 
adaboost algorithm training algorithm :10.1.1.37.5438
em method common approach unsupervised learning :10.1.1.133.4884
promising research problems main problems 
performance system behaves task precision recall portability difficult adapt system new domain 
portability problem adapt system language language easily 
adapts english resolution system french spain languages 
thesis considering automatically port system language language human involvement handle multi lingual documents 
systems requiring hand crafted rules hand labeled training data easy goal 
adapting english system western language easier adapting asian language word segmentation problems 
information extraction requires constraints prediction including local ones globe ones 
maximum entropy models shown modeling constraints nlp areas name entity recognition pos word sense disambiguation parsing attribute grammar language modeling text segmentation word spelling check machine translation text classification information extraction :10.1.1.103.7637:10.1.1.104.8226:10.1.1.43.7345
standard method supervised approach means label training data gis iis train model 
performance supervised learning system largely depends labeled training data uses 
natural language processing obtaining labeled training data easy tasks 
research efforts devoted combining unlabeled data supervised learning framework called semi supervised learning 
research efforts devoted combining unlabeled data supervised learning framework called semi supervised learning 
section introduce approaches 
research problem thesis incorporate unlabeled training data models 
problem training 
known biggest limitation model computational complexity gis training iis training :10.1.1.43.7345
find way improve training great contribution models 
possible research problem keep mind research 
port english system chinese need implement chinese word segmentation system chinese pos tagger 
second third problems want solve thesis 
section briefly discusses semi supervised learning literature 
include detailed reviews 
approaches unlabeled data learning fall categories expectation maximization em training adaptive regularization kernel expansion transduction 
interested em approach training approach 
expectation maximization em algorithm general method finding maximum likelihood estimation parameters underlying distribution data set data incomplete missing values :10.1.1.133.4884
assume data observed independently identically generated process parameterized 
call incomplete data assume complete data set exists missing values 
goal em algorithm find optimal gives largest likelihood log likelihood observation argmax argmax log object function log sum generally closed form solution distribution nice form 
em algorithm situation 
version em called generalized em gem converges slower standard em 
proof convergence em literatures see 
known local maximum problem em straightforward applying em generally yield performance 
variants different applications 
uses active learning method select unlabeled data text classification problem overcome local maximum problem standard em authors initialize em labeled data criterion derived query committee qbc select informative unlabeled data add labeled data pool rerun em :10.1.1.13.8629
presents case study addresses unlabeled data help improve naive bayes text classifier small amount labeled training data unlabeled training data penalize influence unlabeled data authors multiply constant log likelihood term unlabeled data 
separate distribution equal parts emphasize influence core lexicon words 
combines em multiple discriminant analysis proposed discriminant em em algorithm image retrieval application 
modified em algorithm include reward optimization problem reinforcement learning 
selecting unlabeled data carefully selecting representative unlabeled data active learning 
monitoring iterative procedure carefully early necessary 
high likelihood necessary mean high performance application continue iteration may result performance degrading chinese word segmentation 
iteration may result fitting 
training training proposed see :10.1.1.111.4626
applicability training depends assumptions 
assumes data correctly classified different views providing sufficient labeled data views conditionally independent class 
formally write data different views data 
views sufficient classification classifiers classify data view view respectively target function label example training labels examples target function assign label example regardless view uses example observed label example web page classification problem web pages representations representation words page representation words hyperlinks pointing page 
formally minimize xi log xi subject con xi straints xk xk pk 
solving constraint optimization problem similar way get solution pk ifi xk xk ifi xk xk 
relationship maxent ml 
log linear model log likelihood lc complete observations empirical probability xk defined lc log xk xk xk log xk form lc log xk xk xk log xk log hand lagrange equation xk log xk xk xk fi pfi xk log log optimal solution constraint maximum entropy model equal solutions maximum likelihood estimation constraint distribution form 
result provides added justification maximum entropy principle notion selecting model basis maximum entropy isn compelling happens model models parametric form best account training sample :10.1.1.103.7637
solution shown unique :10.1.1.43.7345
maxent model 
advantages maxent model 
attractive merit simplicity maxent model represents system features features capture regularities system 
solving constraint optimization problem similar way get solution pk ifi xk xk ifi xk xk 
relationship maxent ml 
log linear model log likelihood lc complete observations empirical probability xk defined lc log xk xk xk log xk form lc log xk xk xk log xk log hand lagrange equation xk log xk xk xk fi pfi xk log log optimal solution constraint maximum entropy model equal solutions maximum likelihood estimation constraint distribution form 
result provides added justification maximum entropy principle notion selecting model basis maximum entropy isn compelling happens model models parametric form best account training sample :10.1.1.103.7637
solution shown unique :10.1.1.43.7345
maxent model 
advantages maxent model 
attractive merit simplicity maxent model represents system features features capture regularities system 
features selected maxent model trained efficient algorithms gis iis 
attractive merit simplicity maxent model represents system features features capture regularities system 
features selected maxent model trained efficient algorithms gis iis 
natural language processing occasions know explicit parametric form distribution easy apply maximum likelihood estimation 
maximum entropy model language represented features features capture regularities language 
example maximum entropy check spelling errors english atom features feature induce algorithm automatically induce features train parameters gis algorithm :10.1.1.43.7345
model simple works 
maxent model fitting problem happens ml estimation maxent incrementally constructs field captures salient properties incorporating increasing detailed collection features allowing generalization new configurations :10.1.1.43.7345
overfitting maxent existing 
third great merit convenient form maxent model kind log linear model beautiful form easy train 
natural language processing occasions know explicit parametric form distribution easy apply maximum likelihood estimation 
maximum entropy model language represented features features capture regularities language 
example maximum entropy check spelling errors english atom features feature induce algorithm automatically induce features train parameters gis algorithm :10.1.1.43.7345
model simple works 
maxent model fitting problem happens ml estimation maxent incrementally constructs field captures salient properties incorporating increasing detailed collection features allowing generalization new configurations :10.1.1.43.7345
overfitting maxent existing 
third great merit convenient form maxent model kind log linear model beautiful form easy train 
trained complete data model achieve maximum trained incomplete data log linear form simpler traditional em algorithm local maximum problem training speed faster 
problems maxent model maxent model attractive merits successful nlp task 
definite answer question 
currently people just put useful patterns features 
empirical matter 
efficiently induce features 
currently algorithm doing greedy search :10.1.1.43.7345
time consuming hope advanced algorithm 
efficiently incorporate unlabeled data 
maxent framework supervised learning framework performance depends labeled training data 
nlp tasks labeled data easy get 
complete distribution form 
log likelihood lc lc log xk xk xk log xk log take derivative lc respect set zero lc fi fi clearly fi depends coupling system easy solve directly 
numeric optimization methods problem darroch ratcliff gis generalized iterative scaling constraint sum features constant 
della pietra 
proposed iis improved iterative scaling constraint :10.1.1.43.7345
sum features constant iis reduces gis 
iis improved iterative scaling iis algorithm define auxiliary function ac defined lower bound difference lc lc :10.1.1.43.7345
lc lc log log ifi log lc lc ifi ifi fi call ac ac likelihood improved straightforward method take derivative ac respect set zero solve equations 
introduce coupling system 
numeric optimization methods problem darroch ratcliff gis generalized iterative scaling constraint sum features constant 
della pietra 
proposed iis improved iterative scaling constraint :10.1.1.43.7345
sum features constant iis reduces gis 
iis improved iterative scaling iis algorithm define auxiliary function ac defined lower bound difference lc lc :10.1.1.43.7345
lc lc log log ifi log lc lc ifi ifi fi call ac ac likelihood improved straightforward method take derivative ac respect set zero solve equations 
introduce coupling system 
get define fi ac ifi jensen inequality fi ac ifi fi call bc differentiating bc respect bc fi fi solve equation pseudo code iis shown start arbitrary values convergence compute solve equation bc set maximum entropy models name entity recognition assuming want assign kinds name entities token test corpus problem formulated calculating probability ct information derive token maximum entropy model information derived token represented binary features equation special form 
example english word begins capitalized large probability person name numbers 
supervised construction segment raw unsegmented corpus hand collect words segmented corpus build lexicon 
unfortunately chinese characters commonly building complete lexicon hand impractical 
number unsupervised segmentation methods proposed segment chinese japanese text 
approaches form em learn probabilistic model character sequences employ viterbi decoding procedures segment new text words 
reason em algorithm widely adopted unsupervised training guaranteed converge probability model locally maximizes likelihood posterior probability training data :10.1.1.133.4884
chinese segmentation em usually applied extracting lexicon contains candidate multi grams training corpus initializing probability distribution lexicon standard iteration adjust probabilities multi grams increase posterior probability training data 
em segmentation assume sequence characters ct wish segment chunks sm chunks si chosen lexicon si 
probability distribution si defined lexicon compute segmentation sequence ct chunks sm follows 
segmentation calculate joint likelihood prob task find segmentation achieves maximum likelihood argmax prob argmax prob note number possible segmentations feasible enumerate determine best segmentation efficiently recovered viterbi algorithm employs dynamic programming approach build best segmentation bottom manner 
chinese segmentation em usually applied extracting lexicon contains candidate multi grams training corpus initializing probability distribution lexicon standard iteration adjust probabilities multi grams increase posterior probability training data 
em segmentation assume sequence characters ct wish segment chunks sm chunks si chosen lexicon si 
probability distribution si defined lexicon compute segmentation sequence ct chunks sm follows 
segmentation calculate joint likelihood prob task find segmentation achieves maximum likelihood argmax prob argmax prob note number possible segmentations feasible enumerate determine best segmentation efficiently recovered viterbi algorithm employs dynamic programming approach build best segmentation bottom manner 
dempster update function case examining prob log prob maximizing constraint parameter re estimation formula si prob sj sj prob obtain numerator sum possible segmentations number occurrences word si weighted probability segmentation :10.1.1.133.4884
similarly denominator weighted sum number words possible segmentations 
weighted frequency count 
equation efficiently calculated forward backward algorithm efficiently approximated viterbi algorithm see detailed algorithms 
self supervised chinese segmentation advantage unsupervised lexicon construction automatically discover new words words acquired high probability 
segmentation comes distributions prob may true 
influence application 
fact think process way segmentation comes distribution merge half space distributed core lexicon half space distributed candidate lexicon 
prob task find segmentation achieves maximum likelihood argmax prob effect equal argmax prob argmax prob convenience description equation discussion 
similarly em segmentation update function em update prob log prob maximizing constraints yields parameter re estimation formulas si prob si si prob sj prob sj sj prob si number times si occurring segmentation standard re estimation formulas differently successive optimizations see :10.1.1.133.4884
cases denominator weighted sum number words possible segmentations numerator normalization constant weighted frequency counts 
updates efficiently calculated forward backward algorithm efficiently approximated viterbi algorithm main difficulty applying em problem probability distributions complex typically cause em get trapped poor local maxima 
help guide em better probability distributions partition lexicon core candidate lexicon initialized empty contains words 
pass starting uniform distribution em increase likelihood training corpus 
firstly segment character string chunks compute probability pos tags chunks search best segmentation complicated standard segmentation 
consider parts prob prob 
chinese part speech tagger pos information useful nlp task including information retrieval speech recognition language modeling text speech synthesis 
max ent name entity recognition system going pos information 
publicly available pos tagger english claimed language independent pos tagger chinese :10.1.1.11.7760:10.1.1.104.8226:10.1.1.109.179
easy port pos tagger english western language spain french languages similar sense 
character represented byte computer 
words delimited explicitly whitespace 
word formation similar words prefix suffix stem 

public available benchmark tagged data training testing 
alleviated release chinese treebank 
section discuss chinese pos tagger trained chinese treebank 
basic idea choosing pos tagger current pos taggers fall categories rule tagger hidden markov gram tagger maximum entropy tagger hybrid tagger :10.1.1.11.7760
rule tagger induces rules training corpus :10.1.1.11.7760
advantage storage ability consider long range information 
rule approach robust easy port new tag sets new languages 
markov gram taggers statistical taggers easily ported tag sets 
public available benchmark tagged data training testing 
alleviated release chinese treebank 
section discuss chinese pos tagger trained chinese treebank 
basic idea choosing pos tagger current pos taggers fall categories rule tagger hidden markov gram tagger maximum entropy tagger hybrid tagger :10.1.1.11.7760
rule tagger induces rules training corpus :10.1.1.11.7760
advantage storage ability consider long range information 
rule approach robust easy port new tag sets new languages 
markov gram taggers statistical taggers easily ported tag sets 
take account words may lose long range information useful tagging 
rule approach robust easy port new tag sets new languages 
markov gram taggers statistical taggers easily ported tag sets 
take account words may lose long range information useful tagging 
hybrid tagger combination rule tagger markov gram tagger order take advantages rule tagger markov gram tagger 
maximum entropy tagger statistical tagger markov gram tagger take account long range information clues capitalization :10.1.1.104.8226
maximum entropy tagger state art performance uses sophisticated methods handle unknown words invented trigrams tags tagger better maximum entropy model 
gave comparison taggers swedish 
tri gram tagger performances best 
performance depends size training corpus 
handling unknown words influential part pos tagger ability handle unknown words 
english taggers prefix suffix information estimate pos tag unknown word 
chinese problem difficult 
solutions consideration 
incorporating unlabeled data possible methods include training boosting active learning em :10.1.1.13.8629:10.1.1.133.4884
experiments multi lingual information extraction system system architecture constructing chinese segmenter pos tagger build self contained chinese name entity recognition system 
basic structure system shown fig 
input chinese document chinese segmenter maxent ne finder output name entities chinese pos tagger self contained chinese name entity recognition system experiments design want evaluate system real data 
year chinese news downloaded www com 
humphreys gaizauskas cunningham wilks language independent domain model multilingual information extraction 
proceedings ijcai workshop software industry ai contribution 
beeferman berger lafferty text segmentation exponential models 
proceedings nd empirical methods natural language processing aaai providence ri 
berger pietra pietra maximum entropy approach natural language processing computational linguistics march gentle tutorial em algorithm application parameter estimation gaussian mixture hidden markov models :10.1.1.103.7637
technical report university berkeley icsi tr 
blum learning labeled unlabeled data graph 
proceedings icml 
blum mitchell combining labeled unlabeled data training 
brent efficient probabilistically sound algorithm segmentation word discovery 
machine learning 
brent tao chinese text segmentation making training corpora proceedings acl 
brill simple rule part speech tagger 
proceedings anlp brill transformation error driven learning natural language processing case study part speech tagging :10.1.1.11.7760
computational linguistics dec 
castelli cover relative value labeled unlabeled samples pattern recognition unknown mixing parameter 
ieee trans 
information theory vol 
darroch ratcliff iterative scaling log linear models 
annals mathematical statistics volume issue oct 
language modeling variable length sequences theoretical formulation evaluation 
icassp 
dempster laird rubin maximum likelihood incomplete data em algorithm :10.1.1.133.4884
journal royal statist 
soc 
ser 

manning sch tze foundations statistical natural language processing 
mit press cambridge massachusetts 
mark markov random field models natural language ph thesis washington university 
mccallum freitag maximum entropy information extraction segmentation proceedings icml 
mccallum nigam employing em pool active learning text classification :10.1.1.13.8629
proceedings icml 
comparing data driven learning algorithms pos tagging swedish 
proceedings emnlp 
mitchell role unlabeled data supervised learning 
peng schuurmans self supervised chinese word segmentation appear proceedings th international symposium intelligent data analysis ida 
peng schuurmans hierarchical em approach word segmentation appear proceedings th natural language processing pacific rim symposium 
pierce cardie limitations training natural language learning large datasets 
proceedings conference empirical methods natural language processing emnlp 
pietra pietra lafferty inducing features random fields :10.1.1.43.7345
ieee transactions pattern analysis machine intelligence vol 

pietra pietra lafferty maximum entropy iterative scaling excerpts submitted publication 
croft word segmentation procedure information retrieval 
croft word segmentation procedure information retrieval 
proceedings symposium document analysis information 
rabiner tutorial hidden markov models selected applications speech recognition 
proceedings ieee vol 
ratnaparkhi maximum entropy models natural language ambiguity resolution :10.1.1.104.8226
ph thesis computer information science department university pennsylvania 
riezler probabilistic constraint logic programming ph thesis university stuttgart 
riezler kuhn johnson lexicalized stochastic modeling constraint grammars log linear measures em training 
proceedings th annual meeting association computational linguistics acl hong kong 
