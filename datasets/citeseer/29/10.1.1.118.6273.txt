informed prefetching caching hugo patterson gibson daniel jim may cmu cs school computer science carnegie mellon university pittsburgh pa disk parallelism file cache buffers traditional file systems induces stall time degrades performance modern microprocessor systems 
aggressive mechanisms tailor file system resource management needs intensive applications 
particular show application disclosed access patterns hints expose exploit parallelism allocate dynamically file buffers competing demands prefetching hinted blocks caching hinted blocks reuse caching data accesses 
approach estimates impact alternative buffer allocations application execution time applies cost benefit analysis allocate buffers greatest impact 
implemented informed prefetching caching dec osf operating system measured performance mhz alpha equipped disks running range applications including text search scientific visualization relational database queries speech recognition computational chemistry 
informed prefetching reduces execution time applications 
