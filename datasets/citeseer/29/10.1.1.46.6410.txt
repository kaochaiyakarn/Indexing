hybrid architecture situated learning reactive sequential decision making ron sun todd peterson edward merrill university alabama email cs ua edu phone abbreviated title hybrid learning january developing autonomous agents usually emphasizes situated procedural knowledge ignoring explicit declarative knowledge 
hand developing symbolic reasoning models usually emphasizes declarative knowledge ignoring procedural knowledge 
contrast developed learning model clarion hybrid connectionist model consisting localist distributed representations level approach proposed sun 
clarion learns utilizes procedural declarative knowledge tapping synergy types processes enables agent learn situated contexts generalize resulting knowledge different scenarios 
unifies connectionist reinforcement symbolic learning synergistic way perform line bottom learning 
summary presents version architecture results experiments 
approach applied learning mazes navigation tasks robot control sutton mahadevan connell lin kaelbling dietterich 
approach genetic algorithm holland tackle kind task grefenstette schultz 
handle temporal credit assignment problem drifting worlds 
distinguish procedural declarative knowledge 
terms learning declarative knowledge rules tasks identified characteristics task render existing algorithms inapplicable require exemplar sets michalski quinlan incrementally consistent instances mitchell lebowitz fisher utgoff complex manipulations learned structures inconsistency discovered typically complex limited time reactive agent may hirsh :10.1.1.10.181
drifting analyzed clearly noise inconsistency considered learning algorithms clark niblett involves changes time leads radical changes learned knowledge 
requires extra dedicated mechanisms widmer kubat 
rule learning algorithms handle learning sequences necessarily involves temporal credit assignment 
remainder highlight hybrid model clarion type task section 
discuss briefly sets experiments model demonstrate advantages model section 
clarion compared models section 
concluding remarks section 
incremental algorithm explicit statistics changes gradually accommodated schlimmer 
hybrid models discuss subsections motivations model parts published see sun peterson :10.1.1.54.5248:10.1.1.46.8088
subsections discuss details model mainly involve rule learning algorithm different previously published algorithms model cf 
sun peterson :10.1.1.54.5248:10.1.1.46.8088
procedural declarative knowledge agent develop set skills highly specific geared particular situations highly efficient time acquire sufficiently general knowledge readily applied variety different situations communicated 
humans possess abilities able achieve appropriate balance sides existing systems fall short 
concluding remarks section 
incremental algorithm explicit statistics changes gradually accommodated schlimmer 
hybrid models discuss subsections motivations model parts published see sun peterson :10.1.1.54.5248:10.1.1.46.8088
subsections discuss details model mainly involve rule learning algorithm different previously published algorithms model cf 
sun peterson :10.1.1.54.5248:10.1.1.46.8088
procedural declarative knowledge agent develop set skills highly specific geared particular situations highly efficient time acquire sufficiently general knowledge readily applied variety different situations communicated 
humans possess abilities able achieve appropriate balance sides existing systems fall short 
appears missing duality procedural declarative knowledge specific skills generic knowledge 
great deal demonstrating difference procedural knowledge declarative knowledge anderson anderson lebiere keil damasio 
update bottom level accordance learning 
update rule network rule extraction revision 

go back step 
describe details version architecture deterministic rule learning version simplest version illustrates basic idea architecture see sun peterson details statistical rule learning versions :10.1.1.54.5248:10.1.1.46.8088
bottom level value evaluation quality action state indicates desirable action state consists sensory input 
choose action values 
acquire values standard learning algorithm temporal difference reinforcement learning algorithm determined earlier 
details learning see watkins developments bertsekas tsitsiklis kaelbling parr russell dietterich precup 
justify cognitively shown willingham declarative knowledge influence procedural performance 
allows different operational modes relying top level relying bottom level combining outcomes levels weighing differently 
operational modes roughly correspond folk psychological notions intuitive reactive mode deliberative mode various mixture different percentage dreyfus dreyfus 
methods combining outcomes levels tried 
example stochastic method combine corresponding values action levels weighted sum top level indicates action activation value rules binary bottom level indicates activation value value final outcome breiman :10.1.1.32.9399
stochastic decision making boltzmann distribution weighted sums performed select action possible actions 
see willingham psychological justifications combination method 
model putting levels implemented connectionist fashion model see 
rule level reactive level detailed clarion architecture contrast characteristics levels 
top level discrete rigorously verified experience random exploration learns error shot fashion 
bottom level continuous graded statistical nature rigorously verified random exploration learns gradual cumulative fashion 
complement 
note generalization rules complements generalization bottom level bottom level generalization continuous graded rule generalization top level discrete crisp capturing different kinds regularities 
possess different characteristics level tends learn differentially combination stochastic averaging result improved performance breiman freund :10.1.1.32.9399
necessity having level architecture summed follows ffl bottom level agent able represent procedural skills sufficiently 
skills may involve graded uncertain knowledge autonomous stochastic exploration numeric calculation probabilistic firing 
may captured simpler mechanisms anderson 
ffl learning bottom level agent able learn experience able dynamically acquire procedural skills bottom level cf 
agent allotted time period reach target success hit mine failure run fuel failure 
agent severe time pressure reactive decision making 
random environment generated episode repetition experience agent experiment 
learning difficult agent take account varying environments 
reinforcements agent produced successful agent episode sun peterson :10.1.1.54.5248:10.1.1.46.8088
navigation input display upper left corner fuel gauge vertical upper right corner range gauge round middle bearing gauge sonar gauges bottom 
learning speeds experiment minefield contains number mines case 
mines randomly placed starting point agent target 
target location randomly selected time 
example sun peterson contains details experiments alternative versions clarion architecture statistical rule learning versions 
sun peterson contains details computational experiments domain maze domain 
results demonstrate performance advantages architecture 
discussions experiments verifying predictions earlier predictions section received empirical support 
shown see sun peterson rule learning helps speed learning shown earlier rule learning helps improve trained performance shown sun peterson rule learning helps facilitate transfer shown earlier learned rules transfer better system see sun peterson :10.1.1.54.5248:10.1.1.46.8088
note results serve existence proofs proof necessity model advantages 
concept formation symbolic reasoning learning rules clarion forms concepts 
available distributed feature representations bottom level specifying rule conditions separate node set top level represent conditions rule connects feature representations 
induced rules localist representations formed 
