bayesian approach learning bayesian networks local structure david maxwell chickering dmax microsoft com david heckerman microsoft com microsoft research redmond wa christopher meek meek microsoft com researchers investigated techniques data learn bayesian networks containing compact representations conditional probability distributions cpds stored node 
majority concentrated decision tree representations cpds 
addition researchers typically apply non bayesian asymptotically bayesian scoring functions mdl evaluate goodness fit networks data 
investigate bayesian approach learning bayesian networks contain general decision graph representations cpds 
describe evaluate posterior probability bayesian score network database observed cases 
second describe various search spaces conjunction scoring function search procedure identify high scoring networks 
experimental evaluation search spaces greedy algorithm bayesian scoring function 
set observations domain common problem data analyst faces build models process generated data 
years researchers uai community contributed enormous body problem bayesian networks model choice 
works include cooper herskovits buntine spiegelhalter heckerman 

substantial amount early learning bayesian networks observed data infer global independence constraints hold domain interest 
global independences precisely follow missing edges bayesian network structure 
researchers including boutilier friedman goldszmidt extended classical definition bayesian network include efficient representations local constraints hold parameters stored nodes network 
notable features majority effort concentrated inferring decision trees structures explicitly represent parameter equality constraints researchers typically apply non bayesian asymptotically bayesian scoring functions mdl evaluate goodness fit networks data 
apply bayesian approach learning bayesian networks contain decision graphs generalizations decision trees encode arbitrary equality constraints represent conditional probability distributions nodes 
section introduce notation previous relevant 
section describe evaluate bayesian score bayesian network contains decision graphs 
section investigate search algorithm conjunction scoring function identify networks data 
section data various domains evaluate learning accuracy greedy search algorithm applied search spaces defined section 
section conclude discussion extensions 
background section describe notation discuss previous relevant 
remainder lower case letters refer variables upper case letters refer sets variables 
write xi observe variable xi state observe state variable set call set observations state arguably abuse notation find convenient index states set variables single integer 
example set containing binary variables may write denote 
section define bayesian network 
section describe decision trees represent probabilities bayesian network 
section describe decision graphs generalizations decision trees 
bayesian networks consider domain discrete variables xn xi finite number states 
bayesian network represents joint probability distribution encoding assertions conditional independence collection probability distributions 
specifically bayesian network pair bs bs structure network set parameters encode local probability distributions 
structure bs components global structure set local structures acyclic directed graph dag short contains node variable xi edges denote probabilistic dependences variables par xi denote set parent nodes xi xi refer variable corresponding node set local structures 
mn set mappings variable xi mi maps value xi par xi parameter 
assertions conditional independence implied global structure bayesian network impose decomposition joint probability distribution xi par xi mi set parameters contains node xi state xi parent state single parameter encodes condi sum xi par xi mi bayesian network tional probabilities equation 
xi par xi mi note function depends mi notational simplicity leave dependency implicit 
ri denote number states variable xi qi denote number states set par xi 
ij denote set parameters characterizing distribution xi par xi mi ij ri denote set parameters characterizing conditional distributions xi par xi mi qi ij classical implementation bayesian network node xi stores ri qi distinct parameters large table 
mi simply lookup table 
note size table grows exponentially number parents qi 
decision trees equality constraints hold parameters researchers mappings complete tables efficiently represent parameters 
example consider global structure depicted assume nodes binary 
furthermore assume value depend mz mz decision tree shown implement mapping mz represent mz single distribution mz mz 
contain ri distinct parameters distribution 
simplicity leave implicit remainder 
decision tree node decision trees described detail breiman represent sets parameters bayesian network 
tree dag containing exactly root node node root node exactly parent 
leaf node contains table distinct parameters collectively define conditional probability distribution xi par xi mi 
non leaf node tree annotated name parent variables par xi 
going edges node tree annotated mutually exclusive collectively exhaustive sets values variable 
node decision tree annotated name say splits 
edge child annotated value say child corresponding note definition edge annotations child node corresponding value unique 
traverse decision tree find parameter follows 
initialize root node decision tree 
long leaf node par xi splits reset child corresponding value determined par xi repeat 
leaf return parameter table corresponding state xi 
decision tree expressive mappings complete tables represent parameters complete table complete decision tree 
complete decision tree ti node xi tree depth par xi node vl level ti splits lth parent par xi exactly children value 
follows definition ti complete tree map distinct parameter distinct precisely behavior complete table 
researchers decision trees useful eliciting probability distributions experts extensive knowledge equality conditional distributions 
furthermore researchers developed methods learning local structures data 
decision graphs section describe generalization decision tree known decision graph represent richer set equality constraints local parameters 
decision graph identical decision tree decision graph nonroot nodes parent 
consider example decision graph depicted 
decision graph represents conditional probability distribution node different equality constraints tree shown 
specifically decision graph encodes equality decision graph node di denote decision graph node xi 
mapping node xi implemented di di mi denote mapping 
decision graph di explicitly represent arbitrary set equality constraints form ij ij demonstrate consider complete tree ti node xi 
transform ti decision graph represents desired constraints simply merging leaf nodes contain sets equal 
interesting note equality constraint form equation interpreted independence constraint xi par xi par xi par xi allow nodes decision graph di split node xi nodes par xi represent arbitrary set equality constraints parameters return issue section assume nodes di split xi 
learning decision graphs researchers derived bayesian fit called bayesian score network assuming equalities parameters 
friedman goldszmidt derive bayesian score structure containing decision trees 
section show evaluate bayesian score structure containing decision graphs 
derive bayesian score need assumption process generated database particular assume database random exchangeable sample unknown distribution constraints represented network structure bs containing decision graphs 
saw previous section structure bs imposes set independence constraints hold distribution represented bayesian network structure 
de hypothesis independence fine bh constraints imposed structure bs hold joint distribution database generated contains independence constraints 
refer reader heckerman 
detailed discussion structure hypotheses 
bayesian score structure bs posterior probability bh observed database concerned relative scores various structures case constant ignored 
consequently extend definition bayesian score function proportional bh bh 
assume efficient method assessing bh assuming distribution uniform example concentrate derive marginal likelihood term bh 
integrating unknown parameters bh researchers typically number simplifying assumptions collectively allow equation expressed closed form 
introducing assumptions need notation 
showed section local structure node xi decision graph di sets parameters ij ij identical derivations follow find useful enumerate distinct parameter sets equivalently find useful enumerate leaves decision graph 
remainder section adopt syntactic convention 
referring parameter set stored leaf decision graph denote node index denote parent state index 
referring parameter set context specific parent state node denote node index denote parent state index 
enumerate set leaves decision graph da define set leaf set indices la idea la contains exactly parent state index leaf graph 
precisely denote number leaves da 
la 
bl defined set properties 
la 
la property ensures index corresponds different leaf second property ensures leaf included 
assumption derive equation closed form parameter independence assumption 
simply stated assumption says hypothesis bh knowledge distinct parameter set ab give information distinct parameter set 
assumption parameter independence bh la ab assumption researchers dirichlet assumption 
assumption restricts prior distributions distinct parameter sets dirichlet 
assumption dirichlet la ab bh ra abc ra abc abc recall ra denotes number states node xa 
hyperparameters abc characterize prior knowledge parameters 
heckerman 
describe derive exponents prior bayesian network 
return issue 
assumptions derive bayesian score structure contains decision graphs completely analogous method heckerman 

showing result define inverse function 
denote arbitrary parameter 
function denotes set index triples maps 
dijk denote number cases xi par xi define follows ijk abc dijk intuitively number cases provide information parameter abc 
letting nab ab abc write bayesian score follows ab nab ab ra la abc abc determine counts node xa follows 
initialize counts zero 
case database kc jc denote value xi par xi case respectively increment count corresponding parameter abc xi kc par xi jc da 
parameter efficiently traversing da root 
say scoring function node decomposable factored product functions depend node parents 
node decomposability useful efficiently searching space global network structures 
note equation node decomposable long bh node decomposable 
consider node decomposable distributions bh 
simplest distribution assume uniform prior network structures 
set bh constant equation 
simple prior experiments described section 
approach priori favor networks fewer parameters 
example 
note corresponds uniform prior structure hypotheses 
simple prior parameters assume abc choice values corresponds uniform prior parameters explored cooper herskovits context bayesian networks containing complete tables 
call bayesian scoring function uniform scoring function hyperparameters set 
prior works practice easy implement 
additional assumptions heckerman 
show abc derived prior bayesian network 
idea abc proportional prior probability obtained prior network states xi par xi map parameter abc 
specifically prior bayesian network set abc ijk abc xi par xi single equivalent sample size asses exponents par xi denotes parents xi opposed parents prior network 
understood measure confidence parameters call bayesian scoring function pn scoring function prior network scoring function exponents assessed way 
heckerman 
derive constraints context bayesian networks complete tables 
full version show constraints follow decision graphs slight modifications additional assumptions 
provide details decision graph structure efficiently compute exponents abc prior network way computed values database 
search scoring function evaluates merit bayesian network structure bs learning bayesian networks data reduces search structures high score 
chickering shows finding optimal structure containing complete tables mappings np hard bayesian scoring function 
result reasonable assume allowing general decision graph mappings problem remains hard consequently appropriate apply heuristic search techniques 
section define search space structures single node xi assuming parent set par xi fixed 
space defined apply space number known search algorithms 
experiments described section example apply greedy search 
section describe greedy algorithm combines local structure search decision graphs nodes global structure search edges decision graph search section assume states search space correspond possible decision graphs node xi 
order search algorithm traverse space define set operators transform state 
operators define operator modification current set leaves decision graph 
definition complete split leaf node decision graph par xi parent xi 
complete split adds ri new leaf nodes children child corresponds distinct value 
definition binary split leaf node decision graph par xi parent xi 
binary split adds new leaf nodes children child corresponds state child corresponds states 
definition merge distinct leaf nodes decision graph 
merge merges single node 
resulting node inherits parents 
show result type operator decision graph node parents states 
add pre condition operator change parameter constraints implied decision graph 
allow example complete split new children correspond impossible states third child correspond original constraints 
note starting decision graph containing example application type operator original decision graph result applying result applying result applying single node root leaf node generate complete decision tree repeatedly applying complete splits 
discussed previous section represent parameter set equalities merging leaves complete decision tree 
consequently starting graph containing node exists series operators result set possible parameter set equalities 
note repeatedly merge leaves decision graph single parameter set resulting graph equivalent terms parameter equalities graph containing single node 
operators sufficient moving set parameter constraints set parameter constraints 
discuss methods simplify terms number nodes decision graphs represent set parameter constraints 
complete split operator needed ensure parameter equalities reached complete split replaced series binary splits resulting parameter set constraints identical 
included complete split operator hopes help lead search algorithm better structures 
section compare greedy search performance various search spaces defined including subsets operators 
combining global local search section describe greedy algorithm combines global structure search edges local structure search decision graphs nodes suppose decision graph di node xi non leaf node annotated parent par xi 
case xi independent parents remove par xi violating decomposition equation 
fixed structure learn local decision graphs nodes delete parents independent 
consider adding edges follows 
node xi add par xi non descendants xi learn decision graph xi delete parents contained decision graph 
shows greedy algorithm uses combines ideas 
experiments started algorithm structure contains edges graph di consists single root node 

score current network structure bs 
node xi 
add non descendant parent xi par xi 
possible operator decision graph di 
apply bs 
score resulting structure 

remove parent added xi step 
best score step better current score 
operator resulted best score 
split operator complete binary node xj par xi add xj par xi 
apply bs 
goto 
return bs greedy algorithm combines local global structure search note result merge operator decision graph di xi may rendered independent parents par xi di contains node annotated 
simple example repeatedly merge leaves single leaf node resulting graph implies xi depend parents 
experimentally algorithm phenomenon rare 
testing parent deletions expensive chose check experiments described section 
greedy approach learning structures containing decision trees explored friedman goldszmidt 
idea score edge operations adding deleting reversing edges applying operation greedily learning local decision trees nodes parents changed result operation 
full version compare approach theirs 
experimental results section investigate varying set allowed operators affects performance greedy search 
disallowing merge operator search algorithms identify decision tree local structures bayesian network 
consequently see learning accuracy changes context greedy search generalize local structures decision trees decision graphs 
experiments described section measure learning accuracy posterior probability identified structure hypotheses 
researchers criteria predictive accuracy holdout set structural difference generative model 
reason criteria evaluating search algorithm performs various search spaces goal search algorithm maximize scoring function 
evaluating bayesian scoring functions approximate criteria 
experiment consider promoter gene sequences database uc irvine collection consisting cases 
variables domain 
variables 
represent base pair values dna sequence possible values 
variable promoter binary indicates sequence promoter activity 
goal learning domain build accurate model distribution promoter consequently reasonable consider static graphical structure par promoter search decision graph node promoter 
table shows relative bayesian scores best decision graph learned greedy search various parameter priors search spaces 
searches started decision graph containing single node current best operator applied step operator increased score current state 
column corresponds different restriction search space described section labels indicate operators greedy search table greedy search performance various bayesian scoring functions different sets operators promoter domain 
cb cm bm cbm uniform pn pn pn pn pn allowed denotes complete splits denotes binary splits denotes merges 
column labeled bm example shows results greedy search binary splits merges complete splits 
row corresponds different parameter prior bayesian scoring function 
pn scoring function special case pn scoring function prior network imposes uniform distribution variables 
number pn row labels indicates equivalent sample size 
results uniform prior structure hypotheses 
value zero row table denotes hypothesis lowest probability identified parameter prior 
values denote natural logarithm times identified hypothesis lowest probability 
comparing relative values searches merges searches don merges see exception adding merge operator results significantly probable structure hypothesis 
conclude greedy search decision graphs results better solutions greedy search decision trees 
interesting observation complete split operator reduces solution quality restrict search decision trees 
performed identical experiment classification problem simplicity results uniform scoring function 
recall section uniform scoring function hyperparameters abc set 
second experiment run splice junction gene sequences database uc irvine repository 
database contains dna sequence problem predict position middle sequence intron exon boundary exon intron boundary 
results table 
uniform prior structure hypotheses 
table greedy search performance uniform scoring function different sets operators splice domain 
cb cm bm cbm table greedy search performance uniform scoring function node alarm network 
included uniform score model comp cb cm bm cbm table supports claim get significant improvement decision graphs decision trees 
final set experiments done alarm domain known benchmark learning algorithms 
alarm network described beinlich 
bayesian network diagnosis medical domain 
parameters network stored complete tables 
experiment alarm domain demonstrate fixed global structure hypothesis identified searching local decision graphs nodes significantly better hypothesis corresponding complete tables nodes 
generated cases alarm network computed uniform bayesian score alarm network assuming parameter mappings complete tables 
expect posterior model quite re evaluating generative model structure 
uniform scoring function applied greedy searches previous experiments identify decision graphs nodes network 
kept global structure fixed identical global structure alarm network 
results shown table values semantics previous tables 
score column labeled comp score complete table model 
table demonstrates search performance decision graphs identify significantly better models just decision trees 
fact complete table model attains low score best hypothesis times probable complete table hypothesis 
surprising examination probability tables stored table performance greedy algorithm combines local global structure search different sets operators alarm domain 
included result greedy algorithm searches global structure assuming complete tables 
comp cb cm bm cbm table performance restricted version greedy algorithm different sets operators alarm domain 
included result greedy algorithm initialized global structure alarm network searches global structure assuming complete tables 
comp cb cm bm cbm alarm network tables contain parameter set equalities 
experiment alarm domain test structure learning algorithm section 
generated database cases uniform scoring function uniform prior structure hypotheses 
ran versions algorithm corresponding possible sets local structure operators previous experiments 
ran greedy structure search algorithm assumes complete tables nodes 
initialized search global network structure edges operators single edge modifications graph deletion addition reversal 
table show results 
column labeled comp corresponds greedy search structures complete tables 
note allow nodes contain decision graphs get significant improvement solution quality 
note search complete table structures performed algorithm restricted algorithm search decision trees containing complete splits complete splits binary splits 
final experiment repeated previous experiment allowed algorithm add parents descendants generative model 
restricted global search dags violate partial ordering alarm network 
ran greedy structure search algorithm searches structures complete tables initialized search alarm network 
results experiment shown table 
table see constrained searches exhibit relative behavior unconstrained searches 
experiment alarm domain tables values measure performance search relative worst performance experiment 
table compare search performance experiments alarm domain 
value zero table corresponds experiment set operators led table comparison bayesian scores experiments alarm domain comp cb cm bm cbm learned hypothesis lowest posterior probability experiments operator restrictions considered alarm domain 
values table relative lowest posterior probability 
row labels correspond experiment denotes experiment performed local searches static global structure denotes second experiment performed unconstrained structural searches denotes final experiment performed constrained structural search 
surprising hypothesis learned global structure search decision graphs higher posterior hypothesis learned generative static structures 
discussion showed derive bayesian score network structure contains parameter maps implemented decision graphs 
defined search space learning individual decision graphs static global structure defined greedy algorithm searches global local structure simultaneously 
demonstrated experimentally greedy search structures containing decision graphs significantly outperforms greedy search structures containing complete tables structures containing decision trees 
consider extension decision graph mentioned section 
recall decision graph parameter sets stored table leaves 
decision graphs implemented way parameter abc belong exactly distinct parameter set 
important consequence property priors parameter sets dirichlet assumption posterior distributions dirichlet 
dirichlet distribution conjugate respect likelihood observed data 
result easy derive bayesian scoring function closed form 
allow nodes decision graph di split node xi represent arbitrary set parameter constraints form example consider baysian network variable domain parent decision graph splits represent constraint dy dy unfortunately allow types constraints dirichlet distribution longer conjugate respect likelihood data parameter independence assumption violated 
consequently derivation described section apply 
conjugate priors decision graph di splits node xi exist full version weaker version parameter independence derive bayesian score graphs closed form 
conclude noting easy extend definition network structure represent constraints parameters different nodes network ij buntine thiesson consider types constraints 
bayesian score structures derived simple modifications approach described 
beinlich beinlich suermondt chavez cooper 

alarm monitoring system case study probabilistic inference techniques belief networks 
proceedings second european conference artificial intelligence medicine london pages 
springer verlag berlin 
buntine buntine 

theory refinement bayesian networks 
proceedings seventh conference uncertainty artificial intelligence los angeles ca pages 
morgan kaufmann 
buntine buntine 

learning graphical model 
technical report fia nasa ame 
chickering chickering 

learning bayesian networks np complete 
submitted lecture notes statistics 
cooper herskovits cooper herskovits 

bayesian method induction probabilistic networks data 
machine learning 
friedman friedman 

bias variance loss curse dimensionality 
data mining knowledge discovery 
heckerman heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
proceedings tenth conference uncertainty artificial intelligence seattle wa pages 
morgan kaufman 
heckerman heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
machine learning 
spiegelhalter spiegelhalter dawid lauritzen cowell 

bayesian analysis expert systems 
statistical science 
thiesson thiesson 

score information recursive exponential models incomplete data 
technical report institute electronic systems aalborg university aalborg denmark 
friedman goldszmidt koller 

independence bayesian networks 
proceedings twelfth conference uncertainty artificial intelligence portland pages 
morgan kaufmann 
friedman olshen stone 

classification regression trees 
wadsworth 
