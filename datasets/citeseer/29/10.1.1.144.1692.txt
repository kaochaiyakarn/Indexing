esann proceedings european symposium artificial neural networks bruges belgium april facto public isbn pp 
support vector machines multi class pattern recognition weston watkins department computer science royal holloway university london egham surrey tw ex uk dcs rhbnc ac uk 
solution binary classi cation problems support vector machines svms developed multi class problems classes typically solved combining independently produced binary classi ers 
propose formulation svm enables multi class pattern recognition problem solved single optimisation 
propose similar generalization linear programming machines 
report experiments bench mark datasets methods achieve reduction number support vectors kernel calculations needed 

class pattern recognition class pattern recognition problem construct decision function iid independent identically distributed samples points unknown function typically noise xi vector length yi kg represents class sample 
natural loss function number mistakes 

solving class problems binary svms binary pattern recognition problem case support vector approach developed 
classical approach solving class pattern recognition problems consider problem collection binary classi cation problems 
versus rest method constructs classi ers class 
th classi er constructs hyperplane class classes 
particular point assigned class distance margin positive direction direction class lies class rest maximal 
method widely esann proceedings european symposium artificial neural networks bruges belgium april facto public isbn pp 
support vector literature solve multi class pattern recognition problems see example :10.1.1.42.1588
alternatively hyperplanes constructed versus method separating class class decision function constructed ad hoc voting system 

class support vector machines natural way solve class problems construct piecewise linear separation classes single optimisation 
binary svm optimisation problem generalised minimise subject kx wm wm yi xi wm xi bm gives decision function arg max wn bn note rst formulation optimisation problem reduces exactly binary svm solution ifwe take pattern class pattern class 
second decision function form powerful set versus rest binary classi ers sense possible construct multi class datasets separated perfectly decision rule type training data classi ed error rest 
example consider classes lie inside segments dimensional sphere see details 
nd solution optimisation problem dual variables nding saddle point lagrangian lx kx kx wm wm lx kx wm xi bm lx kx esann proceedings european symposium artificial neural networks bruges belgium april facto public isbn pp 
dummy variables constraints yi yi yi maximised respect minimised respect introducing notation ai kx yi yi various manipulations omitted due lack space arrive maximise yi xi xj quadratic function terms alpha linear constraints ai yi nd values bn solving set simultaneous equations kuhn tucker optimality conditions obtaining values dual variables interior point optimizer noted smola scholkopf 
obtains decision function arg max ai xi bn usual inner products xi xj replaced generalised inner product xi xj see 

class linear programming machines consider generalisation linear programming machine method 
minimise linear program yi esann proceedings european symposium artificial neural networks bruges belgium april facto public isbn pp 
subject ym yi mk xi xm yn yj nk xi xn decision rule arg max yi ik xi bn formulation coe cients independent number classes methods coe cients 
furthermore regularization directly attempts reduce number non zero coe cients 

analysis binary svms expectation probability error test example bounded ratio expectation number training points support vectors number examples training set error number training points support vectors number training vectors bound holds multi class case voting scheme methods rest multi class support vector method 
seen noting training point support vector classi ed correctly left training set 
note means interested size union set support vectors de ne hyperplane classi er equivalent calculations required sum sizes sets 
secondly noting solution rest method feasible solution multi class method necessarily optimal 
seen considering rest method formal step 
hard margin case required minimize wm wm constraints xi wj xi bj 
see constraints satis ed constraints 
means multi class method lower value wm wm small value binary case corresponds low vc dimension large margin mean generalization 
esann proceedings european symposium artificial neural networks bruges belgium april facto public isbn pp 
table comparison error rates name pts atts class mc sv mc lp iris wine glass vowel postal table comparison number support vectors mc sv mc lp name svs svs svs svs iris wine glass vowel postal 
simulations tested method collection benchmark problems uci machine learning repository test set provided data split randomly times tenth data test set 
performance multi class support vector method mc sv multi class linear programming method mc lp compared rest versus binary classi cation sv methods 
enable comparison decision functions constructed hard margin parameter radial basis function kernel algorithm 
results summarised tables 
mc sv performed comparably voting scheme methods smaller number non zero coe cients svs kernel calculations note means lower values upper bound generalization error 
mc lp signi cantly reduced number svs equivalent inner product associated multiplier url www ics uci edu mlearn mlrepository html 
training set postal rst examples postal service database lecun testing set 
number kernel calculations usually number support vectors algorithms cached support vectors kernel calculation 
esann proceedings european symposium artificial neural networks bruges belgium april facto public isbn pp 
achieve generalization ability 
realized due di erence decision rules methods di erent number basis functions decision rule may perform di erent choices kernel methods kept choice xed 

described new methods solving multi class pattern recognition problems support vector machines 
results obtained benchmark datasets suggest new methods reduce number support vectors kernel computations 
construct examples new methods separate data voting scheme methods 
re ected error rates datasets 
communication vapnik blanz discovered independently derived multi class support vector method described article 
providing nancial support gr support vector bayesian learning algorithms 
mark stitson writing support vector implementation experiments 
bennett mangasarian 
multicategory discrimination linear programming 
optimization methods software 
blanz scholkopf burges vapnik vetter 
comparison view object recognition algorithms realistic models 
arti cial neural networks icann pages berlin 
springer lecture notes computer science vol 

cortes vapnik 
support vector networks 
machine learning 
scholkopf burges vapnik :10.1.1.42.1588
extracting support data task 
fayyad uthurusamy editors proceedings international conference knowledge discovery data mining 
aaai press menlo park ca 
vapnik 
nature statistical learning theory 
springer verlag new york 
vapnik 
statistical learning theory 
wiley new york 
