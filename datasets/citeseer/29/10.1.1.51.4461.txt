constructive theory refinement knowledge neural networks parekh vasant honavar artificial intelligence research group department computer science hall iowa state university ames ia 
cs iastate edu knowledge artificial neural networks offer approach connectionist theory refinement 
algorithm refining extending domain theory incorporated knowledge neural network constructive neural network learning algorithms 
initial domain theory comprising propositional rules translated knowledge network threshold logic units tlu 
domain theory modified dynamically adding neurons existing network 
constructive neural network learning algorithm add train additional neurons sequence labeled examples 
presence domain specific prior knowledge domain theories concept learned greatly enhance performance inductive learning system 
prior knowledge assist cutting training time result system superior generalization capability compared system learned inductively examples 
prior knowledge inaccurate incomplete 
inductive theory refinement process modifying domain theories help labeled examples 
inductive theory refinement systems proposed literature broadly classified symbolic inductive logic programming connectionist approaches depending underlying learning model :10.1.1.23.1676:10.1.1.29.2069:10.1.1.54.4913
focus connectionist theory refinement approaches 
knowledge artificial neural networks offer attractive framework combining domain specific prior knowledge usually form propositional rules connectionist inductive learning :10.1.1.51.3526:10.1.1.54.4913
knowledge neural networks find applications datamining initial domain theory available 
kbann algorithm proposed towell shavlik provides framework translating initial domain knowledge called domain theory neural network topology refines theory backpropagation learning algorithm 
prior knowledge inaccurate incomplete 
inductive theory refinement process modifying domain theories help labeled examples 
inductive theory refinement systems proposed literature broadly classified symbolic inductive logic programming connectionist approaches depending underlying learning model :10.1.1.23.1676:10.1.1.29.2069:10.1.1.54.4913
focus connectionist theory refinement approaches 
knowledge artificial neural networks offer attractive framework combining domain specific prior knowledge usually form propositional rules connectionist inductive learning :10.1.1.51.3526:10.1.1.54.4913
knowledge neural networks find applications datamining initial domain theory available 
kbann algorithm proposed towell shavlik provides framework translating initial domain knowledge called domain theory neural network topology refines theory backpropagation learning algorithm 
practice domain theories incomplete inaccurate 
kbann limited fact modify network topology 
kbann algorithm proposed towell shavlik provides framework translating initial domain knowledge called domain theory neural network topology refines theory backpropagation learning algorithm 
practice domain theories incomplete inaccurate 
kbann limited fact modify network topology 
prevents incorporation new rules restricts algorithm ability compensate existing inaccuracies 
constructive neural network learning algorithms offer approach dynamically constructing near minimal neural network architectures pattern classification tasks :10.1.1.16.9105:10.1.1.51.3055
constructive algorithms offer advantages conventional backpropagation style learning approaches ffl obviate need ad hoc priori choice network topology 
ffl guaranteed converge zero classification errors finite non contradictory datasets 
ffl elementary threshold logic units tlu trained perceptron style weight update rules 
ffl involve extensive parameter fine tuning 
ffl guaranteed converge zero classification errors finite non contradictory datasets 
ffl elementary threshold logic units tlu trained perceptron style weight update rules 
ffl involve extensive parameter fine tuning 
constructive algorithms lend design knowledge neural networks 
domain theory translated initial network topology comprising tlu :10.1.1.54.4913:10.1.1.51.3055
new rules incorporated inaccuracy existing rules corrected dynamically adding new network 
new trained sequence labeled examples 
discuss approach constructive learning algorithms knowledge neural networks 
section ii places perspective related knowledge neural networks 
section iv describes results experiments non trivial datasets viz 
financial advisor 
section concludes summary gives directions research 
ii 
related constructive learning method dynamically adding neurons initial knowledge network studied fletcher :10.1.1.51.3055
approach starts initial network representing domain theory modifies theory training single hidden layer tlu labeled training data 
resultant network topology depicted fig 

method uses hyperplane detection examples construct single hidden layer trains new output unit pocket algorithm 
constructing single hidden layer allow constructive learning algorithm build network hidden layers necessary initial network representing domain theory 
fig 

provides general framework incorporating domain knowledge constructive neural network learning algorithm 
performance different constructive learning algorithms differs quite significantly different datasets advantageous scheme allows construction appropriate network topology augment initial network limiting network construction single hidden layer :10.1.1.16.9105:10.1.1.51.3055
neural network domain theory input units constructive learning fig 

constructive learning knowledge networks opitz shavlik extensively studied constructive approaches theory refinement :10.1.1.121.3039:10.1.1.48.2048
topgen algorithm introduces new rule kbann heuristically searching possible ways adding neurons determine network topology best refines initial domain theory 
provides general framework incorporating domain knowledge constructive neural network learning algorithm 
performance different constructive learning algorithms differs quite significantly different datasets advantageous scheme allows construction appropriate network topology augment initial network limiting network construction single hidden layer :10.1.1.16.9105:10.1.1.51.3055
neural network domain theory input units constructive learning fig 

constructive learning knowledge networks opitz shavlik extensively studied constructive approaches theory refinement :10.1.1.121.3039:10.1.1.48.2048
topgen algorithm introduces new rule kbann heuristically searching possible ways adding neurons determine network topology best refines initial domain theory 
topgen performs beam search space network architectures 
domain theory translated initial network called kbann 
kbann trained backpropagation placed queue 
topgen regent evaluated datasets human genome project perform better compared standard backpropagation kbann algorithm 
approach considerably simpler topgen regent 
construct single network tlu population networks constructed topgen regent 
impact training time knowledge networks 
topgen regent taken days search networks report best :10.1.1.48.2048
hand approach requires minutes cpu time training 
related issue training time topgen regent expensive backpropagation style training simple perceptron type weight update rules approach 
backpropagation algorithm effective networks large number layers propagated error tends diffuse considerably layer 
topgen regent allow weight changes part network incorporates original domain theory 
domain theory revision performed constructively adding new neurons network training 
approach preserves original domain theory 
datasets available university wisconsin madison site www ftp ftp cs wisc edu shavlik group datasets 
iii 
constructive knowledge neural network learning algorithm embedding domain theory neural network symbolic knowledge encoding procedure translates domain theory form propositional rules network tlu :10.1.1.54.4913:10.1.1.51.3055
weights individual tlu chosen satisfy rules 
example consider simple financial advisor rule base due table sav adeq adeq invest stocks dep sav adeq sav adeq assets hi sav adeq dep adeq earn steady adeq debt lo adeq sav dep dep sav adeq assets income assets hi income dep dep adeq debt income debt lo table financial advisor rule base fig 
depicts initial network topology corresponding domain theory 
tlu network computes bipolar function tlu outputs gamma weighted sum inputs 
tlu network computes bipolar function tlu outputs gamma weighted sum inputs 
neurons hidden layer encode rules rule base 
lone tlu second hidden layer computes logical function encodes rule 
constructive neural network learning algorithm constructive neural network learning algorithm augment initial network topology 
approach allows commonly constructive neural network learning algorithms :10.1.1.16.9105:10.1.1.51.3055
purpose experiments novel hybrid algorithm combines features tiling pyramid learning algorithms 
tiling algorithm constructs strictly layered network 
layer maintains master neuron responsible classifying training patterns 
master neuron correctly classify training patterns ancillary neurons added trained achieve faithful representation pattern set 
faithful layer constructed tiling algorithm trains master neuron layer 
master neuron connected neurons previous layer 
new layer similarly trained achieves faithful representation patterns 
convergence algorithm demonstrated showing successive master neuron reduces number mis classifications training set 
interested reader referred detailed description tiling algorithm extensions handle pattern sets real valued attributes multiple output categories :10.1.1.16.9105:10.1.1.51.3055
pyramid algorithm successively adds new layers network 
newly added layer network new output layer 
neurons new layer connected input layer previously added layers network 
convergence pyramid algorithm proved demonstrating total classification error training set decreases added layer 
neurons new layer connected input layer previously added layers network 
convergence pyramid algorithm proved demonstrating total classification error training set decreases added layer 
original pyramid algorithm relies patterns binary bipolar attributes 
extension pyramid algorithm handle real valued attributes requires pre processing pattern set 
individual patterns normalized projected surface guarantee convergence :10.1.1.16.9105:10.1.1.51.3055
alternatively continuous valued features discretized 
discretization quantization involves mapping pattern space equivalent bipolar binary representation gamma typically 
hybrid constructive learning algorithm careful observation operation tiling algorithm indicates layer tiling algorithms performs quantization input patterns 
outputs layer discrete valued gamma 
recognition binding sites dna studied part human genome project 
dataset comprises imperfect domain theory income dependents assets savings earn steady debt assets hi adeq invest stocks savings adeq dep sav adeq dep adeq debt lo fig 

embedding financial advisor domain theory neural network developed dna expert set labeled examples indicating particular dna sequence contains binding site 
financial advisor rule base shown table set patterns training testing correctly match financial advisor rule base randomly generated case experiments performed fletcher :10.1.1.51.3055
hybrid tiling pyramid constructive learning algorithm augment initial domain knowledge 
hybrid network trained thermal perceptron learning rule 
tlu trained epochs initial weights chosen randomly gamma initial temperature thermal perceptron set 
binding sites dataset contains imperfect domain theory 
fold cross validation exactly folds experiments performed opitz shavlik augment domain theory 
fold cross validation generalization accuracy measured network representing imperfect domain theory prior performing theory refinement 
report average generalization accuracy average network size standard deviations runs table ii 
seen theory refinement provide improvement generalization accuracy 
compare results results topgen regent reported :10.1.1.121.3039
note standard deviations available experiments topgen regent 
tiling pyramid topgen regent test size test size test size sigma sigma table ii experiments dataset financial advisor incomplete knowledge financial advisor rule base modeled pruning certain rules antecedents rule base 
incomplete domain theory augmented constructive learning 
experiments follow performed fletcher :10.1.1.51.3055
compare results results topgen regent reported :10.1.1.121.3039
note standard deviations available experiments topgen regent 
tiling pyramid topgen regent test size test size test size sigma sigma table ii experiments dataset financial advisor incomplete knowledge financial advisor rule base modeled pruning certain rules antecedents rule base 
incomplete domain theory augmented constructive learning 
experiments follow performed fletcher :10.1.1.51.3055
table iii summarize average generalization test patterns average network size runs different pruning points 
generalization accuracy corresponding network prior theory refinement rules reported 
assets hi rule see substantial increase generalization accuracy theory refinement 
generalization accuracy network assets hi rule significantly high slight generalization performance refining theory assets hi rule pruned attributed fitting 
table iii summarize average generalization test patterns average network size runs different pruning points 
generalization accuracy corresponding network prior theory refinement rules reported 
assets hi rule see substantial increase generalization accuracy theory refinement 
generalization accuracy network assets hi rule significantly high slight generalization performance refining theory assets hi rule pruned attributed fitting 
table iv results experiments note standard deviations results experiments available :10.1.1.51.3055
pruning point tiling pyramid rules test size test dep sav adeq sigma sigma assets hi sigma sigma dep adeq sigma sigma debt lo sigma sigma sav adeq sigma sigma adeq sigma sigma table iii experiments financial advisor rule base pruning point rules test hidden units test constructed dep sav adeq assets hi dep adeq debt lo sav adeq adeq table iv experiments financial advisor rule base summary discussion approach constructive learning knowledge networks 
knowledge networks allow domain theory embedded initial neural network architecture 
approach uses constructive learning algorithm dynamically add train augment initial domain theory 
experimental results demonstrate performance hybrid tiling pyramid algorithm compares favorably algorithm financial advisor rule base terms generalization accuracy network size 
type rules incorporated network limited propositional rules 
mechanism handling uncertainty rules 
extension knowledge neural networks handle rules predicate logic handle uncertainty adjusting weights individual connections clearly interest 
research partially supported national science foundation iri iri vasant honavar 
ourston mooney theory refinement combining analytical empirical methods artificial intelligence vol :10.1.1.29.2069
pp 

thompson langley iba background knowledge concept formation proceedings eighth international machine learning workshop pp 

pp 

thompson langley iba background knowledge concept formation proceedings eighth international machine learning workshop pp 

muggleton inductive logic programming academic press san diego :10.1.1.23.1676
towell shavlik refinement approximate domain theories knowledge neural networks proceedings eighth national conference artificial intelligence boston ma pp :10.1.1.54.4913

fu integration neural heuristics knowledge inference connection science vol 
pp 

thompson langley iba background knowledge concept formation proceedings eighth international machine learning workshop pp 

muggleton inductive logic programming academic press san diego :10.1.1.23.1676
towell shavlik refinement approximate domain theories knowledge neural networks proceedings eighth national conference artificial intelligence boston ma pp :10.1.1.54.4913

fu integration neural heuristics knowledge inference connection science vol 
pp 

pp 

fu knowledge connectionism refining domain theories ieee transactions systems man cybernetics vol 

towell shavlik knowledge artificial neural networks artificial intelligence vol :10.1.1.51.3526
pp 

rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations microstructure cognition vol 
foundations 
honavar generative learning structures processes generalized connectionist networks ph thesis university wisconsin madison 
honavar uhr generative learning structures processes information sciences vol 
pp 

parekh yang honavar constructive neural network learning algorithms multi category real valued pattern classification tech :10.1.1.16.9105:10.1.1.51.3055
rep cs tr department computer science iowa state university submitted review ieee transactions neural networks 
fletcher combining prior symbolic knowledge constructive neural rk learning connection science vol :10.1.1.51.3055
pp 

pp 

parekh yang honavar constructive neural network learning algorithms multi category real valued pattern classification tech :10.1.1.16.9105:10.1.1.51.3055
rep cs tr department computer science iowa state university submitted review ieee transactions neural networks 
fletcher combining prior symbolic knowledge constructive neural rk learning connection science vol :10.1.1.51.3055
pp 

gallant perceptron learning algorithms ieee transactions neural networks vol 
pp 

gallant perceptron learning algorithms ieee transactions neural networks vol 
pp 
june 
opitz shavlik dynamically adding symbolically meaningful nodes knowledge neural networks knowledge systems vol :10.1.1.48.2048
pp 

opitz shavlik connectionist theory refinement genetically searching space network topologies journal artificial intelligence research vol :10.1.1.121.3039
pp 
june 
opitz shavlik dynamically adding symbolically meaningful nodes knowledge neural networks knowledge systems vol :10.1.1.48.2048
pp 

opitz shavlik connectionist theory refinement genetically searching space network topologies journal artificial intelligence research vol :10.1.1.121.3039
pp 

artificial intelligence design expert systems benjamin cummings redwood city ca 
nadal learning feed forward networks tiling algorithm phys 
