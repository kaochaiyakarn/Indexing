model search combinatorial optimization comparative study mark marco dorigo dept computer science technion israel institute technology haifa israel cs technion ac il iridia universit libre de bruxelles brussels belgium ulb ac introduce model search unifying framework accommodating proposed heuristics combinatorial optimization ant colony optimization stochastic gradient ascent cross entropy estimation distribution methods 
discuss similarities distinctive features method propose extensions comparative experimental study algorithms 
necessity solve np hard problems existence efficient exact algorithms highly led wide range heuristic algorithms implement sort search solution space 
heuristic algorithms classified similarly done machine learning field instance model 
classical search methods may considered instance generate new candidate solutions solely current solution current population solutions 
typical representatives class genetic algorithms local search variants example simulated annealing iterated local search hand decade new methods may classified model search mbs algorithms proposed 
mbs algorithms candidate solutions generated parameterized probabilistic model updated previously seen solutions way search concentrate regions containing high quality solutions 
mbs approaches ant colony optimization aco metaheuristic stochastic gradient ascent sga cross entropy ce method estimation distribution algorithms edas considered common framework analysis similarities distinctive features provided 
carried author iridia universit libre de bruxelles belgium 
merelo 
eds ppsn vii lncs pp 

springer verlag berlin heidelberg mark marco dorigo provide detailed comparative analysis algorithms context unconstrained binary coded problems 
addition analytical comparison experimental comparison mbs methods discussed maxsat problem test bed 
model search consider minimization problem set feasible solutions objective function assigns solution sa cost value 
goal minimization problem find feasible solution minimal cost 
general level model search approach attempts solve minimization problem repeating steps candidate solutions constructed parameterized probabilistic model parameterized probability distribution solution space 
candidate solutions modify model way deemed bias sampling high quality solutions 
algorithm belonging general scheme components corresponding steps need instantiated probabilistic model allows efficient generation candidate solutions 
rule updating model parameters 
restrict attention problems solutions coded unconstrained binary strings aco sga ce applied general context 
general discussion methods relations 
methods described employ probabilistic model generating candidate solutions bit position associated parameter bits si generated independently si parameter vector resulting probability distribution solution space denoted pt correspondingly 
parameters typically initialized way initial distribution uniform 
noted algorithms fall mbs framework allow dependencies bit positions described literature mimic boa variants aco 
algorithms reported yield certain improvement simpler algorithms generate bits different positions independently 
fact algorithms considered sga simply model search combinatorial optimization comparisons performed basis equal number iterations equal computational time 
hand personal experience results literature suggest considerable computational overhead imposed complex model renders dependencies learning algorithms 
consequently algorithms considered 
algorithms section give brief description existing mbs algorithms discuss relationships 
reader referred detailed discussion 
emphasized probabilistic model employed algorithms difference way parameters interpreted modified 
ant colony optimization established approach belongs mbs framework ant colony optimization aco metaheuristic 
aco algorithms solutions generated stochastic procedures called artificial ants construct iteratively adding solution components 
components chosen probability function called pheromone values associated components 
constructing solutions pheromone values associated components belonging solutions increased 
metaheuristic successfully applied solution numerous np hard problems time varying stochastic optimization problems 
particular variant metaheuristic called hyper cube hc aco proposed context combinatorial problems binary coded solutions 
hc aco pheromones bounded zero pheromone update rule described general scheme hc aco update st qf si st qf 
st sample th iteration learning rate qf st quality function typically required non increasing respect defined set st considered case unconstrained problems pheromones equal marginal probabilities corresponding positions bits different positions assigned independently preliminary study considered heuristic information similarly 
improvement performance negligible compared improvement obtained local search 
decided limit discussion simpler version hc aco 
mark marco dorigo different aco algorithms may different quality functions sets 
example aco algorithm ant system quality function set st st proposed scheme called iteration best update set singleton containing best solution st iteration best solutions chosen randomly :10.1.1.145.8411
global best update set contained best iteration best solutions global best solution earliest chosen 
max min ant system maximum minimum pheromone trail limits introduced 
modification probability generate particular solution kept positive threshold helps preventing search stagnation premature convergence suboptimal solutions 
hc aco approach translates requirement marginal probabilities kept range parameter controls amount exploration 
worth noting shown learning rate particular choice quality function hc aco equivalent cross entropy method 
stochastic gradient ascent method updates described somewhat heuristic nature sga method allows derive parameters update rule principled manner 
sga method replaces original optimization problem equivalent continuous maximization problem qf argmax denotes expectation respect pt maximization problem turn tackled stochastic gradient ascent qf ln pt st st sample iteration demonstrated required gradient ln pt calculated general class probabilistic models 
model consider binary variables dependencies different positions verified resulting parameter update rule sga update qf si si st order guarantee stability resulting algorithm desirable bounded gradient ln pt 
reason natural model search combinatorial optimization representation inappropriate 
suggest logistic function shown case update exp rule qf qf si st gradient bounded 
sga method originally formulated iteration independent quality functions demonstrated alternative derivation sga update ce method justifies iteration dependent quality functions 
example may indicator function qf threshold value set percentile say lower minimization problems cost distribution iteration 
quality function expectation qf equals probability generating solutions cost threshold threshold modified adaptively 
update function henceforth referred top quality update 
similarly hc aco may choose bound marginal probabilities order increase amount exploration 
order keep marginal probabilities logistic function representation described st pheromones kept range ln ln 
estimation distribution algorithms mentioned section classical genetic algorithms considered example instance approach search carried evolving population candidate solutions selection crossover mutation operators 
new algorithms generate new solutions probabilistic models crossover mutation proposed evolutionary computation community 
population incremental learning pbil algorithm population replaced probability vector pi initially set 
iteration sample generated probability vector probability vector updated follows pbil update fixed number lowest cost solutions pi pi si learning rate 
easily seen update virtually identical rescaling learning rate hc aco top quality update 
particular case best solution update hc aco iteration best update obtained 
additional updates suggested 
intended negative examples shifting probability vector best solution positions differs worst solution mark marco dorigo best pi nl pi best worst best worst respectively best worst solutions nl called negative learning rate 
second update probability vector randomly perturbed effect similar mutation standard ga modify pi mut pi probability 
mut mutation shift mutation direction di probability 
updates performed addition basic pbil update described 
compact genetic algorithm cga proposed modification pbil intended represent faithfully dynamics real ga algorithm :10.1.1.46.5188
iteration solutions generated probability vector probability vector updated follows assuming loss generality lower cost cga update ai bi ai pi pi pi pi parameter equivalent population size classical ga basic scheme extended larger samples 
variants proposed :10.1.1.46.5188
variant intended simulate tournament size sample size generated basic update pair set 
second variant round robin tournament simulated basic update pair solutions sample 
shown update tournament size cga written pi pi si si best 
round robin tournament cga shown update described rank highest rank assigned 
easily verified updates virtually identical hc aco iteration best rank updates respectively 
difference cga hc aco form evaporation factor 
cga equal si hc aco equal pi simply expected value 
empirical comparison model search combinatorial optimization section describe results empirical comparison mbs algorithms described maxsat test bed 
maxsat optimization variant sat problem shown npcomplete 
weighted maxsat problem formulated follows 
clauses ck binary variables xn weights wk find assignment maximizes sum weights satisfied clauses 
comparison setting comparison carried randomly generated weighted maxsat instances satlib maxsat benchmark collection 
benchmark set contains groups problems variables clauses respectively 
group contains instances 
algorithms evaluated different running times 
problem size stopping times chosen time takes hc aco population size perform iterations respectively algorithms described parameters choice clearly affect performance algorithm 
optimal parameter setting may depend available computational time shorter times higher learning rate smaller samples appropriate problem size 
date established methods automatic tuning metaheuristics parameters decided problem group tuning algorithms testing 
specifically algorithm run variety parameter settings described times setting algorithm running time combination configuration best average performance chosen 
automatic tuning procedure insures comparison biased favor algorithms 
separation training problem test problems guarantees statistical validity performance estimates algorithms considered learning rate sample sizes 
hc aco sga considered pheromone bounds 
pbil additional parameters negative learning rate nl mutation probability mutation shift mut 
noted due extensive code reuse algorithms implementation running times algorithms local search virtually 
example chose best performing configuration problem individually resulting average performance longer unbiased estimate actual expected performance 
mark marco dorigo hc aco sga pbil tested iteration best top quality updates 
tested separately basic ce method corresponds top quality hc aco algorithm learning rate bounds probabilities 
single tournament round robin tournament versions cga method tested 
furthermore algorithm described considered hybrid version update population elite solutions highest quality solutions far sample 
algorithms including hybrid versions tested local search comparison results algorithm run times problem parameter setting determined tuning procedure described previous section 
optimal solution costs benchmark problems study known cost best solution algorithms test runs estimates 
performance single run algorithm evaluated cost best solution run estimate optimal solution cost expected cost solution generated uniform distribution note random solution results coming different problems put scale allows meaningful averaging problems 
results comparison summarized tables 
column corresponds comparison particular problem size stopping time 
average score empirically best algorithm printed bold typeface results worse best confidence shown italic 
local search leads drastic improvement performance results table multiplied clarity presentation 
superscript denotes hybrid versions algorithms augmented population denotes problem size measured number variables 
local search cases round robin tournament cga produces significantly better results 
performance cga local search common practice eda research field results reported indicate certainly considered 
clause variables proportion non satisfying assignments verified maxsat problems benchmarks wj 
statistical analysis performed tukey kramer test modification test adapted multiple comparisons 
model search combinatorial optimization table 
average performance algorithm local search 
ce ce hc hc aco top hc hc aco best sga top sga best pbil top pbil best cga rr cga st relatively poor recall expected performance score randomly generated solution 
local search leads improvement orders magnitude significant differences algorithms case note cga significantly worse 
hypothesize differences algorithms local search just artifact particular tuning procedure evidence advantage methods 
topic ongoing research 
decade new approach solving combinatorial optimization problems emerging observed 
approach refer model search mbs tackles combinatorial optimization problem sampling solution space probabilistic model adaptively modified search proceeds 
comparative analysis existing mbs methods construct binary coded solutions generating bit independently 
theoretical analysis revealed considerable structural similarity algorithms empirical comparison showed actual performance algorithms quite similar especially algorithms hybridized local search 
hope extend analysis general class mbs algorithms compare algorithms different types combinatorial optimization problems 
mark marco dorigo table 
average performance multiplied algorithm local search 
ce ce hc hc aco top hc hc aco best sga top sga best pbil top pbil best cga rr cga st acknowledgments mark supported training site fellowship funded improving human potential programme commission european community cec ct 
marco dorigo acknowledges support belgian senior research associate 
generally partially supported metaheuristics network research training network funded improving human potential programme cec ct 
information provided sole responsibility authors reflect community opinion 
community responsible data appearing publication 

baluja caruana 
removing genetics standard genetic algorithm 
proceedings icml pages 
morgan kaufmann publishers palo alto ca 

blum roli dorigo 
hc aco hyper cube framework ant colony optimization 
proceedings mic volume pages porto portugal 

de bonet isbell viola 
mimic finding optima estimating probability densities 
proceedings nips pages 
mit press cambridge ma 
model search combinatorial optimization 
di caro dorigo 
antnet distributed stigmergetic control communications networks 
journal artificial intelligence research 

dorigo 
ed su 
phd thesis dipartimento di elettronica politecnico di milano milan italy 

dorigo di caro 
ant colony optimization meta heuristic 
corne dorigo glover editors new ideas optimization pages 
mcgraw hill london uk 

dorigo gambardella 
ant colony system cooperative learning approach traveling salesman problem 
ieee trans 
evol 
comp 

dorigo meuleau birattari 
updating aco pheromones stochastic gradient ascent cross entropy methods 
proceedings pages 
springer verlag berlin germany 

harik lobo goldberg 
compact genetic algorithm 
ieee trans 
evol 
comp 

hoos st tzle 
randomly generated benchmark problems maxsat 
technical note department computer science university british columbia march 

larra aga lozano 
estimation distribution algorithms 
new tool evolutionary computation 
kluwer academic publishers 

meuleau dorigo 
ant colony optimization stochastic gradient descent 
artificial life 

ramat 
similarities bsc pbil birth new meta heuristic 
technical report laboratoire informatique universit de tours 

pelikan goldberg cant paz 
boa bayesian optimization algorithm 
proceedings gecco volume pages 
morgan kaufmann publishers san francisco ca 

quinlan 
combining instance model learning 
proceedings twelfth international conference machine learning ml pages 
morgan kaufmann publishers san mateo ca 

robbins monro 
stochastic approximation method 
annals mathematical statistics 

roli blum dorigo 
aco maximal constraint satisfaction problems 
proceedings mic volume pages porto portugal 

rubinstein 
cross entropy method combinatorial continuous optimization 
methodology computing applied probability 

st tzle hoos 
max min ant system 
generation computer systems 

birattari meuleau dorigo 
model search combinatorial optimization 
technical report tr iridia iridia universit libre de bruxelles 
