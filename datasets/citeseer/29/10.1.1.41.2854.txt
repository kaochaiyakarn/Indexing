information extraction hmms shrinkage dayne freitag dayne justresearch com andrew mccallum mccallum justresearch com just research henry street pittsburgh pa hidden markov models hmms powerful probabilistic tool modeling time series data applied success language related tasks part speech tagging speech recognition text segmentation topic detection 
describes application hmms language related task information extraction problem locating textual sub segments answer particular information need 
hmm state transition probabilities word emission probabilities learned labeled training data 
machine learning problems lack sucient labeled training data hinders reliability model 
key contribution statistical technique called shrinkage signi cantly improves parameter estimation hmm emission probabilities face sparse training data 
experiments seminar announcements reuters acquisitions articles shrinkage shown reduce error resulting hmm outperforms state art rule learning system 
information extraction process lling elds database automatically extracting sub sequences human readable text 
examples include extracting location meeting email message extracting name acquired newswire article 
advocates hidden markov models hmms information extraction 
hmms type probabilistic nite state machine probabilistic tool modeling sequences observations 
applied signi cant success language related tasks including part speech tagging kupiec speech recognition rabiner text segmentation topic detection van :10.1.1.131.2084
hmms foundations statistical theory rich body established techniques learning parameters hmm training data classifying test data 
hmm probabilities word emission probabilities learned labeled training data 
case machine learning problems large amounts training data required learn model generalizes high accuracy 
training data usually labeled hand dicult obtain small quantities available training data limiting factor performance learned extractor 
hmm subset states distinguished target states words document determined generated states part extracted sub sequence 
reports described systems break documents small fragments purposes learning prediction 
approaches require unsatisfactory heuristic choices result huge space possible fragments 
approach ectively solves fragment enumeration problem means viterbi algorithm ecient method nding probable sequence model states corresponding document 
previous projects hmms information extraction di ering structures shrinkage hmm states leek bikel :10.1.1.51.4029
related project focuses quite di erent task learning hmm state transition structure information extraction seymore mccallum rosenfeld 
describe experiments real world data sets line seminar announcements reuters newswire articles acquisitions 
results show shrinkage consistently improves performance absolute discounting 
representative hmm outperforms state art rule learning system extraction tasks 
imagine article result stochastic process involving unigram language models background model typically emits tokens said model speci prices typically emits tokens 
generate document process emits tokens background model point switching price model tokens returning background model complete document 
slightly realistically number specialized models process draws devoted context purchasing price models responsible pre fragments purchased xyz 
hmms represent precisely kind process 
hmm nite state automaton stochastic state transitions symbol emissions rabiner :10.1.1.131.2084
automaton models probabilistic generative processes sequence symbols produced starting designated start state transitioning new state emitting symbol selected state transitioning emitting symbol designated nal state reached 
associated set states fs 
probability distribution symbols emission vocabulary fw probability state emit vocabulary item written 
similarly associated state distribution set outgoing transitions 
em begins initializing initial values say iterating steps change step calculate degree node predicts words state held set js js step derive new guaranteed improved weights normalizing correct conceptually simple method available training data held set 
problem evaluating step individual word occurrence held turn 
method similar leave cross validation commonly statistical estimation 
experiments experimental results information extraction problems corpora collection seminar announcements posted local newsgroups large university collection articles describing corporate acquisitions taken reuters dataset lewis 
datasets problems de ned described detail previously published freitag :10.1.1.41.5985
performance algorithm measured document document 
task extract start time seminar announcement assume single correct answer di erent times super cially different ways 
ask learner single best speaker location stime etime window window window table ect performance changing window size number context states seminar announcement elds absolute discounting 
speaker location stime etime uniform global hier 
compared earlier model see universal increase performance increase performance location eld 
eld performance hmm far better non hmm learning methods seen 
combination shrinkage appropriately designed topologies yields learning algorithm sound statistical principles runs eciently performs level competitive better best learning algorithms experimented 
table compares hmm performance speaker location stime etime srv hmm acquired purchaser acqabr dlramt status srv hmm table srv representative hmm elds domains seminar announcements corporate acquisitions 
information extraction problems srv consistently strong rule learning algorithm described freitag :10.1.1.41.5985
model question context window size target paths uses global shrinkage con guration 
etime acquired hmm obtains higher score srv 
note etime performance uniform shrinkage con guration beat srv see table 
related hmms suited range language processing tasks previously applied problem information extraction ways di er approach various ways 
related hmms suited range language processing tasks previously applied problem information extraction ways di er approach various ways 
related project seymore mccallum rosenfeld ort learn hmms state transition structure 
approach uses single hmm extract elds densely packed moderately structured text research headers 
elds represented single hmm observed expected relative orderings elds captured structure learning challenging necessary task 
leek applies hmms problem extracting gene locations biomedical texts leek :10.1.1.51.4029
contrast models study leek models carefully engineered task hand general topology hierarchical complex language models individual states 
leek uses network topology model natural language syntax training examples presentation model 
states unigram language models unclear smoothing policy 
unknown tokens handled special gap states 
