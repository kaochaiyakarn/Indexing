colored maximum variance unfolding le song alex smola karsten arthur national ict australia canberra australia university cambridge cambridge united kingdom mpi biological cybernetics bingen germany le song alex smola com au eng cam ac uk arthur tuebingen mpg de maximum variance unfolding mvu effective heuristic dimensionality reduction 
produces low dimensional representation data maximizing variance embeddings preserving local distances original data 
show mvu optimizes statistical dependence measure aims retain identity individual observations constraints 
general view allows design colored variants mvu produce low dimensional representations task subject class labels side information 
years maximum variance unfolding mvu introduced saul gained popularity method dimensionality reduction 
method simple heuristic maximizing variance embedding preserving local distances neighboring observations 
sun show dual connection mvu goal finding fast mixing markov chain 
connection intriguing 
offers limited insight mvu data representation 
provides statistical interpretation mvu 
show algorithm attempts extract features data simultaneously preserve identity individual observations local distance structure 
reasoning relies dependence measure sets observations hilbert schmidt independence criterion 
relaxing requirement retaining maximal information individual observations able obtain colored mvu 
traditional mvu takes source information account colored mvu allows integrate sources information single embedding 
able find embedding leverages goals preserve local distance structure source information data maximally align second sources information side information 
note features inherent data interesting objective 
instance want retain reduced representation data classification discriminative features relevant 
colored mvu achieves goal elucidating primarily relevant features aligning embedding objective provided side information 
examples illustrate situation details bag pixels representation images data usps digits find embedding reflects categories images side information 
vector space representation texts web data newsgroups find embedding reflects hierarchy topics side information 
tf idf representation documents data nips papers find embedding reflects authorship relations documents side information 
strong motivation simply merging sources information single distance metric firstly data side information may heterogenous 
unclear combine single distance metric secondly side information may appear form similarity distance 
instance authorship relations similarity documents papers share authors tends similar induce distance documents papers share authors assert far apart 
thirdly test time inserting new observation existing embedding source information available side information missing 
maximum variance unfolding giving brief overview mvu projection variants proposed 
set observations 
zm distance metric find inner product matrix kernel matrix 
distances preserved kii kjj kij ij pairs sufficiently close nearest neighbors observation 
denote set denote graph formed having pairs edges 

embedded data centered 



trace maximized maximum variance unfolding part 
variants algorithm including large scale variant proposed 
large optimization problem looks follows maximize tr subject kii kjj kij ij 
numerous variants exist distances allow shrink slack variables added objective function allow approximate distance preserving uses low rank expansions cope computational complexity semidefinite programming 
major drawback mvu results necessarily come somewhat surprise 
clear invoking mvu specific interesting results produce 
hindsight easy find insightful interpretation outcome priori clear aspect data representation emphasize 
second drawback general generating brilliant results statistical origins somewhat obscure 
aim address problems means hilbert schmidt independence criterion 
hilbert schmidt independence criterion sets observations drawn jointly distribution 
hilbert schmidt independence criterion measures dependence random variables computing square norm cross covariance operator domain hilbert space 
shown provided hilbert space universal norm vanishes independent 
large value suggests strong dependence respect choice kernels 
reproducing kernel hilbert spaces rkhs associated kernels respectively 
cross covariance operator cxy defined cxy exy 
defined square hilbert term kernels schmidt norm cxy cxy hs yy exy ex ey 
samples 
xm ym size drawn joint distribution empirical estimate tr kernel matrices data labels respectively hij ij centers data labels feature space 
convenience drop normalization tr 
measure independence random variables select features cluster data see appendix details 
different way try construct kernel matrix dimension reduced data preserves local distance structure original data maximally dependent side information seen kernel matrix advantages dependence criterion 
satisfies concentration measure conditions random draws observation provides values similar 
desirable want metric embedding robust small changes 
second easy compute kernel matrices required density estimation needed 
freedom choosing kernel allows incorporate prior knowledge dependence estimation process 
consequence able incorporate various side information simply choosing appropriate kernel colored maximum variance unfolding state algorithmic modification subsequently explain reasonable key idea replace tr tr kl covariance matrix domain side information respect extract features 
instance case nips papers happen author information kernel matrix arising coauthorship euclidean distance vector space representations documents 
key reasoning lemma lemma denote positive semidefinite matrix defined hij ij optimization problems equivalent maximize tr subject constraints kii kjj kij 
maximize tr kl subject constraints kii kjj kij 
solution solves solution solves centering 
proof denote ka kb solutions respectively 
kb feasible tr kbl tr 
implies tr tr 
vice versa feasible 
tr tr kbl requirement optimality kb 
combining inequalities shows tr tr kbl solutions equivalent 
means centering imposed mvu constraints equivalent centering means dependence measure tr 
words mvu equivalently maximizes tr dependence identity matrix corresponds retain maximal diversity observations lij ij 
suggests colored version mvu maximize tr subject kii kjj kij ij 
see extracting euclidean embedding maximally depends coloring matrix side information preserving local distance structure 
second advantage restrict allowing part linear subspace formed principal vectors space remains feasible constrained mvu formulation may infeasible may satisfied 
dual problem gain insight structure solution derive dual problem 
approach uses results 
define matrices eij rm edge nonzero entries ij ii eij jj eij ij eij ji 
distance preserving constraint written tr ij lagrangian tr tr kz wij tr ke ij ij tr ij ij wij 
setting derivative respect zero yields 
plugging condition gives dual problem 
minimize wij ij subject ij 
note amounts graph laplacian weighted graph adjacency matrix dual constraint effectively requires eigen spectrum graph laplacian bounded 
interested properties solution primal problem particular number nonzero eigenvalues 
recall optimality karush kuhn tucker conditions imply tr kz row space lies null space rank upper bounded dimension null space recall design graph laplacian weighted graph edge weights wij 
corresponds connected graphs eigenvalue vanishes 
eigenvectors zero eigenvalues correspond lying image 
arises label kernel matrix class classification problem vanishing eigenvalues translates nonvanishing eigenvalues contrast observation plain mvu 
case eigenvalue vanishes 
vanishing eigenvalues translates nonzero eigenvalues corroborated experiments section 
implementation details practice requiring distances remain unchanged embedding require preserved approximately 
penalizing slackness original distance embedding distance maximize tr kii kjj kij ij subject controls tradeoff dependence maximization distance preservation 
semidefinite program usually time complexity 
renders direct implementation problem infeasible toy problems 
reduce computation approximate orthonormal set vectors size smaller positive definite matrix size vav conveniently choose number dimensions smaller resulting semidefinite program respect tractable clearly approximation 
obtain matrix employ regularization scheme proposed 
construct nearest neighbor graph refer graph adjacency matrix 
form stacking bottom eigenvectors graph laplacian neighborhood graph key idea neighbors original space remain neighbors embedding space 
require similar locations bottom eigenvectors graph laplacian provide set bases functions smoothly varying graph 
subsequent semidefinite program perform local refinement embedding gradient descent 
objective reformulated dimensional vector xx initial value obtained leading eigenvectors solution 
experiments ultimately justification algorithm practical applicability 
demonstrate datasets embedding digits usps database newsgroups dataset containing usenet articles text form collection nips papers 
compare colored mvu called maximum unfolding mvu pca highlighting places produces meaningful results incorporating side information 
details effects adjacency matrices comparison neighborhood component analysis relegated appendix due limitations space 
images euclidean distance pixel values base metric 
text documents perform standard preprocessing steps words stemmed porter stemmer ii filter common meaningless stopwords iii delete words appear documents iv represent document vector usual tf idf term frequency inverse document frequency weighting scheme 
euclidean distance vectors find nearest neighbors 
construct nearest neighbor graph considering nearest neighbors point 
subsequently adjacency matrix graph symmetrized 
regularization parameter set default 
choose dimensions decompose embedding matrix final visualization carried dimensions 
results comparable previous 
usps digits dataset consists images hand written digits resolution pixels 
normalized data range test set containing observations 
digit recognition task 

construct matrix applying kernel 
kernel promotes embedding images class grouped tighter 
shows results produced mvu pca 
properties embeddings similar methods left right top bottom 
arguably produces clearer visualization 
instance images clustered tighter case methods 
furthermore results better separation images different classes 
instance overlap produce mvu pca largely reduced 
similar results hold 
shows produced different methods 
eigenvalues sorted descending order normalized trace patch color bar represents eigenvalue 
see results significant eigenvalues mvu results pca produces grading eigenvalues seen continuously changing spectrum spectral diagram 
confirms reasoning section spectrum generated considerably sparser mvu 
newsgroups dataset consists usenet articles collected different newsgroups 
subset documents experiments articles newsgroup 
remove headers articles preprocessing keeping subject line 
clear hierarchy newsgroups 
instance topics related computer science related religion related recreation 
different topics side information apply delta kernel 
similar usps digits want preserve identity individual newsgroups 
encode hierarchical information mvu recover meaningful hierarchy topics seen 
preprocessed data available www usyd edu au datasets html 
embedding usps digits produced mvu pca respectively 
colors dots denote digits different classes 
color bar shows learned kernel matrix embedding newsgroup articles produced mvu pca respectively 
colors shapes dots denote articles different newsgroups 
color bar shows learned kernel matrix distinctive feature visualizations groups articles individual topics tightly mvu pca 
furthermore semantic information preserved 
instance left side embedding computer science topics placed adjacent comp sys ibm pc hardware comp os ms windows misc adjacent separated comp sys mac hardware comp windows comp graphics 
meaningful apple computers popular graphics windows systems scientific visualization 
likewise see top find recreational topics rec sport baseball rec sport hockey clearly distinguished rec autos rec motorcycles groups 
similar adjacency talk politics mideast soc religion christian quite interesting 
layout suggests content talk politics guns sci crypt quite different usenet discussions 
nips papers regular nips papers 
scanned proceedings transformed text files ocr 
table contents toc available 
parse toc construct coauthor network 
goal embed papers coauthor information account 
kernel simply number authors shared papers 
illustrate highlighted known researchers 
furthermore annotated papers show semantics revealed embedding 
shows results produced mvu pca 
methods correctly represent major topics nips papers artificial systems machine learning positioned left side visualization natural systems embedding nips papers produced mvu pca 
papers representative combinations researchers highlighted indicated legend 
color bar shows learned kernel matrix yellow diamond graph denotes current submitted nips 
placed location nearest neighbor details appendix 
computational neuroscience lie right 
confirmed examining highlighted researchers 
instance papers smola sch lkopf jordan embedded left papers sejnowski dayan bialek right 
unique visualization clear grouping papers researchers 
instance papers reinforcement learning barto singh sutton upper left corner papers hinton computational cognitive science near lower left corner papers sejnowski dayan computational clustered right side adjacent 
interestingly papers jordan time best known graphical models grouped close papers reinforcement learning 
singh jordan 
interesting trend papers new fields research embedded edges 
instance papers reinforcement learning barto singh sutton left edge 
consistent fact interesting new results period recall time period dataset 
note groups papers authors preserving macroscopic structure data reveals microscopic semantics papers 
instance papers numbered smola pf hinton dayan close 
titles convey strong similarity information papers handwritten digits experiments 
second example papers dayan 
papers neuroscience side papers numbered reinforcement learning machine learning side 
third example papers bialek hinton spiking neurons numbered 
hinton papers mainly left spiking boltzmann machines closer bialek papers spiking neurons 
discussion summary provides embedding data preserves side information possibly available training time 
way means controlling representation data obtain having rely luck representation mvu just happens match want obtain 
feature extraction robust spurious interactions observations noise see appendix example adjacency matrices discussion 
fortuitous side effect matrix containing side information low rank reduced representation learned lower rank obtained mvu 
showed mvu formulated feature extraction obtaining maximally dependent features 
provides information theoretic footing brilliant heuristic maximizing trace covariance matrix 
notion extracting features data maximally dependent original data far general described 
particular may show feature selection clustering seen special cases framework 
acknowledgments funded australian government backing australia ability initiative part arc research supported pascal network 
weinberger sha saul 
learning kernel matrix nonlinear dimensionality reduction 
proceedings st international conference machine learning banff canada 
sun boyd xiao diaconis 
fastest mixing process graph connection maximum variance unfolding problem 
siam review 
bousquet smola sch lkopf 
measuring statistical dependence hilbert schmidt norms 
jain simon tomita editors proceedings algorithmic learning theory pages berlin germany 
springer verlag 
weinberger sha zhu saul 
graph laplacian regularization large scale programming 
neural information processing systems 
bach jordan 
dimensionality reduction supervised learning reproducing kernel hilbert spaces 
mach 
learn 
res 
goldberger roweis hinton 
neighbourhood component analysis 
advances neural information processing systems 
song smola 
supervised feature selection dependence estimation 
proc 
intl 
conf 
machine learning pages 

song smola 
dependence maximization view clustering 
proc 
intl 
conf 
machine learning pages 


