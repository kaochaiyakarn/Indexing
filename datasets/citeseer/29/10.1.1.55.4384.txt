theory proximity clustering structure detection optimization jan puzicha thomas hofmann joachim buhmann institut fur informatik iii artificial intelligence laboratory university bonn massachusetts institute technology bonn germany ma usa cs uni bonn de hofmann ai mit edu october systematic optimization approach clustering proximity similarity data developed 
starting fundamental invariance robustness properties set axioms proposed discussed distinguish different cluster compactness separation criteria 
approach covers case sparse proximity matrices extended nested partitionings hierarchical data clustering 
solve associated optimization problems rigorous mathematical framework deterministic annealing mean field approximation 
efficient optimization heuristics derived canonical way clarifies relation stochastic optimization gibbs sampling 
similarity clustering techniques broad range possible applications computer vision pattern recognition data analysis 
extensions indispensable analysis large data sets general prohibitive unnecessary due redundancy exhaustively perform pairwise comparisons objects 
large scale applications group structure typically occurs different resolution levels strongly favors hierarchical partitioning schemes 
suitable objective function identified principle known optimization technique applied find optimal solutions 
np hardness data partitioning problems renders application exact methods large scale problems intractable heuristic optimization techniques promising candidates find approximate clustering solutions 
particular stochastic optimization section offers robustness matches peculiarities data analysis problems closely related methods derived monte carlo algorithm known gibbs sampler deterministic variant known mean field annealing approaches rely computational temperature offer number advantages general cover clustering objective functions ii yield scalable algorithms terms complexity quality tradeoff iii temperature defines natural resolution scale hierarchical clustering problems theory derived algorithms tested validated application areas unsupervised segmentation textured images information retrieval document databases :10.1.1.130.3511:10.1.1.130.3511:10.1.1.34.54
axiomatization clustering objective functions assume data form proximity matrix ir entries ij quantifying dissimilarity objects domain objects 
furthermore assume number clusters fixed 
boolean representation data partitionings introduced terms assignment matrices mn mn indicator variable assignment object cluster object belongs cluster assignment constraints definition mn assure data assignment complete unique 
elementary axioms general principles proximity clustering expressed axioms definition clustering criterion 
robust strong sense condition holds ir defined ffi ik ffi il gamma ffi ik ffi il intuitively robustness assures single measurements weak robustness measurements belonging single object strong robustness macroscopic influence costs configuration 
robustness properties invariant criteria summarized proof table pc pc ps ps ps ps weak robustness strong robustness emphasize remarkable facts pc criterion fulfills strong robustness axiom ii invariant inter cluster separation criterion robust strong sense iii ps criterion constant cluster weights robust weak sense 
cluster separation criteria lack strong robustness 
reason configurations large macroscopic cluster gamma small microscopic clusters number inter cluster dissimilarities scales compared total number scaling 
strong robustness obtained restricting sets admissible solutions macroscopic clusters ps macroscopic clusters ps ps considerations yield ranking invariant clustering criteria respect asymptotic robustness properties pc ps ps ps ps pc axiomatization criterion pc clearly distinguished additive criteria :10.1.1.130.3511:10.1.1.130.3511:10.1.1.130.3511
cluster separation measures ps identified promising candidate due robustness properties 
interestingly intrinsic connection means objective function central clustering 
assume data generated vector space representation ir ij gamma pc km km gamma usual centroid definition exists intimate relation ward agglomerative clustering method distance pair clusters defined cost increase merging clusters objective function heuristically minimized greedy merging starting singleton clusters 
case pc procedure exactly yields ward method 
essential result ways averaging result invariant criteria sparse proximity matrices 
intra cluster compactness objective equal relation refers additional diagonal contributions ii negligible large alternatively definition additivity extended cover reflexive case get true identity 
functions average intra cluster dissimilarity calculated step averages cascaded averaging ii ij ii ij similar way average inter cluster dissimilarities generalized 
different possibilities weighting clusters averaging independent freely combined 
sparse data variants obtained pc pc ij pc ii ij hierarchical clustering second extension data clustering objective functions concerns problem hierarchical clustering particular important claimed partitional methods inherently non hierarchical hierarchical clustering solution data partitionings specified levels max order different data partitionings consistent required consecutive solutions cluster split sub clusters :10.1.1.34.54
simplicity consistent numbering clusters enforced 
definition 
sequence data partitionings mn max hierarchical cluster indices ff exist ff ff definition implies hierarchical clustering solution completely described finest data partitioning sequence splits ff implicitly encode topology complete binary tree 
underlying principle definition hierarchical clustering criteria defined additive composition single level solutions 
partition costs complexity costs log ir added 
partitions kept possess range lowest total costs 
motivation complexity costs proportional log stems expected cost decay random instance limit 
notice index necessarily identify single true partition eliminates implausible partitions suboptimal choices particular model selection criterion bayesian sense 
optimization annealing simulated annealing gibbs sampling simulated annealing popular stochastic optimization heuristic successfully applied large scale problems computer vision operations research :10.1.1.123.7607
simulated annealing performs random search modeled inhomogeneous discrete time markov chain converging equilibrium distribution gibbs distribution ph exp gammah exp gammah formally denote fp 
space probability distributions entropy gamma ts log generalized free energy plays role objective function arbitrary assignment problems gibbs distribution ph minimizes generalized free energy ph arg data clustering problem focus local algorithms state transitions restricted pairs configurations differ assignment object site 
denote ff matrix obtained substituting th row unity vector ff convenience introduce site visitation schedule map ng limit site visited infinitely 
sampling scheme known gibbs sampler advantageous possible efficiently sample conditional distribution ph site assignments sites fj schedule gibbs sampler defined non zero transition probabilities ff exp gammah ff exp gammah basic idea annealing gradually lower temperature transition probabilities depend 
arbitrary clustering criterion site visitation schedule 
arbitrary initial conditions asynchronous update scheme converges local minimum generalized free energy new exp gamma exp gamma fi fi old old notice variables called mean fields analogy naming convention statistical physics auxiliary parameters notation 
update scheme essentially non linear seidel relaxation iteratively solve coupled equations 
combining convergent update scheme annealing schedule yields algorithm convex sufficiently large 
derivation mathematical details refer tight relationship quantities involved implementing gibbs sampler mean field equations :10.1.1.34.54
rewriting arrive hg mean field averaged version local costs gibbs sampling clustering proximity data efficiently implement gibbs sampler optimize evaluation sequence locally modified assignment matrices 
important observation quantities computed additive shift may depend site index choice gamma leads compact analytical expressions contributions reduced system site subtracted 
functional representation additive clustering criteria proposition general formula gibbs fields ij gamma ij jk gamma jk gammai gamma jk gamma jk gammai gamma cluster size adding object cluster gammai gamma cluster size eliminating object cluster :10.1.1.123.7607
order derive gibbs fields specific cost functions occuring differences contribution functions calculated 
combining convergent update scheme annealing schedule yields algorithm convex sufficiently large 
derivation mathematical details refer tight relationship quantities involved implementing gibbs sampler mean field equations :10.1.1.34.54
rewriting arrive hg mean field averaged version local costs gibbs sampling clustering proximity data efficiently implement gibbs sampler optimize evaluation sequence locally modified assignment matrices 
important observation quantities computed additive shift may depend site index choice gamma leads compact analytical expressions contributions reduced system site subtracted 
functional representation additive clustering criteria proposition general formula gibbs fields ij gamma ij jk gamma jk gammai gamma jk gamma jk gammai gamma cluster size adding object cluster gammai gamma cluster size eliminating object cluster :10.1.1.123.7607
order derive gibbs fields specific cost functions occuring differences contribution functions calculated 
exemplary display gibbs field equation pc explicitly pc gammai ij gamma gammai jk order obtain efficient implementation gibbs sampler propose utilize book keeping quantities intra cluster compactness criteria cluster sizes ij computation gibbs fields bookkeeping quantities requires nk arithmetical operations 
locally changing assignment single object update book keeping quantities requires operations complete sweep 
generalization case sparse proximity matrices straightforward 
