advances neural information processing systems pp 
mit press 
generalization reinforcement learning successful examples sparse coarse coding richard sutton university massachusetts amherst ma usa rich cs umass edu large problems reinforcement learning systems parameterized function approximators neural networks order generalize similar situations actions 
cases strong theoretical results accuracy convergence computational results mixed 
particular boyan moore reported year meeting series negative results attempting apply dynamic programming function approximation simple control problems continuous state spaces 
positive results control tasks attempted significantly larger 
positive results control tasks attempted significantly larger 
important differences sparse coarse coded function approximators global function approximators learned online learned offline 
boyan moore suggested problems encountered solved actual outcomes classical monte carlo methods td algorithm 
experiments resulted substantially poorer performance 
conclude reinforcement learning robustly conjunction function approximators little justification avoiding case general reinforcement learning function approximation reinforcement learning broad class optimal control methods estimating value functions experience simulation search barto bradtke singh sutton watkins :10.1.1.132.7760
methods dynamic programming temporal difference learning build estimates part basis estimates 
may practice estimates exact large problems parameterized function approximators neural networks 
estimates imperfect turn targets estimates possible ultimate result poor estimates divergence 
methods shown unstable theory baird gordon tsitsiklis van roy practice boyan moore 
methods dynamic programming temporal difference learning build estimates part basis estimates 
may practice estimates exact large problems parameterized function approximators neural networks 
estimates imperfect turn targets estimates possible ultimate result poor estimates divergence 
methods shown unstable theory baird gordon tsitsiklis van roy practice boyan moore 
hand methods proven stable theory sutton dayan effective practice lin tesauro zhang dietterich crites barto :10.1.1.132.7760
key requirements method task order obtain performance 
experiments part narrowing answer question 
reinforcement learning methods variations sarsa algorithm singh sutton 
method td algorithm sutton applied state action pairs states predictions basis selecting actions :10.1.1.132.7760
hand methods proven stable theory sutton dayan effective practice lin tesauro zhang dietterich crites barto :10.1.1.132.7760
key requirements method task order obtain performance 
experiments part narrowing answer question 
reinforcement learning methods variations sarsa algorithm singh sutton 
method td algorithm sutton applied state action pairs states predictions basis selecting actions :10.1.1.132.7760
learning agent estimates action values defined expected reward starting state action policy 
estimated states actions policy currently followed agent 
policy chosen dependent current estimates way jointly improve ideally approaching optimal policy optimal action values 
experiments actions selected call ffl greedy policy time action selected state action estimate largest ties broken randomly 
mountain car task actions positive torque negative torque torque reward gamma steps 
see appendix 
steps trial log scale trials typical single run median runs smoothed average runs learning curves goal raise tip line torque applied tip learning curves 
effect key question reinforcement learning better learn basis actual outcomes monte carlo methods td learn basis interim estimates td 
theoretically asymptotic advantages function approximators dayan bertsekas empirically thought achieve better learning rates sutton :10.1.1.132.7760
hitherto question put empirical test function approximators 
figures shows results test 
steps trial averaged trials runs mountain car accumulate replace cost trial averaged trials runs replace world effects ff mountain car world tasks 
summarizes data systematic studies different tasks picture effect cases performance inverted shaped function performance degrades rapidly approaches worst performance obtained 
