integrated architectures learning planning reacting approximating dynamic programming richard sutton course october th outline dyna architecture dyna pi navigation task problems changing worlds dyna pi vs dyna changing world experiments strengths weaknesses dyna motivation robot decide 
plan move planning plan moves compile results set rapid reactions reactive systems learn set reactions trial learning combination dyna architecture dyna pi dyna approximating policy iteration dynamic programming policy iteration continually evaluates improves policy guaranteed converge optimal policy finite mdps stochastic environments finite set state action spaces dyna pi architecture dyna pi fixed policy random policy reactive system policy continually adjusted integrated planning learning process planning learning integrated choice world hypothetical real experiences dyna pi algorithm policy cs obtain ns update cs ns real hypothetical real hypothetical 
policy rs experienced 
obtain ns world update evaluation function ns update policy strengthen weaken current policy ns cs navigation task grid system deterministic agent know experience real world hypothetical experience generated model includes planning hypothetical experiences done previously visited states navigation task planning agent finds optimal path reasonable number trials planning agent finds optimal solution faster cost real hypothetical experiences performs best 
dyna pi performance half way second trial trial agent learns step leading goal second trial agent learns step steps goal agent learns extensive policy dyna pi implementation policy state action table entry xa entry state action evaluation function updated actions selected distribution policy updated xa xa initially values policy table entries values zero problem 
dyna pi performed finding optimal path may find problems changing worlds blocking problem barrier added blocks optimal path dyna pi uses previously learned values hundreds times shortcut problem barrier removed permits shorter path start goal dyna pi explores find new optimal path dyna solves problem changing worlds adding exploration dyna simpler structure dyna pi combination dyna architecture watkins qlearning uses memory structure evaluation function policy called uses rule update evaluation function policy dyna pi vs dyna policy update rule evaluation function update rule action selection dyna pi xa xa dyna xa xa xa xa xa xa dyna dyna exploration bonus uses new memory structure xa number time steps elapsed tried real experience 
