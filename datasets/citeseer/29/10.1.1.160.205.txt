software practice experience softw 
pract 
exper 
swap compression resurrecting old ideas toni cortes yolanda ra departament de universitat polit cnica de catalunya campus nord jordi girona barcelona spain summary performance memory intensive applications tends poor due high overhead added swapping mechanism 
problem may highly loaded multi programming systems running applications swap space order able execute time 
solution problems 
idea consists compressing swapped pages keeping swap cache possible 
idea compressed swap cache proposed years ago achieve expected results due hardware limitations 
processors faster performance gap processors disk bigger believe right time revisit idea 
new implementation compressed cache plus enhancements significantly improve performance swapping mechanism 
copyright john wiley sons key words compressed cache swap mechanism virtual memory core applications limited size physical memory considerable problem environments 
reason operating system offers known virtual memory 
ideas virtual memory portions code data needed time 
allows system keep disk portions applications maintains physical memory portions 
application needs data currently physical memory operating system brings disk physical memory 
time operating system may move portion code data disk 
operation moving memory disk called swapping paging completely transparent application 
correspondence toni cortes departament de universitat polit cnica de catalunya campus nord jordi girona barcelona spain 
mail toni ac upc es contract sponsor spanish ministry education cicyt contract number tic tic received august copyright john wiley sons revised october accepted december cortes memory intensive applications currently industrial academic environments 
applications take advantage swapping mechanism execute available physical memory run 
multi user environments important scenario virtual memory needed 
systems tend loaded applications swap part memory applications run concurrently 
main problem swapping mechanism effect performance applications 
time application needs move data swapping device needs access disk 
disks slow devices overhead degrades performance applications 
overhead may applications unusable slow 
important problem disk space limited 
system may running swap space case applications able run 
scenario frequent may problem environments increasing resources feasible 
instance non profit organizations resource budget limitations buy disks expensive 
performance space problems motivated objectives 
important goal speed swap mechanism 
improvement increase performance applications reason keep part memory swap space 
minor objective increase size memory offered applications increasing number disk blocks swap partition 
important notice objectives conflict design favor performance capacity important problem 
want proposed mechanism easy core applications manage memory relying swapping mechanism offered operating system 
main idea accomplish aforementioned objectives consists compressing pages swapped 
increases number pages placed swap partition 
furthermore allows build cache compressed pages decrease number times system access swap device 
important notice previous studies show compression ratios achieved compressing memory pages 
idea similar essence proposed douglis 
poor performance results obtained previous technology changed ways old idea excellent solution today problems 
processors faster years ago especially compared disks 
overhead compressing decompressing pages important 
furthermore propose enhancements improve performance mechanism 
approach terminology starting describe compressed swap cache believe necessary define terms get mixed 
copyright john wiley sons softw 
pract 
exper 
swap compression resurrecting old ideas memory page 
simplify memory management system divides memory portions fixed size 
portions memory called pages 
cache buffer 
extract cache keep swapped pages 
cache buffers may contain memory pages 
size buffer equal page size 
disk block 
contents cache buffer stored disk 
disk block portion disk data buffer kept 
simplicity assume size disk block multiple size cache buffer 
swap 
system needs free memory page available sends swapping device 
operation called swapping 
swap 
complementary operation swap bring page swap device physical memory 
operation needed applications pages memory disk 
basic idea main performance problem swapping mechanism comes fact pages fit memory kept disk slow device 
proved caching solution increase performance disk operations 
scenario cache swapped pages increase swapping performance problems solved 
accessing cache decrease number disk reads required pages cache 
swapping pages take advantage cache swapped page deallocated swap space sent disk 
examine proposal detail see system taken physical memory application build cache 
means applications memory 
done adding cache take fast memory applications offer amount memory somewhat slower 
accessing cache means system trap plus extra memory copies necessary accessing user memory 
reason simple cache solution increase performance applications 
ideal solution take fast memory users offer somewhat slower times larger memory 
solution reduce number times system access disk paging improve swapping performance 
solution achieved compressing swapped pages cached proposed douglis 
swap module sends page disk compressed placed cache 
hand swap module requests page system gets cache disk decompresses handing swap module 
allows cache hold pages number buffers taken user memory 
furthermore decreases number disk accesses page disk block sent copyright john wiley sons softw 
pract 
exper 
cortes application application page fault operating system swapping code page compress page uncompress cache 
conceptual vision compression cache mechanism 
swapping device 
shows path proposed swapping pages compressed swap system 
simple enhancements idea batching multiple pages write contiguously disk known solution avoid disk latency speed disk operations 
idea easily adapted compressed cache 
system need know pages stored disk written system decide send buffers single request place contiguous disk blocks 
batched writes plus fact page kept buffer reduce disk time significantly improving performance large applications 
idea batched writes compressed cache proposed douglis limited buffers 
propose perform large batched writes take full advantage mechanism 
interested performance swap mechanism increasing available memory page divided buffers reduce fragmentation 
allowing kind page division mean swapping page require accessing disk 
disk operations bottleneck making disk requests get single page question 
copyright john wiley sons softw 
pract 
exper 
swap compression resurrecting old ideas read write path presenting basic ideas compressed swap cache focus may happen inside compressed cache 
improve understanding cache behavior allow significant enhancements 
examine read hit compressed cache may find scenarios 
case requested page cache swapped system sent buffer disk 
disk contents cache buffer discarded place new pages 
kind hit happens requested page just written cache 
reason give kind hit name read hit due write 
second possible case page cache request different page brought 
example application requests page located buffer holds pages second page brought cache 
second page requested buffer discarded cache hit due previous request different page 
second kind hit called read hit due read 
kinds hits seen prefetching hits 
page requested system pages disk block pages requested read hit due read occurs 
clarify kinds read hits example 
presents sequence events shows kinds hits 
diagram presents initial scenario compressed swap cache clean application started 
application swaps page pk compressed stored cache 
application needs page pl page cache system reads disk block holds requested page decompresses 
notice disk block pages cache 
third step application needs page pk just swapped 
read hit due write time page entered cache due swap 
hand application needs page pm finds cache different kind hit 
case page brought cache swapping page pl 
example read hit due read 
believed read hits due write may occur read hits due read expected 
rationale assumption requesting page swapped time ago hit due write frequent working set application fit physical memory 
hand order pages swapped order swapped 
reason believed easy reuse pages brought cache buffer swapping page hit due read 
idea led believe cache write buffer swapped buffers placed cache 
order examine empirical way effect previous assumption implemented versions preliminary prototype read buffers placed cache second 
versions ran set benchmarks obtained results 
hand observed hit ratio similar prototypes 
read hit ratio obtained cache bypassed swapping operation copyright john wiley sons softw 
pract 
exper 
cortes application cache swap pk swap pl pk pl pm pl pm initial state swapping outp swapping inp swap swap pl pm pl pm read hit due write read hit due read 
graphical example kinds read hits 
swapping code page compress page uncompress cache 
conceptual vision different swapping paths 
copyright john wiley sons softw 
pract 
exper 
swap compression resurrecting old ideas read operations placed buffers cache 
small difference may important compared benefits obtain bypassing cache read operations 
hand observed execution benchmarks cache bypassed slower different paths 
performance loss explained facts 
cache hold read buffers number buffers written disk order magnitude lower cache filled read buffers 
happens pages stay longer cache probability deallocated swap space sent swap device increases significantly 
written disk 
reason reduces number buffers written reduction writes swapping pages 
reads need room cache swap operation perform write operation clean dirty buffer 
measured number times situation happens quite frequent find swap operations need clean buffer reading new page 
furthermore examples percentage grows 
system bypasses cache swapping operations system avoid extra write operations 
second dirty buffers cache system batch full buffers single write operation reducing number requests disk 
important side effect caching read requests simplification code 
want get details clear swapping operation search page cache read disk 
worry cleaning buffers cache avoids locking problems 
idea different swapping paths swapping swapping 
idea new version base final prototype section 
detailed study effect placing read buffers cache extensive technical report written authors 
final prototype description gone main ideas design time give detailed description way new swapping mechanism works 
section describe important operations policies algorithm final prototype 
regarding implementation details mention prototype built linux operating system kernel version 
system chosen widely 
furthermore machines runs usually moderate amount resources 
availability source code important issue needed modify 
compression algorithm chosen lzo ziv lempel data compressor 
algorithm chosen obtained ratio speed compression 
hand achieved compression ratios better experiments see table ii 
hand average time needed compress kbyte page needed decompress buffer 
times measured copyright john wiley sons softw 
pract 
exper 
cortes pentium ii mhz 
information hardware configuration methodology section 
swapping swapping module swap page examines compressed cache check page 
remember read buffers placed cache system keeps swapped pages 
page cache system decompresses returns swapping module 
best scenario disk read avoided fortunately experiments observed scenario happens nearly half time 
course application data dependent expect high number hits compressed cache 
page cache system reads disk block page system block decompresses page sends swapping module 
important notice block placed cache discarded 
page disk block requested new read operation needed 
notice new mechanism number disk reads original system pages cached compressed 
swapping swapping page somewhat complex swapping 
thing system compress 
compressed size known system find free space cache keep page 
fit algorithm choose buffer page placed 
strategy chosen efficient simple implement 
preliminary prototype studied typical algorithms fit best fit worst fit significant differences 
case system find hole discards clean buffer places compressed page 
clean buffer buffer pages updated disk information lost reusing 
happens system perform clean operation room new page 
operation described detail paragraphs 
important keep mind cleaning operation usually done system runs free buffers critical path swapping page 
cache cleaning algorithm clean cache need send buffers disk 
allows system reuse date copy data kept swap device 
intuitive idea cleaning cache consists sending disk buffers full 
difficult fill buffer completely compressed pages modified concept full buffer follows copyright john wiley sons softw 
pract 
exper 
swap compression resurrecting old ideas full buffers ones free space average size compressed pages 
new concept page fit cache full buffers sent disk single operation written contiguously disk 
increases performance write operations significantly 
may find contiguous free space disk 
situation largest contiguous portion blocks fit free space written 
important notice situation frequent described garbage collection section 
system need perform clean operation full buffers buffer 
want wait free space left cache clean system writes full buffers 
decide full buffers set parameter cleaning threshold percentage cache size 
full buffers cleaning threshold cleaning operation started 
detailed study impact parameter performance section 
important detail system executes cleaning operations time 
buffer needed cleaning operation done system waits cleaning operation finishes clean buffer available 
garbage collection scenario garbage collection procedure detecting portion swap device free reused store new pages 
traditional swapping system deciding portions swap device available ones simple 
swapped page disk block page freed swap space disk block page kept available 
system propose garbage collection problem twofold 
hand need know disk block valid information reused store new swapped pages 
hand may find unused portions disk block holes filled compressed pages 
holes appear cache buffer completely filled pages held freed 
concentrate problems associated reusing free holes 
read buffers placed cache buffer goes disk modifying contents reuse holes mean disk accesses 
read operation get disk block swap device block filled new compressed page rewritten disk 
accessing twice disk costly decided fill empty holes buffer sent disk 
thing happens cached buffers prototype system reuse space deallocated page 
basically due simplicity interested speed space 
trying reuse holes extra willing 
studied effect number pages compute average significant differences pages 
copyright john wiley sons softw 
pract 
exper 
cortes table length batched write information 
cache size mbyte mbytes mbytes batched write size kbyte blocks observed batched write size kbyte blocks simplification garbage collection simple task 
need mechanism detect buffer disk block active pages means considered free system reuse 
detecting block free simple keeping counter number active pages block 
previous section mentioned need large portions contiguous disk blocks perform large batched writes 
system find large portions free space able perform large batched writes objectives design 
reason wanted study free space usually available hand mechanism needed guarantee large portions free space 
order study behavior executed set benchmarks described detail methodology section different cache sizes cleaning threshold parameter 
executions measured average length batched writes compared expected size find contiguous free space table 
observe cases average size batched write larger theoretical expected value 
means batched writes contiguous free space extra mechanisms needed 
happens blocks usually stay swap area long 
furthermore time batched writes large system free allocated blocks probability finding large portions free disk quite high 
find environment system obtain contiguous free space implement compacting mechanisms similar ones proposed log structured file systems 
need arises idea add overhead complicated algorithms 
methodology testbed results measured pentium ii running mhz 
amount physical memory mbytes size swap partition mbytes 
partition located ultra scsi hard disk 
copyright john wiley sons softw 
pract 
exper 
swap compression resurrecting old ideas order avoid kind external influence benchmarks executed single user 
furthermore machine rebooted execution set machine initial state benchmarks executions 
total execution times measured time command counts load execution application 
notice load file system requests application takes amount time file system cache cleaned execution rebooting machine test 
benchmark description measure performance proposal need see effect set benchmarks 
getting detailed description benchmarks describe characteristics benchmark may affect behavior studied system 
concurrency 
important see number processes benchmark may affect behavior system 
process running system application swapping pages wait application may locked resources needs 
application tries swap pages 
parameter may affect results amount file system performed benchmark 
may conflict performed paging system requested disk different partitions 
compression ratio 
compression ratio may affect system ways 
pages compression ratio number pages kept cache higher 
number disk accesses lower bad compression ratios 
second better pages compress larger final effective size swap area 
described important characteristics benchmarks get results 
fft 
fft executes fast fourier transformation matrix 
values elements matrix set randomly gaussian distribution 
fft 
benchmark similar previous fft executed concurrently size matrices decreased elements 
sort 
benchmark perform memory sort text file 
input file build appending usr dict word file times possible 
sort 
benchmark sort executed concurrently 
file sorted built previous times smaller limit execution time benchmark 
simulator 
simulator network disks currently research group 
event simulator uses large amounts memory 
memory compresses events queues similar information 
furthermore memory compression ratio compressed size page size means high percentages denote bad compression 
copyright john wiley sons softw 
pract 
exper 
cortes table ii 
benchmark characteristics 
concurrent file compress benchmark processes ratio fft fft sort start sort start simulator start simulator start part part library free allocated memory free 
keeps memory blocks hash queue 
freed memory easy compress filled zeros 
compression ratio application bit unrealistic applications typical unix system achieve compression ratios better awk 
simulator 
concurrent executions simulator smaller input 

visualization video file avi format 
video dithered floyd steinberg algorithm means file decompressed memory dithering visualized 
benchmark run windows system needed perform graphic 
concurrent executions previous benchmark input video 
table ii summarizes characteristics benchmark parameters described part subsection 
experimental evaluation just tries show mechanism useful increase performance applications need selection possible experiments 
show effect cache size cleaning threshold 
effect threshold fixed cache size 
parameters tuned system depending hardware expected load 
show best configuration chosen reasonable configurations sufficient significant performance improvement 
general performance results experiment configured compressed cache thought adequate parameters 
mbyte cache cleaning threshold cache 
copyright john wiley sons softw 
pract 
exper 
swap compression resurrecting old ideas speedup fft fft sort sort simulator simulator 
effect compressed swap workloads 
values executed benchmarks computed speed obtained compared original swapping mechanism 
presents results 
chart see benchmarks observe speed 
means applications run faster original swapping mechanism 
exceptions rule fft simulator 
fft achieves speed means runs slower original system 
slowdown due basic factors 
compression ratio pages compressed bytes 
means difficult place page buffer disk block 
second reason memory application data structures cache buffers significant effect application 
memory working set needed phases longer fits memory application pages original system 
second exception reasonable speed execution concurrent simulations simulator 
benchmark achieves speed 
impressive improvement due incredible compression ratio 
pages compress swapped pages fit cache nearly disk accesses needed 
exceptions frequent expect performance improvement significant gain 
copyright john wiley sons softw 
pract 
exper 
cortes speedup fft fft sort sort simulator cache size kbytes 
influence cache size application performance 
unexpected result low speed obtained simulator benchmark 
benchmark excellent compression ratio expected significant speed 
reason result behavior original system 
swaps pages small periods time original system group sending disk performs similar batched write 
reason gain obtain batching write operations gained original system 
situation happens original kernel pages swapped disk busy 
kernel coalesces requests single request contiguous 
furthermore situation happen see speed ups obtained benchmarks 
influence cache size second experiment tried study influence cache size performance new mechanism 
experiment ran benchmarks varying cache size kbytes mbytes cleaning threshold remained 
results obtained 
graph results simulator curve speed ups complicate study graph 
main observation perfect cache size benchmarks 
clear cases large caches take pages applications swap far 
hand caches small part potential benefits obtained 
reason believe lies variable cache size adapted dynamically needs system studied simulation wilson 
copyright john wiley sons softw 
pract 
exper 
swap compression resurrecting old ideas speedup fft fft sort sort simulator cache size 
influence cleaning threshold application performance 
mono process benchmarks bad idea caches small process accessing disk swapping reasons time lost done 
hand multiprocess benchmarks swapping advance completion 
concurrence takes stress swapping mechanism benefits smaller cache 
strange behavior observed sort 
performance benchmark unpredictable behavior follow clear pattern 
reason behavior large amount performs 
tries perform file system cleaning cache file system operation delayed slowing application 
depending size cache clear pattern cleaning operations may conflicts applications dynamically adapting cache reasonable size mbyte performance gains significant compared original swapping mechanism 
cleaning threshold parameter studied cleaning threshold 
value defines percentage buffers full cleaning operation started 
study influence parameter set cache size mbyte varied threshold 
results obtained benchmarks 
previous experiment performance simulator included graph reason applies 
observe parameter small 
case small write operations performed seek search actions important overhead write operations 
copyright john wiley sons softw 
pract 
exper 
cortes new swap space mbytes fft fft sort sort simulator simulator 
size swap area needed capacity mbytes compressed swap 
threshold grows performance applications tends increase disk latency important 
large values behavior system depends benchmark mono multi process 
mono process benchmarks cache cleaned synchronously process accesses disk benefits large writes continues issue 
hand multi process benchmarks process cleaning cache may want swap page 
cleaning operation long read write operation wait long time performance process affected negative way 
benchmark sort unpredictable behavior reason behavior large amount performs 
way avoid collisions case results 
experiments run reasonable value threshold 
dynamic value studied important cache size 
increase swap space far seen performance benefits compressing swap area 
mentioned performance main objective project second goal copyright john wiley sons softw 
pract 
exper 
swap compression resurrecting old ideas consisted increasing number pages placed swap area 
study objective subsection 
number pages fit swap partition depends compression ratio important factor 
fragmentation inside buffers important parameter final size swapping space 
system leaves large unused portions buffers able place pages original system swap partition 
example pages compress kbytes system needs kbyte buffers keep 
hand kbyte kbyte pages average compression ratio kbyte buffers needed keep 
presents size swap area obtain fill pages compression ratio fragmentation obtained benchmarks 
see cases system increases size swap partition 
larger swap space may difference able execute application able execute 
limited gain mbytes limited size internal data structures double physical swap space 
higher limit larger swap space obtained benchmarks simulator 
idea core applications having compressed swap cache described possible implemented inside operating system 
reason process designing compressed swap cache kept mind flexible implemented user level programmed inside applications 
applications handle memory know better operating system 
applications swapping done explicitly application ideas 
application decide part memory build cache 
compress decompress pages storing cache 
clean cache performing large batched writes 
implementing mechanism may complex applications currently doing believe worthwhile far performance concerned 
furthermore functionality implemented standard library simplify mechanism considerably 
advantage implementing idea compressed swap cache inside application may take advantage application knows access patterns 
knowledge improve performance mechanism 
related research done area memory swap compression 
knowledge idea proposed different papers appel li pursue issue wilson come back idea describe section :10.1.1.12.2903
copyright john wiley sons softw 
pract 
exper 
cortes douglis postulate benefits compression cache 
furthermore built evaluated prototype 
proposal similar essence important differences 
swap pages compressed kept cache increase size virtual memory performance applications need swapping mechanism 
big difference previous lacked study kinds read hits obtained cache 
study led important design modifications having different paths swapping swapping 
shown distinction obtained significant performance benefits 
important difference way tested 
benchmarks single process believe multi process benchmarks studied due special behavior 
results dependent compression ratio proposal prototype 
improvement probably due differences hardware improvements design 
pentium ii system times faster decstation douglis 
remember change technology reason led original idea 
study developed wilson 
prove compressed swap cache idea improve performance swapping mechanism 
solutions adaptive cache size may usable proposal 
hand results obtained simulation real prototype built 
furthermore differentiation kinds read hits studied important contribution 
examine general way divide basic issues increasing size memory reducing average time needed swap page 
paragraphs briefly discuss done fields 
idea increasing size memory commercial products compress physical memory 
software mechanisms applications believe system larger amount physical memory 
achievements obtained systems clear 
idea proposed hardware better performance gains simulations done 
project proposed memory system hardware compressor dynamically compresses main memory working set system fit physical memory 
proposals decrease number disk accesses swapping issues 
instance devoted minimize number pages swapped 
contents page irrelevant application execution page need kept swap 
line software developed study utilization pages improve programs reduce number pages swapped 
tried group pages swapped larger writes done 
approach compressing information sending disk widely database environments file systems 
research issues objective idea compression swapping aimed solving possible problems 
line making people aware new old copyright john wiley sons softw 
pract 
exper 
swap compression resurrecting old ideas technique 
reason section set research issues addressed project start point 
lack spatial locality main problems compressed cache mechanism 
uncommon page located buffer previously swapped 
may ways cluster pages higher locality achieved 
case different paths swapping pages reexamined 
similar way system try read ahead needed pages application requests 
prefetching may imply changes current proposal 
seen deciding right size compressed cache easy task 
research done find algorithms compute size dynamic way 
wilson working line simulations done far 
real prototype tested 
stated wilson done designing compression algorithms specially targeted compress memory pages 
faster obtain better compression ratios 
compressed swap mechanism take advantage knowledge programmer compiler may application 
knowledge help improve mechanism ways 
ideas may useful fields web caching network ram file system cache mechanism decreases swapping overhead increasing application performance 
furthermore mechanism implemented inside linux kernel widely 
tested system set different benchmarks achieved speed ups cases higher speed ups 
mechanism increases size virtual memory available increasing disk space assigned 
results show increasing swap space times easy achieve 
furthermore interesting notice theoretical studies similar ideas obtained performance results different test suits 
postulated ideas inside core applications handle memory 
widens range idea compressed cache 
important notice mechanism useful performance gap disk processor increases 
obtaining software mechanism installed linux systems simple way 
details please access web page www ac upc es homes toni www page html copyright john wiley sons softw 
pract 
exper 
cortes source code implementation described detail written authors 
professor dr help 
interesting comments increased quality significantly 
grateful helped benchmarks testing proposed mechanism 
debt fischer patience guiding important details scsi driver linux 
anonymous reviewers 
comments helped improve focus quality final version 

mowry tc ak krieger automatic compiler inserted prefetching core applications 
proceedings second international symposium operating system design implementation usenix association 

robinson em el 
page utilization fortran programs 
proceedings international conference parallel distributed processing techniques applications csrea press july 

verghese resource management issues shared memory multiprocessors 
phd thesis stanford university march 

gooch jones empirical study memory data characteristics compressibility 
proceedings iee computers digital techniques vol 
january 

russinovich ram compression analysis 
reilly online publishing report 
ftp de info windows win update model html february 

wilson pr kaplan sf smaragdakis case compressed caching virtual memory systems 
proceedings usenix technical conference usenix association june 

douglis compression cache line compression extend physical memory 
proceedings winter technical conference usenix association january 

smith aj 
disc cache ratio design considerations 
acm transactions computer systems 

levy lipman virtual memory management vax vms operating system 
ieee computer 

cortes swap 
technical report upc dac universitat polit cnica de catalunya departament de 

card dumas vel programming linux editions 

mf 
lzo 
uni linz ac mfx march 

ziv lempel compression individual sequences variable rate coding 
transactions information theory 

silberschatz pb 
operating system concepts fourth edn addison wesley 

rosenblum ousterhout jk 
design implementation log structured file system 
acm transactions computer systems 

burrows lampson mann line data compression log structured file system 
proceedings fifth international conference architectural support programming languages operating systems acm press october 

cormen th nicol dm 
core ffts parallel disks 
acm sigmetrics performance evaluation review 

thakur choudhary runtime support core parallel programs 
input output parallel distributed computer systems jain browne jc eds vol 
kluwer international series engineering computer science kluwer academic publishers 

toledo survey core algorithms numerical linear algebra 
external memory algorithms visualization abello vitter js eds dimacs series discrete mathematics theoretical computer science american mathematical society press providence ri 

appel aw li virtual memory primitives user programs 
proceedings fourth international conference architectural support programming languages operating systems april 
copyright john wiley sons softw 
pract 
exper 
swap compression resurrecting old ideas 
wilson pr 
operating system support small objects 
proceedings international workshop object orientation operating systems ieee press october 

leonard cure ram blues 
net special report 
www cnet com october 

steers tune memory 
pc world 

gooch jones design performance main memory hardware data compressor 
proceedings nd euromicro conference ieee computer society press september 

gooch jones performance evaluation computer architectures main memory data compression 
journal systems architecture 

cheriton dr application controlled physical memory external page cache management 
proceedings architecture support programming operating system october 

subramanian managing discardable pages external pager 
proceedings second usenix mach symposium november 

black carter macdonald jv wang osf virtual memory improvements 
proceedings mach symposium usenix association november 

wilson pr lam ms moher tg 
effective static graph reorganization improve locality garbage collected system 
proceedings sigplan conference programming languages design implementation acm press june 

cortes improving application performance swap compression 
proceedings usenix technical conference freenix track usenix association june 
copyright john wiley sons softw 
pract 
exper 

