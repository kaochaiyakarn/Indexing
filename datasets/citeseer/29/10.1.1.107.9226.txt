frans coetzee steve lawrence lee giles marco gori 
focused crawling context graphs th international conference large databases vldb cairo egypt pp 

focused crawling context graphs coetzee lawrence giles gori nec research institute independence way princeton nj usa dipartimento di dell informazione universit di siena roma siena italy coetzee lawrence giles research nj nec com gori maintaining currency search engine indices exhaustive crawling rapidly impossible due increasing size dynamic content web 
focused crawlers aim search subset web related specific category offer potential solution currency problem 
major problem focused crawling performing appropriate credit assignment different documents crawl path short term gains pursued expense obvious crawl paths ultimately yield larger sets valuable pages 
news financial data entertainment schedules widely disseminated web 
search engines increasingly challenged trying maintain current indices exhaustive crawling 
state art systems altavista crawls pages day exhaustive crawl web take weeks 
exhaustive crawls consume vast storage bandwidth resources control search engine 
focused crawlers aim search retrieve subset world wide web pertains specific topic relevance :10.1.1.22.3686:10.1.1.43.1111
ideal focused crawler retrieves maximal set relevant pages simultaneously traversing minimal number irrelevant documents web 
focused crawlers offer potential solution currency problem allowing standard exhaustive crawls supplemented focused crawls categories content changes quickly 
focused crawlers suited efficiently generate indices niche search engines maintained portals user groups limited bandwidth storage space norm 
due limited resources focused crawler users personal pc implementations 
crawler starts document steps target document documents steps starting document downloaded crawler hits target 
focused crawler tries identify promising links ignores topic documents 
crawler starts document steps target document downloads small subset documents steps starting document 
search strategy optimal crawler takes steps discover target 
focused crawler efficiently seeks documents specific topic guides search content link structure web :10.1.1.43.1111
graphically illustrates difference exhaustive breadthfirst crawler typical focused crawler 
focused crawler implements strategy associates score link pages downloaded 
links sorted scores inserted queue 
best search performed page analyze head queue 
consider example researcher looking papers neural networks 
large number papers home pages researchers computer science departments universities 
crawler finds home page university strategy follow path computer science cs department researchers pages university cs department pages general low relevancy scores 
adaptive focused crawler described principle learn strategy doubtful crawler explore path place especially length path traversed increases 
explicitly address problem rennie mccallum reinforcement learning train crawler specified example web sites containing target documents :10.1.1.1.7474
web site server document appears repeatedly crawled learn construct optimized paths target documents 
approach places burden user specify representative web sites 
initialization slow search result crawling substantial fraction host web site 
furthermore approach face difficulty hierarchy distributed number sites 
example researcher home page entered say link list papers conference site strategy crawler find department member list search pages researchers department 
explicit link exists researcher page cs department member list existing focused crawlers move hierarchy cs department home page 
focused crawler utilizes compact context representation called context graph model exploit hierarchies 
crawler utilizes limited backward crawling possible general search engine indices efficiently focus crawl web 
rennie mccallum approach approach learn context target documents located small set web sites principle back crawl significant fraction web starting seed topic document :10.1.1.1.7474:10.1.1.1.7474
furthermore approach efficient initialization context constructed directly branching set documents model parents siblings children seed set 
context focused crawler focused crawler call context focused crawler uses limited capability search engines altavista google allow users query pages linking specified document 
data construct representation pages occur certain link distance defined minimum number link traversals necessary move page target documents 
representation train set classifiers optimized detect assign documents different categories expected link distance document target document 
goal able assign web document particular layer merged context graph 
document poor fit layer wish discard document label category 
major difficulty implementing strategy single classifier mapping document set classes corresponding layers category absence model training set category 
solve problem modification naive bayes classifier layer 
classifier architecture provides reasonable performance high speed meets requirement system likelihood estimate provided classification studied :10.1.1.1.5684:10.1.1.1.7474
assume document di represented vector corresponding reduced tf idf representation relative vocabulary documents class defined correspond layer assumed prior probability web denote probability vector element wt occurs documents class wt classify page wish find class di maximized 
solution bayes rule di di wd th feature document di 
naive bayes assumption ignores joint statistics formally assumes document class features occur independently yielding final solution di di wd indicates number features document di 
denotes maximum depth context graphs discriminant functions di built corresponding layers discriminant functions allow page di assigned layers merged context graph finding layer discriminant function di maximized 
denotes maximum depth context graphs discriminant functions di built corresponding layers discriminant functions allow page di assigned layers merged context graph finding layer discriminant function di maximized 
subsequently computing likelihood function di winning layer comparing likelihood threshold possible discard weakly matching pages 
pages effectively marked avoids need construction explicit model documents context graph 
effect build set parallel class naive bayes classifiers layer select winning layer maximizing posteriori likelihood layer context graph 
training classifiers documents occur layer seed document context graphs combined serve training data set phrase probabilities wt computed sets counting occurrences feature wt normalizing words documents class wt di wt di di wt di number occurrences wt document di number phrases vocabulary :10.1.1.1.5684:10.1.1.1.7474
parameters calculated estimating number elements layers merged context graph 
useful layers contain excessive numbers nodes previously stated practical limitations prevent storage documents outermost layers 
cases class probabilities set fixed constant value number layers 
corresponds maximum likelihood selection winning layer 
promising approach maintains graph representation current crawl separate line process uses quantify effect different queue thresholds 
areas interest extension feature space include page analysis html structure statistics gained search develop ranking procedures presenting results user performing online classifier adaptation 
adaptation system restricted periodically incorporating context graphs newly discovered target documents queue re building classifier tests feature avoid intro retrieved documents context focused crawler focused crawler downloads performance focused crawlers category 
parameter 
online parameter updating classifiers em approach result efficient continuous optimization classifier performance :10.1.1.133.4884
approach background gathering web material computational bandwidth requirements crawler sufficiently modest crawler interactive session cable modem connection home pc 
focused crawler valuable supplement cases replacement standard search engine database queries 
doubt improvement focused crawling soon crawling privilege large companies afford expensive infrastructures personal tool widely available retrieving information world wide web 
web surpasses documents inktomi nec press release available www inktomi com jan 
approach background gathering web material computational bandwidth requirements crawler sufficiently modest crawler interactive session cable modem connection home pc 
focused crawler valuable supplement cases replacement standard search engine database queries 
doubt improvement focused crawling soon crawling privilege large companies afford expensive infrastructures personal tool widely available retrieving information world wide web 
web surpasses documents inktomi nec press release available www inktomi com jan 
chakrabarti van der berg dom focused crawling new approach topic specific web resource discovery proc :10.1.1.43.1111
th international world wide web conference www 
cho garcia molina page efficient crawling url ordering proceedings seventh world wide web conference 
mccallum nigam rennie seymore building domain search engines machine learning techniques proc 
aaai spring symposium intelligent agents cyberspace 
bharat henzinger improved algorithms topic distillation hyperlinked environments proceedings st int acm sigir conference 
kleinberg authoritative sources hyperlinked environment report rj ibm may 
chakrabarti van den berg dom distributed hypertext resource discovery examples vldb proceedings th international conference large data bases september edinburgh scotland uk atkinson orlowska valduriez zdonik brodie eds pp 
morgan kaufmann 
rennie mccallum reinforcement learning spider web efficiently proc :10.1.1.1.7474
international conference machine learning icml 
bharat broder henzinger kumar connectivity server fast access linkage information web www computer networks pp 

chakrabarti surfing backwards web proc th world wide web conference www 

mitchell machine learning 
mcgraw hill 
nigam mccallum thrun mitchell text classification labeled unlabelled documents em appear machine learning 
dempster laird rubin maximum likelihood incomplete data em algorithm statist :10.1.1.133.4884
soc 
vol 
pp 

