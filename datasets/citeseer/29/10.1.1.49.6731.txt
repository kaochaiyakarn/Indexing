induction decision trees relieff igor kononenko university ljubljana faculty electrical engineering computer science si ljubljana slovenia tel fax mail igor kononenko fer uni lj si context machine learning examples deals problem estimating quality attributes dependencies 
greedy search prevents current inductive machine learning algorithms detect significant dependencies attributes 
kira rendell developed relief algorithm estimating quality attributes able detect dependencies attributes 
show strong relation relief estimates impurity functions usually heuristic guidance inductive learning algorithms 
propose relieff extended version relief myopic impurity functions 
reimplemented assistant system top induction decision trees relieff estimator attributes selection step 
extensive bibliography field michalski tecuci 
contribution inductive learning examples 
typical inductive learning algorithm uses greedy search strategy overcome combinatorial explosion search hypotheses 
heuristic functions estimate potential successors current state search space play major role greedy search 
current inductive learning algorithms variants impurity functions information gain gain ratio quinlan gini index breiman distance measure mantaras weight evidence michie measure smyth goodman :10.1.1.167.3624
measures assume attributes independent domains strong dependencies attributes greedy search poor chances revealing hypothesis 
kira rendell developed algorithm called relief shown efficient estimating quality attributes 
example parity problems various degrees significant number irrelevant random additional attributes relief able correctly estimate relevance attributes time proportional number attributes square number training instances 
original relief deal discrete continuous attributes deal incomplete data limited class problems 
relation impurity functions derivation show relief estimates strongly related impurity functions 
obvious relief estimate attribute approximation difference probabilities different value instance different class gammap different value instance class eliminate requirement selected instance nearest formula different value class gammap different value class rewrite equal value class value obtain bayes rule gamma gamma gamma need equalities theta equality trivial 
derivation equality follows jv jv theta equalities obtain theta gini gamma constant theta theta gini gini theta gamma highly correlated gini index breiman classes values attribute difference factor gini index uses probability instances value attribute eq 
kind normalization factor multi valued attributes 
impurity functions tend overestimate multi valued attributes various normalization heuristics needed avoid tendency gain ratio quinlan binarization attributes cestnik :10.1.1.167.3624
equation shows relief implicitly uses normalization 
derivation eliminated nearest instance condition probabilities 
put back interpret relief estimates average local estimates smaller parts instance space 
enables relief take account dependencies attributes detected context locality 
average weighted prior probability class gamma diff class theta diff idea algorithm estimate ability attributes separate pair classes regardless classes closest 
note complexity relieff theta attributes number training instances 
induction decision trees assistant assistant assistant learning system top induction decision trees cestnik 
basic algorithm goes back cls concept learning system developed hunt 
reimplemented authors see quinlan overview :10.1.1.167.3624
describe main features assistant differences implemented assistant 
binarization attributes algorithm generates binary decision trees 
decision step version attribute selected maximizes information gain attribute 
continuous attributes decision point selected maximizes attribute information gain 
decision step version attribute selected maximizes information gain attribute 
continuous attributes decision point selected maximizes attribute information gain 
discrete attributes heuristic greedy algorithm find locally best split attribute values subsets 
purpose binarization reduce replication problem strengthen statistical support generated rules 
top induction decision tree algorithms construct binary decision trees order prevent detailed splits decision node hunt breiman cestnik quinlan fayyad fayyad irani :10.1.1.167.3624
decision tree pruning techniques pruning unreliable parts decision trees 
user defined thresholds provided minimal number training instances minimal attributes information gain maximal probability majority class current node 
method developed niblett bratko uses laplace law succession estimating expected classification error current node pruning pruning subtree 
incomplete data handling learning training instances missing value selected attribute weighted probabilities attribute value conditioned class label 
