dynamics reinforcement learning cooperative multiagent systems caroline claus craig boutilier department computer science university british columbia vancouver canada cs ubc ca reinforcement learning provide robust natural means agents learn coordinate action choices multiagent systems 
examine factors influence dynamics learning process setting 
distinguish reinforcement learners unaware ignore presence agents explicitly attempt learn value joint actions strategies counterparts 
study simple form learning cooperative multiagent systems perspectives focusing influence game structure exploration strategies convergence optimal suboptimal nash equilibria 
propose alternative optimistic exploration strategies increase likelihood convergence optimal equilibrium 
application learning problem coordination multiagent systems mass increasingly popular ai game theory 
reinforcement learning rl particular attracted attention :10.1.1.55.8066:10.1.1.135.717
noted rl means achieving coordinated behavior attractive generality robustness 
standard techniques rl example learning applied directly mass success 
general understanding conditions rl usefully applied exactly form rl take mass problems tackled depth 
ask questions differences agents learn agents single agent rl algorithms agents attempt learn values specific joint actions strategies employed agents 
rl algorithms guaranteed converge multiagent settings 
converge optimal equilibria 
rates convergence limit points influenced system structure action selection strategies 
address questions specific context repeated games agents copyright american association artificial intelligence www aaai org 
rights reserved 
common interests cooperative mass 
focus attention simplified form learning due relative simplicity certainly general efficacy consider factors influence dynamics multiagent learning provide partial answers questions 
focus simple setting expect apply broadly 
distinguish compare forms multiagent rl marl 
independent learners ils apply learning classic sense existence agents 
joint action learners contrast learn value actions conjunction agents integration rl equilibrium coordination learning methods 
briefly consider importance exploration strategies examine series examples game structure exploration strategies influence dynamics learning process convergence equilibrium 
show ils converge equilibrium specific setting fully cooperative repeated games 
fact information disposal perform differently ils straightforward application learning mass observe games multiple equilibria optimality agreed equilibrium assured 
describe optimistic exploration strategies designed increase likelihood reaching optimal equilibrium 
provides way having exploit additional information possess 
conclude discussion related mention issues promise integration rl coordination learning exciting area research foreseeable 
preliminary concepts notation single stage games interest application rl algorithms sequential decision problems system controlled multiple agents 
interests simplicity investigations focussed player cooperative common interest repeated games 
sequential primary interest discuss issue sections 
view prob hold mutatis mutandis sequential multiagent markov decision processes multiple states lem hand distributed bandit problem 

formally assume collection heterogeneous agents agent having available finite set individual actions agents repeatedly play stage game independently select individual action perform 
chosen actions point constitute joint action set denoted associated possible rewards rewards stochastic simplicity simply refer expected reward decision problem cooperative agent reward drawn distribution reflecting utility assessment agents 
agents wish choose actions maximize expected reward 
adopt standard game theoretic terminology 
randomized strategy agent distribution set distributions agent action set 
intuitively denotes probability agent selecting individual action strategy deterministic strategy profile collection strategies agent expected value acting fixed profile easily determined 
deterministic think joint action 
reduced profile agent strategy profile agents denoted 
profile strategy best response agent expected value strategy profile maximal agent agent 
better strategy say strategy profile nash equilibrium iff component best response 
agent note cooperative games deterministic equilibria easy find 
equilibrium joint action optimal greater value 
example consider simple agent stage game agents actions disposal bac de respectively 
sq equilibria optimal expect agents play learning coordination games action selection difficult multiple optimal joint actions 
instance vl jw example agent reason prefer actions 
choose randomly way reflecting personal biases risk choosing suboptimal uncoordinated joint action 
general problem equilibrium selection addressed ways 
instance communication agents admitted impose conventions rules restrict behavior ensure coordination 
entertain suggestion coordinated action choice learned repeated play game agents :10.1.1.135.717
repeated play random selection similar agents large population object considerable study 
see interesting issues emerge 
especially simple effective learning model achieving 
agent keeps count number times agent action past 
game encountered treats relative frequencies moves indicative current randomized strategy 
agent assumes plays action probability sb ced set strategies forms reduced profile agent adopts best response 
play updates counts appropriately actions agents 
think counts reflecting beliefs agent regarding play agents initial counts weighted reflect priors 
simple adaptive strategy converge equilibrium simple cooperative games assuming agents randomize multiple best responses exist converge optimal equilibrium appropriate mechanisms adopted probability coordinated equilibrium interactions arbitrarily high increasing sufficiently 
hard see agents reach equilibrium remain best response reinforces beliefs agents coordinated equilibrium remains force 
note game theoretic models assume agent observe actions executed counterparts certainty 
pointed addressed assumption unrealistic 
general model allows agent obtain observation related stochastically actual joint action selected denotes probability observation obtained agents joint action performed 
investigate model mention subsumes special cases describe 
reinforcement learning action selection difficult agents unaware rewards associated various joint actions 
case reinforcement learning agents estimate past experience expected reward associated individual joint actions 
refer survey rl techniques :10.1.1.134.2462
simple understood algorithm single agent learning learning 
formulation learning general sequential decision processes sophisticated need 
stateless setting assume jk value provides estimate value performing individual joint action agent updates jk estimate pl acm sample follows jk sample agm experience obtained agent action performed resulting reward learning rate governing extent new sample replaces current estimate 
decreased slowly learning actions sampled infinitely learning converge true values actions single agent setting generally taken denote long term value convergence learning depend exploration strategy 
agent try actions time requirement perform actions currently estimated best 
course hope enhance performance learning sense intuitively bias selection better actions 
distinguish forms exploration 
exploration agent randomly chooses actions uniform probability 
attempt learned improve performance aim simply learn values 
exploration agent chooses best estimated action probability chooses action probability exploitation probability increased slowly time 
call nonoptimal action choice exploration step exploration probability 
nonoptimal action selection uniform exploration biased magnitudes values 
popular biased strategy boltzmann exploration action chosen probability actions agents 
contrast ils illustrated example il learn values actions jal learn values joint actions exploration strategies require care 
example currently values joint actions expected value performing depends crucially strategy adopted determine relative values agent jal beliefs strategies agents 
empirical distributions possibly initial weights fictitious play 
agent instance assumes agent choose actions accordance current beliefs empirical distribution action choices 
general agent assesses expected value individual action ev jk 
agent temperature parameter decreased time exploitation probability increases done way convergence assured 
existence multiple agents simultaneously learning potential impediment successful employment learning rl generally multiagent settings 
agent learning value actions presence agents learning nonstationary environment 
convergence values guaranteed 
naive application learning mass successful ensure agent strategy eventually settle questions explore 
application learning rl methods met success past 
distinct ways learning applied multiagent system 
say marl algorithm independent learner il algorithm agents learn values individual actions equation 
words perform actions obtain reward update values regard actions performed agents 
experiences agent take form acm action performed reward 
agent unaware existence agents identify actions reason believe agents acting strategically appropriate method learning 
course conditions hold agent may choose ignore agents actions 
joint action learner jal agent learns values joint actions opposed individual actions 
experiences agent form agm joint action 
implies agent observe forming action state incorporates consideration values possible states action leads 
learning method fact basic stochastic approximation technique 
misuse notation terminology emphasize connection action selection 
values just values implementing strategy exploration note ils viewed special cases partially observable model mentioned allowing experiences pl agi acm form action performed joint action observation 
preliminary version studies methods model 
comparing independent joint action learners compare relative performance independent joint action learners simple coordination game form described thing note ils exploration deem choices average better 
instance values action converge say executed probability de executed 
course point due stochastic nature strategies decay learning rate expect learned values identical agents converge reason prefer action 
unfortunately biases need coordinated 
pursuing direction consider case ils boltzmann exploration strategies 
exploitation known values allows agents coordinate choices reasons equilibrium learning methods agents know reward structure 
shows probability ils selecting op expression ev justifiable assumption agents selecting actions independently 
reasonable assumption choices uncorrelated correlated choices 
correlations emerge due dynamics belief updating agents aware choosing optimal joint actions likelihood convergence optimal joint actions dependency penalty temp decay temp decay probability choosing optimal joint action independent learners joint action learners likelihood convergence optimal joint action number interactions penalty convergence coordination ils averaged trials 
likelihood convergence opt 
equilibrium function penalty averaged trials 
joint action function number interactions 
temperature parameter ww decayed factor st interaction 
see ils coordinate quite quickly 
preference equilibrium point equilibria attained half trials 
show convergence values note values actions equilibria attained pl rq tended actions tended probability optimal action selection increase smoothly individual trials averaged probabilities reflect likelihood having reached equilibrium time exploration probabilities 
point faster convergence different parameter settings decaying temperature rapidly 
defer general remarks convergence section 
shows convergence circumstances 
perform somewhat better fixed number interactions shown graph 
information disposal convergence enhanced dramatically 
retrospect surprising 
able distinguish values different joint actions ability information circumscribed action selection mechanism 
agent maintains beliefs strategy played agents exploits actions expected value beliefs 
words value individual actions plugged exploration strategy values learned ils distinction compute explicit belief distributions joint values updating directly 
agents may fairly sure relative values joint actions boltzmann exploration exploit 
correlation especially frequencies particular joint actions ignored 
key reason difference ils larger difference values bias boltzmann exploration slightly estimated optimal action 
note strategies alleviate problem certain degree 
convergence game structure simple game considered isn difficult see independent learners joint action learners converge equilibria long decreasing exploration 
convergence smooth illustrated 
know consider ways game structure influence dynamics learning process 
consider class games variable expected penalty game penalty deterministic equilibria pl rq pl rq preferred 
say find third actions unattractive random exploration 
il average rewards values bac quite low jal beliefs strategy actions low expected value 
similar remarks apply self confirming nature equilibria virtually assure convergence sq closer lower likelihood agents find third actions unattractive stochastic nature exploration means occasionally actions high estimated utility convergence optimal equilibria occur 
shows probability convergence optimal influenced magnitude penalty surprisingly different equilibria attained different likelihoods 
far examples show agents proceeding direct route equilibria albeit various rates destinations chosen stochastically 
unfortunately convergence straightforward general 
consider climbing game results shown general pattern holds true ils 
prob 
actions strategy agent prob 
prob 
prob 
performed joint actions joint actions joint action joint action joint action joint action joint action joint action joint action joint action joint action number interactions number interactions strategy climbing game joint actions climbing game prob 
actions strategy agent prob 
prob 
prob 
number interactions strategy climbing game initially learners certainly going play nonequilibrium strategy pl profile seen clearly figures 
settle point long exploration continues agent soon find attractive long continues primarily choose nonequilibrium point sq attained agent tracks move begins perform action equilibrium reached agents remain 
phenomenon obtain general allowing conclude multiagent learning schemes proposed converge equilibria surely 
conditions required cases learning rate decreases time rk agent samples actions infinitely 
parameter settings figures initial temperature decayed rate probability agent choosing action nonzero 
agent exploration strategy 
random variable denoting event nonoptimal action taken estimated values time conditions required learning third implemented appropriately appropriately decayed temperature ensure second 
furthermore ensures agents adopt deterministic exploration strategies strictly correlated 
condition ensures agents exploit knowledge 
context play variants exploration strategy asymptotically myopic 
necessary ensure equilibrium reached 
conditions theorem random variable denoting probability deterministic equilibrium strategy profile played time ils je intuitively somewhat informally dynamics learning process behaves follows 
agents equilibrium nonzero probability moving equilibrium generally requires dense series exploratory moves agents 
probability occurring decreases time making likelihood leaving equilibrium just obtained vanish time ils 
point agents estimated values nonequilibrium likelihood state affairs remaining vanishes time 
example consider climbing game 
agents play regularly agent required explore 
sufficient sampling dh action agent simultaneously exploring moving away look attractive best reply adopted 
decreasing exploration ensures odds simultaneous exploration de crease fast assure happens high probability 
similar reasoning shows best reply path eventually followed point equilibrium 
theoretical guarantee convergence may limited practical value sufficiently complicated games 
key difficulty convergence relies decaying exploration necessary approximate best response condition fictitious play 
gradual decay time required shift current entrenched strategy profile better profile long 
agents profile large distance terms best reply path equilibrium shift required take longer occur decay exploration 
furthermore pointed probability concurrent sufficiently small ensure expected value shift best reply path greater shift introduce delays process 
longer delays lower learning rate requiring experience overcome initially biased estimated values 
key drawback know joint values fact beliefs lot experience require considerable amount contrary experience overcome 
example shift significant amount time needed switch observe performing overcome large degree belief continue doing don report initial experiments windows finite histories base beliefs shown considerable practical value 
biasing exploration strategies optimality thing notice marl strategies described ensure convergence optimal equilibrium 
little done case ils 
considerably information disposal form joint values 
example penalty game agents converge suboptimal pl acd sq equilibrium agents learned game structure realize coordinated strategy profile suboptimal 
attained usual exploration strategies permit escape equilibrium small diminishing probability 
intuitively imagine agents trying break equilibrium attempt reach desirable point pl rq say 
instance agent sample number times order induce switch strategy fact worthwhile penalties received attempt compensated long run sequence high rewards obtained optimal equilibrium achieved 
note type action selection runs counter requirement best response cho fictitious play histories appropriately chosen length shown converge 
imagine il bias action selection values high variance adhere multimodal distribution indicative agent acting simultaneously run contrary spirit ils 
accumulated reward combined strategy wob strategy nb strategy ob strategy number interactions sliding avg 
reward penalty game sen random exploration 
type switch requires agents intentionally choose immediately suboptimal actions 
ultimately decision attain long run optimal equilibrium expense finite sequence penalties cast sequential decision problem 
instance rewards highly discounted agents may risk deviating suboptimal equilibrium 
decision problem especially move complex settings intractable 
consider augmented exploration strategies encourage long run optimality 
propose myopic heuristics current state tend induce long run optimal behavior 
heuristics optimistic boltzmann ob agent action maxq jk 
ag choose actions boltzmann exploration strategy suffice maxq value weighted ob wob explore boltzmann factors maxq optimal match 
combined xk maxq aq ev ev choose actions boltzmann exploration xk value ob optimistic sense agent assesses actions agents act order match choice action 
wob realistic version ob assessment action tempered likelihood matching current beliefs 
combined strategy flexible uses normal exploration strategy introduces maxq factor bias actions potential coefficient allows tune bias 
experiment uses performed preliminary experimentation heuristics 
illustrates results heuristics normal boltzmann nb exploration penalty game 
shows sliding average reward obtained interactions strategy 
shows convergence behavior penalties incurred attempting reach optimal equilibrium 
nb behaves converging optimal suboptimal equilibrium 
surprisingly ob fares poorly presence multiple equilibria impossible behaves reasonably simpler games 
agents coordinate permitted account strategy agent 
wob circumvents difficulty ob beliefs ensure coordination converges optimal equilibrium time 
combined strategy guarantees long run optimality better performance way 
draw formal time think myopic heuristics exploration deserves considerably study 
methods combined strategy allow problem dependent tuning exploration strategy especially promising 
focusing particular sequential optimality criteria intelligent parameter tuning possible 
concluding remarks seen described basic ways learning applied multiagent cooperative settings examined impact various features success interaction equilibrium selection learning techniques rl techniques 
demonstrated integration requires care learning nearly robust single agent settings 
convergence guarantees especially practical complex games new exploration heuristics may help regard 
proposals put forth closely related 
tan sen hale apply rl independent agents demonstrate empirical convergence :10.1.1.55.8066
results consistent properties convergence points optimal equilibrium considered 
wheeler narendra develop learning automata la model fully cooperative games 
show model agents converge equilibrium unique pure strategy equilibrium coordination problem interests addressed directly 
furthermore la model different learning model address 
connections models deserve exploration 
number important directions remain pursued 
obvious generalization ideas general sequential problems learning designed instance addressed 
interesting issue emerges tries directly apply fictitious play models setting estimating value actions values states actual value obtained hinge coordination lack thereof states 
application generalization techniques deal large state action spaces great importance especially multiagent domains size joint action spaces grow exponentially number agents 
expect ideas generalize settings zero sum games fictitious play known converge 
leslie kaelbling michael littman helpful discussions early stages daniel koditschek helpful pointers 
supported nserc ogp iris ii project ic 
boutilier 
learning conventions multiagent stochastic domains likelihood estimates 
proc 
th intl 
conf 
uncertainty ai pp portland 
boutilier 
planning learning coordination multiagent decision processes 
proc 
th conf 
theor 
aspects rationality knowledge pp amsterdam 
brown 
iterative solution games fictitious play 
koopmans editor activity analysis production allocation 
wiley new york 
claus boutilier 
dynamics reinforcement learning cooperative multiagent systems 
aaai 
multiagent learning pp providence 
fudenberg kreps 
lectures learning equilibrium strategic form games 
core foundation louvain la neuve belgium 
fudenberg levine 
steady state learning nash equilibrium 
econometrica 
hu wellman 
self fulfilling bias multiagent learning 
proc 
icmas pp kyoto 
kaelbling littman moore :10.1.1.134.2462
reinforcement learning survey 
art 
intel 
res 
kalai lehrer 
rational learning leads nash equilibrium 
econometrica 
rob 
learning mutation long run equilibria games 
econometrica 
littman 
markov games framework multi agent reinforcement learning 
proc 
th intl 
conf 
machine learning pp new brunswick nj 
shapley 
fictitious play property games identical interests 
econ 
th 
myerson 
game theory analysis conflict 
harvard university press cambridge 
robbins munro 
stochastic approximation method 
annals math 
stat 
sandholm crites 
learning iterated prisoner dilemma 
biosystems 
sen hale 
learning coordinate sharing information 
aaai pp seattle 
shoham tennenholtz 
emergent conventions multi agent systems initial experimental results observations 
kr pp cambridge 
shoham tennenholtz 
synthesis useful social laws artificial agent societies 
proc 
aaai pp san jose 
singh jaakkola littman ri 
convergence results single step policy reinforcement learning algorithms 
machine learning 
appear 
tan :10.1.1.55.8066
multi agent reinforcement learning independent vs cooperative agents 
proc 
th intl 
conf 
machine learning pp amherst ma 
watkins dayan 
learning 
machine learning 
wei 
learning coordinate actions multi agent systems 
proc 
ijcai pp chambery fr 
wheeler narendra 
decentralized learning markov chains 
ieee trans 
aut 
control 
peyton young 
evolution conventions 
econometrica 
