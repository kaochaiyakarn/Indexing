solving large weakly coupled markov decision processes nicolas meuleau hauskrecht kee kim leonid peshkin leslie pack kaelbling thomas dean computer science department box brown university providence ri nm kek 
ldp cs brown edu craig boutilier department computer science university british columbia vancouver bc canada cs ubc ca technique computing approximately optimal solutions stochastic resource allocation problems modeled markov decision processes mdps 
exploit key properties avoid explicitly enumerating large state action spaces associated problems 
problems composed multiple tasks utilities independent 
second actions taken respect resources allocated task influence status task 
view task mdp 
mdps weakly coupled resource constraints actions selected mdp restrict actions available 
describe heuristic techniques dealing classes constraints solutions individual mdps construct approximate global solution 
demonstrate technique problems involving tasks approximating solution problems far reach standard methods 
markov decision processes proven tremendously useful models stochastic planning decision problems 
computational difficulty applying classic dynamic programming algorithms realistic problems spurred research techniques deal large state action spaces 
include function approximation reachability considerations aggregation techniques :10.1.1.34.3923
general method tackling large mdps decomposition 
mdp specified terms set pseudo independent subprocesses automatically decomposed subprocesses 
solved solutions merged construct approximate global solution 
techniques divided broad classes supported part darpa rome labs planning initiative part nsf iri iri supported nserc research ogp iris ii project ic undertaken author visiting brown university 
generous support foundation 
copyright american association artificial intelligence www aaai org 
rights reserved 
state space mdp divided regions form mdp union loose sense treated concurrent processes loosely forming global mdp 
focus form decomposition offers great promise allowing solve exponentially smaller global mdp 
solutions effectively guide search global solution directly dramatic improvements solution time obtained 
problems address sequential stochastic resource allocation problems 
number different tasks objectives addressed actions consist assigning various resources different times tasks 
assume tasks additive utility independent utility achieving collection tasks sum rewards associated task 
addition assume state space mdp formed number features apart resources relevant specific task 
furthermore assignment resources task bearing features relevant task 
means task viewed independent subprocess rewards transitions independent fixed action policy assignment resources 
degree independence generally easy find optimal policy 
resources usually constrained allocation resources task point time restricts actions available point time 
complex optimization problem remains 
resource constraints processes completely independent 
solved individually optimal global solution determined concurrent execution optimal local policies solution time determined size 
resource constraints local optimal solutions computed merging non trivial 
question best exploit local solutions determine global policy subject model applied generally processes action broken components affecting different process independently resource allocation specific example 

note resource allocation problems action space extremely large possible assignment resources tasks making standard approximation methods programming unsuitable 
singh cohn treat version problem constraints feasible joint action choices 
observe value functions produced solving obtain upper lower bounds global value function 
bounds improve convergence value iteration form action elimination global mdp 
unfortunately algorithm requires explicit state space action space enumeration rendering impractical mdps 
take different approach willing sacrifice optimality assured singh cohn algorithm computational feasibility 
develop greedy techniques deal variants problem types resource constraints differ 
hallmark heuristic algorithms division phases 
line phase computes optimal solutions value functions associated individual tasks 
value functions compute gradient heuristic search assign resources task current state 
action taken resource assignments reconsidered light new state entered system 
problem formulation motivated military air campaign planning problem tasks correspond targets global constraints total number weapons available instantaneous constraints induced number available aircraft number weapons may deployed single time step 
actions inherently stochastic outcomes problem fully observable 
type problem structure fairly general seen domains stochastic job shop scheduling allocation repair crews different jobs disaster relief scheduling wide variety bandit type problems 
able solve problems type involving hundreds tasks state space exponential number thousands resources action space factorial number 
problems far reach classic dynamic programming techniques typical approximation methods programming 
section discuss relevant background mdps define specific problem class formally 
section describe markov task decomposition mtd means breaking large loosely coupled decision processes describe general terms solutions construct global solution presence various types action constraints 
section describe air campaign planning problem detail show particular characteristics problem especially suited mtd heuristic policy construction methods 
section demonstrates results mtd applied large instances problem 
conclude section remarks reasons tuple mtd success 
markov task sets finite markov decision process finite set states finite set actions transition distribution probability distribution ir bounded reward function 
intuitively denotes probability moving state action performed state denotes immediate utility associated transition action 
generally probabilities rewards non stationary depending time action performed transition 
mdp objective construct policy maximizes expected accumulated reward horizon interest 
focus finite horizon decision problems 
horizon aim construct non stationary policy value maximum 
standard dynamic programming methods compute sequence optimal stage value functions horizon interest optimal policy derived 
consider special form mdp suitable modeling stochastic resource allocation problems described 
markov task set mts tasks defined tuple vector state spaces set primitive states markov task vector action spaces set integers limit describing allocation amount resource task vector reward functions time ir specifying reward conditional starting state resulting state action time vector state transition distributions specifying probability task entering state previous state task action cost single unit resource instantaneous local resource constraint amount resource may single step global resource constraint amount resource may total 
assume finite horizon mts induces mdp obvious way state space consists may possible extend apply real valued amounts resources multiple resource types 
reward values stationary time index may omitted 
techniques may extended optimality criteria infinite horizon discounted average reward 
cross product individual state spaces available resources action space set resource assignments assignment feasible state sum exceeds total resources available state rewards determined summing original component rewards action costs transition probabilities multiplying independent individual task probabilities change resources determined action 
formulating flat mdp explicitly retain factored form possible 
goal find optimal non stationary policy local policy task maximizes subject constraints simplicity described involving single resource type 
general may multiple resources cost may subject local global constraints 
section problem resources interrelated constraints 
note allow model reusable resources machines shop floor local global constraints consumable resources raw materials global possibly induced local constraints 
finding optimal policy hard problem small equivalent mdp large 
practical purposes impossible solve exactly number tasks individual state spaces available resources small 
major source difficulty decision apply resource task influences availability resource tasks 
tasks exhibiting tremendous independence strongly interacting solutions 
local policy task take account state tasks precluding state space reduction general turn attention approximation strategies limit scope interactions 
markov task decomposition approximation strategy called markov task decomposition mtd 
mtd method divided phases 
offline phase value functions calculated individual tasks dynamic programming 
second line phase value functions calculate action function current state processes 

resource constraint sub processes solved individually local policies defined mappings global constraints consider case global resource constraint limit total number resources applied single step 
line phase compute component value functions current state task time step number resources remaining max 
expected cumulative reward optimal policy task starting state time resources 
words ignored tasks resources allocate task expect value 
useful note stage may suboptimal spend remaining resources 
relatively simple compute dynamic programming long way tightly bounding values considered 
section describe domain tight bounds quantities available 
hand proceed line phase 
faced particular situation described current state remaining global time step calculate action taken resources spent current time step applied task 
ignoring instantaneous constraints require 
allocating current time step generally optimal 
optimal allocation required take account contingencies probabilities value holding back contingencies 
solve complex optimization problem rely fact local value functions give idea value assigning resources task time furthermore implicitly determines policy task telling resources current step mtd works loop executed decision stage functions heuristically assign resources task 
determine action taken currently task arg max execute action observe resulting state compute remaining resources 
component mtd left open heuristic allocation resources tasks 
doing generally require specific domain knowledge 
describe greedy approach section works extremely air campaign planning domain fundamental characteristics problem hold true wide class problem domains 
approach plausible chosen optimally respect criteria described policy produced generally suboptimal reasons 
estimate utility allocation task exactly utility solving task resources clearly lower bound value achieve allocations step re evaluated 
particular resources task mtd allocates resources task current stage 
optimal bellman equations indicate optimal allocation take account course task reason contingencies regarding tasks assess value resources tasks 
adding instantaneous constraints quite difficult incorporate local constraints satisfying way 
obvious strategy simply enforce constraint line phase mtd 
result generation admissible policies may poor quality serious overestimate value function 
allocations determined step may assumption resources parallel 
despite potential drawbacks pursue strategy type application described section 
type strategy appeal computational simplicity leads reasonably results domain 
complex strategies dealing instantaneous constraints easily accommodated mtd framework 
strategies subject study 
example air campaign planning simplified air campaign planning problem tasks correspond targets resource types weapons planes 
status target damaged 
target window availability length denoted reward damaged window reward received probability single weapon damage target noisy model assumed multiple weapons note policy produced mtd constructed incrementally isn policy se plans states reached 
single hit sufficient damage target individual weapons hit probabilities independent 
probability weapon missing target 
cost weapon 
plane capacity take fixed simplicity carry weapons 
plane loaded weapons assigned target deliver weapons 
consider variety different constraints 
section treat case constraint global constraint total number weapons 
section treat case constraint local constraint number planes single stage 
plane carry limited number weapons constraint planes induces constraint weapons 
sophisticated type local constraint previously 
action satisfy combine global weapon constraints local plane constraints section 
apply calculating component value functions discussion somewhat brief informal formal discussion detailed derivations provided forthcoming technical report 
problem component value functions considerably simplified 
state target damaged value need compute value state target 
second restricted window opportunity target need compute value function horizon equal number time steps target window opportunity 
window lengths typically shorter horizon entire problem considerably simplifies dynamic programming problem need compute factor strongly influences shape local value functions ultimately heuristic algorithm noisy transition model 
probability damaging target policy uses weapons steps depends weapons equal 
policies may differ expected utility depending choose allocate weapons time affects expected number weapons 
optimal send weapon time 
ignore cost plane 
weapons sent time instance optimal policy single target problem number weapon sent target time function 
optimal policy sends increasing number weapons step case weapons previous step failed damage target 
shows example single target policy window reward hit probability optimal allocation weapons greater cumulative total shown 
furthermore show increases monotonically point point remains constant point marginal utility resource allocation zero marginal utility resource negative cost weapon exceeds value small increase success probability allocated 
implies need compute significantly reducing effort needed compute 
target compute store results table line phase mtd 
dynamic programming equation max 
value spending weapons described terms expected reward due damaging target current time step second cost weapons third value trying damage target weapons left 
resource constraints resource constraints line phase mtd required 
tasks completely decoupled optimal policy described component value functions recall action required damaged target arg max window 
requires simple search values bounded 
weapon global constraints constraints number weapons available online phase crucial 
component value functions disposal current state targets number weapons remaining time goal choose weapons assign target state step line algorithm section maximize 
adopt greedy strategy 
define marginal utility assigning additional weapon target weapons assigned 
assign weapons target highest value current assignment weapons gradient ascent 
proceeds weapons assigned 
concavity local value functions assures proposition process described chooses maximize 
despite argued necessarily result optimal policy 
domain empirical results impressive discuss section 
plane instantaneous constraints unlimited number weapons required reach zero marginal utility generally deal constraints number simultaneously deliverable weapons number planes available 
strategy adopt similar greedily allocate planes weapons 
subtlety lies fact may optimal load plane capacity recall weapons plane delivered 
proceed time allocating planes active targets 
assume optimistically targets allocated optimal number weapons optimistic plane constraints weapon availability words computational reasons deal plane constraints current stage 
assume assigned planes weapons target far 
active target compute number bombs new plane carry min 
compute marginal expected utility assigning new plane active target case global constraints assign planes active targets greedily 
note marginal utility associated increasing number planes weapons previous section 
weapon plane constraints approach case necessarily complicated 
assigning weapons targets greedy strategy outlined section plane constraints accounted 
result assignment resources target 
action determined current stage assign planes target current stage suffice carry weapons 
action may infeasible planes required carry action available 
greedy deallocation reallocation process 
deallocation requires remove certain weapons current assignment 
greedily removing assigned planes note need range active targets 
intuitively proceed follows compute number weapons removed target deallocated single plane compute change utility accompany deallocation optimally reallocate weapons new target possibly target forced stage deallocate plane perform reallocation results smallest decrease utility 
see requires care 
point time list active targets planes deallocated 
target may consider assigning new weapons deallocated target 
want consider providing new plane current stage just taken away compute marginal utility adding weapon follows consider weapon time current stage 
target plane deallocated computed 
denote change utility assign new weapons 
number weapons removed active target planes deallocated satisfy instantaneous constraint 
compute value weapons optimally adding deallocated list temporarily simulating greedy algorithm described section 
difference measure marginal utility target deallocated list 
notice weapons taken reallocated value reallocation derived stages 
weapons assigned target 
simulating mean compute reallocation take place reallocated weapons won necessarily take place decide deallocate plane target 
quantities determine target plane deallocated 
active target define negative change expected value deallocating weapons plane target 
deallocate choosing target largest 
selected plane deallocated weapons reallocated greedily 
fact values computation stored purpose simulated reallocation imposed 
note weapons reallocated active target may cause require additional plane fact occur active targets 
deal simply allocate new planes 
deallocation plane may cause allocation planes process eventually terminate deallocated target reallocated weapons current 
empirical results validate heuristics tried instances air campaign problem compared optimal policy calculated flat dp greedy policy applies action highest expected immediate reward semi greedy policy applies action active target regard potential interactions calculation optimal policy dp infeasible problems moderate size 
instance solution time problem targets weapons order minutes targets weapons hours practically computable 
contrast execution time mtd shown 
instantaneous constraints mtd solve problem thousands targets tens thousands weapons minutes 
problem targets weapons planes imposing constraints solved minutes 
compare quality mtd optimal solutions produced dp restricted small problems tasks instantaneous constraints task instantaneous constraints computational demands dp 
compared greedy semi greedy strategies 
performance mtd encouraging closely tracking optimal performance greedy semi greedy note equal 
currently exploring sophisticated strategies prevent happening 
dp solution known value results show average reward obtained simulations process function initial global resources 
minutes minutes problem size problem size complexity mtd time execution function problem size problem size targets weapons planes 
graph left shows time solve problem local constraints 
graph right shows time problem local constraints planes 
mtd dp greedy mtd dp semi greedy greedy mg mg comparison quality policies generated mtd optimal dp greedy semi greedy strategies small test problems 
graph left shows results task problem local constraints 
graph right describes task problem local constraints 
values averaged runs 
policies suggests problems difficult differentiate mtd 
larger problems mtd compares favorably greedy methods showing performance target instantaneous constraints target constraints problems 
policy produced mtd performs substantially better greedy semi greedy policy 
problems reach classic dp 
method markov task decomposition solving large weakly coupled mdps particular stochastic resource allocation problems 
described instantiations technique dealing different forms resource action constraints 
empirical results air campaign problem extremely encouraging demonstrating ability mtd techniques solve problems thousands tasks 
key insights allowed approximately solve large mdps fashion 
ability decompose process pseudo independent subprocesses construct optimal policies value functions feasibly 
special features domain case noisy dynamics limited windows exploited solve effectively 
second value functions offer guidance construction policies account interactions processes 
special domain features convexity value functions offer guidance regarding appropriate heuristic techniques 
third line policy construction alleviate need reason fu mtd mtd semi greedy greedy greedy mg mg comparison quality policies generated mtd greedy semi greedy strategies large test problems 
graph left shows results task problem local constraints 
graph right describes task problem local constraints 
values averaged runs 
ture contingencies 
line methods popular crucial success line component mtd ability quickly construct actions heuristically component value functions :10.1.1.117.6173
mtd family algorithms exploit specific structure problem domain decisions effectively 
requires problem specified specific form advantage utility independence probabilistic independence action effects 
research focussed representations mdps structure explicit automatically discovering appropriate problem abstractions decompositions 
extent effective markov task decompositions automatically extracted suitable problem representations remains interesting open question 
barto bradtke singh :10.1.1.117.6173
learning act real time dynamic programming 
artificial intelligence 
bellman 
dynamic programming 
princeton university press princeton 
berry 
bandit problems sequential allocation experiments 
chapman hall london 
bertsekas 
tsitsiklis 
neuro dynamic programming 
athena belmont ma 
boutilier brafman geib 
prioritized goal decomposition markov decision processes synthesis classical decision theoretic planning 
proc 
ijcai pp nagoya 
boutilier dearden goldszmidt 
exploiting structure policy construction 
proc 
ijcai pp montreal 
dean givan 
model minimization markov decision processes 
proc 
aaai pp providence 
dean kaelbling kirman nicholson 
planning time constraints stochastic domains 
artificial intelligence 
dean kanazawa 
model reasoning persistence causation 
computational intelligence 
dean lin 
decomposition techniques planning stochastic domains 
proc 
ijcai pp montreal 
dearden boutilier 
abstraction approximate decision theoretic planning 
artificial intelligence 
howard 
dynamic programming markov processes 
mit press cambridge 
keeney raiffa 
decisions multiple objectives preferences value trade offs 
wiley new york 
nicholson kaelbling 
approximate planning large stochastic domains 
aaai spring symp 
decision theoretic planning pp stanford 
precup sutton 
multi time models temporally planning 
mozer jordan petsche editor nips 
mit press cambridge 
puterman 
markov decision processes discrete stochastic dynamic programming 
wiley new york 
singh cohn 
dynamically merge markov decision processes 
mozer jordan petsche editor nips 
mit press cambridge 
