adapting unknown smoothness wavelet shrinkage david donoho iain johnstone department statistics stanford university july attempt recover function unknown smoothness noisy sampled data 
introduce procedure sureshrink suppresses noise thresholding empirical wavelet coe cients 
thresholding adaptive threshold level assigned dyadic resolution level principle minimizing stein unbiased estimate risk sure threshold estimates 
computational ort procedure order log function sample size sureshrink smoothness adaptive unknown function contains jumps reconstruction essentially unknown function smooth piece reconstruction essentially smooth mother wavelet allow 
procedure sense optimally smoothness adaptive near minimax simultaneously interval besov scale size interval depends choice mother wavelet 
know previous authors traditional smoothing methods kernels splines orthogonal series estimates optimal choices smoothing parameter unable perform near minimax way spaces besov scale 
examples sureshrink advantages method particularly evident underlying function jump discontinuities smooth background 
key words 
minimax decision theory 
stein unbiased estimate risk 
james stein estimator 
besov holder sobolev triebel spaces 
nonlinear estimation 
thresholding wavelet coe cients 
white noise model 
nonparametric regression 
orthonormal bases compactly supported wavelets 

rst author supported berkeley nsf dms nasa contract nca att foundation 
second author supported part nsf dms nih phs gm att foundation 
wavelets decision theory optimal smoothing wavelets applications workshop france march workshop trends analysis curve data university heidelberg march 
contents sureshrink discrete wavelet transform thresholding noisy wavelet coe cients threshold selection sure threshold selection sparse cases main result estimation sequence space adaptive estimation besov bodies adaptive estimation single resolution level comparison adaptive linear estimates adaptive linear estimation james stein linear adaptation fourier discussion simulation results visual quality reconstruction hard thresholding estimated noise level literature appendix proofs theorems suppose noisy samples function yi ti zi ti zi iid 
goal estimate vector ti small mean squared error nd estimate depending yn small risk fjj ti ti order develop nontrivial theory usually speci es xed class functions supposed belong 
may seek estimator attaining minimax risk inf sup 
approach led theoretical developments considerable interest stone nemirovskii tsybakov 
practical point view di culty rarely corresponds usual situation data knowledge priori class repair di culty may suppose unknown member scale function classes may attempt behave simultaneously entire scale 
example sobolev scale set function classes indexed parameters degree di erentiability quantitative limit th derivative ff jj dm fjj cg dtm pinsker example shows construct estimates simultaneously minimax range methods perform asymptotically unknown quantities known 
results limited case smoothness measures 
scales function spaces sobolev spaces ff jj dm cg dtm linear methods attain optimal rate convergence class known nemirovskii donoho johnstone 
adaptive linear methods attain optimal rate convergence 
admits degree type smoothness unknown known estimate smooth functions adaptively 
section introduce method sureshrink isvery simple implement attains broader adaptivity properties previously proposed methods 
new results multivariate normal decision theory interesting right 
sureshrink ingredients 
discrete wavelet transform noisy data 
noisy data transformed discrete wavelet transform obtain noisy wavelet coe cients yj 

thresholding noisy wavelet coe cients 
sgn jyj denote soft threshold sets zero data absolute value pulls data origin amount wavelet coe cients yj subjected soft thresholding level dependent threshold level tj 

stein unbiased estimate risk threshold choice 
level dependent thresholds arrived regarding di erent resolution levels di erent wavelet transform independent multivariate normal estimation problems 
level xed data yj wj zj wishes estimate wj stein unbiased estimate risk yj gives estimate risk particular threshold value minimizing gives selection threshold level level 
slight modi cation recipe employed case data vector small norm case unbiased risk estimate noisy xed threshold employed 
brie describe examples method action 
depicts speci functions wavelet analyze repeatedly 
blocks 
piecewise constant function jumps 
bumps 
sum bumps ti si locations ti places jumps blocks heights hi widths si vary individual bumps form 
heavisine 
sinusoid period jumps 
doppler 
variable frequency signal sin 
precise formulas appear table 
examples chosen represent various spatially inhomogeneous phenomena 
regard blocks caricature acoustic impedance layered medium geophysics pro le certain images arising image processing problems 
regard bumps caricature spectra arising example nmr infrared absorption spectroscopy 
displays noisy versions functions 
noise independent 
displays outcome applying sureshrink case 
results qualitatively appealing reconstructions jump true object jumps reconstructions smooth true object smooth 
emphasize computer program parameters produced reconstructions user intervention permitted required 
sureshrink automatically smoothness adaptive 
section gives theoretical result shows smoothness adaptation near optimal 
sureshrink asymptotically near minimax large intervals besov sobolev triebel scales 
speed convergence optimum best smoothness condition obeyed true function long optimal rate speed limit set regularity wavelet basis 
increasingly high order wavelets wavelets vanishing moments smoothness speed limit may expanded arbitrarily 
cost expansion computational ort directly proportional smoothness wavelet employed 
linear methods kernel spline orthogonal series estimates ideal choice bandwidth unable converge minimax speed members besov sobolev triebel scales involving lp smoothness measures 
sureshrink achieve advantages classical methods level rates 
fact advantages plainly visible concrete problems object recovered exhibits signi cant spatial homogeneity 
illustrate give example accomplished representative adaptive linear method 
method applies james stein adaptive linear see section dyadic fourier corona littlewood paley blocks 
method related describe section proposal pinsker 
method number pleasant theoretical properties automatically achieves minimax rate linear estimates large intervals besov triebel sobolev holder scales 
shows adaptive linear method performs signi cantly worse sureshrink cases signi cant spatial variability 
small simulation study described section shows range sureshrink achieves level performance samples adaptive linear methods achieve samples 
avoid possible confusion emphasise method sureshrink described di ers variants visushrink discussed dj donoho johnstone kerkyacharian picard choice threshold 
data choice threshold sureshrink explicitly adaptive unknown smoothness better large sample mean square error properties 
comparative discussion see section 
sureshrink describe detail ingredients procedure 
discrete wavelet transform suppose yi 
family discrete wavelet transforms indexed parameters additional adjective periodic boundary adjusted 
construction relies heavily concepts daubechies meyer cohen 

xed value get matrix matrix yields vector wavelet coe cients wy simplicity exposition employ periodic version case transform exactly orthogonal inversion formula tw 
brief comments minor changes needed boundary corrected version section dj 
crucial detail transform implemented matrix multiplication sequence special nite length ltering steps result order transform 
choice wavelet transform essentially choice lter 
see strang daubechies 
vector convenient index elements scheme wj remaining element label 
coe th row ofw 
inversion formula yi wj cients wj denote expressing sum basis elements wj coe cients wj special case transform reduces discrete haar transform 
wj proportional 
proportional constant function 
wavelet coe cients measure di erences function various scales function reconstructed building blocks zero mean localized square waves 
case building blocks transform smoother square waves 
case vector wj plotted function continuous localized appearance motivates label wavelet 
bounded away extreme cases condition approximation wj mother wavelet arising wavelet transform ir described daubechies 
approximation improves increasing oscillating function compact support 
speak wj localized spatial interval size frequency near basis element wj increasingly smooth visual appearance larger parameter construction matrix daubechies shown parameter controls smoothness number derivatives smoothness proportional vectors wj outside range come types 
longer resemble dilations mother wavelet may longer localized 
fact including 
qualitatively low frequency terms 
second terms boundaries cases fail satisfy 
transform wj approximated dilation circularly wrapped version transform boundary adjusted boundary element wj approximated boundary wavelet de ned cohen 

displays wj speci cases haar wavelet daubechies wavelet coi daubechies nearly linear phase wavelet 
smoother wavelets broader support 
usual displays wavelet transforms mallat idea multiresolution decom position mallat bc 
adapts situation follows 
xi data wj denote partial reconstruction gross structure terms wj denote partial reconstruction terms resolution level scale recovered components usual examine behavior components displaying graphs 
functions wavelet 
contrast look just blocks heavisine functions see haar transform behaves figs 
daubechies transform behaves figs 

usual way display wavelet transforms look wavelet coe cients directly 

display level depicts wj line height proportional wj horizontal position low resolution coe cents displayed 
coe cients displayed wavelet analysis functions consideration 
note considerable sparsity wavelet coe cient plots 
plots coe cients displayed small fraction nonzero resolution dot inch laser printer 
interest note position nonzero coe cients high resolution number cluster discontinuities spatial inhomogeneities function instance data compression properties wavelet transform 
transform preserves sum squares wavelet coe cients sum squares concentrated smaller fraction components raw data 
comparison display haar coe cients object compression pronounced object blocks fact better case compression pronounced object heavisine transform 
thresholding noisy wavelet coe cients orthogonality discrete wavelet transform fundamental statistical consequence transforms white noise white noise 
yj wavelet coe cients yi collected model wj wavelet coe cient ti yj wj zj zj noise sequence 
wavelet coe cients noisy sample just noisy versions noiseless wavelet coe cients 
transforms estimators domain estimators domain isometry risks 
wj estimates wavelet coe cients estimate ti domain obtained losses obey parseval relation jj jj fjj connection goes direction estimator de nes estimator isometric risk 
data compression remarks meant create reader mental picture coe cients noiseless wavelet transform ectively zero 
accepting slogan reformulates problem recovering recovering coe cients signi cantly nonzero gaussian white noise background 
motivates thresholding scheme kills small yj keeps large yj particular soft thresholding scheme introduced instance 
shown results provide case wavelet transform 
illustrate works wavelet domain display haar transform noisy version blocks 
display thresholded version transform raw data reconstruction 
reconstruction obtained device selecting noisy wavelet coe cients level threshold tj applying threshold empirical wavelet coe cients level reconstruction obviously choice threshold crucial 
threshold selection sure bead dimensional vector xi multivariate normal observations mean vector 
particular xed estimator charles stein introduced method estimating loss unbiased fashion 
stein showed nearly arbitrary nonlinear biased estimator estimate loss 
write gi function rd rd stein showed di erentiable xi gi 
fkg consider soft threshold estimator xi apply stein result 
weakly di erentiable stein sense get quantity sure fi tg unbiased estimate risk sure 
consider estimator risk select threshold dx arg min sure arguing heuristically expects large dimension sort statistical regularity set law large numbers ensure sure close true risk optimal threshold case hand 
theory developed show hope justi ed 
computational evidence ts reasonable threshold selector 
vector dimension consists consecutive followed zeros 
white gaussian noise variance added 
pro le sure displayed resembles quite closely actual loss course know arti cial example 
sure principle select threshold applied data resulting estimate estimate mean vector 
estimate sparse noisy raw data 
note shrinkage non zero part signal 
optimization problem computationally straightforward 
suppose loss generality xi reordered order increasing 
intervals lie values sure strictly increasing 
minimum value ts data values 
arranged increasing order collection values sure may computed order additions multiplications appropriate arrangement calculations 
may cost order log calculations arrange order ort calculate ts order log 
scarcely worse order calculations required simply apply thresholding 
threshold selection sparse cases sure principle just described serious drawback situations extreme sparsity wavelet coe cients 
cases noise contributed sure pro le coordinates signal zero information contributed sure pro le coordinates signal nonzero 
consequently sureshrink employs hybrid scheme 
depicts results small scale simulation study 
dimension contained dc nonzero elements size independent noise added 
sure estimator ts applied 
amplitudes tried studied 
replications tried parameter combination root mean squared errors displayed 
evidently root mse tend zero linearly sparsity tends 
theoretical results section behavior unacceptable 
contrast portrays results experiment fixed thresholding estimator threshold set tf log independent data 
losses tend larger sure dense situations smaller near zero 
rationale choice log authors thoroughly dj 
displays results applying hybrid method label designed behave dense situations sparse ones 
performance roughly desired 
detail hybrid method works follows log de ne 
denote random subset half indices dg denote complement 
ts denote minimizers sure respect respective subsets indices additional restriction search range similarly ts 
de ne estimate arg min sure xi xi xi xi words half sample estimate threshold half sample convincing evidence signal non negligible set threshold log 
half sample scheme developed proof theorems 
practice half sample aspect estimate unnecessary 
practice simpler estimator derived xi xi ers performance bene ts simulations 
see 
apply multivariate normal theory wavelet setting 
de nition term sureshrink refers estimator assuming nand noise normalized standard deviation set xj yj yj xj estimator derives inverse discrete wavelet transform 
note fully automatic modulo choice speci wavelet transform 
appropriate arrangement computational ort involved order log scarcely worse linear sample size extensive experience computations macintosh show performance quite reasonable personal computers 
matlab command sureshrink takes seconds complete array size 
main result section unknown degree smoothness 
state result wemust de ne besov spaces 
popov 
denote th di erence pr kh 
th modulus smoothness lp wr jj fjj rh besov index de ned wr dh wr sup 
besov ball bp class functions ir satisfying lp standard besov spaces triebel 
measure smoothness includes various settings commonly measures 
example denote holder class functions jf cjs tj 
distributional derivative satisfying jfj 
similarly sobolev space 
besov scale essentially includes traditional spaces 
example space functions bounded variation superset subset 
similarly lp sobolev spaces contain bm contained bm 
theorem discrete wavelet analysis correspond wavelet having null moments continuous derivatives max minimax risk denoted inf sup bp sureshrink simultaneously nearly minimax sup bp bp words estimator knows priori degree type amount regularity object achieves optimal rate convergence attain knowing regularity 
class attains optimal rate sobolev class achieves optimal rate sobolev classes achieves optimal rate 
mentioned linear estimator achieves optimal rate lp sobolev classes result modi cation sureshrink achieves usual estimates optimal bandwidth known priori 
results lines proved 
particularly interesting result refers haar basis theorem denote class functions unit interval total variation denote sureshrink haar basis 
estimator simultaneously nearly minimax sup 
knowing priori limit total variation estimator behaves essentially knowing limit 
shows plausibility result 
estimation sequence space proof theorem uses method sequence spaces described dj 
key idea approximate problem estimating function nite noisy data problem estimating nite sequence wavelet coe cients contaminated white noise 
heuristic replacement follows 
due empirical wavelet coe cient yj wj zj discrete wj obeys wj dt certain wavelet 
terms continuous wavelet coe cients dt tempting act observations zj thing zj zj standard sequence 
due parseval relation fk wk approximation tempted act loss fk 
admittedly vague approximation heuristics lead study sequence space problem 
observe nite sequence data yj zj zj unknown 
wish estimate small squared error loss jj jj denote set wavelet coe cient sequences arising bp 
search method nearly minimax range 
suppose solve sequence problem 
certain conditions imply theorem 
speci cally big wavelet regularity estimator simultaneously near minimax sequence space problem applied empirical wavelet coe cients original problem study simultaneously near minimax original function space problem 
approximation arguments necessary establish correspondence discussed dj reasons space omit 
see brown low 
adaptive estimation besov bodies collections expansions arising functions bp related certain simpler sets dj call besov bodies 
sets jj jj jj jj js consider problem estimating observed gaussian white noise known priori lie certain convex set jj jj cg 
put short 
di culty estimation setting measured minimax risk inf minimax risk threshold estimates sup jj inf tj sup tj jj tj stands estimator tj yj dj shows rt 

threshold estimators nearly minimax 
furthermore dj show minimax risk minimax threshold risk sets equivalent constants sets provided large calibration sureshrink style estimator problem applying level set level 
xj yj particular adaptive threshold estimator 
yj xj theorem 
sup rt short knowing obtains results asymptotically know parameters 
result ective nite range parameters question 
minimax risk close minimax threshold risk solves problem adapting scale besov bodies 
theorem approximation arguments alluded section proves theorems 
adaptive estimation single resolution level theorem depends analysis adaptive threshold selection sure principle 
return setup section 
denote ideal threshold risk whichwe achieve information optimal threshold inf risk scalar setting 
course hope know ideal threshold attaining expression 
result says adaptive estimator performs know ideal threshold 
theorem uniformly ir jj jj log uniformly dd jj jj log comparison adaptive linear estimates brie explain informal fashion sureshrink may expected compare favorably adaptive linear estimates 
adaptive linear estimation james stein multivariate normal setting section james stein positive part estimate js xi js shrinkage coe cient cjs kxk kxk 
linear estimators smallest risk uses coe cient kk unknown quantity trying estimate linear represents unattainable ideal 
see james stein shrinkage coe cient js essentially estimate ideal shrinkage coe cient fact james stein estimate extremely job approaching ideal 
theorem consider ideal estimator statistic 
rd jj js jj jj jj pay price james stein ideal 
high dimensions price negligible 
apply james stein wavelet domain wj js xj inverting wavelet transform gives estimate call 
anumber nice adaptivity properties follow immediately theorem 
consider ideal linear shrinkage estimator statistic wj wis xj inverse wavelet transform id immediate corollary theorem id log hard see space besov scale covered theorem ideal estimator achieves constant factor minimax risk linear estimators 
minimax risk measured behaves certain 
statistic james stein estimate statistic log follows achieves optimal rate convergence linear estimates besov scale 
fact better adaptivity result previously established adaptive linear schemes holds broad scale spaces 
theory aside estimate practice 
gives example cases gures 
reconstruction noisier sureshrink 
seen display coe cients resolution level signi cant coe cients need kept james stein estimate keeps coe cients incurring large variance penalty 
obtain estimators acceptable performance spatially variable functions sureshrink adaptively keep large coordinates kill small ones 
adaptive linear estimator operates coordinates level multiplicative factor 
linear adaptation fourier suppose identify circular interpretation 
pinsker soviet literature consider adaptive linear estimators empirical fourier coe cients 
divides frequency domain corona uses linear ci weights chosen adaptively analysis fourier coe cients corresponding 
letting vi denote vector coe cients belonging th corona choice pinsker essentially ci ep vi propose adaptive linear scheme di ers pinsker choice ways 
propose dyadic dyadic fourier occur frequently littlewood paley theory frazier jawerth weiss triebel 
second corona shrink james stein estimate ci cjs vi nicer theoretical properties pinsker choice 
estimator get way shall label 
adaptive linear estimator 
theorem risk term log worse ideal linear estimator lp de ned obvious way 
ideal linear estimator constant shrinkage dyadic performance worse constant factor times performance called weights conclude constants replicates adaptive rate advantages pinsker choice 
ers advantages pinsker choice 
achieves optimal rate linear estimators range sobolev holder besov spaces 
theoretically adaptive linear estimator 
seen reconstructions 
answers signi cantly noisier obtained sureshrink 
result comparable disappointing performance 
deeper reason similarity derives littlewood paley theory frazier jawerth weiss 
discussion simulation results small scale simulation experiment conducted investigate performance methods 
objects study applied di erent methods noisy versions data sureshrink haar db wavelet bases wavelet basis nally procedure dj wavelet bases denoted 
uses xed threshold chosen yield minimax performance mean square error oracle 
threshold values tabulated dj 
dyadic sample sizes studied 
sample results table reports root loss fk square 
decided report risk loss averaged ensemble realizations replications told nearly story 
examples little quantitative di erence methods small visual di erence 
large sureshrink wavelets consistently outperforms linear adaptive obtaining equivalent precisions half half sample size 
extreme case object blocks performance shrinkage haar basis sample size comparable performance sample size 
results sureshrink remarkably similar 
visual quality reconstruction reader sureshrink reconstructions contain structure scales 
inherent method priori knowledge smoothness lack smoothness object 
occasional spurious ne scale structure reconstruction method able adapt spatially true ne scale structure 
readers may annoyed tendency sureshrink show small amount spurious ne scale structure demand thorough explanation 
presence ne scale structure demanded task minimizing norm loss involves tradeo noise bias 
norm balances equilibrium insures noise artifacts visible 
enhanced visual quality obtained keeping noise term tradeo minimum 
may obtained uniformly applying threshold log adaptive selection 
ensures essentially pure noise wavelet coe cients coe cients wj set zero thresholding 
resulting curve shows structure little noise 
discussion threshold selection log rule called visushrink connection optimum visual quality may dj 
hard thresholding persons asked possible hard thresholding jyj jyj place soft thresholding hard thresholding natural 
prefer soft thresholding various statistical advantages continuity rule simplicity sure formula 
principle results equally derived hard thresholding 
complicated sure formula required implement idea data 
proofs complicated 
resulting estimator called 
estimated noise level practical important estimate noise level data assume noise level known 
practice derive estimate nest scale empirical wavelet coe cients median kj 
believe important robust estimator median case ne scale wavelet coe cients contain small proportion strong signals mixed noise 
literature compared results considerable literature cross validation select bandwidth xed kernel smoothers compare johnstone hall 
earlier applications sure linear estimates compare li 
discussed applications wavelet thresholding density estimation johnstone kerkyacharian picard 
interesting topics reasons space 
appendix proofs theorems proceed reverse rst collecting tools establishing theorem nally returning theorem 
exponential inequalities 
rst recall basic exponential inequalities bounded variates hoe ding note corresponding inequality chi square variates yn independent ai yi bi yn pn yi 
tg expf nx bi ai xm sampled replacement fc cng suppose ci set xm xi ci 
tg expf nt zn 
elementary arguments pfj tg jsj max jj optimising tg nt preparatory propositions 
bound deviation unbiased risk estimate expectation 
recapitulate setting section suppose xi independent 
fd denote empirical distribution function ig 
xi denote mean squared error soft threshold estimate single ordinate de ne particular stein unbiased risk estimate fd jj ud sure ifx ifx expectation fd 
study deviation zd ud fd uniformly td log proposition uniformly ir sup fd td log proof combining bound write zd pd yi zero mean summands uniformly bounded hoe ding inequality gives xed arbitrary rd rdd expf distinct nd fi 
ud ud ift nd may bound fd fd recalling td fd td long jt zd dtd nd dtd choose tj td clearly ad rdd dd ed td dd tj rdd ed sup sup jt jz tj rdd choose dtd ed contained nd tj tj rdd tj tj rdd say large tj tj inequality rdd 
hoe ding tj tj rdd cardinality ad dd td set rd blog log expf expf ad td dd log 
may rephrase su sg ces convergence claimed 
proposition uniformly ir td log fi jj log proof 
similar proposition simpler uses hoe ding inequality 
notation set ci fd xm fi fi fd xm rdd expf tj td follows jt jz dtd small dtd rdd rd blog rdd pf sup td td proposition yields result 
rdd lemma xi independent dd pfs dd sup proof simple statement tails non central chi squared distribution 
write xi zi zi 
event may rewritten ad fd dd fd lower bound 
elementary inequalities cd fd bd cd easily veri ed dd large 
bd apply exponential inequality obtain dd bd expf expf dg log proof theorem 
decompose risk outcome pre test event ad fs dd goal showing jj jj ad log jj jj log event ad xed thresholding xi ad dd oracle inequality dj shows conversely log min dd lemma follows xi rst note event ad log dd dd ad condition holds true 
event ac adaptive half sample thresholding applies 
denote expectation distribution xi ei denote expectation random choice half sample dr eif xi ti xi ti fi resp fi fd denote empirical distribution functions ig resp dg set symmetry wehave ti fi ti fi ti fi complete proof su ces show ti fi log natural decomposition ei ti fi ti fi ei ti fi rmin fi ei rmin fi rmin fd rmin tf note rmin fd 
fi fd fi fi simple observation rmin jjr jj conclude fi fd jj log proposition 
ud denote unbiased risk estimate derived subset proposition ti fi ud ti ti rmin fi ei jjr fi putting obtain 
jj log proof theorem 
jj jj small pretest dd high probability lead xed threshold tf log error term theorem arises empirical process uctuations connected minimization sure germane improved 
decompose risk ad asd 
ad xed thresholding exploit inequalities proved dj conclude dr dx dr jj jj log jj jj large large derivations inequalities bound remaining term 
symmetry dr ei xi ti dg cauchy schwartz inequality noting limited translation structure get dr cd arguing similarly note hypothesised bound implies fd dd fd dd bd cd exponential inequality standard gaussian inequalities give bd expf log cd expf dd imply log pd log shows ofr completes proof 
proof theorem 
de nitions write jj jj xj jjj jk jk jk 
speci ed theorem levels theorem jj inf tj jk cj tj jj jjj cj jj jjj maximizing sup sup inf tj tj jj rst term right side precisely remains verify chosen error terms 
error term negligible log 
jjj term sj negligible log 
requirements compatible note theorem requires jjj jjj guaranteed sj holds log compatible log long proof theorem 
rst recall risk positive part james stein estimator js larger original james stein estimator restriction shrinkage coe cient positive dropped 
stein unbiased estimate risk alternatively lehmann jensen inequality wehave direct calculation jj js jj jj js jj jjxjj jj jj jj jj jj jj jj jj di erence bounded jj jj brown low 

asymptotic equivalence nonparametric regression white noise 
manuscript 
cohen daubechies jawerth vial 
multiresolution analysis wavelets fast algorithms interval 
comptes rendus acad 
sci 
paris 

daubechies 

orthonormal bases compactly supported wavelets 
pure applied mathematics 
daubechies 
lectures wavelets siam philadelphia 
devore popov 

interpolation besov spaces 
trans 
am 
math 
soc 
donoho johnstone 
minimax risk balls lq loss 
technical report department statistics stanford university 
donoho johnstone 
minimax estimation wavelet shrinkage 
technical report department statistics stanford university 
donoho johnstone 
ideal spatial adaptation wavelet shrinkage 
technical report department statistics stanford university 
tentatively accepted biometrika donoho johnstone kerkyacharian picard 

wavelet shrinkage 
technical report department statistics stanford university 
yu 
pinsker 
learning algorithm nonparametric ltering 
automat 

russian 
frazier jawerth 

decomposition besov spaces 
indiana univ math 

frazier jawerth discrete transform decomposition distribution spaces 
journal functional analysis 
frazier jawerth weiss littlewood paley theory study function spaces 
nsf cbms regional conf 
ser mathematics 
american math 
soc providence ri 

adaptive asymptotically minimax estimates smooth signals 


risk bound sobolev class regression 
ann 
statist 

johnstone hall 
empirical functionals cient smoothing parameter selection 
roy 
stat 
soc 
appear 
johnstone kerkyacharian picard 
estimation une de par methode ondelettes 
appear comptes rendus acad 
sciences paris 
lehmann 
theory point estimation 
wiley new york 
meyer 
ondelettes bases 
revista mathematica 

li 
stein unbiased risk estimates method generalized cross validation 
ann 
statist 

mallat 

multiresolution approximation wavelet orthonormal bases ir 
trans 
amer 
mat 
soc 
mallat 

theory multiresolution signal decomposition wavelet representation 
ieee transactions pattern analysis machine intelligence 
mallat 

channel decompositions images wavelet models 
ieee transactions acoustics speech signal processing 
meyer 

ondelettes 
paris hermann 
meyer 

ondelettes sur 
revista mathematica 
nemirovskii 
nonparametric estimation smooth regression functions 

akad 
nauk 
ssr 

russian 
comput 
syst 
sci 
english 
nemirovskii tsybakov 
rate convergence nonparametric estimates maximum likelihood type 
problems information transmission 


spline smoothing asymptotic ciency 
ann 
statist 
pinsker 
optimal ltering square integrable signals gaussian white noise 
russian problems information transmission english 


new thoughts besov spaces 
duke univ math 
series 
number 
stein 
estimation mean multivariate normal distribution 
annals statistics 
stone 

optimal global rates convergence nonparametric estimators 
ann 
statist 

triebel 
theory function spaces 
birkhauser verlag basel 
list figures 
spatially variable functions 

formulas 

functions gaussian white noise rescaled signal noise ratio sd 
sureshrink reconstruction soft thresholding nearly symmetric daubechies wavelet cuto 
reconstruction de ned section cuto 
typical rows wavelet transform matrix corresponding cases 

mallat multiresolution decomposition basic functions wavelet 

mallat multiresolution blocks heavisine haar wavelets 
plot wavelet coe cients 
display depicts line height proportional wj horizontal position 
wavelet coe cients haar wavelet 
compare amounts compression 

sureshrink reconstruction haar wavelet 
raw wavelet coe cients data coe cients thresholding 

illustration choice threshold sure 
see section 
root mean squared errors simulated data varying levels sparsity threshold chosen sure log variants hybrid method described design section 
reconstructions noisy data wavelet 
table 
formulas test functions blocks 
tj sgn tj hj bumps 
tj wj jtj tj hj wj heavisine 
sin sgn sgn doppler 
sin epsilon 
