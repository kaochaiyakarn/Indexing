implicit imitation multiagent reinforcement learning bob price craig boutilier department computer science university british columbia vancouver canada cs ubc ca imitation actively studied effective means learning multi agent environments 
allows agent learn act optimally passively observing actions cooperative teachers experienced agents environment 
propose straightforward imitation mechanism called model extraction integrated easily standard model reinforcement learning algorithms 
roughly observing mentor similar capabilities agent extract information capabilities unvisited parts state space 
extracted information accelerate learning dramatically 
illustrate benefits model extraction integrating prioritized sweeping demonstrating improved performance convergence observation single multiple mentors 
stringent assumptions regarding observability possible interactions common abilities briefly comment extensions model relax 
application reinforcement learning rl multiagent systems offers unique opportunities challenges 
agents viewed independently trying achieve ends interesting issues interaction agent policies resolved appeal equilibrium concepts :10.1.1.40.3185:10.1.1.135.717
possibility agents may coordinate policies mutual gain distribute search optimal policies communicate partial results offers accelerating rl enhancing agent performance :10.1.1.55.8066:10.1.1.40.3185
way individual agent performance improved having novice agent learn reasonable behavior expert mentor 
type learning brought explicit teaching demonstration sharing privileged information elaborate psychological theory imitation :10.1.1.75.7884:10.1.1.40.3185:10.1.1.27.292
imitation agent exploration ground observations agent behaviors capabilities resolve ambiguities observations arising partial observability noise 
common thread mentor guide exploration observer 
guidance typically achieved form explicit communication mentor observer 
direct form teaching involves observer extracting information mentor mentor making explicit attempt demonstrate specific behavior interest 
develop model implicit imitation observation mentor 
roughly agent observes state transitions induced mentor actions 
assuming actions similar unknown action taken mentor agent information update value function 
addition mentor provide hints observer parts state space may worth focusing attention 
observer attention area take form additional exploration area additional computation brought bear agent prior beliefs area 
derive new technique model extraction independent specific rl algorithm best suited model methods 
illustrate effectiveness empirically incorporating prioritized sweeping 
model advantages direct forms imitation teaching 
require agent explicitly play role mentor teacher 
observers learn simply watching behavior agents mentors mentor shares certain subtasks observer observed behavior incorporated indirectly observer 
important situations observer learn mentor unwilling unable alter behavior teach observer communicate information 
example common communication protocols may able agents designed different developers internet agents agents may competing may simply incentive agent provide information agent learns observation exploit existence multiple mentors essentially distributing search 
observer constrained directly imitate mentor decide imitation worthwhile 
assume observer knows actual actions taken mentor mentor shares reward function goals mentor 
strict assumptions model generalized interesting ways describe 
generalizations subject ongoing research 
basic model assumptions underlying model extraction section 
develop model extraction algorithm demonstrate simple mechanism focus agent updating mechanism promising areas state space section illustrate performance algorithm empirically section 
conclude section remarks related discussion current research especially respect relaxation certain modeling assumptions 
model assumptions model assumes number agents acting achieve objectives common environment 
keep model agents 
observer reinforcement learner observe aspects mentor behavior 
extension multiple agents viewed adding potential mentors system consider rl problem facing single observer mo ao pro hsm am prm rmi agents act control multiagent markov decision process mmdp agents select actions independently joint actions determine stochastic state transitions order focus imitation getting issues strategic reasoning game theoretic concepts equilibria assume agents actions agent ignore behavior predicting effect actions system reward receives 
factor mmdp standard single agent mdps pertaining observer mentor respectively 
fori fo mg state space set reasons consistency term mentor describe agent observer learn mentor unwilling participant 
say mentors learners 
viewed form stochastic game shapley original formulation involved zero zum assumption different terminology avoid confusion 
actions pri tjs stochastic state transition function reward function defined 
instance grid world assume mentor observer move affecting ability move impact reward receives occupying certain grid cells 
observer learn observing mentor relationship space potential behaviors implement 
especially strong assumptions regard 
agents identical state spaces sm 
second observer capabilities mentor tjs pro tjs common 
assumptions description imitation task somewhat simpler relaxed 
essential feature analogical mapping state transitions observations mentor recast terms observer understands see section :10.1.1.30.5471
formulation mapping trivial 
assume reward functions related specific way 
initially observer know transition model pro assume knows reward 
typical rl models learner prior knowledge consistent interpretation rl automatic programming agent designer provide predicates evaluate quality situation advance provide model state dynamics 
learn mentor observer able observe certain aspects behavior 
communicates policy selected actions observer actual actions taken mentor unknown observer 
see effects action 
instance ro max noisy grid world mentor may attempt move north move east observer sees actual state transition intended transition actual action attempted 
assume observer see specific state mentor 
mentor state fully observable observer 
strong assumption indicate ways relaxed section 
model extraction implicit imitation observer problem determine optimal course action maximizes expected value value function bellman equation ao xt tjs assume discounted infinite horizon context discount factor assume agent actions influence agent reward structure assumption relaxed 
rl context observer access transition samples ti recall 
optimal policy determined directly learning model methods estimate solve generally asynchronously incrementally equation prioritized sweeping :10.1.1.134.8196
limited scope observations mentor state trajectories influence computation 
observer knows reward function observations mentor influence reward model 
observations update estimate transition order apply bellman backups estimated value function 
consider forms influence section 
possible direct influences explored observer attempt infer best action perform mentor trajectory observer observations directly compute value function constraints value function states visited 
section describe model extraction algorithm context model rl 
section indicate ideas incorporated model free algorithms learning 
model extraction general terms mentor expert achieving objectives assume executing stationary deterministic denoting action choice states 
behavior induces markov tjs prm tjs observing mentor transitions observer construct ro max max chain tjs simply estimated relative observed frequency mentor xt tjs ao xt tjs transitions taken 
observer knows actiona asm exists tjs tjs 
unfortunately observer know transition model estimates observe mentor actions directly know identity 
mere fact exists action estimated transition probabilities prm js effect observer value function estimation 
augmented bellman equation usual bellman equation extra term added second summation denoting expected value duplicating mentor 
unknown action identical observer actions term redundant augmented value equation valid 
course observer augmented backup operation rely estimates quantities 
observer exploration policy ensures state visited estimates converge true values 
mentor policy observer control 
mentor policy ergodic state spaces terms converge true values 
mentor policy restricts subset forming basis markov chain estimates converge correctly respect tos chain ergodic states ins remain unvisited 
observer apply augmented equation states visited mentor 
cases equation impact usual convergence results rl algorithms 
primary interest behavior system initial stages learning advantage knowledgeable mentor difference observer 
assuming mentor pursuing greedy policy states observer accurate estimates tjs pro tjs specifica 
observer learning explore state space causing frequent visits tos action space spreading experience generally ensuring sample size greater specific action 
apart accurate tjs give informed value estimates states prior action models generally flat uniform distinguishable state observer sufficient experience states 
mentor markov chain ergodic mixing rate sufficiently low mentor may visit certain infrequently 
state rarely visited mentor may provide misleading estimate small sample prior mentor chain value mentor unknown action ats mentor policy control observer misleading value may persist extended period 
stems part fact maximization combine values estimates mentor chain observer action models part mean values estimated probabilities observed samples 
augmented bellman equation consider reliability information sources 
overcome incorporated estimate model confidence augmented backups 
mentor markov chain observer action transitions assume dirichlet prior parameters multinomial distributions 
reflect observer initial uncertainty possible tran sition probabilities 
sample counts mentor observer transitions update distributions 
information attempt perform optimal bayesian estimation value function sample counts small normal approximations appropriate simple closed form expression resultant values 
employ approximate method combining information sources inspired kaelbling interval estimation method 
compute observer optimal simple max mean value state possible action usual method 
benefit dirichlet distributions construct lower value state observer model derived behavior lower bound suitable confidence interval expected value state 
second lower mis constructed model observations mentor 
ifv mentor inspired model fact lower expected value specified degree confidence uses nonoptimal action observer perspective mentor inspired model lower confidence 
case reject information provided mentor model derived observer observations 
inequality choose confidence level dirichlet distributions small sample counts highly non normal 
note reasoning holds mentor implementing stationary stochastic policy expected value stochastic policy fully observable mdp greater optimal deterministic policy 
direction offered mentor implementing deterministic policy tends focussed empirically mentors offer broader guidance moderately stochastic environments implement stochastic policies tend visit state space 
note extension multiple mentors straightforward mentor model incorporated augmented bellman equation difficulty 
model assumed action costs rewards depend state general reward functions reward form 
difficulty lies backing action costs mentor chosen action unknown 
simple heuristic method find action observer closest mentor current transition probability estimates construct estimate value state information derived mentor transitions action 
denote observer action closest action executed mentor states 
defined action observed transition distribution minimum kullback leibler distance mentor observed tran max max ro xt tjs ao ro xt tjs sition distribution tjs tjs augmented bellman equation rewritten function follows note bayesian methods estimate action costs mentor chain 
model extraction algorithm requires observer maintain tjs markov chain induced mentor policy estimate updated observed transition backups performed estimate value function augmented backup equation confidence testing second way observer exploit observations mentor focus attention states visited mentor 
model approach specific focusing mechanism adopt requires observer perform possibly augmented bellman backup states mentor transitions ofs 
effects 
mentor tends visit interesting regions space shares certain positive negative reward structure observer states observer attention drawn 
importantly focus attention directs computational effort parts state space estimated tjs changes estimated value observer actions may change 
addition computation focused model accurate discussed 
model extraction specific rl algorithms model extraction focusing mechanism described integrated model rl algorithms relative ease 
experiments described section prioritized sweeping :10.1.1.134.8196
roughly prioritized sweeping works maintaining estimated transition reward 
experience sampled estimated model change bellman backup done incorporate revised model usually fixed number additional backups performed selected states 
course backups implemented estimated tjs tjs 
states selected priority estimates potential change values changes earlier backups see details :10.1.1.134.8196:10.1.1.134.8196
essentially computational resources backups focused states benefit backups 
incorporating ideas simply requires changes observer takes estimated tjs updated augmented backup equation performed states 
augmented backups performed fixed number states usual priority queue implementation 
observed mentor ti estimated tjs updated augmented backup performed ats 
augmented backups performed fixed number states usual priority queue implementation 
fact augmented backups standard bellman backups implements model extraction requirement backups performed observed transitions addition experienced transitions incorporates focusing mechanism 
view rl algorithm embodying form implicit imitation 
observer forced follow mimic actions mentor directly 
back value information mentor trajectory 
ultimately observer move states discover actions important value information propagated guide exploration discuss exploration detail 
note usual convergence results hold prioritized sweeping algorithms certainty equivalence model extraction 
models converge augmentation bellman equation irrelevant 
initial phases training induce different exploratory computational behavior observer instances faster convergence 
particular see implicit imitation causes learner obtain higher accumulated reward initial stages learning 
model extraction fits naturally model rl methods ideas readily incorporated model free algorithms learning 
example observer augment function fictitious action mentor update values mentor transitions imposing 
updating values actions value estimated maximum values actions 
action selection exploration key part rl algorithm adopt greedy action selection decaying exploration rate time 
remarks apply equally pseudo greedy methods boltzmann exploration 
selecting action states greedy action selection requires choose action highest expected value tjs 
mentor policy dictates especially appealing action states may tjs greater value known action case observer better trying duplicate action mentor 
unfortunately action unknown 
define greedy action observer action estimated tjs minimum kullback leibler distance tjs 
proves especially effective early training value estimates actions samples vary widely confidence mentor model higher empirical validation empirical tests report incorporation model extraction focusing mechanism prioritized sweeping 
results illustrate types problems scenarios implicit imitation form provide advantages rl agent 
experiments expert mentor introduced experiment serve model observer agent 
case mentor greedy policy small order 
tends cause mentor trajectories lie cluster surrounding optimal trajectories reflect optimal policies 
environment stochasticity mentors generally cover entire state space confidence testing important 
experiments prioritized sweeping fixed number backups observed experienced sample greedy exploration decaying 
observer agents uniform dirichlet priors 
observer agents compared control agents benefit mentor experience identical implementing prioritized sweeping similar parameters exploration policies 
tests performed stochastic grid world domains clear easy construct cases mentor lower expected value expected value greater ofb 
case sense better greedy action 
recognize ultimately confidence factors incorporated judgments mentor models relatively small sample sizes priority 
generally number backups set roughly equal length optimal noise free path 
obs fa series ctrl scale basic stoch delta basic observer control agent comparisons influence domain size noise extent observer mentor optimal policies overlap fail 
assume connectivity cells state grid neighbors including agent north south east west actions 
focus primarily imitation improves performance learning learner converge optimal policy uses imitation 
show typical profile observer model extraction expert mentor vs control agent 
agents attempt learn policy maximizes discounted return grid world 
start upper left corner seek goal value lower corner 
reaching goal agents restarted top 
actions noisy resulting unintended transition time 
discount factor 
plot cumulative number goals obtained previous time steps observer obs control ctrl results averaged runs 
observer able quickly incorporate policy learned mentor value estimates 
results steeper learning curve 
contrast control agent slowly explores space build model 
delta curve shows difference performance agents 
agents converge optimal value function 
experiment illustrates sensitivity imitation size state space action noise level 
plot delta curves basic scenario just described scale scenario state space size increased stoch scenario noise level increased results averaged runs 
difference observer non imitating prioritized sweeping agent increases state space size 
reflects whitehead observation grid worlds increase quickly state space size optimal path length increases linearly 
see guidance mentor help larger state spaces 
increasing noise reduces observer ability act information received mentor advantage control agent 
note benefit imitation degrades gracefully increased noise relatively extreme noise level 
observer prior beliefs transition probabilities mentor mislead observer cause generate inappropriate values 
confidence mechanism proposed previous section prevent observer fooled misleading priors mentor transition probabilities 
experiment scenario illustrated 
agent task navigate top left corner corner grid order attain reward 
created pathological scenario islands high value enclosed obstacles 
observer priors reflect connectivity uniform high valued cell middle island believed reachable states diagonally adjacent small prior probability 
reality agent action set precludes agent able realize value 
islands scenario create fairly large region center space high value potentially trap observer 
test confidence mechanism mentor follows path outside obstacles path lead observer trap 
combination high initial exploration rate ability prioritized sweeping spread value large distances virtually guarantees observer lead trap 
scenario ran observer agents control 
observer confidence interval width rule cover approximately percent arbitrary distribution 
second observer environment misleading priors complex maze cr series ctrl obs cmb series obs delta misleading priors may degrade performance interval effectively disables confidence testing 
observer confidence testing consistently stuck 
examination value function revealed consistent peaks trap region observation agent showed stuck trap 
observer confidence testing consistently escapes trap 
observation value function time shows trap forms fades away observer gains experience throw erroneous priors 
performance observer confidence testing shown performance control agent results averaged runs 
see observer performance slightly degraded control agent pathological case 
experiment demonstrates potential gains imitation increase qualitative difficulty problem 
maze scenario introduce obstacles order increase difficulty learning problem 
maze set grid obstacles complicating agent journey top left bottom right corner 
solution takes form step path distracting paths length delta ctrl imitation complex space branching solution path necessitating frequent backtracking 
discount factor 
noise optimal goal attainment rate goals steps 
graph results averaged runs see control agent takes order steps build decent value function reliably leads goal 
point achieving goals steps average exploration rate reasonably high unfortunately decreasing exploration quickly lead faster value function formation 
imitation agent able take advantage mentor expertise build reliable value function steps 
control agent unable reach goal steps delta control imitator simply equal imitator performance 
imitator quickly achieve optimal goals steps exploration rate decays quickly 
augmented backup rule require reward csb series delta obs ctrl maze shortcut transfer non identical rewards structure mentor observer identical 
useful scenarios rewards dissimilar induce value functions policies share structure 
experiment demonstrate interesting scenario comparatively easy find suboptimal solution difficult find optimal solution 
observer finds suboptimal path observer agent able exploit observations mentor see shortcut significantly shortens path goal 
structure scenario seen 
suboptimal solution lies path scenic route goal 
mentor takes vertical path shortcut 
discourage shortcut novice agents lined cells marked jump back start state 
difficult novice agent executing random exploratory moves way shortcut obtain value reinforce 
observer control generally find scenic route 
performance goals steps control observer compared averaged runs indicating value observations 
observations show observer control agent take time average find long scenic route goal observer instantly recognizes shortcut jumps double goal rate 
experiment shows mentors improve observer policies observer goals mentor path 
final experiment illustrates model extraction readily extended observer extract models multiple mentors exploit valuable parts 
learner move start location goal location 
expert agents different start goal states serve potential mentors 
mentor repeatedly moves location location dotted line second mentor departs location ends location dashed line 
experi multiple mentors scenario ment observer combine information examples provided mentors independent exploration order solve problem 
see observer successfully pulls information sources order learn quickly control agent results averaged runs 
explored empirically negative transfer accommodated framework 
naturally want observer learn negative rewards 
mentor unsuccessful ends particularly bad state observer point view may profitable study 
concluding remarks related imitation common strategy information transfer natural agents octopus primate forms imitation occurring cmm series obs learning 
knowledge transfer occurs direct communication 
imitation teaching type transfer enhance imitative capabilities 
ctrl delta learning seen form learning observation 
goal observe human mentor extract convenient rules behavior recommend mentor 
differs observer attempting solve decision problem simply trying improve abilities mentor 
done knowledge agent decisions place partial order constraints value function states 
learning multiple mentors species 
human imitation studied contexts varied motor development language acquisition 
researchers considered imitation important means enhancing learning multiagent systems interesting ideas pursued literature imitation teaching :10.1.1.45.8898:10.1.1.40.3185:10.1.1.27.292
relies observer sharing common similar possibly perceptual state mentor 
physical systems mobile robots achieved forcing observer follow mentor arranging communication occur agents close proximity :10.1.1.40.3185
viewed ensuring full observability mentor state resolving state analogy problem 
approach useful partial observability issue plagued synchronization alignment problems 
principled approaches partial observability incorporate pomdp models see 
action analogy problem addressed formally algebraic formulation implications different abilities part mentor observer provided :10.1.1.30.5471:10.1.1.30.5471
discuss clear specific formulation rl :10.1.1.30.5471
imitation requires mentor directly communicate intended action reward provide feedback observer complete state reward action information directly accessible :10.1.1.75.7884:10.1.1.40.3185
research explicit teaching demonstration behaviors includes focused domains specific robotics tasks behaviors learned replicating observed human behaviors 
relies cooperative relationship mentor observer contrast model assumption 
middle ground mentor decide facilitate learning agent prove interesting research topic 
tan explores multiagent rl agents share reward information pool perceptions focus ac possible extensions model model widely applied number restrictive assumptions relaxed :10.1.1.55.8066
currently exploring 
agents different reward structures assumed common state action spaces 
requiring observer abilities mentor crucial simple augmented bellman equation unrealistic 
currently extending algorithms deal agents differing abilities methods allow observer reason approximate mentor trajectory specific action methods reasoning similarity observer actions observed mentor transition distribution state 
preliminary model free settings shown remarkable transfer mentor observer different actions disposal 
analogical mappings state spaces relax assumption agents common state space 
see mappings :10.1.1.30.5471
allow partially observable models especially respect observer obtaining information true state trajectories mentor 
model easily extended allow partial observability currently tackling extensions model extraction algorithm framework 
hope provide principled approach devising strategies facilitate learning framework contrast heuristics explicit 
challenging extension application model extraction models allow interactions observer mentor 
self interested game theoretic model required account strategic reasoning 
observer may need account possibility deception part mentor 
cooperative settings issues coordination resolved 
hand new opportunities facilitation emerge mentor may alter environment observer learn readily learn environment 
current modeling assumptions extensions algorithm explored 
currently extend ing explicit confidence measures order address reasoning mentors different abilities observer 
sophisticated exploration techniques exploit confidence factors :10.1.1.54.1256
hope explore generalization value function approximation methods framework 
summary described formal principled approach imitation called model extraction 
stochastic problems explicit forms communication possible underlying model framework combined model extraction provides alternative imitation learning observation systems 
shown model extraction offer significant transfer capability test problems proves robust face noise capable integrating multiple mentors provide benefits increase difficulty problem 
initial assumptions somewhat restrictive observer mentor similar state action spaces instance relax 
model extraction simple addition model reinforcement learning expect able generalize technique domains model techniques applied 
compact form principled framework expect model extraction sound basis build variety algorithms implicit forms imitation 
anonymous referees suggestions 
bob mccalla early support research imitation 
research supported nserc research ogp iris phase iii project bac 
atkeson schaal 
robot learning demonstration 
icml 
paul bakker kuniyoshi :10.1.1.27.292
robot see robot overview robot imitation 
aisb workshop learning robots animals pages 
billard kerstin dautenhahn 
grounding communication autonomous robots experimental study 
robotics autonomous systems special issue scientific methods mobile robotics recce nehmzow eds 
billard hayes 
learning communicate imitation autonomous robots 
icann pages 
kerstin dautenhahn 
getting know artificial social intelligence autonomous robots 
robotics autonomous systems 
dearden friedman russell 
bayesian learning 
aaai pages 
aaai press 
degroot 
probability statistics 
addison wesley new york 

observational learning octopus 
science 
leslie pack kaelbling 
learning 
mit press cambridge 
kuniyoshi inaba inoue 
learning watching extracting reusable task knowledge visual human performance 
ieee transactions robotics automation 
long ji lin :10.1.1.75.7884
self improving reactive agents reinforcement learning planning teaching 
machine learning 
michael littman :10.1.1.40.3185:10.1.1.135.717
markov games framework multi agent reinforcement learning 
proceedings eleventh international conference machine learning pages new brunswick nj 
maja mataric :10.1.1.40.3185
communication reduce locality distributed multi agent learning 
journal experimental theoretical artificial intel 
nicolas meuleau paul bourgine 
exploration environments local measures back propagation uncertainty 
machine learning 
appear 
mitchell mahadevan steinberg 
leap learning apprentice vlsi design 
ijcai pages 
miyamoto schaal koike osu nakano wada kawato :10.1.1.134.8196
learning robot dynamic optimization theory 
ieee int 
workshop robot human comm 
andrew moore christopher atkeson :10.1.1.134.8196
prioritized sweeping reinforcement learning data real time 
machine learning 
hiroshi motoda yoshida :10.1.1.30.5471
machine learning techniques computers easier 
artificial intelligence 
nehaniv kerstin dautenhahn :10.1.1.30.5471
mapping dissimilar bodies affordances algebraic foundations imitation 
proc 
european workshop learning robots edinburgh 
shapley 
stochastic games 
proc 
national academy sciences 
keith nelson 
fresh look imitation language learning pages 
springer verlag london uk 
ming tan :10.1.1.55.8066
multi agent reinforcement learning independent vs cooperative agents 
icml pages 
paul utgoff jeffrey clouse 
kinds training information evaluation function learning 
proceedingsof ninth national conference artificial intelligence pages 
watkins dayan 
learning 
machine learning 
steven whitehead 
complexity analysis cooperative mechanisms reinforcement learning 
aaai pages 
