data driven theory refinement kbdistal yang parekh vasant honavar laboratories rd ca usa yang wins com research planning ctr rd menlo park ca usa com computer science dept iowa state university ames ia usa honavar cs iastate edu genetics dept iowa state university ames ia usa iastate edu 
knowledge artificial neural networks offer attractive approach extending modifying incomplete knowledge bases domain theories process data driven theory refinement 
efficient algorithm data driven knowledge discovery theory refinement distal novel inter pattern distance polynomial time constructive neural network learning algorithm 
initial domain theory comprising propositional rules translated knowledge network 
domain theory modified distal adds new neurons existing network needed reduce classification errors associated incomplete domain theory labeled training examples 
proposed algorithm capable handling patterns represented binary nominal numeric real valued attributes 
inductive learning systems attempt learn concept description sequence labeled examples 
artificial neural networks massive parallelism potential fault noise tolerance offer attractive approach inductive learning 
systems successfully data driven knowledge acquisition application domains 
systems generalize labeled examples 
availability domain specific knowledge domain theories concept learned potentially enhance performance inductive learning system :10.1.1.101.4125
hybrid learning systems effectively combine domain knowledge inductive learning potentially learn faster generalize better purely inductive learning learning labeled examples 
practice domain theory incomplete inaccurate 
inductive learning systems information training examples modify existing domain theory augmenting new knowledge refining existing knowledge called theory refinement systems 
theory refinement systems broadly classified categories 
examples systems include ptr 
approaches inductive logic programming represent knowledge order logic restricted subsets 
examples systems include focl 
connectionist approaches artificial neural networks typically operate embedding domain knowledge appropriate initial neural network topology refine training resulting neural network set labeled examples 
kbann system related approaches offer examples approach :10.1.1.51.3526:10.1.1.54.4913
experiments involving datasets human genome project kbann reported outperformed symbolic theory refinement systems learning algorithms backpropagation id 
kbann limited fact modify network topology theory refinement conducted solely updating connection weights 
prevents incorporation new rules restricts algorithm ability compensate inaccuracies domain theory 
background constructive neural network learning algorithms ability modify network architecture dynamically adding neurons controlled fashion offer attractive connectionist approach data driven theory refinement :10.1.1.53.2895:10.1.1.16.9105
kbann system related approaches offer examples approach :10.1.1.51.3526:10.1.1.54.4913
experiments involving datasets human genome project kbann reported outperformed symbolic theory refinement systems learning algorithms backpropagation id 
kbann limited fact modify network topology theory refinement conducted solely updating connection weights 
prevents incorporation new rules restricts algorithm ability compensate inaccuracies domain theory 
background constructive neural network learning algorithms ability modify network architecture dynamically adding neurons controlled fashion offer attractive connectionist approach data driven theory refinement :10.1.1.53.2895:10.1.1.16.9105
available domain knowledge incorporated initial network topology rules network algorithm means :10.1.1.54.4913
inaccuracies domain theory compensated extending network topology training examples 
depicts process 
constructive neural network learning algorithms circumvent need priori specification network architecture construct networks size complexity commensurate complexity data trade network complexity training time generalization accuracy :10.1.1.53.2895:10.1.1.16.9105
experiments involving datasets human genome project kbann reported outperformed symbolic theory refinement systems learning algorithms backpropagation id 
kbann limited fact modify network topology theory refinement conducted solely updating connection weights 
prevents incorporation new rules restricts algorithm ability compensate inaccuracies domain theory 
background constructive neural network learning algorithms ability modify network architecture dynamically adding neurons controlled fashion offer attractive connectionist approach data driven theory refinement :10.1.1.53.2895:10.1.1.16.9105
available domain knowledge incorporated initial network topology rules network algorithm means :10.1.1.54.4913
inaccuracies domain theory compensated extending network topology training examples 
depicts process 
constructive neural network learning algorithms circumvent need priori specification network architecture construct networks size complexity commensurate complexity data trade network complexity training time generalization accuracy :10.1.1.53.2895:10.1.1.16.9105
variety constructive learning algorithms studied literature 
background constructive neural network learning algorithms ability modify network architecture dynamically adding neurons controlled fashion offer attractive connectionist approach data driven theory refinement :10.1.1.53.2895:10.1.1.16.9105
available domain knowledge incorporated initial network topology rules network algorithm means :10.1.1.54.4913
inaccuracies domain theory compensated extending network topology training examples 
depicts process 
constructive neural network learning algorithms circumvent need priori specification network architecture construct networks size complexity commensurate complexity data trade network complexity training time generalization accuracy :10.1.1.53.2895:10.1.1.16.9105
variety constructive learning algorithms studied literature 
distal polynomial time learning algorithm guaranteed induce network zero classification error non contradictory training set 
handle pattern classification tasks datasets available ftp ftp cs wisc edu machine learning datasets 
domain theory constructive neural network input units fig 
handle pattern classification tasks datasets available ftp ftp cs wisc edu machine learning datasets 
domain theory constructive neural network input units fig 

theory refinement constructive neural network patterns represented binary nominal numeric attributes 
experiments wide range datasets indicate classification accuracies attained distal competitive algorithms :10.1.1.53.2895
distal uses inter pattern distance calculations easily extended pattern classification problems patterns variable sizes strings complex symbolic structures long suitable distance measures defined 
distal attractive candidate data driven refinement domain knowledge 
available domain knowledge incorporated initial network topology 
inaccuracies domain theory corrected distal adds additional neurons eliminate classification errors training examples 
available domain knowledge incorporated initial network topology 
inaccuracies domain theory corrected distal adds additional neurons eliminate classification errors training examples 
background kbdistal data driven constructive theory refinement algorithm distal 
constructive theory refinement knowledge neural networks section briefly describes constructive theory refinement systems studied literature 
fletcher designed constructive learning method dynamically adding neurons initial knowledge network :10.1.1.51.3055
approach starts initial network representing domain theory modifies theory constructing single hidden layer threshold logic units labeled training data algorithm 
algorithm divides feature space hyperplanes 
fletcher algorithm maps hyperplanes set trains output neuron pocket algorithm 
kbdistal algorithm proposed fletcher constructs single hidden layer 
differs important aspect uses computationally efficient distal algorithm constructs entire network pass training set relying iterative approach fletcher requires large number passes training set 
rapture system designed refine domain theories contains probabilistic rules represented certainty factor format 
rapture approach modifying network topology differs kbdistal follows rapture uses iterative algorithm train weights employs information gain heuristic add links network 
kbdistal simpler uses non iterative constructive learning algorithm augment initial domain theory 
opitz shavlik extensively studied connectionist theory refinement systems overcome fixed topology limitation kbann algorithm :10.1.1.48.2048:10.1.1.121.3039
topgen algorithm uses heuristic search space possible expansions kbann network constructed initial domain theory 
topgen maintains queue candidate networks ordered test accuracy cross validation set 
step topgen picks best network explores possible ways expanding 
new networks generated strategically adding nodes different locations best network selected 
topgen maintains queue candidate networks ordered test accuracy cross validation set 
step topgen picks best network explores possible ways expanding 
new networks generated strategically adding nodes different locations best network selected 
networks trained inserted queue process repeated 
regent algorithm uses genetic search explore space network architectures :10.1.1.121.3039
creates diverse initial population networks kbann network constructed domain theory 
genetic search uses classification accuracy cross validation set fitness measure 
regent mutation operator adds node network topgen algorithm 
uses specially designed crossover operator maintains network rule structure 
population networks subjected fitness proportionate selection mutation crossover generations best network produced entire run reported solution 
kbdistal considerably simpler topgen regent 
constructs single network pass training data opposed training evaluating population networks computationally expensive backpropagation algorithm generations 
significantly faster topgen regent 
parekh honavar propose constructive approach theory refinement uses novel combination tiling pyramid constructive learning algorithms :10.1.1.51.4461
symbolic knowledge encoding procedure translate domain theory set propositional rules procedure rules networks algorithm towell shavlik kbann topgen regent :10.1.1.54.4913
yields set rules antecedent 
rule set mapped graph turn directly translated neural network 
tiling pyramid algorithm uses iterative perceptron style weight update algorithm setting weights tiling algorithm construct hidden layer maps binary numeric input patterns binary representation hidden layer pyramid algorithm add additional neurons needed 
kbdistal considerably simpler topgen regent 
constructs single network pass training data opposed training evaluating population networks computationally expensive backpropagation algorithm generations 
significantly faster topgen regent 
parekh honavar propose constructive approach theory refinement uses novel combination tiling pyramid constructive learning algorithms :10.1.1.51.4461
symbolic knowledge encoding procedure translate domain theory set propositional rules procedure rules networks algorithm towell shavlik kbann topgen regent :10.1.1.54.4913
yields set rules antecedent 
rule set mapped graph turn directly translated neural network 
tiling pyramid algorithm uses iterative perceptron style weight update algorithm setting weights tiling algorithm construct hidden layer maps binary numeric input patterns binary representation hidden layer pyramid algorithm add additional neurons needed 
tiling pyramid significantly faster topgen regent slower kbdistal reliance iterative weight update procedures 
rule set mapped graph turn directly translated neural network 
tiling pyramid algorithm uses iterative perceptron style weight update algorithm setting weights tiling algorithm construct hidden layer maps binary numeric input patterns binary representation hidden layer pyramid algorithm add additional neurons needed 
tiling pyramid significantly faster topgen regent slower kbdistal reliance iterative weight update procedures 
kbdistal data driven theory refinement algorithm section briefly describes approach knowledge theory refinement distal 
distal inter pattern distance constructive neural network algorithm distal simple relatively fast constructive neural network learning algorithm pattern classification :10.1.1.53.2895
key idea distal add hidden neurons time greedy strategy ensures hidden neuron added correctly classifies maximal subset training patterns belonging single class 
correctly classified examples eliminated consideration 
process repeated network correctly classifies entire training set 
happens training set linearly separable transformed space defined hidden neurons 
incorporation prior knowledge distal current implementation kbdistal simple approach incorporation prior knowledge distal 
input patterns classified rules 
resulting outputs classification input pattern augmented pattern connected constructive neural network 
explains distal constructive neural network efficiently requiring conversion rules neural network 
experiments section reports results experiments kbdistal data driven theory refinement financial advising problem fletcher binding site promoter site prediction shavlik group data human genome project :10.1.1.101.4125:10.1.1.48.2048:10.1.1.51.3055
comprises domain theory set labeled examples 
input short segment dna nucleotides goal learn predict dna segments contain binding site 
rules domain theory examples dataset 
promoters data human genome project consists domain theory set labeled examples 
promoters data human genome project consists domain theory set labeled examples 
input short segment dna nucleotides goal learn predict dna segments contain promoter site 
rules domain theory examples dataset 
financial advisor financial advisor rule base contains rules shown 
set labeled examples consistent rule base randomly generated :10.1.1.51.3055
examples training remaining testing 
sav adeq adeq invest stocks dep sav adeq sav adeq assets hi sav adeq dep adeq earn steady adeq debt lo adeq sav dep dep sav adeq assets income assets hi income dep dep adeq debt income debt lo fig 

financial advisor rule base 
case promoters dataset kbdistal produced comparable generalization accuracy smaller network size 
network pruning boosted generalization accuracy sigma significantly smaller network size sigma 
time taken approach significantly approaches 
kbdistal takes fraction minute minutes cpu time dataset experiments 
contrast topgen regent reported taken days obtain results reported :10.1.1.121.3039
financial advisor rule base explained earlier patterns generated randomly satisfy rules patterns training remaining patterns testing network 
order experiment different incomplete domain theories rules pruned antecedents experiment 
instance sav adeq selected pruning point rules sav adeq dep sav adeq assets hi eliminated rule base 
words rules pruned 
instance sav adeq selected pruning point rules sav adeq dep sav adeq assets hi eliminated rule base 
words rules pruned 
rule modified read adeq invest stocks 
initial network constructed modified rule base augmented constructive learning 
experiments follow performed :10.1.1.51.3055:10.1.1.51.4461
see table kbdistal outperformed approaches gave results 
resulted higher classification accuracy approaches cases produced fairly compact networks substantially lower amount computational resources 
human genome project datasets network pruning boosted generalization cases smaller network size 
pruning points table sequence dep sav adeq adeq generalization accuracy improved network sizes respectively 
results financial advisor rule base kbdistal tiling pyramid 
pruning point kbdistal tiling pyramid rules test size test size test dep sav adeq sigma sigma assets hi sigma sigma dep adeq sigma sigma debt lo sigma sigma sav adeq sigma sigma adeq sigma sigma summary discussion theory refinement techniques offer attractive approach exploiting available domain knowledge enhance performance data driven knowledge acquisition systems 
neural networks extensively theory refinement systems proposed literature 
systems translate domain theory initial neural network architecture train network refine theory 
kbann algorithm demonstrated outperform learning algorithms domains :10.1.1.51.3526:10.1.1.54.4913
significant disadvantage kbann fixed network topology 
topgen regent algorithms hand allow modifications network architecture 
experimental results demonstrated topgen regent outperform kbann applications 
:10.1.1.48.2048:10.1.1.121.3039
kbann algorithm demonstrated outperform learning algorithms domains :10.1.1.51.3526:10.1.1.54.4913
significant disadvantage kbann fixed network topology 
topgen regent algorithms hand allow modifications network architecture 
experimental results demonstrated topgen regent outperform kbann applications 
:10.1.1.48.2048:10.1.1.121.3039
tiling pyramid algorithm proposed constructive theory refinement builds network perceptrons :10.1.1.51.4461
performance terms classification accuracies attained reported comparable regent topgen significantly lower computational cost :10.1.1.51.4461
implementation kbdistal experiments reported uses rules directly augmenting input patterns outputs obtained rules opposed common approach incorporating rules initial network topology 
distal network construction kbdistal significantly faster approaches rely iterative weight update procedures perceptron learning backpropagation algorithm computationally expensive genetic search 
significant disadvantage kbann fixed network topology 
topgen regent algorithms hand allow modifications network architecture 
experimental results demonstrated topgen regent outperform kbann applications 
:10.1.1.48.2048:10.1.1.121.3039
tiling pyramid algorithm proposed constructive theory refinement builds network perceptrons :10.1.1.51.4461
performance terms classification accuracies attained reported comparable regent topgen significantly lower computational cost :10.1.1.51.4461
implementation kbdistal experiments reported uses rules directly augmenting input patterns outputs obtained rules opposed common approach incorporating rules initial network topology 
distal network construction kbdistal significantly faster approaches rely iterative weight update procedures perceptron learning backpropagation algorithm computationally expensive genetic search 
experimental results demonstrate kbdistal performance terms generalization accuracy competitive computationally expensive algorithms data driven theory refinement 
topgen regent algorithms hand allow modifications network architecture 
experimental results demonstrated topgen regent outperform kbann applications 
:10.1.1.48.2048:10.1.1.121.3039
tiling pyramid algorithm proposed constructive theory refinement builds network perceptrons :10.1.1.51.4461
performance terms classification accuracies attained reported comparable regent topgen significantly lower computational cost :10.1.1.51.4461
implementation kbdistal experiments reported uses rules directly augmenting input patterns outputs obtained rules opposed common approach incorporating rules initial network topology 
distal network construction kbdistal significantly faster approaches rely iterative weight update procedures perceptron learning backpropagation algorithm computationally expensive genetic search 
experimental results demonstrate kbdistal performance terms generalization accuracy competitive computationally expensive algorithms data driven theory refinement 
additional experiments real world data domain knowledge needed explore capabilities limitations kbdistal related algorithms theory refinement 
extensions variants kbdistal worth exploring 
fact distal relies inter pattern distances induce classifiers data straightforward extend handle broader class problems including involve patterns variable sizes strings symbolic structures long suitable inter pattern distance metrics defined 
steps rigorous definitions distance metrics information theory outlined 
variants distal kbdistal utilize distance metrics currently investigation 
authors investigated approaches rule extraction neural networks general connectionist theory refinement systems particular :10.1.1.75.2556
goal represent learned knowledge form comprehensible humans 
context rule extraction classifiers induced kbdistal interest 
practical applications interest data needed synthesizing reasonably precise classifiers available 
calls incremental algorithms continually refine knowledge data available 
