optimal feature selection daphne koller mehran sahami gates building computer science department stanford university stanford ca cs stanford edu examine method feature subset selection information theory 
initially framework de ning theoretically optimal computationally intractable method feature subset selection 
show goal eliminate feature gives little additional information subsumed remaining features 
particular case irrelevant redundant features 
give cient algorithm feature selection computes approximation optimal feature selection criterion 
conditions approximate algorithm successful examined 
empirical results number data sets showing algorithm ectively handles datasets large number features 
keywords feature selection entropy 
classic supervised learning task training set labeled xed length feature vectors instances induce classi cation model 
model turn predict class label set previously unseen instances 
building classi cation model information class inherent features utmost importance 
theoretical sense having features give discriminating power real world provides reasons generally case 
foremost induction methods su er curse dimensionality 
number features induction task increases time requirements algorithm grow dramatically exponentially 
set features data su ciently large induction algorithms simply intractable 
problem exacerbated fact features learning task may irrelevant redundant features respect predicting class instance 
context features serve purpose increase induction time 
furthermore learning algorithms viewed performing biased form estimation probability class label set features 
domains large number features distribution complex high dimension 
unfortunately real world faced problem limited data induce model 
di cult obtain estimates probabilistic parameters 
order avoid tting model particular distribution seen training data algorithms employ occam razor blumer ehrenfeucht haussler warmuth bias build simple model possible achieves acceptable level performance training data 
occam razor bias leads prefer small number relatively predictive avery large number features taken proper complex combination entirely predictive class label 
irrelevant redundant features cause problems context may confuse learning algorithm helping obscure distributions small set truly relevant features task hand 
light considerations number researchers addressed issue feature subset selection machine learning 
de ned john kohavi eger divided lines lter wrapper models 
lter model feature selection performed preprocessing step induction 
bias learning algorithm interact bias inherent feature selection algorithm 
known lter methods feature selection relief kira rendell focus almuallim dietterich 
relief subset features directly selected feature relevance weighting indicating level relevance class label 
important note method ine ective removing redundant features predictive highly correlated features high relevance weightings 
focus algorithm conducts exhaustive search feature subsets determine minimal set features provide consistent labeling training data 
consistency criterion focus sensitive noise inconsistencies training data 
exponential growth size power set features algorithm impractical domains features 
feature selection received attention wrapper model john caruana freitag langley sage 
model employs search space feature subsets estimated accuracy induction algorithm measure goodness particular feature subset 
feature selection wrapped induction algorithm bias operators de ne search induction algorithm strongly interact 
methods encountered success induction tasks prohibitively expensive run break large numbers features 
furthermore methods leave desired terms theoretical justi cation 
important aspect feature selection method helps induction algorithm terms accuracy measures important understand induction problem general ected feature selection 
address theoretical empirical aspects feature selection 
describe formal framework understanding feature selection ideas information theory cover thomas 
cient implemented algorithm theoretical intuitions 
algorithm overcomes problems existing methods sound theoretical foundation ective eliminating irrelevant redundant features tolerant inconsistencies training data importantly lter algorithm incur high computational cost conducting search space feature subsets wrapper methods cient domains containing hundreds thousands features 

theoretical framework data instance typically described system assignment fn set features fn 
usual assume data instances including training set drawn probability distribution space feature vectors 
formally assignment wehave probability pr 
classi er procedure takes input data instance classi es belonging number possible classes 
classi er decision assignment associated instance 
optimistically feature vector fully determine appropriate classi cation 
rarely case typically access features deterministic decision 
probability distribution model classi cation function 
precisely assignment distribution pr di erent possible classes learning algorithm implicitly uses approximate version conditional distribution pr empirical frequencies observed training set construct classi er problem 
known number features strong ect performance learning algorithm 
hand existence irrelevant redundant features degrade accuracy learning algorithm causing optimal features classi cation 
hand computational complexity learning algorithms depends heavily number features 
useful reduce feature set classi er constructed 
consider ect feature space reduction distribution characterizes problem 
subset example consist pair features consist single feature feature vector fg denote projection variables example feature vector 
consider particular data instance characterized original distribution data instance induces distribution pr 
reduced feature space instance induces possibly di erent distribution pr fg 
goal select distributions close possible 
distance metric information theoretic measure cross entropy known kl distance kullback leibler 
view selecting set features cause lose amount information distributions 
formally distributions probability space cross entropy de ned log note roles symmetrical de nition 
generally speaking idea right distribution approximation 
measures extent error substitute cross entropy particularly suitable application pr role right distribution pr fg role case probability space set possible classi cations fc cng 
de ne pr pr fg 
course order metric allows compare feature set integrate values di erent feature vectors single quantity 
naively think simply sum cross entropy di erent feature vectors consider maximum cross entropy feature vectors 
ideas take consideration fact feature vectors far occur mind making larger mistake certain rare cases 
want nd feature set fpr reasonably small 
clearly feature set minimizes quantity simply maintains exact distribution 
suggests backward elimination algorithm state eliminate feature fi way allows remain close distribution possible 
intuitively greedy algorithm eliminate feature fi cause smallest increase feature set initially set stage want eliminate feature fi ffig close possible unfortunately impractical simply implement idea described computation exponential number features domain 
furthermore really compare approximate distribution true conditional distribution pr precise distribution available 
training set provides rough approximation 
cases large number features number data instances training set corresponding particular assignment small 
number features grow ability training set approximate conditional distribution decreases exponentially 
utilize ideas probabilistic reasoning pearl circumvent problem extent 
intuitively features cause small increase give additional information obtain features wecan capture intuition formal notion conditional independence 
de nition variables fi said conditionally independent set variables assignment values variables respectively pr pr see pearl details 
gives information easy show proposition subset features fi ing 
fi conditionally independent eliminate conditionally independent feature fi increasing distance desired distribution 
intuitively removing feature conditionally independent distance grow large 
impractical test conditional independence reformulation problem points way solution 
intuitively information fi subsumed features certainly subsumed subset features 
usually features required 
de nition set features contain fi 
say markov blanket fi fi conditionally independent 
see pearl 
easy see markov blanket fi case class conditionally independent feature fi corollary subset features fi ing 
assume subset markov blanket fi 
markov blanket condition stronger conditional independence 
requires subsume information fi features 
di cult nd set discussed section markov basis feature elimination number desirable properties 
intuitively wewant remove features nd markov blanket set remaining features 
features judged unnecessary criterion remain unnecessary rest process 
assume example remove feature fi markov blanket phase remove feature fj general removal fj fi relevant add fi back able remove 
case 
theorem current set features assume previously removed feature fi markov blanket fj feature remove markov blanket fi markov blanket 
proof proof basic independence properties probability distributions described pearl 
notation denote conditional independence variables sets variables set variables mi markov blanket fi note necessarily markov blanket order remove fi rst place mj markov blanket remove fj 
straightforward show mi contain fj remains markov blanket fi removal fj consider case fj mi de ne mi show mj markov blanket fi 
denote mj 
need show fi mj 
markov blanket assumption fj decomposition property wehavethat fj mj 
weak union property obtain fj mj 
similarly derive fi mj fi mj 
facts contraction property show desired result 
markov blanket criterion removes attributes really unnecessary 
interesting fact converse true 
types attributes generally perceived unnecessary attributes completely irrelevant target concept attributes redundant attributes 
easy see markov blanket criterion captures 
attributes completely irrelevant simply unconditionally removed markov blanket consisting empty set features 
attributes correlated completely independent class variable markov blanket criterion remove stage remaining irrelevant features markov blanket trying remove 
hand value fully determined probabilistically determined set able remove markov blanket 
feature selection techniques able deal types unnecessary features 
interesting compare approach seemingly similar literature singh provan 
starting full feature set eliminating features empty set features add features 
usually metric add features information gain current feature fj maximizes expected cross entropy pr pr 
fairly easy show idea markov blanket estimate cross entropy applied case forward selection 
approaches essentially minor variants theme 
claim case formal framework provides tools compare forward selection backward elimination justi es choice backward elimination 
idea follows 
recall goal remain close pr pr pr 
pr forward vs backward selection possible correct conditional distribution pr 
removing features take small steps away distribution remain close 
contrast forward selection scheme starts prior distribution pr features 
tries take large steps away distribution 
agree goal process get close possible right distribution problem clear 
guarantee large step away initial distribution gets closer goal distribution 
example illustrated adding fj take larger step adding fi resulting distribution pr fj right distribution pr fi 
show section behavior occurs data sets 

approximate algorithm previous section showed eliminate feature fi candidate feature set nding markov blanket fi 
unfortunately full markov blanket feature approximately subsumes information content feature 
furthermore nding true approximate markov blanket hard 
section simple algorithm provides heuristic approach dealing problem 
broadly algorithm iteratively selects candidate set mi feature fi uses rough heuristic estimate close mi markov blanket fi feature fi mi closest markov blanket eliminated algorithm repeats 
intuition constructing candidate markov blanket intuition assume fi fact markov blanket mi 
wecan think fi directly uencing features mi 
features tend quite strongly correlated fi 
features hand conditionally mi 
fi uences indirectly mi 
known folk theorem probabilistic uence tends attenuate distance direct uence typically stronger indirect uence 
shown formally empirically certain special cases draper hanks singh 
heuristically choose approximation markov blanket set features strongly correlated fi 
gure close mi markov blanket fi 
unfortunately evaluating conditional independence expression de nition typically expensive 
try approximate notion observing mi really markov blanket fi pr fm fi fi pr fm assignment feature values fm fi fi respectively 
follows techniques corollary 
de ne expected cross entropy fi mi mi fi pr mi fmi fi fi pr fm fi fi pr fm mi fact markov blanket fi fi mi 
hopefully approximate markov blanket value low 
approximations result algorithm computing correlation cov fi fj factor ij stddev fi stddev fj pair features fi fj 
instantiate iterate steps prespeci ed number features eliminated feature fi mi set features fj ij largest magnitude 
compute fi mi 
choose quantity minimal de ne ffig 
algorithm simple fairly easy implement 
clearly suboptimal ways particularly due naive approximations uses 
consequences ways algorithm improved 
current algorithm eliminates prespeci ed number features constructs mi sets xed prespeci ed size easy algorithm automatically expected cross entropy estimate dropping remaining feature gets large 
fairly straightforward extend algorithm pick di erent size mi number features highly correlated fi 
important tradeo kept mind 
theoretically larger conditioning set subsume information feature forming markov blanket 
hand larger conditioning sets fragment training set small chunks corresponding di erent assignment features mi signi cantly reducing accuracy probability cross entropy estimates 
crucial doing modi cation penalty term associated adding additional features mi 
importantly wewould improve techniques choosing candidate markov mi evaluating close markov blanket assumption 
particular expected cross entropy really test markov blanket property expected cross entropy value fi conditionally independent mi 
pointed conditional independence weaker property markov blanket assumption 
fact conditional independence selection criterion lead counterintuitive behavior 
example see results increasing size conditioning set cause results degrade 
due fragmentation training set see caused fact conditional independence monotonic property 
possible certain feature conditionally independent conditioning set strongly correlated strict superset dataset classes features training set size testing set size corral led vote boolean encoding dna boolean encoding reuters reuters table datasets properties arti cial st group real world nd group 
way surprising 
known additional information cause correlations appear pearl 
illustrate context consider text classi cation problem data instances documents features presence absence word classes document topics 
word mining signi cantly correlated topic machine learning 
run algorithm wewould probably eliminate mining fairly early 
word strongly correlated word data condition presence word data strong correlation word mining topic machine learning 
putting word data conditioning set seemingly irrelevant word relevant 
converse occur get estimated relevance feature multiple times change 
behavior datasets 
believe performance algorithm signi cantly improved re ned techniques bayesian methods choose candidate candidates markov blanket precise formula evaluating close di erent candidates ful lling requirement 

results order empirically test theoretical model feature selection implemented approximate algorithm experiments arti cial real world data 
datasets include corral data arti cially constructed john 
speci cally research feature selection led vote dna datasets uci repository murphy aha datasets subset reuters document collection reuters 
datasets detailed table 
selected datasets understood terms feature relevance contain features candidates feature selection 
rst analyze arti cial domains 
corral dataset noted previous researchers john particularly di cult lter methods features domain target concept boolean function features 
fth feature entirely irrelevant sixth feature correlated target concept matches class label time 
lter approaches ward selection select correlated feature 
poses problem induction methods initially split correlated feature fragmenting data true target concept recovered subtrees 
important note due disjunctive nature target function naive bayesian classi er better correlated feature 
shortcoming simplicity induction method aw feature selection methods eliminate correlated feature 
veri ed experimentally nding forward selection conditioning selects correlated feature large step suboptimal direction 
running backward elimination conditioning hand allows overcome problem eliminate correlated feature ect class distribution function features determine target concept large subset thereof 
conditioned features set algorithm drop features example eliminated correlated irrelevant features 
conditioning features eliminate correlated variable due non monotonicity property discussed 
led domain nd situation albeit arti cial conditioning correlated features di cult determine appropriate subset features 
domain contains relevant irrelevant features 
class label led domain entirely determines value relevant feature irrelevant features random 
dependence features class label 
result expect conditioning correlated features confuse algorithm forcing unnecessarily estimate larger number probability values amount data leading poorer estimates 
conjecture veri ed experimentally method fact selected relevant features conditioned variables 
test method feature subset selection ected classi cation employed naive bayesian classi er duda hart langley iba thompson quinlan induction algorithms applied original datasets datasets ltered feature selection algorithm forward selection backward elimination 
accuracy results uci data table 
seen accuracy results corral vote selection appropriate feature set large impact classi cation accuracy 
importantly fact domains feature selection algorithm dramatic reductions feature space consequently improve classi cation performance 
especially true corral domain led domain conditioning vote domain naive bayes conditioning aggressive feature elimination 
dna domain see dramatic results accuracy improvements eliminating features method 
far computational expense lter approach shows promise scaling larger domains 
theoretical empirical results show time complexity algorithm quite low 
theoretically requires log operations computing correlation features naive bayes accuracy accuracy dataset orig final orig 
fwd 

orig 
fwd 

corral led led vote vote dna dna table accuracies naive bayes feature selection 
matrix sorting initial number features number instances 
subsequent feature selection process requires time number features eliminate small xed number conditioning features number classes 
caching schemes possible reduce second term close factor due fact eliminated feature mi remaining features 
need recompute new mi expected cross entropy small number features 
empirically low running time allows deal large domains reasonable amount time 
way comparison kohavi obtains similar accuracy results dna dataset naive bayes wrapper approach notes doing takes hours sun sparc 
experiments ine cient implementation algorithm utilize clever data structures reduce running time reduced dna dataset features minutes machine depending number conditioning variables 
time savings orders magnitude 
approach lter method need re run algorithm induction algorithm choose run reduced feature dataset 
features naive bayes accuracy accuracy dataset orig final orig 
fwd 

orig 
fwd 

reuters reuters table accuracies reuters text datasets feature selection 
ability deal ectively high dimensional datasets allows apply domain information retrieval 
fact part original motivation 
applications word corpus leads overwhelming number features 
datasets exceptional challenge feature selection algorithms 
particular feature selection wrapper method simply intractable due prohibitive cost running induction algorithm thousands times high dimensional data 
cient lter method akin method described suitable approach 
test empirically constructed high dimensional datasets reuters collection contains articles topics classes 
rst subset reuters comprised articles topics ee iron steel 
topics meaningful overlapping words 
reuters hand contains articles reserves gold gross national product similar words di erent contexts topics 
article encoded binary vector feature denoted particular word occured article 
simple pre processing step words occurred times dataset eliminated simply means removing extremely rare words unique names 
ran feature selection algorithm reuters datasets order reduce feature space features nearly original size 
results experiments shown table includes results forward selection 
reuters domain expect distinct terms topics feature interaction see feature selection methods tendency comparably conditioning information producing results 
conditioning introduced results backward elimination method clearly dominate obtained forward selection 
employing forward selection simply inadequate nding features conditioning information 
second reuters domain see employing backward elimination allows algorithm ectively conditioning information increase accuracies induction methods drastically reduced feature space 
surprised nd forward selection worked conditioning information case performance comparable backward elimination conditioning seek address question 
general observation compatible general trend uci datasets forward selection achieve comparable performance backward elimination conditioning features 
due computational cost conditioning process useful understand circumstances unconditioned forward selection ective 
allow apply computationally easier algorithm cases applies backward elimination datasets features contain complex interactions 
resource eliminating features reuters datasets took hours sun sparc 
way comparison rough estimate time required wrapper approach caruana freitag john 
eliminate features order thousands hours assuming method get caught local minima rst prematurely stops eliminating attributes result 

theoretically justi ed model optimal feature selection cross entropy minimize amount predictive information lost feature elimination 
theoretical framework prove desirable properties method feature selection 
algorithm approximates theoretical model provide extensive empirical testing 
show algorithm ective drastically reducing feature space learning tasks helping improve accuracy cases 
important note method attempts eliminate features way keeps conditional probability class features close original distribution possible 
attempting maintain classi cation instance 
desirable goal necessarily speci particular induction algorithm 
focus algorithm independent paradigm feature subset selection viewing induction algorithm biased method approximating probability distribution class labels features transforming distribution classi cation 
stay free bias particular induction algorithm simply maintaining possible underlying conditional distribution class labels induction algorithm attempts approximate 
due large part induction bias free nature approach provides modest gains accuracy domains 
tool obtaining better accuracy high dimensional datasets 
currently faced domain essentially forced simpler computationally intensive induction algorithms naive bayes performance linear number features 
feature selection algorithm pre processing step enable powerful computationally expensive induction algorithms full bayesian classi ers 
way hope able induction methods applicable large problems features 
furthermore argue wrapper methods priori computationally expensive datasets feature reduced datasets resulting algorithm 
allow produce classi er optimized accuracy respect speci induction algorithm searching ltered feature space 
taken hope ectively tackle induction problems large feature spaces 
almuallim dietterich 
learning irrelevant features ninth national conference arti cial intelligence mit press pp 

blumer ehrenfeucht haussler warmuth 
occam razor information processing letters 
caruana freitag 
greedy attribute selection cohen hirsh eds machine learning proceedings eleventh international conference morgan kaufmann publishers cover thomas 
elements information theory wiley 
draper hanks 
localized partial evaluation belief networks proceedings tenth annual conference uncertainty arti cial intelligence uai pp 

duda hart 
pattern classi cation scene analysis wiley 
john kohavi eger 
irrelevant features subset selection problem machine learning proceedings eleventh international conference morgan kaufmann pp 

kira rendell 
feature selection problem traditional methods new algorithm tenth national conference arti cial intelligence mit press pp 

kohavi 
wrappers performance enhancement oblivious decision graphs phd thesis stanford university computer science department 
singh 
sensitivities alternative conditional probabilities bayesian belief networks proceedings eleventh annual conference uncertainty arti cial intelligence uai pp 

kullback leibler 
information su statistics 
ciency annals mathematical langley sage 
induction selective bayesian classi ers proceedings tenth conference uncertainty arti cial intelligence morgan kaufmann publishers seattle wa pp 

langley iba thompson 
analysis bayesian classi ers proceedings tenth national conference arti cial intelligence aaai press mit press pp 

murphy aha 
uci repository machine learning databases www ics uci edu mlearn mlrepository html 
pearl 
probabilistic reasoning intelligent systems morgan kaufmann san mateo ca 
quinlan 
programs machine learning morgan kaufmann publishers los altos california 
reuters reuters collection available anonymous ftp distribution research purposes granted reuters carnegie group 
arrangements access david lewis 
ftp ciir ftp cs umass edu pub reuters 
cient learning selective bayesian network classi ers sub singh provan 
publication 

