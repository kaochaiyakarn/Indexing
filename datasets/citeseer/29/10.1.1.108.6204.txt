massachusetts institute technology artificial intelligence laboratory memo september memo estimating dependency structure hidden variable marina michael jordan morris publication retrieved anonymous ftp publications ai mit edu 
introduces probability model mixture trees account sparse dynamically changing dependence relationships 
family efficient algorithms em minimum spanning tree algorithms learn mixtures trees ml framework 
method extended take account priors wide class priors includes dirichlet mdl priors preserves computational efficiency 
experimental results demonstrate excellent performance new model density estimation classification 
show single tree classifier acts implicit feature selector making classification performance insensitive irrelevant attributes 
tree distributions mixture components called mixture coefficients 
graphical models perspective mixture trees viewed containing unobserved choice variable value probability conditioned value distribution visible variables tree 
trees may different structures different parameters 
note structure variable mixture trees properly belief network results owe belief network perspective 
basic algorithm ml fitting mixtures trees section show mixture trees fit observed dataset maximum likelihood paradigm em algorithm :10.1.1.133.4884
observations denoted corresponding values structure variable 

usual em procedure mixtures expectation step consists estimating posterior probability mixture component tree generate datapoint xi tk xi expected complete log likelihood maximized step algorithm pr model lc model log 
sums represent total number points assigned tree 
ftp ftp ics uci edu pub databases 
chow liu 
approximating discrete probability distributions dependence trees 
ieee transactions information theory may 
dempster laird rubin :10.1.1.133.4884
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
frey geoffrey hinton peter dayan 
wake sleep algorithm produce density estimators 
