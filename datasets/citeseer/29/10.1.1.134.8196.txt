prioritized sweeping reinforcement learning data real time andrew moore christopher atkeson mit arti cial intelligence laboratory technology square cambridge ma new algorithm prioritized sweeping cient prediction control tic markov systems 
incremental learning methods di learning fast real time performance 
classical methods slower accurate full observations 
prioritized sweeping aims best worlds 
uses previous experiences prioritize important dynamic programming sweeps guide exploration state space 
compare prioritized sweeping reinforcement learning schemes number di erent stochastic optimal control prob lems 
gauss seidel expensive algorithm requiring backups real world observation inner loop 
absorption predictions observation ik provide excellent initial approximation iterations required 
interesting observation encountered example previously experienced transition terminal state iterations needed convergence 
prioritized sweeping prioritized sweeping designed perform task gauss seidel iteration careful bookkeeping concentrate computational ort interesting parts system 
operates similar computational regime dyna architecture sutton xed non trivial amount computation allowed real world tion :10.1.1.48.6005
peng williams exploring closely related approach prioritized sweeping developed dyna learning watkins 
prioritized sweeping uses value probability update step previous algorithm determine updates interesting step produces large change state absorption probabilities interesting absorption probabilities predecessors state change opportunity 
hand step produces small change assume urgency process predecessors 
predecessors state states history system performed step transition just changed absorption probabilities maximum possible processing step change predecessor caused change value priority predecessor currently priority queue placed priority queue lower priority promoted 
promote state top priority queue 

allowed processing priority queue empty remove top state priority queue 
call new max actions max new ji ji new max ij jj tiny threshold queue exceeds current priority promote new priority prioritized sweeping algorithm markov decision tasks 
philosophy optimism face uncertainty method successfully developed interval estimation algorithm kaelbling exploration bonus technique dyna sutton :10.1.1.48.6005
philosophy thrun 
slightly di erent heuristic prioritized sweeping algorithm 
minor problems computational expense instability exploration bonus large state spaces 
slightly di erent optimistic heuristic follows 
conceptually similar idea discovered independently 
prioritized sweeping provides cient data processing methods learn state transition model dyna queue performs role learning watkins algorithm avoids building explicit state transition model 
dyna queue careful allows priority queue allows predecessors predicted change interestingness value greater signi cant threshold prioritized sweeping allows change times maximum reward queue 
initial experiments peng williams consist sparse deterministic maze worlds cells 
performance measured total number bellman equation processing steps convergence greatly improved conventional dyna sutton :10.1.1.48.6005
related sutton identi es reinforcement learning asynchronous dynamic programming computational regime prioritized sweeping 
notion optimistic heuristic guide search tree search algorithm nilsson motivated aspect prioritized sweeping schedules nodes expanded albeit di erent priority measure 
korf gives combination dynamic programming lrta algorithm 
lrta di erent prioritized sweeping concentrates search ort nite horizon set states current actual system state 
