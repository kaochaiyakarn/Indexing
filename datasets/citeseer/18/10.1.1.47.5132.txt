error entropy discretization continuous features ron kohavi data mining visualization silicon graphics shoreline blvd mountain view ca ronnyk sgi com mehran sahami gates building room computer science department stanford university stanford ca sahami cs stanford edu comparison error methods discretization continuous features 
study includes extensive empirical comparison analysis scenarios error minimization may inappropriate discretization criterion 
discretization method decision tree algorithm compare existing entropy discretization algorithm employs minimum description length principle proposed error technique 
evaluate discretization methods respect naive bayesian classifiers datasets uci repository analyze computational complexity method 
results indicate entropy mdl heuristic outperforms error minimization average 
analyze shortcomings error approaches comparison entropy methods 
real world classification data mining tasks involve continuous features exist algorithms focus learning nominal feature spaces apte hong cost salzberg 
order handle continuous features algorithms regularly employ simple discretization methods uniform binning data produce nominal features 
naive discretization data potentially disastrous data mining critical information may lost due formation inappropriate bin boundaries 
furthermore discretization may viewed form knowledge discovery critical values continuous domain may revealed 
noted catlett large data sets common data mining applications discretizing continuous features vastly reduce time necessary induce classifier 
result better discretization methods developed methods directly compared analyzed may appropriate employ 
dougherty kohavi sahami provided initial comparison uniform binning discretization method proposed holte entropy method proposed fayyad irani induction algorithms quinlan naive bayesian classifier 
study reported entropy discretization method promising method compare method methods discretization error discretization 
discretization new method applies continuous feature separately determine number thresholds values 
entropy metric gain ratio different criterion number intervals determined pruning opposed fayyad irani stopping criteria 
error discretization compare proposed maass algorithm auer holte maass 
number intervals method constructs optimal discretization continuous feature respect classification error polynomial time 
employ discretization methods listed conjunction naive bayesian classifiers induction algorithms run discretized data show effectiveness discretization method 
computational complexity discretization technique 
light empirical findings analyze situations discretization may inappropriate 
methods briefly describe induction algorithms discretization methods compare 
induction algorithms experimental study test different discretization methods applied naive bayesian classifiers 
quinlan state art top decision tree induction algorithm 
discretize features declare nominal multi way split possible thresholds 
naive bayesian induction algorithm computes posterior probability classes data assuming independence features class 
probabilities nominal features estimated counts gaussian distribution assumed continuous features cases 
naive bayesian classifier experiments implemented mlc kohavi 
discretization algorithms focus discretization methods entropy developed error discretization method 
methods described 
comprehensive review existing discretization literature dougherty kohavi sahami 
fayyad irani method consider discretization entropy minimization heuristic proposed fayyad irani 
method similar catlett offers motivated heuristic deciding number intervals 
algorithm uses class information entropy candidate partitions select threshold boundaries discretization 
finds single threshold minimizes entropy function possible thresholds recursively applied partitions induced 
minimal description length principle mdlp employed determine stopping criteria recursive discretization strategy 
refer algorithm ent mdl 
implementation split considered entropy method takes log time number instances assume fixed number classes 
method chooses thresholds threshold computations done 
upper bound time complexity km log 
bound improved smarter implementation sort 
assume thresholds form balanced tree time sort instances level log time bound reduced log delta log 
practice expect behavior bounds 
space complexity method feature value label instance stored 
discretization decision tree induction algorithm discretization method 
sense applied continuous feature separately build tree contains binary splits test single continuous feature 
algorithm uses gain ratio entropy metric determine partitions discrete intervals 
refer new method disc 
method significantly different fayyad irani employs top stopping criterion mdlp applying single feature builds complete tree feature applies pruning find appropriate number nodes tree number discretization intervals bottom approach 
tree single feature built pruned simply threshold values node induced tree threshold values discretization continuous feature 
default pruning confidence parameter pruning forced pruned heavily order prevent forming intervals 
set confidence factor 
minor variations value effect experiments 
prevent overfitting value try optimize experiments 
time complexity discretize features requires full single feature tree built pruned back 
build time dominates pruning time intervals returned constructed 
assume constant portion instances say split time time bound log gammap delta log log gammap levels tree log time 
space complexity discretization feature value label instance stored 
error discretization significant error discretization carried 
maass developed algorithm optimally discretize continuous feature respect error training set 
algorithm discretizes continuous feature producing optimal set fewer intervals results minimum error training set instances classified single feature discretization 
refer algorithm maximum number intervals user set parameter 
method implemented part induction algorithm auer holte maass induces level decision trees 
circumvented difficulty providing justification value simply setting number classes plus 
algorithm employs dynamic programming approach efficiently compute optimal error discretization thresholds 
heuristic time complexity algorithm log space complexity number training instances 
implementation algorithm induction system tried different approaches setting value approach proposed described call second approach set number intervals proposed running ent mdl method allows compare values call method mdl 
results presenting experimental results analyze 
empirical findings table shows datasets chose comparison 
chose datasets uci repository murphy aha continuous feature 
fold crossvalidation determine error rates application discretization induction method pair dataset 
important note performing cross validation separately discretized training set fold 
discretizing data creating folds cross validation allows discretization method access testing data known result optimistic error rates 
shows results 
report error rate discretization method conjunction normalized error rate original run data prior discretization 
relative error bars show improvement discretization values show degradation classification performance 
generally lower values better 
shows analogous table naivebayes normalized error rate naive bayes normal distribution gaussian continuous features 
results show ent mdl better average run discretization lowering error rate instances significantly increasing 
run ent mdl significantly outperforms discretization provides regularization effect data determine interval boundaries training opposed training data fragmented 
absolute average errors respectively significant differences computed test ionosphere improved value glass improved value cleve improved value 
ent mdl average best performing discretization method methods tried 
noteworthy result method entropy attempt directly minimize error objective function 
looking discretization algorithms error rates increased significantly cases cases slightly decreased 
method hypothyroid degraded significantly 
hypothyroid relative difference significant value 
examined discretization methods carefully noted features error features 
algorithm discretizes feature intervals folds heuristics mdl recommended intervals 
reason problem create adjacent intervals majority class 
explore impact phenomenon artificial example 
reported previous dougherty kohavi sahami form discretization produced large improvements naive bayesian algorithm normality assumption continuous variables 
discretization allows algorithm better approximate true distribution continuous variable distribution normal computes accurate posterior probability instance particular class 
rare cases continuous features domain fact normally distributed case iris features diabetes find discretization causes small increase error rate exception norm 
find discretization applied error rates lower domains relatively unchanged domains worse domains 
discretization methods performed approximately ent mdl slight winner average 
worth noting naive bayes run significantly outperformed 
example performance anneal cleve glass better values performance dataset features dataset majority dataset features dataset majority cont nom size error cont nom size error anneal australian breast cancer cleve crx diabetes german glass glass heart hepatitis horse colic hypothyroid ionosphere iris sick euthyroid vehicle table datasets number continuous features nominal features dataset size baseline error majority inducer folds 
mdl mdl disc different discretization methods 
error ratios different discretization algorithms relative original 
lower values better 
mdl mdl disc error ratio naive bayes different discretization methods 
error ratios different discretization algorithms relative naive bayes assuming normal distribution 
lower values better 
breast diabetes glass heart better values 
running times experiments negligible 
time intensive datasets discretize ent mdl sick euthyroid hypothyroid took seconds fold sgi challenge 
longest running time encountered glass dataset took seconds fold discretize longer datasets examined 
method run letter domain mb main memory 
error vs entropy better understand entropy methods outperformed datasets discretize suggested number intervals simple example show shortcomings error discretization 
consider boolean target function continuous variables range defined 
function projection shown 
note intervals interest threshold intervals interest thresholds 
unable form intervals function instances positive instances negative 
leaves large middle interval instances labeled positive negative depending value feature assuming uniform distribution instances middle interval generally negative instances 
result majority negative instances adjacent partitions problematic observation shows 
observation generate adjacent intervals label 
reason intervals collapsed interval degradation error 
see inherent limitation 
implication possible labelings intervals class problem possible 
entropy discretization methods limitation partition space long class distribution different partitions different 
generated instances uniformly randomly distributed target concept ran fold cross validation ent mdl 
class class instance number class class instances artificial target concept top projection second feature bottom 
note projection instances class instances range mixed 
heuristic recommends intervals feature class problem 
returns intervals really folds 
second threshold close edge 
mentioned returns intervals folds 
bayes error rate partitions returned 
ent mdl hand correct number partitions thresholds close true values 
bayes error rate partitions returned 
conclude error minimization techniques find optimal partition reduce training set error feature entropy methods fare better practice feature interaction long distribution different threshold formed allowing features final discrimination 
dougherty kohavi sahami describe axes discretization methods measured supervised vs unsupervised global vs local static vs dynamic 
methods examined supervised instance label information performing discretization unsupervised methods equal width binning 
compare unsupervised methods previous noted supervised methods tendency better practice 
distinction global local methods stems discretization performed 
global discretization involves discretizing continuous features prior induction 
local methods hand carry discretization induction process particular local regions instance space may discretized differently splits continuous feature differently different branches decision tree 
methods compared applied globally 
aim measure effectiveness methods applied locally 
discretization methods require parameter indicating maximum number intervals produce discretizing feature 
static methods examined perform discretization pass data feature determine value feature independent features 
dynamic methods conduct search space possible values features simultaneously capturing interdependencies feature discretization 
course study looked dynamic versions discretization methods wrapper approach john kohavi pfleger means searching space number discretization intervals variables simultaneously 
significant improvement employing dynamic discretization static counterpart 
results show ent mdl slightly superior methods datasets 
described methods inappropriate cases features interact analyzed time space complexity different algorithms 
acknowledgments lise getoor comments earlier version peter auer providing code 
second author supported arpa nasa nsf stanford digital libraries project 
experiments reported done mlc apte hong 
predicting equity returns security data 
advances knowledge discovery data mining 
aaai press mit press 
chapter 
auer holte maass 
theory applications agnostic pac learning small decision trees 
machine learning proceedings twelfth int 
conference 
morgan kaufmann 
catlett 
changing continuous attributes ordered discrete attributes 
kodratoff ed proceedings european working session learning 
berlin springer verlag 
cost salzberg 
weighted nearest neighbor algorithm learning symbolic features 
machine learning 
dougherty kohavi sahami 
supervised unsupervised discretization continuous features 
machine learning proceedings twelfth int 
conference 
morgan kaufmann 
fayyad irani 
discretization continuous valued attributes classification learning 
proceedings th int 
joint conference artificial intelligence 
morgan kaufmann 

estimation probabilities essay modern bayesian methods 
press 
holte 
simple classification rules perform commonly datasets 
machine learning 
john kohavi pfleger 
irrelevant features subset selection problem 
machine learning proceedings eleventh int 
conference 
morgan kaufmann 
kohavi john long manley pfleger 
mlc machine learning library 
tools artificial intelligence 
ieee computer society press 
www sgi com technology mlc 
maass 
efficient agnostic pac learning simple hypotheses 
proceedings seventh annual acm conference computational learning theory 
murphy aha 
uci repository machine learning databases 
www ics uci edu mlearn 
quinlan 
programs machine learning 
los altos california morgan kaufmann 
