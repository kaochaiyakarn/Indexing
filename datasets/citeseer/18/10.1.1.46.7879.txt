self segmentation sequences ron sun chad sessions nec research institute independence way princeton nj university alabama tuscaloosa presents approach hierarchical reinforcement learning rely priori hierarchical structures 
approach deals difficult problem compared existing 
involves learning segment sequences create hierarchical structures reinforcement received task execution different levels control communicating sharing reinforcement estimates obtained 
algorithm segments sequences reduce non markovian temporal dependencies facilitate learning task 
initial experiments demonstrated basic promise approach 
hierarchical approaches evidently important reinforcement learning existing hierarchical rl models involve automatically developing hierarchies pre determined hierarchies dayan hinton sutton precup parr russell dietterich involve domain specific processes 
models category rely domain specific knowledge procedures generic autonomous example lin moore atkeson singh 
problems hierarchies include inflexibility characteristics domain change time lack generality domain specific hierarchies vary domain domain 
true limited learning fine tune pre determined hierarchies parr russell dietterich 
general approach automatically develop hierarchies scratch generic structures fixed number levels automatically tailor details structures parameters reinforcement learning 
process amounts automatically segmenting sequences self segmentation 
try address issue learning segmentation little domain specific knowledge procedures ring schmidhuber thrun schwartz 
difficult issue tackle doing dealing non markovian temporal dependencies situations optimal actions point may dependent current state states occurring time ago 
dependencies create problems reinforcement learning 
reinforcement learning different levels approach may seek proper configurations nonmarkovian temporal dependencies goal reducing non markovian dependencies segmenting proper places different levels extending approach wiering schmidhuber see details facilitate learning task 
ii 
review rl rl viewed line variation dynamic programming dp 
dp general follows discrete time system state transitions depend actions performed agent 
probabilistic terms markovian process working determining new state state transition action performed prob js gamma gamma prob js determined policy 
normally costs rewards accumulate additively discount factor 
cost reward estimate results optimal policy actions satisfies bellman optimality equation max fl state new state resulting action bellman optimality equation number line rl algorithms 
qlearning algorithm watkins updating done completely line explicitly probability estimates max gammaff ff fl max determined action policy prob temperature determines degree randomness action selection 
updating done actual state transition transition probabilities line simulation performed 
learning allows completely autonomous learning scratch priori domain knowledge 
iii 
sss algorithm approach automatically developing hierarchical rl sss algorithm stands self segmentation sequences 
different usually markovian domains learning deal non markovian domains agent observes local information constitutes observational state observation true state 
true state may determined history observational states 
general idea general idea follows number individual action modules referred modules 
selects actions performed step 
module corresponding controller cq determines step module continue relinquish control 
module currently control relinquishes control higher level controller aq decide module take current point 
state pair cq control 
cq selects continue select action regard current state affect environment generate reinforcements environment 
cq selects control returned aq proceed immediately select regard current state cq corresponding take 
cycle repeats 
algorithm look level hierarchy types learning modules ffl individual action module performs actions learns learning 
ffl individual controller cq cq learns module corresponding cq continue control give control 
learning accomplished separate learning 
ffl controller aq performs learns control actions module select circumstances 
learning accomplished separate learning 
algorithm follows 
observe current state 
currently active cq pair takes control 
active system starts go step 

active cq selects performs control action cq ca different ca 
action chosen cq go step 
active selects performs action different 
active cq performs learning 
go step 

aq selects performs control action aq aa different aa select cq pair active 

aq performs learning 
go step 
learning rules specified follows 
module learns scratch 
segment action sequences 
ffl individual action modules 
active current action action corresponding cq usual learning rule deltaq ff fl max gamma new state resulting action state explanation cqk decides continue qk learns estimate reinforcement generated current action subsequent actions assuming greedy policy action highest value state plus reinforcement generated modules qk cqk relinquishes control see learning rule 
value qk sum estimates parts 
current action cq continue action cq module receives reward maximum value aq deltaq ff fl max aa aq aa gamma new state resulting action state control returned aq cq aa action aq 
explanation corresponding cq terminates control current module aq take action state value discounted total reinforcement received point system action taken greedy policy action highest value current state followed aq current stochastic policies followed modules 
termination module learns total discounted reinforcement received system thereon greedy policy followed aq current stochastic policies followed modules 
combining explanations rules see module decides action action lead higher reinforcement current state subsequent modules 
ffl individual controllers 
corresponding cq separate learning rules different actions 
current action cq continue learning rule usual qlearning deltac continue ff fl max ca cq ca gammac continue new state resulting action continue state ca control action cq explanation cqk decides continue learns estimate reinforcement generated actions corresponding qk assuming cqk gives control higher value plus reinforcement generated qk cqk relinquishes control see learning rule 
cqk learns value continue sum expected reinforcement generated qk cqk cqk continue expected reinforcement generated subsequent modules 
current action cq learning rule deltac ff max aa aq aa gamma cq aa control action aq 
explanation cq ends learns value best action performed aq equal discounted total reinforcement accumulated system current state greedy policy followed aq current stochastic policies followed modules 
combining learning rules effect cq module learns decisions comparing giving continuing control lead reinforcement current state 
ffl controllers 
aq learning rule aa ff ag fl max aa aq aa aa aa control action selects module take control new state aq required decision number time steps taken go determining amount discounting fl ag discounted cumulative reinforcement received control action state control action state ag gamma fl 
explanation aq learns value control action selects discounted cumulative reinforcement chosen module accrue aq action decision plus accumulation reinforcement action assuming greedy policy followed aq current stochastic policies followed modules 
words aq estimates total reinforcement accrued greedy modules follow current policies 
effect aq learns select lower level modules comparing selections lead reinforcement current point 
interaction types modules sss segments sequences create hierarchy subsequences effort maximize reinforcement 
note specification respect level hierarchy 
easily extend levels see full technical report details 
iv 
experiments maze 
maze possible starting locations goal location 
agent occupies cell time local information concerning adjacent cells 
regarding opening wall 
move step goal fig 

maze requiring segmentation 
number indicates unique observational state perceived agent 
goal goal fig 

different segmentations modules modules 
adjacent cell go left go right go go 
adjacent cell wall agent remain original cell step 
reward reaching goal location 
see indicates starting cells 
number indicates observational state true state perceived agent 
domain minimum path length number steps optimal path starting cell goal cell 
shown cells marked perceived agent 
different actions required cells order obtain shortest paths goal 
goes cells marked 
remove nonmarkovian dependencies action module divide sequence starting cells goal number segments different modules segments consistent policy adopted module 
domain experiments show single agent atemporal representation learn task learning curve flat due oscillations averaging values 
adding temporal representation memory may partially remedy problem approach difficulty dealing long range dependencies comparable sss atemporal representation 
adopted see maze experiments 
hand confirmed experiments sss segment sequences remove non markovian dependencies atemporal representation 
minimum cq pairs necessary order segment properly completely remove non markovian dependencies module shown 
finding optimal paths pairs modules lower level proves difficult system able find switching points modules exactly right sequence 
general opt path opt path avg path test pairs 
fig 

effect number modules performance learning 
main modules easier segmentation done better learning 
modules possibilities alternatives proper segmentation removes non markovian dependencies 
anova analysis number modules block training shows significant main effect number modules significant interaction number modules block training indicates number modules significant impact learning performance 
training episodes tested performance resulting systems conditions completely deterministic policy values module stochastic policy values boltzmann action selection scheme 
case indicated figures significant performance improvements terms percentages optimal path traversed terms average path length maximum length imposed path agent go incrementally modules modules 
pairwise tests successive average path lengths confirmed 
see test results 
experiments demonstrate proper segmentation possible sss 
learning set markovian processes atemporal representations higher lower level algorithm removed non markovian dependencies handled sequence involved learned chain markovian processes 
maze 
maze starting location goal location 
reaching goal location agent reach top arms 
agent receives local information concerning adjacent cells regarding opening wall 
see number indicates observational state perceived agent 
domain shortest path consists modules available possibilities segmentation 
example possible ways segmentation modules shown 
goal fig 

maze requiring segmentation reuse modules 
number indicates unique observation state perceived agent 
goal fig 

segmentation modules 
steps 
confirmed experiments minimum cq pairs needed order remove non markovian dependencies segmentation obtain shortest path atemporal representation 
pairs modules lower level exactly way segmenting sequence considering shortest path goal switching different module top cell arm marked switching middle cell arms marked 
see 
segmentation involves repeated segments modules way goal 
reuse modules leads compression sequences 
experiments domain demonstrate points earlier regarding feasibility 
furthermore experiments domain demonstrate reuse module 
reuse modules leads compression sequences allows handling sequences efficiently handled 
example domain handled simpler models segmentation limited linear chaining small number modules reusing modules wiering schmidhuber 
maze 
maze starting location marked goal location marked 
agent reach key marked order open door marked 
opening door agent reach goal 
see 
agent local information concerning adjacent cells left right cells regarding opening wall information concerning modules added reuse modules reduced third module example place second module 
simpler model handle domain modules available chain modules formed 
approach results great inefficiency large number segments 
fig 

maze involving long short range dependencies 
modules perc optimal path perc optimal path fig 

effect number modules performance learning terms percentage optimal paths 
locations vary locations entities re training agent time 
agent information current location step 
domain involves long range non markovian dependencies 
long range dependencies result requisite sequential ordering reaching 
providing aq information requiring aq select lower level modules accomplish subtasks sss reduces long range dependencies aq effect short range dependencies aq aq effect skip intermediate steps 
shortening distance dependencies main point hierarchical reinforcement learning 
hand short range dependencies cq modules result lack current location information local perceptual input relied cq modules local action decisions distinguish different cells identical adjacent cell settings 
mccallum method dealing non markovian dependencies modules 
dependencies module fact short range learning simplified imposing particular order testing preceding steps goes backwards immediately preceding state action pair furthest away 
cq pairs episodes training success rate reaches 
see 
analysis shows learning aq came rely information concerning locations select modules go basis 
due non needed history past actions order correct selections modules example choosing module going aq choose module go 
cq modules hand came rely mainly local information concerning adjacent cells 
needed history order disambiguate situations correct action decisions reach respective subgoals 
turned learning different modules involved different preceding steps desirable savings entails compared fold state space 
separation long range short range nonmarkovian dependencies dealt aq cq respectively resulted greater efficiency compactness resulting policies 
temporal representation module system acquisition temporal representation mccallum method takes long time see temporal dependencies long distance domain 
hand atemporal representation level system segmentation segments created order remove temporal dependencies including local ones 
correspondingly modules needed 
notice shared subroutines developed sub tasks 
example episode similar pair locations episode module may called go go different episodes 
sharing leads compactness resulting representation 
completely separate sets modules different sub tasks modules shared different sub tasks fewer modules needed 
supported part office naval research 
dayan hinton 
feudal reinforcement learning 
nips 
dietterich 
hierarchical reinforcement learning maxq value function decomposition 
ftp www cs orst edu frasconi gori soda 
recurrent neural networks prior knowledge sequence processing 
knowledge systems 


learning simple rl society mind 
technical report university cambridge computer laboratory 
kaelbling 
hierarchical learning stochastic domains preliminary results 
proc 
icml 
morgan kaufmann san francisco ca 
lin 
reinforcement learning robots neural networks 
ph thesis carnegie mellon university pittsburgh 
mahadevan connell 
automatic programming behavior robot reinforcement learning 
artificial intelligence vol pp 

mccallum 
learning selective attention short term mem ory sequential tasks 
proc 
conference simulation adaptive behavior 

mit press cambridge ma 
moore atkeson 
parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
machine learning 
parr russell 
reinforcement learning hierarchies machines 
advances neural information processing systems 
mit press 
precup sutton singh 
multi time models temporary planning 
advances neural information processing systems 
mit press 
schmidhuber 
learning complex extended sequences principle history compression 
neural computation 
schmidhuber 
learning unambiguous reduced sequence descriptions 
advances neural information processing systems 
singh 
learning solve markovian decision processes 
ph thesis university massachusetts amherst ma 
sutton 
td models modeling world mixture time scales 
proc 
icml 
morgan kaufmann san francisco ca 
tham 
reinforcement learning multiple tasks hierarchical cmac architec ture 
robotics autonomous systems 

thrun schwartz 
finding structure reinforcement learning 
neural information processing systems mit press cambridge ma 
watkins 
learning delayed rewards 
ph thesis cambridge university cambridge uk 
wiering schmidhuber 
hq learning 
adaptive behavior 
