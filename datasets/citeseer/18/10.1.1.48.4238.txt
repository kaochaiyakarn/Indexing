aaai proceedings th national conference artificial intelligence 
menlo park ca aaai press 
learning non linearly separable boolean functions linear threshold unit trees madaline style networks mehran sahami department computer science stanford university stanford ca sahami cs stanford edu investigates algorithm construction decisions trees comprised linear threshold units presents novel algorithm learning nonlinearly separable boolean functions networks isomorphic decision trees 
construction networks discussed performance learning compared standard backpropagation sample problem irrelevant attributes introduced 
littlestone winnow algorithm explored architecture means learning presence irrelevant attributes 
learning ability madaline style architecture non optimal larger necessary networks explored 
initially examine non incremental algorithm learns binary classification tasks producing decision trees linear threshold units ltu trees 
decision tree bears similarity decision trees produced id quinlan perceptron trees utgoff promise generality node tree implements separate linear discriminant function leaves perceptron tree generality remaining nodes perceptron tree trees produced id perform test feature 
brodley utgoff shown multivariate tests node decision tree provides greater generalization learning concepts irrelevant attributes 
furthermore brent show ltu tree transformed layer neural network hidden layers output layer input layer counted trained quickly standard back propagation algorithm applied entire network rumelhart hinton williams 
examining transformation new incremental learning algorithm architecture widrow winter learning performed layer networks 
effectiveness algorithm assessed sample non linearly separable boolean function order perform comparisons ltu tree algorithm similar network trained standard back propagation 
primarily interested functions irrelevant attributes exist explore performance winnow algorithm littlestone proven effective learning linearly separable functions presence irrelevant attributes madaline style learning architecture 
contrast performs learning sample non linearly separable function classical fixed increment perceptron updating method duda hart 
examine effectiveness learning procedures nonoptimal madaline style networks comment possible extensions learning architecture 
ltu tree algorithm tree building algorithm non incremental requiring set training instances available outset 
root node tree produce hyperplane separate training set means wish trials back propagation applied node produce single separating hyperplane sets indicates set instances classified separating hyperplane 
instances classified called incorrect create left child node recursively apply algorithm left child training set 
similarly instances classified incorrect create right child node recursively apply algorithm right child training set 
algorithm normally terminates instances original training set correctly classified tree 
classification procedure completed tree requires simply root node determine instance classified hyperplane stored 
classification means follow left branch follow right recursively apply procedure hyperplane stored appropriate child node 
classification leaf node tree final output classification procedure 
note leaves decision tree notation naming conventions description ltu tree algorithm brent 
classify instances labeling classification instance result applying linear discriminator stored leaf node 
experiments certain reasonable limiting assumptions placed building ltu trees order prevent needlessly complex trees helping improve generalization reduce algorithm execution time 
included setting maximum tree depth layers tolerating certain percentage error individual node 
condition set empirical observations indicated number similarly classified instances node certain percentage erroneous classifications acceptable precluding branching particular classification node 
values follows initial testing performed ltu tree architecture variety methods learning linear discriminant node tree sahami 
wishing minimize number erroneous classifications node tree backpropagation appeared promising weight updating procedures 
heuristic minimizing errors node occasionally produce larger optimal trees generally produces trees optimal near optimal size shown produce smallest trees number sample functions compared weight updating procedures 
allowed store hyperplane node entire network interesting angle research apply backpropagation algorithm unit time 
unit linear threshold unit threshold set training completed threshold training 
output unit trained back propagation nk actual real valued output nth trained unit instance output linear threshold unit represents bias weight unit 
updating procedure training node optimal tree contain minimum number linear separators nodes necessary successfully classify instances training set dw momentum dw dw dq dq weight vector updated instance vector 
set momentum experiments 
possible extensions ltu algorithm including irrelevant attribute elimination brodley utgoff producing hyperplanes node different weight updating procedures selecting hyperplane causes fewest number incorrect classifications bayesian analysis determine instance separations langley post processing tree reduce size modifications scope generally fine underlying learning architecture changed 
creating networks ltu trees trees produced ltu tree algorithm mechanically transformed layer connectionist networks implement functions 
ltu tree nodes construct isomorphic network containing nodes tree hidden layer fully connected set inputs 
second hidden layer consisting nodes gates number possible distinct paths root leaf node node children 
output layer merely gate connected nodes previous layer 
connections second hidden layers constructed traversing possible path root leaf tree node recording branch followed get 
node second hidden layer represents single distinct path connected nodes layer correspond nodes traversed path 
nodes second hidden layer merely gates inputs coming hidden layer inverted left branch traversed node corresponding input hidden layer 
examples 
pointed brent efficient classifications tree structure corresponding network computations performed lie single path root tree leaf 
conveniently examine incrementally train network corresponds ltu tree may transform trained network decision tree attain computational benefit classification 
shows node tree produced ltu tree algorithm shows corresponding network performing transformation described 
nodes correspond directly nodes 
node simply output node input path length tree root node considered leaf 
node conjunct inverted output node follow left branch node reach node tree output node 
node simply gate 
shows complex tree produced ltu tree algorithm represents corresponding network 
nodes correspond directly nodes 
node represents path tree inverted output node inverted output node output node inputs 
node represents path node tree considered leaf inverted output node output node inputs 
node corresponds path outputs nodes inputs 
node simply disjunction outputs nodes 
madaline style learning algorithm updating strategy madaline style architecture modifying weight vectors hidden layer nodes appropriately strengthening weakening incorrect predictions network 
knowing structure ltu tree corresponds network training 
instance incorrectly classified know nodes second hidden layer corresponding leaf fired 
look node corresponding leaf node closest threshold strengthen 
examine nodes corresponding non leaf nodes know exists path root leaf node closest threshold 
nodes threshold leaf left child node network corresponding particular non leaf node weakened 
similarly node corresponding non leaf node threshold leaf node path right child node network corresponding non leaf node strengthened 
instance misclassified simply find node second hidden layer network weaken nodes inputs correspond leaf nodes case network translates updating procedure misclassified determine node node closer threshold node closer threshold strengthen node strengthen node 
misclassified node case output node weaken node weaken node 
nodes strengthened weakened learning method madaline style networks 
classical fixed increment referred simply madaline littlestone winnow algorithm referred winnow employed tests follows algorithm updating method fixed increment strengthen madaline weaken winnow strengthen winnow weaken weight vector ith component node modified instance vector misclassified 
note winnow uses fixed threshold set initial experiments 
experimental results testing ltu tree algorithm corresponding network learn non linearly separable bit boolean function 
function defined function effectively disjunction ofk threshold functions linearly separable optimally learned hyperplanes separate instance space 
testing various learning methods function compare ltu tree algorithm training networks configured similarly optimal size network learn function 
training networks compare standard back propagation applied entire network preset fixed weights second hidden output layers simulate appropriate gates novel madaline style learning method discussed 
note learning procedure effectively learning separating hyperplanes hidden layer network corresponding learning nodes ltu tree 
technical note instance vectors ltu tree back propagation applied entire network include original boolean vector comprised complements original vector create double length instance vector preliminary testing showed complements helped improve learning performance algorithms 
madaline style tests instance vectors fixed increment updating composed addition complements winnow instance vectors similar ltu tree complementary attributes added 
number instances training number dimensions input vector varied 
note bits instance vector relevant proper classification added bits simply random irrelevant attributes 
dimensions graphs measure size original instance vector including complementary attributes 
graphs represent test runs algorithm case 
testing done independent randomly generated set instances numbering training set 
error average refers percentage errors testing algorithm test runs 
error best refers smallest percentage errors testing test runs 
see average case trained instances seen madaline network fixed increment updating outperforms algorithms number irrelevant attributes increased 
ltu tree called bp tree performs errors dimensions time consistently producing optimal trees nodes quickly begins degenerate performance trees produces get larger due poor separating hyperplanes produced node 
surprisingly point backpropagation entire network begins degenerate quickly leading realize network getting small properly deal irrelevant attributes 
winnow performs due primarily seeing instance vectors settle solution state 
best case analysis indicates simple linear increase number errors madaline caused linear increase sum weights irrelevant attributes opposed erratic increase indicating boolean function learned 
similarly winnow capable learning function dimensions quickly degenerates indicating learning effectively place opposed occasional misclassifications caused added irrelevant attribute weights 
find bp network unable learn dimensions bp tree effective dimensions 
examine results training instances seen effectiveness madaline style architecture clear 
average case find standard bp network dimensions 
see extremely low error rates madaline way indicating target function learned effect irrelevant attribute weights minimized 
find successful learning target function instances dimensions length predictive accuracy begins fall 
similarly bp tree effective instances dimensions winnow madaline bp network bp tree trained randomly generated instances dimensions dimensions trained randomly generated instances dimensions dimensions tree sizes grow large linear separators node provide poorer splits 
best case see striking results madaline continues low error rate winnow errors entire range dimensions tested 
indicate training number winnow networks cross validation techniques determine highest predictive accuracy learn nonlinearly separable boolean functions extremely high degree accuracy presence irrelevant attributes 
course require knowledge network size provide best results initially running ltu tree algorithm data set provide approximations 
non optimal networks having seen predictive accuracy madaline style networks learning optimal network size known important get idea accuracy networks non optimal 
examining notion optimal network size stems transformation optimal ltu tree 
effects network larger necessary network learn bit non linearly separable problem 
updating procedure network described misclassified determine node closest threshold node closest threshold strengthen node node threshold weaken node 
node closest threshold strengthen node node threshold strengthen node 
node closest threshold strengthen node node threshold weaken node 
misclassified determine node output node weaken node 
output node weaken node 
output node weaken node 
compare previous results madaline winnow smaller network denoted larger network denoted 
looking average test runs training instances see performance madaline worse learning larger network winnow madaline winnow madaline trained randomly generated instances dimensions dimensions trained randomly generated instances dimensions dimensions expect greater possibility confusion nodes update 
seen best case graph see erratic behavior learning winnow algorithm properly learn target function irrelevant dimensions 
madaline algorithm holds promise maintains relatively low error rate dimension mark begins quickly degenerate predictive ability 
striking differences seen examining graphs learning runs training instances 
noting error scale figures previous figures graph readable see average case winnow behavior erratic caused way winnow algorithm greatly modifies weights update leading instability resultant weight vector training ceases error rate stays 
madaline shows small linear decrease predictive ability entire graph reflecting target function effectively learned misclassifications arising cumulative sum small irrelevant attribute weights 
shows impressive results 
madaline slightly higher error rate madaline 
impressively winnow algorithm able maintain error entire range irrelevant attributes reflecting network size entirely crucial effectively learning paradigm 
examination weights larger network indicated fact nodes hidden layer contained appropriate hyperplanes required learn target function nodes somewhat random essentially unused weights terms instance classification 
important note fixed threshold winnow algorithm dependent number irrelevant attributes instance vectors 
reflects problem inherent winnow algorithm threshold choice large impact learning shortcoming madaline style architecture 
great deal needs done examining extending ltu tree madaline style learning algorithms 
terms ltu tree new methods finding better separating hyperplanes incorporation post learning pruning techniques helpful determining proper network size madaline style standard neural networks 
madaline style networks clearly needs done examining larger networks learning complex functions 
interesting problem arises looking methods prune network training produce better classifications 
theoretical measures needed number training instances adequate learning 
acknowledgments author grateful prof nils nilsson ideas guidance help support done 
additional go prof nilsson reading commenting earlier draft 
dr pat langley provided sounding board ideas extending research dealing ltu trees 
brent 
fast training algorithms multi layer neural nets 
numerical analysis project manuscript na dept computer science stanford univ brodley utgoff 
multivariate versus univariate decision trees 
coins technical report dept computer science univ mass duda hart 
pattern classification scene analysis 
new york john wiley sons 
langley 
induction recursive bayesian classifiers 
forthcoming 
littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
littlestone 
redundant noisy attributes attribute errors linear threshold learning winnow 
proceedings fourth annual workshop computational learning theory 
san mateo ca morgan kaufmann publishers nilsson 
learning machines 
new york mcgraw hill 
quinlan 
induction decision trees 
machine learning 

adaptive logic system generalizing properties 
stanford electronics laboratories technical report prepared air force contract af stanford univ rumelhart hinton williams 
learning internal representations error propagation 
parallel distributed processing vol 
eds 
rumelhart mcclelland 
cambridge ma mit press 
rumelhart mcclelland eds 

parallel distributed processing vol 

cambridge ma mit press 
sahami 
experimental study learning non linearly separable boolean functions trees linear threshold units 
forthcoming 
utgoff 
perceptron trees case study hybrid concept representation 
aaai proceedings seventh national conference artificial intelligence 
san mateo ca morgan kaufmann 
widrow winter 
neural nets adaptive filtering adaptive pattern recognition 
ieee computer march 
winston 
artificial intelligence third edition 
reading ma addison wesley 
