regularization theory neural networks architectures federico girosi michael jones tomaso poggio center biological computational learning artificial intelligence laboratory massachusetts institute technology cambridge ma usa previously shown regularization principles lead approximation schemes equivalent networks layer hidden units called regularization networks 
particular standard smoothness functionals lead subclass regularization networks known radial basis functions approximation schemes 
shows regularization networks encompass broader range approximation schemes including popular general additive models neural networks 
particular introduce new classes smoothness functionals lead different classes basis functions 
additive splines tensor product splines obtained appropriate classes smoothness functionals 
furthermore generalization extends radial basis functions rbf hyper basis functions hbf leads additive models ridge approximation models containing special cases breiman hinge functions forms projection pursuit regression types neural networks 
propose term generalized regularization networks broad class approximation schemes follow extension regularization 
probabilistic interpretation regularization different classes basis functions correspond different classes prior probabilities approximating function spaces different types smoothness assumptions 
summary different multilayer networks hidden layer collectively call generalized regularization networks correspond different classes priors associated smoothness functionals classical regularization principle 
broad classes radial basis functions generalized hyper basis functions tensor product splines additive splines generalized schemes type ridge approximation hinge functions perceptron neural networks hidden layer 
appear neural computation vol 
pages 
earlier version appeared mit ai memo 
years argued task learning examples considered cases equivalent multivariate function approximation problem approximating smooth function sparse data examples 
interpretation approximation scheme terms networks vice versa extensively discussed barron barron poggio girosi girosi broomhead lowe moody darken white ripley omohundro kohonen lapedes farber rumelhart hinton williams hertz krogh palmer kung sejnowski rosenberg poggio poggio 
series papers explored quite general approach problem function approximation 
approach ill posed problem function approximation sparse data assuming appropriate prior class approximating functions 
regularization techniques tikhonov tikhonov arsenin wahba typically impose smoothness constraints approximating set functions 
argued form smoothness necessary allow meaningful generalization approximation type problems poggio girosi 
similar argument see section case classification smoothness condition classification boundaries input output mapping 
regularization follows classical technique introduced tikhonov identifies approximating function minimizer cost functional includes error term smoothness functional usually called stabilizer 
bayesian interpretation regularization see kimeldorf wahba wahba poggio torre mitter poggio poggio torre koch stabilizer corresponds smoothness prior error term model noise data usually gaussian additive 
poggio girosi girosi showed regularization principles lead approximation schemes equivalent networks hidden layer call regularization networks rn 
particular described certain class radial stabilizers associated priors equivalent bayesian formulation lead subclass regularization networks known radial basis functions powell franke micchelli nelson dyn hardy buhmann lancaster broomhead lowe moody darken poggio girosi girosi 
regularization networks radial stabilizers studied include classical dimensional schumaker de boor multidimensional splines approximation techniques radial non radial gaussian thin plate splines grimson cox eubank multiquadric functions hardy 
poggio girosi extended class networks hyper basis functions hbf 
show extension regularization networks propose call generalized regularization networks grn encompasses broader range approximation schemes including addition hbf tensor product splines general additive models neural networks 
expected grn approximation properties type shown neural networks girosi poggio cybenko hornik stinchcombe white white irie miyake barron jones micchelli 
plan follows 
discuss solution variational problem regularization 
introduce different classes stabilizers corresponding priors equivalent bayesian interpretation lead different classes basis functions known radial stabilizers tensor product stabilizers new additive stabilizers underlie additive splines different types 
possible show argument extends radial basis functions hyper basis functions leads additive models ridge approximation schemes defined delta appropriate dimensional functions 
special cases ridge approximation breiman hinge functions projection pursuit regression ppr friedman huber diaconis freedman donoho johnstone moody multilayer perceptrons lapedes farber rumelhart hinton williams hertz krogh palmer kung sejnowski rosenberg 
simple numerical experiments described illustrate theoretical arguments 
summary chain arguments shows ridge approximation schemes approximations regularization networks appropriate additive stabilizers 
form depends stabilizer includes particular cubic splines typical implementations ppr dimensional gaussians 
neural networks hidden layer gaussian activation function included 
impossible directly derive regularization principles sigmoidal activation functions typically feedforward neural networks 
discuss simple example close relationship basis functions hinge sigmoid gaussian type 
appendices deal observations related main results technical details 
regularization approach approximation problem suppose set theta rg data obtained random sampling function belonging space functions defined presence noise suppose interested recovering function estimate set data problem clearly ill posed infinite number solutions 
order choose particular solution need priori knowledge function reconstructed 
common form priori knowledge consists assuming function smooth sense similar inputs correspond similar outputs 
main idea underlying regularization theory solution ill posed problem obtained variational principle contains data prior smoothness information 
smoothness taken account defining smoothness functional oe way lower values functional correspond smoother functions 
look function simultaneously close data smooth natural choose solution approximation problem function minimizes functional gamma oe positive number usually called regularization parameter 
term enforcing closeness data second smoothness regularization parameter controls tradeoff term chosen cross validation techniques allen wahba wold golub heath wahba craven wahba wahba principle structural risk minimization vapnik 
shown wide class functionals oe solutions minimization functional form 
detailed rigorous derivation solution problem scope simple derivation general result appendix 
section just family smoothness functionals corresponding solutions variational problem 
refer reader current literature mathematical details wahba nelson dyn 
need give precise definition mean smoothness define class suitable smoothness functionals 
refer smoothness measure oscillatory behavior function 
class differentiable functions function said smoother oscillates 
look functions frequency domain may say function smoother energy high frequency smaller bandwidth 
high frequency content function measured high pass filtering function measuring power norm result 
formulas suggests defining smoothness functionals form oe ds indicates fourier transform positive function tends zero ksk high pass filter class functions expression defined empty 
defined class functions nelson dyn dyn functional finite dimensional null space section devoted giving examples possible choices stabilizer oe 
moment just assume written equation additional assumption symmetric fourier transform real symmetric 
case possible show see appendix sketch proof function minimizes functional form gamma ff ff ff ff ff basis dimensional null space functional oe cases set polynomials referred polynomial term equation 
coefficients ff depend data satisfy linear system psi identity matrix defined ij gamma psi ffi ff notice data term equation replaced gamma differentiable function solution variational principle form coefficients anymore solving linear system equations girosi girosi poggio 
existence solution linear system shown guaranteed existence solution variational problem 
case corresponds pure interpolation 
case existence exact solution linear system equations depends properties basis function micchelli 
approximation scheme equation simple interpretation terms network layer hidden units call regularization network rn 
appendix describes extension vector output scheme 
summary argument section shows regularization network form certain class basis functions equivalent minimizing functional 
particular choice equivalent corresponding choice smoothness functional 
dual representation regularization networks consider approximating function form neglecting polynomial term simplicity 
compact notation expression delta vector functions gamma 
coefficients satisfy linear system solution gamma delta rewrite expression delta vector basis functions defined gamma depends data points regularization parameter representation solution approximation problem known dual equation basis functions called equivalent kernels similarity equation kernel smoothing technique define section silverman hastie tibshirani 
equation difficult part computation vector coefficients set basis functions easily built equation difficult part computation basis functions coefficients expansion explicitly notice depends distribution data input space kernels kernels gamma translated replicas kernel 
notice shown appendix dual representation form exists approximation schemes consists linear superpositions arbitrary numbers basis functions long error criterion determine parameters approximation quadratic 
dual representation provides intuitive way looking approximation scheme value approximating function evaluation point explicitly expressed weighted sum values function examples concept new approximation theory example theory quasi interpolation 
case data points fx coincide multi integers set integers number extensively studied literature known schoenberg approximation schoenberg nelson jackson de boor buhmann dyn 
case approximation function sought form gamma fast decaying function linear combination radial basis functions 
approximation scheme linear superposition radial basis functions functions gamma play role equivalent kernels 
quasi interpolation interesting provide approximation need solving complex minimization problems solving large linear systems 
discussion non iterative training algorithms see 
difficult prove rigorously expect kernels decrease distance data points evaluation point neighboring points affect estimate function providing local approximation scheme 
original basis function local multiquadric kxk basis functions bell shaped local functions locality depend choice basis function density data points regularization parameter shows apparently global approximation schemes regarded local memory techniques see equation 
noted techniques highest possible degree locality parameter controls locality regularization parameter kernels 
possible devise local techniques kernel parameter controls locality bottou vapnik vapnik personal communication 
data equally spaced infinite grid expect basis functions translation invariant dual representation convolution filter 
study properties filters case dimensional cubic splines see silverman gives explicit results shape equivalent kernel 
consider simple experiments show shape equivalent kernels specific situations 
considered data set composed equally spaced points domain theta nodes regular grid spacing equal 
multiquadric basis functions oe kxk oe set 
shows original multiquadric function equivalent kernel case definition gamma ij gamma kernels close border similar data equally spaced translation invariance holds approximately 
consider dimensional example multiquadric basis function oe data set chosen non uniform sampling interval set drawn respectively equivalent kernels definitions 
notice bell shaped original basis function increasing cup shaped function 
notice shape equivalent kernels changes broader moving high low sample density region 
phenomenon shown silverman cubic splines expect appear general cases 
connection regularization theory dual representation clear special case continuous data regularization functional form dx gamma oe function approximated 
functional intuitively seen limit functional number data points goes infinity spacing uniform 
easily seen stabilizer oe form solution regularization functional fourier transform see poggio yuille examples 
solution filtered version original function consistently results silverman form equivalent kernels translates function defined 
notice effect regularization parameter equivalent kernel dirac delta function noise low pass filter 
dual representation illuminating especially interesting case multi output network approximating vector field discussed appendix normalized kernels approximation technique similar radial basis functions called normalized radial basis functions moody darken 
normalized radial basis functions expansion function form ff ff gamma ff ff gamma ff difference equation radial basis functions normalization factor denominator estimate probability distribution data 
discussion relation normalized gaussian basis function networks gaussian mixtures gaussian mixture classifiers tresp ahmad 
rest section show particular version approximation scheme tight connection regularization theory 
joint probability inputs outputs network assume sample pairs randomly drawn goal build estimator network minimizes expected risk gamma done probability unknown usually empirical risk emp gamma minimized 
alternative consists obtaining approximation probability minimizing expected risk 
option chosen regularization approach probability estimation vapnik vapnik vapnik leads known technique parzen windows 
parzen window estimator probability distribution set data fz form nh phi gamma phi appropriate kernel example gaussian norm positive parameter simplicity set 
joint probability expected risk approximated parzen window estimator obtain approximated expression expected risk explicitly minimized 
order show done notice need approximate probability distribution random variable equation 
choose kernel form phi kxk standard dimensional symmetric kernel gaussian 
parzen window estimator kx gamma gamma approximation expected risk obtained kx gamma gamma gamma order find analytical expression minimum impose stationarity constraint ffi ffif leads equation kernel form phi function variables lead obtain choice 
kx gamma gamma gamma ffi gamma performing integral fact obtain kx gamma gamma kx gamma performing change variable integral previous expression fact kernel symmetric conclude function minimizes approximated expected risk kx gamma kx gamma right hand side equation converges number examples goes infinity provided scale factor tends zero appropriate rate 
form approximation known kernel regression nadaraya watson estimator subject extensive study statistics community nadaraya watson rosenblatt priestley chao gasser muller devroye wagner 
similar derivation equation specht equation usually derived different way framework locally weighted regression assuming locally constant model local weight function notice equation form equation centers coincide examples coefficients simply values function data points hand equation estimate linear observations general form equation 
parzen window estimator expression derived framework regularization theory vapnik vapnik vapnik smoothness assumption probability distribution estimated 
means order derive equation smoothness assumption joint probability distribution regression function 
classes stabilizers previous section considered class stabilizers form oe ds seen solution minimization problem form 
section discuss different types stabilizers belonging class corresponding different properties basis functions corresponds different priori assumptions smoothness function approximated 
radial stabilizers commonly stabilizers radial symmetry satisfy equation oe oe rx rotation matrix choice reflects priori assumption variables relevance privileged directions 
rotation invariant stabilizers correspond radial basis function kxk 
attention dedicated case corresponding approximation technique known radial basis functions powell franke micchelli nelson dyn hardy buhmann lancaster broomhead lowe moody darken poggio girosi girosi 
class admissible radial basis functions class conditionally positive definite functions micchelli order shown nelson dyn case functional equation semi norm associated variational problem defined 
radial basis functions derived framework 
explicitly give important examples 
multidimensional splines considered measures smoothness form oe ds ksk case ksk corresponding basis function kxk gammad ln kxk kxk gammad 
case null space oe vector space polynomials degree variables dimension gamma basis functions radial conditionally positive definite represent just particular instances known radial basis functions technique micchelli wahba 
dimensions equation yields called thin plate basis function kxk ln kxk harder grimson 
gaussian stabilizer form oe ds ksk fi fi fixed positive parameter gamma ksk fi basis function gaussian function poggio girosi yuille 
gaussian function positive definite known theory reproducing kernels positive definite functions stewart define norms type 
oe norm null space contains zero element additional null space terms equation needed splines 
disadvantage gaussian appearance scaling parameter fi splines homogeneous functions depend scaling parameter 
possible devise heuristics furnish sub optimal values fi starting points cross validation procedures 
basis functions give list functions basis functions radial basis functions technique associated minimization functional 
table indicate positive definite functions need polynomial term solution conditionally positive definite functions order need polynomial degree solution 
known fact positive definite functions tend zero infinity conditionally positive functions tend infinity 
gammafi gaussian multiquadric inverse multiquadric thin plate splines ln thin plate splines tensor product stabilizers alternative choosing radial function stabilizer tensor product type basis function function form pi th coordinate vector appropriate dimensional function 
positive definite functional oe clearly norm null space empty 
case conditionally positive definite function structure null space complicated consider 
stabilizers equation form oe ds pi leads tensor product basis function pi th coordinate vector fourier transform 
interesting example corresponding choice leads basis function pi gamma jx basis function interesting point view vlsi implementations requires computation norm input vector usually easier compute euclidean norm basis function smooth performance practical cases tested experimentally 
notice approximation needed computing derivatives smoothness appropriate degree clearly necessary requirement see poggio yuille 
notice choice gammas leads gaussian basis function additive stabilizers seen previous section tensor product approximation schemes derived framework regularization theory 
see possible derive class additive approximation schemes framework additive approximation mean approximation form th component input vector dimensional functions defined additive components greek letter indices association components input vectors 
additive models known statistics hastie tibshirani stone wahba buja hastie tibshirani considered generalization linear models 
appealing essentially superposition dimensional functions low complexity share linear models feature effects different variables examined separately 
simplest way obtain approximation scheme choose possible stabilizer corresponds additive basis function certain fixed parameters 
choice lead approximation scheme form additive components form gamma notice additive components independent stage set coefficients postpone discussion point section 
write stabilizers corresponding basis function form fourier transform 
notice fourier transform additive function equation exists generalized sense gelfand involving ffi distribution 
example dimensions obtain ffi ffi interpretation reciprocal expression delicate 
additive basis functions obtained approximate delta functions equation gaussians small variance 
consider example dimensions stabilizer oe ds ffl gamma sy ffl gamma sx ffl corresponds basis function form gammaffl gammaffl limit ffl going zero denominator expression approaches equation basis function approaches basis function sum onedimensional basis functions 
discuss limit process rigorous way 
outline way obtain additive approximations framework regularization theory 
assume know priori function want approximate additive apply regularization approach impose smoothness constraint function single additive component regularization functional form wahba hastie tibshirani gamma ds positive parameters allow impose different degrees smoothness different additive components 
minimizer functional technique described appendix skipping null space terms usual form gamma gamma gamma equation 
notice additive component equation written gamma defined additive components independent parameters fixed 
free parameters coefficients independent additive components 
notice ways outlined deriving additive approximation regularization theory equivalent 
start priori assumptions additivity smoothness class functions approximated 
technique assumptions woven choice stabilizer equation second explicit exploited sequentially 
extensions regularization networks generalized regularization networks section review extensions regularization networks apply radial basis functions additive splines 
fundamental problem practical applications learning pattern recognition choice relevant input variables 
may happen variables relevant variables just totally irrelevant relevant variables linear combinations original ones 
useful original set variables linear transformation wx rectangular matrix 
framework regularization theory taken account making assumption approximating function form wx smooth function smoothness assumption directly smoothness functional oe form 
regularization functional expressed terms gamma oe wx function minimizes functional clearly accordingly results section form gamma plus eventually polynomial 
solution wx wx gamma wx argument rigorous known case classical radial basis functions 
usually matrix unknown estimated examples 
estimating coefficients matrix squares usually idea trying estimate number parameters larger number data points may regularized squares 
proposed moody darken broomhead lowe poggio girosi replace approximation scheme equation similar basic shape approximation scheme retained number basis functions decreased 
resulting approximating function call generalized regularization network grn ff ff wx gamma wt ff centers ff chosen heuristic considered free parameters moody darken poggio girosi 
coefficients ff elements matrix eventually centers ff estimated squares criterion 
elements matrix estimated cross validation allen wahba wold golub heath wahba craven wahba wahba may formally appropriate technique 
special case matrix centers kept fixed resulting technique originally proposed broomhead lowe coefficients satisfy linear equation gc defined vectors matrices ff ff iff wx gamma wt ff technique quite common neural network community advantage retaining form regularization solution complex compute 
complete theoretical analysis results case matrix set identity available sivakumar ward poggio girosi 
sections discuss approximation schemes form cases radial additive basis functions 
extensions radial basis functions case basis function radial approximation scheme equation ff ff kx gamma ff kw defined weighted norm xw wx basis functions equation radial anymore precisely radial metric defined equation 
means level curves basis functions circles ellipses axis need aligned coordinate axis 
notice case important matrix symmetric matrix cholesky decomposition sufficient consider upper triangular 
optimal center locations ff satisfy set nonlinear equations poggio girosi ff ff ff ff ff coefficients depend parameters network necessarily positive 
optimal centers weighted sum example points 
cases may efficient move coefficients ff components ff instance dimensionality inputs high relative number data points 
approximation scheme defined equation discussed detail poggio girosi girosi discuss 
section consider analogue case additive basis functions 
extensions additive splines previous sections seen extension classical regularization technique 
section derive form extension takes applied additive splines 
resulting scheme similar projection pursuit regression friedman huber diaconis freedman donoho johnstone moody 
start classical additive spline derived regularization section gamma scheme smoothing parameters known estimated cross validation 
alternative cross validation consider parameters free parameters estimate square technique coefficients parameters free approximation scheme equation gamma coefficients independent 
course estimate theta coefficients just encounter overfitting problem 
adopt idea section consider approximation scheme form ff ff gamma ff number centers smaller number examples reducing number coefficients estimated 
notice equation written additive component form ff ff gamma ff advantage technique additive components independent dimensional radial basis functions 
argument section introduce linear transformation inputs wx theta matrix 
calling th row performing substitution wx equation obtain ff ff delta gamma ff define dimensional function ff ff gamma ff rewrite approximation scheme equation delta notice similarity equation projection pursuit regression technique schemes unknown function approximated linear superposition dimensional variables projections original variables certain vectors estimated 
projection pursuit regression choice functions left user 
case dimensional radial basis functions example cubic splines gaussians 
choice depends strictly speaking specific prior specific smoothness assumptions user 
interestingly applications projection pursuit regression functions chosen cubic splines choices flexible fourier series rational approximations orthogonal polynomials see moody 
briefly review steps bring classical additive approximation scheme equation projection pursuit regression type approximation 
regularization parameters classical approximation scheme considered free parameters 
number centers chosen smaller number data points 
true relevant variables assumed unknown linear combination original variables notice extreme case additive component just center approximation scheme equation delta gamma basis function gaussian call somewhat improperly network type gaussian multilayer perceptron mlp network threshold function sigmoidal function multilayer perceptron layer hidden units 
sigmoidal functions typically threshold derived directly regularization theory symmetric see section relationship sigmoidal function absolute value function basis function derived regularization 
number computational issues related find parameters approximation scheme equation discuss 
section experimental results describe algorithm obtain 
bayesian interpretation generalized regularization networks known variational principle equation derived context functional analysis tikhonov arsenin probabilistic framework kimeldorf wahba wahba poggio torre koch mitter poggio poggio torre 
section illustrate connection informally addressing related mathematical issues 
suppose set theta rg data obtained random sampling function defined presence noise ffl ffl random independent variables distribution 
interested recovering function estimate set data take probabilistic approach regard function realization random field known prior probability distribution 
define jg conditional probability function examples gjf conditional probability function underlying data probability random sampling function sites fx set measurement fy obtained 
model noise 
priori probability random field embodies priori knowledge function impose constraints model assigning significant probability functions satisfy constraints 
assuming probability distributions gjf known posterior distribution jg computed applying bayes rule jg gjf assumption noise variables equation normally distributed variance oe 
probability gjf written gjf gamma oe gammaf oe variance noise 
model prior probability distribution chosen analogy discrete case function defined finite subset dimensional lattice problem formalized see instance mitter poggio 
prior probability written oe smoothness functional type described section ff positive real number 
form probability distribution gives high probability functions term oe small embodies priori knowledge system 
bayes rule posteriori probability written jg gamma oe gammaf oe simple estimate function probability distribution called map maximum posteriori estimate considers function maximizes posteriori probability jg minimizes exponent equation 
map estimate minimizer functional gamma oe oe ff 
functional equation clear parameter usually called regularization parameter determines trade level noise strength priori assumptions solution controlling compromise degree smoothness solution closeness data 
notice functionals type common statistical physics parisi oe plays role energy functional 
interesting notice case correlation function physical system described oe basis function 
pointed poggio girosi rivest pers 
comm prior probabilities seen measure complexity assigning high complexity functions small probability 
proposed rissanen measure complexity hypothesis terms bit length needed encode 
turns map estimate mentioned closely related minimum description length principle hypothesis described compact way chosen best hypothesis 
similar ideas explored instance solomonoff 
connect data compression coding bayesian inference regularization function approximation learning 
additive splines hinge functions sigmoidal neural nets previous sections shown extend rn schemes called grn include ridge approximation schemes ppr type delta ff ff gamma ff form basis function depends stabilizer list admissible section 
include absolute value jxj corresponding piecewise linear splines function jxj corresponding cubic splines typical implementations ppr gaussian functions 
may natural think sigmoidal multilayer perceptrons may included framework impossible derive directly regularization principles sigmoidal activation functions typically multilayer perceptrons 
section show close relationship basis functions hinge sigmoid gaussian type 
additive splines ramp hinge functions consider dimensional case multidimensional additive approximations consist dimensional terms 
consider approximation lowest possible degree smoothness piecewise linear 
associated basis function jxj shown associated stabilizer oe gamma ds assumption leads approximating dimensional function linear combination appropriate coefficients translates jxj 
easy see linear combination translates jxj appropriate coefficients positive negative equal absolute value yields piecewise linear threshold function oe shown 
linear combinations translates functions approximate dimensional functions 
similar derivative linear combination translates oe functions appropriate coefficients yields gaussian function shown 
linear combinations translates function approximation function 
approximation terms rewritten terms oe turn expressed terms basis function jxj 
notice basis functions jxj underlie hinge technique proposed breiman basis functions oe sigmoidal 
arguments show close relations despite fact jxj strictly legal basis function point view regularization similar smoother gaussian 
notice jxj expressed terms ramp functions jxj gamma layer perceptron activation function oe rewritten terms generalized regularization network basis function jxj 
equivalent kernel effectively local exist sufficient number centers dimension delta 
case projection pursuit regression usual hidden layer perceptrons 
relationships imply may interesting compare basis functions able approximate simple function 
model ff ff ff gamma ff approximate function sin basis functions 
training points test points chosen uniformly 
parameters learned iterative backfitting algorithm friedman hastie tibshirani breiman described section 
looked function learned fitting basis functions 
resulting approximations plotted 
results show performance basis functions fairly close number basis functions increases 
models job approximating sin 
absolute value function slightly worse gaussian function slightly better 
interesting approximation absolute value functions identical approximation sigmoidal function shows absolute value basis functions sum equal sigmoidal piecewise linear function 
numerical illustrations comparing additive non additive models order illustrate ideas provide practical intuition various models numerical experiments comparing performance additive non additive networks dimensional problems 
model consisting sum dimensional gaussians model changed non additive radial basis function network additive network gaussians coordinate axis allows measure performance network changes non additive scheme additive 
different models tested 
differ variances gaussian coordinate axis 
ratio variance variance determines elongation gaussian 
models form written gamma gamma gamma oe oe gamma oe oe models differ values oe oe model oe oe rbf second model oe oe elliptical gaussian third model oe oe additive 
models correspond placing gaussians data point gaussian elongated direction elongated direction 
case rbf elongation model gamma gammax oe gammay oe gamma gammax oe gammay oe oe oe model gamma gammax oe gammay oe gamma gammax oe gammay oe oe oe model gamma gammax oe gamma gammay oe oe model ff ff gamma ff deltax gammat ff model ff ff oe ff delta gamma ff table models tested numerical experiments 
second case elliptical gaussian moderate elongation case additive infinite elongation 
fourth model generalized regularization network model form uses gaussian basis function ff ff gamma ff deltax gammat ff model referred earlier gaussian mlp network equation weight vectors centers coefficients learned 
order see sensitive performances choice basis function repeated experiments model sigmoid basis function derived regularization theory replacing gaussian basis function 
experiments standard sigmoid function oe gammax models summarized table notice model multilayer perceptron standard sense 
models centers fixed learning algorithm equal training examples 
parameters learned coefficients computed solving linear system equations 
fourth fifth model trained fitting basis function time recursive algorithm backfitting friedman hastie tibshirani breiman ffl add new basis function ffl optimize parameters ff ff ff random step algorithm girosi described ffl backfitting basis function ff added far hold parameters functions fixed re optimize parameters function ff ffl repeat backfitting stage significant decrease error 
random step girosi stochastic optimization algorithm simple implement usually finds local minima 
algorithm works follows pick random changes parameter random change lies interval 
add random changes parameter calculate new error output network target values 
error decreases keep changes double length interval picking random changes 
error increases throw changes halve size interval 
length interval threshold reset length interval larger value 
models tested different functions dimensional additive function add sin gamma dimensional gabor function gabor cos training data functions add gabor consisted points picked uniform distribution theta gamma theta gamma respectively 
points randomly chosen serve test data 
results summarized table see girosi jones poggio extensive description results 
expected results show additive model able approximate additive function add better rbf model elliptical gaussian model smooth degradation performance model changes additive radial basis function 
just opposite results seen approximating non additive gabor function gabor shown 
rbf model additive model poor job shown 
shows grn scheme model model model model model add train test gabor train test table summary results numerical experiments 
table entry contains errors training set test set 
model gives fairly approximation learning algorithm finds better directions projecting data axis pure additive model 
notice models considered number parameters equal number data points supposed exactly interpolate data may wonder training errors exactly zero 
reason ill conditioning associated linear system typical problem radial basis functions dyn levin 
hardware biological implementation network architectures seen different network architectures derived regularization making somewhat different assumptions classes functions approximation 
basic common roots tempted argue numerical experiments support claim small differences average performance various architectures see lippmann 
interesting ask architectures easier implement hardware 
schemes number centers examples rbf additive splines expensive terms memory requirements examples simple learning stage 
interesting schemes fewer centers examples linear transformation 
perspectives discussion consider implementation radial vs additive schemes consider different activation functions 
discuss radial vs non radial functions gaussian rbf vs gaussian mlp network 
vlsi implementations main difference computing scalar product distance usually expensive digital analog vlsi 
distance replaced distance sum absolute values computed efficiently 
notice radial basis functions scheme uses norm derived section tensor product stabilizer 
consider different activation functions 
activation functions gaussian sigmoid absolute values equally easy compute especially look table approaches 
analog hardware somewhat simpler generate sigmoid gaussian gaussian shapes synthesized transistors harris personal communication 
practical implementations issues trade offs memory computation chip learning relevant specific chosen architecture 
words general ease implementation possible architectures considered holds clear edge 
point view biological implementations situation somewhat different 
hidden unit mlp networks sigmoidal activation functions plausible albeit simplified model real neurons 
sigmoidal transformation scalar product easier implement terms known biophysical mechanisms gaussian multidimensional euclidean distance 
hand intriguing observe hbf centers tuned cortical neurons behave alike poggio 
particular gaussian hbf unit maximally excited component input exactly matches component center 
unit optimally tuned stimulus value specified center 
units multidimensional centers tuned complex features conjunction simpler features 
description customary description cortical cells optimally tuned complex stimulus 
called place coding simplest universal example tuning cells roughly bell shaped receptive fields peak sensitivities locations input space overlapping cover space 
tuned cortical neurons behave gaussian hbf units sigmoidal units mlp networks tuned response function cortical neurons resembles exp gamma tk oe delta 
stimulus cortical neuron changed optimal value direction neuron response typically decreases 
activity gaussian hbf unit decline change stimulus away optimal value sigmoid unit certain changes away optimal stimulus decrease activity example input multiplied constant ff 
multidimensional gaussian receptive fields synthesized known receptive fields biophysical mechanisms 
simplest answer cells tuned complex features may constructed hierarchy simpler cells tuned incrementally larger conjunctions elementary features 
idea popular immediately formalized terms gaussian radial basis functions multidimensional gaussian function decomposed product lower dimensional gaussians ballard mel poggio girosi 
plausible ways implement gaussian rbf units see poggio girosi poggio particularly simple 
ironically plausible implementations rbf unit may exploit circuits sigmoidal nonlinearities see poggio 
general circuits required various schemes described reasonable biological point view poggio girosi poggio 
example normalized basis function scheme section implemented outlined pool cell activities hidden units output unit shunting inhibition approximating required division operation 
summary remarks large number approximation techniques written multilayer networks hidden layer 
past papers poggio girosi poggio girosi girosi showed derive radial basis functions hyper basis functions types multidimensional splines regularization principles 
regularization yield approximation schemes additive type wahba hastie tibshirani additive splines ridge approximation projection pursuit regression type hinge functions 
show appropriate stabilizers defined justify additive schemes extensions leads rbf hbf leads additive splines ridge function approximation schemes projection pursuit regression type 
generalized regularization networks include depending stabilizer prior knowledge functions want approximate hbf networks ridge approximation tensor products splines perceptron networks hidden layer appropriate activation functions gaussian 
shows diagram relationships 
notice hbf networks ridge approximation networks directly related special case normalized inputs maruyama girosi poggio 
feel common theoretical framework justifies large spectrum approximation schemes terms different smoothness constraints imposed regularization functional solve ill posed problem function approximation sparse data 
claim different networks corresponding approximation schemes derived variational principle gamma oe differ different choices stabilizers oe correspond different assumptions smoothness 
context believe bayesian interpretation main advantages regularization clear different network architectures correspond different prior assumptions smoothness functions approximated 
common framework derived suggests differences various network architectures relatively minor corresponding different smoothness assumptions 
expect architecture best class function defined associated prior stabilizer expectation consistent numerical results see donoho johnstone 
classification smoothness point view regularization task classification regression may represent problem role smoothness obvious 
consider simplicity binary classification output yjx joint probability input output pairs 
average cost associated estimator expected risk see section dxdy gamma problem learning equivalent minimizing expected risk samples joint probability distribution usually solved minimizing empirical risk 
discuss possible approaches problem finding best estimator ffl look estimator class real valued functions known minimizer called regression function dy yp yjx jx real valued network trained empirical risk approximate certain conditions consistency vapnik vapnik chervonenkis conditional probability distribution class jx 
case final estimator real valued order obtain binary estimator apply threshold function final solution turns heaviside function 
ffl look estimator range example form 
case expected risk average number misclassified vectors 
function minimizes expected risk regression function anymore binary approximation 
argue cases sense assume smooth real valued function regularization networks approximate 
argument natural prior constraint classification smoothness classification boundaries impossible effectively generalize correct classification set examples 
furthermore condition usually provides smooth classification boundaries smoothness underlying regressor smooth function usually smooth level crossings 
approaches described suggest impose smoothness approximate regularization network 
complexity approximation problem far discussed approximation techniques point view representation architecture discuss perform approximating functions different functions spaces 
techniques derived different priori smoothness assumptions clearly expect perform optimally priori assumptions satisfied 
difficult compare performances expect technique best different class functions 
measure performances quickly approximation error goes zero number parameters approximation scheme goes infinity general results theory linear nonlinear widths pinkus lorentz devore howard micchelli devore devore yu suggest techniques share limitations 
example approximating times continuously differentiable function variables function parametrized parameters prove best nonlinear parametrization achieve accuracy better jackson type bound gamma 
adjective best sense defined devore howard micchelli nonlinear widths restricts sets nonlinear parametrization optimal parameters depend continuously function approximated 
notice desirable property approximation techniques may results may applicable 
basic intuition class functions intrinsic complexity increases exponentially ratio smoothness index measure amount constraints imposed functions class 
smoothness index kept constant expect number parameters needed order achieve certain accuracy increases exponentially number dimensions irrespectively approximation technique showing phenomenon known curse dimensionality bellman 
clearly consider classes functions smoothness index increases number variables increase rate convergence independent dimensionality obtained increase complexity due larger number variables compensated decrease due stronger smoothness constraint 
order concept clear summarized table number different approximation techniques constraints imposed order approximation error dimension immune curse dimensionality 
notice techniques derived different priori assumptions explicit form constraints different 
example entries table girosi girosi result holds sobolev space functions derivatives order integrable 
notice number derivatives integrable increase dimension order keep rate convergence constant 
similar phenomenon appears entries barron breiman obvious way 
fact shown girosi example space functions considered barron entry breiman entry set functions written respectively kxk gammad kxk gammad function fourier transform integrable stands convolution operator 
notice way apparent space functions constrained dimensions increases due rapid fall terms kxk gammad kxk gammad phenomenon clear results proved rate convergence approximation functions continuous derivatives multilayered feedforward neural networks gamma number continuous derivatives increases linearly dimension curse dimensionality disappears leading rate convergence independent dimension 
important emphasize practice parameters approximation scheme estimated finite amount data vapnik chervonenkis vapnik pollard geman bienenstock doursat haussler baum haussler baum moody 
fact practice minimize empirical risk see equation really minimize expected risk see equation 
introduces additional source error called estimation error usually depends dimension milder way approximation error estimated theory uniform convergence relative probabilities vapnik chervonenkis vapnik pollard 
specific results generalization error combine approximation estimation error obtained barron sigmoidal neural networks niyogi girosi gaussian radial basis functions 
bounds different qualitative behaviour fixed number data points generalization error decreases number parameters increases reaches minimum start increasing revealing known phenomenon overfitting 
general description approximation estimation error combine bound generalization error see niyogi girosi 
additive structure sensory world section address surprising relative success additive schemes ridge approximation type real world applications 
seen ridge approximation schemes depend priors combine additivity dimensional functions usual assumption smoothness 
priors capture fundamental property physical world 
consider example problem object recognition problem motor control 
recognize object small subsets features visual non visual 
perform motor actions different ways 
situations sensory motor worlds redundant 
terms grn means high dimensional centers lower dimensional centers components sufficient perform task 
means high dimensional conjunction replaced components low dimensional conjunctions face may recognized eyebrows mug color 
recognize object may templates comprising features comprising subsets features situations may fully sufficient 
additive small centers limit dimensionality appropriate course associated stabilizers additive type 
splitting recognizable world additive parts may preferable reconstructing full multidimensionality system composed independent additive parts inherently robust simultaneously dependent parts 
small loss uniqueness recognition easily offset gain noise occlusion 
possible meta argument mention sake curiosity 
may argued humans able understand world additive large number necessary examples high dimensionality sensory input image 
may tempted conjecture sensory world biased additive structure 
function space norm approximation scheme ds omega gamma sin delta jones ds omega gamma oe delta barron ds ksk omega gamma jx delta delta breiman ff ff gammat ff girosi ff ff gm kx gamma ff girosi ff ff gamma kx gammat ff oe ff girosi table approximation schemes corresponding functions spaces rate convergence 
function oe standard sigmoidal function function jxj third entry ramp function function gm fifth entry bessel potential fourier transform ksk gamma stein 
sobolev space functions derivatives order integrable 
grateful niyogi friedman moody tresp anonymous referees useful discussions suggestions 
describes research done center biological computational learning department brain cognitive sciences artificial intelligence laboratory mit 
research sponsored office naval research contracts national science foundation contract asc includes funds darpa provided hpcc program 
support laboratory artificial intelligence research provided onr contract 
tomaso poggio supported helen whitaker chair whitaker college massachusetts institute technology 
appendices derivation general form solution regularization problem seen section regularized solution approximation problem function minimizes cost functional form gamma oe smoothness functional oe oe ds term measures distance data desired solution second term measures cost associated deviation smoothness 
wide class functionals oe solutions minimization problem form 
detailed rigorous derivation solution variational principle associated equation outside scope 
simple derivation refer reader current literature mathematical details wahba nelson dyn 
notice depending choice functional oe nonempty null space certain class functions invisible 
cope problem define equivalence relation functions differ element null space oe 
express term terms fourier transform ds ix deltas obtaining functional gamma ds ix deltas ds notice real fourier transform satisfies constraint gammas simplicity notation take constants appear definition fourier transform equal 
functional rewritten gamma ds ix deltas ds gammas order find minimum functional take functional derivatives respect set zero ffih ffi proceed compute functional derivatives second term 
term ffi ffi gamma ds ix deltas gamma ds ffi ffi ix deltas gamma ds ffi gamma ix deltas gamma ix deltat smoothness functional ffi ffi ds gammas ds gammas ffi ffi ds gammas ffi gamma gammat results write equation gamma ix deltat gammat changing gammat multiplying sides equation get gammat gamma ix deltat define coefficients gamma assume symmetric fourier transform real take fourier transform equation obtaining ffi gamma gamma recall defined equivalent functions differing term lies null space oe general solution minimization problem gamma term lies null space oe set polynomials common choices stabilizer oe 
approximation vector fields regularization networks consider problem approximating dimensional vector field set sparse data examples pairs choose generalized regularization network approximation scheme network hidden layer linear output units 
consider case examples centers input dimensionality output dimensionality see 
approximation ff ff gamma ff chosen basis function coefficients ff dimensional vectors ff ff ff ff 
assume simplicity positive definite order avoid need additional polynomial terms previous equation 
equation rewritten matrix notation components output vector denoted superscript greek indices 
cg matrix defined ff ff vector elements ff gamma ff 
assuming simplicity noise data equivalent choosing regularization functional equations coefficients ff imposing interpolation conditions cg introducing notation ff ff ff gamma ff matrix coefficients pseudoinverse penrose albert 
substituting expression equation expression obtained algebraic manipulations expression rewritten functions elements vector depend chosen follows known vector field approximated network linear combination example fields choice regularization network choice positive definite basis function estimated output vector linear combination output example vectors coefficients depend input value 
result valid networks hidden layer linear outputs provided mean square error criterion training 
multiquadric function equivalent kernel multiquadric basis function cases dimensional equally spaced data 
equivalent kernels nonuniform dimensional multiquadric interpolation see text explanation 
absolute value basis function jxj sigmoidal basis function oe gaussian basis function approximation sin basis functions absolute value type sigmoidal type gaussian type 
function approximated 
additive gaussian model approximation model 
grn approximation model 

implementation normalized radial basis function scheme 
pool cell dotted circle activities hidden units divides output network 
division may approximated physiological implementation shunting inhibition 
regularization networks rn rbf additive splines tensor product splines ridge approximation hyperbf regularization radial stabilizer additive stabilizer product stabilizer movable metric movable centers movable metric movable centers movable metric movable centers generalized regularization networks grn classes approximation schemes corresponding network architectures derived regularization appropriate choice smoothness priors associated stabilizers basis functions showing common bayesian roots 


general network hidden layer vector output 
notice approximation dimensional vector field general fewer parameters alternative representation consisting networks dimensional outputs 
free parameters weights hidden layer output simple rbf number examples representations equivalent 
vapnik 
estimation probability density basis method stochastic regularization 
april 
albert 
regression moore penrose pseudoinverse 
academic press new york 
allen 
relationship variable selection data augmentation method prediction 
technometrics 

theory reproducing kernels 
trans 
amer 
math 
soc 
ballard 
cortical connections parallel processing structure function 
behavioral brain sciences 
barron barron statistical learning networks unifying view 
symposium interface statistics computing science reston virginia april 
barron 
approximation estimation bounds artificial neural networks 
technical report department statistics university illinois urbanachampaign champaign il march 
barron 
universal approximation bounds superpositions sigmoidal function 
ieee transaction information theory may 
barron 
approximation estimation bounds artificial neural networks 
machine learning 
baum 
capabilities multilayer perceptrons 
complexity 
baum haussler 
size net gives valid generalization 
neural computation 
bellman 
adaptive control processes 
princeton university press princeton nj 

regularization methods linear inverse problems 
editor inverse problems 
springer verlag berlin 
poggio torre 
ill posed problems early vision 
proceedings ieee 
bottou vapnik 
local learning algorithms 
neural computation november 
breiman 
hinging hyperplanes regression classification function approximation 
ieee transaction information theory may 
broomhead lowe 
multivariable functional interpolation adaptive networks 
complex systems 
buhmann 
multivariate cardinal interpolation radial basis functions 
constructive approximation 
buhmann 
quasi interpolation radial basis functions 
numerical analysis reports na department applied mathematics theoretical physics cambridge england march 
buja hastie tibshirani 
linear smoothers additive models 
annals statistics 
girosi 
nondeterministic minimization algorithm 
memo artificial intelligence laboratory massachusetts institute technology cambridge ma september 
cox 
multivariate smoothing spline functions 
siam numer 
anal 
craven wahba 
smoothing noisy data spline functions estimating correct degree smoothing method generalized cross validation 
numer 
math 
cybenko 
approximation superposition sigmoidal function 
math 
control systems signals 
de boor 
practical guide splines 
springer verlag new york 
de boor 
quasi interpolants approximation power multivariate splines 
micchelli editors computation curves surfaces pages 
kluwer academic publishers dordrecht netherlands 
devore howard micchelli 
optimal nonlinear approximation 

devore 
degree nonlinear approximation 
chui schumaker ward editors approximation theory vi pages 
academic press new york 
devore yu 
nonlinear widths besov spaces 
chui schumaker ward editors approximation theory vi pages 
academic press new york 
devroye wagner 
distribution free consistency results nonparametric discrimination regression function estimation 
annals statistics 
diaconis freedman 
asymptotics graphical projection pursuit 
annals statistics 
donoho johnstone 
projection approximation duality kernel methods 
annals statistics 

spline minimizing rotation invariant semi norms sobolev spaces 
zeller editors constructive theory functions os variables lecture notes mathematics 
springer verlag berlin 
dyn 
interpolation scattered data radial functions 
chui schumaker editors topics multivariate approximation 
academic press new york 
dyn 
interpolation approximation radial related functions 
chui schumaker ward editors approximation theory vi pages 
academic press new york 
dyn jackson levin ron 
multivariate approximation integer translates basis function 
computer sciences technical report university wisconsin madison november 
dyn levin 
numerical procedures surface fitting scattered data radial functions 
siam sci 
stat 
comput april 
eubank 
spline smoothing nonparametric regression volume statistics textbooks monographs 
marcel dekker basel 
franke 
scattered data interpolation tests method 
math 
comp 
franke 
advances approximation surfaces scattered data 
chui schumaker editors topics multivariate approximation 
academic press new york 
friedman stuetzle 
projection pursuit regression 
journal american statistical association 

approximate realization continuous mappings neural networks 
neural networks 
th 
gasser muller 
estimating regression functions derivatives kernel method 
scand 
journ 
statist 
gelfand 
generalized functions 
vol 
properties operations 
academic press new york 
geman bienenstock doursat 
neural networks bias variance dilemma 
neural computation 
girosi 
models noise robust estimates 
memo artificial intelligence laboratory massachusetts institute technology 
girosi 
extensions radial basis functions applications artificial intelligence 
computers math 
applic 
girosi 
regularization theory radial basis functions networks 
cherkassky friedman wechsler editors statistics neural networks 
theory pattern recognition applications 
springer verlag subseries computer systems sciences 
girosi 
rates convergence approximation translates 
memo artificial intelligence laboratory massachusetts institute technology 
girosi 
rates convergence radial basis functions neural networks 
editor artificial neural networks speech vision pages london 
chapman hall 
girosi jones poggio 
priors stabilizers basis functions regularization radial tensor additive splines 
memo artificial intelligence laboratory massachusetts institute technology 
girosi poggio 
networks best approximation property 
biological cybernetics 
girosi poggio 
extensions theory networks approximation learning outliers negative examples 
lippmann moody touretzky editors advances neural information processings systems san mateo ca 
morgan kaufmann publishers 
golub heath wahba 
generalized cross validation method choosing ridge parameter 
technometrics 
grimson 
computational theory visual surface interpolation 
proceedings royal society london 
harder 
interpolation surface splines 
aircraft 

applied nonparametric regression volume econometric society monographs 
cambridge university press 
hardy 
multiquadric equations topography irregular surfaces 
geophys 
res 
hardy 
theory applications multiquadric method 
computers math 
applic 
hastie tibshirani 
generalized additive models 
statistical science 
hastie tibshirani 
generalized additive models applications 
amer 
statistical assoc 
hastie tibshirani 
generalized additive models volume monographs statistics applied probability 
chapman hall london 
haussler 
decision theoretic generalizations pac model neural net learning applications 
technical report ucsc crl university california santa cruz 
hertz krogh palmer 
theory neural computation 
addison wesley redwood city ca 
hornik stinchcombe white 
multilayer feedforward networks universal approximators 
neural networks 
huber 
projection pursuit 
annals statistics 
poggio 
color algorithm examples 
science 
irie miyake 
capabilities layered perceptrons 
ieee international conference neural networks 
jackson 
radial basis functions methods multivariate approximation 
ph thesis university cambridge 
jones 
simple lemma greedy approximation hilbert space convergence rates projection pursuit regression neural network training 
annals statistics march 

scattered data approximation scheme applications computational fluid dynamics computers math 
applic 

scattered data approximation scheme applications computational fluid dynamics ii 
computers math 
applic 
kimeldorf wahba 
correspondence estimation stochastic processes smoothing splines 
ann 
math 
statist 
kohonen 
self organizing map 
proceedings ieee 
kung 
digital neural networks 
prentice hall englewood cliffs new jersey 
lancaster 
curve surface fitting 
academic press london 
lapedes farber 
neural nets 
dana anderson editor neural information processing systems pages 
am 
inst 
physics ny 
proceedings denver conference 
lippmann 
review neural networks speech recognition 
neural computation 
lippmann lee 
critical overview neural network pattern classifiers 
neural networks computing conference snowbird ut 
lorentz 
metric entropy widths superposition functions 
amer 
math 
monthly 
lorentz 
approximation functions 
chelsea publishing new york 
nelson 
multivariate interpolation conditionally positive definite functions 
ii 
mathematics computation january 
nelson 
cardinal splines minimization property 
journal approximation theory 
mitter poggio 
probabilistic solution ill posed problems computational vision 
amer 
stat 
assoc 
maruyama girosi poggio 
connection hbf mlp 
memo artificial intelligence laboratory massachusetts institute technology 

multivariate interpolation arbitrary points simple 
appl 
math 
phys 
mel 
murphy robot learns doing 
anderson editor neural information processing systems 
american institute physics university colorado denver 
mel 
sigma pi column model associative learning cerebral neocortex 
technical report california institute technology 
mel 
pattern discrimination modeled cortical neuron 
neural computation 

approximation properties multilayered feedforward artificial neural network 
advances computational mathematics 

neural networks localized approximation real functions 
kamm editor neural networks signal processing iii proceedings ieee sp workshop pages new york 
ieee signal processing society 
micchelli 
approximation superposition sigmoidal function 
advances applied mathematics 
micchelli 
choose activation function 
hanson cowan giles editors advances neural information processing systems 
san mateo ca morgan kaufmann publishers 
micchelli 
interpolation scattered data distance matrices conditionally positive definite functions 
constructive approximation 
moody 
note generalization regularization architecture selection nonlinear learning systems 
proceedings ieee sp workshop neural networks signal processing pages los alamitos ca 
ieee computer society press 
moody 
effective number parameters analysis generalization regularization nonlinear learning systems 
moody hanson lippmann editors advances neural information processings systems pages palo alto ca 
morgan kaufmann publishers 
moody darken 
learning localized receptive fields 
hinton sejnowski editors proceedings connectionist models summer school pages palo alto 
moody darken 
fast learning networks locally tuned processing units 
neural computation 
moody 
networks learned unit response functions 
moody hanson lippmann editors advances neural information processings systems pages palo alto ca 
morgan kaufmann publishers 

methods solving incorrectly posed problems 
springer verlag berlin 
nadaraya 
estimating regression 
theor 
prob 
appl 
niyogi girosi 
relationship generalization error hypothesis complexity sample complexity radial basis functions 
memo artificial intelligence laboratory massachusetts institute technology 
omohundro 
efficient algorithms neural network behaviour 
complex systems 
parisi 
statistical field theory 
addison wesley reading 
parzen 
estimation probability density function mode 
ann 
math 
statis 
penrose 
generalized inverse matrices 
proc 
cambridge philos 
soc 
pinkus 
widths approximation theory 
springer verlag new york 
poggio 
optimal nonlinear associative recall 
biological cybernetics 
poggio 
theory brain 
cold spring harbor symposia quantitative biology pages 
cold spring harbor laboratory press 
poggio girosi 
theory networks approximation learning 
memo artificial intelligence laboratory massachusetts institute technology 
poggio girosi 
networks approximation learning 
proceedings ieee september 
poggio girosi 
extension theory networks approximation learning dimensionality reduction clustering 
proceedings image understanding workshop pages pittsburgh pennsylvania september 
morgan kaufmann 
poggio girosi 
regularization algorithms learning equivalent multilayer networks 
science 
poggio 
observation cortical mechanisms object recognition learning 
koch davis editors large scale neuronal theories brain 
press 
poggio torre koch 
computational vision regularization theory 
nature 
poggio voorhees yuille 
regularized solution edge detection 
journal complexity 
pollard 
convergence stochastic processes 
springer verlag berlin 
powell 
radial basis functions multivariable interpolation review 
mason cox editors algorithms approximation 
clarendon press oxford 
powell 
theory radial basis functions approximation 
light editor advances numerical analysis volume ii wavelets subdivision algorithms radial basis functions pages 
oxford university press 
priestley chao 
non parametric function fitting 
journal royal statistical society 

build quasi interpolants 
applications splines 

laurent le schumaker editors curves surfaces pages 
academic press new york 

schoenberg approximation 
computers math 
applic 
ripley 
neural networks related methods classification 
proc 
royal soc 
london press 
rissanen 
modeling shortest data description 
automatica 
rosenblatt 
curve estimates 
ann 
math 
statist 
rumelhart hinton williams 
learning representations back propagating errors 
nature october 
schoenberg 
contributions problem approximation equidistant data analytic functions part problem smoothing graduation class analytic approximation formulae 
quart 
appl 
math 
schoenberg 
cardinal interpolation spline functions 
journal approximation theory 
schumaker 
spline functions basic theory 
john wiley sons new york 
sejnowski rosenberg 
parallel networks learn pronounce english text 
complex systems 
silverman 
spline smoothing equivalent variable kernel method 
annals statistics 
sivakumar ward 
best square fit radial functions multidimensional scattered data 
technical report center approximation theory texas university june 
solomonoff 
complexity induction systems comparison convergence theorems 
ieee transactions information theory 
specht 
general regression neural network 
ieee transactions neural networks 
stein 
singular integrals differentiability properties functions 
princeton princeton university press 
stewart 
positive definite functions generalizations historical survey 
rocky mountain math 
stone 
additive regression nonparametric models 
annals statistics 
tikhonov 
solution incorrectly formulated problems regularization method 
soviet math 
dokl 
tikhonov arsenin 
solutions ill posed problems 
winston washington 

theory approximation functions real variable 
macmillan new york 
tresp ahmad 
network structuring training rulebased knowledge 
hanson cowan giles editors advances neural information processing systems 
san mateo ca morgan kaufmann publishers 

cross validation techniques smoothing spline functions dimensions 
gasser rosenblatt editors smoothing techniques curve estimation pages 
springer verlag heidelberg 
vapnik 
estimation dependences empirical data 
springerverlag berlin 
vapnik chervonenkis 
uniform convergence relative events probabilities 
th 
prob 
applications 
vapnik ya 
chervonenkis 
necessary sufficient conditions uniform convergence averages expected values 
ee 
vapnik ya 
chervonenkis 
necessary sufficient conditions consistency empirical risk minimization method 
pattern recognition image analysis 
vapnik 
nonparametric methods restoring probability densities 

wahba 
smoothing noisy data spline functions 
numer 
math 
wahba 
smoothing ill posed problems 
editor solutions methods integral equations applications pages 
plenum press new york 
wahba 
spline bases regularization generalized cross validation solving approximation problems large quantities noisy data 
ward cheney editors proceedings international conference approximation theory honour george lorenz austin tx january 
academic press 
wahba 
comparison gcv gml choosing smoothing parameter generalized splines smoothing problem 
annals statistics 
wahba 
splines models observational data 
series applied mathematics vol 
siam philadelphia 
wahba wold 
completely automatic french curve 
commun 
statist 
watson 
smooth regression analysis 

white 
learning artificial neural networks statistical perspective 
neural computation 
white 
connectionist nonparametric regression multilayer perceptrons learn arbitrary mappings 
neural networks 
yuille 
motion coherence theory 
proceedings international conference computer vision pages washington december 
ieee computer society press 

weakly differentiable functions sobolev spaces functions bounded variation 
springer verlag new york 
