topic detection tracking pilot study final report james allan jaime carbonell george doddington jonathan yamron yiming yang umass amherst cmu darpa dragon systems cmu topic detection tracking tdt darpa sponsored initiative investigate state art finding new events stream broadcast news stories 
tdt problem consists major tasks segmenting stream data especially recognized speech distinct stories identifying news stories discuss new event occurring news small number sample news stories event finding stories stream 
tdt pilot study ran september october 
primary participants darpa carnegie mellon university dragon systems university massachusetts amherst 
report summarizes findings pilot study 
tdt continues new project involving larger training test corpora active participants broadly defined notion topic pilot study 
individuals participated research reported 
james allan umass brian archibald cmu doug beeferman cmu adam berger cmu ralf brown cmu jaime carbonell cmu ira carp dragon bruce croft umass george doddington darpa larry gillick dragon alex hauptmann cmu john lafferty cmu victor lavrenko umass xin liu cmu steve lowe dragon paul van dragon ron papka umass thomas pierce cmu jay ponte umass mike umass charles wayne darpa jon yamron dragon yiming yang cmu 
overview purpose topic detection tracking tdt pilot study advance accurately measure state art tdt assess technical challenges overcome 
study general tdt task domain explored key technical challenges clarified 
document defines tasks performance measures assess technical capabilities research progress presents results cooperative investigation state art 
appear proceedings darpa broadcast news transcription understanding workshop february 

background tdt study intended explore techniques detecting appearance new topics tracking evolution 
portion study notion topic modified sharpened event meaning unique thing happens point time 
notion event differs broader category events spatial temporal localization specificity 
example eruption mount june th consider event volcanic eruption general considered class events 
events unexpected eruption volcano expected political election 
tdt study assumes multiple sources information example various newswires various news broadcast programs 
information flowing source assumed divided sequence stories may provide information events 
general task identify events discussed stories terms stories discuss 
stories discuss unexpected events course follow event stories expected events precede follow event 
remainder section outlines major tasks study discusses evaluation testbed describes evaluation measures 
section presents approaches study members address problem text segmentation discusses results 
detection task taken similarly described section section presents approaches results tracking task including brief section tracking corpus created speech recognition output 

corpus corpus text transcribed speech developed support tdt study effort 
study corpus spans period july june includes nearly stories half taken reuters newswire half cnn broadcast news transcripts 
transcripts produced journal graphics institute 
stories corpus arranged chronological order structured sgml format available linguistic data consortium ldc 
set target events defined support tdt study effort 
events span spectrum event types include expected unexpected events 
described detail documents provided part tdt corpus 
tdt corpus completely annotated respect events story corpus appropriately flagged target events discussed 
flag values possible story discusses event story doesn discuss event brief story mentions event briefly merely event discussion story event question flag values events available file tdt corpus judgments 

tasks topic detection tracking study concerned detection tracking events 
input process stream stories 
stream may may pre segmented stories events may may known system system may may trained recognize specific events 
leads definition technical tasks addressed tdt study 
tracking known events detection unknown events segmentation news source stories 
segmentation task segmentation task defined task segmenting continuous stream text including transcribed speech constituent stories 
support task story texts study corpus concatenated input segmenter 
concatenated text stream include actual story texts exclude external internal tag information 
segmentation task correctly locate boundaries adjacent stories stories corpus 
detection task detection task characterized lack knowledge event detected 
case may wish retrospectively process corpus stories identify events discussed may wish identify new events occur line stream stories 
alternatives supported detection task 
retrospective event detection retrospective detection task defined task identifying events corpus stories 
events defined association linguistic data consortium telephone market street fax suite ldc ldc upenn edu philadelphia pa usa 
www ldc upenn edu values brief listed reducing size judgment file orders magnitude 
vast majority stories flag values events 
stories task group stories study corpus clusters cluster represents event stories cluster discuss event 
assumed story discusses event 
story may included cluster 
line new event detection line new event detection task defined task identifying new events stream stories 
story processed sequence decision new event discussed story processing story processing subsequent stories 
decision story processed 
story discuss event flagged 
story doesn discuss new events flagged 
tracking task tracking task defined task associating incoming stories events known system 
event defined known association stories discuss event 
target event defined list stories discuss 
tracking task target event successive story classified discusses target event 
support task study corpus divided parts part training set second part test set 
division different event order appropriate training test sets 
stories training set flagged discusses target event flags associated text stories information training system correctly classify target event 
tracking task correctly classify stories test set discuss target event 
primary task parameter number stories define train target event division corpus training test function event value specifically training set particular event particular value stories including th story discusses event 
test set subsequent stories 

evaluation assess tdt application potential calibrate guide tdt technology development tdt task performance evaluated formally set rules reasonable story typically discuss single event case 
addition multifaceted stories overlapping events 
example case tdt study corpus target events stories brief tag event 
story tag events carter bosnia violate 
assumption story discusses event reasonable large majority stories vastly simplifies task evaluation 
tdt tasks 
evaluations numerous conditions questions explored 
ffl performance vary processing different sources types sources 
ffl selection training source type affect performance 
general evaluation terms classical detection theory performance characterized terms different kinds errors misses target event detected false alarms target event falsely detected 
framework different events treated independently system separate outputs target events 

segmentation segmentation task addresses problem automatically dividing text stream topically homogeneous blocks 
motivation capability study arises desire apply event tracking detection technology automatically generated transcriptions broadcast news quality improved considerably years 
newswire typical automatically transcribed audio data contains little information stream broken segmentation done processing possible 
segmentation enabling technology applications tracking new event detection 
nature medium topically homogeneous blocks broadcast speech correspond stories segmenter designed task find story boundaries 
approaches described quite general reason technology suitably tuned applied segmentation problems finding topic breaks non news broadcast formats long text documents 
relatively small varied body previous addressed problem text segmentation 
includes methods semantic word networks vector space techniques information retrieval decision tree induction algorithms 
research segmentation carried tdt study led development new complementary approaches directly methods previous approaches share common rationale motivation 

evaluation segmentation evaluated different ways 
segmentation evaluated directly terms ability correctly locate boundaries stories 
second segmentation evaluated indirectly terms ability support event tracking preserve event tracking performance 
segmentation task tdt study corpus reserved evaluation purposes 
means material training segmentation system come sources tdt study corpus 
nature segmentation task segmentation performed single homogeneous data source 
purpose evaluating segmentation task segmentation performed tdt corpus separate sub streams comprising just reuters stories comprising just cnn stories 
addition segmentation task performed explicit knowledge source text newswire transcribed speech 
direct evaluation segmentation segmentation evaluated directly modification method suggested john lafferty 
ingenious method avoids dealing boundaries explicitly 
measures probability sentences drawn random corpus correctly classified belong story 
tdt study calculation performed words sentences 
error probability split parts probability misclassification due missed boundary probability misclassification due extraneous boundary false alarm 
error probabilities defined gammak ffi hyp delta gamma ffi ref gammak gamma ffi ref gammak gamma ffi hyp delta ffi ref gammak ffi ref summations words corpus ffi ae words story choice critical consideration order produce meaningful sensitive evaluation 
tdt study corpus chosen half average document length words text stream evaluate tdt corpus example 
text segmentation exponential models doug beeferman adam berger john lafferty 
reasons words stories 
debate fewer problems deciding delimit words delimit sentences 
second word suitable unit measurement relatively high variability length sentences 
indirect evaluation segmentation segmentation evaluated indirectly measuring event tracking performance stories defined automatic segmentation means 
segment contribute detection errors proportionate overlaps stories contribute error rates 
details evaluation section tracking chapter 

dragon approach theory dragon approach segmentation treat story instance underlying topic model unbroken text stream unlabeled sequence topics 
model finding story boundaries equivalent finding topic transitions 
certain level abstraction identifying topics text stream similar recognizing speech acoustic stream 
topic block text stream analogous phoneme speech recognition word sentence depending granularity segmentation analogous acoustic frame 
identifying sequence topics unbroken transcript corresponds recognizing phonemes continuous speech stream 
just speech recognition situation subject analysis classic hidden markov model hmm techniques hidden states topics observations words sentences 
concretely suppose topics 
language model associated topic calculate probability sequence words 
addition transition probabilities topics including probability topic transition self loop probability implicitly specifies expected duration topic 
text stream probability attached particular hypothesis sequence segmentation topics way 
transition start state topic accumulating transition probability 

stay topic certain number words sentences current topic accumulate probability language model probability 

transition new topic accumulating transition probability 
go back step 
search best hypothesis corresponding segmentation done standard hmm techniques standard speech recognition tricks thresholding search space gets large example 
implementation details entire tdt corpus set aside evaluation training data segmenter come sources 
source available sites portion journal graphics data period january june 
data restricted cnn shows included tdt corpus stories fewer words removed 
left stories average length words 
global unigram model consisting words built data 
topics segmenter referred background topics constructed automatically clustering news stories training set 
clustering done multi pass means algorithm operates follows 
point clusters 
story determine distance closest cluster measure described distance threshold insert story cluster update statistics 
distance threshold create new cluster 

loop stories consider switching story topic measure 
clusters may vanish additional clusters may need created 
repeat step desired 
distance measure clustering variation symmetric kullback leibler kl metric log log story cluster counts word wn background topic language model built cluster 
simplify task number clusters limited topic modeled unigram statistics 
unigram models just smoothed versions raw unigram models generated clusters 
smoothing model consisted performing absolute discounting followed backoff global unigram model 
unigram models filtered list remove common words 
decoding text done code speech recognizer underlying single node models corresponding topics represented unigram model described 
speech text scored models frame time frame corresponding experiments sentence 
topic topic transition penalties folded single number topic switch penalty imposed topic changed frames sentences 
topic switch penalty tuned produce correct average number words segment stories test set 
parameters tune search beam width set large avoid search errors experiments 
results tdt corpus 
segmentation error metric computed dragon system full tdt corpus 
segmenter produced story boundaries compared actual boundaries test set 
exact matches yielding recall rate precision 
cnn vs reuters 
expect data train segmenter background models taken entirely cnn broadcasts performance segmenter cnn portion tdt corpus significantly better performance reuters portion 
explore dragon ran evaluation separately 
system returned segmentation error worse corpus 
cnn error better 
reuters 
explanation anomaly cnn difficult reuters content segmenter dragon 
example written news tends concise broadcast news typical broadcast fillers introductions greetings sign offs 
case length cnn stories varies widely reuters stories problem segmenter single parameter controlling length 
twa corpus 
closed version corpus contains punctuation marks making possible introduce sentence breaks usual way 
recognized transcriptions course contain punctuation breaks introduced arbitrary points segments way produce approximately number sentences closed case 
closed data segmenter returned segmentation error 
recognized data error 
size numbers suggests problem segmenting broadcasts may harder tdt corpus leads believe 
event interesting calibrate error rates result clean transcription twa corpus 
remarkable simple application hmm techniques segmentation tracking achieves promising results 
represents just achieved approach improvements possible incorporating ideas implementations sites generalizations techniques employed 
particular form story modeling attempts recognize features boundaries umass cmu incorporate systems incorporated dragon framework 
way continues spirit speech recognition analogy multi node story models story modeled sequence nodes example models story start models middle models single topic model 
possible improve topic modeling forms basis segmenter 
methods achieving include bigram models place unigram models topics including trigger model kind employed cmu adaptively training background segmentation 
basic speech inspired language models improved incorporating information retrieval measures informed topic information local context analysis umass 

umass approach content lca segmentation umass developed largely complementary segmentation methods 
method technique local context analysis lca 
lca developed method automatic expansion ad hoc queries information retrieval 
somewhat method local feedback shown effective robust 
segmentation task lca thought association thesaurus return words phrases semantically related query text determined collection wide occurrence similarity original sentence 
sentence run query lca database top concepts returned 
original sentence replaced lca concepts effect sentences originally words common typically lca concepts common 
original lca method derived described 
text indexed sentence level offsets encode positions lca features 
example suppose feature simpson occurs sentence 
index encode positions offset previous occurrence concept 
main idea lca segmenter offsets measure shifts vocabulary time 
original method tested wall street journal simple function offsets heuristic measure surprise seeing particular concept particular sentence 
homogeneous collection wall street journal heuristic conjunction lca expansion worked quite 
tdt corpus stories sources happens stories topic occur close proximity 
tdt corpus consists transcribed speech far topic language wall street journal 
example corpus finds social interaction speakers relate current topic 
difficulties circumvented means exponential length model 
looking total size offset model average segment size 
model determine probability occurrence concept segment previous occurrence 
method robust respect multiple stories topic social noise original method performance improved 
lca method thought content method 
works looking changes content bearing words 
somewhat similar topic models dragon method relevance features cmu method 
strong point lca method length model estimation completely unsupervised 
weakness method current implementation somewhat slow requires database query sentence 
sped considerably standard information retrieval query optimization techniques 
second weakness performance lca expansion currently requires sentence breaks 
modification approach fixed sized window sentences atomic unit expansion 
discourse hmm segmentation second segmentation method uses hidden markov model model marker words words predict topic change 
model consists states sentences segment sentences remainder segment 
lca segmenter relies shifts content hmm segmenter relying words predict segment regard content 
somewhat similar cmu vocabulary features 
model trained segmented data 
unknown word probabilities handled simple smoothing method 
additional features 
addition word probabilities features modeled 
included sentence length implicit word segmenter serial clustering tendency distance previous occurrence 
features measured standard score state probabilities estimated training data 
features yielded slight improvement words 
part reason help place distributions features far normal secondly data points cluster mean 
suggests adaptive binning technique better standardized scores 
order shed light conjecture data points lying standard deviation mean discarded new mean standard deviation computed scores 
admittedly poor modification yielded modest improvement initial standard scores suggests adaptive binning appropriate 
known extent results improve better binning 
advantage hmm implementation fast 
training time approximately minutes tdt training corpus segmentation extremely fast expect hmm small number states 
lca method hmm method word level current implementation works sentence level 
disadvantage hmm method requires segmented training data 
results discussion lca method 
lca segmenter achieves error rate tdt corpus 
new method heuristic nature principled lca concepts likelihood improve performance 
additional improvements lca approach 
difficulty lca method gives query lca night concepts gets back essentially random 
current method fairly robust respect reasonable amount random noise better approach model noise words pass lca 
second approach discourse features 
discussed 
hmm method 
hmm segmenter error rate tdt corpus 
caveat approach may rely similarity training data test data somewhat heavily 
shows simple discourse modeling provide useful information 
method robust explicitly modeling regularities source 
example general tag place names names reporters learn probability segment boundaries relative tags specific names current approach 
obvious question extent hybrid approach improve performance method 
example hmm segmenter sample lca concepts locations distribution peaked lca places sure break 
second reasonable hybrid combine content hmm segmenter dragon simple discourse hmm segmenter 
may possible leverage strengths approaches follows 
lca segmenter works unsupervised manner somewhat slow 
hmm segmenter fast requires training data 
time lca segmenter sample incoming data order provide minute training data faster hmm segmenter order keep distributions date language shifts time 

cmu approach motivation original motivation cmu segmentation research arose context multimedia information retrieval applications 
particular news ondemand video library projects informedia digital libraries project require segmentation video stream accurate useful indexing retrieval browsing summarization 
order find natural breaks video stream important concurrent complementary information text closed captions speech output audio image streams 
cmu approach designed idea various features multiple media sources extracted combined statistical model appropriately weighs evidence decides place segment boundaries 
multimedia relevant features include questions phrase coming appear utterance decoded speech 
sharp change video stream frames 
match current image image near segment boundary 
blank video frames nearby 
significant change frequency profile audio stream utterance 
key ingredients basic approach applied subproblem text segmentation 
content features derived pair language models help gauge large scale changes topic 

lexical features extract information local linguistic discourse structure context 

new machine learning algorithm incrementally selects best lexical features combines information provided language models form unified statistical model 
language models described geared finding changes topic segment boundaries 
component similar spirit dragon unigram language models trained clusters segments umass local context analysis technique 
lexical features complement information making fine grained judgments words correlate positively negatively segment boundaries 
feature selection algorithm automatically learns segment observing segmentation boundaries placed sample training text 
algorithm incrementally constructs increasingly detailed model estimate probability segment boundary placed context 
ingredients described detail 
language models 
cmu approach relative behavior adaptive language model compared static trigram language model line manner 
basic idea adaptive model generally gets better better sees material relevant current topic segment 
topic changes performance adaptive model degrades relative trigram model making predictions content words previous topic 
language models essentially employed speech recognition system cmu entry trec evaluation spoken document information retrieval 
static trigram models cnn experiments reuters experiments 
cnn experiments static trigram model tri gamma gamma vocabulary roughly words trained approximately words half years transcripts various news broadcasts including cnn news excluding journal graphics transcriptions overlap time frame tdt corpus 
reuters experiments trigram model vocabulary words trained approximately words wall street journal data 
models katz backoff scheme smoothing 
method construct adaptive model treat static trigram model default distribution add certain features semantic word classes order form family conditional exponential models 
details model described 
adaptive model improve sees material current topic event segment boundary exist adaptive model suddenly shows dip performance lower assigned probability observed words compared short range model 
conversely adaptive model consistently assigning higher probabilities observed words partition 
lexical features 
simple lexical features intended capture words phrases commonly segment particular domain extract simple linguistic discourse clues boundary near 
example domain cnn broadcast news story reporter giving name location report wolf reporting live white house 
domain reuters newswire hand originates written communication story introduced recording day event occurred texas air national guard fighter jet crashed friday remote area southwest texas 
lexical features enable presence absence particular words surrounding context influence statistical segmenter 
presence word reporting broadcast news domain presence word friday newswire domain indicate segment boundary nearby 
way learning algorithm chooses uses features described briefly section 
feature induction procedure combining evidence language models lexical features statistical framework called feature induction random fields exponential models :10.1.1.43.7345
idea construct model assigns position data stream probability boundary belongs position 
probability distribution incrementally constructed loglinear model weighs different features data 
simplicity assumed features binary questions 
way cast problem determining segment boundaries statistical terms construct probability distribution nog random variable describing presence segment boundary context 
consider distributions linear exponential family ae 
deltaf 
oe prior default distribution presence boundary delta 
linear combination binary features 
real valued feature parameters delta 


delta delta delta fn 
normalization constants 
deltaf 
insure family conditional probability distributions 
judgment merit model relative distribution training terms kullback leibler divergence omega 
nog log chosen empirical distribution sample training events maximum likelihood criterion model selection 
training algorithm choosing parameters minimize divergence improved iterative scaling algorithm 
explains model chosen features fn known features 
possibility greedy algorithm akin growing decision tree models closer form certain neural networks 
brief gain candidate estimated improvement model result adding feature adjusting weight best value 
calculating gain candidate feature largest gain chosen added model model parameters adjusted iterative scaling 
manner exponential model incrementally built informative features 
see details 
results exponential models derived feature induction give probability boundary exists position text 
order segment text probability computed line manner scanning text sequentially 
assumed sentence boundaries identified segment boundaries placed sentences 
segment boundary hypothesized probability exceeds pre specified threshold ff boundary previously placed immediately preceding ffl sentences 
parameters ff ffl chosen portion heldout training data minimize error probability set ff ffl cnn data ff ffl newswire data 
described tdt evaluation plan direct evaluation hypothesized segmentation hyp respect segmentation ref computed probability error ref hyp 
probability randomly chosen pair words distance words apart inconsistently classified segmentations pair lies segment pair spans segment boundary 
cnn segmentation model trained approximately words broadcast news data included tdt corpus broadcast news language models described basis language model features 
total features induced model trained improved iterative scaling algorithm 
selection feature pool candidates takes order minutes training weights takes roughly minutes high workstation 
cross validation done determine best stopping point resulting model smoothed way 
advantages feature tion exponential models versus standard machine learning techniques decision trees procedure quite robust fitting 
resulting feature model evaluated cnn portion tdt corpus error rate 
exact match precision recall respectively 
segmentation model reuters portion tdt corpus built collection approximately words ap newswire data wall street journal articles reuters headline news segments extracted internet 
language models trained words wall street journal data 
lack training data reuters domain general absence strong cue phrases story transitions written domain expected resulting segmentation performance inferior obtained broadcast news happened cmu results 
feature model induced training set evaluated reuters portion tdt corpus resulting error rate 
cmu segmentation research carried tdt project clearly directions extended improved practical 
current going cmu build results develop segmentation algorithms multimedia data making parallel text audio video streams 
cmu approach economy scale language models identical speech recognition systems constructed domain 
improved language models speech recognition expected yield improved performance segmentation 
exponential models resulting feature induction concrete sense handful specific features extracted behavior resulting segmenter understood specific explanations decisions 
model directly assigns probability distribution boundaries confidence decisions easy assign 
challenge preserve strengths integrating complementary strengths dragon umass approaches 

discussion remarkable outcomes tdt study segmentation diversity ideas techniques brought bear problem 
broadly speaking ideas techniques fall classes focus story content focus story structure discourse 
details little similarity approaches 
dragon content tdt cnn reuters cc rec dragon umass cmu table segmentation error rates percentages 
tem models stories instances topics described simple unigram statistics umass approach treats stories collections similar queries information retrieval system approach set words bounded marker words phrases cmu system exploits content discourse features simultaneously training exponential model combine information trigger trigram language model features associated story boundaries 
variety approaches segmentation 
means improvements current task realized combining different ideas variety different tasks addressed selecting approach appropriate strengths 
example cnn task large amount matched training data available cmu feature learning mechanism proved effective reuters task matched training material available dragon content system robust see table 
indirect evaluation segmentation described section shows carefully transcribed broadcast data probably segmented current methods subsequent processing tracking suffer 
remains seen said carefully transcribed data produced closed captioning recognition 
small test done twa corpus indicates may hard problem 
hand tdt segmentation broadcast stream enabling technology subsequent tracking detection processes may prove case methods type developed adequate support technologies 

new event detection event detection problem identifying stories continuous news streams pertain new previously unidentified events 
words detection unsupervised learning task labeled training examples 
detection may consist discovering previously unidentified events accumulated collection retrospective detection flagging onset new events live news feeds incoming intelligence reports line fashion line detection 
forms detection design lack advance knowledge new events access unlabeled historical data contrast set 
tdt study input retrospective detection entire corpus 
required output detection system partition corpus consisting story clusters divide corpus event specific groups system judgment 
cmu umass methods exhibit considerably better performance allowed place stories multiple event groups 
input line detection stream tdt stories chronological order simulating real time incoming news events 
output line detection decision story time story arrives indicating story newly reported event 
confidence score decision required 
scores investigate potential trade offs different types errors misses false alarms applying different thresholds scores shifting decision boundary 
information detect unknown events presents new research challenges 
multiple ways approach problem 
ffl cmu approach retrospective event detection cluster stories bottom fashion lexical similarity proximity time 
cmu approach line detection combines lexical similarity distance declining influence look back window days judging current story determine new old distant current story closest story days window 
ffl umass approach line detection similar extent uses variant single link clustering builds clusters groups related stories represent events 
new stories compared groups older stories 
matching threshold adjusted time recognition event reported time passes 
umass retrospective detection method focuses rapid changes monitoring sudden changes term distribution time 
ffl dragon approach observations term frequencies adaptive language models speech recognition 
prediction accuracy adapted language models drops relative background model novel event hypothesized 

detection evaluation detection task entire tdt study corpus input 
detection performance evaluated stories discuss target events flagged flag story 
stories 
retrospective event detection system output retrospective event detection task clustering information necessary associate stories cluster 
story constrained appear cluster 
information recorded file record story records separated newline characters fields record separated white space 
record fields format cluster story decision score ffl cluster index number range 
indicates cluster event affiliation story 
ffl number stories train system event 
detection task kept output maintain format uniformity different tasks 
ffl story tdt corpus index number range 
indicates story processed 
ffl decision indicates system believes story processed discusses cluster event indicates 
decision story member cluster retained output format maintain format uniformity different tasks 
ffl score real number indicates confident system story processed discusses cluster event 
positive values indicate greater confidence 
performance retrospective detection evaluated measuring stories belonging target events match stories belonging corresponding cluster 
presents problem known clusters corresponds particular target event 
necessary associate target event exactly cluster determine correspondence 
accomplished associating target event cluster best matches 
degree match event cluster defined number stories belong event cluster 
note retrospective detection uses entire tdt corpus stories evaluated manually labeled stories events containing total stories 
total non event flags flagged stories 
stories flagged events 
stories flagged brief flagged events 
line new event detection line new event detection task output new event flag time story discusses new event 
evaluation performed set target events small number type trials presents problem estimating performance 
problem addressed artificially changing corpus multiply number type trials skip skip 
done way ffl corpus processed deleting stories brief event tags 
ffl corpus processed second time deleting story discusses target events 
ffl corpus processed skip gamma times time deleting subsequent story discusses target events skip stories discussing target events skipped 
system output line detection task declaration story processed 
output indicate story discusses new event 
information recorded file ascii format record story records separated newline characters fields record separated white space 
record fields format event story decision score skip ffl event event index number 
event affiliation line detection task event set zero retained output format maintain format uniformity different tasks 
ffl number stories train system event 
detection task identically zero retained output format maintain format uniformity different tasks 
ffl story tdt corpus index number range 
indicates story processed 
ffl decision indicates system believes story discuss event story discusses indicates 
ffl score real number indicates confident system story processed discuss event 
positive values indicate greater confidence 
ffl skip number initial stories skipped target events range 
skip evaluation measures story particular event consideration output detection system decision confidence score 
performance average set test stories evaluate detection system 
evaluation measures reported study rate false alarm rate recall precision measure 
false alarm rates official measures pilot study 
measure way balancing recall precision way equal weight 
general form measure fi fi pr fi fi parameter allowing differential weighting measure commonly optimization criterion binary decision making recall precision considered primary performance measures 
addition optimizing binary decisions objective tdt study ability achieve tradeoff different types performance scores level desired 
decision error trade det curve misses false alarms part evaluation 

cmu approach lack knowledge events event detection essentially discovery problem mining data new patterns new paradigm query free retrieval 
cmu takes approach group average agglomerative text clustering aiming discovery natural patterns news stories concepts lexicon terms time 
approach creates hierarchical tree clusters top layers representing rough division general topics lower ones finer division narrower topics events 
cmu investigated incremental average link clustering method produces single level partition tdt corpus 
incremental clustering story cluster representation cmu uses conventional vector space model story vector dimensions stemmed unique terms corpus elements term weights story 
terms mean words phrases general 
cluster represented prototype vector centroid normalized sum story vectors cluster 
term weighting story vector cmu tested typical term weighting schemes combine story term frequency tf inverse document frequency idf different ways 
implementation cmu uses mechanisms provided smart benchmarking retrieval system developed salton group cornell 
term preprocessing includes removal words stemming term weighting 
ltc option smart notation yielded best clustering results experiments weight term story defined log tf theta idf dk denominator dk norm vector square root squared sum elements vector 
similarity stories defined cosine value corresponding story vectors 
similarly similarity clusters defined cosine value corresponding prototype vectors 
having stories clusters represented vectors incremental clustering straightforward 
consecutive story compute cosine similarity story cluster centroid accumulated set 
similarity score story closest cluster threshold pre selected add story cluster member update prototype vector correspondingly 
add story new cluster set 
repeat corpus done 
algorithm results flat partition tdt corpus 
number clusters partition depends clustering threshold step 
setting threshold value obtained partition clusters yielded optimal result evaluated events labeled humans see section 
group average clustering core part cmu method agglomerative algorithm named group average clustering maximizes average pairwise similarity stories cluster 
algorithm uses vector representation documents clusters produces binary tree story clusters bottom fashion leaf nodes tree single story clusters node centroid proximate lowerlevel clusters root node tree algorithm allowed reach point universal cluster contains sub clusters stories 
gac algorithm quadratic complexity time space envisioned improvements cmu yield sub quadratic space complexity increasing time complexity 
order reduce effective complexity exploit natural temporal groupings events news streams cmu modified form gac clustering 
sort tdt stories chronological order initial partition corpus cluster starts single story 

divide partition cluster series nonoverlapping consecutive buckets size fixed terms number clusters contain 

apply gac bucket combine lower level clusters higher level ones bottom fashion bucket size number clusters reduced factor ae 

remove bucket boundaries assemble gac clusters reserving time order clusters 
resulting cluster series updated partition corpus 

repeat step pre determined number clusters achieved final partition 

periodically say iterations step flatten cluster apply gac internally flattened cluster re clustering 
cmu augmentation cutting pedersen algorithm 
enables stories belonging event initially assigned different buckets re assigned common cluster 
line detection algorithm cmu line detection implemented 
algorithm starts empty set past clusters pre determined values parameters ffl detection threshold minimum score system say current story belongs new event ffl combining threshold minimum similarity score adding story new member existing cluster ffl window size maximum number clusters past aging limit terms days cluster member past 

read story current 
compute similarity story clusters past 
ffl largest similarity value detection threshold announce detection new event announce 
ffl largest similarity value clustering threshold add current story closest cluster update prototype vector cluster correspondingly add current story new cluster past remove oldest cluster past exceeded window size 

repeat step input series 
algorithm similar incremental clustering algorithm retrospective detection section modifications ffl past restricted time window fixed number stories days closest current story referring infinite past 
ffl detection threshold independent cluster combining threshold differentiate new old 

umass approach retrospective detection umass different approaches retrospective event detection approach tdt collection examined words noun phrases occur collection occur separate training collection identified potential triggers clusters 
terms examined see occurrence documents heavily concentrated small range time 
term trigger event 
term trigger event documents containing term time range determined standard deviation daily occurrence handed relevance feedback algorithm query representing event created 
umass applied query collection find documents matched event 
final trimming step removed outlier stories considering concentration stories range days 
second approach bottom agglomerative clustering documents similar cmu 
document similarity accomplished queries created line detection described 
document compared running query document query document averaging resulting belief scores 
document pairs standard deviations away mean comparison score eligible invoke clustering 
provides stopping criterion clustering 
line detection umass algorithm line event detection follows steps 
document extract important features needed build query representation document 

calculate belief threshold document corresponding query running query source document 
belief value upper bound threshold adjusted downward described 

compare new document previous queries 
document exceed threshold existing query flag document containing new event 

document exceeds threshold existing query flag document containing new event 

save document query threshold query set 
type query system inquery belief values range 
umass threshold step belief document query 
tried various values values way worked general lower threshold useful larger set features 
umass applied aging factor thresholds time threshold matching grew higher higher 
meant model idea event reported time passes slowly news longer worth reporting 
umass aging factor important factor achieving results 

dragon approach dragon online retrospective detection systems applications clustering technology train background models segmenter 
described segmentation report technology implementation means clustering algorithm 
online detection dragon followed cmu lead approached online detection task clustering problem stories clustered examined 
interpretation online detection natural application means clustering executes pass algorithm 
procedure story corpus defines initial cluster 
remaining stories corpus processed sequentially distance existing clusters computed 
story inserted closest cluster distance greater threshold case new cluster created 
decision create new cluster equivalent declaring appearance new event 
old distance measure iterations dragon implementation means algorithm produces clusters segmenter expect pass provide credible basis online detection system 
turns case 
fact performance dragon clustering algorithm iteration turns essentially dividing corpus chunks consecutive stories declaring clusters 
problem pass arises due subtle property distance measure log log story cluster counts word wn terms interpretation distance story cluster story inserted second distance cluster moves result incorporating story 
problem arises small clusters merging story cluster distributions denominator log story drag small cluster close distance small threshold 
new cluster created clustering algorithm subsequent stories close distance cluster gets big stories threshold settings point new cluster created cycle begins 
new measure dragon fixed measure online task smoothing cluster distribution distance computation background distribution preventing cluster dragged story distribution 
improvements distance subtracted story cluster distance compensate fact small clusters tend look lot background smoothing decay term introduced cause clusters limited duration time 
term just decay parameter times difference number story represented distribution number midway stories cluster 
new measure form log un decay term smoothed cluster count word wn un background unigram count un tuning online detection system means adjusting decay parameter threshold 
currently tuned test corpus 

results analysis sites obtained results retrospective online detection evaluated various metrics discussed including det curves 
tables list reported results runs various sites 
shows det curves best online runs site 
figures det plots retrospective detection systems 
cmu optimization efforts order optimize results cmu investigating dealing oov terms incremental updating idf time windows declining weighting factors dynamically setting clustering thresholds unsupervised incremental learning 
incremental updating inverted document frequency idf defined idf log term current story number stories sequence story corpus tdt tdt current point number stories constrain term sequence current point terms time constraints line detection cmu tried methods 
method time window stories denoted prior current story 
detection decision correct story comparison story story window score gamma max wk method decaying weighting function adjust influence stories window 
score method modified score gamma max wk cos modification decision rely stories closer current time stories far past 
words smoother way time window uniformly weighted window 
cmu window size optimal decaying weighting function size optimal decay weighting 
relative improvement decaying weights measure fixed window 
umass optimization efforts word trigger approach provided reasonably high precision clusters realized bad recall cluster sizes small 
umass believes recall improved relaxing constraints 
bottom agglomerative approach umass unsurprising result higher dimensionality query representations effective 
term queries noticeably effective term queries way line detection 
case term queries outperformed term queries 
false alarm probability random performance cmu umass umass dragon tdt line detection runs false alarm probability tdt retrospective detection runs averaged events random performance cmu dragon umass cmu umass tdt retrospective detection runs averaged events false alarm probability tdt retrospective detection runs event random performance cmu dragon umass tdt retrospective detection runs event false alarm probability tdt retrospective detection runs duplicates allowed event random performance cmu umass tdt retrospective detection runs duplicates allowed event run recall prec micro avg macro avg cmu incremental cmu gac top level dragon umass umass table retrospective detection results partition required 
official evaluation retrospective detection umass small amount alternate types features phrases experiments preliminary results suggest multiword features helpful 
similar cmu time windows umass time sensitive nature event reporting captured aging belief thresholds 
documents query umass raised threshold incrementally subsequent story processed making difficult stories pass threshold 
aging thresholds provided substantial improvements precision impacting recall noticeably 
note aging helps performance unexpected events disasters hurts performance long running events simpson trial 
dragon optimization directions dragon believes careful research clustering measure produce performance gains system 
fact retrospective evaluation indicates new distance measure better old suggests clustering background topics segmenter revisited segmentation experiments rerun topics new measure 
area 

open issues related issues pertaining event detection addressed pilot tdt study evolve naturally including ffl provide global view information space users navigation tools effective efficient search 
ffl approaches generate cluster hierarchy automatically 
choose right level clusters user attention best fits information need user 
ffl summarize information different degrees granularity corpus level cluster level story level sub story level 
provide query specific summaries 
remove redundant parts maximize information summary ffl better temporal information event detection tracking done 
case line detection example taken simplest approach imposing time window data stream 
ffl improve accuracy line detection introducing limited look ahead 
instance noting stories arriving close time highly related different previous time interval indicator new breaking event 

event tracking tdt event tracking task fundamentally similar standard routing filtering tasks information retrieval ir 
sample instances stories describing event stories provide description event task identify subsequent stories describing event 
event tracking different ir tasks events queries tracked events temporal locality general queries lack 
differences shift nature problem slightly time shift possible solutions significantly 
narrowing scope information filtering encourages modifications existing approaches invites entirely new approaches feasible general setting 
report discusses approaches event tracking research teams cmu university massachusetts dragon systems 

tracking evaluation event treated separately independently 
training system particular target event allowable information includes training set test set event flags target event 
information target event 
evaluation conducted values run recall prec micro avg macro avg cmu gac hierarchy umass dups umass dups table retrospective detection results duplicates allowed 

training count just number tags target event exclude brief tags 
full classification story training set brief may training 
stories test set processed evaluations test data value fixed set test data test set corresponding max 
performance fixed test set varies allow stable comparison performance various values test set values test stories processed chronological order decisions line 
detection decision test story output processing subsequent stories 
decisions may deferred 
event tracking system may adapt test data processed unsupervised mode 
supervised feedback allowed 
evaluating set provides essentially equivalent information 
calculating performance stories tagged brief target event included error tally 
tdt tracking trial story event value total trials 
number derived multiplying number target events average number test stories assumed number number values 
trials percent type true trials 
trial outputs logical detection indication indicating system believes story discusses target event second score indicating confident system decision 
confidence indicator compute detection error trade det misses false alarms 
trials tracking task recorded file ascii format record trial trials separated newline characters fields record separated white space 
record fields format event story decision score ffl event index number range 
indicates target event detected 
ffl number stories train system target event 
ffl story tdt corpus index number range 
indicates story processed 
ffl decision indicates system believes story processed discusses target event indicates 
ffl score real number indicates confident system story processed discusses target event 
positive values indicate greater confidence 
large negative numbers indicate lack confidence large positive number indicate high confidence 
indirect evaluation segmentation segmentation see section evaluated indirectly measuring event tracking performance stories defined automatic segmentation means 
straightforward step procedure 
segment corpus segmentation system test 

event tracker evaluated tdt corpus previously run system corpus 
follow standard event tracking rules exceptions ffl train event tracking system original correctly segmented stories 
ffl evaluation auto segmented story follows training story disjoint 

evaluate event tracker results compare results results original correctly segmented stories 
evaluation complicated fact event flags auto segmented stories 
problem solved creating scores original stories run recall prec micro avg macro avg cmu decay win cmu fixed win dragon umass umass umass umass table line detection results average runs 
computed auto segmented stories 
evaluation performed original stories synthetic scores 
synthetic score original story weighted sum scores overlapping stories weight score words overlapping story contributes score orig overlap ij delta score overlap ij ij number words th auto segmented story overlap th original story 
output record format conventional event tracking task 
decision computed standard way synthetic score 
speech tracking twa crash event addition tdt study corpus additional corpus processed explore tracking task different representations speech including machine recognition speech 
corpus consists cnn recordings spans period twa crash occurred 
corpus contains total stories discuss twa crash 
different representations speech processed closed captioning taken cnn broadcast speech recognition output provided cmu 
transcripts twa corpus accurate representation speech 

umass approach efforts umass attack problem focused similarity information filtering 
reason umass training data positive negative create short query intended represent event tracked 
possible ways solving problem mapping event flags original stories auto segmented stories mapping decisions auto segmented stories original stories 
mapping event flags auto segmented stories represent actual application scenario accurately 
mapping scores chosen facilitate clearer comparison results original stories avoid conceptual mechanical difficulties involved mapping event flags 
training data derive threshold comparison query 
query applied subsequent stories matched query tracked 
umass tried approaches 
simple relevance feedback methods ir 
positive training examples negative training examples handed relevance feedback routine built queries words intended represent event 
query run training set select threshold 
second approach shallow parser extract nouns noun phrases single terms weighted features different ways gave features higher weight occurred frequently training story weighted features number training stories occurred 

cmu approach cmu developed methods tracking events nearest neighbor knn classifier decision tree induction dtree classifier 
knn instance classification method 
training instances positive negative event tracked stored efficiently indexed 
approach proceeds converting story vector arrives comparing past stories 
nearest training vectors measured cosine similarity incoming story vote new story members event 
binary decision set threshold scores vote instance tracked event 
instance vote iff score 
cmu ran variation knn effort find approaches yielded high quality results entire vs false alarm tradeoff spectrum exhibited det curves evaluation section 
alternate approaches nearest neighborhoods necessarily size positive instances event negative computing scores gamma respectively similarity weighted voting 
score linear combination ratio neighborhoods scores 
decision trees decision trees classifiers built principle sequential greedy algorithm step strives maximally reduce system entropy 
decision trees select feature maximal information gain ig root node dividing training data values feature branch finding feature ig jf maximized recursively 
decision trees typically sufficient training instances belonging class 
disadvantage output continuously varying tradeoff scores unable generate meaningful det curves efforts produce det curves highly successful 
dragon approach dragon event tracker adaptation segmenter described detail segmentation report 
discussed segmentation algorithm segmentation topic assignment simultaneously 
general topic labels assigned segmenter drawn set automatically derived background topics useful classification number necessarily correspond categories person find interesting 
supplementing background topic models language model specific event interest allowing segmenter score segments model possible segmenter output notification occurrence event news stream assigns event model label story 
implementation topic models role determining background event model score sufficiently identified 
incarnation segmenter asked identify story boundaries 
job merely score story set background models event model report score difference best background model event model 
threshold applied difference determine story event threshold adjusted tune tracker output characteristics 
example low threshold means story score better event model best background model may fail score specified amount declared instance event 
tuning tends result missing stories event probably generate high number false alarms 
event models built words training stories stopwords removed appropriate run prec cmu knn dragon umass comb umass umass rf table tracking results pooled average events evaluated evaluation 
note recall minus rate 
smoothing 
case order provide accurate smoothing event model take backoff distribution mixture background topic models best approximates unsmoothed event model 
different backoff model event value 
evaluation methodology stories event set aside training purposes 
system ability track events tested stories immediately th training story tdt corpus 
note means test sets event different 
system evaluated varying amounts training data 
system allowed positive training examples takes values 
system permitted train positive stories stories occur corpus prior th story 
note training subset may include stories judged brief particular event stories may knowledge judged brief 
test set collection minus training data 
evaluations may averaged events values 
particularly meaningful average values 
results reported standard tdt evaluation measures detection error tradeoff curves 

evaluation results basic results table lists reported results runs various sites 
report exact evaluation error rates thresholds chosen sites averaged events averaging pooling results evaluated test set 
table shows sites able generate results vary widely error rates 
preferred run umass stories judged count judged brief count part part test set 
comb 
comparing run sites shows dramatic differences rates 
umass dragon runs similar false alarm rates difference difference false alarms event average 
cmu substantially lower rate comes expense larger false alarm rate 
results particularly surprising 
cmu tuned decision points false alarm tradeoff measure attempts balance recall precision values 
umass hand tuned approach average precision numbers 
effect clear numbers umass achieves high precision task cmu attains better balance 
det curves shows det curves sample runs site 
comparison possible run 
single point plotted curve represent specific detection error tradeoff threshold values system chose 
detached point associated cmu decision tree approach result confidence threshold entirely conform decisions tracking 
umass rf run dragon run similar effectiveness 
graph shows values plotted umass run turns quickest approach converge values increases 
fact additional training stories appears harm tradeoff errors 
umass hypothesizes stability result noun phrases features 
dragon event models small values umass rf run performs primarily uses small number features 
shown clear minor variations query formation process result substantial differences effectiveness 
cmu nn run fairly insensitive low false alarm rates rate drops training instances important 
result surprising size neighborhood needed match grows order reduce rate mismatches occur supporting evidence training examples help prevent 
surprising umass run degrade fashion cmu 
cmu decision tree approach results unusual det curve decisions result small number confidence scores 
curve huge jumps right indicates large number stories confidence value threshold hits point stories value get included false alarm rate leaps 
decision tree approach tuned specific point value dragon cmu dtree cmu knn umass table shows changes pooled measure systems varies baseline 
actual effectiveness numbers reported table 
curve leftmost knee false alarm rate goal 
det curves show tradeoffs site selecting threshold decisions 
umass dragon show decision points extreme upper left curves reflecting emphasis precision low false alarms 
cmu hand selected points closer middle graph illustrating goal balancing recall precision 
varying values results single value 
limited presentation simplifies points comparison ignores interesting question number training stories affects performance 
det curves run discussed previous section consider just increase effectiveness system achieves changes 
table shows impact varying effectiveness systems measured pooled values measure balances recall precision 
systems effort optimize values cmu dtree dragon cmu knn umass graph data table showing impact various values pooled measure systems 
false alarm probability tdt tracking runs nt cmu dtree cmu knn dragon umass rf umass rf random performance det curve sample tracking runs site 
runs performed training stories evaluated 
actual effectiveness numbers important changes varying causes 
reason percent changes value reported table reports baseline effectiveness value concerned 
clear decision tree approach extremely sensitive amount training data 
poor effectiveness learns rapidly modest gains training instances 
dragon approach knn approach show consistent gains additional training example dragon approach appears continue benefit learning 
umass approach stands stable training instances learns event representation rapidly gains effectiveness point 

indirect evaluation segmentation shows comparison tracking run done actual tdt stories stories generated segmentation run 
runs done dragon earlier version tracker baseline performance slightly lower 
runs nearly identical segmented corpus noticeably degraded performance false alarm rate rate roughly 
shows modest loss effectiveness range 
indirect evaluation done umass showed similar effect degradation slightly larger consistent false alarm rate runs identical 
sets runs suggest segmentation quality reported section modest impact tracking effectiveness 

speech recognized audio transcripts tracking methods developed discussed worked relatively accurate transcripts newswire stories transcribed cnn reuters respectively 
tracking performed noisier channel automatically recognized audio track broadcast news 
price accuracy 
order investigate questions cmu informedia group provided tdt cnn news stories including close captions speech recognized audio 
speech recognition performed sphinx ii system generates word accuracy raw broadcast news 
low accuracy csr due part quality news audio background interference music noise voices significant number vocabulary words 
false alarm probability combined det plots random performance dragon truth dragon seg det curves early dragon tracking run true story boundaries compared run calculated segment boundaries 
event crash twa flight heavily represented small corpus preliminary investigation 
twa events hand labeled informedia available tdt research groups 
cmu tried knn close captions speech data 
dragon preparation time cmu tried trackers producing summary results measured micro average values 
classifier cc transcript csr output tree knn dragon results indicate drop accuracy perfect transcripts imperfect close captions drop accuracy speech recognized audio 
tracking works conditions error rates 
encouraging speech recognition accuracy broadcast news improve tracking technology improve 
may tunable different expected noise levels input channel 
results quite preliminary especially small data set single event tracking 

analysis tracking task difficult analyze somewhat vaguely stated 
preference specified minimizing misses false alarms difficult sites tune systems appropriately 
det curve shows tradeoff particular tracking algorithm algorithms 
task events average stories tracked average stories tracked 
achieving rate false alarm rate mean correctly tracked incorrectly tracked 
reported table systems falls approximately range just barely 
rate systems achieve meaning average uninteresting stories tracked order get interesting ones 
extent systems successful 
low high recall applications results impressive 
rate tracked false alarms arise assuming training examples 
numbers line typical ir ranked re trieval tasks comparison necessarily obvious 
precision recall quite search system suggests problem corpus simpler basic ir 
works tracking task works creating form model event tracked 
experiments suggest ffl event modeled set single terms weights evidence indicates terms preferable 
smaller sets terms provide higher precision cast wide net bring relevant material 
large sets appear cast wide undifferentiated net bringing relevant stories swamping unrelated material 
ffl better set features noun phrases effective producing high quality results 
umass believes higher quality features comb run gave superior performance 
ffl combining multiple approaches deciding story tracked helpful 
evidence combination applied umass substantially stabilized algorithm handling small numbers training stories 
ffl idea addresses problem small event models 
smooth event model consisting story story query training database stories retrieved smoothing material 
dragon performance improves rapidly training examples dramatically improve behavior system small general dragon believes task requires smoothing algorithm aggressively preserves topic suited information retrieval techniques 
results consistent ir searching results particularly surprising reason 
mean methods helped ir help task example query expansion techniques pseudo relevance feedback may fruitful means addressing problem tracking small set positive stories 
example may appropriate areas explore evidence combination explored briefly umass unsupervised learning interactive tracking supervised learning 
study shown fairly simple techniques achieve high quality results substantial needed reduce errors manageable numbers 
fortunately tdt problem focuses broadcast news arbitrary forms information means hope carefully crafted approaches improve tracking results substantially 

section presents broad drawn topic detection tracking pilot study 
known start tdt pilot study state art effectively efficiently address tdt tasks 
show technologies applied solve large portions problem leave substantial room hope improvement 
success existing approaches implications 
quick efforts yielded results continued concentrated problems yield better results 
second current approaches adequate possible move forward investigate complicated problems suggested tdt handling degraded text automatic speech recognition differences topics events building descriptions events tracked detected 
general 
reporting pattern typical event rapid onset followed gradual decline period ranging week weeks 
events re ignite hall helicopter release atypical events sagas sporadic reporting long periods oj dna 
segmentation 
segmentation tractable task known technologies hmm ir machine learning 
fact certain pilot study began 
segmentation possible methods strengths weaknesses 
suggests yield improvements different ideas merged different kinds segmentation problems addressed 
tracking task shows negligible degradation applied segmented text correct segmentation suggesting automatic segmentation technologies may require little improvement task 
detection 
pure retrospective detection performed quite reliably events oj clustering methods significant differences attributable clustering methods 
permitting overlapping clusters improves performance strict partitions presents evaluation concerns 
online detection performed reliably 
onset events detected different airline disasters confused earlier similar events frequently missed 
basic research needed just tuning incrementally improving existing methods 
intermediate points detection line variable deferral period offer interesting intermediate solutions retrospective immediate detection 
tracking 
tracking basically simpler version classic information retrieval ir filtering task conclude uninteresting solved 
fact lies slightly restricted domain ir deals means domain specific techniques applied ir speech machine learning possibly give better performance expect unrestricted approaches 
tracking typical events accomplished fairly reliably instance documents provided 
events tracked fairly reliably training instances 
different technologies tracking decision trees probabilistic queries language model differentials show remarkably similar performance aggregate substantial differences specific events 
degraded text 
preliminary study cmu dragon indicated tracking automated speech recognition output may prove difficult perfect transcriptions especially small numbers training instances 
results show drop effectiveness suggesting area ripe research 
note tdt study focus centrally csr generated text segmentation detection tracking 

beeferman berger lafferty model lexical attraction repulsion proceedings acl madrid spain 

berger della pietra della pietra maximum entropy approach natural language processing computational linguistics 

bookstein klein detecting content bearing words serial clustering proceedings nineteenth annual international acm sigir conference research development information retrieval pp 


della pietra della pietra lafferty inducing features random fields ieee trans 
pattern analysis machine intelligence april 

croft harper probabilistic models document retrieval relevance information journal documentation 

cutting karger pedersen tukey scatter gather cluster approach browsing large document collections th ann int acm sigir conference research development information retrieval sigir 
tdt study focus mode detection 
effectiveness numbers viewed small sample size test corpus 

hearst multi paragraph segmentation expository text proceedings acl 

feder greene optimal algorithms approximate clustering 
proceedings th annual acm symposium theory computing stoc pp 


katz estimation probabilities sparse data language model component speech recognizer ieee transactions acoustics speech signal processing assp march 

kozima text segmentation similarity words proceedings acl 

litman passonneau combining multiple knowledge sources discourse segmentation proceedings acl 

ponte croft text segmentation topic proceedings european conference research advanced technology libraries pp 


salton automatic text processing transformation analysis retrieval information computer addisonwesley 

van rijsbergen information retrieval nd edition butterworths london 

voorhees implementing agglomerative hierarchic clustering algorithms document retrieval information processing management 

xu croft query expansion local global document analysis proceedings nineteenth annual international acm sigir conference research development information retrieval pp 

