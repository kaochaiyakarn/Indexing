hierarchical models object recognition cortex september riesenhuber tomaso poggio department brain cognitive sciences center biological computational learning artificial intelligence laboratory massachusetts institute technology cambridge ma email max ai mit edu tp ai mit edu address correspondence classical model visual processing cortex hierarchy increasingly sophisticated representations extending natural way model simple complex cells hubel wiesel 
somewhat surprisingly little quantitative modeling done years explore biological feasibility class models explain higher level visual processing object recognition 
describe new hierarchical model accounts complex visual task consistent physiological experiments inferotemporal cortex testable predictions 
model novel max operation inputs certain cortical neurons may general role cortical function 
recognition visual objects fundamental cognitive task performed effortlessly brain countless times day satisfying essential requirements invariance specificity 
face recognition example recognize specific face tolerant changes viewpoint scale illumination expression 
brain performs similar object recognition detection tasks fast 

early studies macaque inferotemporal cortex highest purely visual area ventral visual stream thought key role object recognition reported cells tuned views complex objects face cells discharged strongly view face little objects 
hallmark cells robustness firing stimulus transformations scale position changes 
finding interesting question cells show strongly differing responses similar stimuli different faces activate retinal photoreceptors similar ways showing response constancy scaled translated versions preferred stimulus cause different activation patterns retina 
puzzle similar faced hubel wiesel smaller scale decades earlier recorded simple complex cells cat striate cortex cell types responded strongly oriented bars simple cells exhibited small receptive fields strong phase dependence distinct excitatory inhibitory subfields complex cells larger receptive fields phase dependence 
led hubel wiesel propose model simple cells receptive fields neighboring parts space feed complex cell endowing complex cell phase invariant response 
straightforward highly idealized extension scheme lead way simple cells higher order cells starting neocognitron translation invariant object recognition hierarchical models shape processing visual system subsequently proposed explain transformation invariant cells tuned complex objects arise simple cell inputs 
models quantitatively specified compared specific experimental data 
alternative models translation scale invariant object recognition proposed controlling signal appropriately incoming signals shifter circuit extension modulates neuronal responses gain field models invariant recognition experimental studies indicated macaque area cells show attention controlled shift modulation receptive field space little evidence mechanism perform translation invariant object recognition similar mechanism applies transformations scaling 
basic idea hierarchical model sketched perrett oram invariance transformation just image plane transformations case neocognitron built pooling afferents tuned various transformed versions stimulus 
shown earlier viewpoint invariant object recognition possible pooling mechanism 
gaussian rbf learning network trained individual views rotated axis space complex paperclip objects achieve rotation invariant recognition object 
network resulting view tuned units fed view invariant unit effectively represented prototypes learning network interpolated achieve viewpoint invariance 
quantitative psychophysical physiological evidence hypothesis units tuned full partial views probably created learning process hints view invariant output cases explicitely represented small number individual neurons experiment required monkeys perform object recognition task novel paperclip stimuli monkeys seen 
monkeys required recognize views target rotated depth views large number distractor similar structure trained restricted set views target object 
extensive training set paperclip objects neurons anterior selectively responded object views seen training 
design avoided problems associated previous physiological studies investigating mechanisms underlying view invariant object recognition training monkey recognize novel stimuli monkey visual experience objects faces monkey quite familiar possible estimate degree view invariance derived just object view 
large number distractor objects allowed define view invariance respect distractor objects 
key point able compare response neuron transformed versions preferred stimulus neuron response range similar distractor objects view tuned unit invariance range determined just measuring tuning curve sufficient 
study established fig 
training just object view cells showing degree limited invariance rotation training view consistent model cells exhibit significant invariance translation scale changes object previously scale position 
data put sharp focus quantitative terms question circuitry underlying properties view tuned cells 
original model described build view invariant units specify view tuned units come 
key problem explain terms biologically plausible mechanisms invariance translation scaling obtained just object view arises trade selectivity specific object relative tolerance robustness firing position scale changes 
describe model conforms main anatomical physiological constraints reproduces invariance data described predictions experiments view tuned subpopulation cells 
interestingly model consistent data experiments regarding recognition context presence multiple objects cell receptive field results model simple hierarchical feedforward architecture fig 

structure reflects assumption invariance position scale hand feature specificity hand built separate mechanisms increase feature complexity suitable neuronal transfer function weighted sum afferents coding simpler features template match 
summing differently weighted afferents right way increase invariance 
computational point view pooling mechanism produce robust feature detectors measure presence specific features confused clutter context receptive field 
consider complex cell primary visual cortex preferred stimulus bar certain orientation cell responds phase invariant way lines original complex cell model think complex cells receiving input array simple cells different locations pooling results position invariant response complex cell 
alternative idealized pooling mechanisms linear summation sum equal weights achieve isotropic response nonlinear maximum operation max strongest afferent determines response postsynaptic unit 
cases bar receptive field response model complex cell position invariant 
response level signal similar stimulus afferents preferred feature 
consider case complex stimulus paperclip visual field 
linear summation case complex cell response invariant long stimulus stays cell receptive field response level allow infer bar preferred orientation complex cell receptive field output signal sum afferents 
feature specificity lost 
max case response determined strongly activated afferent signal best match part stimulus afferents preferred feature 
ideal example suggests max mechanism capable providing robust response case recognition clutter multiple stimuli receptive field cf 
note sum response saturating nonlinearities inputs brittle requires case case adjustment parameters depending activity level afferents 
equally critical inability sum mechanism achieve size invariance suppose afferents complex cell cell instance show degree size position invariance 
complex cell stimulated object subsequently increasing sizes increasing number afferents excited stimulus afferents showed overlap space scale consequently excitation complex cell increase stimulus size afferents show size invariance borne simulations simplified layer model 
max mechanism cell response show little variation stimulus size increased cell response determined just best matching afferent 
considerations supported quantitative simulations model described suggest sensible way pooling responses achieve invariance nonlinear max function implicitely scanning see discussion afferents type differ parameter transformation response invariant feature size scale invariance selecting best matching afferents 
note considerations apply case different afferents pooling cell looking different parts space responding different objects different parts object visual field case cells lower visual areas broad shape tuning 
pooling combining afferents mix signals caused different stimuli 
afferents specific respond pattern expects final stages model pooling weighted sum rbf network tuned different viewpoints combined interpolate stored views advantageous 
max mechanisms stages circuitry appear compatible neurophysiological data 
instance reported stimuli brought receptive field neuron neuron response appears dominated stimulus produces higher firing rate isolation cell just expected max operation performed level neuron afferents 
theoretical investigations possible pooling mechanisms complex cells support maximum pooling mechanism sakai tanaka soc 
neurosci 
abs 
additional indirect support max mechanism comes studies simplification procedure complexity reduction determine preferred features cells stimulus components responsible driving cell 
studies commonly find highly nonlinear tuning cells fig 

tuning compatible max response function fig 
blue bars 
note linear model fig 
red bars reproduce strong response change small changes input image 
model view tuned units fig 
types operations scanning template matching combined hierarchical fashion build complex invariant feature detectors small localized simple cell receptive fields bottom layer receive input model retina 
need strict alternation operations connections skip levels hierarchy direct connections model fig 

question remains proposed model achieve response selectivity invariance compatible results physiology 
investigate question looked invariance properties view tuned units model tuned view different randomly selected paperclip experiment shows response model view tuned unit rotation scaling translation preferred view see methods 
unit responds maximally training view response gradually falling stimulus transformed away training view 
experiment determine invariance range comparing response preferred stimulus responses distractors 
invariance range defined range model unit response greater distractor objects 
model shown fig 
shows rotation invariance ffi scale invariance octaves translation invariance ffi visual angle 
averaging units obtain average rotation invariance ffi scale invariance octaves translation invariance ffi units show invariance training view range agreement experimentally observed values 
units example fig 
show tuning pseudo mirror views obtained rotating preferred paperclip ffi depth produces pseudo mirror view object due minimal self occlusion observed experimental neurons simulation experimental data far dealt object recognition settings object isolation rarely case normal object recognition settings 
commonly object recognized situated front background appears objects ignored object recognized successfully 
precisely case multiple objects receptive field responses afferents feeding tuned certain object affected little possible presence clutter objects 
max response function posited pooling mechanism achieve invariance right computational properties perform recognition clutter preferred object strongly activates afferents objects interfere tend activate afferents usually influence response due max response function 
cases occlusions preferred feature wrong afferents higher activation clutter course affect value provided max mechanism reducing quality match final stage strength response 
clear achieve highest robustness clutter receive input cells strongly activated relevant definition object preferred stimulus 
version model described far penultimate layer contained cells corresponding different features turned sufficient achieve invariance properties experiment 
top layer connected afferents robustness clutter expected relatively low 
note order connect subset intermediate feature detectors receives strong input number afferents large achieve desired response specificity 
straightforward solution increase number features 
fixed number different features dictionary features expanded increasing number type afferents individual cells see methods 
feature version model invariance ranges low number afferents comparable experimental ranges connected cells strongly excited preferred stimulus model show average scale invariance octaves rotation invariance ffi translation invariance ffi maximum afferents cell cells rotation invariant average ffi scale invariant octaves translation invariant ffi simulations show model capable performing recognition context displays inputs contain neurons preferred clip distractor clip model able correctly recognize preferred clip cases afferents neuron maximum rate afferents dropping afferents compared original version model units addition second clip interfered activation caused clip cases response clip display containing preferred clip fell response distractor clips 
reduction response stimulus display compared response stronger stimulus experimental studies question object recognition presence background object explored experimentally study monkey discriminate polygonal foreground objects irrespective polygonal background appeared 
recordings neurons showed stimulus background condition neuronal response average reduced quarter response foreground object monkey behavioral performance dropped 
compatible simulations model show unit firing rate strongly affected addition background pattern cases firing rate evoked distractor objects allowing foreground object recognized successfully 
model relies decomposing images features 
fooled confusing scrambled image original 
superficially may tempted guess scrambling image pieces larger features fool model 
simulations see fig 
show case 
reason lies large dictionary filters features practically impossible scramble image way features preserved low number features 
responses model units drop image scrambled progressively finer pieces confirmed physiology experiment aware obtaining prediction model 
discussion briefly outline computational roots hierarchical model described max operation implemented cortical circuits role features invariances model 
key operation computer vision algorithms recognition classification objects scan window image position scale order analyze step subimage instance providing classifier decides subimage represents object interest 
algorithms successful achieving invariance image plane transformations translation scale 
addition brute force scanning strategy eliminates need segment object interest recognition segmentation complex cluttered images routinely achieved byproduct recognition 
computational assumption originally motivated model described max operation may represent cortical equivalent window analysis machine vision scan select input data 
centrally controlled sequential scanning operation mechanism max operation locally automatically selects relevant subset inputs biologically plausible 
basic pervasive operation computational algorithms computer vision search selection subset data 
natural speculate max operation may replicated cortex 
simulations simplified layer version model soft maximum approximations max operation see methods strength nonlinearity adjusted parameter show basic properties preserved structurally robust 
approximation max operation realized neurons 
implemented different biologically plausible hypothesis max operation arises cortical lateral possibly recurrent inhibition neurons cortical layer 
example provided circuit proposed gain control relative motion detection visual system fly feedforward recurrent shunting presynaptic postsynaptic inhibition pool cells 
key elements addition shunting inhibition equivalent operation may provided linear inhibition deactivating receptors nonlinear transformation individual signals due synaptic nonlinearities active membrane properties 
circuit performs gain control operation certain values parameters max operation 
softmax circuits proposed studies account similar cortical functions 
adaptation mechanisms underlying short term depression circuit may capable pseudo sequential search addition selection 
novel claim max operation key mechanism object recognition cortex 
model described including stage view tuned view invariant units purely feedforward hierarchical model 
backprojections known exist cortex playing key role models cortical function needed basic performance probably essential learning stage known top effects including attentional biases visual recognition naturally grafted inhibitory softmax circuits see described earlier 
model recognition specific object invariant range scales positions training single view scale representation features invariant transformations 
view invariance hand requires training views individual features sharing appearance transform differently rotation depending structure specific object 
simulations show model performance specific class paperclip object recognition results similar computer rendered images cars objects 
computational point view class models described regarded hierarchy conjunctions disjunctions 
key aspect model identify disjunction stage build invariances max operation 
conjunction stage complexity features increases disjunction stage invariance 
level layer presence strength individual features relative geometry image matters 
dictionary features stage overcomplete activities units measuring feature strength independently precise location yield unique signature visual pattern cf 
system 
architecture described shows approach consistent available experimental data maps class models natural extension hierarchical models proposed hubel wiesel 
methods basic model parameters 
patterns model retina theta pixels corresponds ffi receptive field size literature reports average receptive field size ffi set pixels ffi filtered layer simple cell receptive fields derivative gaussians zero sum oriented ffi ffi ffi ffi standard deviations pixels steps pixels filter responses rectified dot products image patch falling receptive field output cell preferred stimulus receptive field covers image patch jw delta 
receptive field rf centers densely sample input retina 
cells layer pool cells max response function output cell afferents max orientation pixels visual field dimension scales 
pooling range chosen simplicity invariance properties cells robust different choices pooling ranges cf 

different cells combined higher layers combining cells tuned different features give cells responding activations cells tuned different orientations yield cells responding feature cells bigger receptive fields 
simple version illustrated layer contains features pairs orientations cells looking part space gaussian transfer function centered response cell receiving input cells receptive fields location responding different orientations exp gamma gamma gamma gamma delta yielding total cells layer 
units feed view tuned units principle layers units possible 
version model simulated object specific learning occurs level synapses view tuned cells top 
complete simulations account effect visual experience exact tuning properties cells hierarchy 
testing invariance model units 
view tuned units model generated recording activity units layer feeding paperclip views setting connecting weights center gaussian associated unit resp corresponding activation 
rotation viewpoints ffi ffi tested training view arbitrarily set ffi steps ffi scale stimulus sizes pixels half octave steps step pixels translation independent translations sigma pixels axis steps pixels exploring plane sigma theta pixels 
feature version 
increase robustness clutter model units number features increased previous maximum afferents different orientation looking patch space version described cell received input neighboring units theta arrangement arbitrary orientation giving total different types cells potential inputs view tuned cell simulations top level units sparsely connected subset layer units gain robustness clutter cf 
results 
cells combined afferents receptive fields different locations features certain distance apart scale change separation scale changes pooling level done scale bands roughly half octave width scale space filter standard deviation ranges pixels resp 
spatial pooling range scale band chosen accordingly neighborhoods theta theta theta theta respectively note system performance robust respect pooling ranges simulations neighborhoods twice linear size scale band produced comparable results slight drop recognition overlapping stimuli expected simple way improve scale invariance composite feature detectors layer 
centers cells chosen rfs overlapped half rf size dimension 
principled way learn invariant feature detectors trace rule straightforward connection patterns demonstrate simple model shows tuning properties comparable experiment 
softmax approximation 
simplified layer version model investigated effects approximations max operations recognition performance 
model contained pooling stage strength pooling nonlinearity controlled parameter output cell afferents exp delta jx exp delta jx performs linear summation scaled number afferents max operation 
acknowledgments supported onr darpa nsf atr honda 
supported mit fellowship bioinformatics 
supported helen whitaker chair whitaker college mit 
grateful bulthoff crick desimone koch logothetis miller perrett reynolds sejnowski seung vogels useful comments reading earlier versions manuscript 
analyzing average invariance ranges neurons tanaka permission reproduce fig 

thorpe speed processing human visual system 
nature 
bruce desimone gross visual properties neurons area superior temporal sulcus macaque 


ungerleider human brain 
curr 
op 


hubel wiesel receptive fields binocular interaction functional architecture cat visual cortex 
phys 

hubel wiesel receptive fields functional architecture visual areas cat 


fukushima neocognitron self organizing neural network model mechanism pattern recognition unaffected shift position 
biol 
cyb 

perrett oram neurophysiology shape processing 
img 
vis 
comput 

wallis rolls model invariant object recognition visual system 
prog 


anderson van essen shifter circuits computational strategy dynamic aspects visual processing 
proc 
nat 
acad 
sci 
usa 
olshausen anderson van essen neurobiological model visual attention invariant pattern recognition dynamic routing information 
neurosci 

abbott invariant visual responses attentional gain fields 


riesenhuber dayan neural models part hierarchies 
advances neural information processing systems volume mit press cambridge ma 
moran desimone selective attention gates visual processing cortex 
science 
connor gallant van essen spatial attention effects macaque area 
neurosci 

poggio edelman network learns recognize objects 
nature 
bulthoff edelman psychophysical support dimensional view interpolation theory object recognition 
proc 
nat 
acad 
sci 
usa 
logothetis bulthoff poggio shape representation inferior temporal cortex monkeys 
curr 
biol 

tarr rotating objects recognize case study role viewpoint dependency recognition dimensional objects 

bull 
rev 
booth rolls view invariant representations familiar objects neurons inferior temporal visual cortex 

cortex 
wang tanaka effects shape discrimination training selectivity inferotemporal cells adult monkeys 


logothetis poggio shape representation inferior temporal cortex monkeys 
curr 
biol 

perrett oram harries benson thomas object centred coding heads macaque temporal cortex 
exp brain res 

vogels responses macaque inferior temporal neurons overlapping shapes 

cortex 
sato interactions visual stimuli receptive fields inferior temporal neurons awake monkeys 
exp brain res 

riesenhuber poggio just view invariances inferotemporal cell tuning 
advances neural information processings systems jordan kearns solla editors mit press cambridge ma 
see riesenhuber poggio modeling invariances inferotemporal cell tuning 
ai memo cbcl mit cambridge ma 
wang tanaka functional architecture monkey inferotemporal cortex revealed vivo optical imaging 
neurosci 
res 

logothetis object vision visual awareness 
curr 
op 


riesenhuber poggio cortical models really bound binding problem 
neuron press 
rolls responses single neurons temporal visual cortical areas macaque stimulus receptive field 
exp brain res 

vogels categorization complex visual images rhesus monkeys 
part single cell study 
eur 
neurosci 

rowley baluja kanade neural network face detection 
ieee pami 
sung poggio example learning view human face detection 
ieee pami 
koch ullman shifts selective visual attention underlying neural circuitry 
hum 


abbott varela sen nelson synaptic depression cortical gain control 
science 
grossberg nonlinear neural networks principles mechanisms architectures 

netw 

chance nelson abbott complex cells amplified simple cells 
nature neurosci 

douglas koch martin recurrent excitation neocortical circuits 
science 
poggio ground discrimination relative movement visual system fly ii neural circuitry 
biol 
cyb 

lee itti koch braun attention activates winner take competition visual filters 
nature neurosci 

heeger normalization cell responses cat striate cortex 
vis 
neurosci 

nowlan sejnowski selection model motion processing area mt primates 
neurosci 

mumford computational architecture neocortex 
ii 
role cortico cortical loops 
biol 
cyb 

rao ballard predictive coding visual cortex functional interpretation extra classical receptive field effects 
nature neurosci 

reynolds desimone competitive mechanisms attention macaque areas 
neurosci 

mel combining color shape texture histogramming neurally inspired approach visual object recognition 

comp 

tanaka neuronal selectivities complex object features ventral visual pathway macaque cerebral cortex 


learning invariance transformation sequences 
neural comp 

aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa spike rate distractor id best distractors rotation axis azimuth elevation degrees aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa degrees visual angle target response mean best distractors invariance properties neuron modified logothetis 
shows response single cell anterior training monkey recognize paperclip objects 
cell responded selectively view paperclip showed limited invariance training view rotation depth significant invariance translation size changes monkey seen stimulus position scale training 
shows response cell rotation depth preferred view 
shows cell response distractor objects evoked strongest responses 
lower plots show cell response changes stimulus size asterisk shows size training view position ffi size resp relative mean best distractors 
defining invariance yielding higher response transformed views preferred stimulus distractor objects neurons exhibit average rotation invariance ffi training stimuli rotated sigma ffi depth provide full information monkey invariance obtained single view smaller translation scale invariance order sigma ffi sigma octave training view resp 
personal communication 
view tuned cells max weighted sum simple cells complex cells complex composite cells composite feature cells sketch model 
model hierarchical extension classical paradigm building complex cells simple cells 
consists hierarchy layers linear units notation fukushima performing template matching solid lines non linear operations pooling units performing max operation dashed lines 
non linear max operation selects maximum cell inputs uses drive cell key model properties quite different basically linear summation inputs usually assumed complex cells 
types operations respectively provide pattern specificity invariance translation pooling afferents tuned different positions scale shown pooling afferents tuned different scales 
max expt 
sum illustration highly nonlinear shape tuning properties max mechanism 
experimentally observed responses cells obtained simplification procedure designed determine optimal features responses normalized response preferred stimulus equal 
experiment cell originally responds quite strongly image water bottle leftmost object 
stimulus simplified monochromatic outline increases cell firing object consisting bar supporting ellipse 
object evokes strong response bar ellipse produce response permission 
comparison experiment model 
green bars show responses experimental neuron 
blue red bars show response model neuron tuned stem ellipsoidal base transition preferred stimulus 
model neuron top simplified version model shown fig 
types features position receptive field tuned left right side transition region resp feed units pool max function blue bars sum function red bars 
model neuron connected units response maximal experimental neuron preferred stimulus receptive field 
stimulus size viewing angle response viewing angle translation deg translation deg responses sample model neuron different transformations preferred stimulus 
different panels show neuron response varying stimulus sizes inset shows response distractor objects selected randomly physiology experiments rotation depth translation 
training size theta pixels corresponding ffi visual angle 
shows neuron response pseudo mirror views cf 
text dashed line indicating neuron response best distractor 
number tiles average neuronal responses neurons feature version model scrambled stimuli 
example scrambled stimulus 
images theta pixels created subdividing preferred stimulus neuron resp tiles randomly shuffling tiles create scrambled image 
average response model neurons afferents scrambled stimuli solid blue curve comparison average normalized responses neurons scrambled stimuli scrambled pictures trees reported study dashed green curve 

