blind source separation algorithmic information theory pajunen helsinki university technology laboratory computer information science box fin hut finland fax mail pajunen hut fi keywords algorithmic complexity blind source separation previous approaches blind source separation problem independent component analysis making separated components statistically independent 
new contrast blind source separation natural signals proposed measures algorithmic complexity sources complexity mixing mapping 
assumptions underlying probability distributions sources necessary 
required independent source signals low complexity generally true natural signals 
connection previous approaches shown demonstrating minimum mutual information coincides minimizing complexity special case 
experiment dioecult problem separating correlated signals considered 
complexity minimization method seen give clearly accurate results method utilizing ica 
solutions blind source separation problem independent component analysis ica making separated components statistically independent 
approach principle consider real signals joint density sample vectors taken consideration equivalently samples assumed independent identically distributed random proc 
independence arti cial neural networks workshop ann pp 
february spain 
draft images 
vectors 
restricting assumption realworld signals considered 
approaches ica utilized information theoretic concepts see 
approaches yield identical contrasts 
especially minimization mutual information output vector intuitively appealing mutual information attains minimum value zero independent components 
minimizing mutual information random vector consider minimizing algorithmic complexity separated signals separating mapping minimum description length mdl principle 
information content complete signal separation minimization mutual information obtained special case samples assumed random vectors 
drop virtually assumptions source distributions usually needed ica sign kurtosis non gaussian sources algorithmic complexity consider signals traditional sense 
high algorithmic complexity seen natural measure randomness 
randomness algorithmic complexity kraft inequality see essentially states discrete signals nite alphabet strings complex description essentially shorter string 
natural assume real world signals interest description somewhat shorter signal see 
supported fact lossless compression algorithms exist virtually type natural signal example images speech 
compressed signal decompression algorithm constitutes description signal gives upper bound algorithmic complexity signal 
strings low complexity suggests description length algorithmic complexity powerful contrast measuring non randomness naturalness signal 
kolmogorov complexity provides possible way measuring algorithmic complexity 
de ned follows ku min pu jp xn string pu algorithm universal computer generates string shown symbol complexity ku asymptotically independent roughly approximate kolmogorov complexity simply compressing signal general purpose lossless compression algorithm de ning complexity size compressed signal bits 
variant lempel ziv compression algorithm experiments 
application blind source separation starting assumption real signals low complexity summed complexity separated signals cost function minimized 
approach conjecture complexity sum low complexity signals approximately 
intuitively clear construct algorithm describe describe signals due signals 
mixture source signal larger complexity signals 
provides strong argument highly probable uniqueness separating solution 
approach general ica possible mix independent random variables obtain set independent random variables 
natural signals mixture higher complexity complexity signal light kraft inequality 
complete approach complexity separating mapping included cost function minimized 
bss problem de ned follows having observed mixtures nd mixing mapping total complexity mapping separated signals minimized 
seen application mdl principle blind source separation problem 
promising de nition nonlinear bss problem correct separating solution yield global minimum total complexity mixing mapping signals low complexity 
lies measuring complexity mapping methods minimizing complexity neural network proposed 
class linear mappings explicit measure derived 
connection minimizing mutual information consider linear invertible mixture statistically independent sources 
approach separating sources minimize mutual information gammah 
denoting separated sources wx get gamma log gamma log det gamma log det wj log det wj suoecient minimize gamma log det wj depend assume process drawn discrete random variable shown symbol kolmogorov complexity asymptotically equal entropy gamma log 
suggests minimizing special case minimizing kolmogorov complexity source signals realizations random variables 
fact entropy random variable measure complexity random variable possible construct code average code length arbitrarily close entropy 
interpret sum simply measuring complexity components random variables 
need measure complexity mixing mapping approximate algorithmic complexity coding eigenvectors eigenvalues 
eigenvectors unit norm description length eigenvectors approximated constant assuming xed quantization elements 
remains code ues matrix require pre simply represent eigenvalues bits get coding lenght proportional log code length depends magnitude eigenvalues assumed xed quantization eigenvalues 
approximate description length log log det aj results arbitrarily chosen description accuracy 
minimize suoecient minimize log det aj 
gamma permutation scaling get jk log det wj gamma log det wj showing minimizing algorithmic symbol complexity separated signals mixing matrix asymptotically equivalent minimizing mutual information signals assumed random variables 
bayesian blind source separation way justify complexity consider linear model compute prior prior generally considered uninformative prior invariant parameter changes 
note uniform prior mixing matrix necessarily uninformative 
straightforward computations see appendix get det aj gamma prior 
kolmogorov complexity de ne universal probability strings pu gammak prior gives high probability strings short description assumption natural signals 
bss problem number samples known need normalize complexity giving pu gamma maximize probability pu joint probability model explaining observed mixtures 
equivalently maximize log log log pu gamma log det aj gamma 
changing signs replacing yields gamma log det wj case 
complexity seen universal prior probability natural signals favoring signals short description 
term det aj gamma results directly uninformative prior general prior information speci applications prior information specifying 
experiments test approach elementary case simple low complexity signals linearly mixed single parameter orthogonal mixing matrix 
single parameter algorithmic complexity visualized 
orthogonality mixing matrix yields constant prior matrix excluded cost function 
gure summed algorithmic complexity separated signals plotted function mixing parameter 
local minima represent correct separating solutions seen gure mixtures separated signals shown 
seen complexity gradually decreased approaching separating solution 
separating matrix chosen global minimum complexity 
eps total complexity separated sources function mixture matrix weight vector angle 
complexity quite high close separating solution 
nd algorithmic complexity works real signals face images source signals 
linearly mixed randomly chosen square mixing matrix gamma gamma gamma gamma mixed images shown gure separated images gure 
optimization complexity performed global search 
basis vector row eps eps top mixtures 
bottom separated sources 
mix eps mix eps mix eps mix eps mixture images sim eps sim eps sim eps sim eps separated images complexity minimization eps eps eps eps separated images minimizing mutual information separating matrix randomly updated new value corresponding cost function computed 
new weight resulted lower cost function value weight vector replaced new value 
basis vector iterations performed 
magnitude random update gradually decreased achieve accurate nal results 
approach requires considerable amount computation takes longer compute stochastic gradient linear ica algorithms 
purpose experiment see complexity minimization results better nal accuracy minimizing mutual information 
algorithm chosen comparison known algorithm proposed deltaw gamma similar algorithm derived deltaw gamma obtained bell sejnowski algorithm natural gradient 
suitable choice nonlinearity algorithm coincides maximum likelihood approach 
source images negative kurtosis chose cubic nonlinearity algorithm 
separated images shown fig 

results readily explained observing normalized covariance matrix sources gamma gamma sources clearly independent statistical sense highly correlated 
clear linear ica algorithm achieve separation sources 
algorithm requiring expected perform worse separated signals forced uncorrelated 
discussion separating signals minimizing algorithmic complexity numerous bene ts ica approaches 
assumptions signals mixing model required 
fact strongest assumptions low complexity signals independence quite hold applications 
approach suggests algorithmic complexity de nition independence complexity sum low complexity signals independence signals 
complexity sum increase signals highly dependent 
second algorithmic complexity takes signal consideration time dependencies structure general way 
ica approaches consider density sources part available information sources random vectors 
bene ts approach linear bss requires assumptions signals uses available information eoeciently 
experiments seen natural images nal separation accuracy better achieved ica algorithm 
minimization signal complexity seen unifying concept optimizes natural measure 
linear ica bss algorithms minimization directed contrasts kurtosis cases fail special cases example optimizing kurtosis fails estimated kurtosis signals zero 
justi cation contrasts lies fact strictly independent sources measures suoecient separating sources linear mixtures 
generally computational complexity kolmogorov complexity minimization large 
global optimization approach dioecult compute gradients complexity 
mixing model linear approach competitive compared linear ica algorithms considering computational complexity 
mixture model nonlinear global optimization may necessary 
second computation signal complexity computationally heavy 
simple cases concept algorithmic complexity powerful crude approximation complexity yields results 
practice means compression signals stopped early stage part signal reducing computational load 
possible application speci knowledge replacing algorithmic complexity measure easier compute 
algorithmic complexity contrast demonstrated bene ts ica methods bss problem 
theoretical connection minimum mutual information supports contrast proper generalization ica signals sequences random variables 
expected method applied nonlinear blind source separation nonlinear exploratory projection pursuit 
main problem measure complexity nonlinear mapping 
explicit measure linear mapping computed prior 
computational complexity major obstacle proposed approach 
image experiment completed minutes due crude approximation complexity 
problems local optimization possible properties contrast function important 
compelled simple criteria compute gradient directly minimize cost function better objective minimization 
argued complexity measure natural signal 
author wishes useful remarks discussions 
comon component analysis new concept signal processing vol 
pp 

bell sejnowski ian approach blind separation blind deconvolution neural computation vol 
pp 

xu cheung yang 
amari component analysis informationtheoretic approach mixture density proc 
int 
conf 
neural networks icnn houston texas usa pp 
june 
girolami fyfe independent component analysis unsupervised learning emergent properties proc 
int 
conf 
neural networks icnn houston texas usa pp 
june 
yang amari cichocki back propagation blind separation sources non linear mixtures proc 
int 
conf 
neural networks icnn houston usa pp 
june 

cardoso contrasts source separation 
chapter adaptive unsupervised learning haykin ed published 
rissanen shortest data description automatica vol 
pp 

schwarz dimension model annals statistics vol 
pp 

cover thomas elements information theory 
wiley 
schmidhuber neural nets low kolmogorov complexity high generalization capability neural networks vol 
pp 

kolmogorov approaches quantitative de nition information problems information transmission vol 
pp 

ia formal theory inductive inference part information control vol 
pp 

ziv lempel ia universal algorithm sequential data compression ieee tr 
information theory vol 
pp 

mdl cost function neural networks proc 
int conf neural networks alaska usa 
submitted 
cichocki unbehauen learning algorithm blind separation signals electronics letters vol 
pp 
august 
amari cichocki yang ia new learning algorithm blind signal separation proc 
neural information processing systems nips denver usa mit press november 
yang amari online learning algorithms blind source separation maximum entropy minimum mutual information neural computation vol 
pp 

appendix represent eigenvectors eigenvalues compute prior det det ae gamma log xj oe parameter vector containing eigenvectors eigenvalues 
log xj log gamma log det aj log gamma log need consider eigenvalues get gamma log gamma gamma gamma follows gamma gamma gamma delta gamma gamma gamma delta gamma gamma gamma delta gamma gamma gamma det gamma collecting results get log gamma log gamma log det aj 
