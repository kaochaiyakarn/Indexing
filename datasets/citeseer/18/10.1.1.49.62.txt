independent factor analysis attias sloan center theoretical neurobiology foundation center integrative neuroscience university california san francisco avenue san francisco ca neural computation press introduce independent factor analysis ifa method recovering independent hidden sources observed mixtures 
ifa generalizes unifies ordinary factor analysis fa principal component analysis pca independent component analysis ica handle square noiseless mixing general case number mixtures differs number sources data noisy 
ifa step procedure 
step source densities mixing matrix noise covariance estimated observed data maximum likelihood 
purpose expectation maximization em algorithm performs unsupervised learning associated probabilistic model mixing situation 
source model described mixture gaussians probabilistic calculations performed analytically 
second step sources reconstructed observed data optimal non linear estimator 
variational approximation algorithm derived cases large number sources exact algorithm intractable 
ifa algorithm reduces ordinary fa sources gaussian em algorithm pca zero noise limit 
derive additional em algorithm specifically noiseless ifa 
algorithm shown superior ica learn arbitrary source densities data 
blind separation ifa modeling multi dimensional data highly constrained mixture gaussians tool non linear signal encoding 
statistical modeling blind source separation blind source separation bss problem multi variable data measured sensors 
known data arise source signals mixed linear transformation corrupted noise 
known sources mutually statistically independent 
task obtain source signals 
sources observable known properties mutual statistical independence properties mixing process noise 
absence information proceed blindly recover source signals observed noisy mixtures 
despite signal processing appearance bss problem statistical modeling data 
context wishes describe observed variables generally correlated terms smaller set unobserved variables mutually independent 
simplest description probabilistic linear model ij depends linear combinations constant coefficients ij probabilistic nature dependence modeled additive noise signals general statistician task estimate ij regarded independent causes data sense relation actual physical causes highly non trivial 
bss hand actual causes sensor signals source signals model ij mixing matrix known correct description 
expect linear models analyzed applied extensively years solution bss problem textbook review article 
case 
consider close relation known factor analysis fa model see everitt 
context fa unobserved sources termed common factors usually just factors noise specific factors mixing matrix elements ij factor loadings 
factor loadings noise variances estimated data maximum likelihood exists efficient expectationmaximization algorithm purpose leading optimal estimate factors 
ordinary fa perform bss 
inadequacy stems gaussian model probability density factor 
seemingly technical point turns important consequences implies fa exploits second order statistics observed data perform estimates effect require factors mutually independent merely uncorrelated 
result factors factor loading matrix defined uniquely arbitrary rotation likelihood function rotation invariant factor space 
put context bss true sources mixing matrix distinguished rotation thereof second order statistics 
modern statistical analysis methods projection pursuit friedman stuetzle huber generalized additive models hastie tibshirani non gaussian densities modeled non linear functions gaussian variables resulting models quite restricted suitable solving bss problem 
field bss emergence mid see jutten herault comon jutten herault aimed highly idealized version problem mixing square invertible instantaneous noiseless 
version termed independent component analysis ica comon 
satisfactory solution ica years bell sejnowski cardoso laheld pham pearlmutter parra hyvarinen oja 
contrary fa algorithms ica employ non gaussian models source densities 
consequently likelihood longer rotation invariant maximum likelihood estimate mixing matrix unique appropriately chosen see correct 
mixing realistic situations generally includes noise different numbers sources sensors 
noise level increases performance ica algorithms deteriorates separation quality decreases manifested cross talk noisy outputs 
importantly situations relatively small number sensors sources lump low intensity sources regard effective noise separation focuses high intensity ones 
way accomplish ica methods 
important problem ica determining source density model 
ability learn densities observed data crucial 
existing algorithms usually employ source model fixed limited flexibility 
actual source densities problem known advance model tailored accordingly inaccurate model leads failed separation global maximum likelihood shifts away corresponding correct mixing matrix 
principle flexible parametric density model parameters may estimated maximum likelihood mackay pearlmutter parra 
ica algorithms gradient ascent maximization methods result slow learning density parameters 
novel unsupervised learning algorithm blind separation non square noisy mixtures 
key approach lies new probabilistic generative model termed independent factor model described schematically 
model defined associated arbitrary non gaussian adaptive densities factors 
define independent factor analysis ifa reconstruction unobserved factors observed data performing ifa amounts solving bss problem 
ifa performed steps 
consists learning model parametrized mixing matrix noise covariance source density parameters data 
model analytically tractable maintaining ability describe arbitrary sources source density modeled mixture dimensional gaussians 
enables derive expectation maximization em algorithm performs maximum likelihood estimation parameters source densities included 
due presence noise sources recovered sensor signals approximately 
done second step ifa posterior density sources data 
posterior derive different source estimators provide optimal source reconstructions parameters learned step 
estimators second iteratively non linear satisfies different optimality criterion 
number sources increases step algorithm increasingly computationally expensive 
cases derive approximate algorithm shown quite accurate 
approximation variational approach introduced context feedforward probabilistic networks saul jordan 
ifa algorithm reduces ordinary fa model sources gaussian performs principal component analysis pca zero noise limit 
additional em algorithm derived specifically noiseless ifa 
particular version algorithm termed seesaw composed alternating phases shown schematically phase learns unmixing matrix keeping source densities fixed second phase freezes unmixing matrix learns source densities em 
ability learn source densities data efficient manner seesaw powerful extension bell sejnowski ica algorithm separate mixtures ica fails separate 
ifa generalizes unifies ordinary fa pca ica provides new method modeling multi variable data terms small set independent hidden variables 
furthermore ifa amounts fitting data mixture model adaptive gaussians see bottom right gaussians adapt independently strongly constrained move expand 
deals instantaneous mixing 
real world mixing situations generally instantaneous include propagation delays described mathematically convolutions place matrix multiplication 
significant step solving convolutive bss problem taken attias schreiner obtained family maximum likelihood learning algorithms separating noiseless convolutive mixtures torkkola lee 
derived algorithms information maximization considerations 
algorithms noisy convolutive mixing derived extension methods described 
organized follows 
section introduces model 
em algorithm learning generative model parameters section source reconstruction procedures discussed section 
performance ifa algorithm demonstrated application noisy mixtures signals arbitrary densities section 
factorized variational approximation ifa derived tested section 
em algorithm noiseless ifa seesaw version demonstrated section 
derivations relegated appendices 
notation vectors denoted bold faced lower class letters matrices bold faced upper class letters 
vector matrix elements bold faced 
inverse matrix denoted gamma transposition ij ji 
denote ensemble averaging operator different observations random vector vector function ef multi variable gaussian distribution random vector mean covariance sigma denoted gamma sigma det sigma gamma exp gamma gamma sigma gamma gamma implying ex sigma independent factor generative model independent factor analysis step method 
step concerned unsupervised learning task generative model everitt named independent factor model introduce 
theta observed data vector 
wish explain correlated terms hidden variables referred factors mutually statistically independent 
specifically data modeled dependent linear combinations factors constant coefficients ij additive theta random vector dependence non deterministic hx language bss independent factors unobserved source signals data observed sensor signals 
sources mixed matrix resulting mixtures corrupted noise signals originating sources mixing process propagation medium response sensor responses 
order produce generative model probability density sensor signals specify density sources noise 
model sources independent random variables arbitrary distributions individual th source density parametrized parameter set noise assumed gaussian mean zero covariance matrix allowing correlations sensors note situations sensor noise signals independent correlations may arise due source noise propagation noise 
equations define generative model parametrized source parameters mixing matrix noise covariance denote parameters collectively resulting model sensor density dx dx gamma hx dx dx parameters adapted minimize error function measures distance model observed sensor densities 
source model factorial mixture gaussians principle perfectly viable starting point evaluated numerical integration suitably chosen quite computationally intensive practice 
better strategy choose parametric form sufficiently general model arbitrary source densities ii allows performing integral analytically 
form satisfies requirements mixture gaussians mog model 
shall describe density source mixture gaussians means variances mixing proportions gamma fw runs gaussians source mixture normalized mixing proportions source sum unity 
parametric form provides probabilistic generative description sources different gaussians play role hidden states 
generate source signal pick state probability draw number corresponding gaussian density gamma 
viewed dimensional space joint source density formed product dimensional mog mog 
collective hidden states consist possible combinations individual source states upper right illustrates state corresponds dimensional gaussian density mixing proportions mean diagonal covariance matrix determined constituent source states wq delta delta delta ql ql diag ql wq gamma vq gaussians factorize gamma vq gamma sum collective states represents summing individual source states delta delta delta ql note contrary ordinary mog gaussians free adapt independently strongly constrained 
modifying mean variance single source state result shifting column collective states source model mixture adaptive gaussians termed factorial mog 
point hinton zemel proposed studied related generative model differed gaussians covariance em algorithm model derived ghahramani 
different forms adaptive mog hinton 
bishop 

sensor model source model combined noise model leads step generative model observed sensor signals 
model viewed hierarchical feedforward network visible layer hidden layers shown 
generate sensor signals bottom hidden units top hidden units visible units ij ij feedforward network representation generative model 
source signal generated independent state mog model 
sensor signals generated gaussian model mean depends linearly sources 
pick unit source probability wq top hidden layer source states 
unit top generative connection weight units bottom hidden layer 
activated causes unit produce sample gaussian density centered variance probability generating particular source vector bottom hidden layer gamma vq ii unit bottom hidden layer top generative connection weight ij unit visible layer 
generation unit produces sample gaussian density centered ij case independent sensor noise variance density ii generally noise correlated sensors probability generating particular sensor vector visible layer gamma hx important emphasize generative model probabilistic describes statistics unobserved source observed sensor signals densities actual signals model fully described joint density visible layer hidden layers notice sensor signals depend sources source states produced identity state generated irrelevant network layers form top order markov chain 
generative model attributes probability observed sensor data vector able return express closed form 
dx gaussian forms integral sources performed analytically yield gamma hv source density sensor density model adaptive factorial mog illustrated bottom right 
changing element mixing matrix result rigid rotation scaling line states 
learning model amounts fitting sensor data mixture adaptive gaussians deduce model parameters 
learning model error function maximum likelihood estimate model parameters define error function measures difference model sensor density observed 
parameters adapted iteratively minimize error 
choose kullback leibler kl distance function cover thomas defined dy log gammae log gamma operator performs averaging observed known kl distance non negative vanishes 
error consists terms negative log likelihood observed sensor signals model parameters second term sensor entropy independent henceforth dropped 
minimizing equivalent maximizing likelihood data respect model 
kl distance interesting relation mean square point point distance 
see define relative error respect true density gamma omitting dependence expressed terms obtain gamma dy log dy approximation log gamma valid limit small 
parameter regime model near observed density minimizing amounts minimizing mean square relative error model density 
property little computational significance 
straightforward way minimize error gradient descent method starting random values parameters incremented iteration small step direction gradient results slow learning 
shall employ expectation maximization approach develop efficient algorithm learning model 
expectation maximization algorithm expectation maximization em dempster neal hinton iterative method maximize log likelihood observed data respect parameters generative model describing data 
obtained noting addition likelihood log observed sensor data see may consider likelihood log complete data composed observed missing data unobserved source signals states 
observed complete data likelihood function random variable 
iteration consists steps calculate expected value complete data likelihood observed data current model 
calculate gammae log fh observed average term taken unobserved source posterior parameters obtained previous iteration fh entropy posterior see 
result averaged observed second term independent effect 
minimize maximize corresponding averaged likelihood respect obtain new parameters arg min develop em algorithm model 
show bounded error neal hinton 
dropping average observed gamma log gamma log dx gamma dx log second line follows jensen inequality cover thomas holds conditional density em choose source posterior computed parameters previous iteration obtained directly previous iteration approximate error function due markov property model obtained adding terms fb fh defined shortly 
closer inspection reveals depend model parameters terms involves parameters single layer see 
depends parameters visible layer fb depend parameters fw bottom top hidden layers respectively note depend previous parameters contribution different layers gamma dx log fb gamma dx log fw gamma log contribution negative entropy source posterior fh dx log get fb second line obtained 
em procedure follows observing equality choice 
parameter values produced previous iteration step results approximate error coinciding true error 
consider minimize respect new parameters obtained step satisfy proving current em step increase error 
em algorithm learning model parameters derived appendix new parameters iteration obtained terms old ones learning rules mixing matrix noise covariance yi gamma yi delta gamma gamma yih rules source mog parameters ep hx yi ep ep hx yi ep gamma ep notation 
hx yi theta vector denoting conditional mean sources sensors theta matrix yi source covariance conditioned sensors 
similarly hx yi denotes mean sensor conditioned hidden state source observed sensors 
probability state source conditioned sensors 
conditional averages defined 
conditional averages conditional probabilities depend observed sensor signals parameters computed step 
operator performs averaging observed scaling 
bss problem sources defined order permutation scaling 
ambiguity implied effect arbitrary permutation sources cancelled corresponding permutation columns leaving observed unchanged 
similarly scaling source factor oe affect th column scaled oe time 
put way error function distinguish true scaled permuted version possesses multiple continuous manifolds global minima 
point manifolds corresponds valid solution existence may delay convergence cause numerical problems ij may acquire arbitrarily large values 
minimize effect excessive freedom maintain variance source unity performing scaling transformation iteration oe gamma oe oe ij ij oe transformation amounts scaling source standard deviation oe ex gamma ex compensating mixing matrix appropriately 
easy show scaling leaves error function unchanged 
hierarchical interpretation em algorithm natural interpretation context hierarchical generative model see 
point view bears resemblance mixture experts algorithm jordan jacobs 
focusing learning rules top hidden layer parameters notes similarity usual em rules fitting mog model 
connection explicit rewrite rules left column dx dx dx theta dx gamma 
gamma dx 
go left column dx hm yi dx see 
note read shown right column standard em rules learning dimensional mog model parametrized source assuming source signals directly observable 
comparison square bracketed expressions left column shows em rules source parameters precisely rules learning separate mog model source actual replaced values possible observed sensor signals weighted posterior 
em algorithm learning model viewed hierarchically visible layer learns noisy linear model sensor data parametrized hidden layers learn mog model source 
actual sources available possible source signals weighted posterior observed data couples visible hidden layers parameters participate computing posterior 
relation ordinary factor analysis ordinary factor analysis fa uses generative model independent gaussian sources zero mean unit variance mixed see linear transformation added gaussian noise covariance matrix diagonal 
special case model obtained source single state 
resulting sensor density hh collective source state vq see 
invariance fa factor rotation mentioned section manifested fa model density 
theta matrix rows orthonormal pp rotation matrix define new mixing matrix hp 
density discriminate true hh rendering fa unable identify true mixing matrix 
notice factors corresponding obtained true sources rotation contrast model density general invariant transformation rotational symmetry broken mog source model 
true principle identified 
point square mixing symmetry fa density larger arbitrary diagonal noise covariance transformation hh gamma leaves invariant 
mixing noise identified case 
known em algorithm fa rubin thayer obtained special case ifa algorithm freezing source parameters values learning rules 
observed sensors source posterior simply gaussian gamma ae sigma covariance data dependent mean sigma gamma gamma delta gamma ae sigma gamma mog implied 
consequently conditional source mean covariance hx yi ae yi sigma ae ae recovering sources generative model parameters estimated sources reconstructed sensor signals 
complete reconstruction possible noise absent mixing invertible rank case sources pseudo inverse linear relation gamma general estimate sources 
ways obtain parametric estimator unobserved signal data 
discuss mean squares lms maximum posteriori probability map source estimators 
non linear functions data satisfies different optimality criterion 
easy show gaussian sources reduce linear estimator ordinary fa ae 
non gaussian sources lms map estimators differ priori advantage 
choice obtaining source estimate fx sensor data set fyg completes ifa data 
lms estimator known optimal estimate square sense minimizes gamma conditional mean sources observed sensors lms hx yi dx see source posterior depends generative parameters 
conditional mean calculated step em algorithm shown appendix weighted sum terms linear data lms aq bq aq sigma gamma sigma gamma sigma terms generative parameters 
notice weighting coefficients depend non linearly data 
map estimator map optimal estimator maximizes source posterior 
maximizing posterior equivalent maximizing joint logarithm map arg max log log source density histograms solid lines mog models learned ifa dashed lines 
model sum weighted gaussian densities dotted lines 
shown bimodal left uniform middle synthetic signals real speech signal right 
simple way compute estimator maximize quantity iteratively method gradient ascent data vector initialization incremented iteration ffi jh gamma gamma hx gamma joe learning rate oe theta vector logarithmic derivative source density oe gamma log gamma gamma initialization provided pseudo inverse relation gamma posterior may multiple maxima initial values order identify highest maximum 
notice map fixed point equation ffi 
equation non linear reflecting non gaussian nature source densities 
simple analysis shows fixed point stable det gamma oe map equation solved iterating slower gradient ascent 
gaussian sources unit covariance oe map estimator reduces ordinary fa ae 
ifa simulation results demonstrate performance em algorithm algorithm ifa mixtures sources corrupted gaussian noise different intensities 
sec long speech music signals obtained commercial cd original sampling rate khz sampled khz resulting sample points 
signals characterized unimodal densities shown right 
synthetic signals obtained random number generator 
signals arbitrary densities examples shown left middle 
signals scaled unit variance mixed random theta mixing matrix varying number sensors white gaussian signals covariance matrix added mixtures 
different noise levels see 
learning rules iterated batch mode starting random parameter values 
experiments modeled source density state mog provided sufficiently accurate description signals dashed dotted lines shows 
principle prior knowledge source densities exploited freezing source parameters values corresponding mog fit densities learning mixing matrix noise initial final ifa learns adaptive mog model data 
top joint density sources dots individual densities shown 
bottom observed sensor density dots resulting linear theta mixing sources contaminated low noise 
mog source model represented ellipsoids centered means source states corresponding mog sensor model 
note mixing affects rigid rotation scaling states 
starting random source parameters left random mixing matrix noise covariance ifa learns actual values right 
covariance result faster convergence 
allowed source parameters adapt starting random values 
learning source densities illustrated 
top solid lines shows convergence estimated mixing matrix true mixtures sources densities 
plotted matrix elements product gamma notice correct estimate unit matrix recall effect source scaling eliminated prevent possible source permutations affecting measure permuted columns largest element absolute value column ii product shown converge cases 
observe convergence estimated noise covariance matrix true measured kl distance corresponding noise densities 
densities gaussian see easy calculate distance analytically kn du log tr gamma gamma gamma log det gamma recall kl distance non negative notice kn differentiating respect shows minimum point 
shown bottom dashed line kn approaches zero cases 
convergence estimated source densities quantified measuring kl distance true densities 
purpose fitted mog model gamma source obtained parameters 
kl distance em step mixing matrix convergence em step em step sources noise convergence em step top convergence mixing matrix sources left right sensors snr db 
plotted matrix elements solid lines em step number 
bottom convergence noise source densities 
plotted kl distance kn estimated true noise densities dashed line kl distances estimated source densities true ones solid lines 
em step estimated dx log log computed parameters values obtained ifa step denotes value source time point bottom solid lines shows convergence zero sensors 
illustrates accuracy source densities learned ifa 
histogram sources experiment compared mog description obtained adding corresponding weighted gaussians final ifa estimates parameters 
agreement demonstrating ifa algorithm successfully learned source densities 
examines closely precision ifa estimates noise level increases 
mixing matrix error ffl quantifies distance final value define mean square non diagonal elements normalized mean square diagonal elements ffl gamma ij ii gamma signal noise ratio snr obtained noting signal level sensor ij ij recall corresponding noise level eu ii averaging sensors get snr ij ii snr db snr db mixing noise error db snr db snr db source density error db top estimate errors mixing matrix ffl solid line noise covariance kn dashed line signal noise ratio left right 
errors ica estimate mixing matrix dotted line plotted 
bottom estimate errors source densities 
plot mixing matrix error snr top solid line measured db log ffl vs log snr sensors 
plot error ica bell sejnowski estimate mixing matrix top dotted line 
ica formulated square noiseless case employed step procedure principal components pc sensor data obtained ii ica applied yield ica gy resulting estimate mixing matrix ica gamma notice procedure exact zero noise case pc non zero ones problem reduces square noiseless mixing described hx see discussion section 
plotted error estimated noise covariance top dashed line kl distance kn final value 
measuring kl distance db suggested mean square error interpretation 
bottom shows estimate errors source densities kl distance true densities ifa completed 
expected errors decrease increasing snr increasing noise error kn forms exception showing slight increase snr reflecting fact lower noise level harder estimate precision 
general convergence faster larger conclude estimation errors model parameters quite small usually falling range gamma db larger db long noise level higher signal level snr db 
similar results obtained simulations performed 
small values estimate errors suggest errors originate finite sample size convergence undesired local minima 
studied noise level affects separation performance measured quality source reconstructions obtained lms map 
quantified mean square reconstruction error ffl rec measures close reconstructed sources original ones 
error composed components arising presence noise interference sources cross talk additional component arising parameter estimation errors snr db snr db snr db snr db source reconstruction quality sources left right sensors 
plotted reconstruction error ffl rec top cross talk level ffl bottom vs signal noise ratio lms solid lines map dashed lines ica dotted lines estimators 
negligible comparison 
amount cross talk measured ffl ffl rec gamma ffl gamma ex note zero noise perfect separation quantities approach zero infinite sample limit 
reconstruction error normalized ex cross talk level plotted snr lms solid lines map dashed lines source estimators 
plot ica results dotted lines 
expected ffl rec ffl decrease increasing snr significantly higher ica 
notice lms reconstruction error lower map derived demanding minimizes precisely ffl rec contrast map estimator lower cross talk level 
ifa sources factorized variational approximation em algorithm exact required calculations done analytically intractable number sources model increases 
conditional means computed step involve summing possible configurations source states delta delta delta ql number grows exponentially number sources 
long focus separating small number sources treating rest noise describe source small number states step tractable separating example sources states involve theta element sums iteration 
intractability exact learning course problem unique model shared probabilistic models 
general approximations 
suitable starting point approximations function bounded exact error arbitrary density posterior hidden variables generative model values visible variables 
root intractability em choice exact posterior derived bayes rule parametrized generative parameters approximation schemes proposed hinton dayan saul jordan saul ghahramani jordan form generally differs exact posterior set parameters learned separately appropriate procedure 
crucial significance functional form chosen step tractable providing reasonable approximation exact posterior 
parameters optimized minimize distance exact posterior 
case model consider function gamma dx log averaging data implied 
shall variational approach formulated context feedforward probabilistic models saul jordan 
chosen form posterior see minimized iteratively respect variational parameters minimization leads approximate em algorithm ifa derive remaining part section 
assume previous iteration produced step current iteration consists determining values terms solving pair coupled mean field equations 
straightforward show step minimizes kl distance variational exact posteriors kl 
fact distance equals difference gamma 
step approximates exact distance vanishes 
variational parameters determined new generative parameters obtained step conditional source means readily computed terms factorized posterior observation sources model independent sources conditioned data vector correlated 
clear fact conditional source correlation matrix yi non diagonal 
generally joint source posterior density factorize expressed product posterior densities individual sources 
factorized variational approximation assume conditioned data vector sources independent 
approximate posterior source density defined follows 
data vector source state described gaussian distribution dependent mean variance weighted mixing proportion posterior defined simply product gamma alluded variances turn independent 
gain insight approximation notice implies mog form posterior gamma complete analogy prior 
conditioning sources data approximated simply allowing variational parameters depend compare exact posterior 
implies mog form differs contrast approximate posterior exact implies mog form reflecting fact source states signals correlated data 
approximation viewed result shifting source prior true posterior data vector variational parameters assuming shifted values source parameters 
shift course capture correlations sources optimized allow best approximate true posterior maintaining factorized form 
procedure determining optimal values derived section 
factorized posterior advantageous facilitates performing step calculations polynomial time 
variational parameters determined data conditioned mean covariance sources required em learning rule hx yi hx yi hx yi required rules conditioned source states hx yi hx yi recovering sources 
section lms map source estimators exact ifa 
notice part step computing lms estimator exactly quickly intractable number sources increases 
variational approximation replaced lms hx yi depends variational parameters avoids summing source state configurations 
contrast map estimator remains unchanged parameters depends learned variational ifa note computational cost weakly dependent mean field equations fixed learning rules follow solving equations 
equations linear evident gradients appendix solution closed form 
learning rules similarly derived fixing solving 
unfortunately examining gradients appendix shows equations non linear solved numerically 
choose find solution iteration 
define theta matrix gamma equation variances involve easily solved ii gamma means mixing proportions obtained iterating mean field equations data vector ij gamma log log log gamma log ff lagrange multipliers enforce normalization conditions 
note eq 
depends non linearly due non linear dependence solve equations initialize eq 
linear theta system solved standard methods 
new obtained ff ff values substituted back procedure repeated convergence 
data independent approximation 
simpler approximation results setting data vectors means obtained single iteration data vectors equation linear approximation expensive computationally corresponding reduction accuracy shown 
variational ifa simulation results factorized form true posterior data independent simplification exact mean field equations optimize variational parameters approximate posterior accurate possible 
assess quality approximation 
studied accuracy approximate error function 
purpose considered small data set theta vectors generated independently gaussian distribution 
approximate log likelihood gammaf data compared exact log likelihood gammae respect models random parameters realization obtained sampling parameters uniform densities defined appropriate intervals followed scaling source parameters 
case mixing proportions sampled obtained 
state mog densities 
relative error log likelihood ffl gamma computed factorized data independent approximations 
histogram displayed case left middle sources 
examples simulations performed mean error factorized approximation 
data independent approximation expected accurate increases mean error 
investigated variational ifa algorithm learns appropriate values model parameters answer quantified terms resulting reconstruction error 
sec long source signals sampled different densities displayed rate khz generated 
noisy linear mixtures sources data exact ifa algorithm approximations 
learning source signals reconstructed data lms source estimator see discussion section 
data vector reconstruction error ffl rec computed 
histograms log ffl rec db units exact ifa approximations case snr db displayed right 
ica error histogram case plotted 
note variational histogram close exact data independent histogram larger mean error 
ica mean error largest consistent results top 
conclude factorized variational approximation ifa quite accurate 
course real test application cases large numbers sources exact ifa longer 
addition variational approximations defined 
thorough assessment factorial likelihood error likelihood error recognition error db left middle histogram relative error log likelihood ffl random data vectors factorized variational approximation dashed line mean error left middle simplification dashed dotted line mean 
likelihoods computed respect random model parameters sensors left middle sources 
right histogram reconstruction error ffl rec snr db exact ifa solid line mean gamma db factorized dashed line mean gamma db data independent dashed dotted line mean gamma db variational approximations ica dotted line mean gamma db 
lms source estimator 
variational approximations applications somewhat scope published separately 
noiseless ifa consider model noiseless case 
sensor data depend deterministically sources hx mixing matrix recovered exactly estimated observed data pseudo inverse gamma reduces gamma square invertible mixing 
vanishing noise level results linear source estimator independent source parameters 
expect em algorithm noisy case applied noiseless mixing consequence noise covariance acquire small values 
case shall show 
turns zero noise limit algorithm performs principal component analysis pca consequently low noise convergence pca ifa solution slow 
root problem noiseless case type missing data source states source signals longer missing directly observed sensors 
shall proceed derive em algorithm specifically case 
algorithm turn powerful extension bell sejnowski ica algorithm 
expectation maximization algorithm focus square invertible mixing rank write gy unmixing separating matrix gamma columns possibly scaled permuted 
noisy case type missing data source states stochastic dependence sensor data sources deterministic 
conditional density replaced det implied 
factorial mog model sources error function gamma log gamma log det gamma log gamma log det gamma log noisy case obtained approximated error bounded true error sum individual layer contributions see fb fh contributions visible bottom hidden layers depend visible layer parameters gamma log det fb gamma log fw gamma log top layer contribution remains separated compare noting due 
entropy term fh log independent 
point complete form expressions includes replacing gy averaging observed em learning algorithm model parameters derived appendix difficulty arises fact step equation solution new value terms parameters obtained previous em step non linear solved analytically 
solve iteratively em step composed sequence iterations held fixed 
noiseless ifa learning rule separating matrix ffig jg gamma determines learning rate value set empirically 
oe theta vector depends posterior computed parameters previous iteration th coordinate weighted sum states source oe gamma rules source mog parameters ep ep ep ep gamma ep recall linearly related operator averages observed noiseless ifa learning rules follows 
having obtained parameters previous em step new step starts computing posterior setting initial values new parameters set final value ep 
sequence iterations begins iteration consists computing sources gy current ii computing new sources obtained iii computing new sources obtained means variances obtained ii 
iterations continue convergence criterion satisfied note process change frozen 
achieving convergence completes current em step step starts updating posteriors 
recognize learning rules source densities precisely standard em rules learning separate mog model source shown right column 
noiseless ifa algorithm combines separating sources learning rule simultaneously learning densities em mog 
processes coupled priors 
shall show section decouple consequently separating matrix rule bell sejnowski ica rule producing algorithm shown schematically 
point mog learning rules noiseless case obtained noisy case replacing conditional source means hx yi ij replacing source state posteriors 
changes arise vanishing noise level source sensor dependence deterministic 
scaling 
noisy case noiseless ifa augmented scaling transformation iteration oe gamma oe oe ij oe ij sensors sources 
noiseless ifa algorithm assumes square invertible theta mixing matrix 
general case theta mixing treated follows 
start observation case theta sensor covariance matrix cy rank columns contain eigenvectors diagonal 
principal components sensor data non zero 
denoted formed columns corresponding non zero eigenvalues 
algorithm applied find theta separating matrix denoted theta separating matrix required recovering sources sensors simply remains find done matrix diagonalization methods 
alternatively observing columns required eigenvectors span subspace principal component analysis learning rule replaced may purpose 
generalized em relation independent component analysis procedure described noiseless ifa rules strictly em algorithm sufficiently small possible different manner 
alternative procedure defined making changes complete em step update posteriors fixed number iterations regardless convergence ij ij ij final ij fix learn ica initial fix learn em mog seesaw gem algorithm noiseless ifa 
achieved ii em step select parameters set freeze step updating rest choice frozen parameters may vary step 
procedure incorporates ii minimize approximate error step sufficiently large merely reduces 
course em convergence proof remains valid case 
procedure termed generalized em gem algorithm dempster neal hinton 
clearly possible gem versions noiseless ifa 
particular versions defined chase obtained em version simply updating posteriors iteration 
gem step consists single iteration separating matrix rule ii single iteration mog rules iii updating posteriors new parameter values 
source densities follow step step 
seesaw obtained breaking em version phases alternating freeze mog parameters gem step consists single iteration separating matrix rule followed updating posteriors new value ii freeze separating matrix gem step consists single iteration mog rule followed updating posteriors new values mog parameters 
sequence steps phase terminates making steps satisfying convergence criterion 
switch back forth learning learning source densities 
chase seesaw gem algorithms converge faster original em 
notice require updating posteriors step operation computationally expensive source posterior computed individually requires summing states making total cost linearly dependent noisy ifa algorithm contrast updating source state posteriors requires summing collective source states total cost exponential show seesaw combines known algorithms intuitively appealing manner 
source density learning rules em rules fitting mog model source discussed previous section second phase seesaw equivalent em mog 
shown phase equivalent bell sejnowski independent component analysis ica algorithm sigmoidal non linearity replaced function related mog source densities 
seesaw amounts learning ij applying ica observed sensors densities kept fixed fixing ij learning new applying em mog reconstructed sources ij repeat 
algorithm described schematically 
context bss noiseless ifa problem equal number sensors sources formulated problem ica comon 
efficient ica algorithm proposed bell sejnowski information maximization viewpoint soon observed mackay pearlmutter parra cardoso algorithm fact performing maximum likelihood equivalently minimum kl distance estimation separating matrix generative model linearly mixed sources non gaussian densities 
ica densities fixed 
derivation ica noiseless ifa algorithm starts kl error function 
approximating ica minimizes exact error steepest descent method gradient gamma gamma theta vector th coordinate related density source gamma log separating matrix incremented iteration direction relative gradient cardoso laheld amari mackay ffig gammaj resulting learning rule ffig jg gamma je sources computed sensors iteration gy 
ica rule form noiseless ifa separating matrix rule oe replaced defined 
original bell sejnowski algorithm source densities cosh gamma shown mog form produces gamma form oe identical oe noiseless ifa source state posteriors updated iteration 
conclude phase seesaw equivalent ica 
ica accomplish separation inaccurate source density model speech signals laplacian density successfully separated model cosh gamma model inaccuracies lead failure 
example mixtures negative kurtosis signals uniform distribution separated cosh gamma model kurtosis positive 
densities sources hand known advance algorithm ability learn crucial 
parametric source model principle directly incorporated ica mackay pearlmutter parra deriving gradient descent learning rules parameters ffi gammaj addition rule unfortunately resulting learning rate quite low case non parametric density estimation methods pham 
alternatively source densities may approximated cumulant methods edgeworth gram charlier expansions comon amari cardoso laheld approach produces algorithms robust approximations true probability densities non normalizable negative 
contrast noiseless ifa algorithm particular seesaw gem version resolves problems combining ica source density learning rules manner exploits efficiency offered em technique 
noiseless ifa simulation results section demonstrate compare performance chase seesaw gem algorithms noiseless mixtures sources 
sec long speech music signals obtained commercial cd synthetic signals produced random number generator sampling rate khz 
source signal densities example shown 
signals scaled unit variance mixed random theta mixing matrix learning rules manner required chase seesaw procedures iterated batch mode starting random parameter values 
fixed learning rate 
shows convergence estimated separating matrix left source densities right chase top seesaw bottom 
distance gamma true mixing matrix gem step mixing matrix convergence chase gem step source density convergence gem step seesaw gem step top convergence separating matrix left source densities right chase algorithm sources 
plot matrix elements gem step number plot kl distance true densities 
bottom seesaw algorithm 
quantified matrix elements gh notice correct estimate gamma unit matrix recall effect source scaling eliminated prevent possible source permutations affecting measure permuted columns largest element absolute value column ii product shown converge cases 
source densities plot kl distances true densities approach zero learning proceeds 
notice seesaw required smaller number steps converge similar results observed simulations performed 
seesaw manner initializing parameters mog parameters frozen phase proceeded iterations frozen scaling phase ii proceeded maximal relative increment mog parameters decreased theta gamma phase alternation manifested constant changes vice versa 
particular upward jump elements iterations caused scaling performed phase ii 
demonstrate advantage noiseless ifa bell sejnowski ica applied algorithms mixture sources densities plotted left 
seesaw version ifa 
learning recovered sources obtained joint densities displayed ifa middle ica right 
sources recovered ica clearly correlated reflecting fact algorithm uses non adaptive source density model unsuitable case 
relation principal component analysis mentioned section em algorithm ifa section fails identify mixing matrix noiseless case 
shown zero noise limit ji ifa seesaw ica noiseless ifa vs ica 
left source densities histograms 
sources mixed random theta matrix 
middle joint density sources recovered mixtures seesaw 
right ica 
theta unit matrix examine learning rule line 
source posterior singular ffi gamma ae ae gamma loses dependence source states simply expresses fact observation sources conditional mean hx yi zero variance hx yi ae yi ae ae expected zero noise 
rule gamma mixing matrix obtained previous iteration covariance matrix observed sensor data 
rule contains information source parameters effect vanishing noise disconnected bottom hidden layer top 
bottom visible layers form separate generative model gaussian sources source property vanishing correlations mixed linearly noise 
fact columns orthogonal directions defined principal components observed data recall matrix theta algorithm 
see assume diagonal columns orthonormal 
contains eigenvalues data covariance matrix expressed direct substitution rule reduces step contributes minimizing error minimum 
mathematically origin phenomenon lies sensor density conditioned sources non analytic ffi gamma hx 
complete analysis generative model formed linearly mixing uncorrelated gaussian variables tipping bishop shows columns span dimensional space defined principal directions data stationary point corresponding likelihood particular spanned space defined principal directions likelihood maximal point 
conclude zero noise case em algorithm performs pca ifa top layer learning factorial mog model linear combinations principal components 
non zero low noise convergence pca ifa solution slow noiseless ifa algorithm may preferable 
interesting point rule obtained special case noiseless ifa discovered quite tipping bishop independently roweis em algorithm pca 
introduced concept independent factor analysis new method statistical analysis multi variable data 
performing ifa data interpreted arising independent unobserved sources mixed linear transformation added noise 
context blind source separation problem ifa separates non square noisy mixtures sources mixing process noise properties unknown 
perform ifa introduced hierarchical generative model mixing situation derived em algorithm learns model parameters observed sensor data sources reconstructed optimal non linear estimator 
ifa algorithm reduces known em algorithm ordinary fa model sources gaussian 
noiseless limit reduces em algorithm pca 
number sources increases exact algorithm intractable approximate algorithm variational approach derived accuracy demonstrated 
em algorithm specifically noiseless ifa associated linear source estimator derived 
algorithm particular generalized em versions combine separating sources bell sejnowski ica learning densities em rules mixtures gaussians 
chase version source densities learned simultaneously separating matrix seesaw version learns parameter sets alternating phases 
efficient solution provided problem incorporating adaptive source densities ica 
generative model similar proposed lewicki sejnowski 
fact model implicit olshausen field algorithm exposed olshausen 
model uses laplacian source prior integral sources required obtain approximated value integrand maximum approximation improved incorporating gaussian corrections lewicki sejnowski 
resulting algorithm derive efficient codes images sounds lewicki olshausen put forth computational model interpreting neural responses efficient coding framework olshausen field 
contrast ifa algorithm non adaptive source density model may perform poorly non laplacian sources uses gradient ascent efficient em method approximations involved derivation small number sources exact ifa available 
interesting compare performance algorithm variational ifa mixtures sources arbitrary densities 
em algorithm noisy bss restricted discrete sources distributions known advance developed cardoso 
moulines 
proposed em approach noisy mixing continuous sources 
discuss source reconstruction method restricted small number sources extend noiseless mixing essentially insight regarding advantage mixture source models 
related idea discussed roweis ghahramani 
important issue deserves separate discussion determination number hidden sources assumed known 
simple parameter increasing number sources increases number model parameters resulting effect different generative model 
determine model comparison methods extensive literature available see mackay discussion bayesian model comparison evidence framework 
simpler imprecise method exploit data covariance matrix fix number sources number significant respect threshold eigenvalues 
method suggested fact zero noise case number positive eigenvalues precisely noisy case result depend strongly threshold systematic way determine accuracy method expected decrease increasing noise level 
viewed data modeling tool ifa provides alternative factor analysis hand mixture models suggesting description data terms highly constrained mixture adaptive gaussians simultaneously terms independent underlying sources may reflect actual generating mechanism data 
capacity ifa may noise removal completion missing data 
related statistical methods projection pursuit friedman stuetzle huber generalized additive models hastie tibshirani comparative study ifa techniques great interest 
viewed compression tool ifa constitutes new method redundancy reduction correlated multi channel data factorial channel representation reconstructed sources 
known optimal linear compression provided pca characterized absence second order correlations new channels 
contrast compressed ifa representation non linear function original data non linearity effectively optimized ensure absence correlations arbitrarily high orders 
viewed tool source separation realistic situations ifa currently extended handle noisy convolutive mixing matrix filters 
extension exploits spatiotemporal generative models introduced attias schreiner served basis deriving gradient descent algorithms convolutive noiseless mixtures 
related approach problem outlined moulines 

addition complicated mixing models ifa allows complex models source densities resulting source estimators optimized properties sources reconstruct faithfully observed data 
simple extension source model incorporate source auto correlations attias schreiner produce non linear multi channel generalization wiener filter 
powerful models may include useful high order source descriptions 
ifa derivation em algorithm provide derivation em learning rules approximate error 
step obtain terms model parameters substitute gamma hx obtain bit algebra log det tr gamma gamma yy gamma yih yih delta integration sources required compute appears conditional mean covariance sources observed sensor signals defined hm dx xx note conditional averages depend parameters produced previous iteration 
point hx yi theta vector yi theta matrix 
substitute gamma get fb log gamma hx yi gamma hx yi delta integration source indicated fb enters conditional mean variance source observed sensor signals hidden state source defined hm dx note quantity calculating joint conditional average source signal state hx hm dx 
broke posterior hidden variables computational convenience 
top layer gamma log complete step express conditional averages explicitly terms parameters key calculation conditional densities product posterior density unobserved source signals states observed sensor signals 
starting joint straightforward show sensor signals state source drawn known sources gaussian density theta gamma ae sigma covariance matrix mean sigma gamma gamma gamma delta gamma ae sigma gamma gamma gamma delta note mean depends linearly data 
posterior probability source states sensor data obtained able compute conditional source averages 
hx yi ae yi sigma ae ae obtain conditional averages sensors sum states probabilities get hm yi hm yi xx point corresponding source posterior density adaptive mog just sensor density 
notice sums mean delta delta delta ql individual source averages appear corresponding state posterior product summing sources hm yi fq hm yi results 
individual state posterior appearing similarly obtained fq emphasize parameters appearing belong substituting expressions adding completes step yields 
step derive em learning rules minimize obtained respect done computing gradient layer layer 
visible layer parameters gamma yi gamma gamma yi gamma gamma gamma gamma yy gamma yih yih delta gamma bottom hidden layer fb gamma hx yi gamma fb gamma gamma hx yi gamma hx yi gamma delta computing gradient respect top hidden layer parameters ensure probabilities satisfy non negativity normalization constraints 
enforced automatically working new parameters related mixing proportions gradient taken respect new parameters gammap recall conditional source averages state probabilities depend equations include averaging observed set new parameters values gradient vanish obtaining learning rules 
variational ifa derivation mean field equations derive mean field equations start approximate error factorial posterior 
approximate error composed layer contributions negative entropy posterior 
fb conditional source means densities expressed terms variational parameters 
term fh log gamma log const const reflects fact source posterior normalized 
fh obtained factorial posterior 
note term depend generative parameters contribute exact em algorithm crucial variational approximation 
minimize respect compute gradient gamma ii gamma gamma gamma ij gamma ii gamma log log log gamma log gamma ii equation leads directly 
second third equations bit simplification lead 
reflect normalization mixing proportions impose normalization minimize gamma method lagrange multipliers 
noiseless ifa derivation gem algorithm appendix derive gem learning rules noiseless case 
derivation follows steps appendix step substituting gamma get bottom layer fb log gamma top layer gamma log note fb noisy case conditional source means computed 
posterior probability th source states obtained bayes rule step derive learning rule unmixing matrix error gradient gamma gamma delta gamma oe oe 
determine increment relative gradient approximate error ffig gammaj jg gamma joe extremum condition ffig implying eoe gy analytically solvable leads iterative rule 
explained amari cardoso laheld mackay relative gradient advantage ordinary gradient algorithm produces equivariant performance independent rank mixing matrix computational cost lower require matrix inversion 
learning rules mog source parameters obtained gradient bottom top layer contributions fb gamma gamma fb gamma theta gamma gamma gammap line obtained 
miller nagarajan especially useful discussions 
due anonymous referees helpful suggestions 
research supported office naval research sloan foundation 
amari cichocki yang 

new learning algorithm blind signal separation 
touretzky mozer hasselmo 
eds advances neural information processing systems 
mit press cambridge ma 
attias schreiner 

blind source separation deconvolution dynamic component analysis algorithm 
neural computation 
bell sejnowski 

information maximization approach blind separation blind deconvolution 
neural computation 
cardoso 

maximum likelihood source separation discrete sources 
proc 
eusipco 
bishop svens en williams 

gtm generative topographic mapping 
neural computation 
cardoso 
laheld 

equivariant adaptive source separation 
ieee transactions signal processing 
cardoso 

infomax maximum likelihood source separation 
ieee signal processing letters 
comon jutten herault 

blind separation sources part ii problem statement 
signal processing 
comon 

independent component analysis new concept 
signal processing 
cover thomas 

elements information theory 
john wiley new york 
dayan hinton neal zemel 

helmholtz machine 
neural computation 
dempster laird rubin 

maximum likelihood incomplete data em algorithm discussion 
journal royal statistical society 
everitt 

latent variable models 
chapman hall london 
friedman stuetzle 

projection pursuit regression 
journal american statistical association 
ghahramani 

factorial learning em algorithm 
tesauro touretzky alspector 
eds advances neural information processing systems 
morgan kaufmann san francisco ca 
ghahramani jordan 

factorial hidden markov models 
machine learning 
hastie tibshirani 

generalized additive models 
chapman hall london 
hinton williams revow 

adaptive elastic models hand printed character recognition 
moody hanson lippmann 
eds advances neural information processing systems 
morgan kaufmann 
hinton zemel 

minimum description length helmholtz free energy 
cowan tesauro alspector 
eds advances neural information processing systems 
morgan kaufmann san francisco ca 
hinton dayan frey neal 

wake sleep algorithm unsupervised neural networks 
science 

huber 

projection pursuit 
annals statistics 
hyvarinen oja 

fast fixed point algorithm independent component analysis 
neural computation 
jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation 
jutten herault 

blind separation sources part adaptive algorithm neuromimetic architecture 
signal processing 
lee bell lambert 

blind separation delayed convolved sources 
mozer jordan petsche 
eds advances neural information processing systems 
mit press cambridge ma 
lewicki sejnowski 

learning nonlinear overcomplete representations efficient coding 
advances neural information processing systems press 
lewicki olshausen 

inferring sparse overcomplete image codes efficient coding framework 
advances neural information processing systems press 
mackay 

bayesian interpolation 
neural computation 
mackay 

maximum likelihood covariant algorithms independent component analysis 
technical report cavendish laboratory cambridge university 
moulines cardoso 

maximum likelihood blind separation deconvolution noisy signals mixture models 
proceedings ieee conference acoustics speech signal processing vol 

ieee 
neal hinton 

view em algorithm justifies incremental sparse variants 
jordan 
ed learning graphical models 
kluwer academic press press 
olshausen field 

emergence simple cell receptive field properties learning sparse code natural images 
nature 
olshausen 

learning linear sparse factorial codes 
technical report ai memo cbcl artificial intelligence lab mit 
olshausen field 

sparse coding overcomplete basis set strategy employed 
vision research 
pearlmutter parra 

maximum likelihood blind source separation context sensitive generalization ica 
mozer jordan petsche 
eds advances neural information processing systems 
mit press cambridge ma 
pham 

blind separation instantaneous mixture sources independent component analysis 
ieee transactions signal processing 
roweis ghahramani 

unifying review linear gaussian models 
technical report dept computer science toronto 
roweis 

em algorithms pca spca 
advances neural information processing systems press 
rubin thayer 

em algorithms ml factor analysis 
psychometrika 
saul amd jordan 

exploiting tractable structures intractable networks 
touretzky mozer hasselmo 
eds advances neural information processing systems 
mit press cambridge ma 
saul jaakkola jordan 

mean field theory sigmoid belief networks 
journal artificial intelligence research 
tipping bishop 

probabilistic principal component analysis 
technical report ncrg 
torkkola 

blind separation convolved sources information maximization 
neural networks signal processing vi ieee new york 
