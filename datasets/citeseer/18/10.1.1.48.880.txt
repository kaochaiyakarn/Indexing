model invariant object recognition visual system guy wallis edmund rolls february oxford university department experimental psychology south parks road oxford ox ud england 
neurons ventral stream primate visual system exhibit responses images objects invariant respect natural transformations translation size view 
anatomical neurophysiological evidence suggests achieved series hierarchical processing areas 
attempt elucidate manner representations established constructed model cortical visual processing seeks parallel features system specifically multi stage hierarchy topologically constrained convergent connectivity 
stage constructed competitive network utilising modified hebb learning rule called trace rule incorporates previous current neuronal activity 
trace rule enables neurons learn invariant short time periods representation objects objects transform real world 
trace rule enables neurons learn statistical invariances objects transformations associating representations occur close time 
show trace rule training algorithm model learn produce transformation invariant responses natural stimuli faces 
background evidence series cortical processing stages visual system primates produces representation objects shows invariance respect example translation size view shown recordings single neurons temporal lobe desimone rolls tanaka 
rolls reviews specific regard cells responsive faces goes advance theory neurons acquire transform independent selectivity known physiology visual cortex simple self organising principles 
analysis forms basis network described 
fundamental elements rolls hypothesis ffl series competitive networks organised hierarchical layers exhibiting mutual inhibition short range layer 
ffl convergent series connections localised population cells preceding layers cell layer allowing receptive field size cells increase visual processing areas layers 
ffl modified hebb learning rule incorporating temporal trace cell previous activity suggested enable neurons learn transform invariances 
elements hypothesis constrain general architecture network model intended learn invariant representations objects 
simulation results model reveal invariant representations learnt 
success crucially depends inclusion modified version hebb rule 
trace rule basic learning rule implemented simulations utilises spatio temporal constraints placed behaviour real world objects learn natural object transformations 
presenting consistent sequences transforming objects cells network learn respond object naturally transformed states described ak rolls 
learning rule incorporates decaying trace previous cell activity henceforth referred simply trace learning rule 
learning paradigm describe intended principle enable learning transforms tolerated inferior temporal cortex neurons rolls rolls 
clarify reasoning point consider situation single neuron strongly activated stimulus forming part real world object 
trace neuron activation gradually decay time period order say 
limited time window net transformed version original stimulus initially active afferent synapses modify neuron synapses activated transformed version stimulus 
way cell learn respond appearance original stimulus 
cell tend spurious links stimuli part different objects object consistently 
various biological bases temporal trace advanced precise mechanisms involved may alter precise form trace rule 
ak describes alternative trace rule models individual channels 
equally trace implemented extended cell firing reflected representing trace external firing rate internal signal 
ffl persistent firing neurons long ms observed presentations stimuli ms rolls provide time window associate subsequent images 
maintained activity may potentially implemented recurrent connections cortical areas reilly johnson rolls ffl binding period channels may ms may implement trace rule producing narrow time window average activity presynaptic site affects learning rolls rhodes ak 
ffl chemicals oxide may released high neural activity gradually decay concentration short time window learning enhanced ak montague 
trace update rule simulations equivalent ak earlier rule sutton barto summarised follows deltaw ijk ij ij gamma ij jy ij gamma th input neuron 
ij output neuron th column th row output layer 
ij trace value neuron time step ff learning rate 
annealed unity zero 
ijk synaptic weight th input neuron column row trace value 
optimal value varies presentation sequence length 
bound growth cell dendritic weight vector ij explicitly normalised method similarly employed von der malsburg 
alternative biologically relevant implementation local weight bounding operation utilises form long term depression brown review part explored version oja rule oja 
prolonged firing inferior temporal cortex neurons memory delay periods seconds associative links reported develop stimuli seconds apart miyashita chang long time scale immediately relevant theory 
fact associations visual events occurring seconds apart normal environmental conditions detrimental operation network type described probably arise different objects 
contrast system described benefits associations visual events occur close time typically object 
te teo lgn combinations features larger receptive fields receptive field size deg eccentricity deg configuration sensitive view dependent view independence layer layer layer layer stylised image layer network 
convergence network designed provide fourth layer neurons information entire input retina 
convergence visual system adapted rolls 
visual cortex area teo posterior inferior temporal cortex te inferior temporal cortex 
network network designed series hierarchical convergent competitive networks accordance hypothesis advanced part described earlier wallis 
actual network consists series layers constructed convergence information disparate parts network input layer potentially influence firing single neuron final layer see 
corresponds scheme described researchers van essen rolls example primate visual system see 
forward connections cell layer derived topologically related confined region preceding layer 
choice connection layers exists gaussian distribution connection probabilities roll radially focal point connections neuron 
practice minor extra constraint precluding repeated connection cells applied 
cell receives connections grid cells preceding layer initially probability connection comes cells distribution centre effective radius convergence increases slightly layers 
shows general convergent network architecture 
localisation limitation connectivity network intended mimic cortical connectivity partially clear retention retinal topology regions visual cortex 
architecture encourages gradual combination features layer layer relevance binding problem described discussion section 
modelling topological constraints connectivity leads issue concerning neurons iab ab gammae gamma oe gammaffi ffi gamma oe contrast enhancing filter effect local lateral inhibition 
parameters ffi oe variables modify amount extent inhibition respectively 
edges network layers 
principle neurons may receive input edge preceding layer connections repeatedly sample neurons edge previous layer 
practice solution liable introduce artificial weighting active inputs cause edge unwanted influence development network 
real brain naturally smoothed transition locus cellular input fovea low acuity periphery visual field 
poses problem effect simulating small high acuity foveal portion visual field simulations 
alternative solutions elected form connections connections wrap back network opposite sides 
solution advantage making boundaries effectively invisible network 
consequence form connectivity cells final layer access information entire retina case 
confirm result neurons translation invariant responses important judge baseline performance level invariance neurons untrained net exhibit 
order act competitive network form lateral inhibition required layer help ensure stimuli evenly represented neurons layer 
simulations local inhibitory function applied neuron neighbouring cells similar von der malsburg 
inhibition winner take neuron left active competition graded produce soft competitive network 
soft competition advantageous way neurons allocated stimuli bennett particular important advantage leading distributed representations allow stimuli represented network 
inhibition essential net act competitive net help ensure stimuli evenly represented neurons layer 
inhibition simulated linear local contrast enhancing filter consisting positive central spike surrounded negative gaussian field general shape ij ij ijk ijn pictorial representation neural elements referred calculating neural activity 
formula 
network connectivity inhibition acts effects artificial connection scheme tested 
practice choice parameters describing mask ffi oe mean inhibition largely restricted nearest neuronal neighbours reducing spread effects inhibition edges 
addition local inhibitory mechanism global check average cellular activity layer maintained constant value normalising firing rates layer 
intention doing ensure learning stimulus location approximately constant 
firing rates nonlinear function original neural activation realised raising activity neuron fixed power optimal value described 
biological rationale greater linear increase neuronal firing function activation neuron characterises steeply rising portion close threshold sigmoid activation function real neurons 
sophisticated models neural transfer function non zero variable thresholds saturation considered obvious ideas try subsequent new testing model 
general calculation response neuron row column particular layer shown pictorially ij na gammai gammaj iab ji pn na gammai gammaj iab ji th input neuron 
ij output neuron th column th row output layer 
dimensions output layer 

dimensions inhibitory field 
transfer function nonlinearity 
layer subsequent layers 
number inputs neuron 
layer 
ijk synaptic weight th input neuron column row ab inhibitory mask value 
see 
network input unsupervised neural models successful learning produce cells centre surround response properties cells lateral geniculate nucleus oriented edge bar sensitive simple cells von der malsburg nass cooper linsker 
attempt learn response properties simple cells start fixed feature extraction level researchers field hummel biederman buhmann fukushima intention simulating complicated response properties cells inferior temporal cortex 
response characteristics input neurons network provided series fixed spatially tuned filters image contrast sensitivities chosen accord general tuning profiles observed simple cells 
current model symmetric bar detecting filter shapes take form gaussian shape axis orientation tuning filter difference gaussians perpendicular axis 
filter referred henceforth oriented difference gaussians dog filter chosen preference gabor filter grounds better fit available neurophysiological data including zero response parker wallis 
zero filter course produce negative positive output mean simulation simple cell permit negative positive firing 
contrast models response filter zero thresholded negative results form separate anti phase input network 
filter outputs normalised scales compensate low frequency bias images natural objects 
cells layer receive topologically consistent localised random selection filter responses input layer constraint cell samples filter spatial frequency receives constant number inputs 
shows pictorially general filter sampling paradigm typical connectivity layer cell filters input layer 
blank squares indicate connection exists professor watt stirling university assistance implementation filter scheme 
freq freq orientation sign low high frequency spatial filter sampling paradigm 
square represents retinal image network filtered dog filter appropriate orientation sign frequency 
circles represent consistent retinotopic coordinates provide input layer cell 
filters double spatial frequency reader 
left right orientation tuning increases steps segregated pairs positive negative filter responses 
layer cell chosen filters particular orientation sign spatial frequency 
measures network performance describing results experimentation describe metric network solved classification problem invariant responses 
purposes neuron said learnt invariant representation discriminates set stimuli set transformations 
example neuron response translation invariant response set stimuli consistently higher stimuli irrespective presentation location 
note state set stimuli neurons cortex generally selective single stimulus sub population stimuli abbott 
essentially measure ensure low variance neural response transformation transform invariance high variance stimuli stimulus selectivity 
immediately intuitive way measure run way classification anova location stimulus repeated measures 
measures truly random unclear underlying assumptions analysis violated 
mind chosen information measure 
measure relates information gains neural response stimulus known high verses information gains knowing location presentation low 
details calculate measure referred discrimination factor appendix 
freq high freq low typical connectivity single cell layer network input layer represented plotting receptive fields input layer cell connected particular layer cell 
receptive field layer cell centred just centre point retina 
connection scheme allows relatively fewer connections lower frequency cells high frequency cells order cover similar region input frequency 
blank square indicates connection layer neuron input neuron particular filter type 
stimuli experiments 
experimentation having established network model describe experiments theory implemented variety stimuli undergoing variety natural transformations 
case network exhibit neurons response largely invariant transformation highly discriminating stimuli sets stimuli 
stimuli part learning invariance classical properties face cells invariant response facial stimuli translated visual field 
experiment learning translation invariant representations investigated 
order test network set stimuli probable edge cues consisting shape constructed actual stimuli shown 
stimuli chosen partly significance form cues practical note contain fundamental features horizontal bar conjoined vertical bar 
practice means oriented simple cell filters input layer distinguish stimuli basis features 
consequence representation stimuli received network non orthogonal considerably difficult classify case earlier experiments involving trace rule see ak 
expectation layer neurons learn respond spatially selective combinations basic features helping distinguish non orthogonal stimuli 
trajectory followed stimulus consists sweeping left right horizontally locations top row sweeping back right left middle row returning right hand side bottom row tracing shape path retina 
stated pattern presentation locations adopted image translation experiments 
training carried presenting stimuli location total times 
sequence described followed stimulus chakravarty describes application shapes cues interpretation edge junctions tanaka 
demonstrated existence cells responsive stimuli 
sequence start point direction sweep chosen random 
shows response layer neuron selective stimulus 
weighted sum filter inputs reveals combination horizontally vertically tuned filters identifying stimulus 
case connections lower frequency filters reduced zero learning process relevant orientations 
contrasts strongly random wiring training seen previously 
likewise depicts neuronal responses intermediate layers network taken top highly invariant cells merely top 
gradual increase discrimination indicates tolerance shifts preferred stimulus gradually builds layers 
results layer neurons illustrated 
stage translation invariant stimulus identifying cells emerged 
response profiles confirm high level neural selectivity particular stimulus irrespective location 
contrasts measure invariance discrimination factor achieved cells layers averaged separate runs network 
translation invariance clearly increases layers considerable increase translation invariance layers 
sudden increase may result geometry network enables cells layer receive inputs part input layer 
having established invariant cells emerged final layer move important issue 
concerns role trace rule performance network 
convergence network connections toroidal connection scheme employed edges layer certainly provide number cells responsive stimuli location may exhibit degree translation invariance 
order assess importance factors relative importance training network trace rule network tested new conditions 
firstly performance network measured learning occurs initially random connection weights 
secondly network trained trace rule set causes learning proceed memoryless standard hebbian fashion 
shows results training conditions 
results show trace rule decisive factor establishing invariant responses layer neurons 
interesting note hebbian learning results worse achieved chance untrained net 
general hebbian learning highly discriminating cells barely rate higher 
value discrimination corresponds case cell responds stimulus location 
poor performance hebb rule comes direct consequence presentation paradigm employed 
consider image representing vector multidimensional space particular image top left hand corner input retina tend look image location image 
simple competitive network just hebbian learning tend categorise images exact opposite net intended learn 
result probably important far indicates small memory trace acting standard hebbian learning max max tee cross elle relative firing rate location layer cell low freq high freq top left graph shows response layer neuron training stimuli training locations 
alongside results filter inputs neuron 
diagram shows input reconstruction results separated rows differing spatial frequency columns representing filter tuning orientations positive negative complementary pairs 
discrimination factor cell 
max location layer cross cell max tee elle max location layer elle cross tee cell max response profiles intermediate layer neurons discrimination factors 
max location layer cross cell max elle tee max location layer cross cell max tee elle response profiles fourth layer neurons discrimination factors 
cell rank factor factor cell rank hebb trace rand variation neural discrimination factors measure performance top highly discriminating cells layers network averaged runs network 
variation neural discrimination factors measure performance top highly discriminating cells fourth layer training regimes averaged runs network 
paradigm radically alter normal vector averaging image classification performed hebbian competitive network 
question emerges representation final layer network relates evenly network divides resources represent learnt stimuli 
conceivable stimulus stands set stimuli containing distinctive features easier categorise 
may produce unrepresentative number neurons high discrimination factors fact responding stimulus 
important cells code provide information stimuli 
simple check preferred stimulus cell associated measure discrimination added total stimulus practice varied factor stimuli 
regard representation final layer informative see pattern activity varies stimulus changes location different stimuli single location 
expect see consistent activity stimulus locations variable activity changing stimuli location 
neurons final layer easy discern changes patterns activity simply displaying sequence snap shots neural responses 
average response neuron recorded 
upper left graph neural responses averaged presentations stimulus middle locations lower half averaged stimuli centrally 
large contrast neighbouring cells top graph indicates cells consistently active neighbours translation stimulus 
lower left graph average responses distinct suggesting average neural activity location stimulus normalised activity range stimulus location comparison average activity seen fourth layer multiple presentations 
location stimulus training locations 
stimulus stimuli central location 
left hand graphs show actual averages grid cells 
right hand graph shows binned histogram average activity 
stimuli shows little patterning neurons exhibit similar average activity 
contrast brought clearly right hand graph average activity histograms cases plotted 
location histogram average histograms gained different combinations responses stimulus training locations 
stimulus histogram average histograms generated presenting stimuli training locations 
looking location sees nearly thirds cells largely inactive tuned stimuli learnt due competition layers 
majority cells respond locations strongly having close maximal average responses 
averaging stimuli reveals majority cells respond relatively weakly possibly responding stimuli 
taken results reveal cells final layer show consistent active responses stimulus shown training locations appear react comparably stimuli shown location 
results lend weight claim large number neurons final layer successfully learnt invariant response translation stimuli 
final experiment series turn response layer neurons shifts presentation location away training locations 
shows firing rate offset offset offset offset firing rate offset offset offset offset response layer neurons preferred stimuli range coordinate offsets central location 
lighter squares indicate presentation locations produced high firing 
response layer neurons preferred stimulus locations arranged square grid input retina 
training locations correspond offsets 
cells selected responses tested earlier appear 
predict generalisation training locations due spatial non determinacy filters particularly shifts horizontal vertical component 
practice emerges analysis tendency greater tolerance horizontal shifts vertical ones 
presumably reflects significance vertical placement horizontal segment stimuli performing dissociation 
analysis indicates local shift invariance emerges properties filters providing input network global shift invariance provided trace learning rule enables stimulus placed different parts retina activate layer neurons 
stimuli part ii optimal network parameters second series investigations stimuli centres finding optimal parameters elements network degree non linearity neurons optimal trace time constant controls relative effect previous activities current learning described 
series results network performance gauged terms single epoch run network average discrimination factor top sixteen cells discrimination factor nonlinearity power discrimination factor nonlinearity power variation network performance function transfer function non linearity neurons layer 
variation network performance function transfer function non linearity neurons second fourth layers 
fourth layer displayed parameter value 
repeated measures quantity median value ranked cells error bars set upper lower quartile values sixteen cells displayed mean standard error bars 
parameter tested non linearity activation function layer neurons 
results plotted reveal peak nonlinearity value 
value surprisingly high presumably allows layer provide relatively clean sparse output unique features stimulus location subsequent layers associate stimulus sweep sequences 
ultimately trade increased firing sparseness constraint network performance relating sparseness inter layer connectivity 
neurons layer fire cells layer receive input 
trade reflected non linearity values greater network performance observed 
practice required sparseness achieved non zero threshold reduce optimal value non linearity 
number inputs layer differ number inputs neurons layers obvious degree non linearity layer cells reflect optimal value layers 
prompted optimisation non linearity neurons layers results appear 
lower value non linearity optimal experiments layers suggests trade signal clarity excessive sparseness shifted favour keeping relatively neurons active 
probably due factors 
layer served help stimuli reduces need strong competition neurons secondly relatively sparse inter layer connectivity network requires neurons active discrimination factor discrimination factor variation network performance function trace rule parameter neurons layers locations 
variation network performance function trace rule parameter neurons layers presentation locations 
layer ensure information input flows successfully hierarchy 
series tests concern effective length trace controlled parameter displays effect varying value standard presentation locations 
optimal value conceivably change alteration number training locations predict smaller number presentation locations reduced 
confirm network performance measured presentation sweeps locations 
shows results experiment confirm expected shift general profile curve shorter time constant values 
course optimal value derived effect compromise optimal values layers trace operates 
neurons layer different effective receptive field sizes expect layer neurons exposed different portions full sweep particular stimulus 
turn suggest optimal value grow layers 
attempt optimise values independently seen keep amount parameter tweaking network minimum 
experiments described section represent useful test case performance network possible turn important issues performance network affected increasing number stimuli transformations learnt 
faces stimuli part translation consequences relatively small number stimuli experiment encoded uniquely final layer local encoding faces stimuli face translation experiment 
example detector detector 
whilst local grandmother cell type encoding efficient stimuli means observed tuning neurons 
face cells typically respond faces subset faces described 
aim latest set experiments start address important issues network operates invariant representations learned larger number stimuli neuronal encoding changes 
experiment starts address issue network learn complicated real biological stimuli faces 
set face images appear 
practice equalize luminance dc component images removed 
addition minimize effect cast shadows oval hamming window applied face image served remove hard edges image relative plain background set 
results training translation invariance paradigm faces locations shown figures 
network produces neurons high discrimination factors occurs trained trace rule 
comparison performance untrained net hebb trained net reveals slight change previous experiment results random net having slipped hebb results indicating greater level complexity problem 
significant difference previous simulations local encoding neurons layer responding face independently location distributed representation illustrated examples layer neurons shown 
production distributed representation raises question network solved translation invariance problem 
previous experiment characters stimuli simple inspection response profiles neurons confirmed max max location location cell layer max face relative firing rate relative firing rate cell layer max face face face face face face face face face face face face face response profiles neurons fourth layer discrimination factors 
cell rank trace hebb rand cell rank factor factor variation network performance top highly discriminating cells layers network averaged runs network 
variation network performance top highly discriminating cells fourth layer training regimes averaged runs network 
information stimulus identity invariant form 
case distributed code cell largely invariant preferred stimuli general selective single stimulus 
consequence fourth layer may may retain information required stimuli responds 
appropriate conclude distributed code stimuli truly invariant successfully decoded stimulus discriminated irrespective original position retina 
order decode information firing fourth layer neurons fifth layer added net fully sampled fourth layer cells 
layer turn trained supervised manner gradient descent 
note fifth layer intended purely tool analysis decoding representation final layer network interpreted part model primate visual system 
information identity individual stimuli lost representation built layer due consistent pairing stimuli neurons new decoding network able extract information individual stimulus identity 
shows classification performance th layer nets trained hebb trace rules untrained net 
high performance hebb trained net untrained net show power gradient descent learning serve put perfect performance trace rule trained net perspective 
performance distinct main attempt doing analysis confirm information stimulus identity fact retrievable fourth layer fulfilled 
evidence quality representation obtained measuring cells responded stimulus accumulating number neurons responding strongly stimulus multiplied associated discrimination factor done previous experiment 
result time plotted shows stimuli high discrimination total 
evidence taken evidence shown figures indicates representation invariant properties stimulus established layer provided trace learning rule 
faces stimuli part ii rotation network shown able operate usefully difficult translation invariance problem address question network solve types transform invariance intended 
experiment addresses question training network problem stimulus rotation produces non isomorphic transforms determine network build view invariant categorisation stimuli 
trace rule learning paradigm conjunction architecture describe prove capable learning transforms tolerated neurons long stimulus short sequences transformation occurs learned 
experiment continues faces presents centrally retina sequence views rotating face 
images shown 
faces correctly classified face face face face face face discrimination total face face face face face face face face face face face hebb correctly classified random face face face face trace rule stimulus classification achieved training regimes supervised training fifth layer outputs fourth layer 
discrimination totals stimulus neurons fourth layer trained trace rule 
smoothed edges erase harsh image boundaries dc term removed 
epochs learning stimulus chosen random sequence preset views shown sweeping face left right 
actual number images smaller views reason think problem may harder solve previous translation experiments 
simply due fact views exactly overlap maximum images exactly overlapped previous experiment 
partly fact partly capacity network fully images twice large translation experiments 
permitted net discern finer feature detail individual faces 
experiment employs totally new training paradigm usual series optimal parameter testing performed neural activation function size trace parameter optimal non linearity value remained unchanged optimal value slightly lower previous experiments 
shift certainly function shorter sweep lengths fallen steps 
fact value lower presentations sweep confirms general relationship optimal value number steps involved presentation single stimulus various transformed forms 
net able solve invariance problem examples invariant layer neuron response profiles appearing 
major difference results previous experiments cells layer showed limited faces different views stimuli experiment ii 
tolerance shifts viewing angle 
expected slightly rotated views face share basic features location results observed generalisation 
true generalisation views achieved higher layers contribution local generalisation provided cells layer problem meant cells layer exhibit full rotational invariance 
result part due fact images twice large previous experiment extend far retina translation invariance experiment allowing convergence information relevant solving problem occur earlier hierarchy 
view invariance partially solved layer network improvement layer neurons cells high discrimination factors flatter response profiles observed layer 
confirms improvement invariant stimulus representation layers layer provides considerable improvement performance layers 
despite emergence invariant cells previous layer 
shows hebb trained untrained nets performing equally poorly whilst trace trained net shows invariance entire cells selected 
number stimuli reduced training established local sparse representation stimuli 
discussion significant draw experiments network solve invariance problems point virtue inclusion trace rule 
trace rule local biologically plausible signals required alter synaptic strength learning presynaptic firing postsynaptic activation available locally synapse 
max max relative firing rate cell layer max view number relative firing rate cell layer max view number response profiles cells layers network discrimination factors 
cell rank hebb trace rand cell rank factor factor variation network performance top highly discriminating cells layers network averaged runs network 
variation network performance top highly discriminating cells fourth layer training regimes averaged runs network 
learning rule sets proposal apart proposals invariant representations formed 
system operates self organizing competitive learning biologically plausible learning driven actual inputs received external teacher needed lateral inhibition implements competition known property cortical architecture 
models typically combined various attractive elements supervised nonlocal learning poggio edelman fukushima mel extremely idealised simplified stimuli ak hinton prohibitive object object matching processes buhmann non localised connectivity hummel biederman 
fairness models advantages model described model explicitly tackles problem locating attending objects visual field 
model really deals recognition high acuity centre visual field require mechanism locating fixating objects 
described model exposed relatively stimuli 
mel fukushima models example successfully trained larger data sets 
reported net successfully trained accuracy set hand written digits wallis 
element model lacks active solution feature binding problem addressed theories models von der malsburg example 
cell trained respond appearance set features say translation invariant way neuron may respond novel rearrangements features 
fact case real neurons responsive faces neurons reduce response faces features appear perrett true features tanaka 
successfully determining features invariant manner whilst retaining spatial configuration leads binding problem 
models throw away spatial information achieve translation invariance run problem recognising rearrangements features triggering recognition 
certainly true models attempt learn invariance stage mel 
mel records recognition cells features recognition rate trained configurations 
mind emphasise important feature model proposed convergent processing hierarchy 
purpose hierarchy gradually build representations increasing complexity self organizing competitive net layers 
nets intended produce neurons respond combinations inputs forming effective stimuli neurons 
responding local combinations neurons active previous layer arbitrary spatial arrangements features fail activate neuron 
reduce chance finding trigger features supporting recognition new arrangements features revealing partial solution feature binding problem 
important part suggestion local spatial information inherent features combined far form described tanaka 
recordings neurons 
feature combination argument view dependent representation ob jects suitable view dependent processes behavioural responses face expression gesture available stages processing 
representations formed type computation operating combine limited set views objects 
neurons view independent responses visual system evidence suggests receive inputs view dependent neurons region hasselmo perrett 
plausibility providing view independent recognition objects combining set different views objects proposed number investigators koenderink van doorn tarr pinker bulthoff edelman network described reveals representation set recourse gradient decent type learning poggio edelman logothetis 
solution object representations different traditionally proposed artificial vision systems coordinates space objects stored database general purpose algorithms operate perform transforms translation rotation scale change space marr 
limited biologically plausible scheme representation suitable recognition object linking associative memories objects described detail rolls 
solution invariant recognition proposed certainly need large number neurons simply consistent fact half cortex non human primates devoted vision 
turn leads aim discover capacity system terms number objects stimuli learn 
hypothesis invariant properties common objects learned early layers type network described information particular objects represented layers 
aspect model treated detail optimal form trace rule parameter controls length trace 
real world objects may typically viewed fixation durations saccades ms trace real world satisfactory 
simulations described optimal values rose produce somewhat longer trace object shown sequential time steps expect 
detailed quantitative approach optimal form trace rule pursued wallis 
shown variety stimulus presentation paradigms form trace rule weights events exponentially decreasing strength distant past close optimal 
course discussion assumes active resetting visual system inspection different objects 
eye movements accompany orientation new object great active suppression caused complete change inputs reaching visual system transient feedback inhibition produced large visual input produced re orientation 
resetting objects help operation model described model means needs operates trace fixed duration general just shorter average time stimulus inspected 
trace rule seen play crucial role successful learning object invariant responses worth considering major tenets learning apply primate visual system 
firstly possible process different images object ms object may viewed secondly learning take place rapidly cortex order time 
tenets appear satisfied 
ms sufficient time visual stimulus process different images object rolls thorpe 
learning new representations appears occur little presentations new stimuli faces seen rolls 
third final tenet learning means previous cellular activity affect learning visual cortex 
question considered researchers rolls ak rhodes montague reilly johnson summarised section trace rule earlier 
point raised discussion trace learning rule appropriate ventral visual system concerned invariant form representation dorsal visual system motion location processed mishkin ungerleider 
importance having trace rule part visual system involved invariant object recognition importance having rule part visual system involved processing motion location fundamental reason keeping processing streams apart 
appendix measure network performance experiments neuron response varies function location rotation stimulus type 
simple measure invariant response neuron shifts location stimulus simply average variance location stimulus response profiles flat variance low 
addition distinctiveness response particular stimulus gauged average variance response stimuli responses stimulus different allow easy discrimination stimulus groups variance high 
variance left unaccounted denote error variance acts measure reliability variances measured low response neuron consistent location stimulus 
ideal translation invariant highly discriminating neuron determined seeking high value ratio generally low value error describes combine error variance measure derive measure relative amount information rai factors 
formula measured variance regard error number stimuli number locations 
converting rai measures direct quotient corrected variances taken fc gamma nl gamma error nl gamma nl gamma error number stimulus classes nl number examples stimulus stimulus class sample variance location sample variance error sample error fc correction factor attractive quality measure evaluates simply correction factor fc case cell responds stimulus location 
formula fc cases studied slightly different fc 
authors grateful peter ak anonymous reviewers help advice preparing manuscript 
guy wallis supported serc whilst conducting experimental described 
abbott rolls 
representational capacity face coding monkeys 
cerebral cortex press 
anderson rosenfeld 
eds 

neurocomputing foundations research 
cambridge mit press 
rolls leonard 
selectivity faces responses population neurons cortex superior temporal sulcus monkey 
brain research 
bennett 
large competitive networks 
network 
brown keenan 
hebbian synapses biological mechanisms algorithms 
annual review neuroscience 
buhmann lades von der malsburg 
size distortion invariant object hierarchical graph matching 
pages international joint conference neural networks 
new york ieee 
buhmann lange von der malsburg wurtz 
object recognition dynamic link architecture parallel implementation transputer network 
pages kosko 
ed neural networks signal processing 
englewood cliffs new jersey prentice hall 
bulthoff edelman 
psychophysical support dimensional view interpolation theory object recognition 
pages proceedings national academy science usa vol 


size location invariance visual system 
perception 
chakravarty 
generalized line junction labeling scheme applications scene analysis 
ieee transactions pami april 
desimone 
face selective cells temporal cortex monkeys 
journal cognitive neuroscience 
ak 
learning invariance transformation sequences 
neural computation 
ak 
models sensory coding 
tech 
rept 
cued infeng tr 
university cambridge department engineering 
fukushima 
neocognitron self organizing neural network model mechanism pattern recognition unaffected shift position 
biological cybernetics 
hasselmo rolls 
object centred encoding face selective neurons cortex superior temporal sulcus monkey 
experimental brain research 
parker 
spatial properties monkey striate cortex 
proceedings royal society london 
hinton 
parallel computation assigns canonical object frames 
proceedings th international joint conference artificial intelligence 
reviewed rumelhart 
hummel biederman 
dynamic binding neural network shape recognition 
psychological review 
koenderink van doorn 
internal solid shape respect vision 
biological cybernetics 
linsker 
basic network principles neural architecture 
proceedings national academy sciences usa 
logothetis bulthoff poggio 
view dependent object recognition monkeys 
current biology 
marr 
vision 
san francisco freeman mel 
combining color shape texture histogramming neurally inspired approach visual object recognition 
unpublished manuscript 
miyashita chang 
neuronal correlate pictorial short term memory primate temporal cortex 
nature 
montague edelman 
spatial signalling development function neural connections 
cerebral cortex 
nass cooper 
theory development feature detecting cells visual cortex 
biological cybernetics 
oja 
simplified neuron model principal component analyser 
journal mathematical biology 
anderson van essen 
neurobiological model visual attention invariant pattern recognition dynamic routing information 
journal neuroscience 
reilly johnson 
object recognition sensitive periods computational analysis visual 
neural computation 
perrett 
visual cells responsive faces 
trends neurosciences 
perrett oram benson 
organisation functions cells responsive faces temporal cortex 
philosophical transactions royal society london 
poggio edelman 
network learns recognize dimensional objects 
nature 
rhodes 
open time channel facilitates self organisation invariant object responses cortex 
society neuroscience abstracts 
rolls 
neurophysiological mechanisms underlying face processing temporal cortical areas 
philosophical transactions royal society london 
rolls 
learning mechanisms temporal lobe visual cortex 
behavioural brain research 
rolls 
processing speed cerebral cortex neurophysiology visual masking 
proceedings royal society london 
rolls ramachandran 
visual learning reflected responses neurons temporal visual cortex macaque 
society neuroscience abstracts 
rumelhart hinton williams 
learning internal representations error propagation 
chap 
rumelhart mcclelland 
eds parallel distributed processing vol 
foundations 
cambridge massachusetts mit press 
cochran 
statistical methods 
edn 
ames iowa iowa state university press 
sutton barto 
modern theory adaptive networks expectation prediction 
psychological review 
tanaka saito 
coding visual images objects inferotemporal cortex macaque monkey 
journal neurophysiology 
tarr pinker 
mental rotation orientation dependence shape recognition 
cognitive 
thorpe 
biological constraints connectionist models 
pages pfeifer fogelman soulie 
eds connectionism perspective 
london john wiley sons 
rolls 
translation invariance response faces single neurons temporal visual cortical areas alert macaque 
journal neurophysiology 
mishkin 
cortical visual systems 
pages goodale mansfield 
eds analysis visual behaviour 
cambridge massachusetts usa mit press 
ungerleider 
human brain 
current opinion neurobiology 
van essen anderson 
information processing primate visual system integrated systems perspective 
science 
von der malsburg 
self organization orientation sensitive cells striate cortex 
kybernetik 
reprinted anderson rosenfeld 
von der malsburg 
correlation theory brain function 
tech 
rept 

department neurobiology max planck institute biophysical chemistry 
von der malsburg schneider 
neural cocktail party processor 
biological cybernetics 
wallis 
neural mechanisms underlying processing visual areas occipital temporal lobes 
ph thesis department experimental psychology oxford univeristy 
www ftp ftp mpg de pub guy ps wallis 
optimal unsupervised learning invariant object recognition 
submitted review 
www ftp ftp mpg de pub guy nc ps wallis rolls ak 
learning invariant responses natural transformations objects 
pages international joint conference neural networks vol 

