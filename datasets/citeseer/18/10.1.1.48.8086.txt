boosting naive bayesian learning charles elkan department computer science engineering university california san diego la jolla california elkan cs ucsd edu technical report 
cs september 
version may 
called naive bayesian classification unrealistic assumption values attributes example independent class example learning method remarkably successful practice uniformly better learning method known 
boosting general method combining multiple classifiers due yoav freund rob schapire 
shows boosting applied naive bayesian classifiers yields combination classifiers representationally equivalent standard feedforward multilayer perceptrons 
ancillary result naive bayesian classification nonparametric nonlinear generalization logistic regression 
training algorithm boosted naive bayesian learning quite different backpropagation definite advantages 
boosting requires linear time constant space hidden nodes learned incrementally starting important 
realworld datasets method tried far generalization performance better best published result learning method 
standard learning algorithms naive bayesian learning boosting done logarithmic time linear number parallel computing units 
accordingly learning methods highly plausible computationally models animal learning 
arguments suggest plausible behaviorally 
called naive bayesian classification optimal method supervised learning values attributes example independent class example 
assumption violated practice shown naive bayesian learning remarkably effective practice difficult improve systematically domingos pazzani shows possible improve reliably generalization ability naive bayesian classifiers technique called boosting due freund schapire 
argues boosting naive bayesian classification computationally plausible model animal human learning 
specifically shows adaboost algorithm freund schapire combine naive bayesian classifiers resulting combination mathematically equivalent feedforward neural network sparse encoding inputs single hidden layer nodes sigmoid activation functions 
learning algorithm boosting combination naive bayesian learning important advantages 
generalization ability boosted naive bayesian classifiers excellent 
real world example datasets discussed datasets discussed classifier gives better test set accuracy known method including backpropagation decision trees 
second classifiers learned efficiently 
training examples attributes time required learn boosted naive bayesian classifier ef linear 
learning algorithm examines training data faster 
constant factor small 
modern workstation boosted naive bayesian classifier learned examples dimension minute time devoted processing strings dataset 
review naive bayesian learning attributes discrete values predict discrete class example observed attribute values optimal prediction class value maximal 
bayes rule probability equals background probability base rate estimated training data easily 
example probability irrelevant decision making class value learning reduced problem estimating training examples 
bayes rule class conditional probability written delta recursively second factor written delta 
suppose assume outcome independent outcome formally assume equals delta delta delta factor product estimated training data count count shown equation gives maximum likelihood probability estimates probability parameter values maximize probability training examples 
induction algorithm explained called naive bayesian learning earliest known author chapter celebrated perceptrons book minsky papert 
naive bayesian classifiers perceptrons logistic regression suppose just possible classes called observed attribute values test example 
gamma 
normalizing constant 
logarithms gives log gamma log log gamma log log gamma log letting log gamma log log gamma log gives log gamma gamma gamma exponentiating sides rearranging gives gamma gammab general suppose attribute possible values 
jj log jj gamma log jj 
gamma jj jj gammab indicator function oe oe true oe oe false 
equation describes exactly functioning perceptron sigmoid activation function sparse encoding attributes separate input possible values attribute naive bayesian classifiers equivalent representational ability perceptrons localist input representations 
course maximum likelihood learning equation general gives parameter values different obtained training methods 
possible show naive bayesian classification generalizes logistic regression widely statistical method probabilistic classification numerical attribute values 
consider equation 
discrete values count computed directly training examples 
values numerical standard practice quantize discretize attribute 
simplest quantization method divide range attribute fixed number equal width bins 
experiments described chosen arbitrarily 
previous experimental shown benefits complex quantization methods small best dougherty bins equal width tends allow non parametric approximation skewed multimodal heavy tailed probability distributions 
numerical integer valued real valued attribute 
logistic regression assumes model log equation describes functioning perceptron sigmoid activation function single input node attribute attribute values encoded magnitude 
discretization corresponds replacing linear term piecewise constant function range divided intervals ith interval gamma ji function ji gamma ji ji constant 
combining equations yields version equation 
naive bayesian classification nonparametric nonlinear extension logistic regression standard logistic regression equation approximated setting ji gamma ji boosting general idea boosting learn series classifiers classifier series pays attention examples misclassified predecessor 
specifically learning classifier boosting increases weight training examples misclassified learns classifier reweighted examples 
process repeated rounds 
final boosted classifier outputs weighted sum outputs individual weighted accuracy training set 
formally adaboost algorithm freund schapire follows 
example round ffl weights obtain hypothesis 
ffl error ffl jy gamma ffl fi ffl gamma ffl fi gammah ffl normalize sum 
suppose individual classifier useful ffl 
fi jy gamma increases increases algorithm satisfies intuitive description 
experiments variants adaboost algorithm show details critical success basic idea misclassified examples important practice 
proposal freund schapire final combination hypothesis fi gamma linear combination individual classifiers log fi log fi show boosted naive bayesian classifier representationally equivalent multilayer perceptron single hidden layer 
ff fi vote classifier log fi log ff ff gamma log ffv gammalog ff log fi gamma log fi words output combined classifier applying sigmoid function weighted sum outputs individual classifiers 
individual naive bayesian classifiers equivalent perceptrons combined classifier equivalent perceptron network single hidden layer 
derivation shows boosted naive bayesian learning viewed alternative training algorithm feedforward neural networks 
argument extended cover feedforward networks multiple hidden layers 
similar algorithms tiling method nadal incrementally create new hidden nodes correct errors previously created nodes 
computational complexity suppose examples attributes values 
naive bayesian classifier equation fv parameters 
parameters learned accumulating fv counts 
attribute value training example leads incrementing exactly count 
examples training time ef independent time complexity essentially optimal learning algorithm examines attribute value training example worse complexity 
comparison learning decision tree pruning requires ef time 
algorithm underlies result non trivial 
time required update weights boosting ef rounds boosting time total ef constant 
accumulating counts naive bayesian classifier training examples may processed sequentially directly disk tape storage 
amount random access memory required fv independent number training examples 
boosting random access memory needed 
boosted naive bayesian classifiers wellsuited knowledge discovery large databases databases remain disk tape need fit main memory 
straightforward show learning boosted naive bayesian classifier parallel complexity class nc 
class consists problems solved polylogarithmic time polynomial number processors 
communication patterned binary tree height log processors compute count count log time 
naive bayesian learning requires log time processors 
practice constant examples allocated processor parallel implementation far fewer processors run log time 
constant number rounds boosting boosted naive bayesian learning nc 
literature search reveals practical learning problem known nc algorithm 
training simple linear threshold neural nets optimally np complete blum rivest experimental results important stress experiments described exploratory nature 
datasets selected yield particularly informative results 
german credit dataset 
dataset positive negative examples 
seventeen features discrete continuous 
best published prediction error rate naive bayesian classifier fold cross validation friedman goldszmidt boosting improves performance datasets dataset boosting increases prediction error rate quinlan table shows results boosted naive bayesian learning dataset 
combining naive bayesian classifiers boosting gives error rate useful improvement 
rounds error 
table reduction test set error due boosting seen come just rounds 
experiments done far show similar phenomenon datasets usually rounds beneficial 
boosted naive bayesian learning practical anytime learning algorithm 
depending time memory available learning combination classifier may stopped number components learned 
number rounds boosting increased dataset dependent threshold error combined classifier increases slowly 
phenomenon surprising observed boosting base learning methods schapire aim explain phenomenon 
practical applications cross validation way deciding rounds boosting 
monk problems 
sebastian thrun organized comparison dozens learning algorithms synthetic datasets generated propositional formulas attributes 
formulas stated ii exactly value iii 
third dataset classification noise added 
formulas involve comparing attributes xor order relational opposed propositional flavor 
backpropagation version michalski aq builtin ability appropriate new features succeeded problems 
third problem successful methods versions aq decision tree algorithms inductive logic programming algorithms backpropagation weight decay 
boosted naive bayesian classifier succeeds third monk problem 
general boosted naive bayesian learning appears perform realistic tasks true concept resembles disjunction conjunctions noise 
failure monk problems explained qualitatively naive bayesian learning achieve accuracy xor boosting requires intermediate hypothesis including accuracy 
noise added training examples naive bayesian classifier achieve accuracy xor 
experiments investigate boosting amplify advantage 
diabetes pima indians 
dataset ripley book 
typical difficult medical diagnosis problems limited number training examples available attribute values missing examples 
naive bayesian learning attribute values unknown training example attribute values contribute counts 
implementation boosted naive bayesian learning takes advantage fact 
alternative view unknown just possible discrete value attribute 
examples pima indians diabetes dataset consist numerical attributes number glucose concentration tolerance test blood pressure mm hg skin skin fold thickness mm serum insulin micro ml body mass index kg diabetes pedigree function age years complete training examples best learning method reported ripley standard linear discrimination realizes error separate test set 
backpropagation hidden unit achieves hidden units give worse performance 
additional training examples missing data achieves error rate 
shown table complete training examples examples missing attribute values boosting rounds achieves error rate 
result substantial improvement method tried ripley shows practical importance able training examples missing attribute values 
including extra complete examples incomplete examples round error error psychological plausibility strong experimental evidence humans highly sensitive class conditional probabilities see example learning single naive bayesian classifier done incrementally computationally straightforward brain course everyday experience 
learning boosted naive bayesian classifier done past experiences 
boosting involves paying attention training examples classified incorrectly previously plausible precisely examples accessible memory 
behavioral level naive bayesian learning boosting plausible human animal learning method 
ffl method tolerate irrelevant attributes values attributes uncorrelated class example log odds weights attached attribute values tend zero number training examples increases 
ffl multiplicative updating counts additive updating target concepts change time tracked efficiently effectively 
ffl changing initial weights training examples changing probability threshold decisions different costs different misclassification errors may vary example example taken account optimal manner 
extended version contain detailed exposition naive bayesian learning boosting better model human learning behavior backpropagation 
discussion show naive bayesian learning reproduces phenomena human category learning discussed kruschke 
particular naive bayesian learning performs better called filtration tasks condensation tasks garner humans find filtration tasks easier backpropagation works equally tasks types 
corresponding mathematical phenomenon backpropagation hidden layer learns decision plane equally regardless rotation naive bayesian learning accurate optimal decision plane axis parallel 
sejnowski expressed standard opinion wrote biological significance claimed algorithm back propagation network developed 
contrary naive bayesian learning plausible 
nutshell type learning done logarithmic time linear number neurons incrementally constant time example fixed sparse connections neurons low precision connection strengths neurons feedback connections sort associative memory 
major assumption naive bayesian learning sparse localist encoding attributes stimulus dimensions 
encoding called population code quite plausible domains 
example rats place cells fire animal specific spatial location localist neural representations environmental features 
humans face recognition may localist features gabor transforms small facial regions fixed positions relative important note claim humans animals implement adaboost algorithm precisely 
claim computationally feasible low level brain processes implement generally similar algorithm 
little known operation memory learning neural level way progress understanding phenomena identify algorithms meet basic constraints computational feasibility 
computational complexity class nc useful approximation computational capacity low level brain processes 
neurons synapses neuron human brain may possess information processing elements elements relatively slow 
learning methods require sublinear processing time linear number computing units plausible intrinsically sequential methods methods sequential time complexity 
formally speaking cognitively plausible low level brain mechanisms solving problems nc plausible solving problems lie outside nc damasio damasio 
deciding advantageously knowing advantageous strategy 
science february 
blum rivest blum rivest 
training node neural network npcomplete 
neural networks 
fenton kaminsky 
place cells place navigation 
proceedings national academy sciences january 
domingos pazzani domingos pazzani 
independence conditions optimality simple bayesian classifier 
proceedings thirteenth international conference machine learning pages 
morgan kaufmann publishers 
dougherty dougherty kohavi sahami 
supervised unsupervised discretization continuous features 
proceedings twelfth international conference machine learning pages 
morgan kaufmann publishers 
freund schapire yoav freund robert schapire 
decision theoretic generalization line learning application boosting 
proceedings second european conference computational learning theory pages 
friedman goldszmidt nir friedman moises goldszmidt 
building bayesian networks 
proceedings national conference artificial intelligence pages portland oregon august 
aaai press distributed mit press 
garner garner donna 
effect goodness encoding time visual pattern discrimination 
perception psychophysics 
biederman von der malsburg 
predicting face recognition spatial filter similarity space 
personal communication 
kruschke john kruschke 
human category learning implications backpropagation models 
connection science 
sejnowski sejnowski 
network model shape neural function arises receptive projective fields 
nature 
nadal nadal 
learning feedforward layered networks tiling algorithm 
journal physics 
minsky papert marvin minsky seymour papert 
perceptrons computational geometry 
mit press 
quinlan john quinlan 
boosting bagging 
proceedings national conference artificial intelligence pages portland oregon august 
aaai press distributed mit press 
ripley brian ripley 
pattern recognition neural networks 
cambridge university press 
schapire schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
proceedings fourteenth international conference machine learning 
morgan kaufmann publishers 
appear 
