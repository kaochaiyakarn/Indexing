structural regression trees stefan kramer austrian research institute artificial intelligence vienna austria mail stefan ai univie ac keywords inductive learning inductive logic programming regression real world domains task machine learning algorithms learn theory predicting numerical values 
particular standard test domains inductive logic programming ilp concerned predicting numerical values examples relational non determinate background knowledge 
far ilp algorithm predict numbers cope non determinate background knowledge 
exception covering algorithm called fors 
structural regression trees srt new algorithm applied class problems integrating statistical method regression trees ilp 
srt constructs tree containing literal atomic formula negation conjunction literals node assigns numerical value leaf 
srt provides comprehensible results purely statistical methods applied class problems ilp systems handle 
experiments real world domains demonstrate approach competitive existing methods indicating advantages expense predictive accuracy 
research sponsored austrian fonds zur der wissenschaftlichen forschung fwf number mat 
financial support austrian research institute artificial intelligence provided austrian federal ministry science research arts 
real world machine learning domains involve prediction numerical value 
particular test domains inductive logic programming ilp including mesh data sets problem learning quantitative structure activity relations concerned prediction numerical values examples relational background knowledge 
kind learning problem called relational regression formulated normal ilp framework part non monotonic ilp framework includes closed world assumption 
relational regression differs ilp learning tasks negative examples 
far methods applied problem fors builds order theory covering algorithm 
approach combination transforms learning problem propositional language subsequently applies regression tree algorithm transformed problem 
transformation non determinate background knowledge strict limitation approach 
structural regression trees srt new algorithm predicting numerical values examples relational background knowledge 
srt differs fors divide conquer conquer works decision tree algorithm covering algorithm 
contrast srt solves problem original representation require transforming problem 
srt utilize non determinate background knowledge 
simplify presentation review statistics machine learning related approach 
third section describe method including solution problem non determinate literals 
furthermore new method detecting outliers analogy 
subsequently discuss results experiments real world domains 
draw sketch possible directions research 
related classical statistical model prediction numerical values linear squares regression 
refinements extensions non linear models known real world applications 
regression models limitations regression models hard understand 
secondly classical statistical methods assume features equally relevant parts instance space 
thirdly regression models allow easy utilization domain knowledge 
way include knowledge engineer features map symbolic features real valued features 
order solve problems regression tree methods cart developed 
regression trees supposed comprehensible traditional regression models 
furthermore regression trees definition partition instance space features may different importance different parts space 
basic idea regression trees cart minimize squared error split node tree predict average dependent variable covered training instances unseen instances leaf 
differ assign single values leaves linear regression models 
sophisticated post pruning methods developed cart method grows tree leaf pure leaf covers exactly instance 
regression tree resulting growing phase usually bigger classification tree takes nodes achieve pure leaves 
kate learns decision trees examples represented frame language equivalent order predicate calculus 
kate extensive hierarchy heuristics generate branch tests 
knowledge kate system induce order theories divide conquer fashion 
watanabe rendell investigated divide conquer learning order theories 
called structural decision trees prediction categorical classes continuous classes closest literature 
description method overview srt algorithm learns theory prediction numerical values examples relational non determinate background knowledge 
algorithm constructs tree containing literal atomic formula negation conjunction literals node assigns numerical value leaf 
precisely srt generates series increasingly complex trees subsequently returns generated trees preference criterion 
srt preference criterion minimum description length mdl principle 
way try avoid overfitting data presence noise 
construction single tree srt uses method usual top induction decision trees 
algorithm recursively builds binary tree selecting literal conjunction literals defined user defined schemata node tree stopping criterion fulfilled 
selected literal conjunction examples covered node partitioned depending success failure literal example 
selection literal conjunction performed follows set training instances covered leaf partial tree conjunction literals path root tree 
definition terms see table 
possible test evaluated resulting partitioning training instances instances partitioned instances proving succeeds instances proving fails 
possible test calculate sum squared differences actual values training instances average possible tests srt selects minimizes sum squared differences see equation 
stopping criterion fulfilled average assigned leaf predicted value unseen cases reach leaf 
sum squared errors gamma set instances covered leaf partial tree conjunction literals path root tree subset proving set possible tests succeeds subset proving fails number instances ji number instances ji value dependent variable training instance value dependent variable training instance average instances average instances table definition terms point view path starting root seen clause 
time tree extended literal conjunction clauses generated current clause path current partial tree extended respective literal conjunction literals 
clause current clause extended negation literal 
table shows simple example structural regression tree clausal form 
clauses predict biological activity compound structure characteristics 
depending conditions clauses theory assigns unseen instance 
clausal view process 
activity drug struct drug group group group pi group 

activity drug struct drug group group group pi group 
group 
activity drug struct drug group group group pi group 
group 
table example structural regression tree clausal form simplest possible stopping criterion decide grow tree srt stops extending clause literal produce clauses cover required training instances 
parameter called minimum coverage clauses theory 
apart stopping criterion minimum coverage parameter benefits direct control complexity trees built 
smaller value parameter complex tree allow specific clauses tree 
way generate series increasingly complex trees return optimizes preference function 
furthermore solution prevents splits asymmetric parameter allows certain degree minimum coverage controls balanced resulting trees 
srt generates series increasingly complex trees varying minimum coverage parameter 
algorithm starts high minimum coverage decreases iteration iteration 
fortunately iterations skipped change certain values minimum coverage parameter literals conjunctions produce clauses admissible coverage select yields lowest sum squared errors 
literals conjunctions yielding lower sum squared errors coverage low 
maximum coverage literals conjunctions value parameter tree different current tree 
choose value minimum coverage 
srt returns tree series obtains best compression data 
compression measure minimum description length mdl principle discussed section 
tree selection mdl mdl principle tries measure simplicity accuracy particular theory common currency terms number bits needed encoding theory data 
defines message length theory called model article total message length message length describe model message length describe data model 
way complex theory need bits encoded save bits encoding data correctly 
message length model consists encoding literals encoding predicted values leaves 
message length data model simply encoding errors 
predicted values errors real numbers devise suitable coding scheme reals 
coding scheme turn integers multiplication rounding 
factor minimum integer allows discern values training data rounding 
subsequently integers encoded universal prior integers way coding length numbers roughly corresponds magnitude 
encoding tree simply encoding choices respective literals tree built 
single node encode choice possible literals encoding considers predicates possible predicates 
chose mdl cross validation computationally expensive pruning search 
planning compare methods model selection 
non determinate background knowledge literals non determinate introduce new variables bound alternative ways 
non determinate literals introduce additional parts structure adjacent nodes graph 
examples part predicates 
clearly non determinate literals immediately reduce error added clause construction 
greedy algorithm look ahead ignore non determinate literals 
problem introduce non determinate literals controlled manner 
srt user specify literal may extend clause 
firstly user define conjunctions literals limited look ahead 
user defined schemata similar relational 
furthermore user constrain set possible literals depending body clause far 
conditions body arbitrary prolog clauses user possibilities define language antecedent description grammars 
reduce number possibilities set literals conjunctions constrained modes types variables variable symmetries 
outlier detection analogy test instances outliers strongly deteriorate average performance learned regression models 
usually detect test instances outliers little information available task 
relational background knowledge available lot information utilized detect analogy test instances outliers 
intuitively new prediction check test instance similar training instances covered clause fires 
seen test instance consider different prediction suggested clause succeeds instance 
case interpret regression tree defining hierarchy clusters 
srt chooses cluster similar test instance predicts average cluster test instance 
implement kind reasoning analogy define similarity relational structures labeled graphs 
simple approximation similarity properties structures 
context say instance property iff literal conjunction permitted specified schemata immediately succeeds succeeds intermediate variables 
similarity defined follows instance denote set properties instance common set properties instances set common 
similarity test instance set cluster training instances similarity jp instance common jp common similarity simply defined number properties test instance covered training instances common divided number properties training instances common 
srt uses parameter minimum similarity determine similarity test instance training instances covered clause fires large 
way detecting handling outliers adds instance aspect srt 
just additional possibility turned means minimum similarity parameter 
defined similarity measure order logic measures similarity tuples relation relational structures 
experimental results common step pharmaceutical development forming quantitative structure activity relationship relates structure compound biological activity 
domains inhibition escherichia coli test srt 
dataset consists background facts instances compounds partitioned cross validation sets 
background knowledge facts instances compounds perform fold cross validation 
hirst comprehensive comparisons methods domains concluded statistically significant difference methods 
experiments set minimum similarity 
table shows results methods compared results srt 
table summarizes test set performances domains measured spearman rank correlation coefficients 
spearman rank correlation coefficient measure order test instances target variable correlates order predicted induced theory 
reason hirst spearman rank correlation coefficient say average error compare golem predict numbers methods 
srt performs better methods improvement statistically significant 
hirst emphasize difference spearman rank correlation coefficient required data set size 
comparatively performance srt due detection outliers recognized methods 
outliers ones identified domains 
dataset srt performs quite differences statistically significant 
spearman rank correlation coefficient measure quantitative error prediction included measures proposed quinlan 
clearly measures disadvantages represent interesting aspects theory works test set 
unfortunately full comparison methods capable predicting numbers 
tables contain cross validation test set performances srt test domains terms spearman rank despite disadvantage golem hirst state golem method provides understandable rules drug receptor interactions 
srt seen step integrating capabilities 
note differences standard deviations high 
hypothesis equal standard deviations rejected bonferroni adjustment perform analysis variance results 
method 
mean oe 
mean oe lin 
regr 
parameters squares lin 
regr 
attributes squares 
netw 
parameters squares 
netw 
attributes squares golem srt table summary methods biomolecular domains inhibition performances measured spearman rank correlation coefficients measure accuracy 
mean oe 
mean oe 
mean oe spearman rank corr 
coeff 
average error jej correlation relative error re table performances srt domains terms accuracy measures correlation coefficient terms accuracy measures 
furthermore performed experiments domain finite element mesh design details see background knowledge non determinate 
table shows results srt mesh dataset results fossil results methods directly taken 
srt performs better foil worse methods 
statistical analysis shows differences foil algorithms significant 
struct 
foil golem milp fossil fors srt sigma table summary numbers percentages correctly classified edges domain finite element mesh design foil golem foil milp foil fors results test paired test significant 
foil foil srt paired test shows difference significant 
foil paired test highly significant bonferroni adjustment 
foil fossil test shows significance 
applied srt biological problem learning predict mutagenic activity chemical harmful dna 
details see 
domain involves non determinate background knowledge 
table compiled results filled result srt 
table refers structural background knowledge ns refers non structural features ps refers predefined structural features utilized propositional algorithms mdl refers mdl pre pruning 
note results fors best parameters 
experiments instances compounds fold cross validation 
accuracy concerns problem predict chemical active 
srt learns theory predicts activity number evaluate different way compare results 
summing experiments showed domain srt competitive differences srt rest statistically significant 
applied srt domain trying predict surface water aqueous hours 
simplify learning task discretized quantity mapped 
background knowledge non determinate molecular weight global features available 
dataset contains chemicals performed fold cross validation tests 
results srt table 
srt algorithm tested data results appear indicate instances find generalizations 
interestingly srt outlier detection see column improves initial result srt domain column 
note propositional algorithm cart algorithm handle non determinate background knowledge foil golem applied problem 
sum experiments srt turned quantitatively competitive main advantages yields comprehensible explicit rules predicting numbers non determinate background knowledge 
research structural regression trees srt new algorithm applied learning problems concerned prediction numbers examples relational non determinate background knowledge 
srt viewed integrating statistical method regression trees ilp 
srt applied class problems ilp systems fors handle provides comprehensible results purely statistical methods 
main difference srt fors tree covering algorithm 
advantages disadvantages known algorithm accuracy lin regr 
ns lin regr 
ns ps neural network ns neural network ns ps cart ns ps cart ns foil ns progol ns fors ns fors ns mdl srt ns table summary accuracy systems mutagenicity domain measure accuracy mean oe mean oe spearman rank correlation coefficient average error jej correlation relative error re table performances srt outlier detection domain algorithms types apply 
experiments real world domains demonstrate approach competitive existing methods indicating advantages applicability relational regression non determinate background knowledge comprehensibility rules expense predictive accuracy 
srt generates series increasingly complex trees currently iteration starts scratch 
planning extend algorithm parts tree iteration reused iteration 
plan compare way coverage tree selection mdl traditional pruning methods la cart 
addressed problem non determinate literals 
adopted generalized solutions problem involve tiresome task writing new specification admissible literals conjunctions domain 
think generic solution application method easier 
current limitations approach constants discussion tree vs covering algorithms ilp refer 
comparison tree induction rule induction propositional regression see 
assigned leaves linear models 
help build accurate models steps assign linear regression models leaves 
johannes furnkranz bernhard pfahringer gerhard widmer valuable discussions 
wish saso dzeroski providing dataset 
bisson learning fol similarity measure proc 
tenth national conference artificial intelligence aaai 
bostrom covering vs divide conquer top induction logic programs proc 
fourteenth international joint conference artificial intelligence ijcai pp 
san mateo ca 
morgan kaufmann 
breiman friedman olshen stone classification regression trees wadsworth statistics probability series wadsworth international group belmont ca 
cheeseman finding probable model computational models discovery theory formation eds langley morgan kaufmann los altos ca 
cohen grammatically biased learning learning logic programs explicit antecedent description language artificial intelligence 
bratko finite element mesh design engineering domain ilp application proceedings fourth international workshop inductive logic programming ilp gmd studien nr 
pp 

dzeroski numerical constraints learnability inductive logic programming ph dissertation university ljubljana ljubljana 
dzeroski bratko handling noise inductive logic programming proceedings international workshop inductive logic programming tokyo japan 
dzeroski 
personal communication 
dzeroski handling real numbers inductive logic programming step better behavioural clones machine learning ecml eds lavrac wrobel pp 
berlin heidelberg new york 
springer 
furnkranz fossil robust relational learner machine learning ecml eds bergadano de raedt pp 
berlin heidelberg new york 
springer 
hirst king sternberg quantitative relationships neural networks inductive logic programming 
inhibition journal computer aided molecular design 
hirst king sternberg quantitative relationships neural networks inductive logic programming inhibition journal computeraided molecular design 
employing linear regression regression tree leaves proc 
tenth european conference artificial intelligence ecai ed neumann pp 
chichester uk 
wiley 
order regression ph dissertation university ljubljana ljubljana 
lavrac dzeroski inductive logic programming ellis horwood chichester uk 
knowledge intensive induction proceedings sixth international workshop machine learning ed pp 

morgan kaufman 
muggleton feng efficient induction logic programs inductive logic programming ed muggleton academic press london 
pfahringer kramer compression evaluation partial determinations proceedings international conference knowledge discovery data mining 
aaai press 
quinlan learning logical definitions relations machine learning 
quinlan learning continuous classes proceedings ai ed sterling adams pp 
singapore 
world scientific 
quinlan programs machine learning morgan kaufmann san mateo ca 
quinlan case study machine learning proceedings sixteenth australian computer science conference 
rissanen modeling shortest data description automatica 
rissanen stochastic complexity modeling annals statistics 
silverstein pazzani relational constraining constructive induction relational learning machine learning proceedings eighth international workshop ml eds birnbaum collins pp 
san mateo ca 
morgan kaufmann 
srinivasan muggleton king comparing background knowledge inductive logic programming systems proceedings th international workshop inductive logic programming ilp pp 

katholieke universiteit leuven 
srinivasan muggleton king sternberg mutagenesis ilp experiments non determinate biological domain proceedings fourth international workshop inductive logic programming ilp gmd studien nr 
pp 

watanabe rendell learning structural decision trees examples proc 
twelfth international joint conference artificial intelligence ijcai pp 
san mateo ca 
morgan kaufmann 
weiss indurkhya rule machine learning methods functional prediction journal artificial intelligence research 
