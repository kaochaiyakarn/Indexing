bayesian radial basis functions unknown dimension holmes mallick department mathematics imperial college queen gate london sw bz march bayesian framework analysis radial basis functions rbf proposed readily accommodates uncertainty dimension model 
distribution defined space rbf models basis function posteriors computed reversible jump markov chain monte carlo samplers green 
alleviates need select particular architecture modeling process 
show resulting models relatively free user set design parameters exhibit performance characteristics number benchmark test series 
keywords radial basis functions variable architecture selection bayesian neural networks reversible jump markov chain monte carlo 
interested regression target response output dependent variable input covariate predictor independent set variables data set data pairings xn 
take regression curve surface conditional mean function recorded observations corrupted gaussian noise ffl ffl ffl independent identically distributed oe 
concerned approximation estimate form represents basis function hidden node 
approximators type include radial basis functions multi layer perceptrons mlps multivariate adaptive regression splines mars generalized additive models mixture models powell ripley friedman hastie tibshirani titterington 
selecting value number hidden nodes essential part modeling process holds considerable consequences predictive performance 
greater number nodes greater flexibility model 
design issues addressed maximum likelihood ml approach tend yield high variance solutions exhibit poor generalization 
selecting method required accurately consistently compare models different dimension order prevent fitting data 
approaches commonly adopted 

penalized likelihood 
ml method evidence favour simpler models largest model considered tend chosen 
authors considered adding penalty term deviance twice log likelihood model making comparisons 
typically penalty dimension model roughness response curve surface 
referred structural stability regularization penalties respectively 
structural penalty usually form dimension model number data points 
common choices include information criterion aic bayesian information criterion bic akaike schwarz 
regularization penalties second differential operators 
dimensional input data takes form dx defines domain value governs strength roughness penalty response 
note setting value optimization process ml solutions lead see green silverman details 

true predictive assessment 
apparent error rate deviance twice log likelihood model training data invariably biased downwards 
assuming true predictive performance sample error criteria model choice try assess rank choices 
number techniques aimed correcting bias apparent error rate evaluating true error rate directly 
methods bootstrap cross validation see ripley details 
having effective model choice criterion redundant know instance models test 
problem model adequacy 
method different models selection procedure usually experience preference individual modeler 
automated methods stepwise procedures selection model proceeds alongside determination parameter values lebiere platt roberts 
approach backward selection utilises idea parameter saliency see example williams le cun consider training determined models performing parameter pruning criterion inverse hessian error function respect network parameters 
higher level question select just model combine forecasts composite prediction long documented history reports see wolpert jacobs min zellner review genest includes annotated bibliography 
note respects choice combine classical frequentist framework appears little misplaced combining results single model number autonomous submodels open model choice criterion original set 
problem model adequacy formulating set plausible models remains 
take bayesian approach define joint probability distribution model parameters model dimension 
suitably adjusted markov chain monte carlo methods integrate uncertainty model dimension parameter values dimensions 
suppresses need select single model set models comparison averaging 
expectations predictive distributions calculated space 
section provide overview previous bayesian approaches neural networks 
shown difficulties surrounding classical approaches architecture selection elegantly handled adopting bayesian framework 
radial basis functions underlie models briefly discussed section 
bayesian inference involves integration markov chain monte carlo mcmc methods form important tool approximating integrals interested 
introduce mcmc highlight applicability integration analytically intractable high dimensional densities 
describe green reversible jump markov chain monte carlo analysis neural networks true number hidden nodes known 
number benchmark test series including robot arm data set analysed mackay yule sunspot data weigend empirical evidence support claim models computed method show predictive performance 
bayesian neural networks application bayesian statistics done remove ad field neural networks mackay neal 
mackay illustrated bayesian analysis provides significant benefits conventional methods 
advantages include inter alia distributions predictions error bars confidence intervals automatic setting regularization structural penalties prior smoothness model output magnitude network parameters see bishop excellent overview 
central process bayesian framework calculation probability distributions unknown parameter weight vectors 
prior knowledge say small weights updated light experimental data 
posterior distributions calculate expectations target interest take account uncertainty parameter values 
ripley refers predictive approach 
point forecast target random variable current input expectation conditioned training set build model represents parameterization model 
expectation written represents posterior probability parameters training data compare classical plug method represents parameters model set optimum value 
comparing clearly highlights difference classical bayesian methodologies prediction 
bayesian analysis involves integrating uncertainty parameter values whilst classical methods involves optimization parameters noted bishop 
previous approaches bayesian neural networks models architecture fixed prior data analysis small number models analysed averaging selection clearly tend produce sub optimal solutions best sense model happens tested data approximated architecture pre selection single model interpreted discrete prior model space allocates mass probability dimension particular architecture tested 
desire number bayesian model choice criteria exist analogous section assist model selection key 
method uncertainty dimension model space included computations method lets data prior knowledge posses determine complexity solution 
green proposed novel markov chain sampling method bayesian computation accommodates uncertainty dimension parameter space 
green reversible jumps utilise markov chains switch dimensions time exploring parameter space particular dimension richardson green denison 
method practical applications particularly suited non parametric regression techniques input data initially undergoes nonlinear transformation set basis functions variable dimension output recovered final level mapping target domain see 
analysis follows shall consider radial basis functions powell methods adopt generic readily applicable types networks including mlps 
bayesian rbf modal radial basis functions model class radial basis functions 
methods developed function interpolation extended approximation regularization see girosi review 
basis function parameterised knot position prototype centre vector located covariate space dimension data set approximated position vectors usually set covariate points drawn random replacement training set 
name radial basis derives basis response function distance current input basis location 
model output classical rbf model linear combination basis functions low order polynomial term oe gamma fi jm delta denotes distance metric usually euclidean mahalanobis jm represents polynomial degree coefficients fi calculated squares fit data matrix pseudoinverse techniques 
theory radial basis functions specifies different permissible forms function oe take 
acceptable basis types corresponds priori assumption smoothness true regression function approximated girosi 
common choices include ffl linear oe ffl cubic oe ffl thin plate spline oe logz ffl multiquadric oe ffl inverse multiquadric oe gamma ffl gaussian oe exp empirical evidence suggest non local bases oe perform better local basis functions addition sensitive user set basis parameters franke lowe 
choose analyse cubic multiquadric spline functions 
note include linear term low order polynomial model 
strict interpolation data set set number basis functions number training points impose constraint solving fi case ensures networks response exactly match target values training set 
interpolation rarely goal analysing majority real world data sets tend corrupted measurement noise fail record salient independent variables 
setting results overfitting poor sample performance 
broomhead lowe suggested approximating form radial basis networks reduction dimension results simpler models greater bias variance 
extreme recover standard linear model 
think radial basis functions modeling departure data linear plane 
bayesian analysis described allows extent uncertainty departure determined data regardless particular choice basis function type nonlinearity data 
task set joint probability density model space basis function 
rbf model uniquely determined number basis functions corresponding knot positions basis parameter recombination coefficients choose random term fixed coefficients fi determined squares fit data 
parameter space theta written countable union subspaces theta theta theta subspace euclidean space denotes theta parameter space position vector dimensional 
represent data unknowns viewed naturally hierarchical structure writing joint probability distribution delta delta denotes conditional distribution 
improve flexibility reduce impact user set parameters extend structure include extra layer allowing prior number basis functions depend hyperparameter fl 
joint distribution written fl fl fl fl fl interested inference joint distribution conditional data 
forecasts model 
new datum models prediction calculated expectation typically joint distribution high dimensional multi modal 
integral analytically intractable untenable asymptotic methods 
turn specially constructed mcmc samplers approximate integrals kind 
markov chain monte carlo reversible jump samplers quite common bayesian statistics faced problem integrating density analytical asymptotic methods reasonably handle 
markov chain monte carlo methods provide valuable tool circumstances bernardo smith tierney besag 
conceptually simple easy implement 
say wish calculate integral form dw mcmc methods proceed drawing samples direct proportion approximating total number samples generated chain length burn period 
burn ensures markov chain generating sample converged stationary distribution interest 
aim generate samples direct correspondence posterior probability 
metropolis hastings algorithm provides elegant technique achieve goal metropolis hastings 
initial sample point drawn 
taken necessarily prior 
candidate point chain proposed current point randomly generated vector ffl drawn possibly symmetric proposal distribution say gaussian 
ffl ffl drawn oe hat indicates currently proposal state 
proposed sample accepted probability ff min probability probability proposing move note proposal ratio equals symmetric 
accepted set set current point new sample forms starting point proposed state algorithm iterated large number samples drawn 
updated state just function previous state referred markov chains 
independent samples appearing probability distribution interest obtained discarding initial portion chain ensure chain converged th sample remove correlations tierney 
markov chain monte carlo methods mainly restricted densities fixed dimension 
green developed mcmc method samples unknown dimension 
reversible jump algorithm allows moves propose change dimension model 
iteration algorithm selects move proposal number differing move types propose change dimension inclusion move type referred hybrid sampler tierney 
green able show new sampler satisfied criteria ensure markov chain stationary distribution density interest 
density interest posterior probability density model conditioned data 
section describe reversible jump algorithm detail 
derive necessary likelihood prior terms needed bayesian computation posterior densities 
rbf likelihood priors stated data assumed generated relationship ffl represents unknown regression function ffl noise term normal distribution oe oe unknown 
leads log likelihood observed data additive constant gamma oe gamma vague proper gamma prior gamma gamma gamma precision oe gamma represents ignorance noise process 
simulation update value oe gibbs sampling gelfand smith 
knot locations drawn randomly replacement set data points dimension model poisson prior mean parameter gamma 
prior number basis functions model 
practice truncated max restrict centres lie points data set 
low value indicate preference simpler models fewer basis functions 
recall value indicates linear fit data 
mean parameter controls complexity smoothness approximation 
rarely apparent priori expected number basis functions 
setting prove problematic 
assume linear term approximation quite strong prior preference small value single value hard justify 
circumvented giving prior captures ignorance mean 
poisson mean conjugate gamma prior gamma ff fi 
exact setting ff fi discussed sections note invariably values lead strongly subjective prior section describe reversible jump algorithm underlies bayesian radial basis functions 
reversible jump radial basis functions wish sample joint distribution reversible jump mcmc 
previously described algorithm proceeds augmenting usual proposal step conventional metropolis hastings mcmc method number possible move types surrounding change dimension density 
iteration addition possibility attempting move particular parameter subspace sampler propose jump dimension 
refer jumps birth death steps respectively 
steps attempted probabilities related current dimension model minf minf probabilities choosing birth death step model dimension prior taken 
parameter dictates relative frequency move proposals attempt jump dimension sample current subspace 
models set value critical see green 
common jumping dimensions generate random vector augments current state form model vector just datum position location vector drawn random points basis function located 
accordance conventional metropolis hastings methods proposed move jump current state accepted rejected probability ff min fi fi fi fi fi fi fi fi fi fi probability choosing jump type current state see density function final term jacobian arises change variables radial basis model rewrite ff min likelihood ratio theta prior ratio theta proposal ratio jacobian required drawing new location vector independently current parameters ratio equal 
illustrate method consider birth step 
death steps just involve inversion ratios 
prior ratio birth just prior birth ratio taken term represents probability choosing basis locations data points 
birth proposal ratio proposal ratio prob 
death move theta prob 
deleting basis prob 
birth move theta prob 
creating basis gamma likelihood ratio taken 
ratios give formula proposed changes dimension model 
moves subspace proposed probability gamma proposal acceptance algorithm simple basis function current model chosen random location parameter moved new setting drawn randomly points data set basis functions prototypes set 
prior proposal ratio just move accepted probability minf likelihood 
reversible jump algorithm algorithm written pseudo code follows 
draw initial poisson prior mean gamma prior gamma ff fi 
draw initial model dimension prior 
set radial basis centres points drawn random data set replacement 

calculate fi squares technique 

iterate convergence assumed 
draw uniform random variable propose state chain follows perform birth step ii 
elseif perform death step iii 
perform move step 
recalculate coefficients matrix 
draw uniform random variable ff ff accept proposed state ii 
set state current state 
draw noise variance oe gibbs sampling step full conditionals oe gamma gamma gamma gamma number training points sum squared residuals current model see section 
draw poisson mean gibbs step full conditionals gamma ff fi current number basis functions model see section 
repeat move birth death steps simple 
move just selects basis function random resets location vector random point 
birth adds basis function randomly selected point data set contain 
death just selects basis random removes 
output algorithm markov chain stationary distribution 
initial portion chain discarded ensure convergence th sample predictions 
usually involves inspection statistics markov chain mean values sampled parameters see tierney details heuristics 
performance test sets section provides empirical results support claim reversible jump bayesian rbf models provide accurate predictions wide variety test sets 
model user set parameters ff fi define gamma hyper prior poisson mean number basis functions 
mean gamma distribution ff fi variance ff fi tests choose ff fi stated 
represents strongly subjective prior poisson mean expected value basis functions 
shown model fairly insensitive parameters 
better results obtained tuning parameters empirical bayesian approach part data optimize hyperparameters 
choose theoretical justification order universal approach modeling procedure 
simulations run burn period iterations reversible jump mcmc algorithm followed samples th calculations 
runs took minutes minutes dec alpha depending size data set complexity problem 
simple scatter plot smooth introduce methodology set scene consider simple scatter plot analysed fan gijbels 
data generated function covariate points uniformly gamma gamma ffl ffl 
data rescaled input domain lies 
true function data set shown 
thin plate spline function approximation 
shows predictions samples drawn randomly chain burn period 
clearly variance response 
final response shown taken average models proposed post burn decimated chain 
shows model prediction alongside true function 
predictor response true regression curve noise corrupted data set data generated gamma ffl predictor target realisations mcmc sample predictions samples mcmc chain predictor target predictive estimate bayesian thin plate spline final predictive response predictor target true response solid bayesian rbf prediction dashed true function estimate fraction variance unexplained fit 
calculated gamma gamma models prediction true value function mean true function test set 
test set evaluated regular points input domain 
bivariate test functions turn results non linear functions analysed hwang 
study multi layer perceptrons projection pursuit 
functions follows 
simple interaction function gamma gamma 
radial function gamma gamma gamma 
harmonic function gamma gamma gamma 
additive function gamma gamma sin gamma gamma sin gamma 
complicated interaction function sin gamma gammax sin accordance approach data points generated unit square response calculated ffl true test function ffl gaussian white noise drawn distribution 
test set comes generating data points grid unit square 
table lists results units 
hwang methods list best results obtained multi layer perceptron mlp hidden units projection pursuit pp model hidden functions hermite polynomials 
include test results bayesian radial basis models cubic thin plate spline multiquadric basis functions basis parameter set 
mode number basis functions mcmc chain included brackets alongside error value 
mode chain gives indication region marginalised mcmc chain converged 
function mlp pp cubic mq ps simple radial harmonic additive complex table results test set bracketed values modes gives indication functions easily approximated rbf network 
clear reversible jump sampler converges different regions model dimension space different problems different basis functions 
placing hyper prior expected number basis functions rely natural embodiment occam razor bayesian methods exhibit see mackay discussion point berger informal overview 
table shows bayesian rbfs comparable hwang projection pursuit method data sets better mlp 
note values represents best results 
models universal hyper prior data sets 
shows mcmc thin plate spline function simple function 
seen chain samples models varying dimension recorded mode 
iterations mcmc thin plate spline simple function reversible jump mcmc chain thin plate spline robot arm data set data set mackay web site task model mapping dimensional joint angle position arm 
true relationship cos cos ffl sin sin ffl ffl oe oe 
neal compares bayesian multilayer perceptron computed hybrid mcmc duane wol ra phy cam ac uk mackay average squared error gaussian approximation method mackay solution highest evidence solution lowest test error hybrid mcmc neal super transitions best runs neal method super transitions best runs bayesian radial basis functions cubic table results robot arm data set mackay chooses approximate integral predictive expectations gaussians fitted local modes 
neal tests bayesian models model 
models differ number super transitions performs see neal details 
essentially neal uses gradient information help speed mcmc algorithm number super transitions related step sizes sampling algorithm 
tested cubic radial basis function data 
results table 
mode number basis functions chain brackets 
bayesian rbf appear comparable slightly accurate bayesian multi layer perceptrons data set 
notice mode number basis functions data set higher runs data set indicating model appropriate level problem 
test sensitivity model user set parameters ff fi ran number simulations data set varying values run 
results table 
clearly model starts show signs fitting hyper prior vague allowing model freedom 
performance model strongly subjective values low dimension departure linear fit appears fairly insensitive exact setting ff fi 
ff fi test error mode marginalised table sensitivity model ff fi method ff fi mode training error test set second test set weigend mlp na na mq mq mq table results sunspot data set yule sunspots yule sunspot time series served benchmark data set number statistical models weigend tong lim 
data represents yearly averages sunspots dark patches sun recorded years 
weigend trained multi layer perceptron records 
model evaluated test sets representing years 
data sets generated lagging years values inputs years sunspot value target autoregressive ar process 
compare bayesian models weigend tested mq basis parameter set 
results table 
terms 
bayesian networks show considerable improvements performance second test set slightly worse values set 
summary bayesian approach radial basis functions dimension number basis functions things know 
specially constructed markov chain reversible jumps able draw inference true dimension model 
offers complete bayesian approach previous traditionally architecture true model assumed known absolute certainty models tested contain true model 
apparent approach considered fully bayesian fail account uncertainty values coefficients fi fit maximum likelihood 
ml estimates advocated pragmatic grounds fairly straight forward include gibbs sampling step draw coefficients 
include sampling input dimensions jump steps 
appropriate knowledge suggest covariates irrelevant 
addition dimension low order polynomial inferred simulation 
user set parameters model ff fi defining hyper prior mean poisson prior represents penalty term models numbers basis functions value 
priori users strong subjective opinions expectation number radial basis functions analysed data 
motivated inclusion hyper prior 
small value ff implies vague prior data sets analysed appears allow freedom 
unreasonable strong prior preference small value indicating assumption linear term provide approximation 
course user strong reason believe model higher dimension example data high dimensional known highly nonlinear readily accommodated increasing value ff rescaling fi accordingly 
stated method reversible jumps suited nonlinear models data initially undergoes nonlinear transformation basis hidden layer models include radial basis functions mars mlps 
applicability method case mlps cautioned 
mars rbfs considered multi layer perceptrons tunable parameters sigmoidal basis layer 
results strong interactions posterior probability density space different parameters different basis functions 
turn tends result large changes likelihood model basis function removed 
consider example removing node mlp hyperbolic tangent basis functions 
equivalent setting parameters node zero 
parameters model independent drastically effect global likelihood 
strong dependence zeroing node parameter values invariably cast parameters regions lower probability 
due likelihood term death steps rarely accepted parameters tuned data 
higher likelihood current model death steps accepted 
note adding basis functions result decrease likelihood 
authors currently looking ways overcome problem mlps 
promising technique method known simulated tempering geyer thompson probability density heated cooled giving jumps better chance accepted 
presence strong parameter interaction mlps caused problems traditional mcmc techniques models fixed dimension 
dependencies result long chains required order sample properly posterior density space see neal details possible solutions 
acknowledgments authors wish acknowledge helpful comments denison derivation conditional distribution poisson mean current dimension model see step section 
author assisted epsrc research award sponsored water research centre united kingdom 
bernardo smith 
bayesian theory 
wiley besag green higdon mengersen 
bayesian computation stochastic systems 
stat 
sci vol 
pp 

bishop 
neural networks pattern recognition 
oxford 
broomhead lowe 

multivariate functional interpolation adaptive networks 
complex systems vol 
pp 

denison mallick smith 
bayesian tech 
report 
department mathematics imperial college london 
duane kennedy 
hybrid monte carlo 
physics letters vol 
pp 

fahlman lebiere 
cascade correlation learning architecture 
advances neural information processing systems touretzky ed 
vol 
pp 

morgan kaufmann 
fan gijbels 

local polynomial modelling applications 
chapman hall 
franke 
scattered data interpolation tests methods 
mathematics computation vol 
pp 

friedman 
multivariate adaptive regression splines discussion annals statistics 
vol 
pp 

gelfand smith 
sampling approaches calculating marginal densities 
journal american statistical association vol 
pp 

genest 
combining probability distributions critique annotated bibliography 
statistical science vol 
pp 

geyer thompson 
annealing markov chain monte carlo applications ancestral inference 
journal american statistical association 
vol 
pp 

girosi 
regularization theory radial basis functions networks 
statistics neural networks cherkassky friedman wechsler eds 
springer verlag 
girosi jones poggio 
regularization theory neural networks 
neural computation vol 
pp 
green 
reversible jump markov chain monte carlo computation bayesian model determination 
biometrika vol 
pp 

green silverman 

nonparametric regression generalised linear models 
london hall 
hastie tibshirani 
generalised additive models 
london chapman hall 
hastings 
monte carlo sampling methods markov chains applications 
vol 
pp 

jacobs 

methods combining experts probability assessments 
neural computation vol 
pp 

berger ockham razor bayesian analysis 
american scientist vol 
pp 
key 

studies simulation approach bayesian model comparison unpublished phd thesis 
department mathematics imperial college london 
le cun denker solla 
optimal brain damage 
advances neural information processing systems touretzky ed 
vol 
pp 

morgan kaufmann 
lowe 
nonlocal non positive definite basis functions radial basis function networks 
proc 
th int 
conf 
artificial neural networks iee conference publication number pp 

mackay 
bayesian interpolation 
neural computation vol 
pp 

mackay 
practical bayesian framework networks 
neural computation vol 
pp 

metropolis rosenbluth rosenbluth teller teller 
equations state calculations fast computing machines 
chem 
vol 
pp 

min zellner bayesian non bayesian methods combining forecasts applications forecasting international growth rates 
econometrics 
vol pp 

neal 
bayesian learning neural networks 
springer 
platt 
resource allocating network function interpolation 
neural computation vol 
pp 

powell 
radial basis functions multivariate interpolation review 
algorithms approximation mason cox eds 
pp 

oxford clarendon press 
richardson green 
bayesian analysis mixtures unknown number components 
statist 
soc 
appear 
ripley 
pattern recognition neural networks 
cambridge 
roberts tarassenko 
probabilistic resource allocating network novelty detection 
neural computation vol 
pp 

tierney 
markov chains exploring posterior distributions 
ann 
statist vol 
pp 

titterington smith makov 
statistical analysis finite mixture distributions 
chichester wiley 
tong lim 
threshold autoregression limit cycles cyclical data 
statist 
soc 
vol 
pp 

weigend huberman rumelhart 
predicting sunspots exchange rates connectionist networks 
nonlinear modelling forecasting casdagli eubank eds 
addison wesley 
williams 
bayesian regularization pruning laplace prior 
neural computation vol 
pp 

wolpert 
stacked generalization 
neural computation vol 
pp 

