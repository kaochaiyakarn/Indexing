communication free parallelization affine transformations amy lim monica lam computer systems laboratory stanford university stanford ca email cs stanford edu 
describes parallelization algorithm programs consisting arbitrary nestings loops sequences loops 
code produced algorithm yields degrees communication free parallelism obtained loop fission fusion interchange reversal skewing scaling reindexing statement reordering 
algorithm assigns iterations instructions program processors affine processor mappings generates correct code ensuring code executed processor subsequence original sequential execution sequence 
previous research vectorizing parallelizing compilers shown parallelization improved host high level loop transformations 
loop transformations include loop fission loop distribution loop fusion loop interchange loop reversal loop skewing loop scaling loop reindexing known loop alignment index set shifting statement reordering 
focus parallelizing compiler research devise algorithms combine transformations achieve specific goals 
loop interchanges reversals skewing combinations thereof modeled unimodular transformations 
algorithms unimodular transformations tiling improve parallelism locality loops dependences represented distance direction vectors developed 
framework desired combination loop transformations finding suitable unimodular matrix mechanically generate desired spmd single program multiple data code 
major limitations unimodular loop transformation approach operations iteration treated indivisible unimodular transformations apply non perfectly nested loops 
unimodular transformations achieve effects obtained loop fission fusion scaling reindexing 
research supported part darpa contract dabt nsf young investigator award 
shows algorithm combine unimodular transformations loop fusion fission scaling reindexing optimize programs arbitrary nesting loops sequences loops 
shown transformations modeled affine transforms researchers trying find algorithms exploit model effectively 
show affine framework solve important problem parallelization 
synopsis key ideas 
problem formulation optimizing parallelism communication simultaneously 
experience suggests minimizing communication critical achieving scalable parallel performance 
finding parallelism available program just find parallelism fully utilize hardware system incurring 
solves important subproblem goal maximize degree communication free parallelism 
say computation degrees parallelism executes gammap time 
scope algorithm comprehensive suite transformations direction distance vectors 
algorithm applies programs arbitrary nestings loops sequences loops bounds large array accesses affine functions outer loop indices variables 
compilers today represent program dependence information distance direction vectors 
algorithm manipulates affine access functions directly powerful algorithms distance direction vectors 
optimization algorithm algorithm finds maximum degree parallelism assigning computation processors generating spmd code 
observe possible tell particular parallelization communication free simply knowing iterations assigned processor necessary know identity processor iterations assigned 
insight suggests algorithm determine computation partitioned determine particular assignment 
generating spmd code processor assignment relatively straightforward generate correct spmd code 
code executed processor subsequence sequential execution 
contrast code generation problem complicated approached problem finding execution time processor mappings 
details discussed section 
problem statement observe efficient hand parallelized computations largescale mimd multiple instruction multiple data machines synchronize communicate inner loops 
trying find parallelism program reduce communication ask question opposite direction 
approach find maximum amount parallelism available communication allowed 
parallelism available insufficient introduce communication synchronization frequently parts computation obtain parallelism 
focuses subproblem approach maximize amount parallelism available program requires communication 
measure amount parallelism degree defined section 
example interested loop quantization increases amount parallelism constant factor 
goal algorithm find set affine processor mappings instructions program maximizes degree parallelism program 
instruction find affine mapping maps iteration instruction virtual processor 
suppose find degrees parallelism program iterations loop 
algorithm maps independent threads computation dimensional virtual processor space 
threads distributed physical processors system 
easy derive data mappings computation mappings determined 
data mappings obtained approach piece wise affine functions different regions array may mapped processors different affine mappings 
affine computation mappings exploit parallelism achievable series loop fission fusion interchange reversal skewing reindexing statement reordering 
affine mappings express parallelization schemes algorithms generate affine mappings may able achieve maximum degree parallelism possible 
example may able increase degree parallelism assigning piece wise affine processor mapping instruction 
code transformation terms need split index set schedule different subsets iterations different affine mapping 
affine mappings limitation may lead large number idle processors processors computation assigned 
affect utilization hardware cause poor performance 
example instructions code degrees parallelism 
delta delta delta delta delta delta delta delta delta exploit parallelism affine mappings need map iterations instruction surface dimensional processor array 
mappings case majority processors processor array assigned computation 
performance parallelization scheme unacceptable 
fortunately possible avoid creating idle processors piece wise affine mappings 
ideally find best piece wise affine mappings find parallelism uniform affine mappings avoid creation idle processors 
straightforward algorithm partition index set iterations sets uniform data access relationships 
unfortunately technique prohibitively expensive 
presents efficient algorithm find parallelism possible uniform affine mappings introduces piece wise mappings necessary avoid creating idle processors capable finding parallelism uniform affine processor mappings cases 
algorithm outline components algorithm highlighting rationale explaining solve problem 

program partitioned slices share common data standard data dependence tests 
different slices executed order 
steps apply slice 

determine processor mapping instruction slice 
observe possible tell particular parallelization communication free simply knowing iterations assigned processor important know identity processor iterations assigned 
break step substeps 
partition computation sets iterations executed processor 
partition described nullspace linear transformation portion affine processor mapping 
goal satisfy constraints partitioning communication free optimizing degree parallelism 
determine processor mappings 
important decisions partitioning determined 
step finds set processor mappings satisfies partitioning decision 

generate spmd code processor mappings 
communication necessary processors execute independently 
spmd program correct long processor executes assignment operations order appear original sequential execution 
example introduce example illustrate steps algorithm 
gamma gamma iteration space deep loop nest shown assuming 
iteration space divided rectangular regions region corresponds iteration loop nest 
iteration pair white black circles represents instance instruction instance respectively 
arrows represent data dependences 
easy see computation partitioned independent threads 
show algorithm creates parallel code 
fig 

iteration space data dependences example 
partition constraints synchronization data communication required data referenced computation local processor computation executed 
communication free computation partition satisfy constraint expressed definition 
definition 
xjr th array access function array instruction iteration space processor mapping instruction computation partition said communication free xjr gamma 
definition 
jk ik function instruction maps iteration set iterations refers data 
jk xjs function data dependence relationship distinguish read write operations care relative execution order instructions 
concept functions rewrite definition definition 
definition 
jk function instruction computation partition said communication free jk notation kb denote bth component iteration vector example 
array access functions deep loop nest example gamma gamma gamma similarly gamma function maps iteration source instruction set iterations target instruction 
illustrated example set target iterations expressed union subsets target iteration space described set simple equations 
equations forms 
expresses relationship source loop indices target loop indices restricts source target loop indices constant value 
simple example equations relating sets loop indices functions convey information distance vectors 
objective function affine mappings optimize maximum degree parallelism program finding maximum degree parallelism instruction finding consistent set mappings exploit degree parallelism instruction 
assign affine mapping instruction maximizing degree parallelism instruction means maximizing degree parallelism maximizing rank matrix minimizing dimensionality nullspace goal minimize dimensionality nullspace want compute minimal nullspace affine mappings 
recall communication free partition processor mappings satisfy constraints definition 
definition iterations mapped processor gamma nullspace affine mapping nullspace linear combination nullspace definition 
minimal localized iteration space instruction denoted minimal set independent column vectors spanning space gamma constrained mapped processor definition 
describe find minimal localized iteration spaces introduce term definition 
graph nodes dynamic iterations instructions edges connect nodes sharing relationships 
simple cycle cycle begins ends iterations instruction intermediate iterations cycle come different instructions 
indirect self function instruction denoted maps iteration iterations reachable simple cycle minimal localized iteration space minimum set column vectors satisfying conditions 
single instruction map iterations instruction data directly indirectly processor 
gamma span 
multiple instructions iterations instruction mapped processor iterations accessing data accessed mapped processor 
jk jk gamma span gamma 
gamma span straightforward algorithm compute minimal localized iteration space find values initialize condition iterate condition terms converge 
expensive compute terms 
fortunately optimize algorithm observing ultimate goal find necessary compute space constraints impossible include full details optimized algorithm 
briefly algorithm initializes values functions kk uses iterative step combine effects indirect self functions affine mapping constraints terms time 
follow straightforward iterative algorithm find example 
compute example gamma gamma gamma gamma algorithm uses initialize initialize condition 
initialized gamma space spanned similarly initialized algorithm iterates condition 
considering intuitively step finds iterations instruction located data accessed instance mapped processor 
condition ae fi fi fi fi gamma oe ae fi fi fi fi oe ae fi fi fi fi gamma oe ae fi fi fi fi oe gamma span gamma 
gamma span condition satisfied initial need increase algorithm considers ae fi fi fi fi oe ae fi fi fi fi gamma oe ae fi fi fi fi oe ae fi fi fi fi gamma oe gamma span gamma 
gamma span condition satisfied initial addition necessary applying iterative step discover initial remains unchanged desired solution 
maximum degree parallelism instruction computed minimal localized iteration space dim gamma dim dim gamma dim dim gamma dim 
finding processor mappings limit processor mappings uniform affine mappings virtual processor array may need dimensions parallelism order exploit parallelism 
introducing piece wise affine transforms necessary dimensionality virtual processor array simply maximum maximum degrees parallelism instructions 
max sk algorithm finds affine piece wise affine processor mappings constraints satisfied 

instruction nullspace 
pair instructions jk algorithm starts arbitrarily choosing instruction set instructions largest degree parallelism finding affine mapping constraint satisfied 
decision propagated partially constrain affine mappings instructions share data accesses possible new constraints mapping instruction satisfied uniform affine function 
shown possible split index set instruction find piece wise affine mapping functions satisfy constraints 
algorithm repeats process affine mappings 
note calculating mapping non integer entries result 
mappings relative eliminate rational entry multiplying mappings denominator rational number 
continue example compute number dimensions virtual processor array 
max 
choose mapping instruction largest degree parallelism 
suppose choose algorithm finds affine mapping constraint satisfied row span orthogonal space span span 
suppose algorithm sets theta gamma propagate decision instruction share data accesses constraints gamma gamma theta expand set constraints gamma gamma gamma solution satisfies constraints gamma 
find affine mapping theta gamma 
unspecified processor mapping algorithm terminates 
generating code processor mapping instruction need generate spmd code 
computations different processors completely disjoint code correct long code executed processor subsequence original sequential execution 
words generate spmd code guarding instruction predicate checks processor execute instruction 
spmd code example gamma gamma gamma gamma easy see code correct code generated manner extremely inefficient 
algorithm eliminate dynamic tests innermost loops 
find tight loop bounds instruction program 
step irigoin polyhedron scanning code generation technique 
loop index variables surrounding instruction set linear inequalities describing loop bounds instruction create new set inequalities affine processor mapping instruction case piece wise mappings apply index set splitting generate subset instances 
generate loop bounds instances executed particular processor projecting loop indices away reverse order original loop indices 

merge subsequences instruction 
denote elements iteration vector suppose instructions share common loops 
merged code execute instance statement instance statement lexicographically statement appears lexically illustrate code generation algorithm example 
step finds tight loop bounds instructions separately 
loop nests need merged step merge gamma gamma max min gamma gamma gamma max min gamma gamma gamma second step merges subsequences 
starts outermost index follows gamma gamma gamma merge max min gamma gamma max min gamma gamma gamma range partitioned intervals set statements executed possible values interval 
recursively reapply algorithm indices inner levels 
final code example gamma gamma gamma gamma max min gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma related lot code optimizations parallel machines 
restrict discussion compiler techniques linear affine transforms 
significant body research uses affine mappings performed context systolic arrays 
original loop indices mapped affine function processor time domain 
virtual processor space mapped hardware directly dependences program mapped communication typically neighboring connections systolic array 
feautrier uses affine model schedule instructions maximum parallelism 
approach schedule instructions maximize parallelism minimize communication 
scheduling step finds instruction affine mapping time domain 
feautrier refers computations gamma degrees parallelism computations execute linear time 
feautrier algorithms find optimal piece wise affine scheduling functions computations complete linear time 
computations complete linear time models time multi dimensional space heuristics schedule instructions 
note general hierarchical loop structures faithfully modeled need modeled hierarchical time space 
problem generate code schedules resolved 
contrast approach focuses minimizing communication processor assignment 
find affine processor mappings instructions explicitly represent time mappings 
communication allowed problem addressed need specify relative ordering instructions executed processor 
correctness guaranteed making sure instructions executed processor subsequence original sequential execution 
spmd code processor inherits original hierarchical loop structure original program 
huang sadayappan studied problem finding partitioning 
finding maximum degree parallelism program algorithm finds dimension parallelism 
program domain restricted sequence perfectly nested loops instructions loop body scheduled indivisible units 
algorithm achieve effects obtainable loop fusion fission reindexing find kind parallelism algorithm 
unimodular loop transforms map sequential loop nest legal sequential loop nest 
unimodular loop transforms attempt transform code outermost possible loops parallelizable 
outermost loop parallelizable algorithm communication free partition loop nest 
code form easily translated desired spmd code processor simply executes code nested parallel loop 
words processor mapping implicitly indices parallelized loop 
sense unimodular transforms map original loop indices processor time domain 
ideas derived anderson lam algorithm minimizes communication multiple loops 
particular approach finding nullspace processor mappings determining rest mappings 
major differences algorithms 
anderson lam ignore neighborhood displacement communication 
anderson lam algorithm transforms individual loop nests unimodular transforms loop fission finds data mapping 
algorithm integrates transforms powerful 
algorithm treat loop body indivisible unit distance direction vector abstraction find parallelism 
anderson lam algorithm maps data processors affine mapping set data mappings permitted algorithm larger mappings piece wise affine 
anderson lam algorithm complete introduce communication computation necessary desirable 
plan combine ideas algorithm 
contribution formulation compiler optimization problem 
recommend compilers maximize parallelism minimize communication simultaneously 
asking parallelism program ask parallelism exploited amount communication synchronization 
proposes algorithm solve subproblem approach find parallelization scheme 
second contribution show algorithm combine unimodular transformations loop fusion fission scaling reindexing statement reordering optimize programs arbitrary nestings loops sequences loops 
relationship code transformations affine transformations established developing algorithms framework met difficulties 
difficult issues finding affine time schedule instructions program determine dimensionality time model time arbitrarily nested program generate spmd code 
algorithm find communication free parallelization answer issues 
developed algorithm finds degrees parallelism instruction achievable affine processor mapping 
dimensionality time computation degrees parallelism simply gamma 
show constructive algorithm program way degrees parallelism instruction exploited 
second algorithm explicitly represent time mapping instruction 
recognize time domain fundamentally irregular focus finding processor mappings 
third code generated subsequence original program 
efficient code produced straightforward manner processor assignment 
combination ideas creates parallelization algorithm capable performing complex code transformations 
interesting observe compiler internal representation parallelization scheme simple generated code substantially different source 
code transformations achieved ad hoc approaches 

allen callahan kennedy 
automatic decomposition scientific programs parallel execution 
proceedings th annual acm symposium principles programming languages munich germany january 

allen kennedy 
automatic translation fortran programs vector form 
acm transactions programming languages systems october 

amarasinghe lam 
communication optimization code generation distributed memory machines 
proceedings sigplan conference programming language design implementation june 

irigoin 
scanning polyhedra loops 
proceedings third acm sigplan symposium principles practice parallel programming pages april 

anderson lam 
global optimizations parallelism locality scalable parallel machines 
proceedings sigplan conference programming language design implementation june 

torres 
partitioning statement iteration space non singular matrices 
proceedings acm international conference supercomputing july 

banerjee 
speedup ordinary programs 
phd thesis university illinois urbana champaign october 

banerjee 
unimodular transformations double loops 
proceedings third workshop programming languages compilers parallel computing pages august 

banerjee 
loop transformations restructuring compilers 
kluwer academic 

carr kennedy 
compiler numerical algorithms 
proceedings supercomputing pages november 

feautrier 
efficient solution affine scheduling problem part ii multidimensional time 
int 
parallel programming december 

feautrier 
efficient solutions affine scheduling problem part dimensional time 
int 
parallel programming october 

feautrier 
automatic distribution 
technical report institut blaise pascal laboratoire december 

huang sadayappan 
communication free hyperplane partitioning nested loops 
journal parallel distributed computing 

kelly pugh 
framework unifying reordering transformations 
technical report cs tr university maryland april 

kennedy mckinley 
optimizing parallelism data locality 
proceedings acm international conference supercomputing pages july 

kennedy mckinley 
maximizing loop parallelism improving data locality loop fusion distribution 
proceedings sixth workshop programming languages compilers parallel computing august 

sarkar thekkath 
general framework iteration reordering loop transformations 
proceedings sigplan conference programming language design implementation pages june 

torres 
align distribute linear loop transformations 
proceedings sixth workshop programming languages compilers parallel computing august 

wolf 
improving locality parallelism nested loops 
phd thesis stanford university august 
published csl tr 

wolf lam 
loop transformation theory algorithm maximize parallelism 
transactions parallel distributed systems october 

wolfe 
optimizing supercompilers supercomputers 
mit press cambridge ma 

wolfe 
massive parallelism program restructuring 
symposium frontiers massively parallel computation pages october 
article processed macro package llncs style 
