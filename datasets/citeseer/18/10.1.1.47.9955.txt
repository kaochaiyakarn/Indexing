efficient algorithm function approximation hinging hyperplanes 
departamento de ias das ons de universidade de vigo vigo spain mail tsc es presents computationally efficient algorithms function approximation hinging hyperplanes 
approximant units added time method fitting residual 
fit individual unit solve sequence quadratic programming problems approach proven offer significant advantages derivative search algorithms 
empirical results synthetic data illustrate main characteristics algorithms 

universal approximation property feed forward neural networks established past decade spurred significant amount research effort aimed developing new nonlinear function approximation strategies 
difficult develop truly effective method high dimensions interesting results obtained different approaches including multilayer perceptrons radial basis functions wavelet bases 
shown election suitable basis function may help overcoming curse dimensionality associated classic approximation procedures 
result effort know continuous functions compact subsets ir uniformly approximated linear combinations kinds functions sigmoidal functions ridge functions ramps hinging hyperplanes 
know approximation problem constructive solution iterations place involve computations reduced subset approximant functions 
problem kind constructive algorithms combinatorial research supported de galicia 
search finite number feasible solutions problem solved efficiently 
propose constructive algorithm builds node time produces reasonable approximations simple efficient strategies 
basic non linear units piecewise linear nodes hinging hyperplanes hh shown useful nonlinear function approximation 
rest organized follows section describes kind constructive algorithms procedure belongs section reviews hinging hyperplane approach section studies learning problem associated hhs section shows performance algorithms synthetic data section closes discussion results obtained 

constructive algorithms 
constructive algorithms easy possess advantages alternative methods computational advantages associated constructive approach relatively easy determination suitable network size search algorithms robust parameter selection initial conditions 
potential drawbacks associated fact constructive algorithms generally produce networks minimal size 
furthermore size network allowed vary function approximated may efficiently represented model connectivity scheme imposed algorithm 
jones provided constructive solution problem finding finite convex approximations function hilbert space elements taken reduced subset 
proof result constructive provides framework development practical algorithms 
previous studied trade different parameters guarantee rate convergence pre scribed 
propose simple constructive solution new approximation residual combined previous approximation way global error minimized 
algorithms aforementioned hinging hyperplanes updated new optimization procedures 

function approximation hinging hyperplanes 
breiman proved possible hinging hyperplanes members base function expansions approximate continuous functions compact sets guaranteeing bounded approximation error ken kf gamma number nodes network radius sphere compact set contained fl fl fl fl jf results show possible achieve arbitrarily approximations function increasing number nodes network 
possible practice reason 
information function provided way sample set psi contains samples finite number points 
information set usually insufficient uniquely characterize unknown function approximation optimal 

approximate function finite size 

learning problem usually solved polynomial time 
expect obtain estimate approximates closely possible goal method 

hinging hyperplanes hinging hyperplanes hh hyperplanes joined hinge function 
main advantages hhs summarized follows 
upper bound approximation error available 

hh constitute piecewise linear model linear models proven useful large number problems 
functions nonlinear direction remaining linear useful approximation functions similar characteristics 

fast computationally efficient training algorithms 

simple efficient create store functions 
algorithms developed update hh training set 
breiman proposed hinge finding algorithm hfa implements gradient descent method partitioning data set subsets hyperplane update 
method shows important drawbacks convergence guaranteed performance method depends initial conditions final result may global minimum error 
method improve hfa uses damped newton algorithm minimize error hfa developed sj 
approach shares breiman problems spite improvements guarantee convergence method 
comes added disadvantage requiring new parameter handled user step newton method 
subsequent proposed simultaneous estimation parameters network update hyperplanes suffers drawback requiring enormous computational burden 
drawback methods resides algorithm construct network new hinge function updated residual previous approximation base functions new function added 
means new node added network previous nodes recalculated 
results lack simplicity constructive solution 
just algorithms great deal constructive solutions developed nowadays constructive simple way fourier random transforms rely backpropagation procedures modify previously connected weights 
propose barron constructive solution formulated consecutive steps compute fit residual update approximation 
ir function approximated ir initialization 
gamma fn gamma gn arg min kf gamma gamma gamma gamma fn gamma gamma gn endloop stands residual parameter depend norm previous residual selected elegant modification proposed 
problem arises second step finding function gn best fits current residual 
difficult achieve bound approximation error jones proved sufficient produce function optimum 
section review hinging hyperplane approach explain learning procedure sweeping hinge algorithm introduced horne 

learning hinging hyperplanes hh denote function hinging hyperplanes defined follows ae gamma gamma gamma gamma gamma function viewed coupled hyperplanes intersection hinge divides space different subsets fx gamma gamma gamma fx gamma gamma hinge defined terms parameter stands dimensional vector composed vectors gamma hinge line hyperplanes intersect points hinge verify equation gamma gamma 
case problem choose parameter minimizes sense error approximation function information available finite set function samples 
regression vectors denoted id training set consist pairs 
sets gamma restricted training set define partition data space 
forced disjoint including points hinge subsets problem study defined 
points hinge excluded minimization problem 
define empirical risk approximation unknown function way gamma simplify evaluation variables gamma gamma gamma gamma algebra rewrite expression compact form gamma gamma gamma gamma gamma gamma gamma defining matrices gamma gamma write rw gamma 
minimization problem say partition gamma admissible stable matrix definite positive true correlation matrix 
minimizing respect quadratic problem global minima provided definite positive 
seek squares solution rw move hinge vary initial partition longer valid expression completely useless 
set problem way minimization process take place 
solution iterative collection problems starting initial partition problem written constrained quadratic optimization min ae rw gamma oe subject aw matrix rows number samples gamma row gamma form gammax gamma gammax gamma 
moving hinge iterate algorithm procedure move hinge search optimum value 
initialize partition associated parameters fw gamma gamma ag way composed data points minimum 

refine result moving points hinge region minimum 
solve constrained quadratic optimization problem partition encountered 

move hinge stable partitions switching points recomputing set parameters time 
stable partitions measured method keeps achieves minimum 
empirical results done extensive simulations test approximation capabilities method 
section show results obtained method dimensional signals 
figures represent results obtained approximation dimensional function sin cos nodes evolution mean square error iteration 
original function 
approximation nodes 

analysis behavior method simulations come ffl minimization technique employed guarantees convergence method 
ffl depend initial conditions get trapped local minima 
ffl requires fewer parameters 
ffl computational burden comparable breiman method 
ffl purely constructive method benefit backfitting procedures 
ffl error approximation decreases noticed tail hyperplanes introduces error method compensate inclusion new nodes introducing new error zones adjusted 
greatest part error placed peripheral regions training set forcing large number nodes introduced refine approximation areas 
occasionally method produces residues symmetrically distributed zero 
happens approximation improve nodes 
residual function nodes 
number nodes 
drawbacks led look modification method cope problems encountered preserving time behavior algorithm 
exploring modification method consisting truncation hinging hyperplanes way new thh function th defined gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma ir function comprised hyperplanes joined pairwise continuously different hinge locations 
hinges defined components induce linear partitions input space divided regions 
modified sweeping algorithm include search hinges function 
preliminary results confirm predictions reduced error peripheral regions systematic errors zero 
algorithm state requires large computational burden 
current centered intelligent exploration optimal partition 

acknowledgments authors want prof providing motivation analysis tool study problem different perspective 

barron 
universal approximation bounds superpositions sigmoidal function 
ieee trans 
inf 
theory 
jones 
simple lemma greedy approximation space convergence rates projection pursuit regression neural network training 
annals statistics 
horne 
efficient algorithms function approximation piecewise linear sigmoidal networks 
technical report univ new mexico 
breiman 
hinging hyperplanes regression classification function approximation 
ieee trans 
inf 
theory 
abdallah 
constructive function approximation theory practice 
intelligent methods signal processing communications pages 
birkh auser 
sj 
hinge finding algorithm hinging hyperplanes 
technical report linkoping university sweden 
sj 
parameterization hinging hyperplane models 
technical report linkoping university sweden 
sandberg 
note error bounds approximation inner product spaces 
circuits systems signal processing 
