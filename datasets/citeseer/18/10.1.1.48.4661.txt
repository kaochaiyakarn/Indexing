appear machine learning proceedings thirteenth international conference bias plus variance decomposition zero loss functions ron kohavi data mining visualization silicon graphics shoreline blvd mountain view ca ronnyk sgi com david wolpert santa fe institute hyde park rd santa fe nm santafe edu bias variance decomposition expected misclassification rate commonly loss function supervised classification learning 
bias variance decomposition quadratic loss functions known serves important tool analyzing learning algorithms decomposition offered commonly zero misclassification loss functions kong dietterich breiman 
decomposition suffers major shortcomings potentially negative variance decomposition avoids 
show practice naive frequency estimation decomposition terms biased show correct bias 
illustrate decomposition various algorithms datasets uci repository 
bias plus variance decomposition geman bienenstock doursat powerful tool sampling theory statistics analyzing supervised learning scenarios quadratic loss functions 
conventionally formulated breaks expected cost fixed target training set size sum non negative quantities intrinsic target noise quantity lower bound expected cost learning algorithm 
expected cost bayes optimal classifier 
squared bias quantity measures closely learning algorithm average guess possible training sets training set size matches target 
variance quantity measures learning algorithm guess bounces different training sets size 
addition intuitive insight bias variance decomposition provides useful attributes 
chief fact bias variance tradeoff 
modifies aspect learning algorithm opposite effects bias variance 
example usually increases number degrees freedom algorithm bias shrinks variance increases 
optimal number degrees freedom far expected loss concerned number degrees freedom optimizes trade bias variance 
classification quadratic loss function inappropriate class labels numeric 
practice overwhelming majority researchers machine learning community expected misclassification rate equivalent loss 
kong dietterich dietterich kong proposed bias variance decomposition zero loss functions proposal suffers major problems possibility negative variance allowing values zero bias test point 
provide alternative zero loss decomposition suffer problems obeys desiderata bias variance obey discussed wolpert submitted expository primarily concerned bringing zero loss bias variance decomposition attention machine learning community 
presenting decomposition describe set experiments illustrate effects bias variance common induction algorithms 
discuss practical problem estimating quantities decomposition naive approach frequency counts frequency count estimators biased way depends training set size 
show correct estimators unbiased 
definitions notation 
underlying spaces input output spaces respectively cardinalities jxj jy elements respectively 
maintain consistency planned extensions assume countable 
assumption needed provided sums replaced integrals 
classification problems usually small finite set 
target conditional probability distribution yf yf yf valued random variable 
explicitly stated assume target fixed 
example target noise free function fixed yf value yf 
hypothesis generated learning algorithm similar distribution yh yh valued random variable 
example hypothesis single valued function classifiers decision trees value 
drop explicitly delineated random variables probabilities context clear 
example yh yh 
proposition yf yh conditionally independent test point proof yf yh yf yh yh yf yh 
equality true definition depends target test point training set set pairs values 
assumptions distribution pairs 
particular mathematical results require generated 
independently identically distributed manner commonly assumed 
assign penalty pair values loss function theta consider zero loss function defined gamma ffi yh ffi yh yh 
cost real valued random variable defined loss random variables yf yh expected cost yh yf yf zero loss cost usually referred misclassification rate derived follows yh yf gamma ffi yf yf gamma yh yf notation simplified version extended bayesian formalism ebf described wolpert 
particular results depend values test set determined need define random variable values done full ebf 
bias plus variance zero loss show decompose expected cost components provide geometric views decomposition particular relating quadratic loss euclidean spaces 
decomposition general result involving expected zero loss implicit conditioning event arbitrary 
specialize standard conditioning conventional sampling theory statistics single test point target training set size 
gamma yh yf equation gammap yh yf yh yf gammap yh yf yf yh gamma yh gamma yf rearranging terms yh yf gammap yf yh covariance yf gamma yh bias gamma yh variance gamma yf oe interested expected cost target fixed averages training sets size way evaluate quantity write equation get 
proposition independent conditions covariance term vanishes 
gamma oe bias delta bias yf gamma yh gamma yh oe gamma yf simplify exposition conditioning events implicit needs explicit 
better understand formulas note probability noise taken account fixed target takes value point understand quantity write full yh yh yh expression probability generating training set target yh probability learning algorithm guess point response training set yh average training sets generated value guessed learning algorithm point note quadratic loss decomposition involves quadratic terms log loss decomposition involves logarithmic terms wolpert submitted zero loss decomposition involve kronecker delta terms involves quadratic terms 
definitions bias variance noise obey appropriate desiderata including 
bias term measures squared difference target average output algorithm average output 
real valued nonnegative quantity equals zero yh properties shared bias quadratic loss 

variance term measures variability yh 
real valued nonnegative quantity equals zero algorithm guess regardless training set bayes optimal classifier algorithm sensitive changes training set variance increases 
distribution training sets variance measures sensitivity learning algorithm changes training set independent underlying target 
property shared variance quadratic loss 

noise measures variance target definitions variance noise identical interchange yf yh addition noise independent learning algorithm 
property shared noise quadratic loss 
contrast definition bias definitions bias fixed target instance suggested kong dietterich dietterich kong breiman valued binary classification quantify subtler levels mismatch learning algorithm target 
decompositions advantage bias zero bayes optimal classifier may 
major distinction decompositions arises variance term 
desiderata variance listed violated decompositions proposed kong dietterich dietterich kong breiman 
specifically definitions variance negative minimized classifier ignores training set 
kong dietterich note shortcoming explicitly 
examples illustrates phenomenon decomposition suggested breiman 
assume noise free target heads tails 
consider target value tails average error majority classifier slightly probability error aggregate majority classifier 
causes variance negative 
advantage decomposition terms continuous function target 
infinitesimal change target changes class commonly predicted learning algorithm cause large change bias variance noise terms 
contrast definitions bias variance share property 
bias variance decomposition vector form rewrite equation vector notation may give better geometrical interpretation decomposition 
define fh 
vectors jy components indexed values fh matrix jy theta jy denote dot products delta dot product vector squaring covariance tr fh gamma delta tr trace bias gamma variance gamma oe gamma relation quadratic decomposition relate zero loss decomposition familiar quadratic loss decomposition yf jy valued random variable restricted exactly components equals equal single valued component index yf yf re expressed vector unit hypercube 
define yh similarly terms yh yf yh real valued define quadratic loss 
particular recalling squaring vector dot product yf gamma yh ae yf yh yf yh accordingly gamma yf gamma yh delta yh yf loss transforming vector indicator variables see expected cost zero loss half associated quadratic loss 
experimental methodology description experimental methodology discuss problem naive estimation terms decomposition frequency counts 
frequency counts experiments investigate behavior terms decomposition ran set experiments uci repository murphy aha 
experiments dataset learning algorithm estimated average bias variance intrinsic noise error follows 

randomly divided dataset parts world sample training sets evaluate terms decomposition 
idea similar bootstrap world idea efron tibshirani chapter 

generate training sets generated uniform random sampling replacement 
note decomposition require training sets sampled specific manner section 
get training sets size chose size 
allows gamma delta different possible training sets guarantee get duplicate training sets set training sets small values 
ran learning algorithm training sets estimated terms equation generated classifier point evaluation set 
equation estimate jf 
terms estimated frequency counts 
left shows estimate bias different values id quinlan executed datasets uci repository 
clear estimate bias frequency counts shrinks increase infinite gives correct value bias means finite bias estimate biased upwards 
variance term exhibits opposite behavior biased low 
right hand side shows behavior corrected estimators propose subsection 
see biasing behavior arises fix demonstrate bias frequency estimator bias holds immediately implies holds averages assume define 
set training sets option estimate bias distinct sets training sets average estimates get average training set estimate 
option average training sets directly get training set estimate 
show averaged sets training sets strategy results higher estimate bias expected value estimate bias training sets larger expected value training sets 
estimate infinite sets exactly correct means estimate bias biased upwards fewer training sets 
specify sets training sets re examining 
define average training sets set ij ij th training set set see equation 
compute expected difference ways estimating bias estimating set training sets averaging minus estimating full set training sets gamma gamma gamma theta gamma gamma gamma jensen inequality cover thomas term right hand side larger equal second term 
shows average instances twice instances get smaller estimate bias establishes proposition 
similar argument holds variance gets larger grows 
unbiased estimators bias variance way correct biases frequency counts estimators large number training sets 
approach feasible experiments general combinations learning problem learning algorithm associated computational requirements prohibitive 
fortunately straight forward correction apply estimators unbiased 
define wn frequency count estimate jf training sets 
define un wn gamma defined 
frequency counts estimate bias proposed estimator vn gamma wn gamma wn gamma 
proposition assumption know value vn unbiased estimator bias proof bias un show theta gamma wn gamma wn gamma un obvious want unbiased estimator estimated quantities necessarily minimize expected error 
bias variance tradeoff 
felt getting unbiased estimator help understand problem better experiments indicated expected squared error loss proposed unbiased estimator smaller estimator naive frequency counts 
internal replicates logscale led satimage soybean internal replicates logscale led satimage soybean uncorrected bias left corrected right unbiased estimator variance probability sample bernoulli process empirical variance multiplied gamma 
furthermore fixed variance un equal variance wn var un jt var wn jn wn gamma wn gamma definition variance var un jt jt gamma un jt equating right sides equations desired result 
correction variance estimator simply negative correction bias estimator 
shown reasoning similar proof just follows bias variance decomposition fact frequency counts estimator error gives unbiased estimate zero loss 
important realize proposition implicitly assumes training sets formed sampling training set generating process 
true experimental methodology running training set generating process required produce training sets contain duplicate gamma pairs proposition apply suffices allow duplicate training sets 
conclude section comments 

assumption underlying proposition know exactly false estimate data 
estimate constant independent number training sets details learning algorithm 
errors estimate important 

related point oe difficult estimate practice 
frequency count estimator estimate oe zero instances unique regardless true oe 
uci datasets instances unique elected define oe zero considering instances unique 
viewed calculational convenience concerned variation expected cost vary learning algorithm estimate oe algorithm independent 
possibility vn give negative estimate bias expected vn unbiased 
true bias equals zero estimator variance greater zero produce zero average estimate unbiased estimator produce negative numbers 
potentially negative estimate contrasted negative variances accompanying bias variance decomposition kong dietterich breiman 
decomposition decomposition true bias variance non negative potential negative value reflection problematic aspects unbiased estimators underlying quantity estimated 
practice data negative bias estimates occurred 
experiments experiments demonstrating bias variance induction algorithms datasets uci repository murphy aha 
datasets chosen contain instances ensure accurate estimates error nn bias nn var nn bias nn var nn bias nn var anneal dna led segment satimage bias plus variance decomposition nearest neighbor varying number neighbors 
notation nn indicates vote closest neighbors 
table datasets characteristics dataset 
dataset train set features size size anneal chess dna led hypothyroid segment satimage soybean large tic tac toe demonstrate range potential bias variance behaviors 
table shows summary datasets 
small training sets sure evaluation set large 
general chose size datasets instances instances dna features 
generated training sets learning algorithm unbiased estimators discussed 
varying number neighbors nearest neighbor shows bias variance decomposition error nearest neighbor algorithm varying number neighbors 
anneal chess tic tac toe increasing number neighbors increases bias decreases variance bias increases decrease variance error increases 
anneal bias doubles going neighbor neighbors 
tic tac toe variance drastically decreases neighbors 
reason decrease variance instances tic tac toe vary squares allowing neighbors differing squares causes nn large portion space vote effect predicting majority class constant win time 
dna led bias variance decrease number neighbors increases 
dna bias shrinks variance led bias shrinks variance 
segment soybean bias variance increase number neighbors increased 
observed general increase variance number classes large 
segment classes soybean classes 
increasing number neighbors causes ties broken arbitrarily increasing variance 
hypothyroid satimage changes bias variance number neighbors changed small cancel 
hypothyroid increasing neighbors increases bias id bias id var agg bias agg var agg bias agg var anneal led segment soybean hypothyroid error bias variance id aggregated id training set 
aggregation increases bias slightly stabilizes id reducing variance 
decreases variance 
satimage bias increases variance decreases 
datasets exhibit bias variance tradeoff quantity goes goes parameter induction algorithm varied see examples change direction 
combining classifiers lot combining classifiers terms aggregation averages ensembles classifier combinations voting stacking commonly wolpert breiman perrone ali 
simplest scheme multiple classifiers generated vote test instance majority prediction final prediction 
shows id versus combination aggregated trees 
trees generated repeatedly sampling subset training set running id 
contrast bagging defined breiman samples generated uniform sampling replacement 
sample sizes shown training sets training set 
minor variations training set may cause different split change subtree 
consequence decision trees unstable usually gain aggregation techniques breiman 
note smaller internal sample bias potentially add gordon olshen different classifiers leading stable average 
results show voting scheme reduction error solely due reduction variance 
bias goes slightly especially smaller training sets samples size reduction variance significant total error usually decreases 
datasets voting samples size reduced error 
voting samples size reduced error datasets anneal 
summary bias variance decomposition misclassification error equivalent zero loss showed obeys desired criteria 
decomposition suffer shortcomings decompositions suggested kong dietterich breiman 
showed estimating terms decomposition frequency counts leads biased estimates explained get unbiased estimators 
gave examples bias variance tradeoff machine learning algorithms uci datasets 
plan investigate extensions 
particular plan investigate topics 
decomposition equation holds essentially conditioning event 
holds conditioning event bayesian point estimation bias variance decomposition bayesian quantity 
decomposition holds conditioning event external average required 
intend explore alternative conditioning events 

variance directly estimated unlabelled instances test set estimating error learning algorithm hinges estimating bias test better error estimates obtained cross validation style partitions training set estimate bias unlabelled instances test set estimate variance 
bias variance decomposition extremely useful tool statistics rarely utilized machine learning decomposition existed misclassification rate 
hope proposed decomposition overcome problem help improve understanding supervised learning 
acknowledgments jerry friedman tom dietterich discussions bias variance tradeoff 
author silicon graphics second author santa fe institute txn ali 
learning probabilistic relational concept descriptions phd thesis university california irvine 
www ics uci edu ali 
breiman 
bagging predictors technical report statistics department university california berkeley 
breiman 
heuristics instability model selection technical report statistics department university california berkeley 
breiman 
bias variance arcing classifiers technical report statistics department university california berkeley 
available www stat berkeley edu users breiman 
cover thomas 
elements information theory john wiley sons dietterich kong 
machine learning bias statistical bias statistical variance decision tree algorithms technical report department computer science oregon state university 
efron tibshirani 
bootstrap chapman hall 
geman bienenstock doursat 
neural networks bias variance dilemma neural computation 
gordon olshen 
sure consistent nonparametric regression recursive partitioning schemes journal multivariate analysis 
kong dietterich 
errorcorrecting output coding corrects bias variance prieditis russell eds machine learning proceedings twelfth international conference morgan kaufmann publishers pp 

murphy aha 
uci repository machine learning databases www ics uci edu mlearn 
perrone 
improving regression estimation averaging methods variance reduction extensions general convex measure optimization phd thesis brown university physics dept quinlan 
induction decision trees machine learning 
reprinted shavlik dietterich eds 
readings machine learning 
wolpert 
submitted bias plus variance sfi tr ftp santafe edu pub ftp bias plus ps 
wolpert 
stacked generalization neural networks 
wolpert 
relationship pac statistical physics framework bayesian framework vc framework wolpert ed generalization addison wesley 
