genetic programming feature discovery image discrimination walter tackett university southern california dept ee systems hughes missile systems mailing address ave park ca mail code cp phone mail tackett hac com apply genetic programming gp development processing tree classification features extracted images measurements set input nodes weighted combined linear nonlinear operations form output response 
constraints placed size shape order processing network 
network classify feature vectors extracted ir imagery target nontarget categories database training samples 
performance tested separate database samples 
represents significant scaling problems gp applied date 
experiments performed set input classical statistical image features minimize misclassification target non target samples 
second set experiments gp allowed form feature set primitive intensity measurements 
purposes comparison training test sets train adaptive classifier systems binary tree classifier backpropagation neural network 
gp network achieves higher performance reduced computational requirements 
contributions gp schemata subtrees performance generated trees examined 
genetic programming feature discovery image discrimination 
genetic programming relatively technology demonstrated versatile tool automatic program generation variety applications 
applications conditions known optimal solution determined advance 
leaves open question gp scale real world situations answers known data noisy measurements may poor quality 
attempt address constructing problem large volumes noisy image data segmented processed statistical features 
features assign image target nontarget category 
constrained processing requirements segmentation feature measurement coarse unreliable nature 
overlap classes feature space exists discovery correct solution 
experiments performed insert gp generated tree existing system order test established adaptive classifier technologies specifically backpropagation binary tree methods 
proceed examine gp inserted earlier stage processing obviating need costly segmentation feature extraction stages 

genetic programming koza successfully demonstrated genetic programming pattern recognition control planning generation neural networks 
conventional genetic algorithms ga fixed length binary strings gp begins population randomly generated lisp programs represented parse trees 
lisp programs trees represent constructed function set terminal set chosen user 
function takes arguments may terminals may functions 
functions form root internal nodes parse tree 
terminals form leaves parse tree may represent input variables constants sensor measurements 
fitness proportionate selection carried manner conventional ga gp crossover operation randomly selected subtrees chosen parent pair swapped form pair usually unique syntactically correct offspring 
mutation consists randomly replacing subtrees existing members population new randomly generated subtrees 

problem 
target non target discrimination target nontarget discrimination important stage automatic target recognition atr general systems require attention small sub area areas larger image 
highly sophisticated pattern recognizer may fine discriminations subtly different patterns generally high computational cost 
efficient way process information employ simpler detection algorithm entire image identifying subareas interest 
areas coarsely classified target non target lower reliability recognizer 
target areas passed detector recognizer reducing image bandwidth throughput requirements 
constraint problem gp produce solution computationally efficient order useful 
advantage gained gp able bypass costly processing associated feature extraction 
genetic programming feature discovery image discrimination 
image database data taken army terrain board imagery specifically phase phase ii test database 
pixel images simulate ir views vehicles including tracked wheeled vehicles fixed rotary wing aircraft air defense units wide variety cluttered terrain 
range field view sensor resolution individual targets occupy pixels 
total images containing multiple target clutter objects providing total samples classified 
training test data experiments described drawn randomly samples 

tank targets light clutter army terrain table 
feature extraction experiments described genetic programming construct classifiers process feature vectors produced existing algorithm specifically hughes atr system 
system performs sequential steps image processing 

detection filter system uses filtering extract blobs range sizes conforming expected targets 
image divided mosaic pixel overlapping regions large windows 
regions divided overlapping small genetic programming feature discovery image discrimination windows 
blob appropriate size detected described terms primitive features contrast local background global image intensity mean standard deviation means standard deviations large window small window centered 
apply gp classification features 
conventional algorithm passed second processing stage 
size filter value blob contrast global image intensity mean global image intensity standard deviation large window intensity mean large window intensity standard deviation small window intensity mean small window intensity standard deviation table primitive features hierarchy image window intensities 

segmentation feature extraction conventional system features detection filter passed simple linear classifier order determine segmentation feature extraction performed blob 
large image window undergoes decimation filtering reduce bandwidth statistical measures local intensity contrast perform binary ground segmentation resulting closed boundary silhouette 
moment intensity features extracted segmented region 
summarized table 
segmentation scheme depends fixed thresholds varying image conditions silhouettes target type vary significantly 
likewise features encode detailed shape properties possible non target objects rocks encoded target regions feature space 
feature vectors may display significant variance overlap target non target classes 
radius height width width height normalized delta greylevel symmetry range target depression angle perimeter area normalized average segmentation greylevel area height area height range higher order moments area range polarity delta greylevel table statistical measurements segmented image 
approach fitness individual tree computed classify samples 
generation individual performs best training set additionally run sample validation test set results reported 

function set experiments share common function set represents arithmetic operations form nd order nodes branches arguments conditional operation forms th order nodes branches arguments 
genetic programming feature discovery image discrimination operators represent common addition subtraction multiplication indicates protected division division zero yields zero result error 
conditional returns value third argument argument second fourth argument returned 

terminal set fitness function experiment terminal set experiment consists segmented statistical features shown table real random variable resampled produce constant value time selected terminal node 
resulting tree takes bag floating point feature values constants input combines linear nonlinear operations produce numeric result root tree 
result greater equal sample classified target classified clutter non target 
testing trees sample set results measurements probability incorrect classification total fraction samples assigned incorrect category 
previously observed performance enhanced including larger number target samples clutter samples training database see section results 
gives rise problem classifier says target may get relatively high fitness score conveying information shannon sense 
counteract second measure fitness added posteriori entropy class distributions class output observing classifier output 
multiple objectives minimized mapping pareto plane subjecting nondominated sort 
resulting ranking forms raw fitness measure individual 

terminal set fitness function experiment second experiment primitive intensity features discriminant filter 
terminal set consists integer features integer random variable sampled produce constant terminal nodes 
fitness function reformulated experiment historical reasons constraints placed original system 
state detection filter able find targets image probability detecting 
means individual probability detection 
fixed seek minimize probability false alarms fa chance non targets classed targets 
done manner tree tested sample data base values produces stored separate arrays targets clutter 
arrays sorted threshold value exactly target samples produced greater output 
compare value clutter array order determine percentage fall threshold value 
percentage false alarm rate seek minimize 

backpropagation binary tree classifier experimental control test training database build evaluate classifiers 
backpropagation network adaptive stepsize output nodes class 
experiments desired output activation back propagation target sample clutter sample 
differences desired outputs obtained form error terms fed back 
weight updates take place sample presentation 
training inputs backprop network normalized values exceed absolute value 
threshold difference network outputs determine sample target clutter 
threshold genetic programming feature discovery image discrimination experiment variable experiment 
second classifier tested binary tree classifier partitions feature space regions choosing features thresholds decision nodes maximization information rate known mutual information 
particular binary tree classifier originally developed optimized feature set experiment 
experiment 
results experiments performance plotted function probability false alarm fa vs probability detection 
probability false alarm fraction non targets classified targets divided total number non target samples 
probability detection fraction targets correctly classified divided total number target samples 
ideally system achieve fa 

experiment runs performed gp different random seeds 
population run length generations resulting analysis total individuals 
different clutter target ratios ratio training database consistently producing best results backpropagation similarly tested ratios ratio producing best results 
network input nodes features hidden nodes empirically determined best number 
random initial weight sets ratio resulting total learned weight sets 
training epoch network tested separate validation test set 
best result achieved test set reported merit 
aaaaaaaaaaaa aaaaaaaaaaaa aaaaaaaaaaaa aaaaaaaaaaaa aaaaaaaaaaaa aaaaaaaaaaaa aaaaaaaaaaaa aaaaaaaaaaaa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa fa probability fa lse alarm vs de tection genetic programming binary tree classifier modified backprop net gen ratios produce particularly results 
genetic programming feature discovery image discrimination binary tree classifier tested ratios additionally tested input set eliminates gray level features 
random initial element 
various system parameters tree builder refined development cycle 
summarizes performance classifiers 
points corresponding backpropagation depict best performance achieved ratios 
points shown binary tree correspond ratios input feature sets described cluster points higher fa obtained gray level features omitted 
points shown gp best generation individuals selected generations single evolution run ratio 
methods able achieve 
genetic programming achieves false alarm rate lower fa achieved backprop lower achieved binary tree 
best generation tree generation shows best run individual generation indicated arrow dendritic tree represented lisp program format 
program achieved correct false alarm rate 
counting function nodes see mathematical operations required logical comparisons branches 
comparison backpropagation network requires math operations nonlinearities computation required order achieve lower performance 

experiment experiment genetic programming run times generations population ratios fitness function described section 
training testing backpropagation network performed exactly described sections network contained input nodes hidden nodes 
gp trees neural networks ratio resulted best performance 
genetic programming feature discovery image discrimination aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa fa genetic programming backprop fa function primitive feature set 
genetic programming fitness minimization fa fixed dashed line 
depicts performance backpropagation gp systems 
dashed vertical line indicates desired 
value gp tree produced false alarm rate backpropagation produced 
varying threshold generate curves showing fa values 
provides insight algorithms gp fitness function specifically tuned desired fa performance backprop training independent threshold training straightforward way incorporate constraint 
gp chooses function specifically trades performance high values lower values 
genetic programming best generation generation primitive feature set 
successful discriminant full terminal set function set 
individual performance depicted shown lisp expression form 
function requires mathematical operations compared math operations nonlinear function evaluations required backpropagation network 
genetic programming feature discovery image discrimination displays remarkable properties conditional branch ignores input features 
likewise system repeated synthesized 
clearly gives insight features useful low level discrimination task 
small size expression allows unique analysis shows performance obtained decomposing expression unique subtree components postulate comprise gp schemata 
aaaaaaaaaaaa aaaaaaaaaaaa aaaaaaaaaaaa aaaaaaaaaaaa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa fa best generation pe ind schemata subexpressions ir parent expression schema appears values display performance backpropagation network 
significantly schemata approach fa 
way contributing fitness parent expression principle superposition nonlinear interactions 
gp may different notion schemata nonbinary alphabet suggest observation underscores principles previously set forth goldberg 

discussion 
primitive features improved performance significant observation performance obtained experiment better experiment gp tree experiment obtained fa backprop net obtained 
due coincidence methods showed improved performance 

hypothesis powerful nonlinear adaptive methods may discovering inherent features better suited data human synthesized features measurements trees comparable slightly lower performance usually times large 
genetic programming feature discovery image discrimination incorrectly presupposed invariant 
may speculate segmentation process introduces artifacts variations exist raw data 
factors considered mentioned image resolution reduced prior feature extraction compressing groups pixels 
reduction bandwidth may harmful 
reduction number input features may exponentially reduce complexity problem space searched making relatively solutions easier find 
regardless cause may conclude appears result direct benefit development systems 
suggests applying gp raw pixel imagery promising area follow research 

gp offers problem insights seen case experiment structure solution tree offers clues features assist pattern discrimination 
complex analysis applied experiment 
indicates gp provide insights aspects problem important understand problem better examining solutions gp discovers 
symbolic nature genetic programming offers advantage opaque methods neural learning 

gp schema theory 
examination see repeated structures tree experiment experiment 
probability identical structures randomly generated outset successful subtrees schema frequently duplicated spread population exponential rate due contribution fitness individuals containing 
suggest schema theory may developed genetic programming analogous binary ga originally proposed holland 
focus identification analysis specific contributions high value schemata classification process 

parsimony parsimony simplicity solution structures previously shown koza achievable adding tree size expression length component fitness function minimized 
experiments observed high degree correlation exists tree size performance set pretty solution trees highest performance usually smallest set 
observed runs achieved point size complexity trees eventually began grow increasingly larger performance tapered lower values 
suggest open ended exploration problem space parsimony may important factor aesthetic reasons ease analysis direct relationship fitness bound appropriate size solution tree problem 

acknowledgments author wishes dr kenneth dr knut hughes missile systems supporting innovative initiative internal research development funds 

bibliography koza john genetic programming cambridge ma mit press 
ce dh lincoln wp tackett wa ga artificial neural networks automatic target recognition optical engineering dec pp 

goldberg david genetic algorithms search optimization machine learning 
reading ma addison wesley 
genetic programming feature discovery image discrimination hertz krogh palmer rg theory neural computation 
redwood city ca addison wesley 
kanal ln problem solving models search strategies pattern recognition ieee trans 
pattern anal 
machine intell vol 
pami pp 
apr 
bandwidth reduction intelligent target tracking multi function target acquisition processor final technical report 
el segundo ca hughes aircraft may 
holland john adaptation natural artificial systems 
cambridge ma mit press 
goldberg david zen art genetic algorithms proc 
third international conference genetic algorithms 
san mateo ca morgan kaufman 
