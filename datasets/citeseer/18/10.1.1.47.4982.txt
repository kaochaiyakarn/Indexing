speech recognition neural networks joe may cmu cs school computer science carnegie mellon university pittsburgh pennsylvania submitted partial fulfillment requirements degree doctor philosophy computer science thesis committee alex waibel chair raj reddy jaime carbonell richard lippmann mit lincoln labs copyright joe research supported separate phases atr interpreting telephony research laboratories nec siemens ag national science foundation advanced research projects administration department defense contract 
mda 
views contained document author interpreted representing official policies expressed implied atr nec siemens nsf united states government 
keywords speech recognition neural networks hidden markov models hybrid systems acoustic modeling prediction classification probability estimation discrimination global optimization 
iii thesis examines artificial neural networks benefit large vocabulary speaker independent continuous speech recognition system 
currently speech recognition systems hidden markov models hmms statistical framework supports acoustic temporal modeling 
despite state art performance hmms number suboptimal modeling assumptions limit potential effectiveness 
neural networks avoid assumptions learn complex functions generalize effectively tolerate noise support parallelism 
neural networks readily applied acoustic modeling clear temporal modeling 
explore class systems called nn hmm hybrids neural networks perform acoustic modeling hmms perform temporal modeling 
argue nn hmm hybrid theoretical advantages pure hmm system including better acoustic modeling accuracy better context sensitivity natural discrimination economical parameters 
advantages confirmed experimentally nn hmm hybrid developed context independent phoneme models achieved word accuracy resource management database contrast accuracy achieved pure hmm similar conditions 
course developing system explored different ways neural networks acoustic modeling prediction classification 
predictive networks yield poor results lack discrimination classification networks gave excellent results 
verified accordance theory output activations classification network form highly accurate estimates posterior probabilities class input showed easily converted likelihoods input class standard hmm recognition algorithms 
thesis reports optimized accuracy system natural techniques expanding input window size normalizing inputs increasing number hidden units converting network output activations log likelihoods optimizing learning rate schedule automatic search backpropagating error word level outputs gender dependent networks 
iv wish alex waibel guidance encouragement friendship managed extend years collaboration inconvenient oceans efforts provide world class international research environment thesis possible 
alex scientific integrity great ambition earned respect plus standing invitation dinner passes corner world 
wish raj reddy jaime carbonell rich lippmann serving thesis committee offering valuable suggestions thesis proposal final dissertation 
scott fahlman advisor early enthusiasm neural networks teaching means research 
colleagues world influenced thesis including past members boltzmann group group cmu group university karlsruhe germany 
especially want closest collaborators years otto torsten hermann hild patrick haffner arthur mcnair monika michael finke thorsten contributions friendship 
wish acknowledge valuable interactions talented researchers including fil bourlard lin chase mike cohen mark mike paul john hampshire geoff hinton huang mei hwang ken ichi iso jain konig george lakoff kevin lang chris lebiere kai fu lee ester levin stefan jay mcclelland chris mcconnell nelson morgan barak pearlmutter dave plaut dean pomerleau steve renals roni rosenfeld dave rumelhart dave david servan schreiber bernhard suhm sebastian thrun dave touretzky minh tue wayne ward christoph michael witbrock 
am especially indebted konig icsi extremely generous helping understand reproduce icsi experimental results arthur mcnair janus demos focus speech research constantly keeping environment running smoothly 
hal colleagues adaptive solutions assistance parallel computer nigel goddard pittsburgh supercomputer center help cray 
roni rosenfeld lin chase michael finke proofreading portions thesis 
am grateful robert wilensky getting started artificial intelligence especially douglas hofstadter allen newell sharing pivotal hours vi friends helped maintain sanity phd program felt thesis 
wish express love gratitude especially bart reynolds sara fried pam westin marilyn pete fast susan wheeler chen wu roni rosenfeld george necula jade goldstein hermann hild michael finke porsche barbara white anne scott westbrook richard parsons jeanne sheldon 
friendship catherine prasad tadepalli hanna arthur mcnair torsten patrick haffner mark michaylov prasad angela lin chase steve lawson dennis bonnie list 
support friends finished phd 
wish parents virginia robert having raised stable loving environment enabled come far 
rest family relatives love 
thesis dedicated douglas hofstadter book godel escher bach changed life suggesting consciousness emerge subsymbolic computation shaping deepest beliefs inspiring study connectionism late allen newell genius humanity role model dream emulating table contents vii 
iii 

speech recognition 
neural networks 
thesis outline 
review speech recognition 
fundamentals speech recognition 
dynamic time warping 
hidden markov models 
basic concepts 
algorithms 
variations 
limitations hmms 
review neural networks 
historical development 
fundamentals neural networks 
processing units 
connections 
computation 
training 
taxonomy neural networks 
supervised learning 
semi supervised learning 
unsupervised learning 
hybrid networks 
dynamic networks 
backpropagation 
relation statistics 
table contents viii related research 
early neural network approaches 
phoneme classification 
word classification 
problem temporal structure 
nn hmm hybrids 
nn implementations hmms 
frame level training 
segment level training 
word level training 
global optimization 
context dependence 
speaker independence 
word spotting 
summary 
databases 
japanese isolated words 
conference registration 
resource management 
predictive networks 
motivation hindsight 
related 
linked predictive neural networks 
basic operation 
training lpnn 
isolated word recognition experiments 
continuous speech recognition experiments 
comparison hmms 
extensions 
hidden control neural network 
context dependent phoneme models 
function word models 
weaknesses predictive networks 
lack discrimination 
inconsistency 
table contents ix classification networks 
overview 
theory 
mlp posterior estimator 
likelihoods vs posteriors 
frame level training 
network architectures 
input representations 
speech models 
training procedures 
testing procedures 
generalization 
word level training 
multi state time delay neural network 
experimental results 
summary 
comparisons 
conference registration database 
resource management database 

neural networks acoustic models 
summary experiments 
advantages nn hmm hybrids 
appendix final system design 
appendix proof classifier networks estimate posterior probabilities 
bibliography 
author index 
subject index 

speech natural mode communication people 
learn relevant skills early childhood instruction continue rely speech communication lives 
comes naturally don realize complex phenomenon speech human vocal tract articulators biological organs nonlinear properties operation just conscious control affected factors ranging gender emotional state 
result vocalizations vary widely terms accent pronunciation articulation roughness pitch volume speed transmission irregular speech patterns distorted background noise echoes electrical characteristics telephones electronic equipment 
sources variability speech recognition speech generation complex problem 
people comfortable speech interact computers speech having resort primitive interfaces keyboards pointing devices 
speech interface support valuable applications example telephone directory assistance spoken database querying novice users applications medicine fieldwork office dictation devices automatic voice translation foreign languages 
tantalizing applications motivated research automatic speech recognition 
great progress far especially series engineered approaches include template matching knowledge engineering statistical modeling 
computers near level human performance speech recognition appears significant advances require new insights 
people recognizing speech 
human brain known wired differently conventional computer fact operates radically different computational paradigm 
conventional computers fast complex central processor explicit program instructions locally addressable memory contrast human brain uses massively parallel collection slow simple processing elements neurons densely connected weights synapses strengths modified experience directly supporting integration multiple constraints providing distributed form associative memory 
brain impressive superiority wide range cognitive skills including speech recognition motivated research novel computational paradigm assumption models may ultimately lead performance complex tasks 
fascinating research area known connectionism study artificial neural networks 
history field erratic 
mid field matured point realistic applying connectionist models difficult tasks speech recognition 
thesis proposed researchers demonstrated value neural networks important subtasks phoneme recognition spoken digit recognition unclear connectionist techniques scale large speech recognition tasks 
thesis demonstrates neural networks form basis general purpose speech recognition system neural networks offer clear advantages conventional techniques 

speech recognition current state art speech recognition 
complex question system accuracy depends conditions evaluated sufficiently narrow conditions system attain human accuracy harder achieve accuracy general conditions 
conditions evaluation accuracy system vary dimensions vocabulary size 
general rule easy discriminate small set words error rates naturally increase vocabulary size grows 
example digits zero recognized essentially perfectly doddington vocabulary sizes may error rates itakura kimura 
hand small vocabulary hard recognize contains confusable words 
example letters english alphabet treated words difficult discriminate contain confusable words notoriously set error rate considered vocabulary hild waibel 
speaker dependence vs independence 
definition speaker dependent system intended single speaker speaker independent system intended speaker 
speaker independence difficult achieve system parameters tuned speaker trained parameters tend highly speaker specific 
error rates typically times higher speaker independent systems speaker dependent ones lee 
intermediate speaker dependent independent systems multi speaker systems intended small group people speaker adaptive systems tune speaker small amount speech enrollment data 
isolated discontinuous continuous speech 
isolated speech means single words discontinuous speech means full sentences words artificially separated silence continuous speech means naturally spoken sentences 
isolated discontinuous speech recognition relatively easy word boundaries detectable words tend cleanly pronounced 
continu 
speech recognition ous speech difficult word boundaries unclear pronunciations corrupted coarticulation speech sounds example causes phrase sound jou 
typical evaluation word error rates isolated continuous speech respectively bahl 
task language constraints 
fixed vocabulary performance vary nature constraints word sequences allowed recognition 
constraints may task dependent example application may dismiss hypothesis apple red constraints may semantic rejecting apple angry syntactic rejecting red apple 
constraints represented grammar ideally filters unreasonable sentences speech recognizer evaluates plausible sentences 
grammars usually rated perplexity number indicates grammar average branching factor number words follow word 
difficulty task reliably measured perplexity vocabulary size 
read vs spontaneous speech 
systems evaluated speech read prepared scripts speech uttered spontaneously 
spontaneous speech vastly difficult tends disfluencies uh um false starts incomplete sentences stuttering laughter vocabulary essentially unlimited system able deal intelligently unknown words detecting flagging presence adding vocabulary may require interaction user 
adverse conditions 
system performance degraded range adverse conditions furui 
include environmental noise noise car factory acoustical distortions echoes room acoustics different microphones close speaking omnidirectional telephone limited frequency bandwidth telephone transmission altered speaking manner speaking quickly 
order evaluate compare different systems defined conditions number standardized databases created particular characteristics 
example database widely darpa resource management database large vocabulary words speaker independent continuous speech database consisting training sentences domain naval resource management read script recorded benign environmental conditions testing usually performed grammar perplexity 
controlled conditions state art performance word recognition accuracy simpler systems 
database smaller ones research see chapter 
central issue speech recognition dealing variability 
currently speech recognition systems distinguish kinds variability acoustic temporal 
acoustic variability covers different accents pronunciations pitches volumes 
temporal variability covers different speaking rates 
dimensions completely independent person speaks quickly acoustical patterns distorted useful simplification treat independently 
dimensions temporal variability easier handle 
early approach temporal variability linearly stretch shrink warp unknown utterance duration known template 
linear warping proved inadequate utterances accelerate decelerate time nonlinear warping obviously required 
soon efficient algorithm known dynamic time warping proposed solution problem 
algorithm form virtually speech recognition system problem temporal variability considered largely solved acoustic variability difficult model partly heterogeneous nature 
consequently research speech recognition largely focused efforts model acoustic variability 
past approaches speech recognition fallen main categories 
template approaches unknown speech compared set prerecorded words templates order find best match 
advantage perfectly accurate word models disadvantage prerecorded templates fixed variations speech modeled templates word eventually impractical 

knowledge approaches expert knowledge variations speech hand coded system 
advantage explicitly modeling variations speech unfortunately expert knowledge difficult obtain successfully approach judged impractical automatic learning procedures sought 

statistical approaches variations speech modeled statistically hidden markov models hmms automatic learning procedures 
approach represents current state art 
main disadvantage statistical models priori modeling assumptions liable inaccurate system performance 
see neural networks help avoid problem 

neural networks connectionism study artificial neural networks initially inspired neurobiology interdisciplinary field spanning computer science electrical engineering mathematics physics psychology linguistics 
researchers studying neurophysiology human brain attention 
remain unresolved secondary issues duration constraints speaker dependent speaking rates 
neural networks focused general properties neural computation simplified neural models 
properties include 
networks taught form associations input output patterns 
example teach network classify speech patterns phoneme categories 
generalization 
networks don just memorize training data learn underlying patterns generalize training data new examples 
essential speech recognition acoustical patterns exactly 
nonlinearity 
networks compute nonlinear nonparametric functions input enabling perform arbitrarily complex transformations data 
useful speech highly nonlinear process 
robustness 
networks tolerant physical damage noisy data fact noisy data help networks form better generalizations 
valuable feature speech patterns notoriously noisy 
uniformity 
networks offer uniform computational paradigm easily integrate constraints different types inputs 
easy basic differential speech inputs example combine acoustic visual cues multimodal system 
parallelism 
networks highly parallel nature suited implementations massively parallel computers 
ultimately permit fast processing speech data 
types connectionist models different architectures training procedures applications common principles 
artificial neural network consists potentially large number simple processing elements called units nodes neurons influence behavior network excitatory inhibitory weights 
unit simply computes nonlinear weighted sum inputs broadcasts result outgoing connections units 
training set consists patterns values assigned designated input output units 
patterns training set learning rule modifies strengths weights network gradually learns training set 
basic paradigm fleshed different ways different types networks learn compute implicit functions input output vectors automatically cluster input data generate compact representations data provide content addressable memory perform pattern completion 

biological details ignored simplified models 
example biological neurons produce sequence pulses stable activation value exist different types biological neurons physical geometry affect computational behavior operate asynchronously different cycle times behavior affected hormones chemicals 
details may ultimately prove necessary modeling brain behavior simplified model computational power support interesting research 

neural networks usually perform static pattern recognition statically map complex inputs simple outputs ary classification input patterns 
common way train neural network task procedure called backpropagation rumelhart network weights modified proportion contribution observed error output unit activations relative desired outputs 
date successful applications neural networks trained backpropagation 
instance nettalk sejnowski rosenberg neural network learns pronounce english text 
input window characters orthographic text symbols scanning larger text buffer output phoneme code relayed speech synthesizer tells pronounce middle character context 
successive cycles training words pronunciations nettalk steadily improved performance child learning talk eventually produced quite intelligible speech words seen 
tesauro neural network learns winning strategy backgammon 
input describes current position dice values possible move output represents merit move training set examples hand scored expert player 
sufficient training network generalized win gold medal computer london defeating commercial non commercial programs lost human expert 
alvinn pomerleau neural network learns drive car 
input coarse visual image road ahead provided video camera imaging laser rangefinder output continuous vector indicates way turn steering wheel 
system learns drive observing person drives 
alvinn successfully driven speeds miles hour miles variety different road conditions 
handwriting recognition le cun neural networks read zip codes mail envelopes 
size normalized images isolated digits conventional algorithms fed highly constrained neural network transforms visual image class outputs 
system achieved digit recognition accuracy actual mail provided postal service 
elaborate system achieved digit recognition accuracy database 
speech recognition course proving ground neural networks 
researchers quickly achieved excellent results basic tasks voiced unvoiced discrimination watrous phoneme recognition waibel spoken digit recognition 
thesis proposed remained seen neural networks support large vocabulary speaker independent continuous speech recognition system 
thesis take incremental approach problem 
types variability speech acoustic temporal naturally posed static 
thesis outline pattern matching problem amenable neural networks neural networks acoustic modeling rely conventional hidden markov models temporal modeling 
research represents exploration space nn hmm hybrids 
explore different ways neural networks acoustic modeling prediction classification speech patterns 
prediction shown weak approach lacks discrimination classification shown stronger approach 
extensive series experiments performed optimize networks word recognition accuracy show properly optimized nn hmm hybrid system classification networks outperform systems similar conditions 
argue hybrid nn hmm systems offer advantages pure hmm systems including better acoustic modeling accuracy better context sensitivity natural discrimination economical parameters 

thesis outline chapters thesis provide essential background summary related speech recognition neural networks chapter reviews field speech recognition 
chapter reviews field neural networks 
chapter reviews intersection fields summarizing past approaches speech recognition neural networks 
remainder thesis describes research evaluating predictive networks classification networks acoustic models nn hmm hybrid systems chapter introduces databases experiments 
chapter presents research predictive networks explains approach yielded poor results 
chapter presents research classification networks shows achieved excellent results extensive series optimizations 
chapter compares performance optimized systems systems databases demonstrating value nn hmm hybrids 
chapter presents thesis 


review speech recognition chapter brief review field speech recognition 
reviewing fundamental concepts explain standard dynamic time warping algorithm discuss hidden markov models detail offering summary algorithms variations limitations associated dominant technology 

fundamentals speech recognition speech recognition pattern recognition task acoustical signals examined structured hierarchy subword units phonemes words phrases sentences 
level may provide additional temporal constraints known word pronunciations legal word sequences compensate errors uncertainties lower levels 
hierarchy constraints best exploited combining decisions probabilistically lower levels making discrete decisions highest level 
structure standard speech recognition system illustrated 
elements follows raw speech 
speech typically sampled high frequency khz microphone khz telephone 
yields sequence amplitude values time 
signal analysis 
raw speech initially transformed compressed order simplify subsequent processing 
signal analysis techniques available extract useful features compress data factor losing important information 
popular fourier analysis fft yields discrete frequencies time interpreted visually 
frequencies distributed mel scale linear low range logarithmic high range corresponding physiological characteristics human ear 
perceptual linear prediction plp physiologically motivated yields coefficients interpreted visually 

review speech recognition linear predictive coding lpc yields coefficients linear equation approximate history raw speech values 
cepstral analysis calculates inverse fourier transform logarithm power spectrum signal 
practice little difference technique procedures linear discriminant analysis lda may optionally applied reduce dimensionality representation decorrelate coefficients 

assuming benign conditions 
course technique advocates 
structure standard speech recognition system 
signal analysis converts raw speech speech frames 
raw speech signal analysis speech frames acoustic models frame scores sequential constraints word sequence segmentation time alignment acoustic analysis train train test train raw speech values sec 
speech frames coefficients frames sec 
signal analysis 
fundamentals speech recognition speech frames 
result signal analysis sequence speech frames typically msec intervals coefficients frame 
frames may augmented second derivatives providing explicit information speech dynamics typically leads improved performance 
speech frames acoustic analysis 
acoustic models 
order analyze speech frames acoustic content need set acoustic models 
kinds acoustic models varying representation granularity context dependence properties 
shows popular representations acoustic models 
simplest template just stored sample unit speech modeled recording word 
unknown word recognized simply comparing known templates finding closest match 
templates major drawbacks model acoustic variabilities coarse way assigning multiple templates word practice limited word models hard record segment sample shorter word templates useful small systems afford luxury word models 
flexible representation larger systems trained acoustic models states 
approach word modeled sequence trainable states state indicates sounds heard segment word probability distribution acoustic space 
probability distributions modeled parametrically assuming simple shape gaussian distribution trying find parameters describe non parametrically representing distribution directly histogram quantization acoustic space shall see neural network 
acoustic models template state representations word cat 
template state parametric non parametric speech frames state sequence likelihoods acoustic space likelihoods acoustic space 

review speech recognition acoustic models vary widely granularity context sensitivity 
shows chart common types acoustic models lie dimensions 
seen models larger granularity word syllable models tend greater context sensitivity 
models greatest context sensitivity give best word recognition accuracy models trained 
unfortunately larger granularity model poorer trained fewer samples available training 
reason word syllable models rarely highperformance systems common triphone generalized triphone models 
systems monophone models simply called phoneme models relative simplicity 
training acoustic models incrementally modified order optimize performance system 
testing acoustic models left unchanged 
acoustic analysis frame scores 
acoustic analysis performed applying acoustic model frame speech yielding matrix frame scores shown 
scores computed type acoustic model 
template acoustic models score typically euclidean distance template frame unknown frame 
state acoustic models score represents emission probability likelihood current state generating current frame determined state parametric non parametric function 
time alignment 
frame scores converted word sequence identifying sequence acoustic models representing valid word sequence gives acoustic models granularity vs context sensitivity illustrated word market 
granularity models context sensitivity monophone diphone triphone syllable word unlimited mar ket ma ar ke generalized triphone market 

fundamentals speech recognition best total score alignment path matrix illustrated 
process searching best alignment path called time alignment 
alignment path obey certain sequential constraints reflect fact speech goes forward backwards 
constraints manifested words 
word sequential constraints implied sequence frames template models sequence states state models comprise word dictated phonetic pronunciations dictionary example 
words sequential constraints grammar indicating words may follow words 
time alignment performed efficiently dynamic programming general algorithm uses local path constraints linear time space requirements 
general algorithm main variants known dynamic time warping dtw viterbi search differ slightly local computations optimality criteria 
state system optimal alignment path induces segmentation word sequence indicates frames associated state 

better evaluate state sequence single best alignment path composite score possible alignment paths ignore issue 
alignment path best total score identifies word sequence segmentation 
oy input speech boys boys acoustic models matrix frame scores total score segmentation oy alignment path 
review speech recognition segmentation generate labels recursively training acoustic models corresponding frames 
word sequence 
result time alignment word sequence sentence hypothesis utterance 
common return sequences ones highest scores variation time alignment called best search schwartz chow 
allows recognition system passes unknown utterance pass simplified models order quickly generate best list second pass complex models order carefully rescore hypotheses return single best hypothesis 

dynamic time warping section motivate explain dynamic time warping algorithm oldest important algorithms speech recognition itakura sakoe chiba 
simplest way recognize isolated word sample compare number stored word templates determine best match 
goal complicated number factors 
different samples word somewhat different durations 
problem eliminated simply normalizing templates unknown speech equal duration 
problem rate speech may constant word words optimal alignment template speech sample may nonlinear 
dynamic time warping dtw efficient method finding optimal nonlinear alignment 
dtw instance general class algorithms known dynamic programming 
time space complexity merely linear duration speech sample vocabulary size 
algorithm single pass matrix frame scores computing locally optimized segments global alignment path 
see euclidean distance frame speech sample frame template cumulative score optimal alignment path leads resulting alignment path may visualized low valley euclidean distance scores hilly landscape matrix final point 
keeping track backpointers full alignment path recovered tracing backwards 
optimal alignment path computed word template lowest cumulative score considered best match unknown speech sample 
variations dtw algorithm 
example common vary local path constraints introducing transitions slope weighting min 
hidden markov models transitions various ways applying kinds slope constraints sakoe chiba 
word models usually templates may state models shown previously 
states vertical transitions disallowed fewer states frames goal maximize cumulative score minimize 
particularly important variation dtw extension isolated continuous speech 
extension called stage dtw algorithm ney 
goal find optimal alignment speech sample best sequence words see 
complexity extended algorithm linear length sample vocabulary size 
modification basic dtw algorithm word model frame state diagonal path allowed point back word models preceding frame 
local backpointers specify word model preceding point optimal word sequence recovered tracing backwards final point word best final score 
grammars imposed continuous speech recognition restricting allowed transitions word boundaries 

hidden markov models flexible successful approach speech recognition far hidden markov models hmms 
section basic concepts hmms describe algorithms training discuss common variations review problems associated hmms 
dynamic time warping 
alignment path 
local path constraints 
speech unknown word alignment path optimal word template cumulative word score 
review speech recognition 
basic concepts hidden markov model collection states connected transitions illustrated 
begins designated initial state 
discrete time step transition taken new state output symbol generated state 
choice transition output symbol random governed probability distributions 
hmm thought black box sequence output symbols generated time observable sequence states visited time hidden view 
called hidden markov model 
hmms variety applications 
hmm applied speech recognition states interpreted acoustic models indicating sounds heard corresponding segments speech transitions provide temporal constraints indicating states may follow sequence 
speech goes forward time transitions speech application go forward self loop allowing state arbitrary duration 
illustrates states transitions hmm structured hierarchically order represent phonemes words sentences 
simple hidden markov model states output symbols hierarchically structured hmm 
middle sentence level word level phoneme level latitude longitude location kirk willamette display ah ts 
hidden markov models formally hmm consists elements set states 
ij set transition probabilities ij probability transition state state set emission probabilities probability distribution acoustic space describing likelihood emitting possible sound state probabilities satisfy properties notation implicitly confine attention order hmms depend current state independent previous history state sequence 
assumption universally observed limits number trainable parameters training testing algorithms efficient rendering hmms useful speech recognition 

algorithms basic algorithms associated hidden markov models forward algorithm useful isolated word recognition viterbi algorithm useful continuous speech recognition forward backward algorithm useful training hmm 
section review algorithms 

forward algorithm order perform isolated word recognition able evaluate probability hmm word model produced observation sequence compare scores word model choose highest score 
formally hmm model consisting ij compute probability generated output sequence 
state generate output symbol probability state sequence length 
traditional refer emission probability observation probability hmm traditionally generative model speech recognition 
difference moot 
ij ij 
review speech recognition contributes total probability 
brute force algorithm simply list possible state sequences length accumulate probabilities generating clearly exponential algorithm practical 
efficient solution forward algorithm instance class algorithms known dynamic programming requiring computation storage linear define probability generating partial sequence state time initialized initial state states 
computed previous time frame computed recursively terms incremental probability entering state generating output symbol see final state induction see probability hmm generated complete output sequence shows example algorithm operation computing probability output sequence generated simple hmm earlier 
cell shows value values computation proceeds state state time frame proceeding time frame 
final cell see probability particular hmm generates sequence 
forward pass recursion 
illustration forward algorithm showing value cell 
ij 
ij output output output 
hidden markov models 
viterbi algorithm forward algorithm useful isolated word recognition applied continuous speech recognition impractical separate hmm possible sentence 
order perform continuous speech recognition infer actual sequence states generated observation sequence state sequence easily recover word sequence 
unfortunately actual state sequence hidden definition uniquely identified path produced output sequence small probability 
best find state sequence generated observation sequence 
evaluating possible state sequences reporting highest probability exponential infeasible algorithm 
efficient solution viterbi algorithm dynamic programming 
similar forward algorithm main difference evaluating summation cell evaluate maximum implicitly identifies single best predecessor state cell matrix 
explicitly identify best predecessor state saving single backpointer cell matrix time evaluated final state final time frame retrace backpointers final cell reconstruct state sequence 
illustrates process 
state sequence alignment path trivially recover word sequence 
example backtracing 
max ij 

review speech recognition 
forward backward algorithm order train hmm optimize respect hmm likelihood generating output sequences training set maximize hmm chances correctly recognizing new data 
unfortunately difficult problem closed form solution 
best done start initial values iteratively modify improving stopping criterion reached 
general method called em 
popular instance general method forward backward algorithm known baum welch algorithm describe 
previously defined probability generating partial sequence state time define mirror image probability generating remainder sequence starting state time called forward term called backward term 
computed recursively time backward direction see recursion initialized time setting final state states 
define ij probability transitioning state state time output sequence generated current hmm numerator final equality understood consulting 
denominator reflects fact probability generating equals probability generating final states 
define expected number times transition state state taken time backward pass recursion 
jk 
jk ij ij 
hidden markov models summing destination states obtain represents expected number times state visited time selecting occasions state emits symbol obtain reestimate hmm parameters yielding simple ratios terms proven substituting cause increase local maximum 
repeating procedure number iterations hmm parameters optimized training data hopefully generalize testing data 
deriving ij forward backward algorithm 
ij ij ij ij ij ij ij ij ij 
review speech recognition 
variations variations standard hmm model 
section discuss important variations 

density models states hmm need way model probability distributions acoustic space 
popular ways illustrated discrete density model lee 
approach entire acoustic space divided moderate number regions clustering procedure known vector quantization vq 
centroid cluster represented scalar codeword index codebook identifies corresponding acoustic vectors 
input frame converted codeword finding nearest vector codebook 
hmm output symbols codewords 
probability distribution acoustic space represented simple histogram codebook entries 
drawback nonparametric approach suffers quantization errors codebook small increasing codebook size leave training data codeword likewise degrading performance 
continuous density model woodland 
quantization errors eliminated continuous density model vq codebooks 
approach probability distribution acoustic space modeled directly assuming certain parametric form trying find param density models describing probability density acoustic space 
discrete continuous semi continuous 
hidden markov models eters 
typically parametric form taken mixture gaussians weighting factor gaussian mean covariance matrix training reestimation involves reestimation additional set formulas 
drawback approach parameters shared states states system large value may yield total parameters trained adequately decreasing value may invalidate assumption distribution modeled mixture gaussians 
semi continuous density model huang called tied mixture model nahamoo 
compromise approaches 
semi continuous density model discrete model codebook describing acoustic clusters shared states 
representing clusters discrete centroids nearby vectors collapsed represented continuous density functions typically gaussians neighboring space avoiding quantization errors 
number codebook entries weighting factor gaussian mean covariance matrix continuous case gaussians reestimated training codebook optimized jointly hmm parameters contrast discrete model codebook remains fixed 
joint optimization improve system performance 
density models widely continuous densities give best results large databases running times slower 

multiple data streams far discussed hmms assume single data stream input acoustic vectors 
hmms modified multiple streams observation vectors independent data streams modeled separate codebooks gaussian mixtures 
hmm speech recognizers commonly data streams example representing spectral coefficients delta spectral jk jk jk jk jk jk jk jk jk jk jk jk 
review speech recognition cients power delta power 
possible concatenate long vector vector quantize single data stream generally better treat separate data streams independently stream coherent union modeled minimum parameters 

duration modeling self transition probability ii probability remaining state frames indicating state duration hmm modeled exponential decay 
unfortunately poor model duration state durations roughly poisson distribution 
ways improve duration modeling hmms 
define probability remaining state duration frames create histogram training data 
ensure state duration governed eliminate self loops setting ii modify equations reestimation formulas include summations maximum duration terms multiplicative factors represent possible durational contingencies 
unfortunately increases memory requirements factor computational requirements factor frames quite reasonable causes application run times slower 
problem approach may require training parameters adding state adequately trained 
problem mitigated replacing nonparametric approach parametric approach poisson gaussian gamma distribution assumed duration model relatively parameters needed 
improvement causes system run slower 
third possibility ignore precise shape distribution simply impose hard minimum maximum duration constraints 
way impose constraints duplicating states modifying state transitions appropriately 
approach moderate overhead gives fairly results tends favored approach duration modeling 

optimization criteria training procedure described earlier forward backward algorithm implicitly uses optimization criterion known maximum likelihood ml maximizes likelihood observation sequence generated correct model considering models 
instance represent word models correct word model updated respect competing word models ignored 
mathematically ml training solves hmm parameters specifically subset corresponds correct model 
common semi continuous hmms trend single data stream lda coefficients derived separate streams approach common continuous hmms 
ml argmax 
hidden markov models hmm modeling assumptions accurate probability density acoustic space precisely modeled mixture gaussians training data available perfectly estimating distributions ml training theoretically yield optimal recognition accuracy 
modeling assumptions inaccurate acoustic space complex terrain training data limited scarcity training data limits size power models perfectly fit distributions 
unfortunate condition called model mismatch 
important consequence ml guaranteed optimal criterion training hmm 
alternative criterion maximum mutual information mmi enhances discrimination competing models attempt squeeze useful information possible limited training data 
approach correct model trained positively models trained negatively observation sequence helping separate models improve ability discriminate testing 
mutual information observation sequence correct model defined follows term represents positive training correct model just ml second term represents negative training models training mmi criterion involves solving model parameters maximize mutual information unfortunately equation solved direct analysis reestimation known way solve gradient descent proper implementation complex brown rabiner 
note passing mmi equivalent maximum posteriori map criterion expression maximized 
see note bayes rule maximizing expression equivalent maximizing distinguishing logarithm monotonic transparent map extra factor transparent additive constant logarithms value fixed hmm topology language model 
log log log log log log mmi argmax 
review speech recognition 
limitations hmms despite state art performance hmms handicapped known weaknesses order assumption says probabilities depend solely current state false speech applications 
consequence hmms difficulty modeling coarticulation acoustic distributions fact strongly affected state history 
consequence durations modeled inaccurately exponentially decaying distribution accurate poisson bell shaped distribution 
independence assumption says correlation adjacent input frames false speech applications 
accordance assumption hmms examine frame speech time 
order benefit context neighboring frames hmms absorb frames current frame introducing multiple streams data order exploit delta coefficients lda transform streams single stream 
hmm probability density models discrete continuous semi continuous suboptimal modeling accuracy 
specifically discrete density hmms suffer quantization errors continuous semi continuous density hmms suffer model mismatch poor match priori choice statistical model mixture gaussians true density acoustic space 
maximum likelihood training criterion leads poor discrimination acoustic models limited training data correspondingly limited models discrimination improved maximum mutual information training criterion complex difficult implement properly 
hmms suffer weaknesses obtain performance relying context dependent phone models parameters extensively shared turn calls elaborate mechanisms decision trees hwang 
argue neural networks mitigate weaknesses order assumption require relatively parameters neural network speech recognition system get equivalent better performance complexity 

review neural networks chapter brief review neural networks 
giving historical background review fundamental concepts describe different types neural networks training procedures special emphasis backpropagation discuss relationship neural networks conventional statistical techniques 

historical development modern study neural networks began th century began extensive studies human nervous system 
determined nervous system comprised discrete neurons communicate sending electrical signals long axons ultimately branch touch dendrites receptive areas thousands neurons transmitting electrical signals synapses points contact variable resistance 
basic picture elaborated decades different kinds neurons identified electrical responses analyzed patterns connectivity brain gross functional areas mapped 
relatively easy study functionality individual neurons map brain gross functional areas extremely difficult determine neurons worked achieve highlevel functionality perception cognition 
advent high speed computers possible build working models neural systems allowing researchers freely experiment systems better understand properties 
mcculloch pitts proposed computational model neuron binary threshold unit output depending net input exceeded threshold 
model caused great deal excitement shown system neurons assembled finite state automaton compute arbitrary function suitable values weights neurons see minsky 
researchers soon began searching learning procedures automatically find values weights enabling network compute specific function 
rosenblatt discovered iterative learning procedure particular type network single layer perceptron proved learning procedure converged set weights produced desired function long desired function potentially computable network 
discovery caused great wave excitement ai researchers imagined goal machine intelligence reach 

review neural networks rigorous analysis minsky papert showed set functions potentially computable single layer perceptron quite limited expressed pessimism potential multi layer perceptrons direct result funding connectionist research suddenly field lay dormant years 
interest neural networks gradually revived hopfield suggested network analyzed terms energy function triggering development boltzmann machine ackley hinton sejnowski stochastic network trained produce kind desired behavior arbitrary pattern mapping pattern completion 
soon rumelhart popularized faster learning procedure called backpropagation train multi layer perceptron compute desired function showing minsky papert earlier pessimism unfounded 
advent backpropagation neural networks enjoyed third wave popularity useful applications 

fundamentals neural networks section briefly review fundamentals neural networks 
different types neural networks basic attributes set processing units set connections computing procedure training procedure 
discuss attributes 

processing units neural network contains potentially huge number simple processing units roughly analogous neurons brain 
units operate simultaneously supporting massive parallelism 
computation system performed units processor coordinates activity moment time unit simply computes scalar function local inputs broadcasts result called activation value neighboring units 
units network typically divided input units receive data environment raw sensory information hidden units may internally transform data representation output units represent decisions control signals may control motor responses example 

course extent neural network may simulated conventional computer implemented directly hardware 

fundamentals neural networks drawings neural networks units usually represented circles 
convention input units usually shown bottom outputs shown top processing seen bottom 
state network moment represented set activation values units network state typically varies moment moment inputs changed feedback system causes network follow dynamic trajectory state space 

connections units network organized topology set connections weights shown lines diagram 
weight real value typically ranging range limited 
value strength weight describes influence unit neighbor positive weight causes unit excite negative weight causes unit inhibit 
weights usually directional input units output units may directional especially distinction input output units 
values weights network computational reaction arbitrary input pattern weights encode long term memory knowledge network 
weights change result training tend change slowly accumulated knowledge changes slowly 
contrast activation patterns transient functions current input kind short term memory 
network connected kind topology 
common topologies include unstructured layered recurrent modular networks shown 
kind topology best suited particular type application 
example unstructured networks useful pattern completion retrieving stored patterns supplying part pattern layered networks useful pattern association mapping input vectors output vectors recurrent networks useful pattern sequencing sequences neural network topologies unstructured layered recurrent modular 

review neural networks network activation time modular networks useful building complex systems simpler components 
note unstructured networks may contain cycles recurrent layered networks may may recurrent modular networks may integrate different kinds topologies 
general unstructured networks way connections networks way connections 
connectivity groups units layers complete connecting may random connecting local connecting neighborhood 
completely connected network degrees freedom theoretically learn functions constrained networks desirable 
network degrees freedom may simply memorize training set learning underlying structure problem consequently may generalize poorly new data 
limiting connectivity may help constrain network find economical solutions generalize better 
local connectivity particular helpful reflects topological constraints inherent problem geometric constraints layers visual processing system 

computation computation begins presenting input pattern network clamping pattern activation input units 
activations remaining units computed synchronously parallel system asynchronously time randomized natural order case may 
unstructured networks process called spreading activation layered networks called forward propagation progresses input layer output layer 
feedforward networks networks feedback activations stabilize soon computations reach output layer recurrent networks networks feedback activations may stabilize may follow dynamic trajectory state space units continuously updated 
unit typically updated stages compute unit net input internal activation compute output activation function net input 
standard case shown net input unit just weighted sum inputs output activation incoming unit ji weight unit unit certain networks support called sigma pi connections shown activations multiplied allowing gate weighted 
case net input ji 
fundamentals neural networks name sigma pi transparently derived 
general net input offset variable bias term example equation practice bias usually treated weight connected invisible unit activation bias automatically included equation summation range includes invisible unit 
computed unit net input compute output activation function activation function called transfer function deterministic stochastic local nonlocal 
deterministic local activation functions usually take forms linear threshold sigmoidal shown 
linear case simply powerful multiple layers linear units collapsed single layer functionality 
order construct nonlinear functions network requires nonlinear units 
simplest form nonlinearity provided threshold activation function illustrated panel powerful linear function multilayered network threshold units theoretically compute boolean function 
difficult train network discontinuities function imply finding desired set weights may require exponential search practical learning rule exists single computing unit activations net input activation 
standard unit sigma pi unit 
ji ji 
review neural networks layered networks units limited functionality 
applications continuous outputs preferable binary outputs 
consequently common function sigmoidal function illustrated panel sigmoidal functions advantages nonlinearity differentiability enabling multilayered network compute arbitrary real valued function supporting practical training algorithm backpropagation gradient descent 
nonlocal activation functions useful imposing global constraints network 
example useful force network output activations sum probabilities 
performed linearly normalizing outputs popular approach softmax function operates net inputs directly 
nonlocal functions require overhead hardware biologically implausible useful global constraints desired 
nondeterministic activation functions contrast deterministic ones probabilistic nature 
typically produce binary activation values probability variable called temperature commonly varies time 
shows probability function varies temperature infinite temperature uniform probability function finite temperatures sigmoidal probability functions zero temperature binary threshold probability function 
temperature steadily decreased training process called simulated annealing deterministic local activation functions linear threshold sigmoidal 
exp similarly tanh exp exp exp 
fundamentals neural networks network may able escape local minima trap deterministic gradient descent procedures backpropagation find global minima 
point discussed units activation functions general form common form activation function 
types networks learned vector quantization lvq networks radial basis function rbf networks include units type activation function general form difference types units intuitive geometric interpretation illustrated 
case dot product input vector weight vector length projection shown panel 
projection may point opposite direction may lie side hyperplane perpendicular inputs lie side inputs lie opposite side threshold function equation unit classify input terms side hyperplane lies 
classification fuzzy sigmoidal function threshold function 
contrast second case euclidean distance input vector weight vector weight represents center spherical distribution input space shown panel 
distance function inverted function exp input center cluster activation input infinite distance activation 
case decision regions defined hyperplanes hyperspheres discontinuous continuous boundaries positioned input space carve input space arbitrary ways 
set nondeterministic activation functions probability various temperatures 
ji ji 
review neural networks computation net input 
dot product hyperplane difference hypersphere 
construction complex functions hyperplanes hyperspheres 
ji ji ji ji kj 
fundamentals neural networks decision regions overlapped combined construct arbitrarily complex function including additional layer threshold sigmoidal units illustrated 
task training procedure adjust hyperplanes hyperspheres form accurate model desired function 

training training network general sense means adapting connections network exhibits desired computational behavior input patterns 
process usually involves modifying weights moving hyperplanes hyperspheres involves modifying actual topology network adding deleting connections network adding deleting hyperplanes hyperspheres 
sense weight modification general topology modification network abundant connections learn set weights zero effect deleting weights 
topological changes improve generalization speed learning constraining class functions network capable learning 
topological changes discussed section section focus weight modification 
finding set weights enable network compute function usually nontrivial procedure 
analytical solution exists simplest case pattern association network linear goal map set orthogonal input vectors output vectors 
case weights input vector target vector pattern index 
general networks nonlinear multilayered weights trained iterative procedure gradient descent global performance measure hinton 
requires multiple passes training entire training set person learning new skill pass called iteration epoch 
accumulated knowledge distributed weights weights modified gently destroy previous learning 
small constant called learning rate control magnitude weight modifications 
finding value learning rate important value small learning takes forever value large learning disrupts previous knowledge 
unfortunately analytical method finding optimal learning rate usually optimized empirically just trying different values 
training procedures including equation essentially variations hebb rule hebb reinforces connection units output activations correlated ji ji ey 
review neural networks reinforcing correlation active pairs units training network prepared activate second unit known testing 
important variation rule delta rule widrow hoff rule applies target value units 
rule reinforces connection units correlation unit activation second unit error potential error reduction relative target rule decreases relative error contributed network prepared compute output closer unit activation known testing 
context binary threshold units single layer weights delta rule known perceptron learning rule guaranteed find set weights representing perfect solution solution exists rosenblatt 
context multilayered networks delta rule basis backpropagation training procedure discussed greater detail section 
variation hebb rule applies case spherical functions lvq rbf networks rule moves spherical center ji closer input pattern output class active 

taxonomy neural networks basic elements neural networks give overview different types networks 
overview organized terms learning procedures networks 
main classes learning procedures supervised learning teacher provides output targets input pattern corrects network errors explicitly semi supervised reinforcement learning teacher merely indicates network response training pattern bad unsupervised learning teacher network find regularities training data 
networks fall squarely categories various anomalous networks hybrid networks straddle categories dynamic networks architectures grow shrink time 
ji ey ji ji 
taxonomy neural networks 
supervised learning supervised learning means teacher provides output targets input pattern corrects network errors explicitly 
paradigm applied types networks feedforward recurrent nature 
discuss cases separately 

feedforward networks perceptrons rosenblatt simplest type feedforward networks supervised learning 
perceptron comprised binary threshold units arranged layers shown 
trained delta rule equation variations thereof 
case single layer perceptron shown delta rule applied directly 
perceptron activations binary general learning rule reduces perceptron learning rule says input active output wrong ji increased decreased small amount depending desired output respectively 
procedure guaranteed find set weights correctly classify patterns training set patterns linearly separable separated classes straight line illustrated 
training sets linearly separable consider simple xor function example cases require multiple layers 
multi layer perceptrons mlps shown theoretically learn function complex train 
delta rule applied directly mlps targets hidden layer 
mlp uses continuous discrete activation functions sigmoids threshold functions possible partial derivatives chain rule derive influence weight output activation turn indicates modify weight order reduce network error 
generalization delta rule known backpropagation discussed section 
perceptrons 
single layer perceptron multi layer perceptron 
inputs hidden outputs inputs outputs 
review neural networks mlps may number hidden layers single hidden layer sufficient applications additional hidden layers tend training slower terrain weight space complicated 
mlps architecturally constrained various ways instance limiting connectivity geometrically local areas limiting values weights tying different weights 
type constrained mlp especially relevant thesis time delay neural network tdnn shown 
architecture initially developed phoneme recognition lang waibel applied handwriting recognition lipreading bregler tasks 
tdnn operates dimensional input fields horizontal dimension time connections time delayed extent connected units temporally 
tdnn special architectural features 
time delays hierarchically structured higher level units integrate temporal context perform higher level feature detection 

weights tied time axis corresponding weights different temporal positions share value network relatively free parameters generalize 

output units temporally integrate results local feature detectors distributed time network shift invariant recognize patterns matter occur time 

assuming task speech recognition task temporal domain 
time delay neural network 
integration speech input phoneme output tied weights tied weights time inputs time delayed connections hidden 
taxonomy neural networks tdnn trained standard backpropagation 
unusual aspect training tdnn tied weights modified averaged error signal independently 
network classify input patterns learned vector quantization lvq network kohonen 
lvq network single layered network outputs represent classes weights inputs represent centers hyperspheres shown 
training involves moving hyperspheres cover classes accurately 
specifically training pattern best output incorrect second best output correct near midpoint hyperspheres move away 
recurrent networks hopfield studied neural networks implement kind content addressable associative memory 
worked unstructured networks binary threshold units symmetric connections ji ij activations updated asynchronously type recurrent network called hopfield network 
hopfield showed weights network modified hebb rule training patterns attractors state space 
words network corrupted version patterns network activations updated random asynchronous manner previously trained weights network gradually reconstruct activation pattern closest pattern state space stabilize pattern 
hopfield key insight analyze network dynamics terms global energy function necessarily decreases remains unit activation updated reaches minimum value activation patterns corresponding stored memories 
implies network settles stable state may reach local minimum corresponding spurious memory arising interference stored memories 
boltzmann machine ackley hopfield network hidden units stochastic activations simulated annealing learning procedure 
features contributes exceptional power 
hidden units allow boltzmann machine find higher order correlations data hopfield network find learn arbitrarily complex patterns 
stochastic temperature activations shown allow boltzmann machine escape local minima state evolution 
simulated annealing steadily decreasing temperatures training helps 
training algorithm described known lvq improvement original lvq training algorithm 
ji 
review neural networks network learn efficiently low temperature vigorously shaking network viable neighborhoods weight space early training gently network globally optimal positions training 
training boltzmann machine involves modifying weights reduce difference observed probability distributions temperature probability averaged environmental inputs measured equilibrium ith jth units active visible units inputs outputs clamped values corresponding probability system free running clamped 
learning tends extremely slow boltzmann machines uses gradient descent temperature annealing schedule wait network come equilibrium collect lots statistics clamped behavior 
boltzmann machines theoretically powerful successfully applied problems 
types recurrent networks layered structure connections feed back earlier layers 
shows examples known jordan network jordan elman network elman 
networks feature set context units activations copied outputs hidden units respectively fed forward hidden layer supplementing inputs 
context units give networks kind decaying memory proven sufficient learning temporal structure short distances generally long distances 
networks trained standard backpropagation trainable weights feedforward weights 

semi supervised learning semi supervised learning called reinforcement learning external teacher provide explicit targets network outputs evaluates network behavior bad 
different types semi supervised networks distinguished layered recurrent networks 
jordan network elman network 
ji ij ij ij ij copy inputs context hidden outputs copy inputs context hidden outputs 
taxonomy neural networks topologies fairly arbitrary nature environment learning procedures 
environment may static dynamic definition behavior may fixed may change time likewise evaluations may deterministic probabilistic 
case static environments deterministic stochastic evaluations networks trained associative reward penalty algorithm barto anandan 
algorithm assumes stochastic output units enable network try various behaviors 
problem semi supervised learning reduced problem supervised learning setting training targets actual outputs negations depending network behavior judged bad network trained delta rule targets compared network mean outputs error backpropagated network necessary 
approach applied static dynamic environments introduce auxiliary network tries model environment munro 
auxiliary network maps environmental data consisting input output network reinforcement signal 
problem semi supervised learning reduced stages supervised learning known targets auxiliary network trained properly model environment backpropagation applied networks output original network distinct error signal coming auxiliary network 
similar approach applies dynamic environments enhance auxiliary network critic sutton maps environmental data plus reinforcement signal prediction reinforcement signal 
comparing expected actual reinforcement signal determine original network performance exceeds falls short expectation reward punish accordingly 

unsupervised learning unsupervised learning teacher network detect regularities input data 
self organizing networks compressing clustering quantizing classifying mapping input data 
way perform unsupervised training recast paradigm supervised training designating artificial target input pattern applying backpropagation 
particular train network reconstruct input patterns output layer passing data bottleneck hidden units 
network learns preserve information possible hidden layer hidden layer compressed representation input data 
type network called encoder especially inputs outputs binary vectors 
say network performs dimensionality reduction 
types unsupervised networks usually hidden units trained hebbian learning equation 
hebbian learning example train sin 
review neural networks gle linear unit recognize familiarity input pattern extension train set linear output units project input pattern principal components distribution forming compressed representation inputs output layer 
linear units standard hebb rule cause weights grow bounds rule modified prevent weights growing large 
viable modifications sanger rule sanger viewed form weight decay krogh hertz 
rule uses nonlocal information nice property weight vectors converge principal component directions order normalized unit length 
linsker showed modified hebbian learning rule applied multilayered network layer planar geometrically local connections layer human visual system automatically develop useful feature detectors center surround cells orientation selective cells similar human visual system 
unsupervised networks competitive learning output unit considered winner known winner take networks 
winning unit may lateral inhibitory connections output units drive losing activations zero simply comparative inspection output activations 
competitive learning useful clustering data order classify quantize input patterns 
note weights output unit normalized maximizing net input equivalent minimizing difference goal training seen moving weight vectors centers input clusters minimize distance 
standard competitive learning rule equation outputs truly learning rule simplifies applied winning output unfortunately learning procedure units may far away inputs win learn 
dead units avoided initializing weights match actual input samples relaxing winner take constraint losers learn winners number mechanisms hertz krogh palmer 
carpenter grossberg developed networks called art art adaptive resonance theory networks binary continuous inputs respectively support competitive learning way new cluster formed input pattern sufficiently different existing cluster vigilance parameter 
clusters represented individual output units usual art network output units reserved needed 
network uses search procedure implemented hardware 
ji ki 
taxonomy neural networks kohonen developed competitive learning algorithm performs feature mapping mapping patterns input space output space preserving topological relations 
learning rule augments standard competitive learning rule neighborhood function measuring topological proximity unit winning unit units near strongly affected distant units affected 
example map input coefficients dimensional set output units map dimensional set inputs different dimensional representation occurs different layers visual somatic processing brain 

hybrid networks networks combine supervised unsupervised training different layers 
commonly unsupervised training applied lowest layer order cluster data backpropagation applied higher layer associate clusters desired output patterns 
example radial basis function network moody darken hidden layer contains units describe hyperspheres trained standard competitive learning algorithm output layer computes normalized linear combinations receptive field functions trained delta rule 
attraction hybrid networks reduce multilayer backpropagation algorithm single layer delta rule considerably reducing training time 
hand networks trained terms independent modules integrated somewhat accuracy networks trained entirely backpropagation 

dynamic networks networks discussed far static architecture 
dynamic networks architecture change time order attain optimal performance 
changing architecture involves deleting adding elements weights units network opposite approaches called pruning construction respectively 
approaches pruning tends simpler involves merely ignoring selected elements constructive algorithms tend faster networks small lives 
pruning course requires way identify useful elements network 
straightforward technique delete weights smallest magnitude improve generalization eliminates wrong weights hassibi stork 
complex reliable approach called optimal brain damage le cun identifies weights removal cause increase network output error function requires calculation second derivative information 
ji ji 
review neural networks constructive algorithms cascade correlation algorithm fahlman lebiere popular effective 
algorithm starts hidden units gradually adds depth fashion long help cut remaining output error 
stage training previously trained weights network frozen pool new candidate units connected existing non output units candidate unit trained maximize correlation unit output network residual error effective unit fully integrated network candidates discarded weights output layer fine tuned 
process repeated network acceptable performance 
cascade correlation algorithm quickly construct compact powerful networks exhibit excellent performance 
developed constructive algorithm called automatic structure optimization designed temporal tasks speech recognition online handwriting recognition especially limited training data 
aso algorithm starts small network adds resources including connections time delays hidden units state units class dependent way guidance confusion matrices obtained cross validation training order minimize classification error 
aso algorithm automatically optimized architecture ms achieving results competitive state art systems optimized hand 

backpropagation backpropagation known error backpropagation generalized delta rule widely supervised training algorithm neural networks 
importance discuss detail section 
full derivation learning rule 
feedforward neural network highlighting connection unit unit input hidden output ji 
backpropagation suppose multilayered feedforward network nonlinear typically sigmoidal units shown 
want find values weights enable network compute desired function input vectors output vectors 
units compute nonlinear functions solve weights analytically gradient descent procedure global error function define arbitrary unit indices set output units training pattern indices training pattern contains input vector output target vector net input unit pattern output activation unit pattern weight unit unit target activation unit pattern global output error training pattern global error entire training set 
assuming common type network essential activation function differentiable opposed nondifferentiable simple threshold function computing gradient moment 
choice error function somewhat arbitrary assume sum squared error function want modify weight proportion influence error direction reduce small constant called learning rate 

popular choices global error function include sum squared error cross entropy mcclelland error classification merit difference best incorrect output correct output example ji ji log log log ji ji ji 
review neural networks chain rule equations expand follows terms introduces shorthand definition remains expanded 
exactly expanded depends output unit 
output unit equation output unit directly affects set units illustrated chain rule obtain recursion equation refers says dw layer derived directly layer 
derive multilayer network starting output layer equation working way backwards input layer layer time equation 
learning procedure called backpropagation error terms propagated network backwards direction 
unit output unit directly affects units layer 
ji ji def 
kj kj 
backpropagation summarize learning rule equivalently wish define backpropagation faster learning procedure boltzmann machine training algorithm take long time converge optimal set weights 
learning may accelerated increasing learning rate certain point learning rate large weights excessive units saturated learning impossible 
number heuristics developed accelerate learning 
techniques generally motivated intuitive image backpropagation gradient descent procedure 
envision hilly landscape representing error function weight space backpropagation tries find local minimum value incremental steps current direction image helps see example take large step run risk moving far current find shooting nearby possibly higher error 
bearing image mind common heuristic accelerating learning process known momentum rumelhart tends push weights useful direction ji kj ji kj dw ji ji 
review neural networks momentum constant usually 
heuristic causes step size steadily increase long keep moving long gentle valley recover behavior error surface forces change direction 
elaborate powerful heuristic second derivative information estimate far travel techniques conjugate gradient barnard quickprop fahlman 
ordinarily weights updated training pattern called online training 
effective update weights accumulating gradients batch training patterns called batch training superimposing error landscapes training patterns find direction move best group patterns confidently take larger step direction 
batch training especially helpful training patterns uncorrelated eliminates waste brownian motion conjunction aggressive heuristics quickprop require accurate estimates landscape surface 
backpropagation simple gradient descent procedure unfortunately susceptible problem local minima may converge set weights locally optimal globally suboptimal 
experience shown local minima tend cause problems artificial domains boolean logic real domains perceptual processing reflecting difference terrain weight space 
case possible deal problem local minima adding noise weight modifications 

relation statistics neural networks close relationship standard statistical techniques 
section discuss commonalities 
important tasks statistics classification data 
suppose want classify input vector classes obviously decision correspond class highest probability correct decide class 
normally posterior probabilities known inverse information probability distributions may known 
convert posterior probabilities distributions bayes rule ji ji ji 
relation statistics follows directly choose class criterion known bayes decision rule 
perfect knowledge distributions priors decision rule guaranteed minimize classification error rate 
typically distributions priors posteriors unknown collection sample data points 
case analyze model data order classify new data accurately 
existing data labeled try estimate posterior probabilities distributions priors bayes decision rule alternatively try find boundaries separate classes trying model probabilities explicitly 
data unlabeled try cluster order identify meaningful classes 
tasks performed statistical procedure neural network 
example labeled data wish perform bayesian classification statistical techniques available modeling data duda hart 
include parametric nonparametric approaches 
parametric approach assume distribution parametric form gaussian density try estimate parameters commonly done procedure called maximum likelihood estimation finds parameters maximize likelihood having generated observed data 
non parametric approach may volumetric technique called parzen windows estimate local density samples point robustness technique improved scaling local volume contains samples variation called nearest neighbor estimation 
priors estimated simply counting 
alternatively posterior probability estimated nonparametric techniques nearest neighbor rule classifies agreement majority nearest neighbors 
neural network supports bayesian classification forming model training data 
specifically multilayer perceptron asymptotically trained classifier mean squared error mse similar error function output activations learn approximate posterior probability accuracy improves size training set 
proof important fact appendix way labeled training data find boundaries separate classes 
statistics accomplished general technique called discriminant analysis 
important instance fisher linear discriminant finds line gives best discrimination classes data points projected line 
line equivalent weight vector single layer perceptron single output trained discriminate classes delta rule 
case classes optimally separated hyperplane drawn perpendicular line weight vector shown 
unlabeled data clustered statistical techniques nearest neighbor clustering minimum squared error clustering means clustering krishnaiah kanal 
review neural networks alternatively neural networks trained competitive learning 
fact means clustering exactly equivalent standard competitive learning rule equation batch updating hertz 
analyzing high dimensional data desirable reduce dimensionality project lower dimensional space preserving information possible 
dimensionality reduction performed statistical technique called principal components analysis pca finds set orthogonal vectors account greatest variance data jolliffe 
dimensionality reduction performed types neural networks 
example single layer perceptron trained unsupervised competitive learning rule called sanger rule equation yields weights equal principal components training data network outputs form compressed representation input vector 
similarly encoder network multilayer perceptron trained backpropagation reproduce input vectors output layer forms compressed representation data hidden units 
claimed neural networks simply new formulation old statistical techniques 
considerable overlap fields neural networks attractive right offer general uniform intuitive framework applied equally statistical non statistical contexts 

related research 
early neural network approaches speech recognition basically pattern recognition problem neural networks pattern recognition early researchers naturally tried applying neural networks speech recognition 
earliest attempts involved highly simplified tasks classifying speech segments voiced unvoiced nasal fricative 
success experiments encouraged researchers move phoneme classification task proving ground neural networks quickly achieved world class results 
techniques achieved success level word recognition clear scaling problems discussed 
basic approaches speech classification neural networks static dynamic illustrated 
static classification neural network sees input speech single decision 
contrast dynamic classification neural network sees small window speech window slides input speech network series local decisions integrated global decision time 
static classification works phoneme recognition scales poorly level words sentences dynamic classification scales better 
approach may recurrent connections recurrence dynamic approach 
static dynamic approaches classification 
static classification dynamic classification input speech pattern outputs 
related research sections briefly review representative experiments phoneme word classification static dynamic approaches 

phoneme classification phoneme classification performed high accuracy static dynamic approaches 
review typical experiments approach 

static approaches simple elegant experiment performed huang lippmann demonstrating neural networks form complex decision surfaces speech data 
applied multilayer perceptron inputs hidden units outputs peterson barney collection vowels produced men women children formants vowels input speech representation 
iterations training network produced decision regions shown 
decision regions nearly optimal resembling decision regions drawn hand yield classification accuracy comparable conventional algorithms nearest neighbor gaussian classification 
complex experiment elman zipser trained network classify vowels consonants occur utterances ba bi bu da di du ga gi gu 
network input consisted spectral coefficients frames covering entire msec utterance centered hand consonant voicing onset fed hidden layer units leading outputs vowel consonant classification 
network achieved error rates roughly vowels consonants 
analysis hidden units showed tend fea decision regions formed layer perceptron backpropagation training vowel formant data 
huang lippmann 

early neural network approaches ture detectors discriminating important classes sounds consonants versus vowels 
difficult classification tasks called set discriminating english letters 
burr applied static network task results 
network input window spectral frames automatically extracted utterance energy information 
inputs led directly outputs representing set letters 
network trained tested tokens single speaker 
early portion utterance oversampled effectively highlighting disambiguating features recognition accuracy nearly perfect 

dynamic approaches seminal waibel demonstrated excellent results phoneme classification time delay neural network tdnn shown 
architecture delays input hidden layer respectively final output computed integrating frames phoneme activations second hidden layer 
tdnn design attractive reasons compact structure weights forces network develop general feature detectors hierarchy delays optimizes feature detectors increasing scope layer temporal integration output layer network shift invariant insensitive exact positioning speech 
tdnn trained tested samples phonemes manually excised database japanese words 
tdnn achieved error rate compared achieved simple hmm recognizer 
time delay neural network 
integration speech input phoneme output 
related research waibel tdnn scaled recognize japanese consonants modular approach significantly reduced training time giving slightly better results simple tdnn outputs 
modular approach consisted training separate small subsets phonemes combining networks larger network supplemented glue connections received little extra training primary modules remained fixed 
integrated network achieved error rate phonemes compared achieved relatively advanced hmm recognizer 
mcdermott performed interesting comparison waibel tdnn kohonen lvq algorithm database similar conditions 
lvq system trained quantize frame window spectral coefficients codebook entries testing distance input window nearest codebook vector integrated frames tdnn produce shift invariant phoneme hypothesis 
lvq system achieved virtually error rate tdnn vs lvq faster training slower testing memory intensive tdnn 
contrast feedforward networks described recurrent networks generally trickier slower train theoretically powerful having ability represent temporal sequences unbounded depth need artificial time delays 
speech temporal phenomenon researchers consider recurrent networks appropriate feedforward networks researchers begun applying recurrent networks speech 
prager harrison fallside early attempt apply boltzmann machines vowel recognition task 
typical experiment represented spectral inputs binary inputs vowel classes binary outputs network hidden units weights 
applying simulated annealing hours order train tokens speakers boltzmann machine attained error rate 
experiments suggested boltzmann machines give accuracy impractically slow train 
watrous applied recurrent networks set basic discrimination tasks 
system decisions temporally integrated recurrent connections output units explicit time delays tdnn training targets gaussian shaped pulses constant values match behavior recurrent outputs 
watrous obtained results variety discrimination tasks optimizing non output delays sizes networks separately task 
example classification error rate consonants vowels word pair rapid 
robinson fallside applied kind recurrent network proposed jordan phoneme classification 
network output activations copied context layer fed back additional inputs hidden layer shown 
network trained back propagation time algorithm suggested rumelhart unfolds replicates network moment time 
recurrent network outperformed feedforward network 
early neural network approaches comparable delays achieving versus error speaker dependent recognition versus error multi speaker recognition 
training time reduced reasonable level processor array transputers 

word classification word classification performed static dynamic approaches dynamic approaches better able deal temporal variability duration word 
section review experiments approach 

static approaches peeling moore applied mlps digit recognition excellent results 
static input buffer frames seconds spectral coefficients long longest spoken word words padded zeros positioned randomly frame buffer 
evaluating variety mlp topologies obtained best performance single hidden layer units 
network achieved accuracy near advanced hmm system error rates versus speaker dependent experiments versus multi speaker experiments speaker database digits 
addition mlp typically times faster hmm system 
applied variety networks ti word database finding single layer perceptron outperformed multi layer perceptrons dtw template recognizer cases 
static input buffer frames word linearly normalized bit coefficients frame performance improved slightly training data augmented temporally distorted tokens 
error rates slp versus dtw versus speaker dependent experiments versus speaker independent experiments 
lippmann points results impressive mitigated evidence small vocabulary tasks really difficult 
burton demonstrated simple recognizer word vector quantization time alignment achieve speaker dependent error rates low ti word database digits 
surprising simple networks achieve results tasks temporal information important 
burr applied mlps difficult task alphabet recognition 
static input buffer frames spoken letter linearly normalized spectral coefficients frame 
training sets spoken letters testing fourth set mlp achieved error rate speaker dependent experiments matching accuracy dtw template approach 

dynamic approaches lang applied word recognition results 
vocabulary consisting highly confusable spoken letters 
early experiments 
related research training testing simplified representing word msec segment centered vowel segment words differed 
pre segmented data tdnn achieved error rate 
experiments need pre segmentation avoided classifying word output received highest activation position input window relative utterance training msec segments roughly centered vowel onsets automatic energy segmentation technique 
mode tdnn achieved error rate 
error rate fell network received additional negative training counter examples randomly selected background sounds 
system compared favorably hmm achieved error task bahl 
tank hopfield proposed time concentration network represents words weighted sum evidence delayed proportional dispersion word activation concentrated correct word output utterance 
system inspired research auditory processing bats working prototype implemented parallel analog hardware 
reported results network simple digit strings gold obtained results better standard hmm applied hierarchical version network large speech database 
early studies recurrent networks prager harrison fallside configured boltzmann machine copy output units state units fed back hidden layer called jordan network representing kind order markov model 
days training network able correctly identify words training sentences 
researchers likewise obtained results boltzmann machines amount training 
witbrock lee compared performance recurrent network feedforward network digit recognition task 
feedforward network mlp msec input window recurrent network shorter msec input window msec state buffer 
significant difference recognition accuracy systems suggesting important network form memory regardless represented feedforward input buffer recurrent state layer 

problem temporal structure seen phoneme recognition easily performed static dynamic approaches 
seen word recognition likewise performed approach dynamic approaches preferable wider temporal variability word implies invariances localized local features temporally integrated 
temporal integration easily performed network output layer tdnn long operation 
nn hmm hybrids described statically match network fixed resources consider larger chunks speech greater temporal variability harder map variability static framework 
continue scaling task word recognition sentence recognition temporal variability severe acquires new dimension compositional structure governed grammar 
ability compose structures simpler elements implying usage sort variables binding modularity rules clearly required system claims support natural language processing pinker prince mention general cognition fodor pylyshyn 
unfortunately proven difficult model compositionality pure connectionist framework number researchers achieved early limited success lines 
touretzky hinton designed distributed connectionist production system dynamically retrieves elements working memory uses components new states 
smolensky proposed mechanism performing variable binding tensor products 
servan schreiber cleeremans mcclelland elman network capable learning aspects grammatical structure 
jain designed modular highly structured connectionist natural language parser compared favorably standard lr parser 
systems exploratory nature techniques generally applicable 
clear connectionist research temporal compositional modeling infancy premature rely neural networks temporal modeling speech recognition system 

nn hmm hybrids seen neural networks excellent acoustic modeling parallel implementations weak temporal compositional modeling 
seen hidden markov models models weaknesses 
section review ways researchers tried combine approaches various hybrid systems capitalizing strengths approach 
research section conducted time thesis written 

nn implementations hmms simplest way integrate neural networks hidden markov models simply implement various pieces hmm systems neural networks 
improve accuracy hmm permit parallelized natural way incidentally flexibility neural networks 
lippmann gold introduced viterbi net illustrated neural network implements viterbi algorithm 
input temporal sequence speech frames time final output time frames 
related research cumulative score viterbi alignment path permitting isolated word recognition subsequent comparison outputs viterbi nets running parallel 
viterbi net continuous speech recognition yields backtrace information alignment path recovered 
weights lower part viterbi net preassigned way node computes local score state current time frame implementing gaussian classifier 
upper networks compute maximum inputs 
triangular nodes threshold logic units simply sum inputs output zero sum negative delay output time frame synchronization purposes 
network implements left right hmm self transitions final output represents cumulative score state time optimal alignment path 
tested word tokens speaker word lincoln stress style speech database obtained results essentially identical standard hmm error 
similar spirit bridle introduced neural network computes forward probability hmm producing partial sequence state isolated words recognized comparing final scores 
motivates construction 
panel illustrates basic recurrence second panel shows recurrence may implemented recurrent network 
third panel shows additional term factored equation sigma pi units properly computes 
frame level training simply reimplementing hmm neural networks researchers exploring ways enhance hmms designing hybrid systems capitalize respective strengths technology temporal modeling hmm viterbi net neural network implements viterbi algorithm 
output inputs ij ij 
nn hmm hybrids tic modeling neural networks 
particular neural networks trained compute emission probabilities hmms 
neural networks suited mapping task theoretical advantage hmms discrete density hmms accept continuous valued inputs don suffer quantization errors continuous density hmms don dubious assumptions parametric shape density function 
ways design train neural network purpose 
simplest map frame inputs directly emission symbol outputs train network frame frame basis 
approach called frame level training 
frame level training extensively studied researchers philips icsi sri 
initial bourlard focused theoretical links hidden markov models neural networks establishing neural networks estimate posterior probabilities divided priors order yield likelihoods hmm 
subsequent icsi sri morgan bourlard renals bourlard morgan confirmed insight series experiments leading excellent results resource management database 
simple mlps experiments typically input window speech frames phoneme output units hundreds thousands hidden units advantage fact hidden units gave better results parallel computer train millions weights reasonable amount time 
results depended careful neural networks techniques included online training random sampling training data cross validation step size adaptation heuristic bias initialization division priors recognition 
baseline system achieved word error rm database speaker independent phoneme models improved adding multiple pronunciations cross word modeling improved interpolating likelihoods obtained mlp sri decipher system obtained similar conditions 
demonstrated number parameters mlp outperform hmm achieving vs word error parameters mlp fewer questionable assumptions parameter space 
construction final panel 
ij jj ij jj ij jj 
related research lee waibel studied frame level training 
started hmm emission probabilities represented histogram vq codebook replaced mechanism neural network served purpose targets network continuous probabilities binary classes bourlard colleagues 
network input window containing frames speech msec output unit probability distribution modeled network hidden layers recurrent buffer past copies hidden layer fed back hidden layer variation elman network architecture 
buffer represented msec history input window advanced frames msec time 
system evaluated ti nbs speaker independent continuous digits database achieved word recognition accuracy close best known result 

segment level training alternative frame level training segment level training neural network receives input entire segment speech duration phoneme single frame fixed window frames 
allows network take better advantage correlation exists frames segment easier incorporate segmental information duration 
drawback approach speech segmented neural network evaluate segments 
tdnn waibel represented early attempt segment level training output units designed integrate partial evidence duration phoneme network trained phoneme level frame level 
tdnn input window assumed constant width frames phonemes truly operate segment level architecture applied phoneme recognition word recognition 
austin bbn explored true segment level training large vocabulary continuous speech recognition 
segmental neural network snn trained classify phonemes variable duration segments speech variable duration segments linearly downsampled uniform width frames snn 
phonemic segmentations provided state art hmm system 
training snn taught correctly classify segment utterance 
testing snn segmentations best sentence hypotheses hmm snn produced composite score sentence product scores duration probabilities segments snn scores hmm scores combined identify single best sentence 
system achieved word error rm database 
performance improved error snn trained negatively 
hmm output symbols emitted transitions states output unit transition state 

duration probabilities provided smoothed histogram durations obtained training data 

nn hmm hybrids incorrect segments best sentence hypotheses preparing system kinds confusions encounter best lists testing 

word level training natural extension segment level training word level training neural network receives input entire word directly trained optimize word classification accuracy 
word level training appealing brings training criterion closer ultimate testing criterion sentence recognition accuracy 
unfortunately extension nontrivial contrast simple phoneme word adequately modeled single state requires sequence states activations states simply summed time tdnn segmented dynamic time warping procedure dtw identifying states apply frames 
word level training requires dtw embedded neural network 
achieved sakoe architecture called dynamic programming neural network dnn 
dnn network hidden units represent states output units represent words 
word unit alignment path states inputs established dtw output unit integrates activations hidden units states alignment path 
network trained output correct word unit incorrect word units 
dtw alignment path may static established training begins dynamic reestablished iteration training static alignment obviously efficient dynamic alignment shown give better results 
dnn applied japanese database isolated digits achieved word accuracy outperforming pure dtw 
haffner similarly incorporated dtw high performance tdnn architecture yielding multi state time delay neural network ms tdnn illustrated 
contrast sakoe system ms tdnn extra hidden layer hierarchy time delays may form powerful feature detectors dtw path accumulates score frame score state easily extended continuous speech recognition ney 
ms tdnn applied database spoken letters achieved average word accuracy compared sphinx ms tdnn benefitted novel techniques including transition states adjacent phonemic states iy iy states set linear combination activations iy specially trained boundary detection units allowed word transitions activation exceeded threshold value 
hild waibel improved haffner ms tdnn achieving word accuracy database spoken letters resource management spell mode database 
improvements included free alignment word boundaries dtw segment speech wider word identify word boundaries dynamically training word duration modeling penalizing words add 
comparison sphinx advantage context dependent phoneme models ms tdnn context independent models 

related research ing logarithm duration probabilities derived histogram scaled factor balances insertions deletions sentence level training training positively correct alignment path training negatively incorrect parts alignment path obtained testing 
applied ms tdnn large vocabulary continuous speech recognition 
detailed thesis 

global optimization trend nn hmm hybrids global optimization system parameters relaxing system performance handicapped false assumptions 
segment level training word level training important steps global optimization bypass rigid assumption frame accuracy correlated word accuracy making training criterion consistent testing criterion 
step global optimization pursued bengio joint optimization input representation rest system 
bengio proposed hybrid speech frames produced combination signal analysis ms tdnn recognizing word 
activations words sil shown 
hild waibel 
input layer hidden layer phoneme layer dtw layer word layer copy weights time delays time delays hild ps 
nn hmm hybrids neural networks speech frames serve inputs ordinary hmm 
neural networks trained produce increasingly useful speech frames backpropagating error gradient derives hmm optimization criterion neural networks hmm optimized simultaneously 
technique evaluated task speaker independent recognition distinguishing phonemes dx 
hmm trained separately neural networks recognition accuracy trained global optimization recognition accuracy jumped 

context dependence known accuracy hmm improves context sensitivity acoustic models 
particular context dependent models triphones perform better context independent models phonemes 
led researchers try improve accuracy hybrid nn hmm systems likewise making context sensitive 
ways achieve illustrated 
approaches context dependent modeling 
speech hidden classes classes classes classes hidden speech classes hidden speech context window input frames context dependent outputs context input classes speech classes context hidden hidden factorization 
related research technique simply provide window speech frames single frame input network 
arbitrary width input window constrained computational requirements diminishing relevance distant frames 
technique trivial useful neural network virtually nn hmm hybrids combination remaining techniques section 
contrast standard hmm independence assumption prevents system advantage neighboring frames directly 
way hmm exploit correlation neighboring frames artificially absorbing current frame defining multiple simultaneous streams data impart frames deltas lda transform streams single stream 
window input frames provides context sensitivity context dependence 
context dependence implies separate model context model embedded context kab separate model embedded context tap techniques support true context dependent modeling hybrids 
technique naive approach separate output unit contextdependent model 
example phonemes require outputs order model phonemes context immediate neighbor outputs model triphones phonemes context left right neighbor 
obvious problem approach shared analogous hmms training data adequately train parameters system 
consequently approach rarely practice 
economical approach single network accepts description context part input suggested 
left phoneme context dependence example implemented boolean localist representation left phoneme compactly binary encoding linguistic features principal components discovered automatically encoder network 
note order model triphones need double number context units times models 
training efficient full context available training sentences testing may require forward passes different contextual inputs context known 
showed contextual inputs 
left standard implementation 
right efficient implementation 
outputs hidden speech context outputs speech context hid hid 
nn hmm hybrids forward passes efficient heuristically splitting hidden layer shown speech context feed independent parts context effectively contributes different bias output units training complete contextual output biases precomputed reducing family forward passes family output sigmoid computations 
contextual inputs helped increase absolute word accuracy system 
bourlard proposed fourth approach context dependence factorization 
neural network trained phoneme classifier estimates phoneme class speech input 
introduce context dependence estimate phonetic context 
decomposed follows says context dependent probability equal product terms output activation standard network output activation auxiliary network inputs speech current phoneme class outputs range contextual phoneme classes illustrated 
resulting context dependent posterior converted likelihood bayes rule ignored recognition constant frame prior evaluated directly training set 
factorization approach easily extended triphone modeling 
triphones want estimate left phonetic context right phonetic context 
decomposed follows similarly terms estimated neural networks inputs outputs correspond fact terms equation simple evaluated directly training data 
posterior equation converted likelihood bayes rule 
related research ignored recognition terms taken outputs neural networks 
likelihood viterbi alignment 
approach family forward passes recognition reduced family output sigmoid computations splitting hidden layer caching effective output biases contextual inputs 
preliminary experiments showed splitting hidden layer way degrade accuracy network triphone models rendered times slower monophone models 

speaker independence experience hmms shown speaker independent systems typically times errors speaker dependent systems lee simply greater variability speakers single speaker 
hmms typically deal problem merely increasing number context dependent models hope better covering variabilities speakers 
nn hmm hybrids suffer similar gap performance speaker dependence speaker independence 
example lvq hybrid obtained average error speaker dependent data versus error network applied speaker independent data 
techniques aimed closing gap developed nn hmm hybrids 
illustrates baseline approach training standard network data speakers panel followed improvements 
improvement shown technique mixture speaker dependent models resembling mixture experts paradigm promoted jacobs 
approach networks trained independently data different speakers speaker id network trained identify corresponding speaker recognition speech networks parallel outputs speaker id network specify linear combination speaker dependent networks yield result 
approach easier classify phones correctly separates reduces overlap distributions come different speakers 
yields accuracy close speaker dependent accuracy source degradation imperfect speaker identification 
researchers studied approach hampshire waibel approach meta pi network consisted speaker dependent plus speaker id network containing unit tdnn trained backpropagation 
network obtained phoneme accuracy multi speaker mode significantly outperforming 
multi speaker evaluation means testing speakers training set 

nn hmm hybrids baseline tdnn obtained accuracy 
remarkably speakers mht obtained phoneme accuracy speaker id network failed recognize ignored outputs mht tdnn network system formed robust linear combination speakers resembled 
kubala schwartz adapted approach standard hmm system mixing speaker dependent hmms fixed weights speaker id network 
speaker dependent hmms needed order attain word recognition accuracy baseline system trained speakers comparable amount total data case 
cheaper collect large amount data speakers approaches speaker independent modeling 
speech hidden hidden hidden classes classes classes classes hidden speech classes hidden speech cluster baseline mixture speaker dependent models biased speaker cluster speech speech hidden speaker normalization speaker id classes multiplicative weights simple network trained speakers speech recognizer 
related research collect small amount data speakers kubala schwartz concluded technique valuable reducing cost data collection 
incorporated approach lvq hmm hybrid continuous speech recognition 
speaker biased phoneme models pooled males pooled females individuals mixed correspondingly generalized speaker id network activations separate phonemes established rapid adaptation sentences 
rapid adaptation bought small improvement speaker independent results vs word accuracy speaker biased models system 
long term adaptation system parameters received additional training correctly recognized test sentences resulted greater improvement falling short speaker dependent accuracy 
hild waibel performed battery experiments ms spelled letter recognition determine best level speaker parameter specificity networks best way mix networks 
segregating speakers better pooling degree parameter sharing segregated networks helpful limited training data 
particular best mix lower layers shared structure higher layers 
mixing networks results brief adaptation phase generally effective instantaneous speaker id network technique gives comparable results multi speaker testing 
applying best techniques resource management spell mode database obtained word accuracy outperforming sphinx 
way improve speaker independent accuracy bias network extra inputs characterize speaker shown 
extra inputs determined automatically input speech represent sort cluster speaker belongs 
mixture experts approach technique improves phoneme classification accuracy separating distributions different speakers reducing overlap 
additional advantage adapting quickly new speaker voice typically requiring words sentences 
researchers studied approach witbrock haffner developed speaker voice code network system learns quickly identify speaker voice lies space possible voices 
svc unit code derived bottleneck encoder network trained reproduce speaker complete set phoneme pronunciation codes unit code likewise derived bottleneck encoder network trained reproduce acoustic patterns associated particular phoneme 
svc code varied considerably speakers proved remarkably stable speaker regardless phonemes available estimation words speech 
svc code provided extra input 
nn hmm hybrids ms tdnn word accuracy digit recognition task improved error error 
konig morgan experimented speaker cluster neural network continuous speech recognizer mlp inputs supplemented small number binary units describing speaker cluster 
inputs representing speaker gender determined accuracy neural network received supervised training performance resource management database improved error error 
alternatively speakers clustered unsupervised fashion applying means clustering acoustic centroids speaker clusters performance improved intermediate level error 
final way improve speaker independent accuracy speaker normalization shown 
approach speaker designated speaker speaker dependent system trained high accuracy voice order recognize speech new speaker say female acoustic frames mapped neural network corresponding frames speaker voice fed speaker dependent system 
huang explored speaker normalization conventional hmm speaker dependent recognition achieving word error speaker simple mlp nonlinear frame normalization 
normalization network trained adaptation sentences new speaker dtw establish correspondence input frames new speaker output frames speaker 
system evaluated speaker dependent portion resource management database impressively speaker normalization reduced cross speaker error rate error error 
error rate reduced codeword dependent neural networks single monolithic network task network considerably simplified 
final error rate comparable error rate speaker independent systems database huang concluded speaker normalization useful situations large amounts training data available speaker want recognize people speech 

word spotting continuous speech recognition normally assumes spoken word correctly recognized 
applications fact vocabulary words called keywords carry significance rest utterance ignored 
example system prompt user question listen words may embedded long response 
applications word spotter listens flags keywords may useful full blown continuous speech recognition system 
researchers 
related research designed word spotting systems incorporate neural networks hmms 
systems basic strategies deploying neural network 
neural network may serve secondary system putative hits identified primary hmm system 
case network architecture simple detected keyword candidate easily normalized fixed duration network input 

neural network may serve primary word spotter 
case network architecture complex automatically warp utterance scans keywords 
david morgan explored strategy primary word spotter dtw hmms 
system detected keyword candidate speech frames converted fixed length representation fourier transform linear compression speech frames network generated compression combination fixed length representation reevaluated appropriately trained neural network rce network probabilistic rce network modularized hierarchy network decide reject candidate false alarm 
system evaluated database 
arcane combination techniques eliminated false alarms generated primary system rejecting true keywords word spotting accuracy declined 
waibel explored second strategy ms tdnn primary word spotter 
system represented keywords unlabeled state models shared phoneme models due coarseness database 
ms tdnn produced score keyword frame derived keyword best dtw score range frames current frame 
system bootstrapped state level training forced linear alignment keyword trained backpropagation word level positive negative training carefully balanced phases 
achieved merit road rally database 
subsequent improvements included adding noise improve generalization subtracting spectral averages normalize different databases duration constraints grouping balancing keywords frequency occurrence extending short keywords nearby context modeling variant suffixes contributed merit official database official switchboard database 
lippmann singer explored strategies 
high performance tied mixture hmm primary word spotter simple mlp secondary tester 
candidate keywords primary system linearly normalized fixed width neural network 
network reduced false alarm rate database 
network apparently suffered poverty training data 
restricted energy network 
rce trademark nestor 
merit summarizes tradeoff detection rate false alarm rate 
computed average detection rate system configurations achieve false alarms keyword hour 

summary attempts augment training set false alarms obtained independent database failed improve system performance databases different easily discriminable 
second strategy explored primary network closely resembling ms tdnn hidden layer radial basis functions sigmoidal units 
enabled new rbf units added dynamically gaussians automatically centered false alarms arose training simplify goal avoiding mistakes 

summary field speech recognition seen tremendous activity years 
hidden markov models dominate field researchers begun explore ways neural networks enhance accuracy hmm systems 
researchers nn hmm hybrids explored techniques frame level training segment level training word level training global optimization issues temporal modeling parameter sharing context dependence speaker independence tasks isolated word recognition continuous speech recognition word spotting 
explorations especially thesis proposed surprising great deal overlap thesis concurrent developments field 
remainder thesis results research area nn hmm hybrids 

related research 
databases performed experiments nn hmm hybrids different databases atr database isolated japanese words cmu conference registration database darpa resource management database 
chapter briefly describe databases 

japanese isolated words experiments performed database isolated japanese words sagisaka provided atr interpreting telephony research laboratory japan collaborating 
database includes recordings words different native japanese speakers professional experiments data male speaker mau 
isolated word recorded booth digitized khz sampling rate 
hamming window fft applied input data produce spectral coefficients msec 
computational resources limited time chose words database extracted subsets limited number phonemes subset words representing unique words due presence comprised phonemes sh plus eighth phoneme silence 
words trained words tested remaining words training samples novel words 
table shows vocabulary 
subset words representing unique words comprised phonemes kk sh ts tt plus th phoneme silence 
words trained words tested words training samples novel words 
testing set allowed test generalization new samples known words unique words allowed test generalization novel words vocabulary independence 

databases 
conference registration experiments continuous speech recognition performed early version cmu conference registration database wood 
database consists english sentences vocabulary words comprising hypothetical dialogs domain conference registration 
typical dialog shown table sides conversation read speaker 
training testing versions database recorded close speaking microphone quiet office multiple speakers speaker dependent experiments 
recordings digitized sampling rate khz hamming window fft computed produce spectral coefficients msec 
words vocabulary database perplexity testing grammar 
recognition difficult conditions created word pair grammar indicating words follow words textual corpus 
unfortunately perplexity word pair grammar soon proved easy hard identify significant improvements word accuracy 

perplexity measure branching factor grammar number words follow word 
aa ku ai oka au su ao oki aoi isu aka aki osu shi asa ko asai ka ki kai sa asu kou sou sousa ii iu ika oi kau kao sakai iki ou ooi table japanese isolated word vocabulary subset samples including unique words 
testing set words consisted starred words novel words bold 

resource management usually evaluated recognition accuracy perplexity testing dialogs sentences reduced vocabulary grammar 
conference registration database developed conjunction janus speech speech translation system cmu waibel 
full discussion janus scope thesis worth mentioning janus designed automatically translate spoken languages english japanese dialog carried american wants register conference tokyo speaks japanese japanese receptionist speaks english 
janus performs speech translation integrating modules speech recognition text translation speech generation single system 
modules available technology fact various combinations connectionist stochastic symbolic approaches compared years 
speech recognition module example originally implemented lpnn described chapter waibel replaced lvq speech recognizer higher accuracy 
janus expanded wide range source destination languages english japanese german spanish korean task broadened simple read speech arbitrary spontaneous speech domain changed conference registration appointment scheduling 

resource management order fairly compare results researchers outside cmu ran experiments darpa speaker independent resource management database price 
standard database consisting training sentences domain naval resource management recorded speakers contributing roughly sentences training set supplemented periodic releases speaker independent testing data years comparative evaluations 
typical sentences listed hello office conference 
right 
register conference 
registration form 

see 
ll send registration form 
give name address 
address forbes avenue pittsburgh pennsylvania 
name david johnson 
see 
ll send registration form immediately 
questions please ask time 

goodbye 
goodbye 
table typical dialog conference registration database 

databases table 
vocabulary consists words easily confusable fourth singular plural possessive forms nouns abundance function words poorly articulated 
testing normally word pair grammar perplexity 
training set sentences normally actual training speakers cross validation 
performed training subdivided database males training cross validation sentences females training cross validation sentences 
cross validation sentences development parallel training sentences 
official evaluations performed reserved set test sentences male female representing union feb oct releases testing data contributed independent speakers 

word class pair grammar sentences database generated expanding templates word classes 
carriers yellow sea training rating nuclear surface ships miles conifer set unit measure metric draw track copeland fuel level fuel capacity readiness ninth june add area largest fuel capacity sea monday sensor location channel ships bass fuel edit alert involving ajax ships went equipment twelve july equipment problem fixed january sherman downgrade mission area redraw low resolution clear data screens lamps channel clear display pigeon location area mission code december didn england arrive manchester yesterday table typical sentences resource management database 

predictive networks neural networks trained compute smooth nonlinear nonparametric functions input space output space 
general types functions prediction classification shown 
predictive network inputs frames speech outputs prediction frame speech multiple predictive networks phone prediction errors compared prediction error considered best match segment speech 
contrast classification network inputs frames speech outputs directly classify speech segment classes 
course research investigated approaches 
predictive networks treated chapter classification networks treated chapter 
prediction versus classification 
classification frames predictions frame separate networks hidden hidden hidden hidden hidden hidden input input frames frames 
predictive networks 
motivation hindsight initially chose explore predictive networks number reasons 
principal reason scientific curiosity colleagues studying classification networks hoped novel approach yield new insights improved results 
technical level argued 
classification networks trained binary output targets produce quasi binary outputs nontrivial integrate speech recognition system binary phoneme level errors tend confound word level hypotheses 
contrast predictive networks provide simple way get nonbinary acoustic scores prediction errors straightforward integration speech recognition system 

temporal correlation adjacent frames speech explicitly modeled predictive approach classification approach 
predictive networks offer dynamical systems approach speech recognition tishby 

predictive networks nonlinear models presumably model dynamic properties speech curvature better linear predictive models 

classification networks yield output class predictive networks yield frame coefficients class representing detailed acoustic model 

predictive approach uses separate independent network phoneme class classification approach uses integrated network 
predictive approach new phoneme classes introduced trained time impacting rest system 
contrast new classes added classification network entire system retrained 
predictive approach offers potential parallelism 
gained experience predictive networks gradually realized arguments flawed way 
fact classification networks trained binary targets imply networks yield binary outputs 
fact years clear classification networks yield estimates posterior probabilities class input integrated hmm effectively prediction distortion measures 

temporal correlation adjacent frames speech st predicted frame modeled just classification network takes adjacent frames speech input 
matter temporal dynamics modeled explicitly predictive network implicitly classifica 
related tion network 

nonlinearity feature neural networks general advantage predictive networks classification networks 

predictive networks yield frame coefficients class quickly reduced single scalar value prediction error just classification network 
furthermore modeling power network enhanced simply adding hidden units 

fact predictive approach uses separate independent network phoneme class implies discrimination classes predictive approach inherently weaker classification approach 
little practical value able add new phoneme classes retraining phoneme classes normally remain stable years time redesigned changes tend global scope 
fact predictive networks potential parallelism irrelevant yield poor word recognition accuracy 
unaware arguments predictive networks experimented approach years concluding predictive networks suboptimal approach speech recognition 
chapter summarizes performed 

related predictive networks closely related special class hmms known autoregressive hmms rabiner 
autoregressive hmm state associated emission probability density function autoregressive function assumed predict frame function preceding frames residual prediction error noise autoregressive function state frames time trainable parameters function prediction error state time assumed independent identically distributed iid random variable probability density function parameters zero mean typically represented gaussian distribution 
shown 
predictive networks says likelihood generating utterance state path approximated cumulative product prediction error probability emission probability transition probability time frames 
shown recognition maximizing joint likelihood equivalent minimizing cumulative prediction error performed simply applying standard dtw local prediction errors autoregressive hmms theoretically attractive performed standard hmms de la reasons remain unclear 
predictive networks expected perform somewhat better autoregressive hmms nonlinear linear prediction 
shown performance predictive networks likewise disappointing 
time began experiments similar experiments performed smaller scale iso watanabe levin 
researchers applied predictive networks simple task digit recognition encouraging results 
iso watanabe word models composed typically states predictors word training samples japanese digit speakers system achieved digit recognition accuracy error testing data 
confirmed nonlinear predictors outperformed linear predictors error dtw multiple templates error 
levin studied variant predictive approach called hidden control neural network states word collapsed single predictor modulated input signal represented state 
applying state word models obtained digit recognition accuracy multi speaker testing 
note levin experiments iso watanabe experiments non shared models focused small vocabulary recognition 
note digit recognition particularly easy task 
iso watanabe improved system backward prediction shared models covariance matrices obtained word accuracy speaker dependent isolated word japanese word recognition task 
gallinari addressed discriminative problems predictive networks discussed chapter 

linked predictive neural networks 
linked predictive neural networks explored predictive networks acoustic models architecture called linked predictive neural networks lpnn designed large vocabulary recognition isolated words continuous speech 
designed large vocabulary recognition shared phoneme models phoneme models represented predictive neural networks linked different contexts name 
section describe basic operation training lpnn followed experiments performed isolated word recognition continuous speech recognition 

basic operation lpnn performs phoneme recognition prediction shown 
network shown triangle takes contiguous frames speech normally passes hidden layer units attempts predict frame speech 
predicted frame compared actual frame 
error small network considered model segment speech 
teach network accurate predictions segments corresponding phoneme instance poor predictions effective phoneme recognizer virtue contrast phoneme models 
lpnn satis basic operation predictive network 
predictor hidden units predicted speech frame prediction errors prediction input speech frames 
predictive networks fies condition means training algorithm obtain collection phoneme recognizers model phoneme 
lpnn nn hmm hybrid means acoustic modeling performed predictive networks temporal modeling performed hmm 
implies lpnn state system predictive network corresponds state autoregressive hmm 
hmm phonemes modeled finer granularity sub phonetic state models 
normally states predictive networks phoneme shown subsequent diagrams 
hmm states predictive networks sequenced hierarchically words sentences constraints dictionary grammar 

training lpnn training lpnn utterance proceeds steps forward pass alignment step backward pass 
steps identify optimal alignment acoustic models speech signal utterance state level steps unnecessary alignment force specialization acoustic models backward pass 
describe training algorithm detail 
step forward pass illustrated 
frame input speech time feed frame frame parallel networks linked utterance example networks utterance aba 
network prediction frame euclidean distance actual frame computed 
scalar errors broadcast sequenced known pronunciation utterance stored column prediction error matrix 
repeated frame entire matrix computed 
second step time alignment step illustrated 
standard dynamic time warping algorithm dtw find optimal alignment speech signal phoneme models identified monotonically advancing diagonal path prediction error matrix path lowest possible cumulative error 
constraint monotonicity ensures proper sequencing networks corresponding progression phonemes utterance 
final step training backward pass illustrated 
step error point alignment path 
words frame propagate error backwards single network best predicted frame alignment path backpropagated error simply difference network prediction actual frame 
series frames may error network shown 
error accumulated networks frame utterance time weights updated 
completes training single utterance 
algorithm repeated utterances training set 

linked predictive neural networks lpnn training algorithm forward pass alignment backward pass 

speech input phoneme phoneme predictors predictors 

alignment path backpropagation prediction errors 
predictive networks seen backpropagating error different segments speech different networks networks learn specialize associated segments speech consequently obtain full repertoire individual phoneme models 
individuation turn improves accuracy alignments self correcting cycle 
iteration training weights random values proven useful force initial alignment average phoneme durations 
subsequent iterations lpnn segments speech basis increasingly accurate alignments 
testing performed applying standard dtw prediction errors unknown utterance 
isolated word recognition involves computing dtw alignment path words vocabulary finding word lowest score desired matches determined just comparing scores 
continuous speech recognition stage dtw algorithm ney find sequence words lowest score desired best sentences determined best search algorithm schwartz chow 

isolated word recognition experiments evaluated lpnn system task isolated word recognition 
performing experiments explored number extensions basic lpnn system 
simple extensions quickly improve system performance adopted standard extensions experiments reported 
standard extension duration constraints 
applied types duration constraints recognition hard constraints candidate word average duration differed sample rejected soft constraints optimal alignment score candidate word penalized discrepancies alignment determined durations constituent phonemes known average duration phonemes 
second standard extension simple heuristic sharpen word boundaries 
convenience include silence phoneme phoneme sets phoneme linked isolated word representing background silence 
word boundaries sharpened artificially penalizing prediction error silence phoneme signal exceeded background noise level 
experiments carried different subsets japanese database isolated words described section 
group contained samples representing unique words limited particular phonemes second contained samples representing unique words limited particular phonemes 
groups divided training testing sets testing sets included training samples enabling test generalization new samples known words novel words enabling test vocabulary independent generalization 
initial experiments word vocabulary network model phonemes 
training iterations recognition performance perfect novel words correct testing set 
fact novel words recognized better new samples familiar words due 
linked predictive neural networks fact short confusable words kau vs kao vs 
way comparison recognition rate training set 
introduced extensions system 
allow limited number alternate models phoneme 
phonemes different characteristics different contexts lpnn phoneme modeling accuracy improved independent networks allocated type context modeled 
alternates analogous context dependent models 
assigning explicit context alternate model system decide alternate context trying alternate linking whichever yields lowest alignment score 
errors backpropagated winning alternate reinforced backpropagated error context competing alternates remain unchanged 
evaluated networks alternate models phoneme 
expected alternates successfully distributed different contexts 
example alternates specialized context initial ki initial internal respectively 
addition alternates consistently improves performance training data result internal representations generalization test set eventually deteriorates amount training data alternate diminishes 
alternates generally best compromise competing factors 
significant improvements obtained expanding set phoneme models explicitly represent consonants japanese distinguishable duration closure versus kk 
allocating new phoneme models represent au improve results presumably due insufficient training data 
table shows recognition performance best word vocabularies respectively 
optimizations 
performance shown range ranks rank means word considered correctly recognized appears best candidates 
vocab size rank testing set training set novel words table lpnn performance isolated word recognition 

predictive networks word vocabulary achieved recognition rate test data exact match criterion recognition top candidates respectively 
word vocabulary best results test data exact match criterion recognition top candidates respectively 
errors word vocabulary training testing sets approximately due duration problems confusing sei due confusing due missing inserted phonemes versus 
systematicity errors leads believe research recognition improved better duration constraints enhancements 

continuous speech recognition experiments evaluated lpnn system task continuous speech recognition 
experiments cmu conference registration database consisting english sentences vocabulary words comprising dialogs domain conference registration described section 
experiments context independent phoneme models including silence topology shown 
topology similar system ney noll phoneme model consists states economically implemented networks covering states self loops certain amount state skipping allowed 
arrangement states transitions provides tight temporal framework stationary temporally structured phones sufficient flexibility highly variable phones 
average duration phoneme frames imposed transition penalties encourage alignment path go straight state model 
transition penalties set values zero moving state remaining state skipping state average frame prediction error 
neural networks evaluated frame speech 
predictors contextual inputs frames 
network hidden units sparse connectivity experiments showed accuracy unaffected computation significantly reduced 
entire lpnn system free parameters 
lpnn phoneme model continuous speech 
net net net 
linked predictive neural networks database phonetically balanced normalized learning rate different networks relative frequency phonemes training set 
training system bootstrapped iteration forced phoneme boundaries trained iterations loose word boundaries located dithering word boundaries obtained automatic labeling procedure sphinx order optimize word boundaries lpnn system 
shows result testing lpnn system typical sentence 
top portion actual spectrogram utterance bottom portion shows frame predictions networks specified point optimal alignment path 
similarity spectrograms indicates hypothesis forms acoustic model unknown utterance fact hypothesis correct case 
speaker dependent experiments performed conditions male speakers various task perplexities 
results summarized table 
actual predicted spectrograms 
speaker speaker perplexity substitutions deletions insertions word accuracy table lpnn performance continuous speech 

predictive networks 
comparison hmms compared performance lpnn simple hmms evaluate benefit predictive networks 
studied hmm single gaussian density function state parameterized different ways mean coefficients variance ignored assumed unity 
mean coefficients variance coefficients 
mean coefficients including deltas variance ignored 
gaussian means variances case derived analytically training data 
table shows results experiments 
seen configuration gave best results lpnn outperformed simple hmms 
increased number mixture densities gaussians parameterized evaluated hmms 
compared results discriminative lvq system developed otto learned vector quantization automatically cluster speech frames set acoustic features subsequently fed set neural network output units compute emission probability hmm states 
results comparison shown table 
see lpnn easily outperformed hmm mixture densities discriminative lvq system outperforms 
attribute inferior performance lpnn primarily lack discrimination issue discussed detail chapter 
system hmm mixture lpnn substitutions deletions insertions word accuracy table performance hmms single gaussian mixture vs lpnn 
perplexity system hmm hmm hmm lvq lpnn table word accuracy hmm mixture densities lvq lpnn 

extensions measured frame distortion rate systems 
lpnn frame distortion corresponds prediction error 
hmm corresponds distance speech vector mean closest gaussian mixture 
lvq system corresponds quantization error distance input vector nearest weight vector hidden node 
table shows lpnn distortion rate systems despite inferior word accuracy 
suggests training criterion explicitly minimizes frame distortion rate inconsistent poorly correlated ultimate goal word recognition accuracy 
discuss issue consistency chapter section 

extensions attempts improve accuracy lpnn system investigated extensions basic system 
section describes architectural extensions presents results experiments 

hidden control neural network common theme speech recognition systems balance number free parameters amount training data available order optimize accuracy test set 
free parameters system may learn perfectly memorize training set generalize poorly new data 
hand free parameters system learn coarse characteristics task attain poor accuracy training set testing set 
striking optimal balance involves sharing parameters extent 
pure hmm system mean sharing codebooks states sharing distributions states phoneme merging triphones generalized triphones merging distributions 
nn hmm hybrid techniques example section described state phoneme model uses networks sharing distributions states 
way share parameters unique neural networks collapse multiple networks single network modulated hidden control input signal distinguishes functionality separate networks 
idea initially proposed system avg 
frame distortion hmm hmm hmm lvq lpnn table lpnn minimal frame distortion despite inferior word accuracy 

predictive networks levin called hidden control neural network 
context speech recognition involves collapsing multiple predictive networks shared network modulated hidden control signal distinguishes states shown 
control signal typically uses simple thermometer representation comprised unit state successive states represented turning successive units ensuring similar representation adjacent states 
addition reducing number parameters amount memory required computationally efficient set separate networks partial results redundant forward passes cached total number forward passes remains unchanged 
performed number experiments collaboration 
set experiments studied effects varying degrees shared structure shown 
experiments state phoneme models state phoneme models 
architecture labeled basic lpnn hidden control networks required represent phonemes states 
hidden control inputs introduced networks required task phoneme modeled single network modulated hidden control input bits distinguish states 
hidden control idea taken limit big network modulated hidden control inputs specify phoneme state 
table shows results experiments evaluated speaker testing continuous speech recognition tested word recognition word boundaries continuous speech allowed compare acoustic discriminability architectures directly 
table shows observed minor differences performance architectures lpnn slightly discriminant hidden control architecture generalized better ran faster 
architecture poorly presumably shared structure free parameters overloading network causing poor sequence predictive networks replaced hidden control neural network 
sequence predictive networks hidden control neural network state state state state 
extensions nation 
conclude hidden control may useful care taken find optimal amount parameter sharing task 
architectures hidden control experiments 
architecture free parameters weights word accuracy words continuous speech continuous speech table results hidden control experiments 
parameter sharing sparingly 
hidden control input hidden control input predictions speech frames state phoneme state lpnn nets nets net 
predictive networks 
context dependent phoneme models accuracy speech recognizer improved context dependent models 
pure hmm system normally involves diphone triphone models phoneme model context adjacent phonemes 
increases specificity accuracy models increases absolute number orders magnitude models necessary cluster form parameter sharing ensure training data model 
nn hmm hybrid predictive networks context dependent phoneme models implemented separate network diphone triphone 
undoubtedly result system free parameters resulting poor generalization excessive amount memory computation 
desired form parameter sharing 
potential solution shared network context part input signal 
approach appealing requires additional parameters require extra inputs triphones require extra inputs small value provides way distinguish different contexts phoneme 
studied idea augmenting network include contextual inputs shown 
contextual inputs represented adjacent phoneme making right context dependent diphone model felt database small provide adequate coverage triphones 
represented possible values adjacent phoneme contextual inputs clustered phonemes linguistic features proposed rumelhart mcclelland chapter contextual inputs necessary 
phoneme coded dimensions 
dimension bits divide phonemes interrupted consonants stops nasals continuous consonants fricatives liquids vowels 
second dimension bits subdivide classes 
third dimension bits classified phonemes place articulation context dependent standard implementation efficient implementation 
speech hci context outputs hidden speech hci context outputs hid hid 
extensions front middle back 
fourth dimension bits divided consonants voiced unvoiced vowels long short 
conceptually single hidden layer predictive network 
reality divided hidden layer halves shown 
allows forward pass computations half network cached frame state forward pass computations contexts reduced series output sigmoids different precomputed net inputs 
saves considerable amount redundant computation 
evaluated context dependent cmu conference registration database 
best results shown table speaker perplexity 
evaluation predictive network inputs included speech inputs frames speech represented coefficients delta coefficients state inputs contextual inputs network included hidden units speech side plus hidden units context side course outputs representing predicted speech frame 
lpnn phonemes alternate models best automatically linked 
contrast lpnn state phoneme model implemented networks context dependent state phoneme model shown state model silence implemented single network phoneme state inputs 
cd achieved better results lpnn vs suggesting hidden control mechanism provides effective way share parameters context dependence improves specificity phoneme models 
error analysis phoneme level revealed phoneme error rate training complete 
confusions involved phonemes ah ae eh uh system lpnn cd substitutions deletions insertions word accuracy table results lpnn context dependent speaker perplexity 
state phoneme model cd experiments 

predictive networks 
function word models lee showed function words short words particularly difficult recognize strong coarticulation effects frequent continuous speech poorly articulated speaker 
inadequate modeling words significantly degrade system word accuracy 
improved accuracy system introducing function word models 
selected words highest error rate system created word models states indexed hidden control inputs 
representing words sequence standard phoneme models words got independent models represented single additional inputs identify state word levin 
function word models context dependent contextual inputs arbitrarily set initial phoneme function word 
note mutual independence predictive networks need retrain phoneme models new function word models trained 
evaluating system conditions previous section system achieved word accuracy represents fewer errors system function word models 

weaknesses predictive networks experience suggests predictive networks effective speech recognition 
conference registration database perplexity obtained word accuracy basic lpnn system enhanced hidden control inputs context dependent modeling function word modeling 
contrast primitive hmm achieves task simple lvq system achieves word accuracy 
concluded predictive networks suffer weaknesses lack discrimination inconsistency training testing criteria 
may possible correct problems research stopped short doing 
discuss problems sections 

lack discrimination predictive networks ordinarily trained independently result discrimination acoustic models 
means explicit mechanism discourage models resembling leads easily confusable phone models turn degrades word recognition accuracy 
weakness shared hmms trained maximum likelihood criterion problem severe predictive networks quasi stationary nature speech causes 
weaknesses predictive networks predictors learn quasi identity mapping rendering phoneme models fairly confusable 
example shows actual spectrogram frame predictions eh model model 
models fairly accurate predictors entire utterance 
ways partially compensate lack discrimination 
example input frames long temporally close relevant predicted frame making network behave identity mapper easily distinguished 
introduce alternate phone models context dependent models function word models number improvements 
techniques may improve performance system address lack discrimination models performance remain suboptimal 
really needed way discriminate models applying positive training correct model applying negative training incorrect models 
immediately clear kind negative target sense predictive network 
hindsight see general types target positive negative training vector analogous predicted frame scalar corresponding transformation predicted frame predicted frames 
research studied approach thought second approach appears second approach may promise 
remainder section describe approaches 
actual spectrogram corresponding predictions eh phoneme models 

predictive networks 
vector targets prediction system uses vectors training targets definition target positive training actual frame time obvious target vector negative training 
attempts perform negative training studied possible strategies successful 
strategy actual frame time target positive negative training perform gradient descent correct network gradient ascent incorrect networks 
naive approach failed force discrimination proportional prediction error network negative training weakest confusable model stronger models pushed away 
unstable dynamic inevitably throws models chaos 
second strategy returned gradient descent positive negative training tried supply target vector negative training distinguish model 
observed network receives positive training small region acoustic space frames corresponding phoneme consequently input frames compute undefined output may overlap outputs predictors 
network learned approximate identity mapping defined region tend approximate identity mapping undefined region 
discourage behavior applied negative training undefined region target differed actual frame time chose negative target vector average positive frames associated network clearly avoids identity mappings example model trained predict average frame input frames belong unfortunately technique failed network learned compute essentially constant output corresponding average frame associated phone 
happened naturally network trained map virtually input constant output positive predictions happened resemble constant output 
tried networks smaller learning rate negative training emphasize positive samples increasing size networks learn detailed mappings efforts successful 
experience concluded discriminative training predictive system best nontrivial probably infeasible vector targets 

scalar targets interesting alternative involves redefining boundaries predictive network associated prediction error euclidean distance predicted frame actual frame computed internally network special post processing layer 
perspective output network scalar equal euclidean distance 
weaknesses predictive networks lies range frame coefficients range 
transformed class membership function inverting normalizing perfect prediction yields poor prediction yields discriminative training simply matter training correct network target incorrect networks target 
course error backpropagated equations post processing layer 
assume squared error criterion network frame prediction layer note backpropagation equation involves types targets class membership layer frame prediction layer 
seen learning rule causes system discriminate models negative correct network positive incorrect networks 
variation approach normalize scalar outputs sum probabilities superscripts indicate network index 
correct network trained target incorrect networks trained target 
time error measure frame prediction layer exp 
predictive networks gallinari normalized scalar targets introduce discrimination predictive system resembled basic lpnn 
system continuous speech recognizer evaluated performance phoneme recognition 
preliminary experiments discriminative training cut error rate 
subsequent test timit database showed phoneme recognition rate comparable state art systems including sphinx ii 
normalized outputs somewhat discriminative non normalized outputs normalized outputs mutually constrained correct increases incorrect ones decrease 
property called implicit discrimination 
contrast explicit discrimination involves contrasting positive negative training training correct model output training incorrect models output 
note modes discrimination operate independently 
explicit discrimination probably impact implicit discrimination involves greater contrast 
may best combine types discrimination gallinari done 
conclude predictive system discriminative transforming vector outputs scalar outputs network trained targets 
empirically investigate approach research 

inconsistency second major problem predictive networks standard training criterion inconsistent testing criterion 
predictive networks trained accurate predictions speech frames testing criterion completely different word recognition accuracy 
hope hope criteria strongly correlated fact find lpnn excellent frame predictions translate poor word recognition accuracy 
evidently weak correlation frame prediction word recognition 
training testing consistent extending architecture support word level training 
involve introducing word level unit word vocabulary connecting prediction error layer associated dtw alignment path backpropagating error word layer target jk jk 
weaknesses predictive networks correct word incorrect words 
discuss technique word level training greater detail chapter context classification networks 
predictive networks suffer major weaknesses lack discrimination inconsistency training testing criteria 
discussed potential remedies problems 
pursuing remedies research chose move study classification networks support discrimination naturally appeared give superior results 
research classification networks described chapter 

predictive networks 
classification networks neural networks taught map input space kind output space 
example previous chapter explored homomorphic mapping input output space networks taught predictions interpolations space 
useful type mapping classification input vectors mapped classes 
neural network represent classes output units corresponding input vector class activation outputs activation 
typical speech recognition mapping speech frames phoneme classes 
classification networks attractive reasons simple intuitive commonly 
naturally discriminative 
modular design easily combined larger systems 
mathematically understood 
probabilistic interpretation easily integrated statistical techniques hmms 
chapter give overview classification networks theory networks describe extensive set experiments optimized classification networks speech recognition 

overview ways design classification network speech recognition 
designs vary primary dimensions network architecture input representation speech models training procedure testing procedure 
dimensions issues consider 
instance network architecture see 
layers network units layer 
time delays network arranged 
kind transfer function layer 
extent weights shared 
weights held fixed values 
output units integrated time 
speech network see 

classification networks types network architectures classification 
speech input class output phonemes phonemes phonemes phonemes phonemes phonemes phonemes words time delay neural network multi state time delay neural network single layer perceptrons multi layer perceptrons copy time time delays word word word 
theory input representation 
type signal processing 
resulting coefficients augmented redundant information deltas 
input coefficients 
inputs normalized 
lda applied enhance input representation 
speech models 
unit speech phonemes triphones 

context dependence implemented 
optimal phoneme topology states transitions 
extent states shared 
diversity pronunciations allowed word 
function words treated differently content words 
training procedure 
level frame phoneme word network trained 
bootstrapping necessary 
error criterion 
best learning rate schedule 
useful heuristics momentum derivative offset 
biases initialized 
training samples randomized training continue samples learned 
weights updated 
granularity discrimination applied 
best way balance positive negative training 
testing procedure 
viterbi algorithm testing values operate 
network output activations directly 
logarithms applied 
priors factored 
training performed word level word level outputs testing 
duration constraints implemented 
language model factored 
questions answered order optimize nn hmm hybrid system speech recognition 
chapter try answer questions theoretical arguments experimental results 

theory 
mlp posterior estimator discovered multilayer perceptron asymptotically trained classifier mean squared error mse similar criterion output activations approximate posterior class probability class input accuracy improves size training set 
important fact proven gish bourlard hampshire pearlmutter ney see appendix details 
theoretical result empirically confirmed 
classifier network trained frames speech softmax outputs cross entropy training output activations examined see particular activation value associated correct class 
network input network kth output activation represents correct class empirically 
classification networks measured equivalently direct function trained network 
graph horizontal axis shows activations vertical axis shows empirical values 
graph contains bins data points 
fact empirical curve nearly follow degree angle indicates network activations close approximation posterior class probabilities 
speech recognition systems dtw applied directly network class output activations scoring hypotheses summing activations best alignment path 
practice suboptimal reasons output activations represent probabilities multiplied added alternatively logarithms may summed 
hmm emission probabilities defined likelihoods posteriors nn hmm hybrid recognition posteriors converted likelihoods bayes rule ignored recognition constant states frame posteriors may simply divided priors 
intuitively argued priors factored reflected language model grammar testing 
network output activations reliable estimates posterior class probabilities 
probability correct activation actual theoretical 
theory bourlard morgan demonstrate word accuracy hybrid improved log output activation viterbi search 
provide chapter 

likelihoods vs posteriors difference likelihoods posteriors illustrated 
suppose classes likelihood describes distribution input class posterior describes probability class input 
words likelihoods independent density models posteriors indicate class distribution compares 
likelihoods posteriors posteriors better suited classifying input bayes decision rule tells classify class iff wanted classify input likelihoods convert posteriors likelihoods bayes rule yielding complex form bayes decision rule says says classify class iff likelihoods model independent densities posteriors model comparative probability 
posterior 
classification networks note priors implicit posteriors likelihoods explicitly introduced decision rule likelihoods 
intuitively likelihoods model surfaces distributions posteriors model boundaries distributions 
example distributions modeled likelihoods bumpy surface ignored posteriors boundary classes clear regardless bumps 
likelihood models states hmm may waste parameters modeling irrelevant details posterior models provided neural network represent critical information economically 

frame level training experiments classification networks performed frame level training 
section describe experiments reporting results obtained different network architectures input representations speech models training procedures testing procedures 
noted experiments section performed resource management database conditions see appendix details network architecture lda plp input coefficients frame frame input window 
hidden units 
context independent timit phoneme outputs state phoneme 
activations softmax phoneme layer outputs 
training training set sentences male sentences mixed gender 
frames random order weights updated frame 
learning rate schedule optimized search see section 
momentum derivative offset 
error criterion cross entropy 
testing cross validation set sentences male sentences mixed 
grammar word pairs perplexity 
pronunciation word dictionary 
minimum duration constraints phonemes state duplication 
viterbi search log prior phoneme 
network architectures series experiments attempt answer question optimal neural network architecture frame level training speech recognizer 

frame level training 
benefit hidden layer optimizing design neural network question consider network hidden layer 
theoretically network hidden layers single layer perceptron slp form linear decision regions guaranteed attain classification accuracy training set linearly separable 
contrast network hidden layers multilayer perceptron mlp form nonlinear decision regions liable get stuck local minimum may inferior global minimum 
commonly assumed mlp better slp speech recognition speech known highly nonlinear domain experience shown problem local minima insignificant artificial tasks 
tested assumption simple experiment directly comparing slp mlp containing hidden layer hidden units networks trained training sentences 
mlp achieved word accuracy slp obtained accuracy 
hidden layer clearly useful speech recognition 
evaluate architectures hidden layer 
shown cybenko function computed mlp multiple hidden layers computed mlp just single hidden layer hidden units 
experience shown training time increases substantially networks multiple hidden layers 
worth noting experiments word level training see section effectively added extra layers network 
hidden layer necessary word accuracy 
word accuracy multi layer perceptron single layer perceptron 
classification networks 
number hidden units number hidden units strong impact performance mlp 
hidden units network complex decision surfaces form better classification accuracy attain 
certain number hidden units network may possess modeling power model idiosyncrasies training data trained long performance testing data 
common wisdom holds optimal number hidden units determined optimizing performance cross validation set 
shows word recognition accuracy function number hidden units training set cross validation set 
performance training set measured training sentences efficiency 
seen word accuracy continues improve training set cross validation set hidden units added hidden units 
indicates variability speech virtually impossible neural network memorize training set 
expect performance continue improve hidden units gradual rate 
aid powerful parallel supercomputer researchers icsi word accuracy continues improve hidden units network architecture similar 
doubling hidden layer doubles computation time remainder experiments usually settled hidden units compromise word accuracy computational requirements 
performance improves number hidden units 
trainable weights word accuracy hidden units hidden units train jan cross validation set training set 
frame level training 
size input window word accuracy system improves context sensitivity acoustic models 
obvious way enhance context sensitivity show acoustic model just speech frame window speech frames current frame plus surrounding context 
option normally available hmm hmm assumes speech frames mutually independent frame relevance current frame hmm rely large number contextdependent models triphone models trained single frames corresponding contexts 
contrast neural network easily look number input frames context independent phoneme models arbitrarily context sensitive 
means trivial increase network word accuracy simply increasing input window size 
tried varying input window size frames speech mlp modeled context independent phonemes 
confirms resulting word accuracy increases steadily size input window 
expect context sensitivity word accuracy networks continue increase input frames marginal context irrelevant central frame classified 

possible get limitation example introducing multiple streams data stream corresponds neighboring frame solutions unnatural rarely 
enlarging input window enhances context sensitivity improves word accuracy 
word accuracy number input frames input windows dec 
classification networks subsequent experiments limited networks input frames order balance diminishing marginal returns increasing computational requirements 
course neural networks context sensitive contextdependent hmms techniques described sec 

pursue techniques research classification networks due lack time 

hierarchy time delays experiments described far time delays located input window hidden layer 
possible configuration time delays mlp 
time delays distributed hierarchically time delay neural network 
hierarchical arrangement time delays allows network form corresponding hierarchy feature detectors feature detectors higher layers waibel allows network develop compact representation speech lang 
tdnn achieved renowned success phoneme recognition assumed hierarchical delays necessary optimal performance 
performed experiment test assumption valid continuous speech recognition 
compared networks shown simple mlp frames input window input coefficients frame hidden units phoneme outputs weights total mlp number input hidden output units time delays hierarchically distributed layers weights mlp hidden units number weights approximately weights 
networks trained sentences tested cross validation sentences 
surprisingly best results achieved network hierarchical delays advantage statistically significant 
note hild personal correspondence performed similar comparison large database spelled letters likewise simple mlp performed network hierarchical delays 
findings contradict conventional wisdom hierarchical delays tdnn contribute optimal performance 
apparent contradiction resolved noting tdnn hierarchical design initially motivated poverty training data lang argued hierarchical structure tdnn leads replication weights hidden layer replicated weights trained shifted subsets input speech window effectively increasing amount training data weight improving generalization testing set 
lang hierarchical delays essential coping tiny database training samples class waibel valuable small database samples class 
contrast experiments hild train 
frame level training ing samples class 
apparently abundance training data longer necessary boost amount training data weight hierarchical delays 
fact argued large database hierarchical delays theoretically degrade system performance due inherent tradeoff degree hierarchy network 
time delays redistributed higher network hidden unit sees context simpler potentially powerful pattern recognizer seen receives training applied adjacent positions tied weights learns simpler patterns reliably 
consequently relatively little training data available hierarchical time delays serve increase amount training data weight improve system accuracy large amount training data available tdnn hierarchical time delays hidden units unnecessarily coarse degrade system accuracy simple mlp theoretically preferable 
observed experiment large database 

temporal integration output activations tdnn distinguished simple mlp hierarchical time delays temporal integration phoneme activations time delays 
lang waibel argued temporal integration tdnn time shift invariant tdnn able classify phonemes correctly poorly segmented tdnn feature detectors finely tuned shorter segments contribute score matter occur phonemic segment 
temporal integration clearly useful phoneme classification wondered useful continuous speech recognition temporal inte hierarchical time delays improve performance abundant training data 
word accuracy weights 
classification networks performed dtw utterance 
experiment compare word accuracy resulting architectures shown 
network standard mlp second network mlp phoneme level activations summed frames normalized yield smoothed phoneme activations 
case trained network data centered frame database difference prior probabilities 
network softmax activations final layer tanh activations preceding layers 
emphasize temporal integration performed twice second system network order smooth phoneme activations dtw order determine score utterance 
simple mlp achieved word accuracy network temporal integration obtained word accuracy 
conclude tdnn style temporal integration phoneme activations counterproductive continuous speech recognition redundant dtw temporally smoothed phoneme activations useful dtw 

shortcut connections argued direct connections input layer output layer bypassing hidden layer simplify decision surfaces network improve performance 
shortcut connections appear promising predictive networks classification networks direct relationship inputs outputs predictive network 
performed simple temporal integration phoneme outputs redundant helpful 
phonemes word accuracy temporal integration phonemes smoothed phonemes 
frame level training experiment test idea classification network 
compared networks shown standard mlp input frames mlp augmented direct connection central input frame current output frame mlp augmented direct connections input frames current output frame 
networks trained sentences tested cross validation sentences 
network achieved best results small margin 
surprising network achieved slightly better performance networks weights result shortcut connections 
conclude intrinsic advantage shortcut connections negligible may attributed merely addition parameters achieved just easily adding hidden units 

transfer functions choice transfer functions convert net input unit activation value significant difference performance network 
linear transfer functions useful multiple layers linear functions collapsed single linear function rarely especially output layer 
contrast nonlinear transfer functions squash input fixed range powerful exclusively 
popular nonlinear transfer functions shown 
shortcut connections insignificant advantage best 
word accuracy weights 
classification networks sigmoid function output range traditionally served default transfer function neural networks 
sigmoid disadvantage gives nonzero mean activation network waste time early training just pushing biases useful range 
widely recognized networks learn efficiently symmetric activations range non output units including input units symmetric sigmoid tanh functions preferred sigmoid function 
softmax function special property constrains activations sum layer applied useful output layer classification network output activations known estimate posterior probabilities class input add 
note constraint networks outputs typically add range output activation range 
considerations chose give network layer transfer function softmax function output layer symmetric tanh function hidden layer normalized input values lie range 
shows learning curve standard set transfer functions solid line compared configurations 
experiments performed early date trained frames sequential order training sentences updating weights sentence fixed geometrically decreasing learning rate schedule 
curves confirm performance better hidden layer uses symmetric function tanh sigmoid function 
popular transfer functions converting unit net input activation softmax sigmoid symmetric sigmoid tanh tanh 
frame level training see learning accelerated output layer uses softmax function unconstrained function tanh statistically significant difference performance long run 

input representations universally agreed speech represented sequence frames resulting type signal analysis applied raw waveform 
universal agreement type signal processing ultimately gives best performance optimal representation vary system system 
popular representations produced various forms signal analysis spectral fft coefficients cepstral cep coefficients linear predictive coding lpc coefficients perceptual linear prediction plp coefficients 
representation champions expect find difference representations felt obliged compare representations environment nn hmm hybrid system 
studied representations msec frame rate case fft spectral coefficients frame 
coefficients produced fast fourier transform represent discrete frequencies distributed linearly low range logarithmically high range roughly corresponding results training different transfer functions hidden output layers 
word accuracy epochs train tr seq lr ce hidden sigmoid output softmax hidden tanh output tanh hidden tanh output softmax 
classification networks ranges sensitivity human ear 
adjacent spectral coefficients mutually correlated imagined simplify pattern recognition task neural network 
viewed time spectral coefficients form spectrogram interpreted visually 
fft spectral coefficients augmented order differences 
addition delta information explicit implicit window fft frames 
wanted see redundancy useful neural network 
lda compression fft significant dimensions means linear discriminant analysis 
resulting coefficients uncorrelated visually uninterpretable dense information content 
wanted see neural networks benefit compressed inputs 
plp perceptual linear prediction coefficients augmented frame power order differences values 
plp coefficients cepstral coefficients autoregressive pole model spectrum specially enhanced emphasize perceptual features hermansky 
coefficients uncorrelated interpreted visually 
coefficients lie range plp coefficients irregular ranges varying way normalized package 

normalization inputs theoretically range input values affect asymptotic performance network network learn compensate scaled inputs inversely scaled weights learn compensate shifted mean adjusting bias hidden units 
known networks learn efficiently inputs normalized way helps network pay equal attention input 
network learns efficiently inputs normalized symmetrical explained section 
early experiment symmetrical inputs achieved word accuracy asymmetrical inputs obtained accuracy 
studied effects normalizing plp coefficients mean standard deviation different values comparing representations plp inputs normalization 
case weights randomly initialized range input representation trained sentences tested cross validation sentences learning rate schedule separately optimized case 
shows learning curves strongly affected standard deviation 
hand learning erratic performance remains poor iterations 
apparently occurs large inputs lead large net inputs hidden layer causing activations saturate derivatives remain small learning takes place slowly 
hand see normalization extremely valuable 
gave slightly better asymptotic fanin 
frame level training results subsequent experiments 
course optimal value twice large initial weights twice small sigmoidal transfer functions hidden layer tanh half steep 
note implies inputs lie range 
saturating normalized inputs degrade performance suggesting extreme values semantically equivalent values 
quantizing input values bits precision degrade performance 
able conserve disk space encoding floating point input coefficient range single byte range loss performance 
normalization may statistics static collected entire training set kept constant testing dynamic collected individual sentences training testing 
compared methods significant difference long consistently 
performance methods inconsistently training testing 
example experiment training static normalization word accuracy testing static normalization testing dynamic normalization 
static dynamic normalization gave equivalent results consistently conclude dynamic normalization preferable possibility training testing utterances recorded different conditions static statistics apply 
normalization plp inputs helpful 
word accuracy epochs plp input normalization may stdev stdev stdev stdev stdev normalization 
classification networks 
comparison input representations order fair comparison input representations normalized symmetric range 
evaluated network representation input window frames case networks trained sentences tested sentences 
resulting learning curves shown 
striking observation fft gets relatively slow start representation network automatically discover temporal dynamics implicit input window temporal dynamics explicitly provided representations delta coefficients 
performance gap shrinks time conclude delta coefficients moderately useful neural networks 
little difference representations plp coefficients may slightly inferior 
note loss performance compressing fft coefficients lda coefficients lda better fft confirming number coefficients matters information content 
conclude lda marginally useful technique reduces dimensionality input space making computations neural network efficient 
input representations normalized deltas lda moderately useful 
word accuracy epochs train test 
aug fft fft deltas plp deltas lda derived fft 
frame level training 
speech models training data performance system improved increasing specificity speech models 
ways increase specificity speech models including augmenting number phones splitting phoneme closure burst treating independently dictionary word pronunciations increasing number states phone state states phone making phones context dependent diphone triphone models modeling variations pronunciations words including multiple pronunciations dictionary 
optimizing degree specificity speech models database timeconsuming process specifically related neural networks 
great effort optimize speech models 
experiments performed context independent timit phoneme models single state phoneme single pronunciation word 
believe context dependent phone models significantly improve results hmms time explore 
study variations speech models described sections 

phoneme topology experiments single state phoneme times states phoneme simple left right transitions 
experiment training sentences cross validation sentences compared topologies state phoneme states phoneme states phoneme minimum encountered duration phoneme training set 
shows best results obtained states phoneme results deteriorated fewer states phoneme 
experiments minimum phoneme duration constraints duration phoneme constrained means state duplication average duration phoneme measured training set fact state model outperformed state model simply due better duration modeling due fact additional states phoneme genuinely useful received adequate training 

classification networks 
multiple pronunciations word possible improve system performance making dictionary flexible allowing multiple pronunciations word 
tried technique small scale 
examining results typical experiment words caused errors words 
surprising words ubiquitous common pronunciations short long vowels dictionary listed pronunciation word 
example word misrecognized dictionary provided short vowel dx ax 
augmented dictionary include long short pronunciations words retested system 
improved word accuracy system fixing errors introducing new errors resulted confusions related new pronunciations 
may possible significantly enhance system performance systematic optimization dictionary pursue issue considering outside scope thesis 

training procedures backpropagation train networks framework explored variations training procedure 
section research training procedures including learning rate schedules momentum data presentation update schedules gender dependent training recursive labeling 
state phoneme model outperforms state phoneme model 
word accuracy epochs state vs state models state phoneme states phoneme states phoneme 
frame level training 
learning rate schedules learning rate schedule critical importance training neural network 
learning rate small network converge slowly learning rate high gradient descent procedure overshoot downward slope enter upward slope network oscillate 
factors affect optimal learning rate schedule network unfortunately understanding factors 
dissimilar networks trained learning rate schedule unfair compare results fixed number iterations learning rate schedule may optimal networks suboptimal 
eventually realized drawn early experiments invalid reason 
decided systematic study effect learning rate schedules network performance 
experiments standard network configuration training sentences cross validating sentences 
began studying constant learning rates 
shows learning curves terms frame accuracy word accuracy resulted constant learning rates range 
see learning rate small word accuracy just iteration training large frame word accuracy remain suboptimal network oscillating 
learning rate gave best results proved better 
conclude learning rate decrease time order avoid disturbing network approaches optimal solution 
constant learning rates unsatisfactory learning rate decrease time 
accuracy epochs frame word accuracy frame acc 
word acc 

classification networks question exactly learning rate shrink time 
studied schedules learning rate starts optimal value shrinks geometrically multiplying constant factor iteration training 
shows learning rates resulted geometric factors ranging 
see factor halving learning rate iteration initially gives best frame word accuracy advantage soon lost learning rate shrinks quickly network escape local minima wanders 
seen factor constant learning rate causes learning rate remain large learning unstable 
best geometric factor intermediate value gives network time escape local minima learning rate effectively shrinks zero geometric learning rate schedule clearly useful may suboptimal 
know network really learned learning rate vanished 
isn possible learning rate shrink example shrinking 
importantly guarantee fixed learning rate schedule optimized set conditions optimal set conditions 
unfortunately guarantee 
began studying learning rate schedules dynamic search 
developed procedure repeatedly searches optimal learning rate geometric learning rates starting lr better may suboptimal 
accuracy epochs accuracy apr epoch epoch epoch epoch epoch frame acc 
word acc 

frame level training iteration algorithm follows 
initial learning rate iteration train iteration measure cross validation results 
start train iteration time half learning rate measure cross validation results 
comparing results infer optimal learning rate iteration larger smaller values accordingly double halve nearest learning rate try 
continue doubling halving learning rate way accuracy gets worse learning rate 
interpolating known points learning rate accuracy quadratic interpolation best data point left right neighbor find successive learning rates try 
best points learning rate gave best result solve parabola goes points kramer rule find highest point parabola learning rate try 
search continues way expected improvement threshold point waste time continue refining learning rate iteration 
learning rates result indistinguishable performance keep smaller preferable iteration 
move iteration setting initial learning rate set optimal learning rate iteration new round search 
note passing important search criterion testing criterion 
early experiment compared results different searches word accuracy frame accuracy 
search word accuracy yielded word accuracy search frame accuracy yielded word accuracy 
discrepancy arose partly improvements frame accuracy small captured threshold learning rate rapidly zero partly due fact search criterion inconsistent poorly correlated testing criterion 
remaining experiments performed word accuracy search criterion 
search procedure tries different learning rates iteration training procedure obviously increases total amount computation factor depends arbitrary threshold 
typically set threshold relative margin computation time typically increased factor 
ax bx ac 
classification networks illustrates search procedure advantage geometric schedule 
search procedure increases computation time performed experiment training sentences 
lower learning curve corresponds fixed geometric schedule factor recall factor optimized full training set 
upper learning curves correspond search procedure 
different types lines correspond different multiplicative factors tried search procedure example solid line corresponds factor learning rate previous iteration dashed line corresponds factor half learning rate previous iteration 
numbers upper lower curves indicate associated learning rate iteration 
things apparent graph search procedure gives significantly better results geometric schedule 
search procedure trusted find schedule nearly optimal situation outperforming virtually fixed schedule adaptive 
initial learning rate optimal earlier experiment optimal anymore experimental conditions changed case number training sentences decreased 
performance sensitive learning rate schedule turn sensitive experimental conditions conclude misleading compare results experiments performed different conditions searching optimal learning rate schedule 
word accuracy epochs lr search wa apr 
frame level training fixed learning rate schedule 
realized hindsight early experiments reported thesis flawed inconclusive reason 
reinforces value dynamically searching optimal learning rate schedule experiment 
optimal learning rate schedule starts decreases rapidly ultimately asymptotes word accuracy asymptotes 
notice worse accuracy results learning rate multiplied constant factor solid lines factor dashed lines compared optimal factor early iterations 
fact optimal learning rate schedule decreases asymptotically suggested type fixed learning rate schedule decays asymptotically function cross validation performance 
hypothesized learning rate schedule form initial learning rate determined search word error rate cross validation set constant power 
note schedule begins asymptotes cross validation performance asymptotes asymptotic value controlled 
performed experiments learning rate schedule approximate optimized schedule sort asymptotic schedule appeared reliable geometric schedule didn pursue far 
performance different types learning rate schedules search reliably optimal 
lr lr lr lr lr word accuracy epochs lr schedules constant asymptotic geometric search 
classification networks directly compares performance learning rate schedules constant geometric search asymptotic training set male sentences cross validation set male sentences 
schedules start optimal initial learning rate 
see constant learning rate schedule yields worst performance value remains high causes network oscillate iteration 
asymptotic schedule begins optimally search schedule learning rate immediately shrinks half performance erratic asymptotic learning rate high due poor choice best performance search geometric schedule 
note gap search geometric schedules wide earlier experiment virtually disappeared carefully initialized geometric schedule optimal learning rate time 
comparing various learning rate schedules discovered search procedure different conditions observed optimal learning rate schedule affected factors number training sentences 
larger training set implies smaller learning rates iteration shown 
primarily optimal learning rate curve decays little weight update larger training sets travel curve iteration shown 
interestingly suggests learning efficient adjusted learning rate sample iteration time explore idea 
learning rate schedules function training set size optimized search 
optimal learning rate epochs lr schedules dec training sentences training sentences training sentences training sentences training sentences training sentences 
frame level training normalization inputs 
greater standard deviation inputs implies larger learning rates compensate fact hidden units saturated derivative sigmoid vanishes learning inhibited 
unfortunately larger learning rates lead network oscillation saw 
transfer functions output layer 
softmax outputs initially larger learning rate tanh outputs apparently softmax safer tanh sense resulting output activations form better estimates posterior probabilities early training tanh outputs guaranteed sum 
number units 
appears input units regardless representation imply smaller learning rates hidden units imply slower decay rate 
appears learning rate schedule network increases size statistical fluctuations smoothed 
optimal learning rate schedule affected factors input representation shortcut connections hierarchy delays speaker population male vs labels training 
remains unclear schedule affected factors weight update frequency different error criteria 
larger training set smaller learning rates epoch 
lr lr lr lr lr lr lr samples epochs small epochs large learning rate optimal learning rate sample 
classification networks 
momentum heuristics momentum useful technique convergence neural network 
pushes weights previously useful direction momentum effective direction change fairly stable update implying weights updated training sample large number training samples 
unfortunately momentum may increase speed convergence learning rate schedule tricky 
tried momentum early experiments training samples frames sequential order sentence randomized order weights updated sentence frames frame 
momentum value optimal domains high seriously degraded performance 
smaller value somewhat helpful iteration difference subsequent iterations 
shortly abandoned momentum wishing complicate system marginally useful technique 
technique increase convergence rate derivative offset 
value typically added derivative sigmoid function multiplicative factors backpropagation learning stall saturated units sigmoid derivative near zero 
performed early experiments sigmoid derivative unnecessary data soon stopped 
networks trained classifiers get stuck suboptimal corner weight space difficult learn binary targets lie saturation regions sigmoid network develops huge dangerous weights 
researchers avoid problem introducing target offset redefining targets active region sigmoid network learn classify data smaller safer weights 
tried target offsets eventually realized undermined ability system estimate posterior probabilities 
example training output units summed closer outputs didn resemble probabilities kind unable take advantage probabilistic framework hmms 
concluded target offsets useful domains class distributions virtually overlap posterior probabilities estimated network outputs virtually binary subjecting network problems saturation 
case speech class distributions overlap considerably target offsets unnecessary harmful 

training schedule training schedule dimensions sequence presentation training samples weight update frequency 
treat issues studied independently 
training samples may linear sequence occur naturally randomized sequence randomization may occur level frames sentences 
frame level training speakers 
totally randomized sequence preferable exposes network greatest diversity data period time network liable forget previously learned region acoustic space 
weights may updated training samples value max number samples training set 
case called online training called batch training 
smaller values imply randomness weights trajectory means lot wasted movement helps network escape local minima larger values imply smoother trajectory longer intervals weight updates 
fastest learning results max especially training large database fairly redundant data speech domain 
experiments cleanly separate dimensions training schedule considered 
worked kinds training schedules frames sentences 
frame training frames random order updated weights frame 
sentence training frames sequential order sentence updated weights sentence 
note cases max frames case uses online training uses batch training 
early experiment sentence training geometric learning rates measured benefit randomizing training sentences 
training sentences representing average sentences speakers ordered serially time speaker randomly sentences 
randomized sequencing reduced error rate iteration word accuracy vs serial case 
conclude important randomize training sentences grouping speaker allows network focus long acoustic characteristics current speaker performance speakers 
experiments frame training significantly better sentence training 
direct comparison approaches training sentences separately optimized learning rate schedules frame training gave fewer errors sentence training iteration vs word accuracy iterations 
experiments required determine improvement due randomized vs serial frame level sequencing due online vs batch updating 
note established online updating really gives faster learning batch updating small value 

gender dependence speech recognition difficult overlapping distributions 
problem exacerbated speaker independent system different voice characteristics phonetic class distributions spread increasing overlap 
recognition accuracy improved overlapping distributions apart form clustering 
simple elegant way speaker 
classification networks independent system cluster data speaker gender 
words train system male data female data subsequently recognize speech unknown speaker classifying speaker gender applying appropriate gender dependent recognizer 
approach particularly appealing males females substantially different voice characteristics significantly worsen overlap easy distinguish 
example konig morgan simple neural network identify gender utterance accuracy 
shows performance networks male network female network mixed gender network 
male network trained male sentences tested male sentences female network trained female sentences tested female sentences mixed network trained mixed sentences tested mixed sentences 
see gender dependent networks outperforms mixed network significant margin 
fact male female networks outperformed mixed network despite relative poverty training data separability male female distributions 
note cross gender testing gave poor results 
example network trained male data achieved word accuracy male data female data 
reason may necessary identify gender speaker separate network may just unknown utterance male female network return result system obtained highest dtw score 
time confirm merit approach 
gender dependent training improves results separating overlapping distributions 
word accuracy epochs gender dependence combined males females 
frame level training 
recursive labeling order train classifier network require phonetic label target class frame database 
labels generated speech recognizer performing viterbi alignment utterance known phonetic pronunciation identifying correspondence frames states 
quality labels provided affect resulting word accuracy high quality labels give better results sloppy labels 
system learns capable producing better better labels point may capable producing better labels ones trained 
happens system may improved training recursive labels original labels 
cycle repeated point final optimality 
networks initially trained phonetic labels generated sri decipher system provided icsi 
note decipher achieved word accuracy context independent phone models labels generated 
labels train network generated second generation labels viterbi alignment training data 
second generation labels train gender dependent network generated third generation labels 
shows performance gender dependent network subsequently trained generations labels identical conditions geometric learning rate schedule male speakers 
see generation labels improved word accuracy somewhat third generation resulted fewer errors generation 
conclude recursive labeling valuable technique enhancing word accuracy 
recursive labeling optimizes targets improves accuracy 
word accuracy epochs recursively trained labels rd generation labels nd generation labels st generation labels 
classification networks 
testing procedures neural network matter trained yield poor results properly testing 
section discuss effectiveness different testing procedures system 

transforming output activations suggest plausible ways output activations classifier network perform continuous speech recognition 
apply dtw directly activations scoring hypotheses summing activations alignment path 
approach prompted visual inspection output activations noting network generally shows high activation correct phoneme frame low activation incorrect phonemes 

apply dtw logarithms activations summing log activations alignment path 
approach motivated fact activations estimates probabilities multiplied added implying logarithms added 

apply dtw log divide activations priors summing log quotients alignment path 
approach motivated fact activations estimates posterior probabilities 
recall hmm emission probabilities defined likelihoods posteriors nn hmm hybrid recognition posteriors converted likelihoods bayes rule ignored recognition constant states frame posteriors may simply divided priors 
successive approaches better justified previous ones expect approach give best results 
confirmed direct comparison gave results dtw value word accuracy table performance improves output activations transformed properly 
log log 
frame level training 
duration constraints standard hmm state transition probabilities ij reestimated training probabilities influence duration state recognition 
unfortunately saw earlier self transition constant probability implies exponentially decaying duration model accurate bell shaped model known duration modeling plays relatively small role recognition accuracy 
nn hmm hybrid chose ignore issue reestimation ij simply assumed uniform probability distribution ij explored types duration constraints hard minimum maximum duration constraints phoneme word level probabilistic duration constraints applied segments frames 
cases durational statistics obtained labeled training data 
minimum phoneme durations taken training data proved helpful instance phoneme labeled essentially zero duration rendering minimum duration constraint useless testing 
assigned minimum duration phoneme equal times phoneme average duration training set obtained best results searching intervals value experiments 

phoneme duration constraints studied effect hard minimum maximum phoneme duration constraints 
ways impose constraints 
enforce constraints dynamically 
dtw keep track current duration state frame implicit backtrace information place special restrictions final state phoneme forcing self transition phoneme met minimum duration requirement forcing exit transition phoneme met maximum duration requirement 

duplicate states impose pattern transitions enforce duration constraints illustrated 
panel shows state duplication enforce minimum duration constraints panel shows enforce minimum maximum duration constraints 
approaches state duplication clearly requires memory advantage gives correct results method 
suboptimality dynamically enforced constraints demonstrated examples 
duration constraints state duplication 
frames 
frames 

classification networks suppose word states minimum duration constraint frames state 
shows modeled dynamically enforced constraints state duplication 
solid line shows legal path frame matrix 
circled point diagonal predecessor better horizontal predecessor dotted path established point impossible recover local decision entire word rejected 
contrast shown state duplication allows diagonal path proceed straight word cumulative score word rejected outright 
state duplication safer strategy implementing minimum duration constraints 
experimentally minimum duration constraints extremely helpful implemented state duplication harmful enforced dynamically 
example baseline experiment training sentences testing sentences obtained word accuracy duration constraints 
imposed minimum duration constraints state duplication word accuracy jumped dynamically enforced minimum duration constraints accuracy degraded apparently words prematurely rejected basis local decisions 
maximum duration constraints likewise safely implemented state duplication shown example 
suppose word states maximum duration constraint frames state 
shows modeled dynamically enforced constraints state duplications 
point local score point local score point choose predecessor establishing path solid line 
local decision combined maximum duration constraint frames prevent path reaching point local score settle point local score cumulative score solid line worse transition middle state delayed frame 
contrast shows state duplication permits entry middle state postponed determine true optimal path dashed line 
minimum phoneme duration constraints 
state duplication gives optimal results 
dynamically enforced constraints state duplication 
minimum frames state 

frame level training phonemes hypotheses tend short long expect maximum phoneme duration constraints difference 
experimentally enforcing maximum duration constraints dynamically effect word accuracy 
due lack time investigate state duplication expect difference 
tried phoneme duration constraints segment duration probabilities hard minimum maximum limits 
approach labeled training data construct histogram durations phoneme frames illustrated typical phonemes 
recognition transitioning final state phoneme added penalty maximum phoneme duration constraints 
state duplication gives optimal results 
histogram durations typical phonemes 
dynamically enforced constraints maximum frames state 
state duplication 
dur log probability duration frames phoneme durations phoneme phoneme ah phoneme 
classification networks dur segment duration phoneme implicit backtrace probability duration phoneme appropriate histogram scaling factor 
experiments showed best scaling factor best word accuracy achieve approach 
concluded hard minimum maximum limits effective probabilistic model reverted approach subsequent experiments 

word duration constraints tried extending hard duration constraints word level state duplication dynamic enforcement limited success 
word level longer guarantee state duplication give optimal results arbitrary decision distribute duplication states phonemes word distribution may suboptimal 
experiments state duplication tried distributing states evenly phonemes evenly distributing leftover states minimum phoneme duration constraints 
gave worse results contraints word accuracy degraded minimum phoneme duration constraints 
degradation probably reflected fact states distributed evenly proportion average phoneme duration statistical fashion time investigate 
studied word duration constraints dynamic enforcement keeping track current duration word frame implicit backtrace final state word requiring final state self loop met word minimum duration requirement 
improved word accuracy combined phoneme duration constraints 
note final result degradation phoneme duration constraints 
extended approach dynamically enforcing minimum durations word corresponding different amounts word context word vs word preceded word vs word preceded words 
motivated observation hypotheses contained strings adjacent words having short durations 
unfortunately additional constraints degraded word accuracy word constraints respectively cumulatively single double triple minimum word duration constraints 
maximum word durations degraded performance example accuracy 
believe word duration constraints dangerous largely insufficient statistics 
duration constraints derived training sentences 
represents average instances phonemes represents average samples words vocabulary dangerously small population 
tried compensate fact relaxing constraints shaving minimum word durations adding maximum word durations seen training set gave significant improvement 
conclude hard word duration constraints reliable useful 
dur 
frame level training 
word transition penalties standard hmm useful balance acoustic modeling language modeling word transition penalties adding constant penalty transition word 
values generally gave best results performed viterbi search 
values better performed viterbi search directly 

generalization conclude discussion frame level training presenting results generalization 
course performance better training set test set 
system trained just sentences system learn memorize patterns score training set fail miserably independent test set testing data different training data 
training sentences network lose ability memorize training data system exposed representative range data performance test set rapidly improve 
training data system form increasingly accurate model distribution acoustic space performance steadily improve training testing sets 
reason adage data data 
generalization improves training data 
training testing gap shrinks 
log word accuracy training sentences train sents dec cross validation set training set training samples frames 
classification networks measured asymptotic word accuracy training set cross validation set system hidden units weights trained sentences results shown 
expected see performance steadily improves training cross validation sets increasing amounts training data 
immediate rise accuracy training set implies training sentences frames network memorize improvements graph arise accurate modeling distribution acoustic space 

word level training experiments described far frame level training outputs targets defined frame frame basis 
seen optimize performance system exploiting fact outputs provide estimates posterior probabilities techniques division priors expanded window sizes optimized learning rate schedules gender dependent training duration constraints 
optimizations leads better performance natural limit associated frame level training 
fundamental problem frame level training training criterion inconsistent testing criterion training criterion phoneme classification accuracy testing criterion word recognition accuracy 
saw section context predictive networks may weak correlation phoneme accuracy word accuracy expect better performance system consistently uses word accuracy training testing criterion 
order perform word level training define neural network classifies word time inputs represent frames speech word outputs represent words vocabulary compare output activations desired targets correct word incorrect words error network 
network accept variable number input frames dynamic network integrate local evidence duration word tdnn shared subword units phonemes order scale state network hmm 

multi state time delay neural network interesting network combines features multi state time delay neural network haffner waibel 
seen ms tdnn extension tdnn phoneme level word level single state multiple states 
tdnn performs temporal integration summing activations single phoneme single state duration phoneme con 
word level training trast ms tdnn performs temporal integration applying dtw sequence states comprising word duration word 
see section obtained better word accuracy word level training frame level training 
describe ms tdnn greater detail presenting motivating details design 
take incremental approach stepping series possible designs showing improves earlier designs resolving subtle inconsistencies leading design experiments 
shows baseline system frame level training simple tdnn phoneme outputs copied dtw matrix continuous speech performed 
noted system suboptimal training criterion inconsistent testing criterion phoneme classification word classification 
address inconsistency argued train network explicitly perform word classification 
shall define word layer unit word vocabulary illustrated particular word cat 
correlate activation word unit associated dtw score establishing connections dtw alignment path word unit 
give phonemes word independently trainable weights enhance word discrimination example discriminate cat mat may useful give special emphasis phoneme weights tied frames phoneme occurs 
word unit ordinary unit connectivity preceding layer determined dynamically net input normalized total duration word 
word unit trained target depending word correct incorrect current segment speech resulting error backpropagated entire network 
word discrimination treated phoneme discrimination 
network resolves original inconsistency suffers secondary weights leading word unit training ignored testing dtw performed entirely dtw layer 
resolve inconsistency pushing weights level shown 
phoneme activations longer directly copied dtw layer modulated weight bias stored dtw units linear word unit constant weights bias 
word level training error backpropagated targets word level biases weights modified dtw level 
note transformed network exactly equivalent previous preserves properties separate learned weights associated phoneme effective bias word 
network flawed minor inconsistency arising sigmoidal word unit 
problem exist isolated word recognition monotonic function sigmoidal correlate highest word activation highest dtw score 
continuous speech recognition concatenates words sequence optimal sum sigmoids may correspond optimal sigmoid sum leading inconsistency word sentence recognition 
linear word units 
classification networks ms tdnn designed resolving inconsistencies 
tdnn dtw 
adding word layer 
pushing weights 
linear word units continuous speech recognition 








speech cat hidden tdnn train test copy dtw speech cat hidden train test cat bias speech cat hidden train test copy cat word speech cat hidden train test cat bias tdnn dtw word 
word level training shown resolve problem practice linear word units perform slightly better sigmoidal word units 
potential inconsistencies remain network 
ms tdnn training algorithm assumes network connectivity fixed fact connectivity word level varies depending dtw alignment path current iteration 
course training asymptotes segmentation stabilizes negligible issue 
serious inconsistency arise discriminative training performed known word boundaries word boundaries fact unknown testing 
inconsistency resolved discriminating words boundaries free alignment training suggested hild 
unfortunately expensive operation proved impractical system 
settled network known word boundaries training word level experiments 
ms tdnn fairly compact design 
note layers tdnn shared words vocabulary word requires non shared weight bias phonemes 
number parameters remains moderate large vocabulary system cope limited training data 
new words added vocabulary retraining simply defining new dtw layer new word incoming weights biases initialized respectively 
constant weights word layer may argued word level training really just way viewing dtw level training conceptually simpler single binary target word word level discrimination straightforward 
large vocabulary discriminating incorrect words expensive discriminate small number close matches typically 

experimental results evaluated ms tdnn conference registration database resource management database 
sets experiments performed different experimental conditions databases different sizes delay years experiments time developed better approach frame level training techniques carried word level experiments 
section summarizing experimental conditions database 
conference registration experiments ms tdnn spectral coefficients time delays hidden units time delays phoneme units phonemes states dtw units word units giving total weights 
network symmetric unit activations inputs linear dtw units word units 
system bootstrapped asymptotic performance frame level training 
word level training performed classification merit cfm error function 
classification networks correct word activation explicitly discriminated best incorrect word activation hampshire waibel 
cfm proved somewhat better mse word level training opposite true frame level training 
negative word level training performed words sufficiently confusable order avoid disrupting network behalf words learned 
resource management experiments ms tdnn lda coefficients time delays hidden units phoneme units dtw units word units giving total weights 
hidden layer tanh activations phoneme layer softmax activations preserving mlp bootstrap conditions dtw units word units linear 
time understood frame level training bootstrapping phase yields phoneme activations estimate posterior probabilities computed net input dtw unit activation corresponding phoneme unit prior probability 
experiments learning rate schedule optimized dynamic search fixed constant value conference registration experiments 
different amounts training necessary sets experiments 
conference registration experiments typically bootstrapped frame level training iterations continued word level training iterations 
resource management experiments hand typically bootstrapped frame level training iterations continued word level training iterations 
rm database required fewer iterations training times training data training techniques improved years experiments 
shows databases word level training gave significantly better word accuracy frame level training 
example conference registration database word accuracy frame level training word level training representing reduction error rate resource management database word accuracy frame level training word level training representing reduction error rate 
improvement partly due increase number weights system determined partly due word level training 
show performed intermediate experiment database trained network word level updated weights phoneme layer bootstrapping keeping dtw weights fixed results word accuracy databases 
adding new weights word level training leads better word accuracy 
bias weight log bias weight 
summary 
summary chapter seen word recognition accuracy achieved neural networks trained speech classifiers 
networks simply thrown problem carefully optimized 
table summarizes important optimizations system ranked relative impact performance 
note values derived particular experimental conditions values change different conditions factors nonlinearly related 
table represents rough ranking value techniques 
word level training better frame level training new weights added 


speech cat hidden cat training level word accuracy fixed weights bootstrap resource management database conference registration database 
classification networks experiments resource management database ordinarily trained sentences tested cross validation set speaker independent sentences 
periodically evaluated performance test sentences representing combined february october official test sets 
results generally somewhat worse official test set example best results cross validation set official test set 
shows performance versions system official test set 
number differences successive versions system including factors went back forth kept experimenting primary factors changed follows 
baseline system incorporating important techniques table 
normalized plp inputs better learning rate schedule 

online weight update softmax outputs 

search optimal learning rate schedule gender dependence lda inputs 

word level training 
techniques contributed official word accuracy word error rate context independent phoneme models parameters 
final version system described detail appendix technique word accuracy word accuracy error reduction hidden units dtw uses input frames state duplication normalized inputs batch online weight updates asymmetric symmetric sigmoids gender dependence constant dynamic learning rates grouped randomized sentences word level training states phoneme recursive labeling fft lda inputs table ranking techniques impact performance 
log 
summary snapshots system performance official evaluation set 
word error system version si csr improvements performance evaluation set 
classification networks 
comparisons chapter compare performance best nn hmm hybrids various systems conference registration database resource management database 
comparisons reveal relative weakness predictive networks relative strength classification networks importance careful optimization approach 

conference registration database table shows comparison systems developed research group conference registration database 
systems phoneme models states phoneme 
systems follows hmm continuous density hidden markov model mixture densities state described section 
lpnn linked predictive neural network section 
hidden control neural network section augmented context dependent inputs function word models 
lvq learned vector quantization section trains codebook quantized vectors tied mixture hmm 
tdnn time delay neural network section temporal integration output layer 
may called mlp section hierarchical delays 
ms tdnn multi state tdnn word classification section 
experiment trained recorded sentences speaker tested word accuracy set subset sentences speaker 
perplexity word pair grammar derived applied sentences perplexity grammar limited vocabulary words conversations sentences testing perplexity grammar full vocabulary tested conversations sentences perplexity grammar tested sentences 
final column gives word accuracy training set comparison 

comparisons table clearly shows lpnn outperformed systems primitive hmm suggesting predictive networks suffer severely lack discrimination 
hand predictive networks achieved respectable results suggesting lpnn may poorly optimized despite put context dependent inputs table largely compensate lack discrimination 
case lpnn performed discriminative approaches lvq tdnn ms tdnn 
discriminative approaches lvq tdnn systems comparable performance 
reinforces extends word level mcdermott significant difference phoneme classification accuracy approaches lvq computationally efficient training tdnn computationally efficient testing 
best performance achieved ms tdnn uses discriminative training phoneme level bootstrapping word level subsequent training 
superiority ms tdnn suggests optimal performance depends discriminative training tight consistency training testing criteria 

resource management database focused discriminative training classification networks moved speaker independent resource management database 
network optimizations discussed chapter developed database applied conference registration database 
perplexity test training set system hmm hmm hmm lpnn lvq tdnn ms tdnn table comparative results conference registration database 

resource management database table compares results various systems resource management database including best systems boldface researchers 
results obtained word pair grammar perplexity 
systems table follows mlp best multilayer perceptron virtually optimizations chapter word level training 
details system appendix ms tdnn system plus word level training 
mlp icsi mlp developed icsi renals similar hidden units fewer optimizations discussed 
ci sphinx context independent version original sphinx system lee hmms 
ci decipher context independent version sri decipher system renals hmms enhanced cross word modeling multiple pronunciations word 
decipher full context dependent version sri decipher system renals 
sphinx ii latest version sphinx hwang huang includes modeling 
systems context independent phoneme models relatively parameters get moderate word accuracy 
systems context dependent phoneme models millions parameters get higher word accuracy systems included table illustrate state art performance requires parameters study 
system type parameters models test set word accuracy mlp nn hmm feb oct ms tdnn nn hmm feb oct mlp icsi nn hmm feb oct ci sphinx hmm mar ci decipher hmm feb oct decipher hmm feb oct sphinx ii hmm feb oct table comparative results resource management database perplexity 

comparisons see table nn hmm hybrid systems entries consistently outperformed pure hmm systems ci sphinx ci decipher comparable number parameters 
supports claim neural networks efficient parameters hmm naturally discriminative model posterior probabilities class input likelihoods input class parameters model simple boundaries distributions complex surfaces distributions 
see systems outperformed icsi mlp despite icsi relative excess parameters optimizations performed systems 
important optimizations systems icsi gender dependent training learning rate schedule optimized search recursive labeling word level training case ms tdnn 
see best performance ms tdnn need discriminative training tight consistency training testing criteria 
ms tdnn achieved word recognition accuracy parameters significantly outperforming context independent hmm systems requiring fewer parameters 

dissertation addressed question neural networks serve useful foundation large vocabulary speaker independent continuous speech recognition system 
succeeded showing neural networks carefully 

neural networks acoustic models speech recognition system requires solutions problems acoustic modeling temporal modeling 
prevailing speech recognition technology hidden markov models offers solutions problems acoustic modeling provided discrete continuous semicontinuous density models temporal modeling provided states connected transitions arranged strict hierarchy phonemes words sentences 
hmm solutions effective suffer number drawbacks 
specifically acoustic models suffer quantization errors poor parametric modeling assumptions standard maximum likelihood training criterion leads poor discrimination acoustic models independence assumption hard exploit multiple input frames order assumption hard model coarticulation duration 
hmms drawbacks sense consider alternative solutions 
neural networks known ability learn complex functions generalize effectively tolerate noise support parallelism offer promising alternative 
today neural networks readily applied static temporally localized pattern recognition tasks clearly understand apply dynamic temporally extended pattern recognition tasks 
speech recognition system currently sense neural networks acoustic modeling temporal modeling 
considerations investigated hybrid nn hmm systems neural networks responsible acoustic modeling hmms responsible temporal modeling 


summary experiments explored different ways neural networks acoustic modeling 
novel technique prediction linked predictive neural networks lpnn phoneme class modeled separate neural network network tried predict frame speech frames speech prediction errors perform viterbi search best state sequence hmm 
approach suffered lack discrimination phoneme classes networks learned perform similar quasi identity mapping quasi stationary frames respective phoneme classes 
second approach classification single neural network tried classify segment speech correct class 
approach proved successful naturally supports discrimination phoneme classes 
framework explored variations network architecture input representation speech model training procedure testing procedure 
experiments reached primary outputs posterior probabilities 
output activations classification network form highly accurate estimates posterior probabilities class input agreement theory 
furthermore posteriors converted likelihoods input class effective viterbi search simply dividing activations class priors class accordance bayes rule intuitively note priors factored posteriors reflected language model lexicon plus grammar testing 
mlp vs tdnn 
simple mlp yields better word accuracy tdnn inputs outputs trained frame classifier large database 
explained terms tradeoff degree hierarchy network time delays vs network 
time delays redistributed higher network hidden unit sees context simpler potentially powerful pattern recognizer receives training applied adjacent positions tied weights learns simpler patterns reliably 
relatively little training data available early experiments phoneme recognition lang waibel hierarchical time delays serve increase amount training data weight improve system accuracy 
hand large amount training data available csr experiments tdnn hierarchical time delays hidden units unnecessarily coarse degrade system accuracy simple mlp preferable 

remaining factor input ignored recognition constant classes frame 

define simple mlp mlp time delays input layer tdnn mlp time delays distributed hierarchically ignoring temporal integration layer classical tdnn 

advantages nn hmm hybrids word level training 
word level training error backpropagated word level unit receives input phoneme layer dtw alignment path yields better results frame level phoneme level training enhances consistency training criterion testing criterion 
word level training increases system word accuracy network contains additional trainable weights additional weights trainable accuracy improves 
adaptive learning rate schedule 
learning rate schedule critically important neural network 
predetermined learning rate schedule give optimal results developed adaptive technique searches optimal schedule trying various learning rates retaining yields best cross validation results iteration training 
search technique yielded learning rate schedules generally decreased iteration gave better results fixed schedule tried approximate schedule trajectory 
input representation 
theory neural networks require careful preprocessing input data automatically learn useful transformations data practice preprocessing helps network learn somewhat effectively 
example delta inputs theoretically unnecessary network looking window input frames helpful anyway save network trouble learning compute temporal dynamics 
similarly network learn efficiently input space orthogonalized technique linear discriminant analysis 
reason comparison various input representations obtained best results window spectral delta spectral coefficients orthogonalized lda 
gender dependence 
speaker independent accuracy improved training separate networks separate clusters speakers mixing results testing automatic identification unknown speaker cluster 
technique helpful separates reduces overlap distributions come different speaker clusters 
particular separate gender dependent networks gives substantial increase accuracy clear difference male female speaker characteristics speaker gender identified neural network accuracy 

advantages nn hmm hybrids nn hmm hybrids offer theoretical advantages standard hmm speech recognizers 
specifically 
modeling accuracy 
discrete density hmms suffer quantization errors input space continuous semi continuous density hmms suffer model mismatch poor match priori choice statistical model mixture gaussians true density acoustic space 
contrast neural networks nonparametric models suffer quantization error detailed assumptions form distribution modeled 
neural network form accurate acoustic models hmm 
context sensitivity 
hmms assume speech frames independent examine frame time 
order take advantage contextual information neighboring frames hmms artificially absorb frames current frame introducing multiple streams data order exploit delta coefficients lda transform streams single stream 
contrast neural networks naturally accommodate size input window number weights required network simply grows linearly number inputs 
neural network naturally context sensitive hmm 
discrimination 
standard hmm training criterion maximum likelihood explicitly discriminate acoustic models models optimized essentially discriminative task word recognition 
possible improve discrimination hmm maximum mutual information criterion complex difficult implement properly 
contrast discrimination natural property neural networks trained perform classification 
neural network discriminate naturally hmm 
economy 
hmm uses parameters model surface density function acoustic space terms likelihoods input class 
contrast neural network uses parameters model boundaries acoustic classes terms posteriors class input 
surfaces boundaries classifying speech boundaries require fewer parameters better limited training data 
example achieved accuracy parameters sphinx obtained accuracy parameters lee sri decipher obtained accuracy parameters renals 
neural network economical hmm 
hmms known handicapped order assumption assumption probabilities depend solely current state independent previous history limits hmm ability model effects model durations accurately 
unfortunately nn hmm hybrids share handicap order assumption property hmm temporal model nn acoustic model 
believe research connectionism eventually lead new powerful techniques temporal pattern recognition neural networks 
happens may possible design systems entirely neural networks potentially advancing state art speech recognition 
appendix final system design best results context independent phoneme models word accuracy speaker independent resource management database obtained nn hmm hybrid design network architecture inputs lda coefficients frame derived spectral plus delta spectral coefficients 
frame window delays inputs scaled 
hidden layer hidden units 
unit receives input input units 
unit activation tanh net input 
phoneme layer phoneme units 
unit receives input hidden units 
unit activation softmax net input 
dtw layer units corresponding pronunciations words 
unit receives input phoneme unit 
unit activation linear equal net input 
word layer units word 
unit receives input dtw units alignment path 
unit activation linear equal dtw path score duration 
weights weights dtw layer trainable 
initial weights randomized range sqrt fanin 
biases initialized weights 
phoneme model timit phonemes 
state phoneme 
appendix final system design training database resource management 
training set sentences male sentences female 
cross validation set sentences male sentences female 
labels generated viterbi alignment trained nn hmm 
learning rate schedule search cross validation results 
momentum derivative offset 
bootstrap phase frame level training iterations 
frames random order random selection replacement training set 
weights updated frame 
phoneme targets 
error criterion cross entropy 
final phase word level training iterations 
sentences random order 
frames normal order sentence 
weights updated sentence 
word targets 
error criterion classification merit 
error backpropagated correct output 
testing test set sentences feb oct test sets 
grammar word pairs perplexity 
pronunciation word dictionary 
viterbi search log network output activation phoneme prior phoneme duration constraints minimum average duration phoneme 
implemented state duplication 
maximum 
word transition penalty additive penalty 
results word accuracy 
appendix proof classifier networks estimate posterior probabilities discovered multilayer perceptron asymptotically trained classifier mean squared error mse criterion output activations approximate posterior class probability class input accuracy improves size training set 
important fact proven gish bourlard hampshire pearlmutter richard lippmann ney 
proof due ney 
proof 
assume classifier network trained vast population training samples distribution input correct class 
note input different training samples may belong different classes classes may overlap 
network computes function activation kth output unit 
output targets kc training squared error criterion minimizes error proportion density training sample space splitting cases obtain kc kc xk xk kc appendix proof classifier networks estimate posterior probabilities algebraic expansion show equivalent minimized output activation equals posterior class probability 
hampshire pearlmutter generalized proof showing holds network trained standard error criteria target vectors mean squared error cross entropy mcclelland error xk xk bibliography ackley hinton sejnowski 

learning algorithm boltzmann machines 
cognitive science 
reprinted anderson rosenfeld 
anderson rosenfeld 

neurocomputing foundations research 
cambridge mit press 
austin makhoul schwartz 

speech recognition segmental neural nets 
proc 
ieee international conference acoustics speech signal processing 
bahl cohen cole jelinek lewis mercer 

speech recognition natural text read isolated words 
proc 
ieee international conference acoustics speech signal processing 
bahl brown de souza mercer 

speech recognition continuous parameter hidden markov models 
proc 
ieee international conference acoustics speech signal processing 
barnard 

optimization training neural networks 
ieee trans 
neural networks march 
barto anandan 

pattern recognizing stochastic learning automata 
ieee transactions systems man cybernetics 
nahamoo 

tied mixture continuous parameter models large vocabulary isolated speech recognition 
proc 
ieee international conference acoustics speech signal processing 
bengio 

global optimization neural network hidden markov model hybrid 
ieee trans 
neural networks march 


connectionist architectural learning high performance character speech recognition 
proc 
ieee international conference acoustics speech signal processing 


automatic structuring neural networks spatiotemporal real world applications 
phd thesis university karlsruhe germany 
bibliography bourlard 

links markov models multilayer perceptrons 
ieee trans 
pattern analysis machine intelligence december 
originally appeared technical report manuscript philips research laboratory brussels belgium 
bourlard morgan 

continuous speech recognition system embedding mlp hmm 
advances neural information processing systems touretzky 
ed morgan kaufmann publishers 
bourlard morgan wooters renals 

context dependent neural network continuous speech recognition 
proc 
ieee international conference acoustics speech signal processing 
bourlard morgan 

connectionist speech recognition hybrid approach 
kluwer academic publishers 
bregler hild waibel 

improving connected letter recognition lipreading 
proc 
ieee international conference acoustics speech signal processing 
bridle 

alpha nets recurrent neural network architecture hidden markov model interpretation 
speech communication 
brown 

acoustic modeling problem automatic speech recognition 
phd thesis carnegie mellon university 
burr 

experiments neural net recognition spoken written text 
ieee trans 
acoustics speech signal processing 
burton shore buck 

isolated word speech recognition vector quantization codebooks 
ieee trans 
acoustics speech signal processing 


new concept central nervous system 
hochberg eds neurological classics modern translation 
new york hafner 
carpenter grossberg 

art adaptive pattern recognition self organizing neural network 
computer march 
cybenko 

approximation superpositions sigmoid function 
mathematics control signals systems vol 
pp 

de la levinson sondhi 

incorporating time correlation successive observations acoustic phonetic hidden markov model continuous speech recognition 
proc 
ieee international conference acoustics speech signal processing 
doddington 

phonetically sensitive discriminants improved speech recognition 
proc 
ieee international conference acoustics speech signal processing 
bibliography duda hart 

pattern classification scene analysis 
new york wiley 
elman zipser 

learning hidden structure speech 
ics report institute cognitive science university california san diego la jolla ca 
elman 

finding structure time 
cognitive science 
fahlman 

empirical study learning speed back propagation networks 
technical report cmu cs carnegie mellon university 
fahlman lebiere 

cascade correlation learning architecture 
advances neural information processing systems touretzky 
ed morgan kaufmann publishers los altos ca pp 

fodor pylyshyn 

connectionism cognitive architecture critical analysis 
pinker mehler eds connections symbols mit press 
witbrock lee 

speaker independent recognition connected utterances recurrent non recurrent neural networks 
proc 
international joint conference neural networks 
lee waibel 

connectionist viterbi training new hybrid method continuous speech recognition 
proc 
ieee international conference acoustics speech signal processing 
furui 

robust speech recognition adverse conditions 
proc 
esca workshop speech processing adverse conditions pp 
cannes france 
gish 

probabilistic approach understanding training neural network classifiers 
proc 
ieee international conference acoustics speech signal processing 
gold 

neural network isolated word recognition 
proc 
ieee international conference acoustics speech signal processing 
haffner waibel 

integrating time alignment connectionist networks high performance continuous speech recognition 
proc 
ieee international conference acoustics speech signal processing 
haffner waibel 

multi state time delay neural networks continuous speech recognition 
advances neural information processing systems moody hanson lippmann 
eds morgan kaufmann publishers 
hampshire waibel 

meta pi network connectionist rapid adaptation high performance multi speaker phoneme recognition 
proc 
ieee international conference acoustics speech signal processing 
bibliography hampshire waibel 

novel objective function improved phoneme recognition time delay neural networks 
ieee trans 
neural networks june 
hampshire pearlmutter 

equivalence proofs multi layer perceptron classifiers bayesian discriminant function 
proc 
connectionist models summer school morgan kaufmann publishers 
hassibi stork 

second order derivative network pruning optimal brain surgeon 
advances neural information processing systems hanson cowan giles 
eds morgan kaufmann publishers 
hebb 

organization behavior 
new york wiley 
partially reprinted anderson rosenfeld 
hermansky 

perceptual linear predictive plp analysis speech 
journal acoustical society america 
hertz krogh palmer 

theory neural computation 
addison wesley 
hild waibel 

connected letter recognition multi state time delay neural network 
advances neural information processing systems hanson cowan giles 
eds kaufmann publishers 
hinton 

connectionist learning procedures 
artificial intelligence 
hofstadter 

godel escher bach eternal golden braid 
basic books 
hopfield 

neural networks physical systems emergent collective computational abilities 
proc 
national academy sciences usa april 
reprinted anderson rosenfeld 
huang lippmann 

neural net traditional classifiers 
neural information processing systems anderson 
ed 
new york american institute physics 
huang 

phoneme classification semicontinuous hidden markov models 
ieee trans 
signal processing may 
huang 

speaker normalization speech recognition 
proc 
ieee international conference acoustics speech signal processing 
hwang huang 

shared distribution hidden markov models speech recognition 
ieee trans 
speech audio processing vol pp 
hwang huang 

predicting unseen triphones 
proc 
ieee international conference acoustics speech signal processing 
bibliography auger sales 

comparative study neural networks non parametric statistical methods line handwritten character recognition 
proc 
international conference artificial neural networks 
iso watanabe 

speaker independent word recognition neural prediction model 
proc 
ieee international conference acoustics speech signal processing 
iso watanabe 

large vocabulary speech recognition neural prediction model 
proc 
ieee international conference acoustics speech signal processing 
itakura 

minimum prediction residual principle applied speech recognition 
ieee trans 
acoustics speech signal processing february 
reprinted waibel lee 
jacobs jordan nowlan hinton 

adaptive mixtures local experts 
neural computation 
jain waibel touretzky 

parsec structured connectionist parsing system spoken language 
proc 
ieee international conference acoustics speech signal processing 
jolliffe 

principle component analysis 
new york springer verlag 
jordan 

serial order parallel distributed processing approach 
ics technical report ucsd 


experiments isolated word recognition single multi layer perceptrons 
abstracts st annual inns meeting boston 
kimura 

word recognition acoustic segment networks 
proc 
ieee international conference acoustics speech signal processing 
kohonen 

self organization associative memory rd edition 
berlin springer verlag 
konig morgan 

supervised unsupervised clustering speaker space continuous speech recognition 
proc 
ieee international conference acoustics speech signal processing 
krishnaiah kanal eds 

classification pattern recognition reduction dimensionality 
handbook statistics vol 

amsterdam north holland 
krogh hertz 

simple weight decay improve generalization 
advances neural information processing systems moody hanson lippmann 
eds morgan kaufmann publishers 
kubala schwartz 

new paradigm speaker independent training 
proc 
ieee international conference acoustics speech signal processing 
bibliography lang 

time delay neural network architecture speech recognition 
phd thesis carnegie mellon university 
lang waibel hinton 

time delay neural network architecture isolated word recognition 
neural networks 
le cun boser denker henderson howard hubbard jacket baird 

handwritten zip code recognition multilayer networks 
proc 
th international conference pattern recognition june 
lecun denker solla 

optimal brain damage 
advances neural information processing systems touretzky 
ed morgan kaufmann publishers 
lee 

large vocabulary speaker independent continuous speech recognition sphinx system 
phd thesis carnegie mellon university 
levin 

word recognition hidden control neural architecture 
proc 
ieee international conference acoustics speech signal processing 
linsker 

basic network principles neural architecture 
proc 
national academy sciences usa 
lippmann gold 

neural classifiers useful speech recognition 
st international conference neural networks ieee 
lippmann 

review neural networks speech recognition 
neural computation spring 
reprinted waibel lee 
lippmann singer 

hybrid neural network hmm approaches 
proc 
ieee international conference acoustics speech signal processing 
mcculloch pitts 

logical calculus ideas nervous activity 
bulletin mathematical biophysics 
reprinted anderson rosenfeld 
mcdermott 

lvq shift tolerant phoneme recognition 
ieee trans 
signal processing june 
gallinari 

discriminative neural prediction system speech recognition 
proc 
ieee international conference acoustics speech signal processing 
minsky 

computation finite infinite machines 
englewood cliffs prentice hall 
minsky papert 

perceptrons 
cambridge mit press 
partially reprinted anderson rosenfeld 
bibliography shikano 

integrated training spotting japanese phonemes large phonemic time delay neural networks 
proc 
ieee international conference acoustics speech signal processing 
moody darken 

fast learning networks locally tuned processing units 
neural computation 
morgan 

multiple neural network topologies applied keyword spotting 
proc 
ieee international conference acoustics speech signal processing 
morgan bourlard 

continuous speech recognition multilayer perceptrons hidden markov models 
proc 
ieee international conference acoustics speech signal processing 
munro 

dual back propagation scheme scalar reward learning 
ninth annual conference cognitive science society seattle 
hillsdale erlbaum 
ney 

stage dynamic programming algorithm connected word recognition 
ieee trans 
acoustics speech signal processing april 
reprinted waibel lee 
ney noll 

phoneme modeling continuous mixture densities 
proc 
ieee international conference acoustics speech signal processing 
ney 

speech recognition neural network framework discriminative training gaussian models mixture densities radial basis functions 
proc 
ieee international conference acoustics speech signal processing 
mcnair saito waibel 

testing generality janus multi lingual speech translation system 
proc 
ieee international conference acoustics speech signal processing 
peeling moore 

experiments isolated digit recognition multi layer perceptron 
technical report royal speech radar establishment worcester great britain 
waibel 

integrated phoneme function word architecture hidden control neural networks continuous speech recognition 
proc 
european conference speech communication technology 


context dependent hidden control neural network architecture continuous speech recognition 
proc 
ieee international conference acoustics speech signal processing 
pinker prince 

language connectionism 
pinker mehler eds connections symbols mit press 
bibliography pomerleau 

neural network perception mobile robot guidance 
kluwer academic publishing 
prager harrison fallside 

boltzmann machines speech recognition 
computer speech language 
price fisher bernstein pallett 

darpa word resource management database continuous speech recognition 
proc 
ieee international conference acoustics speech signal processing 
rabiner 

tutorial hidden markov models selected applications speech recognition 
proceedings ieee february 
reprinted waibel lee 
rabiner juang 

fundamentals speech recognition 
prentice hall 
reddy 

speech recognition machine review 
proceedings ieee april 
reprinted waibel lee 
renals morgan cohen franco 

connectionist probability estimation decipher speech recognition system 
proc 
ieee international conference acoustics speech signal processing 
richard lippmann 

neural network classifiers estimate bayesian posteriori probabilities 
neural computation winter 
robinson fallside 

static dynamic error propagation networks application speech coding 
neural information processing systems anderson 
ed 
new york american institute physics 
rosenblatt 

principles neurodynamics 
new york spartan 
rumelhart mcclelland pdp research group 

parallel distributed processing explorations microstructure cognition 
mit press 
sagisaka takeda kuwabara 

japanese speech database fine acoustic phonetic distinctions 
technical report atr interpreting telephony research laboratory 
sakoe chiba 

dynamic programming algorithm optimization spoken word recognition 
ieee trans 
acoustics speech signal processing february 
reprinted waibel lee 
sakoe yoshida iso 

speaker independent word recognition dynamic programming neural networks 
proc 
ieee international conference acoustics speech signal processing 
sanger 

optimal unsupervised learning single layer linear feedforward neural network 
neural networks 
bibliography 

lvq model speaker adaptive speech recognition 
proc 
ieee international conference acoustics speech signal processing 
schwartz chow 

best algorithm efficient exact procedure finding sentence hypothesis 
proc 
ieee international conference acoustics speech signal processing 
sejnowski rosenberg 

parallel networks learn pronounce english text 
complex systems 
servan schreiber cleeremans mcclelland 

graded state machines representation temporal contingencies simple recurrent networks 
machine learning 
smolensky 

tensor product variable binding representation symbolic structures connectionist systems 
artificial intelligence 
sutton 

temporal credit assignment reinforcement learning 
phd thesis university massachusetts amherst 
tank hopfield 

neural computation concentrating information time 
proc 
national academy sciences usa pp 
april 
waibel 

large vocabulary recognition linked predictive neural networks 
proc 
ieee international conference acoustics speech signal processing 
waibel 

continuous speech recognition linked predictive neural networks 
proc 
ieee international conference acoustics speech signal processing 


performance consistency connectionist large vocabulary continuous speech recognition 
proc 
ieee international conference acoustics speech signal processing 
tesauro 

wins computer 
neural computation 
tishby 

dynamical systems approach speech processing 
proc 
ieee international conference acoustics speech signal processing 
touretzky hinton 

distributed connectionist production system 
cognitive science 
hopfield tank 

learning time delayed connections speech recognition circuit 
neural networks computing conference snowbird utah 


element wise recognition continuous speech composed words specified dictionary 
march april 
bibliography waibel hinton shikano lang 

phoneme recognition time delay neural networks 
ieee trans 
acoustics speech signal processing march 
originally appeared technical report tr atr interpreting telephony research laboratories japan 
reprinted waibel lee 
waibel shikano 

modularity scaling large phonemic neural networks 
ieee trans 
acoustics speech signal processing 
waibel lee eds 

readings speech recognition 
morgan kaufmann publishers 
waibel jain mcnair saito hauptmann 

janus speech speech translation system connectionist symbolic processing strategies 
proc 
ieee international conference acoustics speech signal processing 
watrous 

speech recognition connectionist networks 
phd thesis university pennsylvania 


explicit time correlation hidden markov models speech recognition 
proc 
ieee international conference acoustics speech signal processing 
witbrock haffner 

rapid connectionist speaker adaptation 
proc 
ieee international conference acoustics speech signal processing 
wood 

conference registration task neural net speech recognition speech collection labeling speaker independent system 
technical reports cmu cs cmu cs carnegie mellon university 
woodland odell valtchev young 

large vocabulary continuous speech recognition htk 
proc 
ieee international conference acoustics speech signal processing 
aoki waibel bu kemp lavie mcnair polzin rose schultz suhm tomita waibel 

janus spontaneous speech translation 
proc 
ieee international conference acoustics speech signal processing 
waibel 

hybrid neural network dynamic programming word spotter 
proc 
ieee international conference acoustics speech signal processing 
waibel 

improving ms tdnn word spotting 
proc 
ieee international conference acoustics speech signal processing cohen franco morgan rumelhart 

contextdependent multiple distribution phonetic modeling mlps 
advances neural information processing systems hanson cowan giles 
eds morgan kaufmann publishers 
author index ackley anandan anderson aoki waibel auger austin bahl baird barnard barto bengio bernstein boser bourlard bregler bridle brown bu buck burr burton carbonell carpenter chiba chow cleeremans cohen cohen cole cybenko darken de la de souza denker doddington duda elman fahlman fallside fisher fodor franco furui gallinari gish gold grossberg author index haffner hampshire harrison hart hassibi hauptmann hebb henderson hermansky hertz hild hinton hofstadter vi hopfield howard huang huang hubbard hwang iso itakura jacket jacobs jain jelinek jolliffe jordan juang kanal kemp kimura kohonen konig krishnaiah krogh kubala kuwabara lang lavie le cun lebiere lee levin levinson lewis linsker lippmann makhoul mcclelland mcculloch mcdermott mcnair mercer minsky moody moore morgan morgan munro nahamoo newell vi ney noll nowlan author index odell pallett palmer papert pearlmutter peeling pinker pitts polzin pomerleau prager price prince pylyshyn rabiner reddy renals richard robinson rose rosenberg rosenblatt rosenfeld rumelhart sagisaka saito sakoe sales sanger schultz schwartz sejnowski servan schreiber shikano shore singer smolensky solla sondhi stork suhm sutton takeda tank tesauro tishby tomita touretzky valtchev waibel watanabe watrous wilensky witbrock wood woodland wooters yoshida young zipser subject index accents accuracy acoustic modeling frame vs word phoneme word prediction posterior probability estimation acoustic analysis modeling modeling accuracy variability activation functions values adverse conditions algorithms 
see dtw stage dtw hmm forward forward backward baum welch viterbi dynamic programming best search backpropagation quickprop cascade correlation simulated annealing neural networks taxonomy lpnn learning rate search alignment path alphabet recognition 
see letter recognition alternate phoneme models alvinn applications neural network speech recognition speech speech translation tdnn architectures 
see neural networks 
art art articulation associative reward penalty algorithm asynchronous update atr attractors automatic structure optimization autoregressive hmm axons backgammon backpropagation time backtracing batch training baum welch algorithm bayes classification decision rule rule bbn bias unit bias initialization biasing network binary outputs bits precision boltzmann machine bootstrapping bottleneck boundaries class word brain 
see neurobiology subject index cascade correlation algorithm cepstral coefficients chain rule clamping classification networks overview vs predictive networks static vs dynamic previous theory frame level training segment level training word level training summary classification statistical classification merit clustering coarticulation codebook codeword cognition competitive learning complete connectivity compositional structure compression 
see dimensionality reduction computation network processing requirements universal conference registration database conjugate gradient connectionism 
see neural networks connections 
see weights topology consistency 
consonant classification constraints architectural hierarchical integration sequential task language constructive networks content addressable memory context dependent models independent models sensitivity units layer contextual inputs continuous density hmm continuous outputs continuous speech recognition convergence correlation frames critic cross entropy cross validation databases dead units decaying memory decipher decision regions 
see boundaries class 
decision trees delta inputs delta rule dendrites density models derivative offset dictionary differentiability digit recognition dimensionality reduction discontinuous speech recognition discrete density hmm discrimination hmms predictive networks classifier networks frame level word level sentence level basic tasks implicit vs explicit discriminant analysis subject index distortion division priors dtw duration modeling hmms phoneme state word probabilistic dynamic classification network topologies programming programming neural network dynamics speech network activations economy representation efficiency elman network emission probabilities encoder network energy function epochs error analysis error functions set estimation maximization em word recognition exponential decay factorization probabilities feature detection feature mapping feedback feedforward networks fft coefficients merit order assumption fisher linear discriminant formants forward algorithm forward propagation forward backward algorithm frame level training frame scores function approximation function word models gaussian densities 
see mixture densities gender gender dependence generalization generalized delta rule 
see backpropagation generalized triphones global minimum global optimization glue gradient ascent gradient descent grammar granularity handwriting recognition hebb rule heuristics faster learning hidden control 
see 
hidden markov models 
see hmm hidden units role number layers hierarchical structure hmm neural network time delays hmm basic concepts algorithms variations weaknesses vs nn hmm hopfield network hybrid network architectures hybrid systems 
see nn hmm hyperplanes hyperspheres subject index icsi identity mapping independence assumption inputs number frames representation number coefficients contextual interference memories isolated word recognition iterations training janus japanese isolated words joint optimization jordan network means clustering nearest neighbor estimation nearest neighbor rule knowledge recognition kramer rule labels language model lateral inhibition layers lda learning learning rate constant geometric asymptotic search factors affect normalization letter recognition likelihoods posteriors vs posteriors see maximum likelihood linear prediction separability units lipreading local connectivity local minima maxima logarithms long term memory loose word boundaries lpc coefficients lpnn basic operation training testing procedures experiments extensions weaknesses vs hmm lvq maximum posteriori maximum likelihood maximum mutual information mcclelland error mean squared error mel scale coefficients memory implementation requirements memorization meta pi network microphones mixture densities mixture expert networks mlp 
posterior estimator vs single layer perceptron vs tdnn models 
see acoustic temporal duration density speech phoneme word statistical parametric nonparametric neural predictive classification pronunciation accuracy subject index model mismatch modeling assumptions modularity momentum 
see phoneme models ms tdnn multi layer perceptrons 
see mlp multimodal system multi speaker recognition natural language processing best search negative training neighborhood function net input nettalk neural networks fundamentals properties taxonomy architectures backpropagation relation statistics history acoustic models speech review see predictive classifier nn hmm neurobiology nn hmm hybrids survey advantages vs hmm noise nondeterminism nonlinearity nonlocal activation functions nonparametric models normalization inputs outputs 
see softmax weights offsets derivative target stage dtw online training optimal brain damage optimization hmm criterion nn criterion nn techniques oscillation output activations 
see activations output units number normalization 
see softmax transformation winner take phoneme level word level overlapping distributions parallelism parameters number role sharing parametric models parzen windows pattern association completion recognition pca perception perceptrons single layer multiple layer 
see mlp learning rule perplexity philips phoneme durations models recognition pitch subject index recognition plp coefficients posterior probabilities vs likelihoods mlp estimation usefulness power coefficient prediction non network predictive networks motivation hindsight lpnn extensions 
weaknesses accuracy vs hmm vs classifier networks related preprocessing principal components prior probabilities probabilities hmm networks see emission transition reestimation posteriors priors densities likelihoods mlp accuracy production system connectionist pronunciation pruning quantization see vector quantization lvq bits 
quantization error quickprop radial basis functions rbf random connectivity initialization training sequence rate speech raw speech read speech recurrent networks types experiments vs feedforward networks recursive labeling redundancy reinforcement learning resource management database robustness sampling rate sanger rule saturation scaling problems search 
see learning rates viterbi second derivative information segment level training segmentation self organizing networks semantic constraints semi continuous hmm semi supervised learning sentence level training sentence recognition sequences sharing 
see parameters shift invariance shortcut connections short term memory sigma pi networks sigmoid signal analysis simulated annealing softmax speaker adaptation speaker cluster neural network speaker dependence independence normalization speaker voice code network subject index speaking manner spectrogram speech compression generation models translation speech recognition fundamentals state art see dtw hmm neural networks nn hmm predictive networks classifier networks sphinx spontaneous speech spreading activation sri state representation speech density models duplication network implementations state art 
see speech recognition static classification statistical models vs neural networks see probabilities 
stochastic networks streams data sum squared error 
see mean squared error supercomputers supervised learning syllable models symmetric connections inputs sigmoid synapses synchronous update syntactic constraints tanh target offset tdnn design considerations experiments ms tdnn telephone transmission temperature templates temporal constraints dynamics integration modeling structure variability tensor products testing conditions procedures vs cross validating final results thermometer representation threshold units tied mixture density hmm time alignment time concentration network time delays 
see tdnn timit topology hmm phoneme network modification tradeoffs free parameters mixture densities amount training data parameter sharing granularity models codebook size duration modeling hierarchical delays learning rate size weight update frequency subject index training hmm neural networks amount data conditions procedures sequence speed transfer functions transforming output activations transitions word transition penalties translation triphones undefined outputs uniformity units see input hidden output universal computer unstructured networks unsupervised learning variability 
see acoustic temporal 
variable binding vector quantization 
see lvq visual processing viterbi net viterbi search algorithm vocabulary independence vocabulary size vocal tract voiced unvoiced discrimination volume vowel classification warping weights decay initialization range tied training update frequency widrow hoff rule window speech winner take networks word boundaries models recognition spotting transition penalties pair grammar word level training vs frame level training xor function 
