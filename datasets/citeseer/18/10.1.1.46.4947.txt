advances radial basis function networks mark orr institute adaptive neural computation division informatics edinburgh university edinburgh eh lw scotland uk june radial basis function networks published web package matlab functions emphasis linear character rbf networks techniques borrowed statistics forward selection ridge regression 
document update developments associated second version matlab package improvements forward selection ridge regression methods new method cross regression trees rbf networks developed 
anc ed ac uk www anc ed ac uk papers intro ps www anc ed ac uk software rbf zip www anc ed ac uk papers ps www anc ed ac uk software rbf zip contents contents mackay hermite polynomial 
friedman simulated circuit 
maximum marginal likelihood 
review 
em algorithm 
dm algorithm 

optimising size rbfs 
review 
efficient re estimation 
avoiding local minima 
optimal rbf size 
trial values contexts 

regression trees rbf networks 
basic idea 
generating regression tree 
hyperrectangles rbfs 
selecting subset rbfs 
best parameter values 
demonstrations 

appendix applying em algorithm 
eigensystem hh 
radial basis function rbf networks published web associated matlab software package 
approach taken stressed linear character rbf networks traditionally single hidden layer borrowed techniques statistics forward selection ridge regression strategies controlling model complexity main challenge facing methods nonparametric regression 
years ago 
improvements new algorithm devised package matlab functions second version 
document describes theory new developments interest practitioners new software package theorists enhancing existing methods developing new ones 
section describes happens expectation maximisation algorithm applied rbf networks 
section describes simple procedure optimising rbf widths particularly ridge regression 
section describes new algorithm uses regression tree generate centres sizes set candidate rbfs help select subset network 
simulated data sets demonstration described 
mackay hermite polynomial data set dimensional hermite polynomial gamma gammax input values sampled randomly gamma gaussian noise standard deviation oe added outputs :10.1.1.27.9072
training set actual function sample hermite data stars actual function curve 
friedman simulated circuit second data set simulates alternating current circuit parameters resistance angular frequency 
radians second inductance capacitance ranges theta gamma theta gamma random samples parameters ranges generate corresponding values impedance 
gamma 
gaussian noise standard deviation oe added 
resulted training set cases dimensional inputs scalar output problem originates 
applying learning algorithms data original inputs different dynamic ranges rescaled range gamma component 
maximum marginal likelihood maximum marginal likelihood expectation maximisation em algorithm performs maximum likelihood estimation problems variables unobserved 
successfully applied density estimation probabilistic principle components example 
section discusses application em rbf networks 
review probability model linear neural network come expression marginal likelihood data 
likelihood ultimately want maximise 
show results applying em algorithm pair re estimation formulae model parameters 
turns similar set re estimation formula derived simpler method converge rapidly em versions 
draw 
review model estimated linear neural network noisy samples written fh fixed basis functions fw unknown weights estimated 
vector residual errors model data gamma hw design matrix elements ij 
bayesian approach analysing estimation process priori probability weights modelled gaussian variance gammam exp gamma conditional probability data weights modelled gaussian variance oe account noise included outputs training set fy oe gammap exp gamma oe joint probability data weights product represented equivalent cost function logarithms multiplying dropping constant terms obtain ln oe ln oe maximum marginal likelihood conditional probability weights data bayes rule involves product gaussian jwj gamma exp gamma gamma gamma gamma gamma oe gamma oe marginal likelihood data dw oe gammap jpj exp gamma py oe gamma ha gamma note equivalent cost function obtained logarithms multiplying dropping constant terms 
ln oe gamma ln jpj py oe em algorithm em algorithm estimates parameters model iteratively starting initial guess 
iteration consists expectation step finds distribution unobserved variables maximisation step re estimates parameters model maximum likelihood observed missing data combined 
context linear neural network possible consider training set observed data weights fw missing data variance noise oe priori variance weights model parameters 
step expectation conditional probability missing data taken substituted step joint probability combined data equivalent cost function optimised respect model parameters oe steps guaranteed increase marginal probability observed data iterated convergence local maximum 
dm algorithm detailed analysis see appendix results pair re estimation formulae parameters oe oe fl oe gamma fl gamma fl gamma tr gamma initial guesses substituted right hand sides produce new guesses 
process repeated local minimum reached 
note equation derived free energy approach 
shown free energy em algorithm intimately connected 
illustrates hermite data described section 
centres radius created training set input 
plots logarithmic contours sequence oe values re estimated 
log log optimisation oe em 
dm algorithm alternative approach minimising simply differentiate set results zero 
easily done results pair re estimation formulae oe gamma fl fl maximum marginal likelihood call method dm algorithm david mackay derived equations :10.1.1.27.9072
disadvantage absence guarantee iterations converge em counterparts known increase marginal likelihood leave fixed point reached 
fixed point dm fixed point em vice versa multiple fixed points guarantee methods converge starting guess 
plots sequence re estimated values training set rbf network initial values oe 
apparent convergence faster dm em example iterations dm compared em 
fact empirical observation dm converges considerably faster em start guess converge local minimum 
furthermore dm failed converge 
log log optimisation oe dm 
started applying em algorithm rbf networks weight decay ridge regression style penalised likelihood ended pair re estimation formulae noise variance oe prior weight variance turned efficient similar pair formulae known literature time 
rbf rr method matlab software package option maximum marginal likelihood mml model selection criterion gcv bic example 
option selected regularisation parameter re estimated default dm equations 
option set em versions 
optimising size rbfs optimising size rbfs previous concentrated methods optimising regularisation parameter rbf network 
key parameter size rbfs methods provided optimisation 
section describes simple scheme find scale size rbfs network 
review basic concepts covered describe improved version re estimation formula regularisation parameter considerably efficient allows multiple initial guesses optimised effort avoid getting trapped local minima details appendix 
describe method choosing best size rbfs number trial values rendered tractable efficient optimisation concluding remarks 
review linear model fixed basis functions fh weights fw model complexity controlled addition penalty term sum squared errors training set combined error gamma optimised large components weight vector inhibited 
kind penalty known ridge regression weight decay parameter controls amount penalty known regularisation parameter 
nominal number free parameters weights effective number due penalty term fl gamma tr gamma design matrix elements ij 
expression fl monotonic model complexity decreased increased raising lowering value parameter bayesian interpretation ratio oe noise corrupting training set outputs priori variance weights see section 
value known optimal weight gamma optimising size rbfs oe may available practical situation usually necessary establish effective value parallel optimising weights 
may done model selection criterion bic bayesian information criterion gcv generalised cross validation mml maximum marginalised likelihood see section particular re estimation formula 
gcv single formula gamma fl gamma gamma tr gamma gamma gamma gamma delta initial guess evaluate right hand side produces new guess 
resulting sequence re estimated values converge local minimum gcv 
iteration requires inverse matrix costs order floating point operations 
efficient re estimation optimisation iteration re estimation formula burdened necessity having compute expensive matrix inverse iteration 
reformulation individual terms equation eigenvalues eigenvectors hh possible perform iteration reuse results subsequent ones 
amount computation required complete optimisation takes steps converge reduced factor unfortunately technique works single global regularisation parameter multiple parameters applying different groups weights individual weights 
suppose eigenvalues eigenvectors hh fu projections eigenvectors shown appendix terms involved re estimation formula gamma fl gamma re estimated computing explicitly calculating inverse computational cost iteration order avoiding local minima overhead initially calculating eigensystem order taken account performed 
problems bigger represents significant saving computation time feasible optimise multiple guesses initial value decrease chances getting caught local minimum 
avoiding local minima initial guess close local minimum gcv model selection criterion employed re estimation get trapped 
illustrate friedman data set described section rbf network gaussian centres coincident inputs training set fixed radius 
solid curve shows variation gcv open circles show sequence re estimated values corresponding gcv scores 
initial guess gamma sequence converged near gamma shown closed circle local minimum 
global minimum near gamma missed 
log log gcv variation gcv friedman problem sequence re estimations starting gamma compare change different guess gamma initial value time guess sufficiently close global minimum re estimations attracted 
note set eigenvalues eigenvectors compute sequences figures identical 
calculation eigensystem dominates computational costs expensive optimise trial value optimise 
avoid falling local minimum trial values spread wide range optimised solution lowest gcv selected winner 
value determine weights ultimately predictions network 
optimising size rbfs log log gcv initial guess gamma optimal rbf size gaussian radial functions fixed width transfer functions hidden units exp gamma gamma gamma unfortunately re estimation formula simple case scale rbf component input 
properly optimise value require nonlinear optimisation algorithm incorporate optimisation optimal value changes changes 
alternative crude approach test number trial values value optimal calculated re estimation method model selection score noted 
values checked associated lowest score wins 
computational cost procedure dominated cost computing eigenvalues eigenvectors hh calculated separately value procedure computationally demanding full nonlinear optimisation drawback capable identifying best value finite number alternatives 
hand value fully optimised model selection criteria heuristic words approximate nature arguable precise location optimal value practical significance 
illustrate method hermite data described section 
training set input rbf centre 
tried different trial values 
trial value plot variation gcv curves optimal closed trial values contexts log log gcv hermite data set sizes rbfs 
circles re estimation described 
radius value led lowest gcv score corresponding optimal regularisation parameter 
initially increases lowest value gcv score optimum decreases 
eventually reaches lowest value 
increase optimised gcv optimal decreases rapidly 
trial values contexts trial values limited cases small number parameters optimise single parameter parameters trial values number different combinations evaluate easily prohibitively large 
rbf networks separate scale parameter dimension transfer functions example case gaussians exp gamma gamma jk jk mn combinations check number trial values jk number basis functions number dimensions 
possible test trial values scale size ff mechanism generate scales jk transfer functions exp gamma gamma jk ff jk approach taken method section regression tree determines values jk scale size ff optimised testing trial values 
optimising size rbfs shown trial values size rbfs compared model selection criterion 
case ridge regression efficient method optimising regularisation parameter helps reduce computational burden training separate network trial value 
technique methods complexity control including regularisation 
matlab software package method configured set trial values rbf scale 
best value chosen generate rbf network matlab function returns 
regression trees rbf networks regression trees rbf networks section novel method nonparametric regression involving combination regression trees rbf networks 
basic idea regression tree recursively partition input space approximate function half average output value samples contains 
split parallel axes expressed inequality involving input components 
input space divided hyperrectangles organised binary tree branch determined dimension boundary minimise residual error model data 
benefit regression trees information provided split statistics relevance input variable 
components carry information output tend split earliest 
weakness regression trees discontinuous model caused output value jumping boundary hyperrectangles 
problem deciding growing tree equivalently prune fully grown familiar bias variance dilemma faced methods nonparametric regression 
radial basis functions conjunction regression trees help solve problems 
outline basic method combining rbfs regression trees appeared originally describe version idea think improvement 
show results summarise 
basic idea combination trees rbf networks suggested context classification regression cases similar 
elaboration idea appeared 
essentially terminal node classification tree contributes hidden unit rbf network centre radius determined position size corresponding hyperrectangle 
tree sets number positions sizes rbfs network 
model complexity controlled parameters determines amount tree pruning software package generate classification trees ff fixes size rbfs relative hyperrectangles 
major reservation approach taken treatment model complexity 
case scaling parameter ff author claimed little effect prediction accuracy accord previous experience rbf networks 
amount pruning demonstrated effect prediction accuracy fixed value benchmark tests 
discussion control scaling pruning optimise model complexity data set 
regression trees rbf networks method variation kubat alterations 

address model complexity issue nodes regression tree fix rbf network generate set rbfs final network selected 
burden controlling model complexity shifts tree generation rbf selection 

regression tree rbfs produced order selections certain candidate rbfs allowed enter model 
describe way achieve ordering demonstrate produces accurate models plain forward selection 

show contrary method typically quite sensitive parameter ff discuss optimisation multiple trial values 
generating regression tree stage method kubat generate regression tree 
root node tree smallest hyperrectangle contains training set inputs fx size half width centre dimension max ik gamma min ik max ik min ik pg set training set indices 
split root node divides training samples left right subsets sl sr side boundary dimensions sl fi ik bg sr fi ik bg mean output value side split sl sr number samples subset 
residual square error model data sl gamma sr gamma hyperrectangles rbfs split minimises possible choices create children root node easily discrete search dimensions cases 
children root node split recursively manner process terminates node split creating child containing samples minimum min parameter method 
compared parent nodes child centres shifted sizes reduced th dimension 
size regression tree determine model complexity need perform final pruning step normally associated recursive splitting methods 
hyperrectangles rbfs regression tree contains root node nonterminal nodes having children terminal nodes having children 
node associated hyperrectangle input space having centre size described 
node corresponding largest hyperrectangle root node node sizes decrease tree divided smaller smaller pieces 
translate hyperrectangle gaussian rbf centre rbf centre size scaled parameter ff rbf radius ff scalar ff value nodes parameter method addition min 
ff quite kubat ff re related inverse factor plays exactly role 
selecting subset rbfs tree nodes translated rbfs step method select subset inclusion model 
contrast method rbfs terminal nodes included model heavily dependent extent tree pruning control model complexity 
selection performed standard method forward selection novel way employing tree guide order candidate rbfs considered 
standard methods subset selection rbfs generated regression tree treated unstructured collection distinction rbfs corresponding different nodes tree 
intuition suggests best order consider rbfs inclusion model large ones small ones synthesise coarse structure fine details 
turn suggests searching rbf candidates traversing tree largest hyperrectangle rbf root smallest hyperrectangles rbfs terminal nodes 
decision include root node model second include children root node terminal nodes reached 
scheme eventually developed selecting rbfs goes somewhat simple picture influenced considerations 
concerns classic problem forward selection regressor block selection explanatory regressors chosen regression trees rbf networks preference considered 
case danger parent rbf block children 
avoid situation considering add children node selected considered effect deleting parent 
method measure backward elimination forward selection 
reminiscent selection schemes developed mars maps algorithms 
second reason departing simple breadth search size hyperrectangle terms volume level guaranteed smaller size hyperrectangles level parent easy achieve strict largest smallest ordering 
view abandoned attempt achieve strict ordering devised search algorithm dynamically adjusts set selectable rbfs replacing selected rbfs children 
algorithm depends concept active list nodes 
moment selection process nodes children considered inclusion exclusion model 
time rbfs added subtracted model active list expands having node replaced children 
eventually active list coincident terminal nodes search terminated 
detail steps algorithm follows 

initialise active list root node model root node rbf 

nonterminal nodes active list consider effect model selection criterion adding just children rbfs possible modifications model 
parent rbf model consider effect removing adding children rbfs just removing possible modifications 

total number possible adjustments model times number active nonterminal nodes depending rbfs model 
possibilities choose decreases model selection criterion 
update current model remove node involved active list replacing children 
modifications decrease selection criterion chose active nodes random replace children leave model unaltered 

return step repeat active nodes terminal nodes 
selection process terminated network weights calculated usual way solving normal equation gamma design matrix 
need regularisation term appears equations example model complexity limited selection process 
best parameter values best parameter values method main parameters model selection criterion min controls depth regression tree ff determines relative size hyperrectangles rbfs 
model selection criterion conservative bic tends produce parsimonious models rarely performed worse gcv significantly better 
line experiences practitioners algorithms subset selection modified gcv conservative bic gave better results gcv 
min ff simple method comparing model selection scores number trial values rbf widths section 
means growing trees trial value min tree selecting models sets rbfs value ff 
cost extra computation trial values longer algorithm takes search 
basic algorithm unduly expensive number trial values kept fairly low alternatives parameter computation time acceptable 
demonstrations shows prediction pure regression tree sample hermite data section 
clarity samples shown just target function prediction 
course model discontinuous horizontal section corresponds terminal node tree 
target prediction pure regression tree prediction hermite data 
regression trees rbf networks tree produced prediction shown grown splitting violated minimum number samples allowed node min 
pruning sophisticated form complexity control kind tree suitable practical prediction method 
method tree create rbf candidates 
model complexity controlled separate process selects subset rbfs network 
shows predictions combined method data set subset rbfs selected pool candidates generated tree nodes 
model continuous complexity matched data 
target prediction combined method hermite data 
demonstration turn attention friedman data set section 
experiments mars algorithm friedman estimated accuracy method replicating data sets times computing mean standard deviation scaled sum square error 
data set best results corresponding favourable values parameters mars method sigma 
compare algorithm mars test effect multiple trial values method parameters min ff conducted similar experiment 
started tried different settings trial values identified gave results test data 
replications applied method twice 
run trial values discovered earlier 
second run single best value parameter average trial values forcing value replicated data set 
results shown table 
apparent results practically identical mars full sets trial values significantly inferior single best values 
min ff error sigma sigma table results replications friedman data set 
test replicated data sets compared alternative methods selecting rbfs candidates generated tree standard forward selection method described section uses tree guide order candidates considered 
difference runs model parameters row table 
performance tree guided selection sigma table forward selection significantly worse sigma 
described method nonparametric regression combining regression trees radial basis function networks 
method similar advantages continuous model automatic relevance determination significant improvements 
main enhancement addition automatic method control model complexity selection rbfs 
developed novel procedure selecting rbfs structure tree 
evidence method comparable performance known mars algorithm novel features trial parameter values tree guided selection beneficial 
detailed evaluations delve data sets preparation preliminary results support 
matlab software package implementations method 
function rbf rt uses tree guided selection rbf rt uses forward selection 
operation function described examples comprehensive manual 
appendix appendix applying em algorithm want maximise marginal probability observed data substituting expectations conditional probability unobserved data cost function joint probability combined data minimising respect parameters oe noise variance priori weight variance 
hwi gamma gamma oe gamma expectation hw wi tr hw tr hw gamma tr gamma gamma oe tr gamma gamma fl step follows fl gamma tr gamma effective number parameters oe regularisation parameter 
similarly ei tr tr gamma tr gamma oe tr ha gamma oe fl gamma hw linear tr ha gamma expression effective number parameters fl 
equations summarise expectation conditional probability substituted joint probability combined data equivalent cost function resulting expression optimised respect oe note parameters held constant old values explicit occurrences oe varied optimisation 
differentiating respect oe equating results zero substituting expectations get re estimation formulae oe fl oe gamma fl eigensystem hh eigensystem hh want derive expressions terms eigenvalues eigenvectors hh 
start singular value decomposition design matrix usv thetap thetam orthogonal thetam delta delta delta delta delta delta 
delta delta delta delta delta delta 
delta delta delta contains singular values note due orthogonality hh uss eigenvalues eigenvectors matrix hh eigenvalues non negative assume ordered largest smallest eigenvectors orthonormal ffi ii 
want derive expressions terms just eigenvalues eigenvectors hh preliminary step derive basic relations 
matrix inverse re estimation gamma gamma sv gamma gamma note second step impossible regularisation term proportional identity matrix analysis breaks case multiple regularisation parameters 
secondly optimal weight vector gamma gamma gamma projection appendix thirdly derive fl gamma tr gamma gamma tr gamma gamma tr gamma gamma assumed step follows gamma eigenvalues zero 
true case gamma singular values annihilated product fourthly preliminary calculations vector residual errors gamma gamma gamma gamma gamma ready tackle terms 
gamma fl gamma set steps similar derivation follows tr gamma gamma tr gamma gamma eigensystem hh step follows similar way step 
tackle term gamma get gamma gamma sum squared residual errors gamma gamma derivation assumed reasons similar stated derivation result true equations express terms eigenvalues eigenvectors hh main goal appendix 
useful expressions involving eigensystem hh ln jpj ln oe oe ln oe gamma ln gamma oe delta py oe oe gamma ha gamma oe noise variance priori variance weights see section 
example expressions substituted equation cost function associated marginal likelihood data ln oe terms cancel leaving ln oe gamma ln jpj py oe ln gamma oe delta oe barron xiao 
discussion multivariate adaptive regression splines friedman 
annals statistics 
bishop williams 
em optimization density models 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press cambridge ma 
bishop 
neural networks pattern recognition 
clarendon press oxford 
breiman friedman olsen stone 
classification regression trees 
wadsworth belmont ca 
dempster laird rubin 
maximum incomplete data em algorithm 
journal royal statistical society 
friedman 
multivariate adaptive regression splines discussion 
annals statistics 
geman bienenstock doursat 
neural networks bias variance dilemma 
neural computation 
kubat 
decision trees initialize radial basis function networks 
ieee transactions neural networks 
kubat 
initialization rbf networks decision trees 
proc 
th belgian dutch conf 
machine learning pages 
mackay :10.1.1.27.9072
bayesian interpolation 
neural computation 
mackay 
comparison approximate methods handling hyperparameters 
accepted publication neural computation 
moody 
effective number parameters analysis generalisation regularisation nonlinear learning systems 
moody hanson lippmann editors neural information processing systems pages 
morgan kaufmann san mateo ca 
neal hinton 
view em algorithm justifies incremental sparse variants 
jordan editor learning graphical models 
kluwer academic press 
orr 
local smoothing radial basis function networks 
international symposium artificial neural networks taiwan 
orr 
regularisation selection radial basis function centres 
neural computation 
orr 
radial basis function networks 
technical report institute adaptive neural computation division informatics edinburgh university 
www anc ed ac uk papers intro ps 
orr 
matlab routines subset selection ridge regression linear neural networks 
technical report institute adaptive neural computation division informatics edinburgh university 
www anc ed ac uk software rbf zip 
orr 
em algorithm regularised radial basis function networks 
international conference neural networks brain beijing china october 
orr 
matlab functions radial basis function networks 
technical report institute adaptive neural computation division informatics edinburgh university 
download www anc ed ac uk software rbf zip 
quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
rasmussen neal hinton van camp ghahramani revow tibshirani 
delve manual 
www cs utoronto ca delve 
tipping bishop 
mixtures principle component analysers 
technical report ncrg neural computing research group aston university uk 
tipping bishop 
probabilistic principal component analysis 
technical report ncrg neural computing research group aston university uk 
