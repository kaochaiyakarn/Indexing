competitive learning methods bernd fritzke systems biophysics institute neural computation ruhr universit bochum draft april additions refinements planned document stay draft status 
comments welcome 
report purpose describing algorithms literature related competitive learning 
uniform terminology methods 
identical examples provided allow qualitative comparisons methods 
line version document contains hyperlinks java implementations discussed methods 
www neuroinformatik ruhr uni bochum de ini vdm research contents common properties notational conventions goals competitive learning error minimization 
entropy maximization 
feature mapping 
goals 
hard competitive learning batch update lbg 
line update basic algorithm 
constant learning rate 
means 
exponentially decaying learning rate 
scl fixed network dimensionality neural gas 
competitive hebbian learning 
neural gas plus competitive hebbian learning 
growing neural gas 
methods 
scl fixed network dimensionality self organizing feature map 
growing cell structures 
growing grid 
methods 
quantitative results discussion chapter area competitive learning large number models exist similar goals differ considerably way 
common goal algorithms distribute certain number vectors possibly highdimensional space 
distribution vectors reflect possible ways probability distribution input signals general explicitly sample vectors 
report review methods related competitive learning 
common terminology comparison methods easy 
software implementations methods provided allowing experiments different data distributions observation learning process 
java programming language implementations run large number platforms need compilation local adaptation 
report structured follows chapter basic terminology introduced properties shared models outlined 
chapter discusses possible goals competitive learning systems 
chapter concerned hard competitive learning models winner input signal adapted 
chapters describe soft competitive learning 
models characterized adapting addition winner units network 
chapter concerned models network fixed dimensionality 
chapter describes models fixed dimensionality may data visualization define mapping usually high dimensional input space low dimensional network structure 
chapters written contain quantitative results discussion 
chapter common properties notational conventions models described report share architectural properties described chapter 
simplicity refer models network model belong usually understood neural network 
network consists set units fc unit associated vector indicating position receptive field center input space 
units network exists possibly empty set ae theta neighborhood connections unweighted symmetric connections weighted connections multi layer perceptrons rumelhart 
methods extend adaptation winner see topological neighbors 
unit denote set direct topological neighbors fi aj cg dimensional input signals assumed generated continuous probability density function finite training data set input signal winner units defined unit nearest vector arg min gamma delta denotes euclidean vector norm 
case tie units chosen winner throwing fair dice 
cases denote current winner simply omitting dependency 
winner second nearest unit distant units interest denote nearest unit winner second nearest unit 
fundamental closely related concepts computational geometry important understand context 
voronoi tessellation delaunay triangulation set vectors wn see voronoi region particular vector defined set points nearest vector ji arg min ng gamma kg order data point associated exactly voronoi region define previously done winner case tie corresponding point mapped random nearest vectors 
alternatively postulate general positions data points vectors case tie zero probability 
known voronoi region convex area 
ff gamma ff ff partition formed voronoi polygons called voronoi tessellation dirichlet tessellation see 
efficient algorithms compute known dimensional data sets preparata shamos 
concept applicable spaces arbitrarily high dimensions 
connects pairs points respective voronoi regions share edge gamma dimensional spaces dimension gets delaunay triangulation see 
triangulation special possible triangulation various respects 
triangulation circumcircle triangle contains point original point set vertices triangle 
delaunay triangulation shown optimal function interpolation omohundro 
competitive hebbian learning method see section generates subgraph delaunay triangulation limited areas input space data 
convenience define voronoi region unit voronoi region vector js cg case finite input data set denote unit term voronoi set subset winner see djs cg chapter 
common properties notational conventions point set corresponding voronoi tessellation corresponding delaunay triangulation 
data set voronoi sets input data set shown partition voronoi sets particular set vectors 
voronoi set contains data points corresponding voronoi field 
chapter goals competitive learning number different mutually exclusive goals set competitive learning systems 
goals discussed 
error minimization frequent goal minimization expected quantization distortion error 
case continuous input signal distribution amounts finding values vectors error gamma minimized voronoi region unit 
correspondingly case finite data set error jdj gamma minimized voronoi set unit typical application error minimization important vector quantization linde gray 
vector quantization data transmitted limited bandwidth communication channels transmitting data vector index nearest vector 
set vectors called codebook context assumed known sender receiver 
receiver transmitted indexes retrieve corresponding vector 
information loss case equal distance current data vector nearest vector 
expectation value error described equations 
particular data distribution clustered contains subregions high probability density dramatic compression rates achieved vector quantization relatively little distortion 
chapter 
goals competitive learning entropy maximization vectors distributed vector chance winner randomly generated input signal jaj interpret generation input signal subsequent mapping nearest unit random experiment assigns value random variable equivalent maximizing entropy gamma log log delta expectation operator 
data generated continuous probability distribution equivalent jaj case finite data set corresponds situation voronoi set contains discretization effects number data vectors jr jdj jaj advantage choosing vectors maximize entropy inherent robustness resulting system 
removal failure vector affects limited fraction data 
entropy maximization error minimization general achieved simultaneously 
particular data distribution highly non uniform goals differ considerably 
consider signal distribution percent input signals come small point region input space percent uniformly distributed huge hypercube 
maximize entropy half vectors positioned region 
minimize quantization error single vector positioned point region reducing quantization error signals basically zero uniformly distributed hypercube 
feature mapping network architectures possible map high dimensional input signals lower dimensional structure way similarity relations original data mapping 
denoted feature mapping useful data visualization 
prerequisite network fixed dimensionality 
case self organizing feature map methods discussed section report 
related question topology preserving mapping input data space discrete network structure similarities preserved quantitative measures proposed evaluate topographic product bauer pawelzik topographic function villmann 

goals goals competitive learning methods density estimation generation estimate unknown probability density input signals 
possible goal clustering partition data subgroups clusters sought distance data items cluster intra cluster variance small distance data items stemming different clusters inter cluster variance large 
different flavors clustering problem exist depending number clusters pre defined result clustering process 
comprehensive overview clustering methods jain dubes 
combinations competitive learning methods supervised learning approaches feasible 
possibility radial basis function networks rbfn competitive learning position radial centers moody darken fritzke 
local linear maps combined competitive learning methods walter martinetz fritzke 
simplest case voronoi region linear model describe input output relationship data voronoi region 
chapter hard competitive learning hard competitive learning winner take learning comprises methods input signal determines adaptation unit winner 
different specific methods obtained performing batch line update 
batch methods lbg possible input signals come finite set case evaluated adaptations done 
iterated number times 
line methods hand means perform update directly input signal 
line methods variants constant adaptation rate distinguished variants decreasing adaptation rates different kinds 
general problem occurring hard competitive learning possible existence dead units 
units due inappropriate initialization winner input signal keep position indefinitely 
units contribute networks purpose error minimization considered harmful unused network resources 
common way avoid dead units distinct sample vectors initialize vectors 
problem remains vectors initialized randomly expected initial local density proportional 
may suboptimal certain goals 
example goal error minimization highly non uniform better regions high probability density vectors dictated regions 
possibility adapt distribution vectors specific goal local statistical measures directing insertions possibly deletion units see sections 
problem hard competitive learning different random initializations may lead different results 
purely local adaptations may able get system poor local minimum started 
way cope problem change winner take approach hard competitive learning winner take approach soft competitive learning 
case winner units adapted see chapters 
general decreases dependency initialization 
batch update lbg lbg generalized lloyd algorithm linde forgy lloyd works repeatedly moving vectors arithmetic mean voronoi sets 
theoretical foundation shown 
line update basic algorithm gray necessary condition set vectors fw jc ag minimize distortion error jdj gamma vector fulfills centroid condition 
case finite set input signals euclidean distance measure centroid condition reduces jr voronoi set unit complete lbg algorithm 
initialize set contain units fc vectors chosen randomly mutually different finite data set 
compute unit voronoi set 
move vector unit mean voronoi set jr 
step change continue step 
return current set vectors 
steps form called lloyd iteration guaranteed decrease distortion error leave unchanged 
lbg guaranteed converge finite number lloyd iterations local minimum distortion error function see example 
extension lbg called lbg fritzke able improve local minima lbg 
lbg performs non local moves single vectors contribute error reduction useful lbg locations large quantization error occur 
normal lbg find nearest local minimum distortion error function 
iterated long lbg generated local minima improve 
lbg requires finite data set guaranteed converge finite number steps 
line update basic algorithm situations data set huge batch methods impractical 
cases input data comes continuous stream unlimited length completely impossible apply batch methods 
resort line update described follows 
initialize set contain units fc vectors chosen randomly 
chapter 
hard competitive learning data set lloyd iterations lloyd iteration lloyd iterations lloyd iterations lloyd iterations lloyd iterations lloyd iterations lloyd iterations lbg simulation 
data set consisting data items 
vectors initialized randomly points corresponding voronoi tessellation shown 
positions vectors indicated number lloyd iterations 
vectors move previous lloyd iteration shown black 
simulation lbg converged lloyd iterations 

constant learning rate 
generate random input signal 

determine winner arg min gamma 
adapt vector winner deltaw ffl gamma 
maximum number steps reached continue step 
learning rate ffl determines extent winner adapted input signal 
depending ffl stays constant decays time different methods possible described 
constant learning rate learning rate constant ffl ffl ffl value vector represents exponentially decaying average input signals unit winner 
see sequence input signals winner 
sequence successive values taken written random signal ffl gamma gamma ffl ffl gamma ffl ffl gamma ffl gamma ffl ffl ffl 
gamma ffl gamma ffl gamma ffl ffl gamma ffl gammai obvious influence past input signals decays exponentially fast number input signals winner see 
input signal determines fraction ffl current value consequences 
system stays adaptive principle able follow non stationary signal distribution 
second reason convergence 
large number input signals current input signal cause considerable change vector winner 
typical behavior system case stationary signal distribution vectors drift initial positions quasi stationary positions chapter 
hard competitive learning ffl ffl ffl ffl influence input signal vector winner function number input signals winner including 
results different constant adaptation rates shown 
respective section axis indicates signals needed influence gamma example learning rate ffl set additional signals section axis near needed happen 
start wander dynamic equilibrium 
better quasi stationary positions terms mean square error achieved smaller learning rates 
case system needs adaptation steps reach quasi stationary positions 
distribution non stationary information non stationarity rapidly distribution change set appropriate learning rate 
rapidly changing distributions relatively large learning rates vice versa 
shows stages simulation simple ring shaped data distribution 
displays final results adaptation steps distribution 
cases constant learning rate ffl 
means having constant learning rate decrease time 
particularly interesting way doing separate learning rate unit set harmonic series ffl time parameter stands number input signals particular unit winner far 
algorithm known means macqueen appropriate name vector exact arithmetic mean input signals winner far 
sequence successive values 
means signals signals signals signals signals signals signals voronoi regions hard competitive learning simulation sequence ring shaped uniform probability distribution 
constant adaptation rate 
initial state 
intermediate states 
final state 
voronoi tessellation corresponding final state 
hard competitive learning simulation results input signals different probability distributions 
constant learning rate 
distribution uniform shaded areas 
probability density upper shaded area times high lower 
distribution uniform shaded area 
distribution circles indicates standard deviation gaussian kernel generate data 
gaussian kernels priori probability 
chapter 
hard competitive learning random signal ffl gamma ffl gamma 
gamma ffl gamma gamma note set signals particular unit winner may contain elements lie outside current voronoi region reason adaptation changes borders voronoi region represents arithmetic mean signals winner time signal may lie voronoi regions belonging units 
important point means strict convergence lbg reason sum harmonic series diverges lim divergence large number input signals correspondingly low values learning rate ffl arbitrarily large modifications input vector may occur principal 
large modification improbable simulations signal distribution stationary vectors usually quickly take values changed 
fact shown means converge asymptotically configuration vector positioned coincides expectation value voronoi region macqueen 
note continuous variant centroid condition 
shows stages simulation simple ring shaped data distribution 
displays final results adaptation steps distribution 
exponentially decaying learning rate possibility decaying adaptation rate proposed ritter 
context self organizing maps 
propose exponential decay ffl ffl ffl ffl max 
exponentially decaying learning rate signals signals signals signals signals signals signals voronoi regions means simulation sequence ring shaped uniform probability distribution 
initial state 
intermediate states 
final state 
voronoi tessellation corresponding final state 
final vectors reflects clusters initial state see particular region higher vector density lower left 
means simulation results input signals different probability distributions described 
chapter 
hard competitive learning exponential decay harmonic series comparison exponentially decaying learning function ffl ffl ffl max harmonic series particular set parameters ffl ffl max 
displayed difference learning rates interpreted noise case exponentially decaying learning rate introduced system gradually removed 
ffl ffl initial final values learning rate max total number adaptation steps taken 
kind learning rate compared harmonic series specific choice parameters 
particular simulation exponentially decaying learning rate considerably larger dictated harmonic series 
interpreted introducing noise system gradually removed suggests relationship simulated annealing techniques kirkpatrick 
simulated annealing gives system ability escape poor local minima initialized 
preliminary experiments comparing means hard competitive learning learning rate indicate method susceptible poor initialization data distributions gives lower mean square error 
small constant learning rates usually give better results means 
special case vector exists jaj completely impossible beat means average case realizes optimal estimator mean samples occurred far 
observations complete agreement darken moody investigated means number different learning rate schedules constant learning rates learning rate square root rate means ffl 
results indicate larger means inferior learning rate schedules 
examples give difference distortion error orders magnitude 
shows stages simulation simple ring shaped data distribution 
displays final results adaptation steps distribution 
parameters examples ffl ffl max 

exponentially decaying learning rate signals signals signals signals signals signals signals voronoi regions hard competitive learning simulation sequence ring shaped uniform probability distribution 
exponentially decaying learning rate 
initial state 
intermediate states 
final state 
voronoi tessellation corresponding final state 
hard competitive learning simulation results input signals different probability distributions described 
exponentially decaying learning rate 
chapter soft competitive learning fixed network dimensionality chapter methods area soft competitive learning described 
common contrast models chapter topology fixed dimensionality imposed network 
case topology neural gas 
cases dimensionality network depends local dimensionality data may vary input space 
neural gas neural gas algorithm martinetz schulten sorts input signal units network distance vectors 
rank order certain number units adapted 
number adapted units adaptation strength decreased fixed schedule 
complete neural gas algorithm 
initialize set contain units fc vectors chosen randomly 
initialize time parameter 
generate random input signal 

order elements distance find sequence indices gamma vector closest vector second closest gamma vector vectors exist gamma gamma martinetz 
denote number associated 
adapt vectors deltaw ffl delta delta gamma 
competitive hebbian learning time dependencies max ffl ffl ffl ffl max exp gammak 
increase time parameter 
max continue step time dependent parameters suitable initial values ffl final values ffl chosen 
shows stages simulation simple ring shaped data distribution 
displays final results adaptation steps distribution 
martinetz 
parameters ffl ffl max 
competitive hebbian learning method martinetz schulten martinetz usually conjunction methods see sections 
instructive study competitive hebbian learning 
method change vectors interpreted having zero learning rate 
generates number neighborhood edges units network 
proved martinetz generated graph optimally topology preserving general sense 
particular edge graph belongs delaunay triangulation corresponding set vectors 
complete competitive hebbian learning algorithm 
initialize set contain units fc vectors chosen randomly 
initialize connection set ae theta empty set 
generate random input signal 

determine units arg min gamma arg min gamma 
connection exist create 
continue step maximum number signals reached 
shows stages simulation simple ring shaped data distribution 
displays final results adaptation steps distribution 
chapter 
scl fixed network dimensionality signals signals signals signals signals signals signals voronoi regions neural gas simulation sequence ring shaped uniform probability distribution 
initial state 
intermediate states 
final state 
voronoi tessellation corresponding final state 
initially strong neighborhood interaction leads clustering vectors relaxes distribution vectors 
neural gas simulation results input signals different probability distributions described 

competitive hebbian learning signals signals signals signals signals signals signals voronoi regions competitive hebbian learning simulation sequence ring shaped uniform probability distribution 
initial state 
intermediate states 
final state 
voronoi tessellation corresponding final state 
obviously method sensitive initialization initial positions equal final positions 
competitive hebbian learning simulation results input signals different probability distributions described 
chapter 
scl fixed network dimensionality neural gas plus competitive hebbian learning method martinetz schulten straight forward superposition neural gas competitive hebbian learning 
denoted topology representing networks martinetz schulten 
term general apply growing neural gas model described 
adaptation step connection winner second nearest unit created competitive hebbian learning 
vectors adapted neural gas method mechanism needed remove edges valid anymore 
done local edge aging mechanism 
complete neural gas competitive hebbian learning algorithm 
initialize set contain units fc vectors chosen randomly 
initialize connection set ae theta empty set initialize time parameter 
generate random input signal 

order elements distance find sequence indices gamma vector closest vector second closest gamma vector vectors exist gamma gamma martinetz 
denote number associated 
adapt vectors deltaw ffl delta delta gamma time dependencies max ffl ffl ffl ffl max exp gammak 
exist create connection set age connection zero refresh edge age 
growing neural gas 
increment age edges emanating age age set direct topological neighbors see equation 

remove edges age larger maximal age max 
increase time parameter 
max continue step 
time dependent parameters suitable initial values ffl final values ffl chosen 
shows stages simulation simple ring shaped data distribution 
displays final results adaptation steps distribution 
martinetz 
parameters ffl ffl max 
network size set 
growing neural gas method fritzke different previously described models number units changed increased selforganization process 
growth mechanism earlier proposed growing cell structures fritzke topology generation competitive hebbian learning martinetz schulten combined new model 
starting units new units inserted successively 
determine insert new units local error measures gathered adaptation process 
new unit inserted near unit accumulated error 
complete growing neural gas algorithm 
initialize set contain units fc vectors chosen randomly 
initialize connection set ae theta empty set 
generate random input signal 

determine winner second nearest unit arg min gamma arg min gamma chapter 
scl fixed network dimensionality signals signals signals signals signals signals signals voronoi regions neural gas competitive hebbian learning simulation sequence ring shaped uniform probability distribution 
initial state 
intermediate states 
final state 
voronoi tessellation corresponding final state 
centers move neural gas algorithm 
additionally edges created competitive hebbian learning removed refreshed 
neural gas competitive hebbian learning simulation results input signals different probability distributions described 

growing neural gas 
connection exist create set age connection zero refresh edge age 
add squared distance input signal winner local error variable deltae gamma 
adapt vectors winner direct topological neighbors fractions ffl ffl respectively total distance input signal deltaw ffl gamma deltaw ffl gamma see equation set direct topological neighbors 
increment age edges emanating age age 
remove edges age larger amax results units having emanating edges remove units 

number input signals generated far integer multiple parameter insert new unit follows ffl determine unit maximum accumulated error arg max ffl determine neighbors unit maximum accumulated error arg max ffl add new unit network interpolate vector frg ffl insert edges connecting new unit units remove original edge ffl decrease error variables fraction ff deltae deltae ffl interpolate error variable 
decrease error variables units deltae gammafi 
stopping criterion net size performance measure fulfilled continue step 
shows stages simulation simple ring shaped data distribution 
displays final results adaptation steps distribution 
parameters simulations ffl ffl ff fi amax 
chapter 
scl fixed network dimensionality signals signals signals signals signals signals signals voronoi regions growing neural gas simulation sequence ring shaped uniform probability distribution 
initial state 
intermediate states 
final state 
voronoi tessellation corresponding final state 
maximal network size set 
growing neural gas simulation results input signals different probability distributions described 

methods methods models fixed network dimensionality known 
proposed method frequent winners get bad conscience winning add penalty term distance input signal 
leads eventually situation unit wins approximately equally entropy maximization 
kangas 
proposed minimum spanning tree units neighborhood topology eliminate priori choice topology models 
methods proposed 
chapter soft competitive learning fixed network dimensionality chapter methods area soft competitive learning described network fixed dimensionality chosen advance 
advantage fixed network dimensionality network defines mapping dimensional input space arbitrarily large dimensional structure 
possible get low dimensional representation data may visualization purposes 
self organizing feature map model stems kohonen builds earlier willshaw von der malsburg 
model similar developed neural gas model see decaying neighborhood range adaptation strength 
important difference topology constrained dimensional grid ij change selforganization 
distance grid determine strongly unit km adapted unit ij winner 
distance measure norm manhattan distance ji gamma kj jj gamma mj km ij ritter 
propose function define relative strength adaptation arbitrary unit network winner rs exp gammad oe standard deviation oe gaussian varied oe oe oe oe max suitable initial value oe final value oe complete self organizing feature map algorithm 
initialize set contain delta units fc 
growing cell structures vectors chosen randomly 
initialize connection set form rectangular theta grid 
initialize time parameter 
generate random input signal 

determine winner arg min gamma 
adapt unit deltaw ffl rs gamma oe oe oe oe max ffl ffl ffl ffl max 
increase time parameter 
max continue step 
shows stages simulation simple ring shaped data distribution 
displays final results adaptation steps distribution 
parameters oe oe ffl ffl max 
growing cell structures model fritzke similar growing neural gas model main difference network topology constrained consist dimensional simplices positive integer chosen advance 
basic building block initial configuration network dimensional simplex 
line triangle tetrahedron 
network configuration number adaptation steps update vectors nodes gather local error information node 
error information decide insert new nodes 
new node inserted splitting longest edge emanating node maximum accumulated error 
doing additional edges inserted resulting structure consists exclusively dimensional simplices 
growing cell structures learning procedure described compared original growing cell structures algorithm described fritzke slight changes simplifications done regarding re distribution accumulated error 
discussion removal units left completely sake brevity 
chapter 
scl fixed network dimensionality signals signals signals signals signals signals signals voronoi regions self organizing feature map simulation sequence ring shaped uniform probability distribution 
initial state 
intermediate states 
final state 
voronoi tessellation corresponding final state 
large adaptation rates large neighborhood range cause strong initial adaptations decrease 
self organizing feature map simulation results input signals different probability distributions described 

growing cell structures 
choose network dimensionality initialize set contain units fc vectors chosen randomly 
initialize connection set ae theta unit connected unit network topology dimensional simplex 

generate random input signal 

determine winner arg min gamma 
add squared distance input signal winner unit local error variable deltae gamma 
adapt vectors direct topological neighbors fractions ffl ffl respectively total distance deltaw ffl gamma deltaw ffl gamma denote set direct topological neighbors 
number input signals generated far integer multiple parameter insert new unit follows ffl determine unit maximum accumulated error arg max ffl insert new unit splitting longest edge emanating say edge leading unit insert connections remove original connection 
re build structure consists dimensional simplices new unit connected common neighbors units set ffl interpolate vector vectors ffl decrease error variables neighbors fraction depends number neighbors deltae gamma ff jn depending problem hand local measures possible number input signals particular unit winner positioning error robot arm controlled network 
local measure generally interested reduce reduced insertion new units 
chapter 
scl fixed network dimensionality ffl set error variable new unit mean value neighbors jn 
decrease error variables units deltae gammafi 
stopping criterion net size performance measure fulfilled continue step 
shows stages simulation simple ring shaped data distribution 
displays final results adaptation steps distribution 
parameters simulations ff fi 
growing grid growing grid incremental network 
basic principles growing cell structures growing neural gas applied modifications rectangular grid 
alternatively growing grid seen incremental variant self organizing feature map 
model distinct phases growth phase fine tuning phase 
growth phase rectangular network built starting minimal size inserting complete rows columns desired size reached performance criterion met 
constant parameters phase 
fine tuning phase size network changed anymore decaying learning rate find final values vectors 
self organizing map network structure dimensional grid ij 
grid initially set theta structure 
distance grid determine strongly unit km adapted unit ij winner 
distance measure norm ji gamma kj jj gamma mj km ij function determine adaptation strength unit winner self organizing feature map rs exp gammad oe width parameter oe remains constant simulation 
chosen relatively small compared values usually self organizing feature map 
note growing grid network grows fraction units adapted winner decreases 
case self organizing feature map achieved constant network size decreasing neighborhood width 
complete growing grid algorithm growth phase 
set initial network width height 
growing grid signals signals signals signals signals signals signals voronoi regions growing cell structures simulation sequence ring shaped uniform probability distribution 
initial state 
intermediate states 
final state 
voronoi tessellation corresponding final state 
construction network structure consists triangles case 
growing cell structures simulation results input signals different probability distributions described 
chapter 
scl fixed network dimensionality initialize set contain delta units fc vectors chosen randomly 
initialize connection set form rectangular theta grid 
initialize time parameter 
generate random input signal 

determine winner arg min gamma 
increase local counter variable winner 
increase time parameter 
adapt unit deltaw ffl rs gamma ffl ffl 
number input signals generated current network size reaches multiple network size delta delta ffl determine unit largest value arg max ffl determine direct neighbor distant vector arg max kw gamma ffl depending relative position continue cases case row grid gamma insert new column units columns interpolate vectors new units vectors respective direct row 

growing grid adjust variable number columns case column grid gamma insert new row units rows interpolate vectors new units vectors respective direct columns 
adjust variable number rows ffl reset local counter values ffl reset time parameter 
desired network size achieved delta min continue step 
fine tuning phase 
generate random input signal 

determine winner arg min gamma 
adapt unit deltaw ffl rs gamma ffl ffl ffl ffl max max delta delta 
max continue step 
shows stages simulation simple ring shaped data distribution 
displays final results adaptation steps distribution 
parameters growth phase oe ffl 
parameters fine tuning phase oe ffl unchanged ffl min 
compares growing grid algorithm incremental methods growing cell structures growing neural gas difference apart topology counter variables redistributed new units inserted 
values set zero row column inserted 
chapter 
scl fixed network dimensionality signals signals signals signals signals signals signals voronoi regions growing grid simulation sequence ring shaped uniform probability distribution 
initial state 
intermediate states 
final state 
voronoi tessellation corresponding final state 
growing grid simulation results input signals different probability distributions described 
note chosen topology theta extreme height width ratio matches distribution hand 
depending initial conditions topologies occur simulations distribution 
topologies deviate square shape usually self organizing maps 
cactus theta mixture distribution theta topology automatically selected algorithm 

methods means statistical information winning frequencies discarded insertion 
gather statistical evidence insert new units time number adaptation steps insertion step proportional network size see equation 
simplifies algorithm increases computational complexity 
principle done growing neural gas growing cell structures effectively eliminating need re distribute accumulated information insertions price increased computational complexity 
parameter oe governs neighborhood range function regularizer 
set large values neighboring units forced similar vectors layout network projected input space appear regular adapted underlying data distribution 
smaller values oe give units possibilities adapt independently 
oe set zero growing grid algorithm apart insertions approaches hard competitive learning 
similar self organizing feature map growing grid algorithm easily applied network structures dimensions 
useful cases dimensional networks networks higher dimensionality visualized easily 
methods number methods fixed dimensionality exist 
bauer villmann proposed method develops grid 
contrast growing grid method algorithm automatically determines suitable dimensionality grid 
blackmore miikkulainen irregular network grow positions plane restricted lie dimensional grid 
rodrigues almeida increased speed normal self organizing feature map developing interpolation method symmetrically increases number units network interpolation 
method reported give considerable speed able choose different dimensions width height grid approach bauer villmann growing grid 
approaches proposed xu 
chapter quantitative results chapter discussion bibliography 
bauer pawelzik 
quantifying neighborhood preservation selforganizing feature maps 
ieee transactions neural networks 

bauer villmann 
growing output space selforganizing feature map 
tr international computer science institute berkeley 
blackmore miikkulainen 
incremental grid growing encoding highdimensional structure dimensional feature map 
tr ai university texas austin austin tx 
darken moody 
fast adaptive means clustering empirical results 
proc 
ijcnn volume ii pages 
ieee neural networks council 

adding conscience competitive learning 
ieee international conference neural networks volume pages new york 
san diego ieee 
forgy 
cluster analysis multivariate data efficiency vs classifications 
biometrics 

fritzke 
growing cell structures self organizing network unsupervised supervised learning 
neural networks 
fritzke 
fast learning incremental rbf networks 
neural processing letters 
fritzke 
growing neural gas network learns topologies 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press cambridge ma 
fritzke 
incremental learning local linear mappings 
fogelman gallinari editors icann international conference artificial neural networks pages paris france 
ec cie 
fritzke 
lbg method vector quantization improvement lbg inspired neural networks 
neural processing letters 
gray 
vector quantization 
ieee assp magazine 
gray 
vector quantization signal compression 
kluwer academic press 
jain dubes 
algorithms clustering data 
prentice hall 
bibliography 
neural network adapts structure set patterns 
hartmann editors parallel processing neural systems computers pages 
elsevier science publishers 
kangas kohonen laaksonen 
variants self organizing maps 
ieee transactions neural networks 
kirkpatrick jr vecchi 
optimization simulated annealing 
science 
kohonen 
self organized formation topologically correct feature maps 
biological cybernetics 
linde gray 
algorithm vector quantizer design 
ieee transactions communication com 
lloyd 
squares quantization pcm 
technical note bell laboratories 
published ieee transactions information theory 
macqueen 
convergence means partitions minimum average variance 
ann 
math 
statist 

macqueen 
methods classification analysis multivariate observations 
volume proceedings fifth berkeley symposium mathematical statistics probability pages berkeley 
university california press 
martinetz 
competitive hebbian learning rule forms perfectly topology preserving maps 
icann international conference artificial neural networks pages amsterdam 
springer 
martinetz schulten 
neural gas network vector quantization application time series prediction 
ieee transactions neural networks 
martinetz ritter schulten 
neural network learning visuomotor coordination robot arm 
international joint conference neural networks pages ii washington dc 
martinetz schulten 
neural gas network learns topologies 
kohonen simula kangas editors artificial neural networks pages 
north holland amsterdam 
martinetz schulten 
topology representing networks 
neural networks 
moody darken 
fast learning networks locally tuned processing units 
neural computation 
omohundro 
delaunay triangulation function learning 
tr international computer science institute berkeley 
preparata shamos 
computational geometry 
springer new york 
ritter martinetz schulten 

addison wesley unchen 
bibliography rodrigues almeida 
improving learning speed topological maps patterns 
proceedings pages paris 
rumelhart hinton williams 
learning internal representations error propagation 
mcclelland editors parallel distributed processing volume pages 
mit press cambridge 
villmann der herrmann martinetz 
topology selforganizing feature maps exact definition measurement 
ieee tnn 
submitted 
walter ritter schulten 
non linear prediction self organizing maps 
international joint conference neural networks pages san diego 
willshaw von der malsburg 
patterned neural connections set self organization 
proceedings royal society london volume pages 
xu 
adding learning expectation learning procedure self organizing maps 
int 
journal neural systems 
