monday april constructive incremental learning local information stefan schaal cc gatech edu www cc gatech edu fac stefan schaal christopher atkeson cga cc gatech edu www cc gatech edu fac chris atkeson college computing georgia institute technology atlanta ga atr human information processing laboratories seika cho soraku gun kyoto introduce constructive incremental learning system regression problems models data means spatially localized linear models 
contrast approaches size shape receptive field locally linear model parameters locally linear model learned independently need competition kind communication 
independent learning accomplished incrementally minimizing weighted local cross validation error 
result obtain learning system allocate resources needed dealing bias variance dilemma principled way 
spatial localization linear models increases robustness negative interference 
learning system interpreted nonparametric adaptive bandwidth smoother mixture experts experts trained isolation learning system profits combining independent expert knowledge problem 
illustrates potential learning capabilities purely local learning offers interesting powerful approach learning receptive fields 
learning spatially localized basis functions popular paradigm machine learning neurobiological modeling 
context radial basis function networks moody darken poggio girosi demonstrated local learning offers alternative learning global basis functions sigmoidal neural networks theoretical foundation grounded approximation theory powell 
neurophysiological studies concept localized information processing form receptive fields known hubel wiesel 
wealth experimental evidence accumulated suggests information processing local receptive fields ubiquitous organizational principle neurobiology offers interesting computational opportunities zipser anderson lee rohrer sparks georgopoulos field olshausen field daugman downing 
explore computational power local receptive incremental learning goal approximating unknown functional relationships incoming streams input output data 
incremental learning just mean parameters learning system updated incrementally 
want address learning scenario limited memory available new data point incorporated learning system discarded re input output distributions data unknown distribution may change time 
situation resembles learning sensory sensorimotor transformations biology applies variety artificial domains ranging autonomous robotic systems process control 
constraints incremental learning major problems need addressed 
allocate appropriate number resources receptive fields order deal tradeoff overfitting called bias variance dilemma geman bienenstock doursat 
second problem incremental learning comes negative interference forgetting useful knowledge learning new data 
methods prevent negative interference require validation data sets memorizing training data strong prior knowledge learning problem 
alternatives available setting described want avoid storing data knowledge structure learning task 
order address problems incremental learning resort techniques nonparametric statistics scott hastie tibshirani 
nearest neighbor algorithms pattern recognition parzen windows density estimation best known methods field duda hart 
interesting note nonparametric methods essentially receptive field predictions data restricted local neighborhood query point 
size neighborhood irregular typically case nearest neighbor approaches symmetric smooth weighting function parzen windows 
receptive fields nonparametric regression built fly discarded right prediction paradigm termed lazy learning aha press 
necessarily nonparametric methods need store training data 
characteristic predictions usually single receptive field 
property inspired field nonparametric regression pursue complex models receptive field instance low order polynomials cleveland cleveland loader 
contrast neural network algorithms radial basis function systems focused combining activation strengths receptive fields optimize predictions 
demonstrate nonparametric regression approach build receptive field learning system incremental function approximation need store training data discarding receptive fields 
locally linear model fitted incrementally receptive field local function approximation accomplished spirit taylor series expansion 
new property learning approach receptive field trained independently receptive fields adjusting parameters locally linear model size shape receptive field bias relevance individual input dimensions 
new receptive fields allocated needed 
resulting algorithm receptive field weighted regression rfwr achieves robust incremental learning 
interesting relations previously suggested learning methods 
interpreted mixture experts system jacobs jordan nowlan hinton jordan jacobs experts trained isolation 
interpreted system set experts trained independently problem profits combining experts making predictions perrone cooper 
rfwr interpreted nonparametric memory learner atkeson moore schaal press stores data surprising 
section give motivation approach incremental learning 
section describes details nonparametric incremental learning system outlines statistical characteristics 
section discusses variety empirical evaluations 
section outlines related section concludes 
incremental learning statistical assumptions assumed statistical model problems standard regression model denotes dimensional vector input variables dimensional vector output variables deterministic vector valued function mapping input output additive random noise assumed independently distributed mean zero unknown distribution denotes expectation operator 
input data distributed density localizing interference interference learning natural side effect ability generalize interpolate extrapolate output unseen input previously learned data 
generalization accomplished allowing changes parameters learning system non local effects 
effects reduce correctness predictions larger extent improve interference called negative catastrophic 
incremental learning particularly endangered negative interference direct way balance amount positive interference generalization amount negative interference parameter update usually greedy concern reduction error current piece training data 
see statistical causes interference consider mean squared error criterion select model approximate true function yx equation states general approximation result depends conditional distribution input distribution data fan gijbels 
infinite amount training data asymptotically depend solely papoulis yx yx finite amount data stable model obtained distributions changes learning 
considerations point major causes negative interference 
changes functional relationship nonstationary parameters learning system may change 
analogously data learning sampled fixed input distribution parameters learning system may change 
particularly change input distribution happen incremental learning 
imagine robot learning inverse dynamics model arm model maps joint positions joint velocities joint accelerations corresponding joint torques 
robot moves receives valid data functional relationship 
robot fulfilling different tasks different times sampled data come quite different input distributions example consider difference movements cooking movements playing tennis 
interesting properties learning localized receptive fields lies potential robustness interference 
learning spatially localized training data location negligible effect parameters distant receptive fields interference spatially localized 
example gives illustration effect 
synthetic data set suggested fan gijbels trained layer sigmoidal feedforward neural network hidden units backpropagation momentum noisy data points uniformly distributed 
excellent function fit obtained shown predicted trace 
continued training network new data points drawn function changed input distribution network learned accommodate new data points doing significantly changed predictions previously learned data data largely separate new training data 
effect due non local nature sigmoidal basis functions prone lead catastrophic interference shown 
repeated experiment receptive field learning system rfwr generates locally linear models receptive field blends predictions 
original training data rfwr achieves comparable results sigmoidal neural network 
training new data interference apparent 
original fit left part graph visibly altered contrast neural network 
robustness negative interference accomplished localizing interference best interference eliminated finite data samples 
original training data new training data true predicted predicted new training data learned organization receptive fields local function fitting fields global function fitting sigmoidal neural network results function approximation function sin exp sigmoidal neural network results function approximation local receptive field algorithm fitting locally linear models receptive field note data traces true predicted predicted new training data largely coincide organization gaussian receptive fields training 
avoiding problem resource allocation due bias variance tradeoff geman learning algorithms include model selection phase order find appropriate compromise overfitting 
usually accomplished setting certain meta parameters instance number hidden units neural network model selection criterion cross validation stone 
question frequently asked model selection free parameters allocated order achieve tradeoff 
approach pursued fixed number free parameters data set spatially limited order achieve bias variance tradeoff remaining data adapting complexity learning system adapt complexity region data drawn 
general nonlinear function approximators unclear answer question 
spatially localized function fitting question translates extent receptive field changed order associated parametric model fit data appropriately 
approach transforms global bias variance problem local 
deriving optimal local bias variance tradeoff remains hard friedman fan gijbels local nature problem allows new ways find satisfying solutions 
possibility prescribing local bias 
intuition idea illustrated case locally linear models 
spirit taylor series expansion assume know adjust region validity size shape receptive field locally linear model approximation error center bias pre set value 
note bias exactly matches pre set value receptive field expanded vice versa 
receptive field deals bias variance region validity linear model receptive field activation linear models overlap linear models strong overlap xq predictions different linear models region validity linear model permitted approximation error function approximation piecewise linear models 
dilemma individually bias priori variance follows automatically 
order approximate entire nonlinear function cover input space sufficiently locally linear models 
importantly matter allocate locally linear models restrict extrapolation linear models bound corresponds minimal activation strength receptive field average outputs linear models query point approximation bound larger error illustrated 
required data point handled locally linear model 
due averaging overlapping linear models tend provide better function estimates spirit limitations ensemble methods perrone cooper 
described procedure capable avoiding resource allocation problem relying prior knowledge reasonable bound 
section demonstrate limits fi nite data bound realized just static flexible way depending curvature function receptive field 
summary discussion sections promising route robust incremental learning local receptive field system adjust extent receptive fields 
care taken goes accomplishing goal 
learning methods competitive learning usually achieve properties described previous section 
competitive learning size receptive field results global competition process local models account training data 
changing number local models causes change extent receptive fields number local models critical choice bias variance tradeoff exactly wish avoid 
section explain alternative approach nonparametric statistics offers route achieve goals resorting competitive learning 
receptive field weighted regression rfwr constructs system receptive fields incremental function approximation 
prediction query point built normalized weighted sum individual predictions receptive fields weights correspond activation strengths corresponding receptive fields 
determined size shape receptive field characterized kernel function 
variety possible kernels suggested atkeson press 
analytical convenience gaussian kernel exp parameterizes receptive field location input space positive definite distance metric determining size shape receptive field 
algorithmic reasons convenient generate upper triangular matrix order ensures positive definite 
receptive field simple parametric function models relationship input output data 
local polynomials low order widespread nonparametric statistics nadaraya watson wahba wold cleveland cleveland devlin 
focus locally linear models accomplish favorable compromise computational complexity quality result hastie loader denotes parameters locally linear model 
clarify elements parameters rfwr gives illustration single output system 
inputs routed receptive fields consists linear gaussian unit 
learning algorithm rfwr determines parameters receptive field independently information receptive fields contrast competitive learning 
rfwr adds prunes receptive fields needed number receptive fields automatically adjust learning problem hand 
dimensional example function fitting rfwr shown noted size receptive field adapted local curvature function certain amount overlap receptive fields center locations chosen respect explicit optimization criterion 
learning rfwr ingredients algorithm need discussed update linear model parameters distance metric add prune receptive fields 
centers changed allocated 
sake clarity drop subscript possible receptive field updated way 
learning linear model learning straightforward problem linear useful leave incremental learning framework moment think terms batch update 
summarize input part training data points rows matrix 
corresponding output part rows matrix 
corresponding weights diagonal matrix diag parameter vector calculated weighted regression wx wy px wy kind locally weighted regression extensive application nonparametric statistics cleveland cleveland loader time series prediction farmer regression learning problems atkeson moore schaal atkeson atkeson press 
result equation exactly calculated recursive squares sequential sweep training data ljung 
training point incremental update yields cv cv xe xx aaa aaa aaa aaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa inputs output linear unit weighted average 
gaussian unit centered network illustration receptive field weighted regression update employed rfwr 
useful note recursive squares corresponds newton training method guaranteed convergence global minimum case weighted squared error criterion atkeson press 
furthermore recursive update avoids explicit matrix inversion 
differing batch update equation equation includes forgetting factor 
changes distance metric learning see change weights reason necessary include order gradually cancel contributions previous data points learned properly ljung 
learning shape size receptive field adjusting shape size receptive field accomplished adjusting distance metric glance hope done gradient descent weighted mean squared error criterion basis solution locally weighted regression equation atkeson press 
unfortunately minimizing may result quite inappropriate solution 
training point receptive field centered right point corresponding chosen receptive field narrow activated data point corresponding linear model fit data point zero error 
function approximation result strongly tend overfitting 
property learning algorithms resort competitive learning fixed number local receptive fields global competitive process prevent receptive fields modeling just data point assuming data points receptive fields moody darken jordan jacobs 
allowing global competitive process takes away property local learner receptive fields spatially localized 
alternative way address overfitting effect leave cross validation 
cost function minimized changes equation notation denotes prediction th data point calculated training learning system th data point excluded training set 
inappropriate receptive field just focus training point error measure calculated data exist training set 
leave cross validation usually computationally expensive fold training learning system required data points training set 
furthermore example sigmoidal neural network unclear combine resultant different learned parameters single solution 
linear regression problems result rendering concerns irrelevant 
due sherman theorem kuh welsh equation rewritten px equation states leave cross validation error obtained fold training learning system adjustment weighted mean squared error help inverted covariance matrix cf 

equation corresponds weighted version press residual error standard linear regression techniques myers 
neglecting moment cost function minimized incrementally obtained criterion adjust schaal atkeson 
unfortunately point concern equation 
minimizing locally weighted leave cross validation error results consistent learning system increasing number training data receptive fields shrink small size 
advantage behavior function approximation asymptotically unbiased consistent disadvantage increasing number receptive fields required represent approximated function 
property avoided introducing penalty term ij px scalar determines strength penalty 
penalizing sum squared coefficients distance metric essentially penalizing second derivatives function site receptive field 
similar approaches taken spline fitting wahba acts low pass filter higher second derivatives smoothing bias introduced locally 
positive effect penalty term bias reduces variance function estimate problem usually associated local function fitting methods friedman 
section outline properties detail 
remains minimize incrementally adjusting gradient descent learning rate applying chain rule derivative written ij px storing data incremental learning cross validation obtain true gradient 
usual approach deriving stochastic gradient drop sums 
approximate gradient quite inaccurate term positive shrinking receptive field reduces weight data point contribution weighted error 
turns able derive better stochastic approximation 
training point associated weight derivative point approximated summing data points recalling stands sum weights cf 
equation equation verified result equation 
despite term possible obtain incremental version stochastic derivative introducing memory traces cf 
notation cv cv cv cv cv xe xx resulting incremental version derivative rl rl rl ij ij rl ij rl rj il ir jl kronecker operator cv cv deriving derivative possible due fact application sherman morrison woodbury theorem allows take derivatives inverted covariance matrix atkeson schaal sum form sv qv written qv operator denotes element wise multiplication homomorphic matrices vectors subsequent summation coefficients sq ij ij interesting note stochastic derivative just concerned reducing error current training point learning algorithms takes account previously encountered training data memory traces 
update rfwr greedy respect current training sample characteristic contribute favorably speed robustness incremental learning 
adding receptive fields automatic bias adjustment new receptive field created training sample activate existing receptive field threshold gen center new receptive field set manually chosen default value def parameters initialized zero matrix corresponds inverted covariance matrix weighted inputs treating constant input th input 
suitable initialization diagonal matrix diagonal elements set ii coefficients usually small quantities ljung 
summarize dimensional vector 
parameters interesting statistical interpretation introduce bias regression coefficients correspond common forms biased regression ridge regression 
probabilistic point view bayesian priors coefficients zero 
algorithmic perspective fake data points form 
atkeson press 
normal circumstances sizes coefficients small introduce noticeable bias 
ridge regression parameters important input data locally rank deficient matrix inversion close singular 
high dimensional input spaces quite common locally rank deficient input data 
rfwr explicitly require matrix inversions rank deficiency affects incremental update generating estimates large variances causing unreliable predictions 
non zero ridge regression parameters reduce variance cost introducing bias 
appropriate compromise including ridge parameters adjustable terms rfwr gradient descent cost update change added additionally necessary add back fraction lost due forgetting factor bias forgotten time 
computations performed surprisingly simple 
appendix details update stochastic approximation analogous derivation 
pruning receptive fields element rfwr pruning facility 
receptive field pruned overlaps receptive field 
effect detected training sample activating receptive fields simultaneously prune receptive field larger determinant distance metric pruned 
computational convenience det approximated sd ii deco 
noted pruning due overlap aims primarily computational efficiency discussed section overlap degrade approximation quality 
second cause pruning bias adjusted weighted mean squared error ij linear model unit excessively large comparison units bias adjustment term derived asymptotic behavior rfwr outlined section detailed schaal atkeson 
empirically usually ways adjust order minimize 
normally want avoid zero matrix 
indicates receptive field performs global regression locally weighted regression 
global linear regression nonlinear function large 
simple outlier detection test receptive fields suffices deal behavior 
receptive field reinitialized randomized values 
normally pruning takes place rarely happens due inappropriate initialization rfwr 
summary rfwr sum receptive field rfwr sets adjustable parameters locally linear model size shape receptive field bias 
linear model parameters updated newton method parameters updated gradient descent 
compact pseudocode overview rfwr shown 
initialize rfwr receptive field rf new training sample rf calculate activation update receptive field parameters subnet activated gen create new rf def rfs activated prune erase rf larger det calculate std rfs rf std reinitialize receptive field def scalar positive outlier removal threshold scalar random value 
choice ensures new distance metric result smaller receptive field converge solution 
second order gradient descent little extra computation possible replace gradient descent update second order gradient descent gain learning speed 
purpose adopted sutton incremental delta bar delta algorithm 
derivation algorithm remains demonstrated sutton standard squares criterion replaced cost function apply updating distance metric 
appendix provides details algorithm 
possible apply second order learning ridge regression update 
empirically find significant improvements doing incorporated second order updates distance metric rfwr 
asymptotic properties rfwr schaal atkeson derived asymptotic properties rfwr cost function 
just mention results directly relevant 
assuming number training data points goes infinity ii range receptive fields second order taylor series expansion fits training function sufficiently accurate iii variance noise locally constant iv input distribution locally uniform statements penalty term cost introduces non vanishing bias low pass filter higher second derivatives hessian function bias incurred 
estimated locally linear model asymptotically unbiased 
distance metric scaled approximation hessian 
appropriate penalty term learning problem computed estimate maximal eigenvalues hessian corresponds smoothness bias 
bias adjusted weighted mean squared error formulated order compare approximation quality receptive fields 
measure employed equation 
asymptotic results confirm penalty term cost function desired characteristics mentioned section section receptive fields shrink zero size controlled amount bias introduced 
interesting estimated locally linear model tends unbiased assumption errors taylor series negligible 
implies applications requiring gradient estimate function approximator expect reliable results 
calculation gradient estimate natural product lookup rfwr 
simulation results basic function approximation rfwr establish rfwr capable competing state art supervised learning techniques fixed training set 
sufficiently complex learning task illustrated nicely approximate function max sample points drawn uniformly unit square 
function consists narrow wide ridge perpendicular gaussian bump origin 
training data drawn uniformly training set replacement training time measured epochs multiples training samples 
test set consists data points corresponding vertices grid unit square corresponding output values exact function values 
approximation error measured normalized mean squared error nmse mse test set normalized variance outputs test set 
rfwr initial parameters set def identity matrix gen prune pruning generation thresholds minor importance just determining overlap receptive fields 
choice penalty term calculated tolerate maximal bias schaal atkeson 
default value distance metric determined manually initial receptive field covered significant portion input space 
ridge regression parameters play role example omitted 
qualitative evaluation confirms rfwr fulfills expectations 
initially large receptive fields adjust learning local curvature function narrow elongated region ridges remain large flat parts function 
number receptive fields increased training epoch final approximation result nmse 


target function approximated approximated function epochs training receptive fields input space epoch contour lines mark centers training data displayed small dots receptive fields epochs training 
compared learning results rfwr algorithms standard global linear regression sigmoidal layer backpropagation neural network baseline comparisons mixture experts algorithm state theart comparison jacobs jordan jacobs xu jordan hinton 
standard linear regression accomplish better result nmse example function linear trend chosen region input space 
sigmoidal network trained backpropagation momentum variety configurations units hidden layer output layer linear unit 
networks accomplish results better nmse training epochs 
doubling number training samples reducing noise level resulted nmse hidden unit net epochs 
cascade correlation algorithm fahlman lebiere fit original data point training set confirmed function difficult learning task sigmoidal networks cascade correlation converge confined sigmoidal hidden units achieve function fitting nmse allowed gaussian hidden units 
natural interesting comparison mixture experts system particularly suggested xu 

xu 
contrast softmax gating network jordan jacobs experts mixture gaussians gating network gating net locally linear models leaf gating net updated analytical version expectation em algorithm dempster laird rubin 
basic elements form rfwr locally linear models gaussian receptive fields training methods systems differ significantly competitive parametric likelihood maximization vs local nonparametric learning 
add resources performance determining parameters experts allocated system initialized 
algorithm tested experts 
initially experts uniformly distributed input space initial covariance matrix gaussians comparable initialization rfwr distance metric 
conducted similar test rfwr setting determining parameter penalty summarizes results 
learning curve average learning trials condition corresponding algorithm training data randomly generated trial 
algorithms achieve nmse training epoch typical signature fast recursive squares updating linear models employed algorithms sigmoidal neural network achieved epochs 
algorithms converge epochs 
adding experts mixture experts improves performance best average value nmse slight trend overfitting experts 
rfwr accomplishes consistently result nmse runs slight tendency overfitting standard deviation error bars indicated black bars learning curve 
surprising achieve ultimate fit accuracy rfwr 
behavior due relative small training set relatively low signal noise ratio training data way gating network assigns training samples expert 
significantly increasing amount training data lowering noise results algorithms indistinguishable 
method credit assignment significant difference 
expectation step uses normalized weights posterior probabilities assign training data experts 
normalized weights create sharper decision boundaries experts unnormalized weights rfwr 
case noise training data algorithm tends establish sharp decision boundaries experts starts fitting noise 
underlying assumption world generate mixture linear models behavior may expected 
test cases world continuous function mixture linear models assumptions approximation explains algorithm perform entirely appropriately 
assumptions rfwr quite different receptive field tries find region validity allows approximate tangent plane region remaining bias 
spirit low order taylor series expansion reasonable way proceed 
rfwr achieves consistent results low variance 
interesting see number receptive fields rfwr grows function penalty factor 
expected derivation cost function small penalty paa nmse training epochs ex ex ex ex nmse receptive fields training epochs average learning curves solid lines rfwr 
black bars indicate standard deviation error bars learning overlapping traces having approximately standard deviation bar shown 
rfwr increase number receptive fields time dashed lines indicated 
rameter causes receptive fields keep shrinking entails continuous growth number receptive fields 
tendency overfitting remained low seen traces 
continuing learning epochs nmse saturated close current values penalty factors 
local cross validation term responsible desirable behavior cross validation overfitting significantly pronounced nmse continued increasing small penalty factors 
dealing irrelevant inputs order establish usefulness ridge regression parameters conducted comparison mixture experts 
sensorimotor control variables learning system equally relevant task 
possible kinds extraneous inputs include constant inputs changing inputs meaningless copies linear combinations inputs 
ideally autonomous learning system robust signals 
explore behavior rfwr cases additional inputs added function constant input input brownian walk interval input copy added gaussian noise 
training data generated uniformly function reduced additive noise improve signal noise ratio 
tests ridge regression coefficients initialized input 
summarizes average results trials algorithm 
show mean nmse standard deviation test sets 
test predictions generated regression coefficients relevant inputs point test set experiment section 
establish coefficients adjusted correctly model target function 
algorithms achieved learning results test 
test probed robustness learned model irrelevant inputs added noisy constant brownian noisy copy input test set added offset signals 
algorithm learned inputs irrelevant change matter 
irrelevant inputs mistakenly employed signal improve nmse training data predictions deteriorate 
demonstrates results rfwr remained virtually unaltered test significantly worse 
outcome explained looking standard deviations regression coefficients locally linear models 
contrast rfwr set regression coefficients irrelevant inputs close zero achieving desired robustness 
behavior due ad corresponding ridge regression parameters increased irrelevant inputs decreased zero relevant inputs 
point designed deal learning problems irrelevant inputs ways improve performance cases 
experiment clearly illustrates necessary deal problem irrelevant inputs local bias adjustment means ridge regression possible way 
shifting input distributions mentioned easy conceive learning tasks input distribution training data changes time 
test rfwr performance problems designed experiment 
sequential episodes training data learning uniformly drawn slightly overlapping input regions unit cube andt algorithm trained points tested trained points tested trained points tested test data regions 
gives example learning proceeded 
test probes previously learned competence forgotten input distribution shifts 
parameters rfwr chosen def set slightly larger value def algorithm constructive suited learning strongly shifting input distributions chose resource allocating network ran platt comparison learning algorithm constructive competitive learning component mean standard deviation regression coefficients rfwr test test nmse rfwr average nmse rfwr training epochs see text explanations mean standard deviation regression coefficients irrelevant inputs 
variety algorithms 
ran radial basis function rbf network adds rbfs site training sample criteria approximation training sample error large rbf activated training sample threshold value 
criteria fulfilled simultaneously create new rbf 
spherical width rbf chosen distance nearest neighboring rbf 
gradient descent momentum rbf centers adjusted reduce mean squared approximation error weights linear regression network second layer rbf net 
strategy ran start initially wide rbfs increase threshold time upper limit reached causing creation smaller rbfs sites large error 
rfwr gaussians equation parametric structure rbf 
summarizes average learning trials algorithm 
rfwr shows large robustness shift input distribution minor increase nmse due interference overlapping parts training data 
contrast seen original ran trace ran significantly increases nmse second third training episode 
ran starts initial rbfs cover entire input space interference properly localized explains observed behavior 
note excluded constant term linear regression layer ran platt term globally active decrease performance significantly 
experience rfwr possible improvements ran come mind 
starting large rbfs initially limit maximal initial size rfwr def second employ hyper radial basis function technique poggio girosi adjust shape rfwr reconstructed function training rbfs gradient descent rfwr 
third having time varying threshold global variable define individual variable rbf removing explicit dependency global training time 
initializing ran def rfwr modification resulted significant improvement robustness ran shown 
note version ran requires half rbfs converges quickly achieves low final approximation errors 
rfwr localizing learning parameters leads clear improvement robustness incremental learning 
sensorimotor learning evaluation traditional example sensorimotor learning approximation inverse dynamics joint arm atkeson 
configuration arm joint angles 
inverse dynamics model map joint angles joint velocities joint accelerations corresponding torques assume arm controller low gain feedback pid controller performance enhanced feedforward commands learned inverse dynamics atkeson hollerbach 
torques shoulder elbow joint learned separate networks reason believe receptive field elbow torque shape shoulder torque rfwr mean outputs hessian definitely case 
task goal draw parts space 
shows desired initial performance learned commands 
training proceeded steps arm performed sinusoidal movements varying frequency content area upper 
total training points sampled hz nmse rbfs training iterations original ran modified ran nmse receptive fields training iterations rfwr average learning curves solid lines average number receptive field radial basis functions dashed lines ran rfwr 
black bars give standard deviation learning 
training training sample sequential order generated 
learning results shown top part rfwr modified ran 
algorithms able track properly 
algorithms trained analogous fashion samples lower 
bottom parts show corresponding learning results 
returning performing upper ran showed significant interference dashed line algorithms initiated def 
note position velocity acceleration inputs normalized prior learning compensate differences units 
interference effect highlights difference learning strategy rbf networks comparison nonparametric statistics approach modeling locally linear models 
rbf networks need sufficient overlap radial basis functions achieve learning results rbf limited function approximation capabilities effect discussed context hyperacuity churchland sejnowski 
gradient descent shape parameter gaussian rbfs quickly decreased example achieve appropriately large overlap 
overlap encourages negative interference evident 
dimensional input space example emphasized need large overlap dimensional example previous section 
experiments fixed original ran algorithm achieve better learning results reasonable training time 
avoid interference unattractive solution adding thousands quite gravity desired learning desired learning interference test rfwr ran learning initial performance joint arm drawing feedforward control signals performance rfwr learning performance ran learning 
narrow overlapping rbfs 
results algorithms allocated receptive fields 
related field contributes development rfwr nonparametric statistics 
cleveland introduced idea employing locally linear models memory function approximation called locally weighted regression lwr 
series subsequent papers colleagues extended statistical framework lwr include multi dimensional function approximation local approximation techniques higher order polynomials cleveland devlin gross devlin 
cleveland loader suggested local tests local press choosing degree local mixing different order polynomials local bandwidth adjustment reviewed large body literature history lwr 
hastie tibshirani give related overviews nonparametric regression methods 
hastie loader discuss usefulness local polynomial regression show locally linear locally quadratic function fitting appealing properties terms bias variance trade 
friedman proposed variable bandwidth smoother dimensional regression problems 
different statistical techniques fan gijbels suggested adaptive bandwidth smoothers lwr provided detailed analyses asymptotic properties algorithms 
purpose time series prediction lwr farmer 
atkeson introduced lwr framework supervised learning robot control 
moore employed lwr learning control learning forward models 
context learning complex manipulation tasks robot schaal atkeson demonstrated lwr extended allow local bandwidth adaptation employing local cross validation local confidence criteria 
schaal atkeson introduced non memory version lwr 
schaal press applied rfwr value function approximation reinforcement learning 
locally weighted learning classification problems lowe 
aha press compiled series papers nonparametric local classification regression learning atkeson moore schaal press give extended survey locally weighted learning locally weighted learning applied control 
nonparametric statistics rfwr related constructive learning algorithms local function approximation radial basis functions rbf kohonen self organizing maps som 
rbf function approximator locally linear model rbf suggested reinforcement learning 
platt suggested constructive rbf learning system 

extended platt method poggio girosi hyper radial basis functions local principal component analysis 
learning control cannon derived constructive radial basis function network wavelet rbfs adapt spatial frequency similar local bandwidth adaptation nonparametric statistics adjustable receptive fields rfwr 
orr discussed recursive squares methods ridge regression learning radial basis function networks 
suggests methods generalized cross validation regularizing ill conditioned regression 
established constructive learning systems cascade correlation fahlman lebiere system sharing ideas projection pursuit regression friedman 
related line research algorithm frean som cascading system littman ritter 
usage locally linear models regression problems context soms ritter schulten extended kohonen maps fit locally linear models llm units som 
related groen algorithm extended llm hierarchical approximation kohonen unit contain llm network 
fritzke demonstrated soms constructively add units context rbf llm regression problems 
sommer combined fritzke ideas martinetz schulten neural gas algorithm accomplish flexible topographic representation original som 
large body literature constructive learning stems fitting high order global polynomials data instance sanger sanger sutton matheus shin ghosh 
due global character learning methods danger negative interference quite large 
additional constructive learning regression survey kwok yeung 
idea mixture experts jacobs 
hierarchical mixtures experts jordan jacobs related rfwr mixture experts approach looks similar partitions input space particularly version xu 

tresp suggested methods improve generalization mixture models fit em algorithm dempster introducing bayesian priors 
closely related hierarchical mixture experts nonparametric decision tree techniques seminal breiman friedman olshen stone introduced classification regression trees cart friedman proposed mars algorithm cart derivative particularly targeted smooth function approximation regression problems 
adaptive receptive fields way receptive fields created rfwr resemble part classification algorithms reilly cooper carpenter grossberg 
discussion emphasizes major points 
truly local learning learning competition gating nets global regression top local receptive fields feasible approach learning compete state art learning systems 
second truly incremental learning learning knowledge input conditional distributions learning cope continuously incoming data partially redundant partially irrelevant inputs needs variety mechanisms sure incremental learning robust 
carefully designed local learning system accomplish robustness 
rfwr borrowed particular nonparametric statistics 
definition jek term nonparametric indicates function modeled potentially consists large families distributions indexed finite dimensional parameter vector natural way 
view summarizes basic assumptions learning system addition prior knowledge smoothness incorporated penalty term 
stressed prior knowledge available particular problem incorporated learning system 
nonparametric learner outperforms problem tailored parametric learning fitting sinusoidal data sinusoid best 
examples highlight local nonparametric learning advantageous claim generally superior learning systems 
hand comes learning having strong prior knowledge problem nonparametric methods quite beneficial 
instance quartz sejnowski submitted claim constructive nonparametric learning key issues understanding development organization brains 
rfwr new algorithmic features 
introduced stochastic approximation leave local cross validation cross validation need validation set anymore 
technique potentially useful domains requires local parameters estimated linear inputs 
employing novel penalized local cross validation criterion able derive locally adaptive multidimensional distance metrics 
distance metrics interpreted local approximations hessians function modeled 
order speed learning distance metric derived second order gradient descent method 
penalized local cross validation criterion employed achieve automatic local bias adjustment relevance input di obtained local ridge regression 
features constructive process rfwr needs monitor activation strength receptive fields order decide create new receptive field constructive learning system need monitor approximation error criterion easily lead unfavorable bias variance tradeoff 
issues addressed left research 
rfwr gradient learning requires proper choice learning rates 
incorporated second order learning derived sutton necessary experimentation choice learning rates order achieve close optimal learning speed entering unstable domains 
necessary choose appropriate initial distance metric cf 
equation characterizing initial size receptive field 
large initial receptive field danger receptive field grows span entire input domain initial receptive field structure data mistaken high variance noise 
positive side effect local learning open parameters explored allowing just small number receptive fields initial data set monitoring learning behavior receptive field learns independently need parameter exploration large number receptive fields 
algorithmic point concerns computational complexity 
recursive squares process quadratic number inputs update full distance metric worse 
dimensionality inputs goes learning task receptive fields run fairly slowly serial computer 
fitting diagonal distance metrics alleviates effect necessary anyway number open parameters learning system large compared number training data points 
discussion naturally leads long standing question local learning methods deal high dimensional input spaces 
nicely described scott curse dimensionality adverse effects systems neighboring points euclidean sense concept neighborhood gradually counterintuitive growing input dimensions pretty vanishes dimensions point distance point 
domains parametric model chosen learning local global key success essentially meaning learning system requires strong bias high dimensional worlds 
remains unclear high dimensional input spaces locally high dimensional distributions 
experience sensorimotor learning may true interesting problems physical systems realize arbitrary distributions 
instance degree freedom anthropomorphic robot arm verse dynamics model requires learning dimensional input space realize locally dimensional input distributions 
research goal incorporate local dimensionality reduction preprocessing step receptive field vijayakumar schaal submitted 
point wonder far local learning system rfwr parallels neurobiological information processing 
particularly inspired visual cortex mainstream assumptions receptive field learning brain receptive fields broadly tuned widely overlapping size receptive fields free parameter normal learning opposed developmental processes lesions nelson sur 
view emphasizes accuracy encoding achieved subsequent postprocessing steps 
contrast rfwr suggest overlapping finely tuned receptive fields accuracy achieved directly overlapping units 
fine tuning achieved change size receptive field plug approaches receptive fields tuned different spatial frequencies contribute learning cannon 
distinguish principles experiments test interference generalization learning provide valuable insights macroscopic organization learning 
motor control uno kawato mussa ivaldi examples investigations 
learning principles rfwr biologically relevant remains speculative 
demonstrated alternative powerful methods accomplish incremental constructive learning local receptive fields interesting look cases learning systems applied 
receptive field local learning interesting research topic neural computation truly local learning methods just starting demonstrate potential 
acknowledgments vijayakumar helpful comments contributed significantly improve manuscript 
research partly funded atr human information processing research laboratories 
additional support schaal provided german research association german scholarship foundation alexander von humboldt foundation 
support atkeson provided air force office scientific research national science foundation presidential young investigator award 
appendix ridge regression derivatives ridge regression parameter conceived weighted data point form 
incorporated regression recursive squares update 
derivative cost function simplified version derivative advantage zero elements ridge data points actual computation derivative greatly sped 
ways incorporate update ridge regression parameters matrix noted need add back fraction ridge parameters forgotten due forgetting factor update equation 
turns quite efficient way perform update 
update receptive field forgetting factor effectively reduces contribution ridge parameter update due gradient descent grad total increment grad due fact ridge vectors unit vectors possible update just executing recursive squares update increment add ridge data point form 
ridge parameter equation 
update accelerated account zeros ridge points 
additional speed obtained updating iteration accumulating increments exceed manually chosen threshold 
second order learning distance metric idea incremental delta bar delta algorithm sutton replace learning rate gradient descent update individual learning rate coefficient ofm form ij ij ij ij ij ij ij ij ij ij exp learning rates ij changed geometric steps gradient descent meta parameter ij meta learning rate 
term ij updated ij ij ij ij ij ij define ij initialized zero receptive field created 
corresponds memory term stores decaying trace cumulative sum changes ij details see sutton 
order apply second order update necessary store parameters ij ij ij compute second derivative 
second derivative cost function respect coefficients distance metric rl ww rl ll ij rl cv cv xx cv cv equation notation results derived 
aha 
press 
lazy learning 
artificial intelligence review 
atkeson hollerbach 

model control robot manipulator 
cambridge ma mit press 
atkeson moore schaal 
press 
locally weighted learning 
artificial intelligence review 
atkeson moore schaal 
press 
locally weighted learning control 
artificial intelligence review 
atkeson 

learning arm kinematics dynamics 
annual review neuroscience pp 
atkeson 

local models control movement 
touretzky 
ed advances neural information processing systems pp san mateo ca morgan kaufmann 
atkeson schaal 

memory neural networks robot learning 
neurocomputing pp 
kuh 

regression diagnostics identifying influential data sources collinearity 
new york wiley 
breiman friedman olshen stone 

classification regression trees 
belmont ca wadsworth international group 
sommer 

dynamic cell structure learns perfectly topology preserving map 
neural computation pp 
cannon 

space frequency localized basis function networks nonlinear system estimation control 
neurocomputing pp 
carpenter grossberg 

massively parallel architecture self organizing neural pattern recognition machine 
computer vision graphics image processing pp 
churchland sejnowski 

computational brain 
boston ma mit press 
cleveland 

robust locally weighted regression smoothing scatterplots 
journal american statistical association pp 
cleveland devlin grosse 

regression local fitting methods properties computational algorithms 
journal econometrics pp 
cleveland devlin 

locally weighted regression approach regression analysis local fitting 
journal american statistical association pp 
cleveland loader 

smoothing local regression principles methods 
technical report bell laboratories murray hill ny 
daugman downing 

gabor wavelets statistical pattern recognition 
arbib 
ed handbook brain theory neural networks pp 
cambridge ma mit press 
de boor 

practical guide splines 
new york springer 
deco 

information theoretic approach neural computation 
new york springer 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society pp 
duda hart 

pattern classification scene analysis new york wiley 
fan gijbels 

variable bandwidth local linear regression smoothers 
annals statistics pp 
fan gijbels 

data driven bandwidth selection local polynomial fitting variable bandwidth spatial adaptation 
journal royal statistical society pp 
fan gijbels 

local modelling applications 
london chapman hall 
farmer 
predicting chaotic time series 
phys 
rev lett pp 
farmer 
exploiting chaos predict reduce noise 
lee 
ed evolution learning cognition 
singapore world scientific 
field 

goal sensory coding 
neural computation pp 
frean 

algorithm method constructing training feedforward neural networks 
neural computation pp 
friedman 

projection pursuit regression 
journal american statistical association theory models pp 
friedman 

variable span smoother 
technical report department statistics stanford university 
friedman 

multivariate adaptive regression splines 
annals statistics pp 
fritzke 

growing cell structures self organizing network unsupervised supervised learning 
neural networks pp 
fritzke 

incremental learning locally linear mappings 
proceedings international conference artificial neural networks paris france oct 


combining local pca radial basis function networks speaker normalization 
girosi makhoul wilson 
eds proceedings ieee workshop neural networks signal processing pp 
new york ieee 


connectionist speaker normalization generalized resource allocating networks 
tesauro touretzky leen 
eds advances neural information processing systems pp 
cambridge ma mit press 
geman bienenstock doursat 

neural networks bias variance dilemma 
neural computation pp 
georgopoulos 

higher order motor control 
annual review neuroscience pp 
jek 

course nonparametric statistics 
san francisco ca holden day 
hastie tibshirani 

generalized additive models 
london chapman hall 
hastie loader 

local regression automatic kernel carpentry 
statistical science pp 
hastie tibshirani 

nonparametric regression classification part nonparametric regression 
cherkassky friedman wechsler 
eds statistics neural networks theory pattern recognition applications 
asi proceedings subseries computer systems sciences 
springer 
hubel wiesel 

receptive fields single neurons striate cortex 
journal neurophysiology 
uno kawato 

internal representations motor apparatus implications generalization visuomotor learning 
journal experimental psychology pp 
jacobs jordan nowlan hinton 

adaptive mixtures local experts 
neural computation pp 
jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation pp 
jutten 

new scheme incremental learning 
neural processing letters pp 
kwok yeung 

constructive feedforward neural networks regression problems survey 
technical report cs department computer science hong kong university science technology clear water bay kowloon hong kong 
lee rohrer sparks 

population coding saccadic eye movement neurons superior colliculus 
nature pp 
ritter 

generalization abilities cascade network architectures 
hanson cowan giles eds advances neural information processing systems pp 
morgan kaufmann 
ljung 

theory practice recursive identification 
cambridge mit press 
lowe 

similarity metric learning variable kernel classifier 
neural computation martinetz schulten 

topology representing networks 
neural networks pp 
nelson sur 

topographic reorganization somatosensory cortical areas adult monkeys restricted 
neuroscience pp 


associative reinforcement learning optimal control 
master thesis csdl massachusetts institute technology cambridge ma 
moody darken 

learning localized receptive fields 
touretzky hinton sejnowski 
eds proceedings connectionist summer school pp 
san mateo ca morgan kaufmann 
moore 

fast robust adaptive control learning forward models 
moody hanson lippmann 
eds advances neural information processing systems 
san mateo ca morgan kaufmann 


modality topographic properties single neurons somatic sensory cortex 
journal neurophysiology pp 
myers 

classical modern regression applications 
boston ma pws kent 
nadaraya 

estimating regression 
theor 
prob 
appl pp 
olshausen field 

emergence simple cell receptive field properties learning sparse code natural images 
nature pp 
ormoneit tresp 

improved gaussian mixture density estimates bayesian penalty terms network averaging 
technical report theoretical computer science foundations artificial intelligence technische universit nchen munich 
orr 

regularization selection radial basis function centers 
neural computation pp 
papoulis 

probability random variables stochastic processes 
new york mcgraw hill 
perrone cooper 

networks disagree ensemble methods hybrid neural networks 

ed neural networks speech image processing 
chapman hall 
platt 

resource allocating network function interpolation 
neural computation pp 
poggio girosi 

regularization algorithms learning equivalent multilayer networks 
science pp 
powell 

radial basis functions multivariate interpolation review 
mason cox 

eds algorithms approximation pp 
oxford clarendon press 
quartz sejnowski 
submitted 
neural basis cognitive development constructivist manifesto 
journal brain behavioral sciences 
reilly cooper 

neural model category learning 
biological cybernetics pp 
ritter schulten 

topology conserving mappings learning motor tasks 
denker 
ed neural networks computing pp 
aip conference proceedings snowbird utah 
sanger 

tree structured adaptive network function approximation highdimensional spaces 
ieee transactions neural networks pp 
sanger sutton matheus 

iterative construction sparse polynomial approximations 
hanson moody lippmann 
eds advances neural information processing systems pp 
san mateo ca morgan kaufmann 
schaal 
press 
learning demonstration 
advances neural information processing systems 
schaal atkeson 

robot juggling implementation memory learning 
control systems magazine pp 
schaal atkeson 

assessing quality learned local models 
cowan tesauro alspector 
eds advances neural information processing systems 
san mateo ca morgan kaufmann 
schaal atkeson 

isolation cooperation alternative system experts 
touretzky mozer hasselmo 
eds advances neural information processing systems pp 
cambridge ma mit press 
schaal atkeson 

receptive field weighted regression 
technical report atr human information processing laboratories seika cho soraku gun kyoto japan 
scott 

multivariate density estimation 
new york wiley 
mussa ivaldi 

interference learning internal models inverse dynamics humans 
tesauro touretzky leen 
eds advances neural information processing systems 
mussa ivaldi 

adaptive representation dynamics learning motor task 
journal neuroscience pp 
shin ghosh 

ridge polynomial networks 
ieee transactions neural networks 
pp 
stone 
cross choice assessment statistical predictors 
journal royal statistical society 
sutton 

gain adaptation beats squares 
proceedings seventh yale workshop adaptive learning systems pp new haven ct sutton 

adapting bias gradient descent incremental version delta 
proceedings tenth national conference artificial intelligence pp cambridge ma mit press 
van der groen 

approximation neural networks local global approximation 
proceedings international conference neural networks ii pp perth 
vijayakumar schaal 
submitted 
local dimensionality reduction locally weighted learning 
wahba wold 

completely automatic french curve fitting spline functions cross validation 
communications statistics wahba 

spline models observational data 
philadelphia pa society industrial applied mathematics 
watson 

smooth regression analysis 
indian journal statistics pp 
xu jordan hinton 

alternative model mixture experts 
tesauro touretzky leen 
eds pp 
cambridge ma mit press 
zipser anderson 
back propagation programmed network simulates response properties subset posterior parietal neurons 
nature pp 

