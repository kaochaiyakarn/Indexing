evolving artificial neural networks xin yao school computer science university birmingham birmingham tt united kingdom email xin cs bham ac uk learning evolution fundamental forms adaptation 
great interest combining learning evolution artificial neural networks anns years 
reviews different combinations anns evolutionary algorithms eas including eas evolve ann connection weights architectures learning rules input features discusses different search operators various eas points possible research directions 
shown considerably large literature review combinations anns eas lead significantly better intelligent systems relying anns eas 
keywords evolutionary computation intelligent systems neural networks 
evolutionary artificial neural networks eanns refer special class artificial neural networks anns evolution fundamental form adaptation addition learning :10.1.1.13.957
eas perform various tasks connection weight training architecture design learning rule adaptation input feature selection connection weight initialization rule extraction anns distinct feature eanns adaptability dynamic environment 
words eanns adapt environment changes environment 
forms adaptation evolution learning eanns adaptation dynamic environment effective efficient 
broader sense eanns regarded general framework adaptive systems systems change architectures learning rules appropriately human intervention 
concerned exploring possible benefits arising combinations anns eas 
emphasis placed design intelligent systems anns eas 
combinations anns eas combinatorial optimization mentioned discussed detail 
published yao evolving artificial neural networks proceedings ieee september 
artificial neural networks architectures ann consists set processing elements known neurons nodes interconnected 
described directed graph node performs transfer function form ij gamma output node jth input node ij connection weight nodes threshold bias node 
usually nonlinear heaviside sigmoid gaussian function 
anns divided feedforward recurrent classes connectivity 
ann feedforward exists method numbers nodes network connection node large number node smaller number 
connections nodes small numbers nodes larger numbers 
ann recurrent numbering method exist 
eq 
term summation involves input high order anns contain high order nodes nodes input involved terms summation 
example second order node described ijk gamma symbols similar definitions eq 

architecture ann determined topological structure connectivity transfer function node network 
learning artificial neural networks learning anns typically accomplished examples 
called training anns learning achieved adjusting connection weights anns iteratively trained learned anns perform certain tasks 
learning anns roughly divided supervised unsupervised reinforcement learning 
supervised learning direct comparison actual output ann desired correct output known target output 
formulated minimization error function total mean square error actual output desired output summed available data 
gradient descent optimization algorithm backpropagation adjust connection weights ann iteratively order minimize error 
reinforcement learning special case supervised learning exact desired output unknown 
information actual output correct 
unsupervised learning solely correlations input data 
information correct output available learning 
essence learning algorithm learning rule weight updating rule determines connection weights changed 
examples popular learning rules include delta rule hebbian rule anti hebbian rule competitive learning rule 
detailed discussion anns 
thresholds biases viewed connection weights fixed input gamma 
evolutionary algorithms eas refer class population stochastic search algorithms developed ideas principles natural evolution 
include evolution strategies es evolutionary programming ep genetic algorithms gas 
important feature algorithms population search strategy 
individuals population compete exchange information order perform certain tasks 
general framework eas described 
generate initial population random set 
repeat evaluate individual population select parents fitness apply search operators parents produce offspring form 
termination criterion satisfied general framework evolutionary algorithms 
eas particularly useful dealing large complex problems generate local optima 
trapped local minima traditional gradient search algorithms 
depend gradient information quite suitable problems information unavailable costly obtain estimate 
deal problems explicit exact objective function available 
features robust search algorithms 
fogel back give various evolutionary algorithms optimization 
evolution evolutionary artificial neural networks evolution introduced anns roughly different levels connection weights architectures learning rules 
evolution connection weights introduces adaptive global approach training especially reinforcement learning recurrent network learning paradigm gradient training algorithms experience great difficulties 
evolution architectures enables anns adapt topologies different tasks human intervention provides approach automatic ann design ann connection weights structures evolved 
evolution learning rules regarded process learning learn anns adaptation learning rules achieved evolution 
regarded adaptive process automatic discovery novel learning rules 
organization article remainder organized follows 
section discusses evolution connection weights 
aim find near optimal set connection weights globally ann fixed architecture eas 
various methods encoding connection weights different search operators eas discussed 
comparisons evolutionary approach conventional training algorithms backpropagation 
general single algorithm winner kinds networks 
best training algorithm problem dependent 
section devoted evolution architectures finding near optimal ann architecture tasks hand 
known architecture ann determines information processing capability ann 
architecture design important tasks ann research application 
important issues evolution architectures representation search operators eas addressed section 
shown evolutionary algorithms relying crossover operators perform searching near optimal ann architecture 
reasons empirical results section explain case 
imagining ann connection weights architectures hardware easier understand importance evolution ann software learning rules 
section addresses evolution learning rules anns examines relationship learning evolution learning guides evolution learning evolves 
demonstrated ann learning ability improved evolution 
research topic early stages studies doubt benefit research anns machine learning 
section summarizes forms combinations anns eas 
intend exhaustive simply indicative 
demonstrate breadth possible combinations anns eas 
section describes general framework eanns terms adaptive systems interactions levels evolution considered 
framework provides common basis comparing different eann models 
section gives brief summary concludes remarks 
evolution connection weights weight training anns usually formulated minimization error function mean square error target actual outputs averaged examples iteratively adjusting connection weights 
training algorithms backpropagation bp conjugate gradient algorithms gradient descent 
successful applications bp various areas bp drawbacks due gradient descent 
gets trapped local minimum error function incapable finding global minimum error function multimodal nondifferentiable 
detailed review bp learning algorithms 
way overcome gradient descent training algorithms shortcomings adopt eanns formulate training process evolution connection weights environment determined architecture learning task 
eas effectively evolution find near optimal set connection weights globally computing gradient information 
fitness ann defined different needs 
important factors appear fitness error function error target actual outputs complexity ann 
case gradient descent training algorithms fitness error function differentiable continuous eas depend gradient information 
eas treat large complex nondifferentiable multimodal spaces typical case real world considerable research application conducted evolution connection weights :10.1.1.56.8747
evolutionary approach weight training anns consists major phases 
phase decide representation connection weights form binary strings 
second evolutionary process simulated ea search operators crossover mutation decided conjunction representation scheme 
different representations search operators lead quite different training performance 
typical cycle evolution connection weights shown 
evolution stops fitness greater predefined value training error smaller certain value population converged 

decode individual genotype current generation set connection weights construct corresponding ann weights 

evaluate ann computing total mean square error actual target outputs 
error functions 
fitness individual determined error 
higher error lower fitness 
optimal mapping error fitness problem dependent 
regularization term may included fitness function penalize large weights 

select parents reproduction fitness 

apply search operators crossover mutation parents generate offspring form generation 
typical cycle evolution connection weights 
binary representation canonical ga binary strings encode alternative solutions termed chromosomes 
early evolving ann connection weights followed approach 
representation scheme connection weight represented number bits certain length 
ann encoded concatenation connection weights network chromosome 
heuristic concerning order concatenation put connection weights hidden output node 
hidden nodes anns essence feature extractors detectors 
separating inputs hidden node far apart binary representation increase difficulty constructing useful feature detectors destroyed crossover operators 
generally difficult apply crossover operators evolving connection weights tend destroy feature detectors evolutionary process 
gives example binary representation ann architecture prede fined 
connection weight ann represented bits ann represented bits weight indicates connection nodes 
node node gamma gamma gamma gamma gamma gamma delta delta delta delta delta delta ak ann connection weights shown binary representation weights assuming weight represented bits 
advantages binary representation lie simplicity generality 
straightforward apply classical crossover point uniform crossover mutation binary strings 
little need design complex tailored search operators 
binary representation facilitates digital hardware implementation anns weights represented terms bits hardware limited precision 
encoding methods uniform gray exponential binary representation 
encode real values different ranges precisions number bits 
trade representation precision length chromosome 
bits represent connection weight training fail combinations real valued connection weights approximated sufficient accuracy discrete values 
hand bits chromosomes representing large anns extremely long evolution turn inefficient 
problems faced evolutionary training anns permutation problem known competing convention problem 
caused mapping representation genotype actual ann phenotype anns order hidden nodes differently chromosomes equivalent functionally 
example anns shown equivalent functionally different chromosomes shown 
general permutation hidden nodes produce functionally equivalent anns different chromosome representations 
permutation problem crossover operator inefficient ineffective producing offspring 
real number representation debates cardinality genotype alphabet 
argued minimal cardinality binary representation best 
formal analysis nonstandard representations operators concept equivalent classes representations ary strings solid theoretical foundation 
real numbers proposed represent connection weights directly real number node node gamma gamma gamma gamma gamma gamma delta delta delta delta delta delta ak ann equivalent binary representation representation scheme 
connection weight 
example representation ann 
connection weights represented real numbers individual evolving population real vector 
traditional binary crossover mutation longer directly 
special search operators designed 
montana davis defined large number tailored genetic operators incorporated heuristics training anns 
idea retain useful feature detectors formed hidden nodes evolution 
results showed evolutionary training approach faster bp problems considered 
bartlett downs demonstrated evolutionary approach faster better scalability bp 
natural way evolve real vectors ep es particularly wellsuited treating continuous optimization 
gas primary search operator ep es mutation 
major advantages mutation eas reduce negative impact permutation problem 
evolutionary process efficient 
number successful examples applying ep es evolution ann connection weights 
examples primary search operator gaussian mutation 
mutation operators cauchy mutation 
ep es allow self adaptation strategy parameters 
evolving connection weights ep implemented follows 
generate initial population individuals random set 
individual pair real valued vectors delta delta delta connection weight vectors variance vectors gaussian mutations known strategy parameters self adaptive eas 
individual corresponds ann 

individual delta delta delta creates single offspring delta delta delta exp denote th component vectors respectively 
denotes normally distributed dimensional random number mean variance 
indicates random number generated anew value parameters commonly set gamma gamma 
eq 
may replaced cauchy mutation faster evolution 

determine fitness individual including parents offspring training error 
different error functions may 

conduct pairwise comparison union parents offspring delta delta delta individual opponents chosen uniformly random parents offspring 
comparison individual fitness smaller opponent receives win 
select individuals delta delta delta wins form generation 
tournament selection scheme may replaced selection schemes 

halting criterion satisfied go step 
comparison evolutionary training gradient training indicated section evolutionary training approach attractive handle global search problem better vast complex multimodal nondifferentiable surface 
depend gradient information error fitness function particularly appealing information unavailable costly obtain estimate 
example evolutionary approach train recurrent anns higher order anns fuzzy anns 
evolutionary algorithm train different networks regardless feedforward recurrent higher order anns 
general applicability evolutionary approach saves lot human efforts developing different training algorithms different types ann 
evolutionary approach easier generate anns special characteristics 
example ann complexity decreased generalisation increased including complexity regularization term fitness function 
case training term need differentiable continuous 
weight sharing weight decay incorporated fitness function easily 
evolutionary training slow problems comparison fast variants bp conjugate gradient algorithms 
eas generally sensitive initial conditions training 
search globally optimal solution gradient descent algorithm find local optimum neighborhood initial solution 
problems evolutionary training significantly faster reliable bp 
described ga training algorithm significantly faster methods generalized delta rule gdr 
tests reported ga training algorithm took total hours minutes gdr took total hours minutes 
bartlett downs gave modified ga order magnitude faster bp bit parity problem 
modified ga better scalability bp twice slow bp xor problem faster bp larger bit parity problem 
interestingly quite different results reported kitano 
ga bp method technique runs ga bp best equally efficient faster variants back propagation small scale networks far efficient larger networks 
test problems included xor problem various size encoder decoder problems spiral problem 
papers report excellent results hybrid evolutionary gradient descent algorithms 
discrepancy seemingly contradictory results attributed partly different eas bp compared 
comparison classical binary ga fast bp algorithm fast ea classical bp algorithm 
discrepancy shows clear winner terms best training algorithm 
best problem dependent 
certainly true free lunch theorem 
general hybrid algorithms tend perform better large number problems 
hybrid training eas inefficient fine tuned local search global search 
especially true gas 
efficiency evolutionary training improved significantly incorporating local search procedure evolution combining ea global search ability local search ability fine tune 
eas locate region space local search procedure find near optimal solution region 
local search algorithm bp random search algorithms 
hybrid training successfully application areas 
lee gas search near optimal set initial connection weights bp perform local search initial weights 
results showed hybrid ga bp approach efficient ga bp algorithm 
consider bp run times practice order find connection weights due sensitivity initial conditions hybrid training algorithm quite competitive 
similar evolution initial weights done competitive learning neural networks kohonen networks 
interesting consider finding initial weights locating region weight space 
defining basin attraction local minimum composed points sets weights case converge local minimum local search algorithm global minimum easily local search algorithm ea locate point set initial weights basin attraction global minimum 
illustrates simple case connection weight ann 
ea find initial weight easy local search algorithm arrive globally optimal weight wb evolution architectures section assumed architecture ann predefined fixed evolution connection weights 
section discusses design ann architectures 
architecture ann includes topological structure connectivity transfer function node ann 
indicated architecture design crucial successful application anns architecture significant impact network information processing capabilities 
learning task ann connections linear nodes may able perform task due limited capability ann large number connections nonlinear nodes may overfit noise training data fail generalization ability 
delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta weight training error wa wb illustration ea find initial weights local search algorithm find globally optimal weights easily 
optimal initial weight lead global optimum wb local search algorithm 
architecture design human expert job 
depends heavily expert experience tedious trial error process 
systematic way design near optimal architecture task automatically 
research constructive destructive algorithms represents effort automatic design architectures 
roughly speaking constructive algorithm starts minimal network network minimal number hidden layers nodes connections adds new layers nodes connections necessary training destructive algorithm opposite starts maximal network deletes unnecessary layers nodes connections training 
indicated angeline structural hill climbing methods susceptible trapped structural local optima 
addition investigate restricted topological subsets complete class network architectures 
design optimal architecture ann formulated search problem architecture space point represents architecture 
performance optimality criteria lowest training error lowest network complexity architectures performance level architectures forms discrete surface space 
optimal architecture design equivalent finding highest point surface 
characteristics surface indicated miller eas better candidate searching surface constructive destructive algorithms mentioned 
characteristics ffl surface infinitely large number possible nodes connections unbounded 
ffl surface nondifferentiable changes number nodes connections discrete discontinuous effect eann performance 
ffl surface complex noisy mapping architecture performance indirect strongly epistatic dependent evaluation method 
ffl surface deceptive similar architectures may quite different performance 
ffl surface multimodal different architectures may similar performance 
similar evolution connection weights major phases involved evolution architectures genotype representation scheme architectures ea evolve ann architectures 
key issues encoding ann architectures decide information architecture encoded chromosome 
extreme details connection node architecture specified chromosome 
kind representation scheme called direct encoding 
extreme important parameters architecture number hidden layers hidden nodes layer encoded 
details architecture left training process decide 
kind representation scheme called indirect encoding 
representation scheme chosen evolution architectures progress cycle shown 
cycle stops satisfactory ann 

decode individual current generation architecture 
indirect encoding scheme detail architecture specified developmental rules training process 

train ann decoded architecture predefined learning rule parameters learning rule evolved training starting different sets random initial connection weights learning rule parameters 

compute fitness individual encoded architecture training result performance criteria complexity architecture 

select parents population fitness 

apply search operators parents generate offspring form generation 
typical cycle evolution architectures 
considerable research evolving ann architectures carried years :10.1.1.12.4114
research concentrated evolution ann topological structures 
relatively little done evolution node transfer functions simultaneous evolution topological structures node transfer functions 
analyze representation scheme topological structures section 
convenience term architecture interchangeably term topological structure topology sections 
section discusses evolution node transfer functions briefly 
explain simultaneous evolution ann connection weights architectures beneficial search operators evolving architectures section 
direct encoding scheme different approaches taken direct encoding scheme 
separates evolution architectures connection weights 
second approach evolves architectures connection weights simultaneously :10.1.1.12.4114
section focus approach 
second approach discussed section 
approach connection architecture directly specified binary representation 
example theta matrix ij thetan represent ann architecture nodes ij indicates presence absence connection node node ij indicate connection ij indicate connection 
fact ij represent real valued connection weights node node architecture connection weights evolved simultaneously 
matrix direct mapping corresponding ann architecture 
binary string representing architecture concatenation rows columns matrix 
constraints architectures explored easily incorporated representation scheme imposing constraints matrix feedforward ann non zero entries upper right triangle matrix 
figures give examples direct encoding scheme ann architectures 
obvious encoding scheme handle feedforward recurrent anns 
shows feedforward ann inputs output 
connectivity matrix entry ij indicates presence absence connection node node example row indicates connections node nodes 
columns connection node connection node 
node connected nodes 
columns 
converting connectivity matrix chromosome straightforward 
concatenate rows columns obtain ann feedforward need represent upper triangle connectivity matrix order reduce chromosome length 
reduced chromosome 
ea employed evolve population chromosomes 
order evaluate fitness chromosome need convert chromosome back ann initialise random weights train 
training error measure fitness 
worth noting ann shortcut connection input output 
shortcuts pose problems representation evolution 
ea capable exploring possible connectivities 
shows recurrent ann 
representation basically feedforward anns 
difference reduction chromosome length possible want explore connectivity space 
ea evolve recurrent anns evolve feedforward ones 
gamma gamma gamma gamma gamma gamma delta delta delta delta delta delta ak example direct encoding feedforward ann 
show architecture connectivity matrix binary string representation respectively 
feedforward architectures consideration binary string representation needs consider upper right triangle matrix 
gamma gamma gamma gamma gamma gamma delta delta delta delta delta delta ak oe ae example direct encoding recurrent ann 
show architecture connectivity matrix binary string representation respectively 
direct encoding scheme described quite straightforward implement 
suitable precise fine tuned search compact ann architecture single connection added removed ann easily 
may facilitate rapid generation optimization tightly pruned interesting designs hit far 
flexibility provided evolution architectures stems fitness definition 
virtually limitation differentiable continuous fitness function defined step 
training result pertaining architecture error training time fitness function 
complexity measurement number nodes connections fitness function 
matter fact criteria information theory statistics readily introduced fitness function difficulty 
improvement ann generalization ability expected criteria adopted 
schaffer experiment showed ann designed evolutionary approach better generalization ability trained bp human designed architecture 
potential problem direct encoding scheme scalability 
large ann require large matrix increase computation time evolution 
way cut size matrices domain knowledge reduce search space 
example complete connection neighboring layers feedforward ann architecture encoded just number hidden layers nodes hidden layer 
length chromosome reduced greatly case 
doing requires sufficient domain knowledge expertise difficult obtain practice 
run risk missing solutions restrict search space manually 
permutation problem illustrated figures section exists causes unwanted side effects evolution architectures 
functionally equivalent anns order hidden nodes differently different representations probability producing highly fit offspring recombining low 
researchers avoided crossover adopted mutations evolution architectures shown crossover may useful important increasing efficiency evolution problems :10.1.1.12.4114
hancock suggested permutation problem severe supposed population size selection mechanism increased number ways solving problem outweigh difficulties bringing building blocks 
thierens proposed genetic encoding scheme anns avoid permutation problem limited experimental results 
worth indicating studies permutation problem concentrate ga genetic operators population sizes selection mechanisms necessary investigate algorithm equally important study representation scheme performance surface defined section determined representation 
research needed understand impact permutation problem evolution architectures 
indirect encoding scheme order reduce length representation architectures indirect encoding scheme researchers characteristics architecture encoded chromosome 
details connection ann predefined prior knowledge specified set deterministic developmental rules 
indirect encoding scheme produce compact representation ann architectures may finding compact ann generalization ability 
argued indirect encoding scheme biologically plausible direct impossible genetic information encoded chromosomes specify independently nervous system discoveries neuroscience 
parametric representation ann architectures may specified set parameters number hidden layers number hidden nodes layer number connections layers parameters encoded various forms chromosome 
harp blueprint represent architecture consists segments representing area layer efferent connectivity projections 
area constrained input output area respectively 
segment includes parts information area number nodes area spatial organization area efferent connectivity 
noted connectivity pattern connection specified 
detailed node node connection specified implicit developmental rules network instantiation software harp 
similar parametric representation methods different sets parameters proposed 
interesting aspect harp combination learning parameters architectures representation 
learning parameters evolve architecture parameters 
interaction explored evolution 
parametric representation method reduce length binary chromosome specifying ann architectures eas search limited subset feasible architecture space 
example encode number hidden nodes hidden layer basically assume strictly layered feedforward anns single hidden layer 
assume neighboring layers fully connected 
general parametric representation method suitable know kind architectures trying find 
developmental rule representation quite different indirect encoding method encode developmental rules construct architectures chromosomes 
shift direct optimization architectures optimization developmental rules brought benefits compact representation evolution architectures 
destructive effect crossover lessened developmental rule representation capable preserving promising building blocks far 
method problems 
developmental rule usually described recursive equation generation rule similar production rule production system left hand side lhs right hand side rhs 
connectivity pattern architecture form matrix constructed basis single element matrix repetitively applying suitable developmental rules non terminal elements current matrix matrix contains terminal elements terminal element existence connection non existence connection non terminal element symbol 
definitions slightly different 
indicate presence absence connection connectivity pattern fully specified 
examples developmental rule 
developmental rule consists lhs non terminal element rhs theta matrix terminal non terminal elements 
typical step constructing connection matrix find rules lhss appear current matrix replace appearance respective rhss 
example set rules described starting symbol state step application rules produce matrix replacing apply rules generate matrix replacing step applying rules lead matrix replacing matrix consists terminals application developmental rules 
matrix ann connection matrix 
summarizes previous rule rewriting steps final ann generated 
note nodes appear ann connected nodes 
example described figures illustrate ann architecture defined set rules 
question get set rules construct ann 
answer evolve 
encode rule set individual called pitt approach encode rule individual called michigan approach 
rule may represented allele positions corresponding elements rhs rule 
lhs represented implicitly rule position chromosome 
position chromosome take different values depending non terminal elements symbols rule set 
example non terminals may range gamma 
gamma 
gamma 
gamma 
gamma 
delta delta delta gamma 
gamma 
gamma 
gamma 
delta delta delta examples developmental rules construct connectivity matrix 
initial element state 
gamma gamma gamma gamma gamma gamma delta delta delta delta delta delta ak oe ae cw development eann architecture rules 
initial state step step step entries matrix terminal elements architecture 
nodes architecture numbered 
isolated nodes shown 

rules lhs theta matrices rhs predefined participate evolution order guarantee different connectivity patterns reached 
different rules lhs respectively chromosome encoding need theta alleles rule 
lhs rule implicitly determined position chromosome 
example rule set represented chromosome elements indicate rhs rule second indicate rhs rule results developmental rule representation method reported various size encoder decoder problems 
method limitations 
needs predefine number rewriting steps 
allow recursive rules 
evolving detailed connectivity patterns individual nodes 
compact representation imply compact representation compact ann architecture 
siddiqi lucas shows direct encoding scheme developmental rule method 
re implemented kitano system discovered performance difference direct indirect encoding schemes caused encoding scheme sparsely connected initial ann architectures initial population 
direct encoding scheme achieved performance achieved developmental rule representation initial conditions 
developmental rule representation method normally separates evolution architectures connection weights 
creates problems evolution 
section discuss detail 
mjolsness described similar rule encoding method rules represented recursive equations specify growth connection matrices 
coefficients recursive equations represented decomposition matrices encoded genotypes optimized simulated annealing eas 
connection weights optimized connectivity simulated annealing entry connection matrix real valued weight 
advantage simulated annealing gas evolution avoidance destructive effect crossover 
wilson simulated annealing ann architecture design 
fractal representation merrill port proposed method encoding architectures fractal subsets plane 
argued fractal representation architectures biologically plausible developmental rule representation 
real valued parameters edge code input coefficient output coefficient specify node architecture 
sense encoding method closer direct encoding scheme indirect 
fast simulated annealing evolution 
representations different approach evolution architectures proposed andersen tsoi 
approach unique individual population represents hidden node architecture 
architecture built layer layer hidden layers added current architecture reduce training error certain threshold 
hidden layer constructed automatically evolutionary process employs ga fitness sharing 
fitness sharing encourages formation different feature detectors hidden nodes population 
number hidden nodes hidden layer vary 
limitation approach deal strictly layered feedforward anns 
limitation usually hidden nodes species similar functionality basically feature detector population 
redundancy needs removed additional clean algorithm 
smith individual represent hidden node ann 
approach deal strictly layered feedforward anns 
evolution node transfer functions discussion evolution architectures far deals topological structure architecture 
transfer function node architecture assumed fixed predefined human experts transfer function shown important part ann architecture significant impact ann performance 
transfer function assumed nodes ann nodes layer 
stork best knowledge apply eas evolution topological structures node transfer functions simple anns nodes considered 
transfer function specified structural genes genotypic representation 
complex usual sigmoid function tried model biological neuron circuitry 
white adopted simpler approach evolution topological structures node transfer functions 
individual ann initial population nodes ann sigmoid transfer function nodes gaussian transfer function 
evolution decide optimal mixture transfer functions automatically 
sigmoid gaussian transfer function evolvable 
parameters functions evolved 
liu yao ep evolve anns sigmoidal gaussian nodes 
fixing total number nodes evolve mixture different nodes algorithm allowed growth shrinking ann adding deleting node sigmoidal gaussian 
type node added deleted determined random 
performance reported benchmark problems 
hwang went step 
evolved ann topology node transfer function connection weights projection neural networks 
chellapilla evolution node transfer function example show importance evolving representations 
representation search key issues problem solving 
evolving solutions representations may effective way tackle difficult problems little human expertise available 
simultaneous evolution architectures connection weights evolutionary approaches discussed far designing ann architectures evolve architectures connection weights 
connection weights learned near optimal architecture 
especially true uses indirect encoding scheme developmental rule method 
major problem evolution architectures connection weights noisy fitness evaluation 
words fitness evaluation described step inaccurate noisy phenotype ann full set weights fitness approximate genotype ann weight information fitness 
major sources noise 
source random initialization weights 
different random initial weights may produce different training results 
genotype may quite different fitness due different random initial weights training 

second source training algorithm 
different training algorithms may produce different training results set initial weights 
especially true multimodal error functions 
example bp may reduce ann error training ea reduce error due global search capability 
noise may mislead evolution fact fitness phenotype generated genotype higher generated genotype mean truly higher quality order reduce noise architecture usually trained times different random initial weights 
average result estimate genotype mean fitness 
method increases computation time fitness evaluation dramatically 
major reasons small anns evolved previous studies 
essence noise identified caused mapping genotypes phenotypes 
angeline fogel provided general discussion mapping genotypes phenotypes 
clear evolution architectures weight information difficulties evaluating fitness accurately 
result evolution inefficient 
way alleviate problem evolve ann architectures connection weights simultaneously :10.1.1.12.4114
case individual population fully specified ann complete weight information 
mapping genotype phenotype fitness evaluation accurate 
issue evolving anns choice search operators eas 
crossover mutation eas 
crossover appears contradict basic ideas anns crossover works best exist building blocks unclear building block ann anns emphasize distributed knowledge representation 
knowledge ann distributed weights ann 
recombining part ann part ann destroy anns 
anns distributed representation localized radial basis function rbf networks nearest neighbor multilayer perceptrons crossover useful operator 
area results reported 
general anns distributed representation compact better generalization capability practical problems 
yao liu developed automatic system epnet ep simultaneous evolution ann architectures connection weights 
epnet crossover operators reason 
relies number mutation operators modify architectures weights 
behavioral functional evolution genetic evolution emphasized epnet 
number techniques adopted maintain behavioral link parent offspring 
shows main structure epnet 
hybrid training addition connection node hidden node deletion random initialisation anns initial partial training rank selection obtain new generation 
successful 
successful 
successful 
mutations training deletion main structure epnet 
epnet uses rank selection mutations hybrid training node deletion connection deletion connection addition node addition 
hybrid training mutation epnet modifies ann weights 
modified bp mbp algorithm adaptive learning rate simulated annealing 
mutations grow prune hidden nodes connections 
number epochs mbp train anns population defined userspecified parameters 
guarantee ann converge local optimum epochs 
training process called partial training 
bridging gap parent offspring 
mutations attempted sequentially 
mutation leads better offspring regarded successful 
mutation applied 
mutation attempted 
motivation ordering mutations encourage evolution compact anns sacrificing generalization 
validation set epnet measure fitness individual 
epnet tested extensively number benchmark problems achieved excellent results including parity problems size spiral problem breast cancer problem diabetes problem heart disease problem thyroid problem australian credit card problem mackey glass time series prediction problem compact anns generalization ability evolved 
different ep systems designing anns tested different benchmark problems 
evolution learning rules ann training algorithm may different performance applied different architectures 
design training algorithms fundamentally learning rules adjust connection weights depends type architectures investigation 
different variants hebbian learning rule proposed deal different architectures 
designing optimal learning rule difficult little prior knowledge ann architecture case practice 
desirable develop automatic systematic way adapt learning rule architecture task performed 
designing learning rule manually implies assumptions necessarily true practice 
example widely accepted hebbian learning rule shown outperformed new rule proposed cases 
new rule learn patterns optimal hebbian rule learn exceptions regularities 
difficult say rule optimal anns 
fact needed ann ability adjust learning rule adaptively architecture task performed 
words ann learn learning rule dynamically designed fixed manually 
evolution fundamental forms adaptation surprising evolution learning rules introduced anns order learn learning rules 
relationship evolution learning extremely complex 
various models proposed deal issue learning guide evolution relationship evolution architectures connection weights 
research evolution learning rules early stages 
research important providing automatic way optimizing learning rules modeling relationship learning evolution modeling creative process newly evolved learning rules deal complex dynamic environment 
research help better understand creativity emerge artificial systems anns model creative process biological systems 
typical cycle evolution learning rules described 
iteration stops population converges predefined maximum number iterations reached 
similar reason explained section fitness evaluation individual encoded learning rule noisy phenotype fitness ann training result approximate genotype fitness learning rule fitness 
approximation may inaccurate 
techniques alleviate problem weighted average training results anns different initial connection weights fitness function 
ann architecture predefined fixed evolved learning rule optimized architecture 
near optimal learning rule different ann architectures evolved fitness evaluation average training result different ann architectures order avoid overfitting particular architecture 
evolution algorithmic parameters adaptive adjustment bp parameters learning rate momentum evolution considered attempt evolution learning rules 
harp encoded bp parameters chromosomes ann architecture 
evolutionary approach different non evolutionary offered jacobs simultaneous evolution algorithmic parameters architectures facilitates 
decode individual current generation learning rule 

construct set anns randomly generated architectures initial connection weights train decoded learning rule 

calculate fitness individual encoded learning rule average training result 

select parents current generation fitness 

apply search operators parents generate offspring form new generation 
typical cycle evolution learning rules 
exploration interactions learning algorithm architectures nearoptimal combination bp architecture 
researchers evolutionary process find parameters bp ann architecture predefined 
parameters evolved case tend optimized architecture generally applicable learning 
number bp algorithms adaptive learning rate momentum non evolutionary approach 
comparison approaches quite useful 
evolution learning rules evolution algorithmic parameters certainly interesting hardly touches fundamental part training algorithm learning rule weight updating rule 
adapting learning rule evolution expected enhance ann adaptivity greatly dynamic environment 
evolution connection weights architectures deal static objects ann weights architectures evolution learning rules dynamic behavior ann 
key issue encode dynamic behavior learning rule static chromosomes 
trying develop universal representation scheme specify kind dynamic behaviors clearly impractical prohibitive long computation time required search learning rule space 
constraints set type dynamic behaviors basic form learning rules evolved order reduce representation complexity search space 
basic assumptions learning rules weight updating depends local information activation input node activation output node current connection weight learning rule connections ann 
learning rule assumed linear function local variables products 
learning rule described function deltaw delta delta delta delta delta deltai gamma time deltaw weight change delta delta delta local variables real valued coefficients determined evolution 
words evolution learning rules case equivalent evolution real valued vectors 
different determine different learning rules 
due large number possible terms eq 
evolution slow impractical terms practice biological heuristic knowledge 
major issues involved evolution learning rules determination subset terms described eq 
representation real valued coefficients chromosomes ea evolve chromosomes 
chalmers defined learning rule linear combination local variables pairwise products 
third fourth order terms 
coefficients scale parameter encoded binary string exponential encoding 
architecture fitness evaluation fixed single layer anns considered number inputs outputs fixed learning task hand 
generations starting population randomly generated learning rules evolution discovered known delta rule variants 
experiments simple preliminary demonstrated potential evolution discovering novel learning rules set randomly generated rules 
constraints set learning rules prevent evolved include third fourth order terms 
similar experiments evolution learning rules carried 
meir chalmers approach evolve learning rules binary perceptrons 
considered local variables terms adopted learning rules included order second order third order terms eq 

baxter took step just evolution learning rules 
tried evolve complete anns including connection weights architectures learning rules single level evolution 
clear search space possible anns enormous constraints set connection weights architectures learning rules 
experiments anns binary threshold nodes considered weights gamma 
number nodes anns fixed 
learning rule considered boolean variables 
baxter experiments simple confirmed complex behaviors learned ann learning ability improved evolution 
bengio approach slightly different chalmers sense gradient descent algorithms simulated annealing eas find near optimal 
experiments local variables zeroth order order second order terms eq 
research related evolution learning rules includes parisi evolve learning rules explicitly 
emphasized crucial role environment evolution occured simple neural networks 
issue environmental diversity closely related noisy fitness evaluation pointed section section 
possible sources noise 
decoding process morphogenesis chromosomes 
second introduced decoded learning rule evaluated train anns 
environmental diversity essential obtaining approximation fitness decoded learning rule reducing noise second source 
general learning rule applicable wide range ann architectures learning tasks needed environmental diversity high different architectures learning tasks fitness evaluation 
order defined number variables product 
combinations artificial neural networks evolutionary algorithms evolution input features practical problems possible inputs ann quite large 
may redundancy different inputs 
large number inputs ann increase size require training data longer training times order achieve reasonable generalization ability 
preprocessing needed reduce number inputs ann 
various dimension reduction techniques including principal component analysis purpose 
problem finding near optimal set input features ann formulated search problem 
large set potential inputs want find near optimal subset fewest number features performance ann subset worse ann input set 
eas perform search effectively 
results better performance fewer inputs reported studies 
evolution input features individual population represents subset possible inputs 
implemented binary chromosome length total number input features 
bit chromosome corresponds feature 
indicates presence feature indicates absence feature 
evaluation individual carried training ann inputs result calculate fitness value 
ann architecture fixed 
evaluation noisy reason explained section 
evolution input features provide way discover important features possible inputs automatically discover new training examples 
zhang described active learning paradigm training algorithm eas self select training examples 
cho cha proposed algorithm evolving training sets adding virtual samples 
artificial neural network fitness estimator evolutionary algorithms success optimize various control parameters 
time consuming costly obtain fitness values control problems impractical run real system combination control parameters 
order get problem evolution efficient fitness values approximated computed exactly 
anns model approximate real control system due generalization abilities 
input anns set control parameters 
output control system output evaluation system easily obtained 
ea search near optimal set control parameters ann fitness evaluation real control system 
combination anns eas couple advantages evolving control systems 
time consuming fitness evaluation real control systems replaced fast fitness evaluation anns 
second combination provides safer evolution control systems 
eas stochastic algorithms 
possible poor control parameters may generated evolutionary process 
parameters damage real control system 
anns estimate fitness need real system avoid damages real system 
successful combination approach depends largely anns learn generalize 
evolving artificial neural network ensembles learning formulated optimization problem machine learning field 
learning different optimization practice want learned system best generalization different minimizing error function training data set 
ann minimum error training data set may best generalization equivalence generalization error training data 
unfortunately measuring generalization quantitatively accurately impossible practice theories criteria generalization minimum description length mdl akaike information criteria aic minimum message length mml 
practice criteria define better error functions hope minimizing functions maximize generalization 
functions lead better generalization learned systems guarantee 
eas maximize fitness function minimize error function face problem described maximizing fitness function different maximizing generalization 
ea optimization learning algorithm 
little done traditional non population learning opportunities improving population learning evolutionary learning 
maximum fitness may equivalent best generalization evolutionary learning best individual maximum fitness population may want 
individuals population may contain useful information help improve generalization learned systems 
beneficial population single individual 
population contains information single individual 
combining different individuals population form integrated system expected produce better results 
population anns called ann ensemble section 
successful experiments show eas evolve ann ensembles 
novel combinations anns eas 
example eas extract rules reinforcement learning system train anns 
pal eas tune circuit parameters templates cellular anns 
eas optimize modified restricted coulomb energy rce ann 
araki eas evolve connection weights hopfield anns 
eas anns combinatorial global numerical optimization order combine ea global search capability ann fast convergence local optima 
concluding remarks evolution introduced anns various levels roughly divided evolution connection weights architectures learning rules 
section describes general framework anns draws 
general framework eanns general framework eanns described :10.1.1.13.957
evolution connection weights proceeds lowest level fastest time scale environment determined architecture learning rule learning tasks 
alternatives decide level evolution architectures learning rules evolution architectures highest level learning rules lower level vice versa 
lower level evolution faster time scale 
engineering perspective decision level evolution depends kind prior knowledge available 
prior knowledge ann architecture learning rules particular class architectures pursued better put evolution architectures highest level knowledge encoded architecture genotypic representation reduce architecture search space lower level evolution learning rules biased type architectures 
hand evolution learning rules highest level prior knowledge available special interest certain type learning rules 
unfortunately usually little prior knowledge available architectures learning rules practice vague statements 
case appropriate put evolution architectures highest level optimality learning rule sense evaluated environment including architecture learning rule applied 
summarizes different levels evolution anns 
viewed general framework adaptive systems restrict eas levels 
simulated annealing gradient descent exhaustive search considered special cases eas 
example traditional bp network considered special case general framework shot candidate search evolution architectures learning rules bp evolution connection weights 
fact general framework provides basis comparing various specific eann models search procedures different levels defines dimensional space represents shot search represents exhaustive search axis 
eann model corresponds point space 
evolution introduced anns different levels 
evolution connection weights provides global approach connection weight training especially gradient information error function difficult costly obtain 
due simplicity generality evolution fact gradient training algorithms run multiple times order avoid trapped poor local optimum evolutionary approach quite competitive 
evolution find near optimal ann architecture automatically 
advantages heuristic methods architecture design characteristics design problem section 
direct encoding scheme ann architectures fine tuning generating compact architecture 
indirect encoding scheme suitable finding particular type ann architecture quickly 
separating evolution architectures connection weights fitness evaluation inaccurate mislead evolution 
simultaneous evolution ann architectures connection weights generally produces better results 
argued crossover produces harm benefit evolving anns distributed representation multilayer perceptrons destroys knowledge learned evolution architectures evolution learning rules evolution connection weights evaluation architectures reproduction architectures evaluation learning rules reproduction learning rules evaluation reproduction tasks weights fitness oe oe oe architecture fitness learning rule fitness general framework eanns 
distributed different connections easily 
crossover suitable localized anns rbf networks 
evolution allow ann adapt learning rule environment 
sense evolution provides anns ability learning learn 
helps model relationship learning evolution 
preliminary experiments shown efficient learning rules evolved randomly generated rules 
current research evolution learning rules normally assumes learning rules specified eq 

constraints learning rules necessary reduce search space evolution prevent interesting learning rules discovered 
global search procedures eas usually computationally expensive 
better employ eas levels evolution 
beneficial introduce global search levels evolution especially little prior knowledge available level performance ann required high trial error heuristic methods ineffective circumstances 
increasing power parallel computers evolution large anns feasible 
evolution discover possible new ann architectures learning rules offers way model creative process result ann adaptation dynamic environment 
partially supported australian research council small scheme 
author grateful dr david fogel anonymous referee constructive comment earlier versions 
yao evolution connectionist networks preprints int symp 
ai reasoning creativity ed queensland australia pp 
griffith university 
yao review evolutionary artificial neural networks international journal intelligent systems vol 
pp 

yao evolutionary artificial neural networks international journal neural systems vol 
pp 

yao evolution connectionist networks artificial intelligence creativity ed pp 
dordrecht kluwer academic publishers 
yao evolutionary artificial neural networks encyclopedia computer science technology kent williams eds vol 
pp 
new york ny marcel dekker 
hinton connectionist learning procedures artificial intelligence vol 
pp 
september 
hertz krogh palmer theory neural computation 
reading ma addison wesley 

schwefel numerical optimization computer models 
chichester john wiley sons 

schwefel evolution optimum seeking 
new york john wiley sons 
fogel owens walsh artificial intelligence simulated evolution 
new york ny john wiley sons 
fogel system identification simulated evolution machine learning approach modeling 
needham heights ma press 
fogel evolutionary computation new philosophy machine intelligence 
new york ny ieee press 
holland adaptation natural artificial systems 
ann arbor mi university michigan press 
goldberg genetic algorithms search optimization machine learning 
reading ma addison wesley 
fogel simulated evolutionary optimisation ieee trans 
neural networks vol 
pp 

back hammel 
schwefel evolutionary computation comments history current state ieee transactions evolutionary computation vol 
pp 

horne progress supervised neural networks ieee signal processing magazine vol 
pp 
january 
rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations cognition vol 
rumelhart mcclelland eds pp 
mit press cambridge ma 
ller scaled conjugate gradient algorithm fast supervised learning neural networks vol 
pp 
june 
lang waibel hinton time delay neural network architecture isolated word recognition neural networks vol 
pp 

fels hinton glove talk neural network interface data glove speech synthesizer ieee trans 
neural networks vol 
pp 
january 
personnaz dreyfus handwritten digit recognition neural networks single layer training ieee trans 
neural networks vol 
pp 
november 
sutton problems backpropagation steepest descent learning procedures networks proc 
th annual conf 
cognitive science society pp 
lawrence erlbaum associates hillsdale nj 
whitley starkweather genetic algorithms neural networks optimizing connections connectivity parallel computing vol 
pp 

chauvin rumelhart ed backpropagation theory architectures applications 
hillsdale nj usa lawrence erlbaum associates publ 
whitley genitor algorithm selective pressure rank allocation reproductive trials best proc 
third int conf 
genetic algorithms applications schaffer ed pp 
morgan kaufmann san mateo ca 
montana davis training feedforward neural networks genetic algorithms proc 
eleventh int joint conf 
artificial intelligence pp 
morgan kaufmann san mateo ca 
dolan parametric connectivity training constrained networks genetic algorithms proc 
third int conf 
genetic algorithms applications schaffer ed pp 
morgan kaufmann san mateo ca 
fogel fogel porto evolving neural networks biological cybernetics vol 
pp 

bartlett downs training neural network genetic algorithm 
technical report dept elec 
eng univ queensland january 
parallel algorithms learning neural networks evolution strategy proc 
parallel computing evans peters eds pp 
elsevier science publishers amsterdam 
belew mcinerney schraudolph evolving networks genetic algorithm connectionist learning tech 
rep cs revised computer science engr 
dept 
univ california san diego la jolla ca usa february 
radcliffe genetic neural networks mimd computers compressed edition 
phd thesis dept theoretical phys university edinburgh scotland uk 
training multilayered neural networks replacing fit hidden neurons proc 
ieee vol 
pp 
ieee press new york ny 
de garis steerable gennets genetic programming steerable behaviors gennets practice autonomous systems proc 
european conference artificial life varela bourgine eds pp 
mit press cambridge ma usa 
de garis genetic algorithm train time dependent behaviors neural networks proc 
international workshop multistrategy learning msl michalski tecuci eds pp 
center artificial intelligence fairfax va usa 
srinivas learning neural network weights genetic algorithms improving performance search space reduction proc 
ieee international joint conference neural networks ijcnn singapore vol 
pp 
ieee press new york ny 
de garis gennets genetically programmed neural nets genetic algorithm train neural nets inputs outputs vary time proc 
ieee international joint conference neural networks ijcnn singapore vol 
pp 
ieee press new york ny 
guan training weights neural networks genetic algorithms messy genetic algorithms proc 
second iasted international symposium expert systems neural networks ed pp 
acta press anaheim ca usa 
new learning algorithm training multilayered neural networks uses genetic algorithm techniques electronics letters vol 
pp 
july 
wieland evolving neural network controllers unstable systems proc 
ieee international joint conference neural networks ijcnn seattle vol 
pp 
ieee press new york ny 
koza rice genetic generation weights architecture neural network proc 
ieee international joint conference neural networks ijcnn seattle vol 
pp 
ieee press new york ny 
dominic das whitley anderson genetic reinforcement learning neural networks proc 
ieee international joint conference neural networks ijcnn seattle vol 
pp 
ieee press new york ny 
dill deer exploration genetic algorithms selection connection weights dynamical neural networks proc 
ieee national aerospace electronics conference vol 
pp 
ieee press new york ny 
bornholdt general asymmetric neural networks structure design genetic algorithms neural networks vol 
pp 

genetically programmed neural network solving pole balancing problem artificial neural networks proc 
int conf 
artificial neural networks icann vol 
kohonen simula kangas eds pp 
north holland amsterdam 
evolving sequential machines amorphous neural networks artificial neural networks proc 
int conf 
artificial neural networks icann vol 
kohonen simula kangas eds pp 
north holland amsterdam 
menczer parisi evidence hyperplanes genetic learning neural networks biological cybernetics vol 
pp 

ichikawa neural network application direct feedback controllers ieee transactions neural networks vol 
pp 
march 
neil genetic training layer neural network electronics letters vol 
pp 
jan 
mixed genetic approach optimization neural controllers proc 
eds pp 
ieee computer soc 
press los alamitos ca 
janson application genetic algorithms training higher order neural networks journal systems engineering vol 
pp 

janson training product unit neural networks genetic algorithms ieee expert vol 
pp 

training neural networks genetic algorithms target detection proceedings spie conf 
science artificial neural networks orlando fl usa vol 
pt pp 

brown alternative learning methods training neural network classifiers proceedings spie conf 
science artificial neural networks orlando fl usa vol 
pt pp 

lewis fagg genetic programming approach construction neural network control walking robot proc 
ieee international conference robotics automation vol 
pp 
ieee computer soc 
press los alamitos ca 
genetic breeding algorithm exhibits self organizing neural networks proc 
iasted international symposium artificial intelligence application neural networks ed pp 
acta press anaheim ca usa 
von application artificial neural networks genetic algorithms personnel selection financial industry proc 
international conference artificial intelligence wall street pp 
ieee computer soc 
press los alamitos ca 
elias genetic generation connection patterns dynamic artificial neural network proc 
int workshop combinations genetic algorithms neural networks whitley schaffer eds pp 
ieee computer society press los alamitos ca 
beer gallagher evolving dynamical neural networks adaptive behavior adaptive behavior vol 
pp 

incremental approach developing intelligent neural network controllers robots ieee transactions systems man cybernetics part cybernetics vol 
pp 

baluja evolution artificial neural network autonomous land vehicle controller ieee transactions systems man cybernetics part cybernetics vol 
pp 

porto fogel fogel alternative neural network training methods ieee expert vol 
pp 

yao wei evolving wavelet neural networks function approximation electronics letters vol 
pp 

greenwood training partially recurrent neural networks evolutionary strategies ieee transactions speech audio processing vol 
pp 

ter comparison bayesian neural techniques problems classification multiple categories nuclear instruments methods physics research section accelerators detectors associated equipment vol 
pp 

neural network training means cooperative evolutionary search nuclear instruments methods physics research section accelerators detectors associated equipment vol 
pp 

kunze comparison performance feed forward neural networks supervised growing neural gas algorithm nuclear instruments methods physics research section accelerators detectors associated equipment vol 
pp 

hirata application improved genetic algorithm learning neural networks solid state communications vol 
pp 

accelerating standard backpropagation method genetic approach neurocomputing vol 
pp 

hung parallel genetic neural network learning algorithm mimd shared memory machines ieee transactions neural networks vol 
pp 

short term load forecasting neural network refined genetic algorithm electric power systems research vol 
pp 

deo datta rastogi deb optimization back propagation algorithm gas assisted ann models hot metal steel research vol 
pp 

skinner neural networks computational materials science training algorithms modelling simulation materials science engineering vol 
pp 

chen yamada nuclear reactor diagnostic system genetic algorithm ga trained neural networks electrical engineering japan english translation vol 
pp 

pattern recognition fuzzy neural networks learning modified genetic algorithms neural network world vol 
pp 

wang yao zhang genetic fuzzy net controller application advances modeling analysis vol 
pp 

optimization neural networks genetic algorithms neural network world vol 
pp 

jain 
peng neural network design genetic learning control single link flexible manipulator journal intelligent robotic systems theory applications vol 
pp 

taha hanna evolutionary neural network model selection pavement maintenance strategy transportation research record vol 
pp 
may 

lee line recognition totally unconstrained handwritten numerals multilayer cluster neural network ieee transactions pattern analysis machine intelligence vol 
pp 


lee jang translation rotation scale invariant pattern recognition spectral analysis hybrid genetic neural fuzzy networks computers industrial engineering vol 
pp 

hansen learning experiments genetic optimization generalized regression neural network decision support systems vol 
pp 


huang 
huang genetic multilayered perceptron taiwan power system short term load forecasting electric power systems research vol 
pp 


huang 
huang application genetic neural networks thermal unit commitment ieee transactions power systems vol 
pp 

chen connell active power line conditioner neural network control ieee transactions industry applications vol 
pp 
july august 
chen chu concurrent training algorithm supervised learning artificial neural networks journal information science engineering vol 
pp 

neural networks process control modelbased reinforcement trained controllers computers electronics agriculture vol 
pp 

sexton dorsey johnson global optimization neural networks comparison genetic algorithm backpropagation decision support systems vol 
pp 

kubo neural network female mate preference trained genetic algorithm philosophical transactions royal society vol 

zhou genetic learning neural networks spatial decision making gis pe rs photogrammetric engineering remote sensing vol 
pp 

yoon holmes efficient genetic algorithms training layered feedforward neural networks information sciences vol 
pp 

training neural networks means genetic algorithms working long chromosomes international journal neural systems vol 
pp 

park 
park park neuro genetic controller phase systems ieee transactions neural networks vol 
pp 

fogel wasson porto step computer assisted mammography evolutionary programming neural networks cancer letters vol 

fogel wasson evolving neural networks detecting breast cancer cancer letters vol 
pp 

classification eeg waveforms rce neural networks genetic algorithms electronics letters vol 
pp 

pal genetic algorithms fuzzy fitness function object extraction cellular networks fuzzy sets systems vol 
pp 

robustness cellular neural networks image deblurring texture segmentation international journal circuit theory applications vol 
part pp 

petridis operating populations different evolution behaviours proceedings ieee international conference evolutionary computation icec piscataway nj usa pp 
ieee press 
thierens non redundant genetic coding neural networks proceedings ieee international conference evolutionary computation icec piscataway nj usa pp 
ieee press 
hutchins identifying nonlinear dynamic systems neural nets evolutionary programming proceedings th asilomar conference signals systems computers 
part los alamitos ca usa pp 
ieee computer society press 
lei 
jiang state estimation cstr system recurrent neural network trained proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 
murty retaining diversity search point distribution breeder genetic algorithm neural network learning proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 
new hybrid neural genetic methodology improving learning proceedings th ieee international conference tools artificial intelligence piscataway nj usa pp 
ieee press 
schultz wechsler data fusion neural networks computational evolution proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 
neural network uses evolutionary learning proceedings ieee international conference evolutionary computation icec piscataway nj usa pp 
ieee press 
aguilar recognition algorithm evolutionary learning random neural networks proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 
allen evolutionary neural networks robust approach software reliability problems proceedings th international symposium software reliability engineering los alamitos ca usa pp 
ieee computer society press 
yan zhu hu hybrid genetic bp algorithm application radar target classification proceedings ieee national aerospace electronics conference 
part piscataway nj usa pp 
ieee press 

yang 
kao 
evolving neural induction regular language combined evolutionary algorithms proceedings st joint conference intelligent systems piscataway nj usa pp 
ieee press 
zhang ohta hybrid adaptive learning control nonlinear system proceedings american control conference 
part pp 

hancock genetic algorithms permutation problems comparison recombination operators neural net structure specification proc 
int workshop combinations genetic algorithms neural networks whitley schaffer eds pp 
ieee computer society press los alamitos ca 
new interpretation schema notation binary encoding constraint proc 
third int conf 
genetic algorithms applications schaffer ed pp 
morgan kaufmann san mateo ca 
radcliffe equivalence class analysis genetic algorithms complex systems vol 
pp 

fogel note representations variation operators ieee transactions evolutionary computation vol 
pp 

saravanan fogel evolving neural control systems ieee expert vol 
pp 

tang chan man kwong genetic structure nn topology weights optimization proceedings st iee ieee international conference genetic algorithms engineering systems innovations applications england pp 
iee conference publication 
sarkar evolutionary programming probabilistic neural networks construction technique proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 
angeline evolving basis functions dynamic receptive fields proceedings ieee international conference systems man cybernetics 
part piscataway nj usa pp 
ieee press 
yao liu fast evolutionary programming evolutionary programming proc 
fifth annual conference evolutionary programming fogel angeline back eds cambridge ma pp 
mit press 
yao lin liu analysis evolutionary algorithms neighbourhood step sizes evolutionary programming vi proc 
sixth annual conference evolutionary programming angeline reynolds mcdonnell eberhart eds vol 
lecture notes computer science berlin pp 
springer verlag 
back 
schwefel overview evolutionary algorithms parameter optimization evolutionary computation vol 
pp 

yao liu fast evolution strategies control cybernetics vol 
pp 

yao empirical study genetic operators genetic algorithms microprogramming vol 
pp 
september 
temporal processing recurrent networks evolutionary approach proc 
fourth int conf 
genetic algorithms belew booker eds pp 
morgan kaufmann san mateo ca 
evolving recurrent neural networks non binary encoding proceedings ieee international conference evolutionary computation 
part piscataway nj usa pp 
ieee press 
jr oshima traditional evolved dynamic neural networks aircraft simulation proceedings ieee international conference systems man cybernetics 
part piscataway nj usa pp 
ieee press 

wu 
chen 
lee cache genetic modular fuzzy neural network robot path planning proceedings ieee international conference systems man cybernetics 
part piscataway nj usa pp 
ieee press 
takano reasoning learning method fuzzy rules neural networks adaptive structured genetic algorithm proceedings ieee international conference systems man cybernetics 
part piscataway nj usa pp 
ieee press 
fahlman faster learning variations back propagation empirical study proc 
connectionist models summer school touretzky hinton sejnowski eds pp 
morgan kaufmann san mateo ca 
johansson goodman backpropagation learning multilayer feed forward neural networks conjugate gradient method int neural systems vol 
pp 

kitano empirical studies speed convergence neural network training genetic algorithms proc 
eighth nat conf 
ai aaai pp 
mit press cambridge ma 
wolpert macready free lunch theorems optimization ieee transactions evolutionary computation vol 
pp 

yao optimization genetic annealing proc 
second australian conf 
neural networks jabri ed sydney australia pp 

stabilization inverted pendulum genetic algorithm proceedings ieee conference emerging technologies factory automation 
part piscataway nj usa pp 
ieee press 
self tuning neuro pid control applications proceedings ieee international conference systems man cybernetics 
part piscataway nj usa pp 
ieee press 
short term load forecasting genetically optimized neural network cascaded modified kohonen clustering process proceedings ieee international symposium intelligent control piscataway nj usa pp 
ieee press 
merelo pat ca nas prieto mor optimization competitive learning neural network genetic algorithms proc 
int workshop artificial neural networks pp 
springer verlag 
lecture notes computer science vol 

wang xu fault detection evolving lvq neural networks proceedings ieee international conference systems man cybernetics vol 
piscataway nj usa pp 
ieee press 
fahlman lebiere cascade correlation learning architecture advances neural information processing systems touretzky ed pp 
morgan kaufmann san mateo ca 
frean algorithm method constructing training feedforward neural networks neural computation vol 
pp 

mozer smolensky skeletonization technique trimming fat network relevance assessment connection science vol 
pp 

dow creating artificial neural networks generalize neural networks vol 
pp 

hirose yamashita back propagation algorithm varies number hidden units neural networks vol 
pp 

lecun denker solla optimal brain damage advances neural information processing systems touretzky ed pp 
morgan kaufmann san mateo ca 
roy kim mukhopadhyay polynomial time algorithm construction training class multilayer perceptrons neural networks vol 
pp 


hwang 

lay 
jou wrong cascaded correlation learning network projection pursuit learning perspective tech 
rep department electrical engineering ft university washington seattle wa 
angeline pollack evolutionary algorithm constructs recurrent neural networks ieee trans 
neural networks vol 
pp 

miller todd hegde designing neural networks genetic algorithms proc 
third int conf 
genetic algorithms applications schaffer ed pp 
morgan kaufmann san mateo ca 
kitano designing neural networks genetic algorithms graph generation system complex systems vol 
pp 

harp samad guha genetic synthesis neural networks proc 
third int conf 
genetic algorithms applications schaffer ed pp 
morgan kaufmann san mateo ca 
schaffer caruana eshelman genetic search exploit emergent behavior neural networks physica vol 
pp 

wilson perceptron redux emergence structure physica vol 
pp 

dodd optimisation artificial neural network structure genetic techniques implemented multiple transputers proceedings welch stiles kunii eds pp 
ios amsterdam 
harp samad guha designing application specific neural networks genetic algorithm advances neural information processing systems touretzky ed pp 
morgan kaufmann san mateo ca 
dress darwinian optimization synthetic neural systems proc 
st ieee int conf 
neural networks vol 
butler eds pp 
ieee new york ny 
bergman breeding intelligent automata proc 
st ieee int conf 
neural networks vol 
butler eds pp 
ieee new york ny 
hancock design neural net face recognition genetic algorithm tech 
rep center cognitive computational neuroscience dept computing sci 
psychology stirling university stirling fk la uk august 
dolan dyer evolution symbols proc 
nd int conf 
genetic algorithms applications pp 
lawrence erlbaum associates hillsdale nj 
fullmer miikkulainen marker genetic encoding neural networks evolve finite state behaviour practice autonomous systems proc 
european conference artificial life varela bourgine eds pp 
mit press cambridge ma usa 
fusion technology design evolutionary machines neural networks artificial neural networks proc 
int conf 
artificial neural networks icann vol 
kohonen simula kangas eds pp 
north holland amsterdam 
dodd optimisation neural network structure genetic techniques proc 
conf 
applications artificial intelligence engineering vi eds pp 
elsevier applied science london uk 
marshall harrison optimization training feedforward neural networks genetic algorithms proc 
second iee international conference artificial neural networks pp 
iee press london uk 
furst distributed genetic algorithm neural network design training complex systems vol 
pp 

mart genetically generated neural networks representational effects proc 
int joint conf 
neural networks ijcnn baltimore vol 
iv pp 
ieee press new york ny 
joost werner synthesis performance analysis multilayer neural network architectures tech 
rep university koblenz institute fur physics koblenz 

voigt born nez evolutionary structuring artificial neural networks tech 
rep technical university berlin evolution techniques lab ack berlin 
mar genetic synthesis discrete time recurrent neural network proc 
int workshop artificial neural networks pp 
springer verlag 
lecture notes computer science vol 

alba fully automatic ann design genetic approach proc 
int workshop artificial neural networks pp 
springer verlag 
lecture notes computer science vol 

white genetic algorithm optimizing topology weights neural network design proc 
int workshop artificial neural networks pp 
springer verlag 
lecture notes computer science vol 

andersen constructive algorithm multilayer perceptron operative population concepts genetic algorithms master thesis university queensland department electrical computer engineering brisbane qld australia september 
identification control simulated distillation plant connectionist evolutionary techniques simulation vol 
pp 

ii 
modeling faulted switched reluctance motors evolutionary neural networks ieee transactions industrial electronics vol 
pp 

evolutionary design application specific neural networks genetic approach neural network world vol 
pp 

sato coevolution recurrent neural networks genetic algorithms systems computers japan vol 
pp 

mondada floreano evolution neural control structures experiments mobile robots robotics autonomous systems vol 
pp 

fukuda arai structure optimization fuzzy neural network genetic algorithm fuzzy sets systems vol 
pp 

fang xi neural network design evolutionary programming artificial intelligence engineering vol 
pp 

hybrid expert system investment advising expert systems vol 
pp 

smith combined biological paradigms neural genetics autonomous systems strategy robotics autonomous systems vol 
pp 

maniezzo genetic evolution topology weight distribution neural networks ieee transactions neural networks vol 
pp 

sklansky genetic selection neural modeling piecewise linear classifiers international journal pattern recognition artificial intelligence vol 
pp 

yao shi preliminary study designing artificial neural networks coevolution proc 
ieee singapore intl conf intelligent control instrumentation singapore pp 
ieee singapore section june 
yao liu epnet chaotic time series prediction selected papers asia pacific conference simulated evolution learning seal yao 
kim eds vol 
lecture notes artificial intelligence berlin pp 
springer verlag 
liu yao population learning algorithm learns architectures weights neural networks chinese journal advanced software research allerton press new york ny vol 
pp 

yao liu designing artificial neural networks evolution applied mathematics computation vol 
pp 

yao liu evolutionary artificial neural networks learn generalise ieee international conference neural networks washington dc usa volume panel special sessions pp 
ieee press new york ny june 
yao liu evolving artificial neural networks medical applications proc 
australia korea joint workshop evolutionary computation pp 
kaist korea september 
yao importance maintaining behavioural link parents offspring proc 
ieee int conf 
evolutionary computation icec indianapolis usa pp 
ieee press new york ny april 
liu yao evolutionary design artificial neural networks different nodes proc 
ieee int conf 
evolutionary computation icec nagoya japan pp 
ieee press new york ny 
yao liu ensemble structure evolutionary artificial neural networks proc 
ieee int conf 
evolutionary computation icec nagoya japan pp 
ieee press new york ny 
yao liu making population information evolutionary artificial neural networks ieee trans 
systems man cybernetics part cybernetics vol 
pp 

yao liu new evolutionary system evolving artificial neural networks ieee transactions neural networks vol 
pp 

yao liu evolving artificial neural networks evolutionary programming evolutionary programming proc 
fifth annual conference evolutionary programming fogel angeline back eds cambridge ma pp 
mit press 
mcdonnell evolving cascade correlation networks time series forecasting int 
artificial intelligence tools vol 
pp 

mcdonnell evolving recurrent perceptrons time series modeling ieee trans 
neural networks vol 
pp 

poli evolving topology weights neural networks dual representation applied intelligence vol 
pp 

richards moriarty miikkulainen evolving neural networks play go applied intelligence vol 
pp 

moriarty miikkulainen discovering complex othello strategies evolutionary neural networks connection science vol 
pp 

conrad combining evolution credit new learning algorithm neural nets neural networks vol 
pp 

esat machinery fault diagnostics direct encoding graph syntax optimizing artificial neural network structure proceedings rd biennial joint conference engineering systems design analysis 
part new york ny usa pp 
asme 
chow chu configuration multilayered feedforward networks evolutionary process proceedings th midwest symposium circuits systems 
part piscataway nj usa pp 
ieee press 
genetic algorithm neural networks dynamical system modeling proceedings ieee international conference evolutionary computation 
part piscataway nj usa pp 
ieee press 
jain johnson genetic algorithms grammar encoding generate neural networks proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 

cho modular neural networks evolved genetic programming proceedings ieee international conference evolutionary computation icec piscataway nj usa pp 
ieee press 
yip hines yu application artificial neural networks sales forecasting proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 
perez holzmann improvements handwritten digit recognition genetic selection neural network topology augmented training proceedings ieee international conference systems man cybernetics 
part piscataway nj usa pp 
ieee press 
genetic determination large signal model proceedings th european microwave conference 
part wells england pp 
microwave publ 
monster ghost connection machine modularity neural systems theoretical evolutionary research proceedings acm ieee supercomputing conference los alamitos ca usa pp 
ieee computer society press 
gutjahr automatic determination optimal network topologies information theory evolution proceedings rd euromicro conference los alamitos ca usa pp 
ieee computer society press 
georgopoulos optimizing structure neural networks evolution techniques proceedings th international conference application high performance computers engineering england pp 
computational mechanics publ 
patel genetic algorithms construct network financial prediction proceedings spie applications artificial neural networks image processing bellingham wa usa pp 
society photo optical instrumentation engineers 
moriarty miikkulainen improving game tree search evolutionary neural networks proceedings st ieee conference evolutionary computation 
part piscataway nj usa pp 
ieee press 
applying crossover operators automatic neural network construction proceedings st ieee conference evolutionary computation 
part piscataway nj usa pp 
ieee press 
minimal network architectures evolutionary growth networks proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 

lee kim evolutionary ordered neural network linked list encoding scheme proceedings ieee international conference evolutionary computation icec piscataway nj usa pp 
ieee press 
sharman genetic programming techniques evolve recurrent neural network architectures signal processing proceedings ieee workshop neural networks signal processing piscataway nj usa pp 
ieee press 
bo li aggarwal johns moore non communication protection transmission line genetic evolved neural network proceedings th international conference developments power system protection england pp 
iee conference publication 
bo li aggarwal johns current transients faulted phase selection technique genetic algorithm evolved neural network proceedings nd universities power engineering conference 
part greece pp 
technological educational institute 
song johns xuan liu genetic algorithm neural networks applied fault classification transmission lines proceedings th international conference developments power system protection england pp 
iee conference publication 
zhao combination iea cea proceedings ieee international conference evolutionary computation icec piscataway nj usa pp 
ieee press 
sarkar feedforward neural networks configuration evolutionary programming proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 
cho combining modular neural networks developed evolutionary algorithm proceedings ieee international conference evolutionary computation icec piscataway nj usa pp 
ieee press 
hwang choi park evolutionary projection neural networks proceedings ieee international conference evolutionary computation icec piscataway nj usa pp 
ieee press 
bichsel seitz minimum class entropy maximum information approach layered networks neural networks vol 
pp 

fogel information criterion optimal neural network selection ieee trans 
neural networks vol 
pp 
september 
moody selecting neural network architectures prediction risk application corporate bond rating prediction proc 
int conf 
ai applications wall street pp 
ieee computer society press los alamitos ca 
spears anand study crossover operators genetic programming proc 
th international symposium methodologies intelligent systems ismis ras eds pp 
springer verlag berlin germany 
mjolsness sharp alpert scaling machine learning genetic neural nets advances applied mathematics vol 
pp 

merrill port configured neural networks neural networks vol 
pp 

gruau genetic synthesis boolean neural networks cell rewriting developmental process proc 
int workshop combinations genetic algorithms neural networks whitley schaffer eds pp 
ieee computer society press los alamitos ca 
siddiqi lucas comparison matrix rewriting versus direct encoding evolving neural networks proc 
ieee international conference evolutionary computation piscataway nj usa pp 
ieee press 
wilson teaching network connectivity simulated annealing massively parallel processor proceedings ieee vol 
pp 
april 
hartley nonconvex optimization fast simulated annealing proceedings ieee vol 
pp 
november 
andersen tsoi constructive algorithm training multilayer perceptron genetic algorithm complex systems vol 
pp 

smith iii learning classifier system type neural network evolutionary computation vol 
pp 
spring 
mani learning gradient descent function space proc 
ieee int conf 
system man cybernetics los angeles ca pp 

lovell tsoi performance neocognitron various cell cell transfer functions 
intelligent machines lab dept elec 
eng univ queensland april 
dasgupta schnitger efficient approximation neural networks comparison gate functions tech 
rep dept computer sci pennsylvania state univ university park pa 
stork walker burns jackson neural circuits proc 
int joint conf 
neural networks vol 
washington dc pp 
lawrence erlbaum associates hillsdale nj 
chellapilla making problems evolutionarily friendly part evolving convenient representations evolutionary programming vii proc 
th annual conference evolutionary programming porto saravanan eiben eds vol 
lecture notes computer science berlin pp 
springer verlag 
fogel phenotypes genotypes operators evolutionary computation proc 
ieee int conf 
evolutionary computation icec perth australia pp 
ieee press new york ny 
rumelhart 
ed parallel distributed processing explorations cognition 
cambridge ma mit press 
jockusch ritter self organizing maps local competition evolutionary optimization neural networks vol 
pp 

zhao higuchi evolutionary learning nearest neighbor mlp ieee transactions neural networks vol 
pp 

zhao stable line evolutionary learning nn mlp ieee transactions neural networks vol 
pp 

zhao higuchi efficient learning nn mlp individual evolutionary algorithm neurocomputing vol 
pp 

whitehead cooperative competitive genetic evolution radial basis function centers widths time series prediction ieee transactions neural networks vol 
pp 

whitehead genetic evolution radial basis function coverage orthogonal niches ieee transactions neural networks vol 
pp 

billings zheng radial basis function network configuration genetic algorithms neural networks vol 
pp 

whitehead evolving space filling curves distribute radial basis functions input space ieee trans 
neural networks vol 
pp 

rbf neural network basis functions genetic algorithms proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 
liu yao population learning algorithm learns architectures weights neural networks proc 
workshop soft computing beijing china pp 

singer different voltage dependent thresholds inducing long term depression long term potentiation slices rat visual cortex nature vol 
pp 
september 
hancock smith phillips biologically supported error correcting learning rule proc 
int conf 
artificial neural networks icann vol 
kohonen simula kangas eds pp 
north holland amsterdam 
maynard smith learning guides evolution nature vol 
pp 
october 
hinton nowlan learning guide evolution complex systems vol 
pp 

belew evolution learning culture computational metaphors adaptive algorithms tech 
rep cs computer science engr 
dept 
univ california san diego la jolla ca usa september 
nolfi elman parisi learning evolution neural networks tech 
rep crt center research language university california san diego la jolla ca july 
muhlenbein kindermann dynamics evolution learning genetic neural networks connectionism perspective pfeifer ed pp 
elsevier science publishers amsterdam 
muhlenbein adaptation open systems learning evolution workshop kindermann eds pp 
gmd postfach st augustin germany 
paredis evolution behavior experiments proc 
int conf 
simulation adaptive behavior animals animats meyer wilson eds mit press cambridge ma 
chalmers evolution learning experiment genetic connectionism proceedings connectionist models summer school touretzky elman hinton eds pp 
morgan kaufmann san mateo ca 
bengio bengio learning synaptic learning rule tech 
rep informatique de recherche op universit de montr eal canada november 
bengio bengio optimization synaptic learning rule preprints conference optimality artificial biological neural networks univ texas dallas feb 
meir evolving learning algorithm binary perceptron network vol 
pp 
november 
ackley littman interactions learning evolution artificial life ii sfi studies sciences complexity vol 
langton taylor farmer rasmussen eds reading ma pp 
addison wesley 
baxter evolution learning algorithms artificial neural networks complex systems green eds pp 
ios press amsterdam 
artificial evolution generalized class adaptive processes preprints ai workshop evolutionary computation yao ed pp 
november 
turney whitley anderson ed special issue baldwin effect evolutionary computation vol 
pp 

kim jung kim park fast learning method backpropagation neural network evolutionary adaptation learning rates neurocomputing vol 
pp 

jacobs increased rates convergence learning rate adaptation neural networks vol 
pp 

widrow hoff adaptive switching circuits ire convention record pp 
ire new york ny 
cecconi nolfi neural networks learn environment network vol 
pp 
april 
narayanan lucas genetic algorithm improve neural network predict patient response methods information medicine vol 
pp 

guo uhrig genetic algorithms select inputs neural networks proc 
int workshop combinations genetic algorithms neural networks whitley schaffer eds pp 
ieee computer society press los alamitos ca 
brill brown martin fast genetic selection features neural network classifiers ieee transactions neural networks vol 
pp 
march 
hsu wu input pattern encoding generalised adaptive search proc 
int workshop combinations genetic algorithms neural networks whitley schaffer eds pp 
ieee computer society press los alamitos ca 
chang lippmann genetic algorithms improve pattern classification performance advances neural information processing systems lippmann moody touretzky eds pp 
morgan kaufmann san mateo ca 
feature extraction technique ann financial forecasting neural network world vol 
pp 

applications neural networks genetic algorithms classification cells pattern recognition letters vol 
pp 

back neural networks genetic algorithms bankruptcy predictions expert systems applications international journal vol 
pp 

dellaert vandewalle automatic design cellular neural networks means genetic algorithms finding feature detector proceedings ieee international workshop cellular neural networks applications piscataway nj usa pp 
ieee press 
summers thompson genetic algorithm evolve optimum input set predictive neural network proceedings st iee ieee international conference genetic algorithms engineering systems innovations applications england pp 
iee conference publication 
feature selection classifiers computerized detection mass lesions digital mammography proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 
brown card evolutionary artificial neural networks proceedings canadian conference electrical computer engineering 
part piscataway nj usa pp 
ieee press 

zhang neural networks teach genetic discovery novel examples proc 
ieee international joint conference neural networks ijcnn singapore vol 
pp 
ieee press new york ny 
cho cha evolution neural network training set addition virtual samples proceedings ieee international conference evolutionary computation icec piscataway nj usa pp 
ieee press 
michalewicz krawczyk modified genetic algorithm optimal control problems computers mathematics applications vol 
pp 

fogel applying evolutionary programming selected control problems computers mathematics applications vol 
pp 

yao solving optimal control problems cost changing control evolutionary algorithms proc 
ieee int conf 
evolutionary computation icec indianapolis usa pp 
ieee press new york ny april 
morimoto de hashimoto intelligent approach optimal control fruit storage process neural networks genetic algorithms computers electronics agriculture vol 
pp 

morimoto suzuki hashimoto optimization fuzzy controller fruit storage neural networks genetic algorithms engineering applications artificial intelligence vol 
pp 

path planning agricultural mobile robot neural network genetic algorithm computers electronics agriculture vol 
pp 


chou 
wu bandwidth allocation virtual paths neural networkbased genetic algorithms iee proceedings communications vol 
pp 


chou 
wu parameter adjustment neural network genetic algorithms guaranteed qos atm networks ieice transactions communications vol 
pp 

wolpert mathematical theory generalization complex systems vol 
pp 

rissanen modeling shortest data description automatica vol 
pp 
september 
akaike new look statistical model identification ieee trans 
automatic control vol 
ac pp 
december 
wallace patrick coding decision trees tech 
rep dept computer science monash university clayton victoria australia august 
yao liu darwen best evolutionary learning complex systems local interactions global phenomena jelinek eds amsterdam pp 
ios press 
evolving artificial neural networks combine financial forecasts ieee transactions evolutionary computation vol 
pp 
april 
liu yao designing neural network ensembles evolution parallel problem solving nature ppsn eiben back schoenauer 
schwefel eds vol 
lecture notes computer science berlin pp 
springer verlag 
comparative study combination schemes ensemble digit recognition neural networks proceedings ieee international conference systems man cybernetics 
part piscataway nj usa pp 
ieee press 
trained rules extracted genetic assisted reinforcement learning system ieee transactions neural networks vol 
pp 

araki application evolution strategy hopfield model associative memory proceedings ieee international conference evolutionary computation icec piscataway nj usa pp 
ieee press 
yip 
pao combinatorial optimization guided evolutionary simulated annealing ieee transactions neural networks vol 
pp 

smith solving redundancy allocation problem combined neural network genetic algorithm approach computers operations research vol 
pp 

ara fujita suzuki reconstruction plasma current profile combinatorial optimization techniques ieee transactions plasma science vol 
pp 

rogers johnson optical field scale groundwater remediation neural networks genetic algorithms environmental science technology vol 
pp 

huang 
liu object recognition genetic algorithms hopfield neural model expert systems applications vol 
pp 


dube hybrid approach vehicle routing neural networks genetic algorithms applied intelligence vol 
pp 

dagli new approaches nesting rectangular patterns journal intelligent manufacturing vol 
pp 

lee dagli parallel genetic neuro scheduler job shop scheduling problems international journal production economics vol 
pp 

optimizing fiber production process combined neural network genetic algorithm approach textile research journal vol 
pp 


han may neural network process models perform silicon dioxide recipe synthesis genetic algorithms ieee transactions semiconductor manufacturing vol 
pp 
may 
nishikawa evolutionary neural network algorithm max cut problems proceedings ieee international conference neural networks 
part piscataway nj usa pp 
ieee press 
weiss combining neural evolutionary learning aspects approaches tech 
rep institut fur informatik technische universitat munchen may 
