design implementation ddh distributed dynamic hashing algorithm robert devine university california berkeley eecs department computer science division devine cs berkeley edu 
ddh extends idea dynamic hashing algorithms distributed systems 
ddh spreads data multiple servers network novel autonomous location discovery algorithm learns bucket locations centralized directory 
describe design implementation basic ddh algorithm networked computers 
performance results show prototype ddh hashing roughly equivalent conventional single node hashing implementations compared cpu time elapsed time 
possible improvements suggested basic ddh algorithm increased reliability robustness 
rapidly hardware costs increasing performance cpus networks mean file database systems constructed networked clusters nodes 
algorithms devised environments 
describes design implementation distributed hashing algorithm 
quick record retrieval obviously crucial file database performance 
design efficient retrieval algorithms rich research area computer science earliest days 
hashing algorithms classified static dynamic 
static hash algorithm uses constant sized hash table 
dynamic hash algorithm differs static hash algorithm table grow shrink initially allocated size accommodate continued insertion deletion records 
ddh distributed dynamic hashing algorithm gracefully expands contracts central controller 
variable number clients servers participate performance conventional single node hashing determined cpu processing time number os needed 
distributed hashing performance messages considered 
messages ddh possible shown minor local hint directory provide mapping hash buckets servers 
organized follows design implementation ddh sections show performance results section conclude section 
goals distributed hashing goals desired distributed hash algorithms listed 
high performance course general goal 
goals certainly possible section suggests 
goals evaluate develop distributed algorithms 
section tells goals realized ddh 

location distributed buckets servers location requirements met 
method needed locating candidate nodes eligible participate distributed hash structure 
second requirement map hash bucket server run time 
restrict selection nodes multiples access locally mounted disk nodes milliseconds average network time away 
nodes lan 
shown computers internet wide area network mean response latency milliseconds 
fact argues having widely distributed hash tables concerns administration boundaries network overload especially bucket splits merges argue extending hash table distributed locally administered boundaries 
directory service identify nodes participate converting names network address 
information relatively static changes happen human time scale nodes added removed 
simple directory services ill suited dealing second requirement tracking rapidly changing bucket location information 
multiple directory services provide consistent view bucket mappings 
bucket splits merges happening execution speed bucket directory problem handled dynamically 

collision resolution possible policies handling filled buckets 
bucket temporarily expanded records overflow overflow bucket buddy bucket bucket immediately split 
effect choosing collision resolution methods reflected performance efficiency algorithm 
overflows employed bucket high rate collisions bucket unbalanced respect average bucket 
alternately policy splitting evens load may cause imbalance ways increased load single server 
expanding bucket size undesirable leads complex algorithms 
requiring permission splitting means agreement achieved non local information 
splitting immediately simpler may cause complication areas notably bucket location algorithm 

load balancing servers ideally server responsible proportional share total data size 
principle enforced actively checking local load level global load level performing adaptive load balancing ameliorate hot spots 
naive approach designate load controller partitions servers fairness criteria 
need scalability obviates naive approach 
centralized solution soon bottleneck hash table grows size 

parallel insertion retrievals desirable support multiple concurrent accesses distributed hash table 
multiple readers easily supported changes occurring 
mixture multiple readers writers servers institute consistency control locking serialize access portion hash table 
individual server necessary consistency protocols guarantee correctness multiple clients inserting records 
addition insertion causes split affected servers mutually ensure consistency parts distributed table 
previous multiple dynamic hashing methods proposed single nodes single processor multiple processors 
lh proposal distributed hashing algorithm 
extends notion linear hashing allow multiple nodes network participate distributed data structure 
lh algorithm single node version linear hashing 
locates buckets algorithms current split level bucket number 
client maintains thinks current split level highest numbered bucket 
table grow shrink client knowing actual bucket may located 
proposed lh drawbacks server bucket 
limit required bucket location calculations 
easily overcome logical server numbering scheme maps actual server number round robin assignment fine 
larger problem single bucket server implies large buckets result high cost split single bucket server hold multiple small buckets expensive split 
second splits ordered clients 
clients directory allow unbalanced splitting splits required follow bucket numbering order level 
bookkeeping ordering requires server send message needed just convey split records 
overhead sending agreement messages split problem acute dealing failures 
message controlling ordering lost server owns current split token crashes servers affected split new token regenerated 
analogous complexity token ring networks compared ethernet 
third multiple clients inserting records exists timing window possible client requires expected maximum forwarding messages 
client slower rate bucket splittings client view lh hash table lag actual configuration 
true hash levels exist single time client actions time synchronous 
may see versions hash table evolves 
version may cause client addressing errors 
fourth important determining bucket split autonomous decision affected server 
ordering splits imposed buckets support character 
restriction inherent lh need buckets determined bucket location functions 
undesirable characteristics result 
strictly order splits proposes special split coordinator participates bucket split operation 
contrary goals load balance high availability 
second buckets allowed immediately split handle overflows locally 
leads poorer performance hot spots 
buckets level split level start split causes premature splitting non full buckets 
distributed dynamic hashing section introduce design ddh 
guiding philosophy ddh local autonomy 
changing global information guides actions individual servers server decides split merge buckets 
clearly common constant policy shared serves avoid anarchy policy constant easily coded servers 
interesting research question minimum level shared policy required preserve collaborative effort necessary distributed data structure 
ddh server program runs server node participating distributed hash algorithm 
responsible controlling storage portion entire table 
server maintains information bucket bucket number split level contents bucket 
server responsible handling messages bucket may receive 
includes forwarding requests server sending appropriate response request analyzing replies requests servers 
ddh servers maintains small local directory store bucket server mapping 
ddh clients need directory advantageous performance results section show 
client calls ddh library routines insert retrieve operation 
client program computes hash key record locates bucket hash key sends request server owns bucket 
server maintains directory containing location information server buckets 
directory exact remote buckets gives location 
receiving request concerning data item client server determines correct recipient message comparing data item key key range buckets 
server discovers right forwards message correct recipient 
operation performed locally 
reply sent server client indicating operation successful 
reply contains current hash level bucket operation eventually performed 
allows client apply ddh algorithm update local perception hash table 
distributed hash table distributed hash table ddh distributed main memory data structure composed buckets spread servers 
bucket holds fixed number records 
required servers bucket size server processes node expected usual arrangement server node best performance 
client processes send requests servers insert retrieve record 
bucket addressing binary radix trees tries 
split level lowest order bits form bucket number 
example suppose record key hashes value 
hex binary current level bucket number 
level bucket number 
level record bucket number hex 
syntax bucket numbering written pair level bucket number 
algorithm begins single bucket numbered bucket matches hash values 
fills contents split buckets bit hash value buckets 
general level bucket split uses bit hash value move records child buckets 
bucket splits forms children buckets 
bucket server location bucket location described tuple number servers server distribution function hash function key current hash table configuration 
items defined entire table known servers clients 
client chooses hash function generates key record 
fifth field current hash table configuration changed insert delete operation known definitively client servers involved operation nodes discover configuration 
server starting knows logical server number total number servers 
fresh client location knowledge starts server bucket bucket existence guaranteed 
client progressively learns current configuration making requests correct wrong come back correct location information 
client sends requests server believes owns bucket adjust directory told guess incorrect 
approaches possible client bucket server mappings learned 
client maintain directory mappings learned previous messages 
messages directory select servers 
second client heuristics guide guesses 
example current average split level information client gleaned previous message replies client produce reasonable guess level message 
performance trade offs compared section 
likewise approaches possible implement address correction protocol 
client incorrect guesses returned retry different server server silently forward message correct server 
case client expects reply bounded time time resend message better client retry avoid time outs caused long server forwarding 
ddh chooses method requiring server forward messages 
collision resolution bucket filled split bucket entirely local decision 
linear hashing order bucket splitting necessary 
extensible hashing entire table split operation 
splitting autonomous operation require global knowledge 
skewed data efficiently handled minimum number split buckets special overflow areas 
implementation ddh practices uncontrolled splitting policy 
splitting server sends message new site server manage new bucket 
server accepts series messages sent server split records 
multiple records sent split message performance optimization 
load balancing data servers ddh lh index split manager 
buckets small ddh achieves fine grained load sharing servers 
starting requests go bucket number buckets increases servers soon get roughly number buckets 
parallel operations multiple clients insert retrieve records parallel 
viewpoint message reception servers operate atomically 
clients see internal state servers inconsistent operations may result 
consistency points defined servers send receive messages 
server operations locally serialized complete processing message starting 
multiple servers synchronize behavior dealing page splits 
ddh networking implementation basic services offered ddh session layer implementation packet encoding decoding retransmission response matching 
simple data types packet rebuilt time sent 
purely homogeneous clusters systems ddh implemented portable session level service 
communication ddh servers uses internet user datagram protocol udp datagram service messages encoded network byte order 
ddh network protocol designed request response model 
requests messages sent clients servers servers servers bucket splits merges 
request sent wrong server server puts network address requester inside packet forwards correct server 
receiving server uses address send back reply directly requesting client 
network failures may cause packets lost operations self contained idempotent 
messages retransmitted reply received time 
packet consists fixed format header followed specified number key value pairs 
key value preceded short word containing length 
integer values converted necessary transport network byte order 
request messages supported retrieve insert delete split bucket 
servers send split bucket requests servers performing split merge operation prepare bulk data transfer insert messages 
requests result responses error notfound success 
performance experiments show implementation distributed hashing application phone service white pages lookup finding phone numbers 
large number names phone number returned 
pairs names phone numbers 
name bytes long served key field 
compare performance hashing packages 
ddh distributed hashing algorithm introduced 
single node hashing package 
single node linear hashing package 
single node dynamic hashing package gnu freeware environment multiple decstation workstations approximately machine ample memory megabytes 
systems running ultrix variant bsd unix 
workstation communicated megabits second ethernet megabits second fddi lan 
program execution timed operating system system call record actual resources 
timer started calls hash packages stopped immediately remove consideration normal process initialization overhead performance measurement 
tests run twice allow code pages fetched disk start overhead affected area 
run idle systems hours minimize possible interference network 
testing ran multi user level single user interference system level daemons running background possible 
necessary run normal level network 
time insert retrieve records records loaded different hash packages 
phase test record test read disk file hashed inserted table 
second phase records reread record retrieved 
note high probability finding record memory single node hashing packages 
times total time insert retrieve records 
elapsed time needed run tests complete time needed run program shell prompt 
values shown times quite large dominated graph 
graph shows ddh comparable hashing packages operate single process node 
experiment disk hashing package best light disk blocks memory 
os performed flushing blocks disk 
contrast ddh numbers best worst messages sent remote node case 
surprises performance results 
widely different elapsed times package package flush pages explicitly called sync system call flush directory pages ultrix file system buffer cache disk 
addition package extensible hashing algorithm doubled directory size periodically 
explains knee curves name pairs point 
user time ddh system time ratio roughly 
ddh run system operating system higher performing messaging elapsed level lower 
ddh elapsed time seconds thousands name phone pairs fig 

elapsed time insert retrieve records bucket location strategies question ddh method autonomous splitting causes clients frequent addressing mistakes examined 
experiments round robin distribution map buckets servers possible 
number server messages caused addressing errors related number split levels bucket may split new server current state unknown probe 
server locating strategies compared 
table shows number messages result addressing errors fresh client retrieving records bucket hash table records stored buckets holding maximum entries servers 
experiment servers value non multiple causes bucket split send records different server 
heuristic algorithms guess bucket split level 
simple heuristic just uses previous reply split level guess request split level 
complex heuristic uses moving average previous table 
bucket addressing errors strategy extra messages buckets heuristic simple heuristic complex incremental quick start replies average gradually converge actual size 
heuristics assume hash table reasonably balanced 
linear hashing store definite information hash table 
constantly mistakes produce errors directory strategies 
mistakes result individual record access directory strategies bucket granularity 
client incremental convergence algorithm maintains directory definitive bucket locations client receives servers 
additionally location bucket inferred definitive knowledge sibling bucket 
strategy incorrectly guess bucket addresses table approximately cases correctly deduce 
bucket determined sibling known 
accounts 
remaining buckets doesn know worst case half located node half different node 
quick start algorithm similar incremental convergence special probe message returns statistics server hash table client starts 
statistics construct complete directory slow learning incremental strategy avoided 
resulting directory completely accurate advantage accurate errors 
strategy bucket addressing errors able completely guess majority current directory constructing directory current depth 
wrong fringes incorrectly assumed higher split level 
strategies possible propose examples produce poor results 
timing windows invalidate previous knowledge result bucket bucket 
realworld cases cause performance 
ability simple client directory produces results 
number messages just main memory hashing judged number memory accesses disk hash algorithm judged number os uses distributed hashing algorithm show messages 
experiment average number messages needed insert retrieve records 
ddh comes close optimal messages 
number messages calculated messages request request reply fi fl request reply messages normal user visible messages 
ddh message protocol requires acknowledgment server client message received 
algorithms assume reliable messages count user visible messages fail account low level acknowledgments enforce message reliability 
low level network messages represented includes messages invisible user code low level flow control network name resolution messages message retransmissions 
user level counted affect performance 
effect bucket splits fi term counts number bucket splits occur servers bucket fills 
inverse function bucket size advantageous message size slightly roughly half bucket size migrated records fit message 
fl term counts addressing errors occurrences forwarded messages servers client asks wrong server due change configuration distributed hash table 
nonlinear function number servers amount data server autonomously decide split bucket multiple clients inserting retrieving records means clients addressing errors 
addressing errors resulting period client learning current hash table 
section measured 
table shows actual counts addressing errors 
messages quick start algorithm 
measured number messages sent average ddh request close optimal message total 
network performance ddh synchronous request response protocol communication udp messages 
result network bottleneck 
performance monitoring showed cpu utilization rate low mid level client side 
amount approximately time spent system time kernel sent received udp messages 
common belief raw speed fddi mbps times faster dimensions mbps ethernet 
fddi produces better response time 
proportion user system time fddi ethernet 
suggests network software layer quite heavyweight 
discover network performance affected ddh performance comparison measurement done ttcp network performance analysis tool 
udp protocol sending byte packets imitate average message size ddh study cpu nearly busy 
workstations study send slightly udp packets second maximum 
ultrix kernel higher code length networking calls file system calls 
comparison read write calls second possible 
modern microkernel oss perform small message rpc call millisecond 
performance strong advantage distributed hashing ability involve multiple servers share load 
series experiments conducted test effect adding servers 
group experiments quantify speed adding servers 
speed means increasing available performance keeping workload constant yield times speedup 
record inserted retrieved 
table 
speed performance elapsed time records server client servers client servers client seconds seconds seconds table exhibit increase speed client uses servers 
marginal speed second server 
explanation minimal speedup messages synchronous 
message throughput bottleneck added server capacity unusable client waits 
second group experiments looked scale increasing workload commensurate increased available performance 
table lists resultant scale 
table shows ddh scalability load capacity re doubled 
ddh design scale exists higher multiples server client pairs 
experiments performed remains unproven 
table 
scale performance elapsed time records server client servers clients seconds seconds growing number networked systems share information distributed manner argument distributed data structure compelling 
freedom single node limitations easy scalability better performance goals 
comparison single node hash table distributed table theoretically allows growth table composite size workstation memory hash table forced reside disk 
ddh distributed dynamic hashing algorithm implemented group workstations quantify benefit distributed solution 
best environment ddh group servers administrative control directly indirectly shared architecture interference systems diminish performance 
systems organized workstation farms best match needs ddh provide network communication protocols low latency 
performance results show comparable current implementations measured cpu time elapsed time 
ddh sends messages systems network bottleneck 
largest test records test required approximately network messages insert retrieve records 
single network message takes milliseconds elapsed time fast workstations ethernet segment cumulative elapsed time quite high 
cost distributed access better disk access far worse memory access 
area distributed data structures introduces challenging research problems areas concurrency control algorithm server failure supporting variable number servers 
servers fail method data redundancy server redundancy employed 
drawback increased complexity dealing replication data consistency 
actual case may complex due server failures independent 
dynamic hashing freed hashing fixed sized hash tables 
distributed hashing expands hashing single node 
logical step allow varying number servers 
question possible preserve retrieval message 
summary find ddh offers useful approach structuring distributed storage systems 
ddh prove useful data stored exceeds size single system memory 
network latency latency disk request huge single table accessed multiple servers better performance 
ddh tolerant skewed data dynamic hashing methods allows bucket splitting demand 

dbm unix programmer manual system pp 


ronald fagin nievergelt nicholas pippenger raymond strong extensible hashing fast access method dynamic files acm transactions database systems volume pp 
september 

richard golding accessing replicated data large scale distributed system university california santa cruz technical report june 

hsiao david dewitt chained declustering new availability strategy multiprocessor database machines proceedings th international conference data engineering february 

larson dynamic hashing bit vol 
pp 


witold litwin linear hashing new tool file table addressing proceedings th international conference vldb october 

witold litwin marie anne donovan schneider lh linear hashing distributed files proceedings acm sigmod 
may 

john postel user datagram protocol usc information sciences institute internet rfc august 

margo seltzer ozan new hashing package unix usenix conference proceedings winter january 

wolberg distributed linear hashing parallel projection main memory databases proceedings th international conference vldb brisbane australia 
article processed macro package llncs style 
