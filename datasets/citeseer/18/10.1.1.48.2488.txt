learning irrelevant features hussein almuallim thomas dietterich hall department computer science oregon state university corvallis cs orst edu tgd cs orst edu domains appropriate inductive bias min features bias prefers consistent hypotheses definable features possible 
defines studies bias 
shown learning algorithm implementing min features bias requires theta ffl ln ffi ffl ln training examples guarantee pac learning concept having relevant features available features 
bound logarithmic number irrelevant features 
presents quasi polynomial time algorithm focus implements min features 
experimental studies compare focus id fringe algorithms 
experiments show contrary expectations algorithms implement approximations min features 
coverage sample complexity generalization performance focus substantially better id fringe learning problems min features bias appropriate 
suggests practical applications training data preprocessed remove irrelevant features id fringe 
historically development inductive learning algorithms step process select representation scheme decision trees ii develop algorithm find instances scheme consistent collections training examples 
shortcoming approach separation specification desired learning behavior algorithm implementation 
specifically bias algorithm adopted implicitly particularly side effect second step 
difficult state bias simple way 
consequently difficult tell advance bias appropriate new learning problem 
authors buntine wolpert advocated different procedure adopt bias space hypotheses equivalently select prior probability distribution space ii select scheme representing hypotheses space iii design algorithm implements bias approximately 
goal pursue second procedure 
consider space binary functions defined boolean input features 
adopt bias call min features bias functions consistent training examples prefer function involves fewer input features break ties arbitrarily 
bias favor simplicity mere syntactic simplicity 
functions fewer variables semantically simpler functions variables 
adopting straightforward representation binary functions defined input features 
analyze sample complexity probably approximately correct pac learning algorithm implements min features bias 
proved theta ffl ln ffi ffl ln training examples required pac learn binary concept involving input features space input features accuracy parameter ffl confidence parameter ffi note bound total number available features appears logarithmically 
irrelevant features costs factor ln training examples detect eliminate consideration 
analysis simple quasi polynomial time algorithm implements min features bias described analyzed 
algorithm called focus identifies features needed define binary function 
applies straightforward learning procedure focuses just features 
glance may appear algorithms approximate bias 
example id quinlan bias favor small decision trees small trees test subset input features 
final section experiments comparing focus id fringe pagallo haussler 
demonstrate id fringe implementations min features bias algorithms produce hypotheses output complex terms number input features hypotheses focus 
cases id fringe extremely simple hypotheses 
results suggest focus algorithm require fewer training examples generalize correctly id domains bias appropriate 
believe domains 
example practical applications known exactly input features relevant represented 
natural response users include features believe possibly relevant learning algorithm determine features fact worthwhile 
situation irrelevant features may body training data learn different binary functions 
cases ensure set features measured data sufficient learn target functions 
learning individual function small subset features relevant 
applies example task learning diagnosis rules different diseases medical records large number patients 
records usually contain information required describing disease 
example littlestone involves pattern recognition tasks feature detectors automatically extract large number features learner consideration knowing prove useful 
notation fx delta delta delta xng denote set boolean features xn denote set assignments features set instances 
binary concept subset xn binary function represents concept 
course binary functions represented boolean formulas 
feature said relevant concept appears boolean formula represents irrelevant 
complexity concept denoted defined minimum number bits needed encode concept respect encoding scheme 
encoding scheme introduced section 
denote set binary concepts complexity defined fx delta delta delta xng 
assume arbitrary probability distribution xn ffl concept said ffl close concept respect sum probability instances symmetric difference ffl 
function represents concept xn value said class pre classified example pair form hx sample concept multi set examples drawn randomly replacement size sample just number instances drawn 
adopt notion probably approximately correct pac learning defined blumer 

respect parameters ffl ffi ffl ffi say learning algorithm pac learns simply learns concept sample size probability gamma ffi algorithm returns hypothesis concept ffl close algorithm sample size fixed unknown formal analysis section define min features bias 
investigate sample complexity algorithm implements min features 
focus algorithm implements bias analyze computational complexity 
min features bias stated simply 
training sample unknown binary function xn set binary functions xn consistent 
called version space mitchell 
subset elements fewest relevant features 
min features bias chooses guess arbitrarily 
wish implement analyze min features bias step choose representation hypotheses 
represent concept concatenation bit vectors bit vector ith bit irrelevant rightmost column truth table boolean function represents defined features fx delta delta delta xng corresponding bits set 
definition analyze sample complexity min features number training examples required ensure pac learning 
define complexity measure corresponding bias 
blumer 
define complexity concept number bits needed encode bit vector representation 
measure property iff number relevant features number relevant features specifically relevant features section blumer 
gives properties satisfied reasonable representation concepts 
reader may verify satisfied method encoding 
example concept represented 
complexity 
theorem gives upper bound sample complexity algorithm implementing min features bias 
theorem class concepts defined features complexity probability distribution ffl ffi ffl ffi concept sample size ffl ln ffi ffl log gamma ln gamma sufficient guarantee algorithm implementing min features bias return hypothesis ffl close probability gammaffi 
proof sketch target concept complexity hypothesis space algorithm implements min features bias contained argue jc gamma log gamman delta gamman result follows immediately applying lemma blumer 
interesting note number examples sufficient learning grows logarithmically number irrelevant features linearly complexity concept 
show bound tight exhibiting identical lower bound methods developed blumer 
exploiting dimension vc dimension 
vc dimension class concepts defined largest integer exists set instances labelled concepts possible ways 
shown number examples needed learning class concepts strongly depends vc dimension class 
specifically ehrenfeucht prove theorem class concepts ffl ffi 
algorithm pac learns concepts respect ffl ffi probability distribution sample size omega ffl ln ffi cdim ffl apply result determine set boolean concepts features having complexity equal lemma theorem 
cdim max ae log gamma log gamma oe proof result lengthy omitted lack space 
state lower bound algorithm focus sample 
delta delta delta fx delta delta delta xng size exist examples sample agree features agree class go 
return concept consistent sample features relevant focus learning algorithm theorem conditions theorem algorithm pac learns sample size omega ffl ln ffi ffl ln gamma ln gamma results show presence irrelevant features learning task substantially difficult terms number examples needed learning sample complexity grows logarithmically number irrelevant features 
analyzed sample complexity min features bias exhibit algorithm implements bias 
algorithm searches returns consistent hypothesis minimal set attributes implements desired bias 
determine computational complexity focus suppose sample size concept complexity condition inner loop tested maintaining array length jaj entry possible assignment features example sample check values features example label corresponding entry array class example 
process label entry reversed result test false 
result true 
cost time delta jaj 
target concept complexity value jaj reach log gamma 
outer loop executed gamma log gamman delta log gamman times 
computational complexity algorithm dominated nested loops algorithm terminate time log gamman 
clearly impractical large values definition learnability blumer says class boolean concepts complexity measure learnable polynomial number examples quasi polynomial time 
analogous result minimum number terms needed encode concept dnf formula complexity measure obtain learnability result polynomial sample size time 
result shown uniform distribution case applies distributions 
experimental learning algorithms appear biases similar min features bias 
particular algorithms related id quinlan attempt construct small decision trees 
algorithms construct decision tree top starting root terminate soon find tree consistent training examples 
features tested node chosen estimated relevance target concept measured mutual information criterion 
section test algorithms see implement min features bias 
particular compare algorithms id described quinlan resolving ties randomly features look equally 
ii fringe pagallo haussler maximum number iterations set 
iii focused id features sufficient produce consistent hypothesis obtained focus 
finding minimal size subset relevant features training examples filtered remove irrelevant features 
filtered examples id construct decision tree 
consider evaluation criteria coverage sample complexity error rate 
coverage learning algorithm measure number distinct concepts learned training sample size precisely consider collection training samples containing distinct examples concept suppose give samples algorithm fraction gamma ffi training samples outputs function ffl close correct concept say frequently approximately correctly fac learns dietterich 
coverage algorithm ffl ffi number concepts fac learned algorithm 
sample complexity algorithm space concepts estimated smallest sample size sufficient enable fac learn concept equivalent sample complexity pac learning measured uniform distribution instances drawn replacement 
error rate algorithm concept measured probability randomly chosen example misclassified hypothesis output algorithm assuming uniform distribution space examples 
objective evaluate learning performance respect min features bias specialized criteria manner 
concepts relevant features counted coverage algorithm concepts fewer features fac learned 
condition included exist trivial algorithms attain high coverage learning uninteresting concepts 
second sample complexity measurement choose class concepts fewer relevant features 
measuring error rates algorithms target concept chosen randomly concepts having fewer features 
technical problem performing experiments immense amount computation involved exact measurement coverage sample complexity number features large 
employed techniques reduce computational costs measurements 
exploited fact algorithms symmetric respect permutations negations input features 
precisely algorithm fac learns concept represented boolean function xn algorithm learns concepts represented xn xn functions obtained permuting negating features symmetry properties partition space concepts equivalence classes suffices test representative concept equivalence class determine fac learnability concepts class 
second measured statistically running algorithm large number randomly chosen samples depending experiment 
number observed large reliably determine fac learnability concepts 
experimental results experiment coverage 
experiment measured min features coverage algorithms 
algorithm counted number concepts learned order function size sample total number features learning parameters ffl ffi varying 
number samples tested concept 
shows result 
results similar 
experiment sample complexity 
experiment estimated sample size needed learn concepts having fewer relevant features total available features 
counting techniques employed find number equivalence classes number concepts equivalence class see harrison slepian 
number examples focus id fringe coverage algorithms 
ffl ffi 
number samples tested concept 
results table 
features id fringe focus experiment error rates 
previous experiments looking worst case performance learning algorithms 
reasonable sample size algorithm may learn concepts consideration exception require substantial increment sample size 
algorithm exhibit poor performance previous experiments 
purpose experiment perform kind average case comparison algorithms 
procedure plot learning curve randomly chosen concepts relevant features 
randomly selected concepts having relevant features 
concepts measured accuracy hypotheses returned algorithms successively increasing sample size 
value accuracy rate averaged randomly chosen samples 
experiment performed 
randomly chosen concepts relevant features tested value shows pattern typical learning curves observed 
concepts examples mean difference accuracy rate focus id variance 
mean difference focus fringe variance 
experiment irrelevant features 
goal experiment see algorithms influenced additional irrelevant features values assigned random 
number examples focus id fringe learning curve randomly chosen concept relevant features 
number total attributes focus id fringe accuracy algorithms randomly chosen concept sample pair irrelevant random features introduced 
sample size 
purpose choose concept sample pair random measure accuracy hypothesis returned algorithm adding irrelevant features sample 
concepts chosen relevant features 
sample size chosen algorithms reasonably accurate tested starting features 
sample size chosen randomly augmented successively adding random features bring total number features 
value accuracy averaged runs 
experiment repeated concept sample pairs 
typical result runs shown 
discussion experiments show conclusively biases implemented id fringe may interesting appropriate domains approximations min features bias 
final experiment shows directly 
min features bias focus maintains constant high level performance number irrelevant features increased 
contrast performance id fringe steadily degrades 
occurs id fringe proposing hypotheses involve extra features different features identified focus 
explains results experiments 
experiment see training examples required id fringe find hypotheses 
extra training examples needed force algorithms discard irrelevant features 
means fixed sample size id fringe learn fewer concepts respect min features bias shown experiment 
experiment shows min features bias appropriate focus give better generalization performance id fringe 
defined studied min features bias 
section tight bound number examples needed guarantee pac learning algorithm implements min features 
introduced focus algorithm implements min features calculated computational complexity 
section demonstrated empirically id fringe algorithms provide implementations min features bias 
consequence id fringe perform nearly focus problems min features bias appropriate 
results suggest rely id fringe filter irrelevant features 
technique employed eliminate irrelevant features focus id fringe relevant ones 
problems research 
need develop test efficient heuristics finding set relevant features learning problem 
analysis performed ensure heuristics near optimal sample complexity 
second need address problem determining relevant features training data noisy 
third efficient variant focus tested realworld learning problems min features bias believed appropriate 
authors gratefully acknowledge support nsf number iri 
hussein almuallim supported scholarship university petroleum minerals arabia 
nick flann helpful comments 
buntine 
myths legends learning classification rules 
proceedings eighth national conference artificial intelligence aaai 
boston ma morgan kaufmann 
blumer ehrenfeucht haussler warmuth 
learnability dimension technical report department computer information sciences university california santa cruz nov 
journal acm 
blumer ehrenfeucht haussler warmuth 
occam razor 
information processing letters 
dietterich 
limitations inductive learning 
proceedings sixth international workshop machine learning 
ithaca ny morgan kaufmann 
ehrenfeucht haussler kearns valiant 
general lower bound number examples needed learning 
proceedings workshop computational learning theory 
boston ma morgan kaufmann 
harrison 
switching automata theory 
mcgraw hill littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
mitchell 
generalization search 
artificial intelligence 
pagallo haussler 
boolean feature discovery empirical learning 
machine learning 
quinlan 
induction decision trees machine learning 
slepian 
number symmetry types boolean functions variables 

math 

learning dnf uniform distribution quasi polynomial time 
proceedings third workshop computational learning theory 
rochester ny morgan kaufmann 
wolpert 
mathematical theory generalization parts ii 
complex systems 
