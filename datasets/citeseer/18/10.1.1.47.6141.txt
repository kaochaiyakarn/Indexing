supervised unsupervised discretization continuous features james dougherty ron kohavi mehran sahami computer science department stanford university stanford ca 
ronnyk cs stanford edu armand prieditis stuart russell eds machine learning proceedings twelfth international conference morgan kaufmann publishers san francisco ca 
supervised machine learning algorithms require discrete feature space 
review previous continuous feature discretization identify defining characteristics methods conduct empirical evaluation methods 
compare binning unsupervised discretization method entropy purity methods supervised algorithms 
performance naive bayes algorithm significantly improved features discretized entropy method 
fact tested datasets discretized version naive bayes slightly outperformed average 
show cases performance induction algorithm significantly improved features discretized advance experiments performance significantly degraded interesting phenomenon considering fact capable locally discretizing features 
algorithms developed machine learning community focus learning nominal feature spaces michalski stepp kohavi 
real world classification tasks exist involve continuous features algorithms applied continuous features discretized 
continuous variable discretization received significant attention machine learning community 
uniform binning data produce necessary data transformations learning algorithm careful study discretization affects learning process performed weiss kulikowski 
decision tree methods quinlan continuous values discretized learning process 
advantages discretizing learning process shown 
include comparison 
reasons variable discretization aside algorithmic requirements mentioned include increasing speed induction algorithms catlett viewing general logic diagrams michalski induced classifier 
address effects discretization learning accuracy comparing range discretization methods naive bayes classifier 
naive bayes classifier implemented mlc kohavi john long manley pfleger described langley iba thompson 
different axes discretization methods classified global vs local supervised vs unsupervised static vs dynamic 
local methods exemplified produce partitions applied localized regions instance space 
global methods busse binning produce mesh entire dimensional continuous instance space feature partitioned regions independent attributes 
mesh contains regions number partitions ith feature 
discretization methods equal width interval binning instance labels discretization process 
analogy supervised versus unsupervised learning methods refer unsupervised discretization methods 
contrast discretization methods utilize class labels referred supervised discretization methods 
believe differentiating static dynamic discretization important 
discretization methods require parameter indicating maximum number intervals produce discretizing feature 
static methods binning entropy partitioning catlett fayyad irani pfahringer algorithm holte perform discretization pass data feature determine value feature independent features 
dynamic methods conduct search space possible values features simultaneously capturing interdependencies feature discretization 
believe methods promising avenue research pursue methods 
related feature discretization section 
section describe detail methods comparative study discretization techniques 
explain experiments results section 
section reserved discussion summary 
related simplest discretization method equal interval width merely divides range observed values variable equal sized bins user supplied parameter 
catlett points type discretization vulnerable outliers may drastically skew range 
related method equal frequency intervals divides continuous variable bins instances bin contains possibly duplicated adjacent values 
unsupervised methods utilize instance labels setting partition boundaries classification information lost binning result combining values strongly associated different classes bin kerber 
cases effective classification difficult 
variation equal frequency intervals maximal marginal entropy adjusts boundaries decrease entropy interval wong chiu 
holte simple example supervised discretization method 
algorithm attempts divide domain continuous variable pure bins containing strong majority particular class constraint bin include prespecified number instances 
method appears reasonably conjunction induction algorithm 
chimerge system kerber provides statistically justified heuristic method supervised discretization 
algorithm begins placing observed real value interval proceeds test determine adjacent intervals merged 
method tests hypothesis adjacent intervals statistically independent making empirical measure expected frequency classes represented intervals 
extent merging process controlled threshold indicating maximum value warrants merging intervals 
author reports random data high threshold set avoid creating intervals 
method statistical tests means determining discretization intervals proposed 
similar flavor chimerge bottom method creates hierarchy discretization intervals phi measure criterion merging intervals 
general chimerge considers merging adjacent intervals time user set parameter just adjacent intervals time chimerge 
merging intervals continues phi threshold achieved 
final hierarchy discretizations explored suitable final discretization automatically selected 
number entropy methods come forefront discretization 
chiu cheung wong proposed hierarchical discretization method maximizing shannon entropy discretized space 
method uses hill climbing search find suitable initial partition continuous space bins axis re applies method particular intervals obtain finer intervals 
method applied primarily information synthesis task bears strong similarities discretization machine learning researchers 
catlett explored entropy discretization decision tree domains means achieving impressive increase speed induction large data sets continuous features 
uses conditions criteria stopping recursive formation partitions attribute minimum number samples partition partitions minimum information gain 
fayyad irani recursive entropy min global local rd holte adaptive quantizers chimerge kerber vector quantization supervised catlett hierarchical maximum entropy fayyad irani ting fayyad irani supervised mcc predictive value max 
equal width interval unsupervised equal freq 
interval means clustering unsupervised mcc table summary discretization methods heuristic discretization couple minimum description length criterion rissanen control number intervals produced continuous space 
original method applied locally node tree generation 
method quite promising global discretization method ting method global discretization 
pfahringer uses entropy select large number candidate split points employs best search minimum description length heuristic determine discretization 
adaptive quantizers chan srinivasan method combining supervised unsupervised discretization 
begins binary equal width interval partitioning continuous feature 
set classification rules induced discretized data id algorithm tested accuracy predicting discretized outputs 
interval lowest prediction accuracy split partitions equal width induction evaluation processes repeated performance criteria obtained 
method appear overcome limitations unsupervised binning high computational cost rule induction process repeated numerous times 
furthermore method implicit assumption high accuracy attained 
example random data system splits post processing step needs added 
bridging gap supervised unsupervised methods discretization van de developed methods general heading contrast criterions mcc 
criterion dubbed unsupervised author unsupervised clustering algorithm seeks find partition boundaries produce greatest contrast contrast function 
second method referred mixed supervised unsupervised simply redefines objective function maximized dividing previous contrast function entropy proposed partition 
calculating entropy candidate partition requires class label information method thought supervised 
busse taken similar approach cluster method find candidate interval boundaries applying consistency function theory rough sets evaluate intervals 
predicative value maximization algorithm weiss galen tadepalli supervised discretization method finding partition boundaries locally maximal predictive values correct classification decisions 
search boundaries begins coarse level refined time find locally optimal partition boundaries 
dynamic programming methods applied find interval boundaries continuous features fulton kasif salzberg 
methods pass observed values data identify new partition continuous space intervals identified point 
general framework allows wide variety impurity functions measure quality candidate splitting points 
maass introduced dynamic programming algorithm finds minimum training set error partitioning continuous feature log time number intervals number instances 
method tested experimentally 
vector quantization kohonen related notion discretization 
method attempts partition dimensional continuous space voronoi tessellation represent set points region region falls 
discretization method creates local regions local discretization method 
alternatively thought complete instance space discretization opposed feature space discretizations discussed 
table shows summary discretization methods identified global local supervised unsupervised dimensions 
methods static 
methods study consider methods discretization depth equal width intervals rd method proposed holte algorithm entropy minimization heuristic fayyad irani catlett 
equal width interval binning equal width interval binning simplest method discretize data applied means producing nominal values continuous ones 
involves sorting observed values continuous feature dividing range observed values variable equally sized bins parameter supplied user 
variable observed values bounded xmin xmax method computes bin width ffi xmax gamma xmin constructs bin boundaries thresholds xmin iffi gamma 
method applied continuous feature independently 
instance class information whatsoever unsupervised discretization method 
holte holte describes simple classifier induces level decision trees called decision stumps iba langley 
order properly deal domains contain continuous valued features simple supervised discretization method 
method referred rd sorts observed values continuous feature attempts greedily divide domain feature bins contain instances particular class 
scheme possibly lead bin observed real value algorithm constrained forms bins minimum size rightmost bin 
holte suggests minimum bin size empirical analysis number classification tasks experiments value 
minimum bin size discretization interval pure possible selecting moving partition boundary add observed value particular bin count dominant class bin greater 
recursive minimal entropy partitioning method discretizing continuous attributes minimal entropy heuristic catlett fayyad irani experimental study 
supervised algorithm uses class information entropy candidate partitions select bin boundaries discretization 
notation closely follows notation fayyad irani 
set instances feature partition boundary class information entropy partition induced denoted js jsj ent js jsj ent feature boundary tmin minimizes entropy function possible partition boundaries selected binary discretization boundary 
method applied recursively partitions induced tmin stopping condition achieved creating multiple intervals feature fayyad irani minimal description length principle determine stopping criteria recursive discretization strategy 
recursive partitioning set values stops iff gain log gamma delta number instances set gain ent gamma delta log gamma gamma delta ent gamma delta ent gamma delta ent number class labels represented set partitions branch recursive discretization evaluated independently criteria areas continuous spaces partitioned finely relatively low entropy partitioned coarsely 
dataset features train test majority continuous nominal sizes accuracy anneal cv sigma australian cv sigma breast cv sigma cleve cv sigma crx cv sigma diabetes cv sigma german cv sigma glass cv sigma glass cv sigma heart cv sigma hepatitis cv sigma horse colic cv sigma hypothyroid sigma iris cv sigma sick euthyroid sigma vehicle cv sigma table datasets baseline accuracy results experimental study compare discretization methods section preprocessing step algorithm naive bayes classifier 
induction algorithm state art top method inducing decision trees 
naive bayes induction algorithm computes posterior probability classes data assuming independence features class 
probabilities nominal features estimated counts gaussian distribution assumed continuous features 
number bins equal width interval discretization set maxf delta log number distinct observed values attribute 
heuristic chosen examining plus histogram binning algorithm spector 
chose sixteen datasets irvine repository murphy aha continuous feature 
datasets test instances ran single train test experiment report theoretical standard deviation estimated binomial model kohavi 
remaining datasets ran fold cross validation report standard deviation cross validation 
table describes datasets column showing accuracy predicting majority class test set 
table shows accuracies induction algorithm quinlan different discretization methods 
table shows accuracies naive bayes induction algorithm 
shows line plot discretization methods log binning entropy 
plotted difference accuracy discretization induction algorithm original accuracy 
discussion experiments reveal discretization methods naive bayes classifier lead large average increase accuracy 
specifically best method entropy improves performance datasets loss insignificant 
entropy discretization method provides significant increase accuracy 
attribute disparity accuracy shortcomings gaussian distribution assumption inappropriate domains 
observed discretization continuous feature roughly approximate class distribution feature help overcome normality assumption continuous features naive bayesian classifier 
performance significantly improved datasets cleve diabetes entropy discretization method significantly degrade dataset decrease slightly 
entropy discretization global method suffer data fragmentation pagallo haussler 
significant dataset continuous bin log entropy rd bins anneal sigma sigma sigma sigma sigma australian sigma sigma sigma sigma sigma breast sigma sigma sigma sigma sigma cleve sigma sigma sigma sigma sigma crx sigma sigma sigma sigma sigma diabetes sigma sigma sigma sigma sigma german sigma sigma sigma sigma sigma glass sigma sigma sigma sigma sigma glass sigma sigma sigma sigma sigma heart sigma sigma sigma sigma sigma hepatitis sigma sigma sigma sigma sigma horse colic sigma sigma sigma sigma sigma hypothyroid sigma sigma sigma sigma sigma iris sigma sigma sigma sigma sigma sick euthyroid sigma sigma sigma sigma sigma vehicle sigma sigma sigma sigma sigma average table accuracies different discretization methods 
continuous denotes running data bin log bins equal width binning respective number intervals entropy refers global variant discretization method proposed fayyad irani 
dataset naive bayes continuous bin log entropy rd bins anneal sigma sigma sigma sigma sigma australian sigma sigma sigma sigma sigma breast sigma sigma sigma sigma sigma cleve sigma sigma sigma sigma sigma crx sigma sigma sigma sigma sigma diabetes sigma sigma sigma sigma sigma german sigma sigma sigma sigma sigma glass sigma sigma sigma sigma sigma glass sigma sigma sigma sigma sigma heart sigma sigma sigma sigma sigma hepatitis sigma sigma sigma sigma sigma horse colic sigma sigma sigma sigma sigma hypothyroid sigma sigma sigma sigma sigma iris sigma sigma sigma sigma sigma sick euthyroid sigma sigma sigma sigma sigma vehicle sigma sigma sigma sigma sigma average table accuracies naive bayes different discretization methods dataset acc diff dataset naive bayes acc diff comparison entropy solid log binning dashed 
graphs indicate accuracy difference naive bayes 
line indicates performance naive bayes prior discretization 
datasets arranged increasing differences discretization methods 
degradation accuracy global discretization method conjecture induction algorithm full advantage possible local discretization performed data local discretization help induction process datasets tested 
possible advantage global discretization opposed local methods provides regularization prone variance estimation small fragmented data 
confidence level naive bayes entropy discretization better datasets worse 
average performance assuming datasets coming realworld distribution entropy discretized naivebayes compared original naive bayes 
supervised learning methods slightly better unsupervised methods simple binning tends significantly increase performance naive bayesian classifier assumes gaussian distribution continuous attributes 
summary empirical comparison discretization continuous attributes showed discretization prior induction significantly improve accuracy induction algorithm 
global entropy discretization method best choice discretization methods tested 
entropy discretized naive bayes improved average performance slightly surpassed 
performance degrade data discretized advance entropy discretization method cases improved significantly 
methods tested dynamic feature discretized independent features algorithm performance 
plan pursue wrapper methods john kohavi pfleger search space values indicating number intervals attribute 
variant explored local versus global discretization fayyad irani method 
acknowledgments done mlc library partly funded onr nsf iri iri 
jason catlett usama fayyad george john bernhard pfahringer useful comments 
third author supported fred foundation arcs scholarship 
catlett 
machine learning large databases phd thesis univeristy sydney 
catlett 
changing continuous attributes ordered discrete attributes kodratoff ed proceedings european working session learning berlin germany springerverlag pp 

chan srinivasan 
determination quantization intervals rule model dynamic systems proceedings ieee conference systems man cybernetics virginia pp 

chiu cheung wong 
information synthesis hierarchical entropy discretization journal experimental theoretical artificial intelligence 
busse 
global discretization continuous attributes preprocessing machine learning third international workshop rough sets soft computing pp 

fayyad irani 
multi interval discretization continuous valued attributes classification learning proceedings th international joint conference artificial intelligence morgan kaufmann pp 

fulton kasif salzberg 
efficient algorithm finding multi way splits decision trees unpublished 
holte 
simple classification rules perform commonly datasets machine learning 
iba langley 
induction level decision trees proceedings ninth international conference machine learning morgan kaufmann pp 

john kohavi pfleger 
irrelevant features subset selection problem machine learning proceedings eleventh international conference morgan kaufmann pp 

available anonymous ftp stanford edu pub ronnyk ml ps 
kerber 
chimerge discretization numeric attributes proceedings tenth national conference artificial intelligence mit press pp 

kohavi 
bottom induction oblivious read decision graphs strengths limitations twelfth national conference artificial intelligence pp 

available anonymous ftp stanford edu pub ronnyk aaai ps 
kohavi 
study cross validation bootstrap accuracy estimation model selection proceedings th international joint conference artificial intelligence 
available anonymous ftp stanford edu pub ronnyk ps 
kohavi john long manley pfleger 
mlc machine learning library tools artificial intelligence ieee computer society press pp 

available anonymous ftp stanford edu pub ronnyk mlc ps 
kohonen 
self organization associative memory berlin germany springer verlag 
langley iba thompson 
analysis bayesian classifiers proceedings tenth national conference artificial intelligence aaai press mit press pp 

maass 
efficient agnostic pac learning simple hypotheses proceedings seventh annual acm conference computational learning theory pp 

michalski 
planar geometric model representing multidimensional discrete spaces multiple valued logic functions technical report uiucdcs university illinois champaign 
michalski stepp 
learning observations conceptual clustering michalski carbonell ed machine learning artificial intelligence approach tioga palo alto 
murphy aha 
uci repository machine learning databases information contact ml repository ics uci edu 
pagallo haussler 
boolean feature discovery empirical learning machine learning 
pfahringer 
compression discretization continuous attributes prieditis russell eds proceedings twelfth international conference machine learning morgan kaufmann 
quinlan 
programs machine learning morgan kaufmann los altos california 

class driven statistical discretization continuous attributes extended lavrac wrobel eds machine learning ecml proc 
european conf 
machine learning lecture notes artificial intelligence springer verlag berlin heidelberg new york pp 

rissanen 
stochastic complexity modeling ann 
statist 
spector 
plus duxbury press 
ting 
discretization attributes instance learning technical report university sydney 
van de 
decision trees numerical attribute spaces proceedings th international joint conference artificial intelligence pp 

weiss kulikowski 
computer systems learn morgan kaufmann san mateo ca 
weiss galen tadepalli 
maximizing predicative value production rules artificial intelligence 
wong chiu 
synthesizing statistical knowledge incomplete data ieee transaction pattern analysis machine intelligence pp 

