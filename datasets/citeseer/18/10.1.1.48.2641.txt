generalization scaling reinforcement learning david ackley michael littman cognitive science research group bellcore morristown nj associative reinforcement learning environment generates input vectors learning system generates possible output vectors reinforcement function computes feedback signals input output pairs 
task discover remember input output pairs generate rewards 
especially difficult cases occur rewards rare expected time algorithm grow exponentially size problem 
reinforcement function possesses regularities learning algorithm exploits learning time reduced non generalizing algorithms 
describes neural network algorithm called complementary reinforcement back propagation crbp reports simulation results problems designed offer differing opportunities generalization 
reinforcement learning requires search reinforcement learning sutton barto anandan ackley allen requires learner familiar supervised learning paradigm 
supervised learning supplies correct answers learner reinforcement learning requires learner discover correct outputs stored 
reinforcement paradigm divides neatly search learning aspects rewarded system internal adjustments learn discovered input output pair punished system internal adjustments search 
making reinforcement error anderson williams extend backpropagation algorithm associative reinforcement learning 
start garden variety backpropagation network vector binary input units propagates zero layers hidden units ultimately reaching vector sigmoid units continuous values range 
interpret probability associated random bit takes value 
call continuous deterministic vector search vector distinguish stochastic binary output vector input vector forward propagate produce search vector perform independent bernoulli trials produce output vector gamma pair evaluated reinforcement function reward punishment ensues 
suppose reward occurs 
want backpropagation just take desired target produce error vector gamma adjust weights normally 
suppose punishment occurs indicating correspond choice error vector backpropagation allows push search vector direction way go 
absence problem specific information pick appropriate direction certainty 
decision involve assumptions 
minimal don assumption employed anderson williams ackley pushes directly away gamma error vector 
slightly stronger assumption employed barto anandan ackley pushes directly complement gamma gamma error vector 
approaches agree signs error terms differ magnitudes 
explore second possibility embodied algorithm called complementary reinforcement back propagation crbp 
summarizes crbp algorithm 
algorithm reflects modifications basic approach just sketched 
step directly probabilities advantageous stretch values parameter necessary reach zero produce deterministic output 
second step important smaller learning rate punishment compared reward 
third consider step forward propagation performed stochastic binary output vector generated procedure step compared identical punishment occurred different reward occurred error vector generated weight update performed 
loop continues different output generated case failure original output regenerated case success 
modification improved performance significantly added small percentage total number weight updates performed 

build back propagation network input dimensionality output dimensionality 
pick random forward propagate produce 

generate binary output vector uniform random variable parameter ae gamma 

compute reinforcement 
increment 
generate output errors gamma gamma gamma 

errors 

update weights 
deltaw jk je gamma parameters gamma 
forward propagate produce new 
generate temporary output vector go 
exit returning go 
complementary reinforcement back propagation crbp line generalization possible outputs correct pairings rare computational cost associated search correct answers profound 
search correct pairings accelerated search strategy effectively generalize reinforcement received input 
speed algorithm problem relative non generalizing algorithms provides measure generalization call line generalization 

array length set random numbers gamma 

pick random input 
compute reinforcement 
increment 
mod 
exit returning go 
table lookup algorithm ref consider table lookup algorithm ref summarized 
algorithm separate storage location possible input 
prevents memorization gamma pair interfering 
similarly selection candidate output vector depends slot table corresponding input 
learning speed ref depends input output dimensionalities number correct outputs associated input 
problem possesses input bits output bits correct output vector input vector ref runs time counting input output judgment 
cases expects take gamma just find correct gamma pair exponential time avoided priori information 
generalizing algorithm crbp compare ref simulations scalable problems tested crbp simple problems designed offer varying degrees types generalization 
simulations section details apply input output bit counts equal 
parameters dependent independent reinforcement function hand picked gamma 
data points medians runs 
stopping criterion interpreted max fit lines figures squares solutions theta significant digits 
notational convenience fraction ones input 
majority consider majority rules problem 
gamma mapping 
problem provides opportunity anderson called output generalization correct output states pair output bits completely correlated cases reward occurs 
table crbp crbp majority problem displays simulation results 
note ref faster crbp small values crbp slower growth rate vs allows cross outperforming ref bits 
note violation 
conventional wisdom majority linearly separable problem performance crbp hidden units better 
hidden units helpful linearly separable problems opportunities output generalization 
copy attractors family second example consider copy problem 
gammao mapping values output bits rewarding states completely uncorrelated value output bit completely correlated value corresponding input bit 
displays simulation results 
copy log time table crbp crbp copy problem low values ref faster crbp rapidly ref increases 
copy majority crbp performs better hidden units 
majority copy problems extreme cases spectrum 
majority viewed attractors problem correct outputs zeros ones correct output closer hamming distance 
dividing input output bits groups performing majority function independently group generates attractors problem 
general dividing input output bits groups generates attractors problem 
majority results copy results 
displays simulation results bit problems generated varied advantage hidden units low values evident advantage shortcut connections direct input output weights larger values note combination hidden units shortcut connections performs better 
attractors log time table crbp crbp crbp majority copy attractors family excluded middle functions considered far linearly separable 
consider folded majority function 
majority rewarding output states determination output state correct linearly separable input space 
excluded middle problem yields eqv complement xor function functions parity nc get non linear increasing excluded middle 
table crbp excluded middle problem displays simulation results 
crbp slowed somewhat compared linearly separable problems yielding higher cross point bits 
structuring degenerate output spaces scaling problems previous section designed single correct output possible input 
allows difficult problems small sizes rules important aspect generalizing algorithms associative reinforcement learning multiple satisfactory outputs inputs generalizing algorithm may impose structure mapping produces 
demonstrations effect bit count inverse arithmetic 
bit count problem simply states number bits output equal number bits input 
ref rapidly finds solutions involving hundreds different output patterns 
crbp slower especially relatively hidden units regularly finds solutions involving just output patterns form sequence bit changing step 
theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta gamma theta gamma theta xi theta theta theta xi theta theta theta gamma theta theta gamma theta theta xi theta theta theta xi theta theta theta sample crbp solutions inverse arithmetic inverse arithmetic problem summarized follows find ffi pi gamma theta xi ffi pi bits output interpreted bit binary numbers bit operators task pick output evaluates bit binary input usual rules operator precedence left right evaluation integer division division zero fails 
shown crbp solves problem essentially discovering positional notation produces globally structured solutions particularly outputs lower valued wider range solutions 
basic concepts supervised learning appear different guises paradigm reinforcement learning applied large output spaces 
learning phase followed generalization test reinforcement learning search problem generalization test performed simultaneously learning 
information put soon acquired 
problem overfitting learning noise issue learning stops automatically consistent success reached 
experiments reported gradually increased number hidden units bit copy problem observing performance decline associated free parameters 
attractors folds generalizing excluded middle families provide starter set sample problems easily understood distinctly different extreme cases 
degenerate output spaces generalization decisions seen directly discovered mapping 
network analysis required see net 
possibility ultimately generating useful new knowledge reinforcement learning algorithms ruled 
ackley 
connectionist machine genetic hillclimbing 
boston ma kluwer academic press 
ackley 
associative learning inhibitory search 
touretzky ed advances neural information processing systems 
san mateo ca morgan kaufmann 
allen 
developing agent models neural reinforcement technique 
ieee systems man cybernetics conference 
cambridge ma 
anderson 
learning problem solving multilayer connectionist systems 
university mass ph dissertation 
coins tr 
amherst ma 
barto 
learning statistical cooperation self interested neuron computing elements 
human neurobiology 
barto anandan 
pattern recognizing stochastic learning automata 
ieee transactions systems man cybernetics 
rumelhart hinton williams 
learning representations backpropagating errors 
nature 
sutton 
temporal credit assignment reinforcement learning 
university mass ph dissertation 
coins tr 
amherst ma 
williams 
theory reinforcement learning connectionist systems 
college computer science northeastern university technical report nu ccs 
boston ma 
