discriminative vs informative learning dan rubinstein trevor hastie department statistics stanford university stanford ca ruby stat stanford edu trevor stat stanford edu goal pattern classification approached points view informative classifier learns class densities discriminative focus learning class boundaries regard underlying class densities 
review synthesize tradeoffs approaches simple classifiers extend results modern techniques naive bayes generalized additive models 
data mining applications operate domain high dimensional features tradeoffs informative discriminative classifiers especially relevant 
experimental results provided simulated real data 
kdd classification automatic classification main goals data mining systems fayyad piatetsky shapiro smyth 
database observations consisting input predictor output response class label variables classifier seeks learn relationships predictors response allow assign new observation response unknown predetermined classes 
goal classification minimize misclassifications expected cost misclassifications types mistakes costly 
classifiers segmented groups 
informative classifiers model class densities 
classification done examining likelihood class producing features assigning class 
examples include fisher discriminant analysis hidden markov models naive bayes 
class density considered separately models relatively easy train 
copyright fl american association artificial intelligence www aaai org 
rights reserved 

discriminative attempt model underlying class feature densities 
focus modeling class boundaries class membership probabilities directly 
examples include logistic regression neural networks generalized additive models 
requires simultaneous consideration classes models harder train involve iterative algorithms scale 
shall see approaches related bayes rule lead different decision rules especially class density model incorrect training observations relative number parameters model 
tradeoffs approaches terms ease training classification performance 
precise statements simple classifiers lessons applied sophisticated techniques 
review known statistical results apply simple non discriminative classifiers demonstrate modern techniques categorized discriminative 
naive bayes gam applied simulation real data exemplify counter intuitively discriminative training may lead best classifiers 
propose methods combining approaches 
focus parametric techniques similar results obtain non parametric case 
advent increasingly sophisticated classification techniques important realize category classifier falls assumptions problems fixes type different 
overview bayesian classification theory formally classification problem consists assigning vector observation classes 
true class denoted kg 
clas mapping assigns class labels observations fl kg 
cost matrix describes cost associated misclassifying member class class 
special case loss gamma ffi 
underlying observations true joint density yjx xjy unknown 
goal minimize total cost errors known risk achieved bayes classifier duda hart 
fl min gamma max gamma kjx loss loss reduces classifying class class posterior probability kjx maximum 
practice true density unknown available set training observations classification techniques seek estimate class posterior probabilities kjx see optimal classification achieved known perfectly discussion relationship class posteriors neural net outputs see ney 
convenience follows discriminant function log kjx kjx discriminant preserves ordering class posterior probabilities classification 
informative classification estimate class posteriors yjx directly class densities xjy priors estimated 
operative equation bayes rule gives class posteriors terms class densities priors kjx xjy xjy typically model chosen class densities example gaussian xjy sigma sigmag model parameters estimated data maximizing full log likelihood mle max gamma log max gamma log jy log gaussian case yields known estimates nk sigma gamma gamma number observations class discriminant functions log gamma sigma gamma gamma gamma sigma gamma fi fi linear note kp gamma parameters estimated discriminants involve gamma parameters 
important points informative training 
model xjy assumed class densities 

parameters obtained maximizing full log likelihood log log yjx 

decision boundary induced model parameters may appear way reduces effective number parameters discriminant 
discriminative classification discriminative approach models class posteriors discriminants directly 
discriminative approach flexible regard class densities capable modeling 
restricting discriminant log kjx kjx log xjy xjy capable modeling class densities exponential tilts xjy xjy particular informative model regards class densities seen special instance general discriminative model 
example special case gaussian carrier density xjy sigma fi fi corresponding discriminative model allows carrier density xjy fk fi fi long discriminant linear 
parameter estimation discriminative case carried maximizing condition log likelihood discr max gamma log jx hand maximizing conditional likelihood natural thing directly focused class posteriors yjx required order classify 
ignoring part data marginal distribution 
compare full likelihood case observation contributes yjx 
discriminative approach uses term right side throws away information marginal density class density model correct discriminative approach ignores useful information 
ignoring class models may incorrect 
table summarizes main comparisons approaches 
informative discriminative objective function full log likelihood log conditional log likelihood log jx model assumptions class densities xjy discriminant functions parameter estimation easy hard advantages efficient model correct borrows strength flexible robust fewer assumptions disadvantages bias model incorrect 
may biased 
ignores information logistic regression vs linear discriminant analysis lot insight gained examining class case class densities assumed gaussian xjy sigma det sigma exp gamma gamma sigma gamma gamma priors populations gaussian informative classification efficient discriminative fewer training observations required fixed number training observations better classification obtained efron neill ruiz 
class densities gaussian circumstances classes separated informative training discriminative mclachlan 
informative approach requires estimating class means pooled covariance requires single sweep data 
discriminative approach requires iterative optimization gradient descent conditional likelihood 
class densities density class class class densities density class class class densities density class class class densities cases simulation data 
class boundaries derived training observations normal discriminant analysis lda logistic regression shown points left boundary classified class 
shows class densities simulation experiments 
case gaussian class case expect lda better models learned training data 
case training sets observations class drawn class densities pictured 
lda classifiers trained set exact probability error computed integration grid error fl xjy dx fl xjy dx table provides error rates procedures 
column corresponds different density case depicted 
rows best sense model trained complete density sample training observations 
remaining rows averages standard errors error rates training sets contained observations class 
case lda best best lda se lda se expected lda better classes gaussian case 
interesting result case lda significantly better vs know true distributions 
case true distribution highly non gaussian 
number observations relative dimensionality informative methods may surprisingly model incorrect see gam naive bayes example 
statlog data statlog experiments compared classification techniques various datasets 
datasets logistic discrimination better corresponding informative approach lda michie spiegelhalter taylor 
cases chromosome dataset lda better logistic discrimination 
cases informative model apparently important information marginal density 
naive bayes gam naive bayes classifiers specialized form bayesian network john langley langley sage fall informative category classifiers 
class densities assume independence predictors xjy jy log xjy log jy naive reason 
langley john langley considered class densities products univariate gaussians flexible gaussian kernel densities 
corresponding discriminative procedure known generalized additive model gam hastie tibshirani 
gam assume log ratio class posteriors additive predictors log kjx kjx const theorem naive bayes classifiers specialized case gam 
proof suffices show induced discriminant log additive 
log kjx kjx log xjy xjy log xjy gamma log xjy log gamma log const comparisons follow ensure representations possible procedures 
particular informative case model class densities densities imply additive spline discriminant fi kjx exp exp natural cubic spline basis 
simulation study simulation study shown discriminant taken additive spline uniformly spaced fixed knots 
class complicated mixture density outer ring class middle exponential tilt discriminant class 
naive bayes classifier assumes density see stone appear separately dimension class 
asymptotically gam classifier achieves bayes error rate true discriminant log additive construction 
asymptotically naive bayes nb classifier worse gam class densities product form 
finite sample training observations available naive bayes classifier surprisingly behavior noted langley contours class densities training observations 
langley sage 
simulation experiments training sets containing observations class train nb gam classifiers 
average error rates nb gam standard errors respectively 
instance informative training slightly better discriminative training discriminative model correct informative 
friedman friedman shown comes classification bias class posteriors critical discretization assignment rule 
class density model incorrect biased may get upper hand especially lower variance estimates class posteriors training sets 
best informative approach confidence model correctness high 
suggests promising way combining approaches partition feature space 
train informative model dimensions correct discriminative model 
experimental results approach forthcoming 
investigating techniques combining procedures 
goal discrimination classes pays investigate performance corresponding informative model borrows strength marginal density 
mclachlan 
logistic regression compared normal discrimination nonnormal populations 
australian journal statistics 
duda hart 
pattern classification scene analysis 
wiley 
efron 
efficiency logistic regression compared normal discriminant analysis 
journal american statistical association 
fayyad piatetsky shapiro smyth 
data mining knowledge discovery overview 
advances knowledge discovery data mining 
mit press 

friedman 
bias variance loss curse dimensionality 
technical report dept statistics stanford university 
hastie tibshirani 
generalized additive models 
chapman hall 
john langley 
estimating continuous distributions bayesian classifiers 
proceedings eleventh conference uncertainty artifical intelligence 
san mateo morgan kaufmann 
langley sage 
induction selective bayesian 
proceedings tenth conference uncertainty artifical intelligence 
seattle wa morgan kaufmann 
michie spiegelhalter taylor 
machine learning neural statistical classification 
ellis horwood 
ney 
probabilistic interpretation neural network classifiers discriminative training criteria 
ieee transactions pattern analysis machine intelligence 
neill 
general distribution error rate classification procedure application logistic regression discrimination 
journal american statistical association 
ruiz 
asymptotic efficiency logistic regression relative linear discriminant analysis 
biometrika 
stone hansen truong appear 
polynomial splines tensor products extended linear modeling 
annals statistics 
