mp locks replacing synchronization primitives message passing chen chi kuo john carter department computer science university utah salt lake city ut shared memory programs guarantee correctness concurrent accesses shared data interprocessor synchronization operations 
common synchronization operators locks traditionally implemented mix shared memory accesses hardware synchronization primitives test set 
argue synchronization operations implemented fast message passing lock managers attractive alternative dedicated synchronization hardware 
propose message passing lock mp lock algorithms centralized distributed reactive provide implementation guidelines 
mp locks reduce design complexity runtime occupancy dsm controllers exploit software inherent flexibility adapt differing applications lock access patterns 
compared performance mp locks common shared memory lock algorithms test test set mcs locks mp locks scale better 
machines nodes applications mp locks ran faster applications shared memory locks 
small systems nodes applications mp locks slow slowed due higher software overhead 
conclude locks message passing considered replacement hardware locks scalable multiprocessors support efficient message passing mechanisms 
guarantee semantic correctness shared memory programs control concurrent accesses shared data synchronization operations common lock unlock 
inefficient implementation synchronization impacts performance shared memory programs directly time required perform synchronization operations indirectly increasing amount time processes blocked waiting processes relinquish locks 
general rule multiprocessor architects tend implement primitive operations custom hardware 
example lock unlock operations traditionally implemented combination hardware implemented shared memory atomic email addresses cs utah edu 
research supported part space naval warfare systems command advanced research projects agency arpa contract arpa order 
views contained authors interpreted necessarily representing official policies endorsements expressed implied darpa government 
tion primitives test set compare swap load linked store conditional 
designers cray broke rule abandoned dedicated high performance hardware barrier network supported 
believe similar argument applies locks emerging high performance message passing mechanisms locks message passing viable alternative hardware locks scalable multiprocessors 
test set locks spin shared memory locations hardware instructions 
major problem locks instruction performed spinning involves global communication 
test test set locks add extra shared memory load primitive eliminate unnecessary migrations 
performance degrades multiple nodes waiting lock released mellor crummey scott mcs locks avoid global spinning entirely maintaining distributed queue processes waiting lock 
design improve performance reducing global communication guarantees locks granted fifo order 
downside mcs locks involve global operations locks lock free 
performance mcs locks heavily dependent lock access patterns cooperative parallel processes lim agarwal proposed adaptive scheme called reactive locks adopts mcs lock semantics depending degree lock contention 
queue lock bit mechanism associates special lock bit cache lines data 
complete hardware implementation may efficient lock mechanism proposed requires significant changes processor cache controller dsm protocol engine designs 
unsuitable implementation multiprocessors current commodity microprocessors consider locks 
conventional shared memory lock mechanisms mcs reactive locks designed performance characteristics bus architectures 
machines broadcast invalidations updates cheap 
remote access increasingly expensive scalable architectures gap speeds processor widens approaches considered 
contemporary multiprocessor architectures support efficient message passing addition shared memory increasing number high performance network interfaces protocols proposed 
propose low latency message passing exploited implement synchronization hardware synchronization primitives 
call method implementing lock primitives message passing mp lock 
primitive tion operations shared memory acquire release locks mp locks send messages lock managers mediate lock requests 
implementation mp locks lock managers embedded inside operating system kernel guarantee fast responses lock requests 
addition eliminating design overhead supporting scalable synchronization primitives hardware software inherent flexibility exploited provide different implementations locks style style 
mp locks minimize number network transfers required transfer lock ownership suffer unnecessary invalidations reloads due general purpose shared memory protocols 
furthermore mp locks offload dsm controller network controller reducing dsm controller occupancy 
mp lock software implementation prefetching data feasible need non standard hardware shrink critical sections 
evaluate tradeoffs implementing locks software compared performance message passing locks mcs locks application programs fairly heavy synchronization requirements 
previous locking studies concentrated microbenchmarks focused complete applications determine impact various locking mechanisms 
message passing locks scale better machines nodes applications mp locks ran faster applications shared memory locks 
small systems nodes performance lags shared memory lock performance due higher software overhead 
mp lock applications slow slowed 
remainder organized follows 
section presents mp lock algorithms 
describe simulation environment test applications section results experiments section 
draw section 
design mp lock mp lock software lock mechanism requires special purpose hardware atomic primitives 
mp lock model lock managers manage lock ownership 
evaluated lock manager organizations single centralized lock manager mp cent ii set cooperating lock managers running node mp dist iii adaptive distributed manager reverts centralized mode little contention mp react 
shared memory loads stores atomic primitives mp lock synchronization libraries send lock requests lock manager 
request lock manager queue request satisfied forward lock manager currently lock 
evaluated user level kernel lock managers context switching overhead user level lock managers significant 
restrict focus kernel lock managers 
facilitate designing mp lock algorithms classify lock acquires categories depending state lock acquire initiated remote idle acquire local idle acquire remote busy acquire 
remote idle acquire occurs node attempts acquire lock node released 
acquire case succeed immediately remote operations required detect update lock status complete 
local idle acquire remote idle acquire state nodes 
idle cases occur frequently little contention 
remote busy acquire occurs acquire node issued current lock holder different node releases lock 
occurs frequently high contention locks 
performance implementation primarily depends factors number messages call hops sent acquire release lock number interrupts required nodes acquire release 
mp cent lock managers handle lock unlock requests 
node acquires lock performing non blocking send designated lock manager spins waiting reply 
releasing node non blocking send central lock manager 
acquire request granted immediately lock free 
acquire request queued lock ownership returned 
conceptually mp cent similar global location 
requires hops read valid lock variable acquire exclusive ownership lock free 
acquiring lock mp cent requires hops interrupt central lock manager circumstances 
mp cent performs multiple processes acquire lock little contention 
lack support caching forwarding leads poor performance local idle remote busy acquires 
mp dist lock managers cooperate manage distributed queue pending lock requests 
process wants acquire lock consults local lock manager 
lock free lock ownership cached locally local lock manager returns lock immediately 
process sends lock request designated remote lock manager returns lock forwards request 
release lock process consults local lock manager 
request pending ownership lock forwarded requesting process directly single message 
request pending local lock manager caches lock ownership 
mp dist performs locks heavily contested frequently reused 
periods heavy contention lock ownership passed successive owners directly single message extra latency messages required forward lock request queue effectively hidden part required stall lock free 
lock local idle acquires common mp dist caching lock ownership acquires cheap 
acquires remote idle acquires mp dist performs relatively poorly requires average hops messages interrupts transfer ownership current holder requester 
different applications different lock request patterns developed reactive lock protocol akin reactive locks called mp react 
lock managers mediate lock acquire release requests accurate knowledge lock access patterns 
central lock manager initially locks lock ownership returned locks released 
acquire requests locks require hops satisfied mp cent 
lock manager detects repeated requests node intervening requests nodes instance lock node performing remote operations 
lock heavily contested lock manager forward barnes node node node node mcs slow tts slow cent slow dist slow react slow barnes node node node node mcs fast tts fast cent fast dist fast react fast radiosity node node node node mcs fast tts fast cent fast dist fast react fast radiosity node node node node mcs slow tts slow cent slow dist slow react slow 
performance charts barnes radiosity execution time relative mcs 
acquire requests mp dist ownership transfers take hop 
mp react better information access patterns reactive locks central lock manager track accesses 
mp react effective decisions switch modes 
minimize overhead mp lock software implementation low latency message passing mechanism called direct deposit dd embedded lock managers kernel 
carefully distributed lock management nodes minimize load imbalance effects 
pertinent implementation details expanded version simulation methodology benchmarks experiments performed execution driven simulation avalanche architecture 
simulation environment includes kernel bsd provides scheduling interrupt handling memory management limited system call capabilities 
simulating kernel provides fair accounting software mp lock overheads 
interconnect modeled myrinet network 
ran experiments different switch fall delays cycles model high commercial dsm systems dsm systems clusters workstations aggressive shelf interconnects 
programs conduct study mp splash benchmark suite barnes radiosity raytrace splash benchmark suite spark sparse matrix kernel suite 
believe broad set synchronization access patterns represented applications 
detailed simulation parameters application sizes distribution lock categories applications studied expanded version 
results figures performance mcs mp cent mp dist mp react relative mcs lock version applications studied 
graphs application 
graph left presents results fast interconnect graphs right results slow interconnect 
graphs raytrace spark detailed breakdown graphs execution time expanded 
barnes synchronization occurs initializing octree structure represents space 
average critical section long chance processes contending lock small 
lock acquires occur lock busy locks reused node node system 
time spent synchronization small compared execution time little need special hardware support locking mp locks provide equal better performance shared memory locks 
shown shared memory lock implementations mcs mp lock implementations distributed reactive perform equally cases node slow interconnect configuration 
lack caching penalizes mp cent implementations 
number nodes increases degree lock contention increases 
controller occupancy cache conflicts increase user shared memory access time noticeably shared memory locks 
result mp dist outperforms mcs node slow interconnect configuration 
radiosity degree contention locks pro mp node node node node mcs fast tts fast cent fast dist fast react fast mp node node node node mcs slow tts slow cent slow dist slow react slow 
performance charts mp execution time relative mcs 
distributed queues global barrier buffer pool depends number nodes 
small systems nodes lock acquires local idle acquires 
locks protect image patches show little contention due fine granularity 
poor temporal locality patch locks result accesses remote idle variety 
due low level contention mp lock higher software overhead mcs locks perform better mp locks node fast interconnect configuration 
slow interconnect increased shared memory access time reduces performance gap 
number nodes increases contention increases 
leads decrease percentage local idle acquires increase percentage remote busy acquires 
regardless configuration percentage remote idle acquires constant large amount number fine grained patch locks 
result changes lock access pattern mp react starts outperform mcs node configuration time configuration reached nodes outperforms mcs locks 
comparing just schemes contention low mp cent outperforms mp dist expected 
number nodes network latency increases mp dist prevails 
mp react able adapt best mp cent mp dist performs better schemes 
locks raytrace accessed manner similar locks radiosity 
node configurations acquires locks protect memory pool remote idle category accounts lock requests 
small configurations locks protecting distributed task queues local idle 
small configurations shared memory locks perform better mp locks 
number nodes increases sets locks busy 
result locks node configuration heavily contested causes mp locks perform better shared memory locks reasons radiosity 
mp degree contention locks extremely low fewer acquires busy locks 
addition little reuse acquires fall remote idle category 
case mp locks require interrupts lock manager current lock holder hops acquire lock 
nodes shared memory locks outperform mp lock implementations 
performance gap shrinks number nodes network latency increases due tight dependence shared memory lock performance remote memory access latency ii severe impact dsm controller occupancy caused shared memory locks 
example mp locks perform better mcs node configuration slower network better node configuration 
comparing just mp lock mechanisms mp cent outperforms mp dist node configuration locks accesses remote idle locks 
lock accesses remote idle locks configuration mp dist outperforms mp cent lock caching 
spark locks temporal locality due way processes assigned 
node node configurations shared memory locks perform better due low latency lock unlock routines 
number nodes network latency increases perform better shared memory locks 
summary applications high lock contention best mp lock algorithm outperforms best shared memory lock algorithm 
particular mp locks tend outperform shared memory locks system size reaches nodes fast interconnect nodes slow interconnect 
superior scalability mp locks applications occurs reasons 
mp locks handle remote busy locks better shared memory locks lock ownership forwarded single message 
achieve similar performance shared memory locks require special hardware shared memory protocols modern machines 
second mp locks increase dsm controller occupancy interfere shared memory data accesses lead significantly lower average remote memory latency non lock shared data 
software overhead induced lock managers amortized nodes reduces impact 
applications low lock contention mp locks best shared memory lock implementation small systems nodes nodes applications remaining applications 
fast hardware shared memory lock implementations handle low contention locks efficiently mp locks 
trends observed expect mp locks scale better shared memory locks number nodes increases nodes 
believe mp locks attractive alternative hardware synchronization primitives scalable shared memory multiprocessors support efficient message passing 
demonstrate software locks attractive alternative hardware implementations 
socalled mp lock approach efficient message passing mechanisms supported contemporary multiprocessor interconnects 
basing locks message passing dedicated hardware mp lock reduces design complexity runtime occupancy dsm controllers 
addition mp locks exploit software inherent flexibility support lock protocols intelligently adapt differing application lock access patterns 
evaluated performance mp lock algorithms efficient hardware locks algorithms test set mcs locks applications variety lock access patterns 
mp locks scale better mcs locks avoid shared memory support direct point point transfer lock ownership periods high lock contention 
result mp locks consistently perform equal better hardware locks systems consisting sixteen nodes 
extreme mp locks improved performance 
small system sizes nodes interrupt handling software overhead caused performance mp lock versions lag shared memory locks 
difference applications remaining applications 
focusing mp lock algorithms isolation mp cent performed best applications mp poor lock locality frequent remote idle accesses 
reason applications relinquishing locks back lock centralized lock manager minimizes message traffic 
contention high locks reused frequently mp dist significantly outperforms mp cent direct lock forwarding lock caching handle situations 
mp react exploits global access pattern observations adaptively switch centralized distributed modes leads performance best performance applications demonstrate mix access patterns 
contributions 
results study compares performance message passing locks shared memory locks macrobenchmarks 
took great pains conduct fair comparison including detailed bsd kernel simulation environment accurately simulate software overhead proposed message passing mechanisms 
second identified tradeoffs shared memory locks message passing locks system sizes network latencies vary 
results assist architects designing synchronization mechanisms 
third classified lock access patterns known shared memory benchmarks various number processors help researchers understand locking behavior applications 
provided guidelines designing synchronization mechanisms clusters workstations equipped message passing communication mechanisms 
example show lock caching essential designing messagepassing locks 
leigh stoller mark swanson provided invaluable assistance direct deposit protocol 
davis michael scott provided helpful comments 
members avalanche project university utah referees useful suggestions 
basu vogels von eicken 
net user level network interface parallel distributed computing 
proc 
th sosp december 
jacobson mackey wilkes 
implementation interface architecture 
proc 
nd osdi october 
carter davis 
kuo swanson stoller 
avalanche communication memory architecture scalable parallel computing 
tr uucs univ utah 
burger goodman 
efficient synchronization eat 
proc 
th isca may 
kranz johnson agarwal kubiatowicz lim 
integrating message passing shared memory early experience 
proc 
ppopp pp 
may 

kuo carter 
mp locks replacing synchronization primitives message passing 
tr uucs univ utah november 
stanford flash multiprocessor 
proc 
st isca pp 
may 

lim agarwal 
reactive synchronization algorithms multiprocessors 
proc 
sixth asplos asplos vi pp 
october 
mellor crummey scott 
algorithms scalable synchronization shared memory multiprocessors 
acm trans 
computer systems february 
hallaron shewchuk gross 
architectural implications family irregular computations 
proc 
fifth hpca pp 
feb 
chien 
high performance messaging workstations illinois fast messages fm myrinet 
proc 
sc 
reinhardt larus wood 
user level shared memory 
proc 
st isca pp 
apr 
rudolph segall 
dynamic decentralized cache schemes mimd parallel processors 
proc 
th isca pp 
may 
singh 
weber gupta 
splash stanford parallel applications shared memory 
tr csl tr stanford univ apr 
stoller swanson 
direct deposit basic userlevel protocol carpet clusters 
tr uucs univ utah march 
woo singh gupta 
splash programs characterization methodological considerations 
proc 
nd isca pp 
june 
