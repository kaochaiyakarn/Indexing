bayesian partitioning classification regression holmes denison mallick may department mathematics imperial college london sw bz dept statistics texas university college station tx propose new bayesian approach data modelling 
bayesian partition model constructs arbitrarily complex regression classification surfaces splitting design space unknown number disjoint regions 
region data assumed exchangeable come simple distribution 
conjugate priors marginal likelihoods models obtained analytically proposed partitioning space number location regions assumed unknown priori 
markov chain monte carlo simulation techniques obtain distributions partition structures averaging samples smooth prediction surfaces formed 
keywords bayesian linear model classification markov chain monte carlo mcmc regression voronoi tessellation 
regression classification widely researched problems statistics 
importance science spreads fields particular engineers computer scientists devoted time topics 
regression problem easily stated set historic data pairings response variable explanatory variables estimate response location prespecified domain commonly taken convex hull data 
task considered fitting mean response surface domain usual assumption additive noise masks true functional relationship ffl ffl zero mean error vector unknown true regression function surface trying estimate 
shall denote estimate classification problems element response vector takes possible values ck relating classes consideration 
note classes typically specified ordering 
task assign class conditional probability jx predictor value similar regression analysis case fit probability surfaces adopting simple parametric forms jx known restrictive situations due computational simplicity practical method adopt fairly 
increases computational power led statisticians researchers develop data adaptive nonparametric methods flexible parametric counterparts 
examples methods include multivariate adaptive regression splines mars friedman classification regression trees cart breiman local polynomial models fan gijbels neural networks bishop additive models hastie tibshirani 
lately considerable research involving bayesian extensions classical models 
general approach embed classical nonparametric models bayesian probabilistic framework 
examples type approach include chipman 
denison 
holmes mallick smith kohn 
methods shown significant predictive power sample generated models averaged 
partly due unavoidable model countered model averaging 
bayesian model averaging effect smoothing composite model differentiability continuity forced single model sample 
model averaging documented non bayesian procedures methods boosting bagging empirically dramatically improve performance classification models freund schapire breiman 
propose generic approach approximating jx previously discussed classical model 
proposal construct disjoint regions data domain data assumed exchangeable simple distributions 
number regions boundary positions assumed unknown 
model continuous differentiable give unattractive single model estimates commonly output classical analysis 
bayesian framework posterior model averaging produces smooth estimates non smooth single models 
method shall call bayesian partition model bpm simple intuitive idea decomposing design space number homogeneous regions 
specifically domain interest partitioned voronoi tessellation green sibson 
region jx approximated bayes linear bayes multinomial model 
shall assume models independent regions 
method similarities tree structured models breiman data space partitioned disjoint regions 
refrain hierarchical tree structure tends induce strong correlations parameters model variables implicitly related tree structure 
simulation posterior distribution problematic denison smith mallick discussed section 
section 
chosen hard soft partitions data point assigned region 
soft partitions obtained gaussian mixtures sigmoidal hyperplanes assign data points probabilistic fashion different regions 
benefit hard partitioning coupled simple distributions adopt regions provides tractable analytic expression marginal likelihood proposed tessellation 
availability marginal likelihood greatly reduces model space parameters relating data distributions regions integrated allows inference directly partition structure 
soft partition structure marginal likelihood computationally infeasible evaluate classification tasks greatly increased regression case 
discussed section 
soft gaussian mixtures intuitively unattractive points far away gaussian centres automatically assigned gaussian component largest variance discussed section 
samples posterior distribution bpm model space generated markov chain monte carlo methods gelfand smith 
availability marginal likelihood allows adopt flat priors number location region centres 
contrast previous methods explicit prior penalising model dimension adopted avoid overfitting data green denison 
note marginal likelihood contains automatic penalty complex models kass raftery 
modelling data partitions new idea 
examples bayesian setting include barry hartigan green 
papers partitions single problems form dependence usually assumed parameters neighbouring partitions 
leads distinct problems 
firstly dependence partitions needs specified making model complex user set parameters 
secondly marginal likelihoods typically available dependence assumed making computations far difficult simulations having take place high dimensional posterior probability surfaces 
shall demonstrate partition methodology perform regression classification essentially model 
main advantages model straightforward nature simulation algorithm clever computational tricks need performed efficient simulation posterior 
rest follows 
section introduce bayesian model mcmc algorithm 
give examples curve surface fitting section details real data examples regression classification 
section provides short discussion 
bayesian partition models bayesian partition models motivation bayesian partition model bpm points nearby design space similar values response space 
basis construct number disjoint regions data regions assumed exchangeable 
choose take regions defined voronoi tessellation green sibson 
tessellation determined number centroids split disjoint regions rm points closer centre fx jjx gamma jj jjx gamma jj ig shall take jj jj normalised weighting vector jjwjj places different emphasis different directions generally adopt distance metric 
fig 
shows tessellation structure crosses marking centres region 
took corresponds usual euclidean distance metric 
define notation ease exposition 
take number disjoint regions define ij denote th observation region number points denote design matrix points including intercept term ith partition 
denote set region centres weighting vector define tessellation denotes set model parameters 
regression case assume follows multivariate normal distribution region jx fi oe fi dimensional identity matrix 
suggests data follows linear model region coefficient values fi global error dispersion oe performing classification multinomial likelihood region ik ik ik ik ik jx ik number points class notation described likelihood complete data tessellation model parameters just simple evaluate regression classification model 
bpm similarities classification regression tree cart model breiman bayesian counterparts chipman denison 
written decision tree partition boundaries flexible just axis parallel splitting node see fig 

bpm overcomes difficulties usual tree structure splitting root node allowing multiple splits 
model problematic hierarchical structure making simulation posterior distribution tree structure difficult denison 
prior specification ability assign conjugate priors disjoint regions central modelling approach 
allows obtain marginal likelihood proposed partition structure integrating parameters model jt jt integral explicitly 
conjugate priors strictly necessary adopt approach 
form suggests hierarchical prior setup assign priors jt equality jt 
regression case linear models region choose independent normal prior distributions conditional noise variance allocated inverse gamma prior bernardo smith jt oe fi oe gamma oe gamma ga oe gamma jfl fl prior precision matrix fi fl fl hyperparameters 
find marginal likelihood data jt fl fl gammaf fl gamma fl gamma fl jv delta denotes determinant matrix gamma delta usual gamma function gamma fl gamma posterior variance fi identity matrix suitable dimension 
just usual bayesian linear model discussed lindley smith 
classification problems independent dirichlet priors placed probabilities class bernardo smith jt di jff ff ff ff ik ff ik ff ik prior probability member class standard simplification yields chipman marginal likelihood jt ae gamma ff gamma ff oe gamma ik ff gamma ff need choose suitable values hyperparameters ff ff known marginal likelihood contains natural penalty complex models basis model comparison selection bayesian statistics kass raftery 
afford adopt uniform priors number regions distance weighting vector positions region centres 
choose look region centres marginal predictor values data centres encouraged data prior discrete uniform distribution 
classification tasks require minimum number centres equal number classes 
computational strategy posterior distribution analytically intractable resort approximate sampling methods mcmc techniques gelfand smith 
starting algorithm centre regression problems centres classification tasks centres chosen random marginal predictor values propose moves probability ffl construct new region adding new centre picked random marginal values 
ffl remove region deleting centre 
ffl move centre position randomly marginal predictor points 
ffl alter distance weighting normal proposal density tuned burn phase give acceptance rate 
marginal likelihood computed depending modelling task proposed change accepted probability ff min ae jt jt oe subscripts refer current proposed models respectively 
note fraction term just bayes factor favour model define model probable model structure proposed algorithm accepts proposed model accepted bayes factor 
just metropolis hastings hastings acceptance step proposals drawn prior 
algorithm proceeds samples deemed drawn initial portion discarded ensure algorithm converged close distribution interest jy 
final collection samples construct posterior distributions parameters interest predictive distributions information gleaned 
examples section demonstrate basic ideas underlying partition model 
illustrative purposes choose analyse low dimensional dataset regression classification problems 
efficacy methodology tested challenging problems predictors 
classification consider bpm models different priors 
model referred bpm uses noninformative prior ff ff 
classification contexts prior beliefs suggest class region design space class dominates 
consider bpm informative prior referred bpm ff ffi ffi ik ffi ij ffi ij 
prior strongly influences jth regions classify data coming jth class 
centres tend place middle allocated class clusters 
speech recognition data consider classification task speech recognition data set analysed peng 

data shown fig 
represents categories words uttered speakers speaker repeating word twice 
data points speakers th category missing 
covariates second formants taken spectral analysis recorded utterances 
formants vocal tract resonant frequencies 
particular words labels listed table 
accordance method adopted peng 
split data randomly point training set point test set 
original split unavailable produced separate training test sets order gain comparative word class heard heed hid head hud hod hood table words spoken produce speech recognition dataset 
dataset cart oc bpm bpm average table average misclassification errors runs different seeds point test sets training dataset bpm oc cart models 
results shown noninformative informative prior settings bpm model 
measure performance ran algorithm iterations collecting sample 
results method informative noninformative priors listed table alongside oc software murthy kasif salzberg available ftp cs jhu pub oc 
oc software tool estimating variety different classification tree models 
models considered cart standard classification tree permits axis parallel splits oblique cart oc allows splits linear combinations includes random perturbation step help escape local minima tree structures 
murthy 
compare oc tree learning algorithms appear accurate classifiers kind 
note dataset peng 
analysed misclassification rate fair comparisons result difficult 
example appears informative prior aids classification data set error rate rank bpm statlog best bpm bpm bpm bpm credit diabetes table statlog test errors formance bpm 
reason occasionally class bpm model fails probable class partitions 
causes classification error missing class 
illustrate soft boundaries induced posterior model averaging hard partitions reran model data 
fig 
display contour plots class conditional probabilities jx 
difficult decision boundaries captured model notion variability 
statlog data examples compare predictive performance bpm classification models analysed credit diabetes datasets statlog project 
statlog project provided comparative study common statistical models variety datasets 
details findings project datasets www ncc pt ml statlog 
australian credit data set class classification problem data points predictors 
statlog tests fold cross validation performance measure 
diabetes dataset binary classification problem 
task predict presence diabetes patient recorded attributes 
examples dataset 
time fold cross validation compare performance performance various models 
table show performance bpm relative rank model models tested statlog error rate best result obtained models tested 
note error rate average number cases misclassified 
error rates close best statlog suggests method competitive non tree algorithms 
regression regression examples non informative priors gamma fl fl 
curve fitting dimension task fitting regression function known curve fitting scatterplot smoothing 
basic example simple function comprising linear plus gaussian term shown fig 

true regression function exp gamma gamma rescaled predictor values lie unit interval 
data generated drawing values uniform distribution ffl draw ffl 
function introduced fan gijbels required flexible fitting approach approximation 
studied denison 
dataset fig 
comparison methodologies 
take results iterations markov chain algorithm initial burn period 
iterations marginal likelihoods model settled chose burn period 
fact small burn required due loose model structure impose 
rigid model structures stochastic version tree algorithms chipman denison far greater problems searching model space 
fact parameters integrated helps respect probability surface search take place greatly reduced 
discontinuous nature single partition models shown fig 

model generated sample little interpretability averaging sample obtain meaningful results 
fig 
demonstrates averaging smooth approximation truth 
fact results terms coefficient determination similar denison 

smoothing effect averaging models generated sample seen examining influence neighbouring points predictions different locations 
slowly varying regions points far away influence smooth opposite case curve rapidly changing 
due nature model determine explicitly smoothing performed 
note rewrite bpm single linear model dfi ffl delta delta delta ym diag xm fi fi fi defining similarly expected fitted values fi dv sy hastie tibshirani call smoothing matrix transforms actual response values fitted values 
rows referred equivalent kernels allow direct comparison linear smoothers 
regions true regression function slowly varying expect smooth response areas function rapidly changing 
mcmc output build posterior mean estimate smoothing matrix shown fig 
curve fitting example 
display expected smoothing neighbouring points induced predictor locations 
higher smooth weight placed nearby locations 
median value owner occupied homes crim capita crime rate town zn proportion residential land lots sq ft indus proportion non retail business acres town charles river dummy variable tract bounds river nox concentration parts rm average number rooms dwelling age proportion owner occupied units built prior dis weighted distances boston employment centres rad index accessibility radial highways tax full value property tax rate pupil teacher ratio town bk gamma bk proportion blacks town lstat lower status population table variables boston housing dataset 
smoothing kernel bell shaped see page hastie tibshirani symmetric nature kernel interior points shows model mixes include points equally side 
height smooth indicates estimate receives weighting points close comparison smooth 
due higher curvature function illustrates spatially adaptive nature model 
respect bpm similar local linear regression models variable bandwidth fan gijbels 
bandwidth automatically determined partitioning structure 
curve similar accuracy visually appealing random spline fit denison 
smooth 
fact include scatterplot smoothing example illustrate features model 
dimensional smoothing models specifically designed purpose adequate 
believe bpm suited problems predictors 
regression boston house prices boston housing data introduced harrison rubinfeld hedonic house price indices 
come serve benchmark regression problem number studies nonlinear models quinlan holmes mallick available statlib lib stat cmu edu 
regression problem predict median price owner occupied homes regions tracts boston covariates intercept term 
description variables table 
gain indication relative accuracy bpm method fold model test error mse linear regression model tree neural network bpm table comparison methods boston housing data 
results bpm model reproduced quinlan 
cross validation procedure data splits studied quinlan comparison various learning methods 
table shows comparative results common methods quinlan study result bpm 
seen bpm fits linear regression region 
posterior predictive density point remains locally linear just average linear models samples 
adds extra level interpretability nonlinear models example neural networks projection pursuit regression 
illustrate took point random data th tract plotted fig 
predictive density response densities local linear coefficients 
densities predictor coefficients rescaled standard deviations generated sample 
easy see relevant predictors specifically particular data point largest magnitudes 
find obviously important coefficients crim nox lstat 
broadly agree findings 
discussion bayesian partition model appears offer flexible unified approach tackling classification regression problems shown performs favourably state art nonlinear models 
concept partitioning predictor space regions simple distributions data natural method dealing nonlinearities data main motivation mixture models richardson green 
unappealing nature sharp boundaries individual models overcome posterior model averaging produces soft neighbourhoods point 
method achieving soft boundaries considerable advantages directly modelling gaussian mixtures 
fig 
see unintuitive nature boundaries defined gaussian mixture model 
points far away centres direction assigned gaussian largest variance 
bpm partitioning thought limiting gaussian mixture model component mixture variance tending zero points assigned component nearest centre 
potential disadvantage soft partitions gaussian mixture case posterior partition boundaries determined joint distribution centres variance components 
tendency induce posterior correlation parameters effect mixing mcmc sampler 
constructing partitions determined solely position centres reduce dimensionality model space mp covariates gaussians covariance matrices 
alternative method partitioning provided tree structured methods cart related non axis parallel versions oc 
soft partitioning versions non axis parallel trees described jacobs jordan bayesian analysis considered waterhouse peng papers tree structure assumed known priori 
soft tree models sigmoidal ridge functions suffer unintuitive results shown fig 

tree structures tendency induce strong correlations posterior parameter distributions splits lower tree implicitly defined relative higher 
simulations posterior problematic chipman related discussion especially tree structure treated random 
marginal likelihood computationally infeasible calculate classification models soft partition structure average possible data assignments incurring extra computations form regions data points 
method acceptance probability proposed changes partition structure determined ratio marginal likelihoods straightforward calculate hard partition model 
ratio known bayes factor motivated model choice criteria prevents model overfitting data partitions assign uniform prior number location partitions 
conjugate priors allows analytic determination required bayes factors 
non conjugate priors model adopted technical difficulties simulating regression coefficients class probabilities region 
note partitioning technique model 
examples include tessellations different distance metrics hyperplane partitioning 
key ability derive marginal likelihoods proposed partitioning space 
priors partitions developed 
idea currently investigation regression model distance weighting vector weight prior precision linear coefficients regions 
variables appear little influence shrunk 
mention believe greatest strengths model simplicity explain statisticians connected field 
linear regression multinomial models understood process grouping data points nearby predictor space easy concept grasp 
acknowledgments author supported epsrc research water research centre uk 
third author supported national cancer agency ca 
authors rod murray smith useful discussions prof jacobs providing speech dataset 
barry hartigan 
bayesian analysis change point problems 
am 
statist 
assoc 
bernardo smith 
bayesian theory 
new york wiley 
bishop 
neural networks pattern recognition 
oxford clarendon press 
breiman 
bagging predictors 
machine learning 
breiman friedman olshen stone 
classification regression trees 
wadsworth belmont california 
chipman george mcculloch 
bayesian cart model search discussion 
am 
statist 
assoc 
clark pregibon 
tree models 
statistical models eds 
chambers hastie pp 

pacific grove ca wadsworth 
denison mallick smith 
automatic bayesian curve fitting 
statist 
soc 

bayesian cart algorithm 
biometrika 
bayesian mars 
statistics computing appear 
denison smith mallick 
comment chipman george mcculloch 
am 
statist 
assoc 
fan gijbels 
local polynomial modelling applications 
london chapman hall 
freund schapire 
experiments new boosting algorithm 
machine learning proceedings th international conference 
friedman 
multivariate adaptive regression splines discussion 
annals statistics 
green 
markov chain monte carlo computation bayesian model determination 
biometrika 
gelfand smith 
sampling approaches ti calculating marginal densities 
am 
statist 
assoc 
green sibson 
computing dirichlet tessellations plane 
comp 

harrison rubinfeld 
hedonic housing prices demand clean air 
journal environmental economics management 
vol pp 

hastie tibshirani 
generalized additive models 
london chapman hall 

curve surface estimation dynamic step functions 
practical nonparametric semiparametric bayesian statistics eds 
dey muller sinha 
pp 

springer verlag new york 
madigan raftery 
bayesian model averaging 
technical report 
department statistics colorado state university 
holmes mallick 
bayesian radial basis functions variable dimension 
neural computation 
smoothing piecewise linear models 
technical report 
department mathematics imperial college london 
jordan 
jacobs 
hierarchical mixtures experts em algorithm 
neural computation 
kass raftery 
bayes factors 
am 
statist 
assoc 
murthy kasif salzberg 
system induction oblique decision trees 
artifical intell 
res 
quinlan 
combining instance model learning 
machine learning proceedings tenth international conference amherst massachusetts 
morgan kaufmann 
peng jacobs tanner 
bayesian inference hierarchical experts models application speech recognition 
am 
statist 
assoc 
richardson green 
bayesian analysis mixtures unknown number components discussion 
roy 
statist 
soc 

smith kohn 
nonparametric regression bayesian variable selection 
econometrics 
figures voronoi tessellation 
centroid nearest bayesian partition model represented tree structure 
new point assigned terminal node corresponding whichever centroid closest 
distribution terminal node family indexed parameters formant speech data set 
see table labels 
contour plot class conditional posterior probability surface produced partition model 
gaussian plus linear function data generated function independent added noise 
draw mcmc simulation 
posterior mean estimate regression curve 
posterior mean smoothing matrix taken dashed line solid line dotted line 
indus zn crim nox rm age dis rad tax lstat marginal densities th region boston housing data 
predictive density shown top left graph 
densities normalised unit variance comparison 
gaussian mixture model densities dashed dotted lines respectively 
solid line indicates probability scaling reasons point comes gaussian centred 
