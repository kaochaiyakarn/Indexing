computer experiments koehler owen department mathematics university colorado denver department statistics stanford university deterministic computer simulations physical phenomena widely science engineering 
computers describe flow air airplane wing combustion flame behavior metal structure stress safety nuclear reactor 
widely computer models ones lead area arise design semiconductors computers 
process simulator starts data structure representing unprocessed piece silicon simulates steps etching ion injection produce semiconductor device transistor 
device simulator takes description device simulates flow current varying conditions determine properties device switching speed critical voltage switches 
circuit simulator takes list devices way arranged computes properties circuit 
computer simulations user specify values governing variables 
example process simulation user specify duration temperature step doses energies ion implantation steps 
continuously valued variables 
may discrete variables wet dry 
chapter treats case continuous variables easily adaptable discrete variables especially values 
denote vector input values chosen computer program 
write row vector superscripts denote components assume component continuously adjustable lower upper limit linear transformation taken respectively 
results input dichotomous see 
computer program denoted computes output quantities denoted important quantities describing computer model number inputs number outputs speed computed 
vary enormously applications 
semiconductor problems considered usually 
computer experiments scores hundreds input variables 
motivating applications usually larger 
example interest center switching speed device stability measured breakdown voltage 
problems takes hours evaluate supercomputer runs milliseconds personal computer 
equation differs usual relationship studied statisticians random error term 
program run twice obtained times 
worth discussing statistical approach called 
computer programs written calculate known value way search values goals suppose initial choice give desirable 
engineer scientist deduce program output select new value improvement 
improvement process repeated satisfactory design 
disadvantage procedure may easily designs fully explore design space 
slow especially large improvements say tend appear vice versa 
commonly way exploring design space vary time 
known statisticians approach misleading strong interactions components increasing may improvement increasing may improvement increasing things worse 
usually determined confirmation run increased 
greater difficulty interactions stems missed opportunities best combination increase decreasing time experimentation lead user try 
techniques experimental design may expected help exploring input space 
chapter presents compares statistical approaches computer experiments 
randomness required order generate probability confidence intervals 
approach introduces randomness modeling function realization gaussian process 
second approach random input points balance properties 
goals computer experiments different related goals arise computer experiments 
problem described previous section finding value criterion goals computer experimentation finding simple approximation accurate region values estimating size error gamma estimating fdx sensitivity analysis respect changes finding important response finding competing goals conflict visualizing function uncovering bugs implementation optimization engineering design problems take form optimizing allowable values problem may find fastest chip expensive soda 
usually additional constraint response chip stable able withstand specified internal pressure 
standard optimization methods quasi newton conjugate gradients see example gill murray wright unsatisfactory computer experiments 
methods usually require possibly second derivatives may difficult obtain expensive run 
standard methods depend strongly having starting values 
computer experimentation described useful early stages optimization searching suitable starting value 
useful searching widely separated regions predictor space values 
starting value standard methods superior needs locate optimum precisely 
visualization diaconis points able compute function value necessarily imply understands function 
know function continuous bounded unimodal optimum asymptotes 
computer experimentation serve primitive way visualize functions 
evaluates chosen set points obtaining responses data visualization methods may applied dimensional points plotting responses versus input variables pq plots identifies strong dependencies plotting residuals fit show weaker dependencies 
selecting points desirable values producing histograms plots corresponding values identify promising subregion values 
koehler owen took approach find increasing certain implant dose helped different threshold voltages near common targets nearly equal 
similar exploration identify input combinations crash simulator 
computer experiment designs purpose visualizing functions fit data 
approximation original program may exceedingly expensive evaluate 
may possible approximate simple function approximation holding adequately region interest necessarily domain function fast evaluate instance polynomial neural network mars model see friedman may feasible millions evaluations 
possible brute force approximations problems 
example optimization approached finding best value random runs approximation computer experiments involves choosing gather pairs construct approximation assess accuracy approximation 
integration suppose target value input vector system modeled actual value random distribution df hopefully concentrated near naturally interested df average value distribution 
similarly variance probability exceeds threshold expressed terms integrals 
sort calculation interest researchers studying nuclear safety 
mckay surveys literature 
integration optimization goals appear problem 
robust design problems seek value minimizes variance varies randomly neighborhood approaches computer experiments main statistical approaches computer experiments bayesian statistics frequentist sampling techniques 
essential introduce randomness ways especially problem gauging estimate differ true value 
bayesian framework surveyed sections realization random process 
sets prior distribution space functions values forms posterior distribution certain aspects 
approach extremely elegant 
prior distribution usually taken gaussian finite list function values multivariate normal distribution 
posterior distribution observed function values multivariate normal 
posterior mean interpolates observed values posterior variance may give posterior probability intervals 
method extends naturally incorporate measurement prediction derivatives partial derivatives definite integrals bayesian framework developed evidenced cited sections 
common bayesian methods may difficulty finding appropriate prior distribution 
simulator output derivatives underlying physical reality assuming smoothness function lead gibbs effect overshoots 
numerical difficulty arises bayesian approach requires solving linear equations unknowns data points 
effort involved grows effort computing grows proportionally inevitably limits size problems addressed 
example suppose spends hour computing minute solving linear equations 
finds necessary run times function evaluations time compute grows hour day time solve linear equations grows minute half days 
difficulties bayesian approach motivate search alternative 
frequentist approach surveyed sections introduces randomness function values partially determined pseudo random number generators 
randomness propagated randomness 
approach allows consider deterministic particular avoid having specify distribution material expands proposal owen 
done 
bayesian prediction inference bayesian approach modeling simulator output spatial model adapted geo statistical kriging model 
approach treats bias systematic departure response surface linear model realization stationary random function 
model exact predictions observed responses predicts increasing error variance prediction point move away design points 
section introduces kriging bayesian approach modeling response surfaces computer experiments 
correlation families discussed effect prediction error analysis 
additionally extensions model allow modeling gradient information 
kriging model kriging approach uses component model 
component consists general linear model second lack fit component treated realization stationary gaussian random function 
define design space scaled dimensional vector input values 
kriging approach models associated response fi known fixed functions fi unknown coefficients estimated stationary gaussian random function covariance cov oe gamma point simulator output point gaussian distribution mean fi variance oe linear component models drift response systematic lack fit bias modeled second component 
smoothness properties delta controlled delta 
design fx ng ae yield responses fy consider linear predictor unobserved point kriging approach treats random variable substituting yd best linear unbiased predictor finds minimizes mse yd gamma subject condition yd fi gamma gamma hd fi hd ij ij cov cov cov fi gamma gamma gamma yd generalized squares estimate fi 
mean square error mse oe gamma hd vd gamma component equation generalized squares prediction point design covariance matrix vd second component pulls generalized squares response surface observed data points 
elasticity response surface pull solely determined correlation function delta 
predictions design points exactly corresponding observations mean square error equals zero 
prediction point moves away design points second component equation goes zero yielding generalized squares prediction mean square error point goes oe gamma gamma 
fact results true wide sense gaussian assumption removed 
example consider experiment oe exp gamma 
response unknown function design 
dashed line generalized squares prediction surface delta fi 
effect second component equation pull dashed line observed design points shown solid line 
shape surface amount elasticity pull determined vector gamma function completely determined delta 
dotted lines sigma mse pointwise confidence envelopes prediction surface 
interpretation pointwise confidence envelopes point unknown function truly generated random function constant mean correlation function exp gamma approximately sample paths go observed design points dotted lines predictions confidence intervals different different oe delta 
effect different correlation functions discussed section 
clearly true function generated stochastically 
model prediction quantify uncertainty prediction 
naturally leads bayesian interpretation methodology 
prediction example 
fully bayesian interpretation alternative interpretation equation fully bayesian interpretation uses model way quantifying uncertainty unknown function 
bayesian approach uses model different interpretation fi 
fi random variables prior distribution effect prior distributions quantify prior belief unknown function put prior distribution large class functions hopefully true function delta mixed convolution yield prior distribution pi subsets functions ae data yd observed posterior distribution pi yd calculated 
mean pi yd dg variance var yd gamma pi yd dg posterior distribution input point predictor measure error respectively point 
general kriging bayesian approaches lead different estimators 
prior distribution delta gaussian prior distribution fi diffuse approaches yield identical estimators 
example consider case prior distribution vector fi fi sigma prior distribution delta stationary gaussian distribution expected value zero covariance function equation 
simulator function evaluated experimental design posterior distribution fi fi yd fi sigma fi sigma gamma yd gamma sigma gamma sigma gamma gamma sigma gamma gamma posterior distribution yd gamma yd fi oe gamma gamma gamma gamma posterior distribution gaussian longer stationary 
fi fi sigma gamma gamma posterior variance var yd oe gamma gamma gamma gamma oe gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma oe gamma gammah gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma oe gamma hd vd gamma variance kriging approach 
delta gaussian prior distribution fi diffuse prior bayesian kriging approaches yield identical estimators 
mitchell morris provide depth discussion bayesian approach model fixed mean 
hagan discusses bayes linear estimators ble connection equations 
bayesian approach uses random functions method quantifying uncertainty unknown simulator function delta subjective kriging frequentist approach 
approaches require prior knowledge objective method estimating covariance function bayesian approach additionally requires knowledge parameters prior distribution fi sigma 
reason kriging results bayesian approach diffuse prior distributions gaussian assumption widely computer experiments 
correlation functions discussed selection delta plays crucial role constructing designs predictive process 
consider example section expf 
shows effect prediction 
fi surface elasticity low 
predictions outside design higher observed surface convex nature observed response indicate design range contains local minimum total process 
eventually extrapolations return value fi 
additionally pointwise confidence intervals narrower range design 
displays prediction 
fi surface elasticity high 
prediction line typically smooth curves pulling surface design points 
pointwise confidence intervals wider 
section presents simplifying restrictions delta families univariate correlation functions generating simplified correlation functions 
examples realization families shown explain effect prediction varying parameter families 
furthermore maximum likelihood method estimating parameters correlation family technique implementation discussed section 
effects prediction 
restrictions delta positive definite function correlation function simplicity common restrict delta gamma process delta stationary 
types nonstationary behavior mean function delta modeled linear term equation 
restriction correlation function depend magnitude distance 
jx gamma higher dimensions product correlation function jx gamma mathematical convenience 
delta product univariate correlation functions univariate correlation functions interest 
product correlation function prediction spatial settings 
choices factors product correlation function outlined 
cubic univariate cubic correlation family parameterized ae fl gamma gamma ae fl gamma ae gamma fl fl jdj realizations cubic correlation function ae fl 
ae fl restricted ae fl fl gamma fl fl ensure function positive definite see mitchell morris 
ae corr correlation endpoint observations fl corr correlation endpoints derivative process 
cubic correlation function implies derivative process linear correlation process parameter fl 
prediction model dimension family cubic spline interpolator 
dimensions correlation product univariate cubic correlation functions predictions piece wise cubic variable 
processes generated cubic correlation function mean square differentiable 
shows realizations processes cubic correlation function parameter pairs 
notice realizations quite smooth linear parameter pair 
realizations exponential correlation function 
exponential univariate exponential correlation family parameterized exp gamma jdj 
processes exponential correlation function ornstein uhlenbeck processes 
exponential correlation function mean square differentiable 
presents realizations dimensional processes exponential correlation function 
realizations small global trends local variation 

mitchell morris necessary sufficient conditions correlation function derivative process exponential correlation function 
called smoothed exponential correlation functions 
gaussian sacks welch mitchell wynn generalized exponential correlation function exp gamma jdj realizations gaussian correlation function 

recovers exponential correlation function 
increases correlation function produces smoother realizations 
long processes mean square differentiable 
gaussian correlation function case associated processes infinitely mean square differentiable 
bayesian interpretation correlation function puts prior mass analytic functions 
correlation function appropriate simulator output known analytic 
displays realizations various gaussian correlation function 
realizations smooth 
mat ern univariate correlation functions described zero infinitely times mean square differentiable 
stein recommends flexible family correlation function 
mat ern correlation function parameterized gamma jdj gamma gamma jdj delta modified bessel function order associated process times differentiable amount differentiability controlled realizations bessel correlation function 
controls range correlations 
correlation family flexible correlation families described due control differentiability predictive surface 
displays realizations processes bessel correlation function various values 
small values realizations smooth flat realizations erratic large values 
summary correlation functions described applied computer experiments 
software predicting described 
cubic correlation function yields predictions cubic splines 
exponential predictions non differentiable gaussian predictions infinitely differentiable 
mat ern correlation function flexible degree differentiability smoothness predictions controlled 
general prior information fix parameters particular correlation family oe available 
pure bayesian approach place prior distribution parameters family posterior distribution parameter estimation process 
alternatively empirical bayes approach uses data estimate parameters correlation family oe 
maximum likelihood estimation procedure discussed section 
correlation function estimation maximum likelihood previous subsections section kriging model families correlation functions 
families correlations parameterized parameters control range correlation smoothness corresponding processes 
model assumes oe family parameters delta known 
general values completely known priori 
appropriate correlation family known simulator designers experience regarding smoothness function 
ranges oe parameters delta known similar computer experiment performed 
pure bayesian approach quantify knowledge prior distribution oe delta 
distribute non informative prior different correlation families family unclear 
furthermore calculation posterior distribution generally intractable 
alternative objective method estimating parameters empirical bayes approach finds parameters consistent observed data 
section presents maximum likelihood method estimating fi oe parameters fixed correlation family underlying distribution delta gaussian 
best parameter set correlation family evaluated find best oe delta 
consider case distribution delta gaussian 
distribution response design points yd likelihood fi oe yd gamman oe gamman gamma theta exp ae gamma oe gamma fi gamma gamma fi oe rd design correlation matrix 
log likelihood ml fi oe rd yd gamma ln gamma ln oe gamma ln jr gamma oe gamma fi gamma gamma fi ml fi oe yd fi gamma oe gamma yd gamma gamma fi set zero yields maximum likelihood estimate fi generalized squares estimate fi ml gamma gamma gamma yd similarly ml fi oe rd yd oe gamma oe oe gamma fi gamma gamma fi set zero yields maximum likelihood estimate oe oe ml yd gamma fi gamma yd gamma fi rd known maximum likelihood estimates fi oe easily calculated 
delta parameterized delta delta delta ml fi oe rd yd gamma gamma oe gamma fi gamma gamma fi gamma tr ae gamma rd oe oe gamma fi gamma rd gamma gamma fi generally yield analytic solution set zero delta delta delta 
commonly need assumed 
alternative method estimating nonlinear optimization routine equation function optimized 
value estimates fi oe calculated equations respectively 
equation calculating partial derivatives objective function 
see mardia marshall overview maximum likelihood procedure 
estimating derivatives manufacturing sciences deterministic simulators help describe relationships product design manufacturing process product final characteristics 
allows product designed manufactured efficiently 
equally important effects uncontrollable variation manufacturing parameters product 
product characteristics sensitive slight variations manufacturing process yield percentage units produced may decrease 
furthermore understanding sensitivities product characteristics help design reliable products increase quality product 
simulators need solve differential equations provide gradient response design point little additional computational cost 
simulators require gradient approximated difference equation 
cost finding directional derivative point equal evaluating additional point approximating total gradient requires additional runs 
consider example showing effects including gradient information prediction 
solid lines true function derivative respectively long dashed lines kriging predictors observations 
expected goes design points poor predictor short dashed lines predictors derivative information notice predictor matches interpolations better 
addition gradient information substantially improves fits dotted lines predictors fairer comparison derivative costs equal response cost 
predictor little better interior worse kriging methodology easily extends model gradients 
see delta gamma cov theta theta gamma theta due stationarity delta delta cov theta theta lim ffi ffi gamma ffi lim ffi ffi gamma ffi oe lim ffi ffi gamma ffi oe differentiable delta 
similarly cov theta gammaoe cov theta gammaoe general higher derivatives morris mitchell delta delta delta ap th component cov gamma oe gamma 
response derivative example response predictors 
example derivative predictors 
furthermore directional derivatives directional derivative direction gamma cov theta theta cov oe oe similarly cov theta gammaoe cov theta gammaoe kl matrix nd partial derivatives evaluated kriging methodology modified model gradient information letting theta nm il direction th directional derivative mn combined covariance matrix design responses derivatives entries prescribed equations 
gamma gamma gamma gamma cov cov 
notice differentiable random functions need twice differentiable correlation functions 
problem total gradient information rapid increase covariance matrix 
additional design point increases rows columns 
fortunately new rows columns generally lower correlations corresponding rows columns equal number response 
inversion computationally stable equally sized vd research needed provide general guidelines gradient information efficiently 
complexity computer experiments progress complexity theory branch theoretical computer science shed light computer experiments 
dissertation ritter contains excellent summary area 
consider case regression function 
th order partial derivatives exist mean square sense obey holder condition order fi possible see ritter approximate error decays gamma fi 
error root mean square average randomly generated functions covariance tensor product form considered better 
ritter show error rate approximation case gammar gamma log gamma products covariances satisfying sacks conditions order 
dimensional wiener sheet process result gamma log gamma established 
general case rate integration gamma times rate approximation 
theorem shows rate gammad approximation usually turned rate gammad gamma integration simple device fitting approximation function evaluations integrating approximation adjusting result average approximation error monte carlo function evaluations 
tensor product kernels rate integration gammar gamma log gamma see favorable power log arise theorem 
fact better rates possible tensor product models general covariances suggests tensor product assumption may strong 
tensor product assumption strong average case curse dimensionality approximation 
bayesian designs selecting experimental design key issue building efficient informative kriging model 
random error model wish find designs minimize 
experimental design theories investigate case bias solely variance plays crucial role error fitted model designs pure bias problem computer experiments unclear 
box draper studied effect scaling factorial designs order polynomial model true function quadratic polynomial 
box draper extended results quadratic polynomial model true response surface cubic polynomial 
mean squared error optimal designs close bias optimal designs 
steinberg extended ideas prior model proposed young puts prior distributions coefficients sufficiently large polynomial 
model flexible high ordered polynomials better designs needed 
section introduces design optimality criteria computer experiments entropy mean squared error maximin minimax designs 
entropy designs maximize amount information expected design mean squared error designs minimize expected mean squared error 
designs require priori knowledge correlation function delta 
design criteria described case fixed design size simple sequential designs location th design point determined gamma points evaluated due tendencies replicate 
sequential block strategies designs starting blocks 
depending maximum entropy designs gaussian correlation function 
ultimate goal computer experiment design block utilized refine design reduce design space 
entropy designs lindley introduced measure shannon entropy amount information provided experiment 
bayesian measure uses expected reduction entropy design criterion 
criterion box hill model discrimination 
wynn showed design space discrete lattice minimizing expected posterior entropy equivalent maximizing prior entropy 
definition design de maximum entropy design gamma ln de min gamma ln density yd gaussian case equivalent finding design maximizes determinant variance yd gaussian prior case fi sigma determinant unconditioned covariance matrix fi fi sigmah fi fi fi fi fi fi fi fi vd sigmah fi fi fi fi fi maximum entropy designs gaussian correlation function 
fi fi fi fi fi vd gamma sigmah sigmah fi fi fi fi fi fi fi fi fi fi vd gamma sigmah fi fi fi fi fi fi fi fi fi fi sigmah gamma vd gamma sigmah fi fi fi fi fi fi fi fi fi fi vd sigmah gamma fi fi fi fi fi jv fi fi fi sigmah gamma fi fi fi jv fi fi gamma gamma sigma gamma fi fi fi fi fi fi sigma fi fi fi sigma fixed maximum entropy criterion equivalent finding design de maximizes jv fi fi gamma gamma sigma gamma fi fi fi prior distribution diffuse maximum entropy criterion equivalent jv fi fi gamma fi fi fi fi treated fixed maximum entropy criterion equivalent jv maximum entropy designs gaussian correlation function 
wynn applied measure designs spatial models 
mitchell scott applied entropy measure finding designs computer experiments 
measure amount information experimental design dependent prior knowledge delta delta 
general delta known priori 
additionally optimal designs difficult construct due required theta dimensional optimization design point locations 
describe algorithm adopted successively removes adds points improve design 
shows optimal entropy designs expf gamma 
entropy designs tend spread points plane favor edge design space interior 
example designs displayed points edge points interior 
furthermore designs similar different correlation functions differences 
generally ratio edge interior points constant 
entropy criterion appears insensitive changes location interior points 
johnson moore indicate entropy designs extremely weak correlation functions limiting sense maximin designs see section 
mean squared error designs box draper proposed minimizing normalized integrated mean squared error welch extended measure case bias complicated 
sacks schiller sacks discuss detail designs computer experiments 
definition design integrated mean squared error design min oe gamma dx dependent delta 
design expressed oe gamma trace vd gamma dx pointed sacks elements products functions single input variable multidimensional integral simplifies products dimensional integrals 
entropy design criterion minimization optimization theta dimensions dependent delta 
sacks schiller describe simulated annealing method constructing designs bounded discrete design spaces 
sacks quasi newton optimizer cray mp 
optimizing design delta delta delta took minutes 
pace program uses optimization program solve optimization continuous design space 
optimization requires minutes dec powerful machine cray 
generally algorithms find local minima random starts required 
dependent delta robust designs need general delta 
sacks expf gamma see section details gaussian correlation function design robust terms relative efficiency 
analysis quadratic polynomial model results may extend higher dimensions different linear model components 
sacks optimal design gaussian correlation function design efficiency robustness 
displays designs delta delta delta 
designs general lie interior fixed design size designs usually similar geometrically different values scale decreasing increases 
symmetry values particularly 
notice case design takes unique values input variables 
designs tend clumped projections lower dimension marginals input space 
better projection properties needed true function dependent subset input variables 
maximin minimax designs johnson moore developed idea minimax maximin designs 
designs dependent distance measure metric 
delta delta metric minimum integrated mean square error designs gaussian correlation function 
definition design dmi minimax distance design min max max dmi min minimax distance designs ensure points far design point 
delta delta euclidean distance consider placing dimensional sphere radius design point 
idea minimax design place points design space covered spheres minimal illustration consider owner petroleum wants open franchise gas stations 
gas locate stations convenient sites customers 
minimax strategy placing gas stations ensure customer far stations 
shows minimax design delta delta euclidean distance 
maximum distance design point 
small minimax designs generally lie interior design space 
definition design dma maximin distance design max min min dma minimum integrated mean square error designs gaussian correlation function 
delta delta euclidean distance 
maximin designs pack design points associated spheres design space maximum radius 
parts sphere may design points analogous minimax illustration position owners gas station 
wish minimize competition locating stations far apart possible 
maximin strategy placing ensure stations close 
shows maximin design delta delta euclidean distance 
small maximin designs generally lie exterior fill interior large 
hyperbolic cross points tensor product covariance models possible approximate integrate functions greater accuracy general case 
gets rates convergence univariate problems apart multiplicative penalty power log hyperbolic cross point designs known sparse grids shown achieve optimal rates cases 
see ritter 
point sets developed 
interpolation wahba gordon integration 
chapter ritter gives description construction points lists 
minimum integrated mean square error designs gaussian correlation function 
frequentist prediction inference frequentist approach prediction inference computer experiments numerical integration 
scalar function consider regression model form fi row vector predictor functions fi vector parameters 
suitable functions include low order polynomials trigonometric polynomials wavelets functions specifically geared application 
ordinarily includes component equal order introduce intercept term equation 
unrealistic expect function exactly representable finite linear combination unrealistic expect residual random variable mean zero fixed write 
ways define best value fi especially natural approach choose fi minimize mean squared error approximation respect distribution optimal value fi fi ls df gamma df integrate domain fit regression approximations 
quality approximation may assessed globally integrated mean squared error gamma fi df minimax minimax maximin maximin minimax maximin designs euclidean distance 
simplicity take distribution uniform simplicity integration schemes considered usually estimate df chosen points fi ls may estimated linear regression fi gamma integrals squares cross products known fi df gamma choosing components orthogonal basis tensor products orthogonal polynomials multivariate fourier series wavelets equation simplifies fi avoid cost matrix inversion 
computation required equation grows proportionally nr number regression variables computations grow example section hour function evaluation followed minute algebra scale day function evaluation followed hours algebra days algorithm require 
exhibit sparsity may possible reduce algebra order order log idea turning function data making exploratory plots extended turning function data applying regression techniques 
theoretically simplest technique take iid iid pairs complication zero variance variance matrix fi gamma var gamma delta gamma orthogonal predictors simplifies var gamma delta integration scheme allows estimate variances covariances averages times components allows estimate sampling variance matrix regression coefficients fi 
iid sampling estimate variance matrix gamma gamma gamma fi gamma fi row vector comprises intercept additional regression coefficients 
approach computer experimentation improve accurate integration techniques substituted iid sampling 
owen investigates case latin hypercube sampling central limit theorem holds 
clearly needed method practical 
instance scheme deciding predictors regularizing fi required 
frequentist experimental designs frequentist approach proposed previous section requires set points numerical integration allow estimate sampling variance corresponding integrals 
goals somewhat odds 
iid sample variance estimation easier complicated schemes described improve accuracy variance estimation harder 
basic goal getting points interesting corners input space important features usually served point sets numerical integration 
assume region interest unit cube integrals interest respect uniform distribution cube 
regions interest usually reduced unit cube distributions changed uniform change variable subsumed section consider example plot design points grids varying coordinate time cause important aspects natural consider sampling regular grid 
chooses different values runs combinations 
works small values larger completely impractical number runs required grows 
distinct points points grid 
shows projection points uniform grid input variables 
notice runs distinct values appear plane representing input settings variables 
distinct values appear input variable taken singly 
situations responses depends strongly inputs grid design leads wasteful duplication 
grid design lend variance estimation averages grid random 
accuracy grid integral typically univariate integral evaluations 
see davis 
large severe disadvantage 
lattice points significant improvement grids may obtained integration method lattice points 
see sloan joe background fang wang applications statistics 
lattice points ae gamma oe fzg modulo minus greatest integer equal integers 
points ih integer form lattice points versions lattice points confined unit cube term refers careful choice usually number theory 
shows fibonacci lattice 
details see 

fibonacci lattice available dimensions 
appendix fang wang lists choices lattice points smallest value point fibonacci lattice 
discusses greedy algorithms finding lattice points smaller text discusses lattice rules integration generalize method lattice points 
patterson consider randomly perturbing lattice points adding modulo random vector uniform random offsets data points gives nr observations gamma degrees freedom estimating variance 
lattice integration rules extraordinarily accurate smooth periodic approach computer experiments patterson method expected smooth periodic 
bates buck wynn explored lattice rules designs computer experiments 
latin hypercubes lattice points start improving low dimensional projections grids latin hypercube sampling starts iid samples 
latin hypercube sample gamma independent uniform random permutations integers independent random variables independent latin hypercube sampling introduced mckay beckman conover widely considered computer experiments 
sample points stratified points latin hypercube sample 
range input variable may partitioned bins equal width drawn horizontal vertical dotted lines bin contains points 
input axes 
common variant latin hypercube sampling centered points gamma point sets type studied patterson called lattice samples 
shows projection points centered latin hypercube sample variables coordinate axes 
input variable gets explored equally spaced bins 
stratification latin hypercube sampling usually reduces variance estimated integrals 
stein finds expression variance sample mean latin hypercube sampling 
assuming df write ff df ff gamma df gammaj df gammaj dx uniform distribution input variables th 
equation expresses sum grand mean univariate main effects ff residual additivity 
stein shows latin hypercube sampling var df iid sampling var df ff df balancing univariate margins latin hypercube sampling removed main effects function error variance 
owen proves central limit theorem latin hypercube sampling bounded functions loh proves central limit theorem weaker conditions 
variance estimation latin hypercube sampling see 
better latin hypercubes latin hypercube samples look random scatter bivariate plot quite regular univariate plot 
effort find especially latin hypercube samples 
approach find latin hypercube samples input variables small correlations 
conover perturbed latin hypercube samples way reduces diagonal correlation 
owen showed technique typically reduces diagonal correlations factor method empirically reduce diagonal correlations factor order gamma gamma 
removes certain bilinear terms lead term error 
iterating method lead large improvements 
small correlations desirable sufficient construct centered latin hypercube samples zero correlation equal modulo highly structured 
example points arranged diamond shape plane missing center corners input space 
researchers looked latin hypercube samples having properties considered designs bayesian prediction 
park studies criterion morris mitchell consider entropy 
randomized orthogonal arrays orthogonal array matrix integers gamma 
array strength submatrix possible rows appear number times 
course independently owen tang considered orthogonal arrays improve latin hypercube samples 
randomized orthogonal array versions just latin hypercube sampling versions 
latin hypercube sampling corresponds strength 
independent uniform permutations gamma 
patterson considered schemes centered version 
points randomly centered randomized orthogonal array 
whichever variables plotted point square 
plot points randomized orthogonal array fewer coordinates result regular grid 
points randomized orthogonal array strength appear randomly scattered dimensions 
shows projection points randomly centered randomized orthogonal array variables coordinate points 
pair variables gets explored square bins 
plot centered version randomized orthogonal array identical grid shown 
analysis variance decomposition latin hypercube sampling extended include interactions factors 
see efron stein owen wahba details 
gu wahba describe estimate form confidence intervals main effects noisy data 
owen shows main effects interactions fewer variables contribute asymptotic variance mean randomized orthogonal array owen shows variance approximately gamma times sum integrals squares interactions inputs 
tang introduced orthogonal array latin hypercube samples 
points designs latin hypercube samples orthogonal array 
integer smallest integer equal tang shows strength array main effects variable interactions contribute integration variance 
shows projection points oa latin hypercube sample variables coordinate points 
variable individually gets explored equal bins pair variables gets explored squares 
points orthogonal array latin hypercube sample 
whichever variables plotted point square bounded solid lines 
variable sampled horizontal vertical bins 
scrambled nets orthogonal arrays developed balance discrete experimental factors 
seen embedded unit cube randomized result sampling variance reduced 
numerical analysts developed integration techniques directly adapted balancing continuous space 
describe nets 
full account nets 
randomization described owen 
integers 
elementary subcube base form integers integer 
set points net base elementary subcube base volume gammam exactly points 
cell point sequence point sequence 
strong form weakening somewhat constructions values available 
nonnegative integer 
finite set points net base elementary subcube base volume gammam contains exactly points sequence 
cells points points cells point 
points scrambled net base 
whichever variables plotted point square 
variable sampled equal bins 
common usage name net assumes letter denote dimension input space speak nets 
convention note subcubes half open 
convenient partition input space congruent subcubes 
balance properties net greater orthogonal array 
net base orthogonal array strength gamma tg 
net balance properties rounded different powers axes long powers sum gamma net combines aspects orthogonal arrays multi level orthogonal arrays point set 
case net base points count elementary subcubes volume varying aspect ratios points 
infinite sequence points sequence base finite sequence kb net base advantage sequence finds points sufficient integration problem find points form net tend fill places occupied set 
continues point having nets complete set points comprises net 
theory nets sequences 
famous result theory integration net attain accuracy order log gamma restricting sequences raises slightly log 
results require integrand points scrambled net base 
whichever variables plotted result grid point latin hypercube samples 
variable sampled equal bins 
triple variables partitioned congruent cubes point 
bounded variation sense hardy krause 
large takes unrealistically large rates clearly better gamma examples outperform simple monte carlo 
construction nets sequences described 
prime numbers construction faure gives nets base extended method prime powers 
see 
choose smallest prime power greater equal variables corresponding sequence base owen describes scheme randomize nets sequences 
points written base expansion certain random permutations applied coefficients expansion 
result permuted uniformly distributed preserving net sequence structure ensemble sample estimate gamma unbiased df variance may estimated replication 
test randomized nets outperformed counterparts 
appears nets considerable structure stemming algebra underlying structure liability integration 
shows points scrambled net base projected input coordinates 
points initial points sequence base 
design properties oa latin hypercube sample 
consecutive points sequence properties 
points shown properties points scrambled net base 
whichever variables plotted square divided squares side rectangles side rectangles side rectangle points 
variable sampled equal bins 
triple variables partitioned hyperrectangles different ways hyperrectangle points 
quadruple variables partitioned congruent hypercubes side point 
triple input variables split subcubes pair variables points appear grid point latin hypercube samples individual input variable split cells having point 
points shown 
owen finds variance formula means randomized nets sequences 
formula involves wavelet anova combining nested terms coordinate crossed 
turns square integrable integrand resulting variance gamma beats usual variance reduction techniques typically reduce asymptotic coefficient gamma smooth variance fact gamma general case owen shows variance gamma log gamma 
selected applications largest fields developing deterministic simulators designing manufacturing vlsi circuits 
alvarez describe iii sedan ii designing devices 
aoki dimensional device simulator optimizing devices accurate prediction device sensitivities 
supreme iii ii compute cmos device characteristics function technology parameters 
director describe fabrics ii estimate circuit delay times integrated circuits 
input variables generally device sizes metal concentrations implant doses gate oxide temperatures 
multiple responses threshold voltages subthreshold slopes saturation currents linear output variables concern depend technology investigation 
engineers physical numerical simulators assist optimizing process device circuit design costly step building prototype devices 
concerned minimizing transmitted variability significantly reduce performance devices reduce yield 
example welch sacks discuss simulators investigate effect transistor dimensions clocks 
want find combination transistor widths produce zero clock skews small transmitted variability due uncontrollable manufacturing variability transistors 
simulator developed tucker iii helps optimizing compression mold filling process manufacturing automobiles 
process sheet compound cut placed heated mold 
mold slowly closed constant force applied reaction 
controlling variables process geometry thickness part compound viscosity shape location charge mold closing speed 
simulator predicts position flow front function time 
miller discuss computers solve systems differential equations describing chemical kinetic models 
inputs simulator vectors possibly unknown combustion rate constants outputs induction delay times concentrations chemical species specified reaction times 
objectives investigations find values rate constants agree experimental data find important rate constant process 
sacks explore design issues applications field 
thermal energy storage model developed alan solomon colleagues oak ridge national laboratory simulates heat transfer wall containing layers different phase change material 
utilize computer experiment 
inputs layers dimensions thermal properties materials characteristics heat source 
object interest finding configuration input variables produce highest value heat storage utility index 
foam models transport aromatic hydrocarbon spills streams structure activity relationships 
bartell modified model predict fate introduced 
model tracts evaporation surface synthetic oil degradation dissolved suspended matter accumulation pond 
monte carlo error analyses assess effect uncertainty model parameters results 
alvarez abdi young weed herald application statistical design response surface methods computer aided vlsi device design 
ieee trans 
computer aided design 
aoki masuda shimada sato new design centering methodology vlsi device development 
ieee trans 
computer aided design 
bartell gardner neill error analysis predicted fate simulated pond 
environmental toxicology chemistry 
bartell simulated transport aromatic artificial streams 
mitch editors energy ecological modelling pages 
elsevier new york 
bates buck wynn experimental design observation large systems discussion 
royal stats 
soc 
pp 

total entropy criterion dual problem model discrimination parameter estimation 
journal royal statistical society series 
box draper basis selection response surface design 
journal american statistical association 
box draper choice second order rotatable design 
biometrika 
box hill discrimination mechanistic models 
technometrics 
church mitchell fleming computer experiments optimize compression mold filling process 
talk workshop design computer experiments oak ridge tn november 
patterson randomization number theoretic methods multiple integration 
siam numer 
anal 
cressie kriging nonstationary data 
journal american statistical association 
mitchell morris bayesian prediction deterministic functions applications design analysis computer experiments 
journal american statistical association 
performance improvement restricted pairing algorithm latin hypercube sampling draft report energy information administration 
davis methods numerical integration second edition academic press san diego 
diaconis bayesian numerical analysis gupta berger editors statistical decision theory related topics iv vol 
pages springer verlag new york 
efron stein jackknife estimate variance 
ann 
statist 

fang wang number theoretic methods statistics chapman hall london 
faure discr des suites associ ees un syst eme de num eration en dimension acta 
friedman multivariate adaptive regression splines discussion annals statistics 
gill murray saunders wright user guide version fortran package nonlinear programming 
sol stanford optimization laboratory dept operations research stanford university california january 
gill murray wright practical optimization 
academic press london 
gordon blending function methods bivariate multivariate interpolation approximation 
siam numer 
anal 

gu wahba smoothing spline anova component wise bayesian confidence intervals 
comp 
graph 
stat 
quadrature error bounds applications lattice rules siam numer 
anal 
ho hansen iii program integrated circuit process modeling simulation 
tr sel stanford electronics laboratories 
conover free approach inducing rank correlation input variables commun 
stat 
johnson moore minimax maximin distance designs 
journal statistical planning inference 
mining 
academic press london 
koehler design estimation issues computer experiments 
dissertation dept statistics stanford university 
lindley measure information provided experiment 
mathematical statistics 
loh 
latin hypercube sampling technical report dept statistics purdue university 
loh 
combinatorial central limit theorem randomized orthogonal array sampling designs technical report dept statistics purdue university 
mardia marshall maximum likelihood estimation models residual covariance spatial regression 
biometrika 
mat ern method estimating accuracy line sample plot surveys 

inst 
principles 
econom 

mckay evaluating prediction uncertainty 
report cr los alamos national laboratory 
mckay beckman conover comparison methods selecting values input variables analysis output computer code 
technometrics 
miller sensitivity analysis parameter estimation dynamic modeling chemical kinetics 
international journal chemical kinetics 
mitchell algorithm construction optimal experimental designs 
technometrics 
mitchell morris existence smoothed stationary processes interval 
stochastic processes applications 
mitchell morris level fractional factorials bayesian prediction 
statistica sinica 
mitchell scott computer program design group testing experiments 
communications statistics theory methods 
morris mitchell exploratory designs computational experiments journal statistical planning inference 
morris mitchell bayesian design analysis computer experiments derivative surface prediction 
technometrics 
director fabrics ii statistically ic fabrication process simulator 
ieee trans 
computer aided design 
random number generation quasi monte carlo methods siam philadelphia 
hagan comment design analysis computer experiments 
statistical science 
owen central limit theorem latin hypercube sampling 
journal royal statistical society 
owen orthogonal arrays computer experiments integration visualization statist 
sinica pp 

owen lattice sampling revisited monte carlo variance means randomized orthogonal arrays 
ann 
statist 
pp 

owen controlling correlations latin hypercube samples 
journal american statistical association 
owen randomly permuted nets sequences 
editors monte carlo quasi monte carlo methods scientific computing pp 
springer new york 
owen monte carlo variance scrambled net quadrature 
siam numer 
anal appear 
owen scrambled net variance integrals smooth functions 
technical report number department statistics stanford university 
average case complexity multivariate integration smooth functions 
complexity 
park 
optimal latin hypercube designs computer experiments journal statistical planning inference 
parzen stochastic processes 
holden day san francisco 
patterson errors lattice sampling 
journal royal statistical society 
quality engineering robust design prentice hall englewood cliffs nj 
pinto ii continuity equation solver 
stanford electron 
lab 
ripley spatial statistics 
wiley new york 
ritter average case analysis numerical problems dissertation university erlangen 
ritter multivariate integration stochastic processes 
numerical integration brass eds pp 
birkhauser verlag basel 
ritter multivariate integration approximation random fields satisfying sacks conditions 
ann 
appl 
prob 

visualization exploration high dimensional functions functional anova decomposition 
dissertation dept statistics stanford university 
sacks schiller spatial designs 
gupta berger editors statistical decision theory related topics iv vol 
pages springer verlag new york 
sacks schiller welch designs computer experiments 
technometrics 
sacks welch mitchell wynn design analysis computer experiments 
statistical science 
shannon mathematical theory communication 
bell system technical journal 
koehler owen simulators model transmitted variability ic manufacturing 
ieee trans 


wynn maximum entropy sampling 
journal applied statistics 
wynn maximum entropy sampling simulation codes 
proceedings th world congress scientific computation volume pages 

sloan joe lattice methods multiple integration oxford science publications oxford 
quadrature interpolation formulas tensor products certain classes functions 
soviet math 
dokl 

stein large sample properties simulations latin hypercube sampling 
technometrics 
stein comment design analysis computer experiments 
statistical science 
steinberg model robust response surface designs scaling level factorials 
biometrika 
tang latin hypercubes designs dissertation dept statistics science university waterloo 
tang orthogonal array latin hypercubes amer 
statist 

masuda aoki dimensional device simulator highly convergent matrix solution algorithms 
ieee trans 
electron device ed 
wahba interpolating surfaces high order convergence rates associated designs applications ray image reconstruction 
university wisconsin madison statistics department tr may 
wahba spline models observational data 
cbms nsf regional conference series applied mathematics siam 
integration approximation multivariate functions average case complexity wiener measure bull 
amer 
math 
soc 

full version approx 
theory 
average case complexity multivariate integration 
bull 
amer 
math 
soc 

welch mean squared error criterion design experiments 
biometrika 
welch yu kang sacks computer experiments quality control parameter design 
journal quality technology 
correlation theory stationary related random functions volume 
springer new york 
designs random fields 
srivastava editor survey statistical design linear models pages amsterdam 
north holland 
young bayesian approach prediction polynomials 
biometrika 
yu chang supplementary report sedan ii 
tr stanford electronics laboratories 
