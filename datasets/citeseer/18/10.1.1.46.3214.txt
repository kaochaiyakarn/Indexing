data mining tools irfan school information studies charles university nsw australia edu au sergey department mathematics university queensland brisbane qld 
australia sergey maths uq edu au markus computer sciences laboratory advanced computational systems crc australian national university canberra act australia markus anu edu au steve roberts computer sciences laboratory advanced computational systems crc australian national university canberra act australia stephen roberts anu edu au department statistics university adelaide adelaide sa australia stats adelaide edu au graham williams csiro australia gpo box canberra act australia graham williams csiro au discuss scalable parallel discovery predictive data mining tools 
successfully address computational challenges associated analysis data sets millions billions records tens hundreds attributes 
particular describe parallel scalable methods additive models approximate thin plate splines adaptive regression splines 
addition describe method discover symbolic descriptions key areas large data sets 
keywords data mining tools thin plate splines regression smoothing additive models parallel data mining evolutionary hot spots high performance computing ieee concurrency due increasing computing power advances data collection databases growing size stage traditional techniques analysis visualization data breaking 
data mining systems specifically designed able analyze large data sets 
data mining automatic discovery patterns changes associations anomalies large data sets go undiscovered 
data mining emerging key enabling technology variety scientific engineering medical business applications 
data mining techniques origins methods statistics pattern recognition databases artificial intelligence high performance parallel computing visualization 
increasing number industries banking insurance retail outlets started analyze customer data understand needs preferences behaviors customers 
industries employed statistical analysis tools success years unseen patterns data remain discovered aid new data mining tools 
data mining tools early stages development 
range data mining tools need analysis large data set 
cases data mining analysts sure data data warehouses data consistent accurate decide data mining tools suitable 
preparing data apply data mining tools known data cleaning process 
process data cleaned sense missing invalid values treated known valid values consistent 
cleaning data process may transform data appropriate transformations maximize information extracted 
consequence transformation removal ect outliers 
consequence data interpretable 
example logarithm variable highly skewed distribution yields transformed variable distribution closer normal distribution 
various approaches data mining er distinct benefits features reality exists basic techniques basis data mining systems 
main classes identified data mining process discovery predictive modeling forensic analysis 
discovery activity finding hidden patterns large database 
large databases contain huge number patterns 
power suitability discovery method employed data mining system measured size data handle quality information pattern delivers 
predictive modeling stage patterns discovery stage predict observations 
discovery stage basically reveals patterns database predictive modeling stage uses patterns estimate values new data items 
applying discovered patterns find unusual data elements forms forensic analysis stage data mining process 
detailed explanation process standard methods shortcomings benefits 
patterns extracted data set represented tables logical expressions equations applied method 
parallel data mining methods discuss represent patterns regression equation 
additive models second approximate thin plate splines finite element method third spline multivariate adaptive regression splines 
fourth method discovers key interesting areas data representing patterns sets rules 
decades high performance computing focused developing algorithms software hardware problems computationally intensive 
contrast problems data mining computationally intensive data intensive sense involve large data sets just concerned high performance data management high performance computing 
hand high performance computing crucial ensuring system scalability interactivity data sets grow size complexity 
remaining sections approaches prediction mainly regression discovery 
address di erent aspects major computational challenges data mining relating data size high dimensionality 
survey joint done data mining program advanced computational systems cooperative research centre 
section algorithm determination additive models 
algorithm able handle large data sizes need fit main memory 
algorithm parallel partitioning data set 
application risk analysis insurance showed outperformed earlier processor method factor processors maintaining quality 
section talk smoothing approach thin plate splines able deal large data sets dimensional functions 
note just years ago thought impossible handle data sets points radial basis functions 
algorithm successfully applied data points 
example detect interactions taxation fraud data 
section variation popular mars algorithm splines parallel implementation 
hierarchical approach scales allowed better resolution local variations higher computational ciency 
addition parallel implementation method shown scale data size number processors 
parallel system discovery symbolic descriptions key areas large data sets 
parallel additive models predictive data mining tools deal large data sets high dimensions big noise signal ratio 
example application coming insurance industry required determination risk customer lodging claim function variables 

risk function needs determined insurance database contains records average residual estimate small 
predictors capitalize fact predictive functions frequently smooth 
estimate obtained kind average observed values values close approach fails case large high dimensional data sets due curse dimensionality 
fact high dimensions nearest neighbors average point far away point conversely large number points approximately distance 
disastrous ect computational costs day determination predictive model remains major challenge circumstances 
order overcome curse class models proposed known generalized additive models 
take form 
special case additive models linear models 
additive models appear analysis variance predictor variables categorical 
similar generalized linear models generalized additive models allow inclusion constraints estimation risks 
di erent generalization obtained allowing functions variables 
example interaction model known mars algorithm 
discuss parallel version mars algorithm section 
estimation additive model requires estimation dimensional functions original high dimensional problem reduced set coupled dimensional problems curse dimensionality longer active 
determination dimensional functions fact assumes observations instances random variables best linear estimate expectation linearity expectation operator gets know compute dimensional smoother 
vector values observation points denoted 
smoothing matrix vector observed values fix point problem case linear smoothers takes form 


fix point problem typically solved block gauss seidel method solution algorithm called back fitting 
particular linear case back fitting studied 
application smoother back fitting step require reading data iteration step 
furthermore values smooth residual need stored 
storage amount total database required 
furthermore data needs accessed iteration step 
frequently realistic keep total database plus residual memory disk access required frequently slows algorithm considerably 
principle implement back fitting multiple processors 
change respect memory requirements processor need access data iteration step 
leads proposal new parallel algorithm determine estimators functions step data partitioned blocks equal size 
model fitted block back fitting algorithm 
order reduce bias approach smoothing parameters smoothers chosen data overfitted 
parallel smoothing step results merged average individual smoothes computed 
merging steps final smoothing step reduces variance smoother 
merge fit partition merge depth fitting additive model splitting data blocks fitting additive model block merging models 
blocking merging algorithm illustrated 
overfitting achieved smoother small bandwidth computing smooth fine grid 
typical path functions illustrated original rough data final smooth partitioning merging final smoothing step 
algorithm steps 
partition data set disjoint blocks equal size containing randomly selected elements 
block fit local memory 

fit additive model back fitting data partition 
select smoothing parameter fitted model low bias possibly high variance 
compute values smooth fixed fine grid 

merge local results averaging function values di erent partitions 

final smoothing step order reduce variance 
theoretical investigations pending approach successfully applied synthetic real data 
seen compared usual regression spline approach parallel algorithm times faster reducing analysis time hours minutes maintaining quality fit 
furthermore algorithm scalable number data points number predictor variables 
initial overfitting step shown essential 
best fit generalized cross validation criterion initial smoothing steps markedly increases bias 
illustration function calculated 
data divided blocks 
block back fitting algorithm fits function data block merging step merges di erent fits 
thin plate splines surface fitting smoothing splines techniques widely fit smooth data 
algorithm various applications surface fitting dimensional data digital elevation models data mining interaction terms additive model 
algorithm interpreted approximate thin plate spline handle data sizes millions records 
combines favorable properties finite element surface fitting ones thin plate splines 
briefly mathematical foundations algorithm 
details method 
standard thin plate spline function minimizes functional dx predictor variables response variables 
smoothing problem approximated finite elements 
order reduce complexity problem suggest simple elements tensor products piecewise linear functions 
functions required second derivatives exist 
non conforming finite element principle 
gradient denote usual sobolev space dx satisfies consider minimizer functional dx minimum taken functions zero mean subject constraint 
functional exactly minimum observation points collinear 
function defined provides smoother essentially smoothing properties original thin plate smoothing spline 
discretization parallel rapid development microcomputers fast networking high speed switches parallel processing distributed clusters workstations emerged cost ective method high performance computing various applications 
data mining applications data sets large large scale data sets usually logically physically distributed 
distributed cluster workstations parallel computing environment serve handle distributed data sets 
pick cluster workstation parallel computing environment implementations 
spmd single program multiple data programming model suits implementation 
spmd model code asynchronously executed di erent workstations handling di erent sets data 
discuss parallel implementation algorithm paragraphs 
discretization equations previous section produces linear system 
seen linear system symmetric indefinite sparse 
size system independent number data points size linear system depends number nodes finite element discretization 
linear system consists sub systems 
fourth sub system dimension dimension total dimension linear system 
employed generalized cross validation multigrid algorithm solve linear system 
explain algorithm details 
approach reported 
matrix third subsystem singular adjacent elements finite element discretization contain data points 
algorithm fails cases 
order avoid cases slightly modify linear system 
idea similar gauss elimination process 
gauss elimination pivot entry zero row consideration replaced rows produces non zero pivot element 
replace positions third fifth sub systems 
modified algorithm form block gauss seidel iteration linear system longer symmetric 
solve sub systems coe cient matrix matrix symmetric positive definite diagonals non zero elements 
employ conjugate gradient method pre conditioning solve sub systems iteration 
note fourth sub system dimension 
solved direct method ignored discussions 
parts algorithm benefits increased computing power 
part assembly matrices xy ny second part solution system 
assembly matrices depends data 
data point visited form matrices 
time complexity part algorithm 
assembly remaining matrices independent number data points time required assemble negligible relative assembly time xy ny 
example sun sparc station total time assembly xy ny ms grid resolution data points 
takes ms assembly setting 
input data file equally partitioned files processors di erent file 
approach improves assembly time matrices xy ny reading time large data set case data mining applications 
processor assembles local matrices 
assembly local matrices collected summed processor produce final matrices xy ny 
matrix symmetric consists diagonals non zero elements 
floating point values stored communicated matrix matrix dimension 
ny xy vectors dimensions floating point values communicated matrices 
obtain ideal speedup assembly stage amount communicated data floating point values small compared number data points table show test results parallel implementation assembly matrices 
test platform sun sparc workstation cluster networked mbit twisted pair ethernet 
model workstations cpu mhz mbytes main memory 
number nodes log comput 
time log commun 
time speedup process 
illustration speedup communication computation times processors table introduce timings milliseconds di erent number processors 
serial assembly takes ms number parallel assembly processors computation communication total table timings milliseconds assembly matrices employ processors parallel solution linear system 
processors solve simultaneously second sub systems respectively 
sends resulting vector solves third fifth sub systems 
convergence check sends vector completes step iteration process 
approach maximum achievable speed amdahl law solve sub systems parallel environment 
data set speedups achieved respectively 
parallel multivariate adaptive regression splines original mars designed estimate high dimensional regression functions continuous categorical covariates 
regression function estimation problem cast form 
data set 
estimate regression function relates covariates response 
independent identically distributed zero mean random variables account dependency factors represented data mining algorithms regression estimation required produce interpretable models scale large data sets reasonably 
interpretable model defined model regression function involves covariates exhibit strongest ects 
mars proved able meet requirements 
original mars data set mars builds regression models linear combinations certain tensor product spline basis functions 
number structure basis functions included model determined algorithm adaptively data set hand 
sake simplicity confine discussion case continuous variables 
jth tensor product basis function form kj kj 
number univariate factors basis function kj takes values argument kth univariate truncated power basis function kj kj characterized knot location kj covariate 
exponent order spline approximation 
friedman implementation mars piecewise linear argument tensor product basis functions 
mars algorithm uses forward backward stepwise strategy produce set basis functions 
forward part recursive 
specifically starts basis function 
suppose basis functions current model jth step procedure 
functions model form 
st step produces tensor product basis functions 
new basis functions formed multiplying previously produced basis functions univariate factors characterized argument covariates involved basis function knot location optimal values parameters correspond largest decrease residual sum squares resulting inclusion new functions current model 
forward stepwise procedure mars produces max tensor product basis functions max set large order ensure relevant basis functions included model 
due greedy nature algorithm functions model may prove sub optimal 
removal functions normally improves accuracy level model accomplished example standard backward elimination algorithm 
algorithm selects sub model full model produced forward part mars certain estimate prediction error minimized 
generalized cross validation score estimate prediction error regression model gcv 
smoothing parameter regression set tensor product basis functions regression coe cients determined squares fit 
modification mars splines mentioned mars utilizes truncated power basis functions 
functions known poor numerical properties 
introduce new version mars splines behaved numerical point view 
principle implement splines order 
splines second order hat piecewise linear functions having compact support 
reason functions fold utilization simplest continuous splines significantly simplifies implementation algorithm second approximation piecewise linear variable functions resistant called ects 
owing compact support property spline introduce notion scale spline just length support interval 
set knots placed continuous variable splines various scales introduced selecting appropriate subsets knots 
example knots construct functions log di erent scales splines th scale log built subset derived form full set knots retaining st knot dropping rest 
original mars algorithm consists phases forward stepwise procedure intended construct model large number basis functions backward elimination procedure removes sub optimal basis functions model 
forward procedure starts mars strategy di erence splines largest available scale allowed form tensor product basis functions kj 
clarify point consider st iteration case th iteration functions model form 
st iteration adds new basis function 
selected basis functions predictor variables index referring univariate spline basis functions variable largest available scale 
parameters defining set provide largest reduction residual sum squares 
proceeding lines algorithm reach point approximating ability splines largest scale exhausted 
order determine appropriate moment changing splines smaller scale estimates prediction error current model gcv score 
estimation carried step addition new basis function model 
gcv score ceases decrease algorithm change largest scale implies spline functions scale allowed participate construction new basis functions see 
algorithm proceeds way required number basis functions produced 
backward elimination procedure similar utilized original mars 
parallelization despite fact computational complexity algorithm linear number data points regression estimation large data sets encountered data mining approximating ability current scale exhausted basis function generate new tensor product decrease current scale change scales 
lengthy procedure 
important consider parallelization computational cost reduced 
parallel version algorithm implemented parallel virtual machine pvm programming environment 
pvm enables collection heterogeneous computer systems viewed single parallel virtual machine distributed memory 
processor memory multiprocessor system mentioned discussion understood components virtual multiprocessor system meant 
concerned forward part approach parallelize backward elimination procedure 
explained earlier structure new basis function added model forward stepwise procedure depends structure previously generated basis functions 
produce basis functions parallel 
quite possible come scalable algorithm constructing basis function 
order determine optimal values parameters perform squares fits appropriate values parameters 
parallelization squares fit result uniform distribution computational load processors system 
algorithm intended perform squares fit amounts computation number scalar products approach data partitioning appropriate situation 
assuming system comprised processors data partitioning involves allocation records data set processor system corresponding partial scalar products computed 
parallel master slave paradigm 
slave processors run identical code intended computation partial scalar products 
store portions data set 
addition code computation partial scalar products master processor runs program performs various house holding tasks 
test performance parallel applied large data set records attributes record provided motor vehicle insurance 
multiprocessor system sparc processors gbytes shared memory experiment 
ciency proved close ideal parallel algorithm equals number processors involved computations ranging 
note processors set aside run pvm daemon 
comparison took original mars hours carry task finished minutes 
evolutionary hot spots describe parallel system discovery symbolic descriptions key areas large data sets 
having identified data mining aims deliver novel useful knowledge large collections data characterize task identifying key areas large data set importance interesting data owners 
additive model thin plate splines multivariate adaptive regression splines techniques aid exploration process 
recognizing data mining projects usually ill defined goals expressed vaguely terms making interesting discoveries exploratory tools provide insights set directions data mining exercise 
real world data mining exercises actual goals refined clarified process proceeds 
address dynamic nature data mining exploring approach development goal part problem solving process 
parallel evolutionary algorithm measure interestingness descriptions groups data evolved influence user guiding system significant discoveries 
interesting hot spots characterize concept interestingness terms attempting identify key areas large multi dimensional data sets 
areas interesting exhibit surprising unexpected characteristics may lead actions taken modify business processes data owners 
introduce interestingness context hot spots methodology 
data set consists set real world entities set policy holders insurance set medical patients set taxpayers 
generally universal relation 
am attributes entities 
data set consists set entities 
entity tuple 
value attribute 
number attributes number tuples typically large greater 
hot spots methodology data driven approach generates set symbolic rules 
describe groups sets entities data true described rule 
synonymous 
refer element 
set nuggets 


generally smaller substantial millions 
rule consists conjunction conditions condition numeric attributes 
categorical attributes 
reduced dimensionality problem real world applications generally remains large manual consideration 
hot spots algorithm combines clustering tree building generate rule set 
hot spot particular interest domain user loyal customer groups regular high insurance 
crucial step hot spots algorithm evaluation set find particular interest 
define function eval purpose 
function critically domain dependent key ectively mining knowledge mine 
empirically ective approach reported williams building statistical summaries subsets 
key variables play important role business problem hand characterized filters developed pick nuggets profiles ordinary 
data mining exercise proceeds filters refined developed 
domain users provide ective evaluation discovered nuggets 
sets large manual approaches ective 
mechanism capturing meant interestingness employing algorithmically required 
interestingness simple hot spots approach described employs statistical summaries data items identify groups average values variables interesting standard deviation population average 
general concept interestingness di cult formalize varies considerably domains 
growing literature data mining addressing question 
early attempted identify objective measures interestingness confidence support measures association algorithms examples objective measures 
explored concept 
measure interestingness receive attention 
hot spots methodology provide starting point exploration interesting areas large data sets 
evolutionary approach builds framework allows domain users explore nuggets rules corresponding data subsets allow nuggets evolve guidance measure interestingness 
initial measure interestingness simple statistics group group interesting average value attribute group greater standard deviations average value attribute data set 
augmented tests size group generally don want large groups tend exhibit expected behavior non specific interest meta conditions limit number nuggets fewer 
assessing interestingness relying human resources carry actual investigations time consuming task 
proceed working closely domain user ideas interesting nuggets discovered refined complex 
requires constant refinement measure loops construction process explore search space 
evolutionary approach attempts tackle construction nuggets measure interestingness developed 
rule evolution system allows ectively search interesting groups guidance domain expert helping develop measure interestingness search data 
measure interestingness developed explicit function eval 
initial random measure interestingness sets process train 
measure fitness current nuggets evolve populations traditional crossover mutation operations genetic algorithms 
having evolved fit population nuggets small subset domain user evaluation express believe interesting nuggets 
population measures interestingness evolved user feedback 
architecture summarized fig 

analysis measures interestingness evolve evolve ruleset hot spots sample human ranking dataset attributes architecture indicative population sizes 
nature genetic algorithms crucial concurrency exploited 
measure interestingness evolving rule sets applied rule particular rule set generally computationally expensive requiring calculations data sets 
performs evaluations nuggets concurrently 
time may multiple populations nuggets independently evolved di erent measures fitness 
performed concurrently 
di erent strategies evolution multiple island approach increase propensity significant payo level concurrency increases 
empirical processor gb main memory sun enterprise system demonstrate feasible interactive data mining tool compared serial implementations requiring days computation results obtained 
provides data mining algorithm support process exploring interesting areas large data sets 
analysis large business data sets new methods developed 
due size data sets data mining applications new techniques computationally cient scalable 
typical data set analyze high dimensional variables 
analysis high dimensional data sets multivariate regression important technique determine functional relationships 
standard regression approaches high dimensional data face curse dimensionality problem 
approaches usually scalable 
developed multivariate regression methods additive models 
methods scalable analyze data sets millions billions records 
computationally cient compared standard techniques 
overcome curse dimensionality problem 
applied synthetic real large data sets arise insurance tax digital elevation model magnetic field areas 
theoretical investigation additive model investigation compared usual regression spline approach considerable reduced analyzing times large data sets 
progress develop parallel generalized cross validation form 
evolutionary algorithm provide directions data mining applications 
bibliography 
characterization data mining technologies processes information discovery www com dm tech htm 
bellman adaptive control processes guided tour princeton university press princeton 

hastie tibshirani generalized additive models vol 
monographs statistics applied probability chapman hall london 

friedman multivariate adaptive regression splines annals statistics vol 
pp 


mcintosh generalised additive models thesis australian national university 

roberts finite element thin plate splines surface fitting proc 
int conf 
computational techniques applications world scientific pp 


miller subset selection regression chapman hall london 

cox practical spline approximation topics numerical analysis springer verlag new york pp 


chan milne stone williams analysis motor vehicle claims data statistical data mining csiro technical report 

williams huang mining knowledge mine hot spots methodology mining large real world databases advanced topics artificial intelligence vol 
lecture notes computer science springer verlag berlin pp 


williams evolutionary hot spots data mining architecture exploring interesting discoveries proc 
int conf 
knowledge discovery data mining advances data mining lecture notes computer science springer verlag berlin 
