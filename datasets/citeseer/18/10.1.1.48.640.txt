unification glossing vasileios hatzivassiloglou department computer science columbia university new york ny vh cs columbia edu kevin knight usc information sciences institute admiralty way marina del rey ca knight isi edu approach syntax machine translation combines unification style interpretation statistical processing 
approach enables translate japanese newspaper article english quality far better word word translation 
novel ideas include feature structures encode word lattices unification compose manipulate lattices 
unification allows specify features delay target language synthesis source language information assembled 
statistical component enables search efficiently competing translations locate high english fluency 
background knight project goals scale knowledge machine translation techniques handle newspaper mt achieve higher quality output currently available develop techniques rapidly constructing mt systems 
built version months participated arpa evaluation mt quality white connell effort larger nmsu crl mt project 
approach framework fall back statistical methods knowledge gaps arise inevitably 
syntactically analyze japanese text map semantic representation generate english 
shows sample translation 
parsing bottom driven augmented context free grammar format roughly shieber grammar rules look np 
np syn infl ta form syn syn syn comp plus syn mod syn supported part advanced research projects agency order contract mda department defense 
input fi theta interlingua sem instance goal 
instance business mod instance new virgin phenomenon instance launch agent 
temporal locating instance month index output new plans establish february 
sample translation 
semantic representation contains conceptual tokens drawn term sensus ontology knight luk semantic analysis proceeds bottom walk parse tree style montague moore dowty moore semantics compositional parse tree node assigned meaning meanings children 
leaf node meanings retrieved semantic lexicon meaning composition rules handle internal nodes 
semantic rules lexical entries sensitive syntactic structure 
sem instance business np 
np syn form sem instance rc modified object sem head sem sem rel mod sem map subject role map object role map object role generation performed penman penman includes large systemic grammar english 
gaps generator knowledge filled statistical techniques knight hatzivassiloglou knight including model rank potential generator outputs 
english lexicon includes roots comparable size roots japanese syntactic analysis 
kbs drive full semantic throughput 
major missing pieces include large japanese semantic lexicon set ontological constraints 
attacking problems combination manual automatic techniques okumura hovy knight luk want test current lexicons rules analyzers mt system 
modified system include short cut path japanese english describe 
path skips semantic analysis knowledge generation uses syntactic analyses lexicons full system 
call short cut glossing features new component called glosser job transform japanese parse tree english easily obtainable resources 
glosser achieves throughput parser fails fully analyze input sentence produces fragmentary parse tree 
bottom glossing thinking glossing problem turning japanese parse trees english goals insights ffl quality 
glossing necessarily involves guessing obvious ambiguous word bei may glossed rice american 
semantic analysis improved guessing road improved quality 

potential translation guesses packed english word lattice sort speech recognition systems 

guesses ranked statistical language model promising ones identified search procedure 
ffl component re 
possible build glosser quickly re representations modules full system 

word lattices stored manipulated feature structures 

compositional semantic interpreter serve glosser provide new knowledge bases 

statistical model built ranking generator outputs knight hatzivassiloglou glossing 
section describes put mt system ideas 
concentrate components knowledge bases deferring linguistic statistical aspects sections 
word lattices model ambiguities sources japanese syntactic analysis lexical glossing english synthesis 
small sample lattice affirmation defendant accused defendant accused innocent affirmation innocent lattice encodes possible translations main pathways correspond different parses 
star symbol stands empty transition 
original bottom semantic analyzer transforms parse trees semantic feature structures 
produce word lattices encode lattices disjunctive feature structures gloss op op op op empty op affirmation op op op op empty op defendant accused op op op innocent op op defendant accused op op op innocent op op op op op empty op affirmation representation marks mutually disjoint components gloss features op op represent sequentially ordered portions gloss 
structure transformed automatically format suitable statistical processing 
part transformation bit english morphology simplify analyzer 
analyzer runs bottom walk parse tree unification implement montague style composition 
replace conventional semantic lexicon gloss lexicon easily obtainable online dictionary 
gloss firm replace semantic rules glossing rules np 
np gloss op gloss gloss op gloss op gloss tmp tmp rule says gloss japanese noun phrase np created relative clause combining noun phrase np glue english gloss child np relative pronoun english gloss rule propagates features tmp child np parent 
return features section 
built set complex rules match structures syntactic grammar 
new semantic analyzer composes glosses word lattices meanings call glosser 
compares glossing semantic interpretation 
parse tree node annotated analysis 
sentence level analyses appear top 
analyses fed subsequent modules generator case semantic analysis directly statistical model case glossing 
linguistic aspects shows semantic interpretation flexible unification combinator glossing 
fact glossing rules simply concatenate word lattices insert function words 
concatenation lets put direct object verb english example comes verb japanese 
japanese structures different english strategy breaks 
consider sentence john ga bill ni parsed ta past eat force bill ni np np pp pp ga john translation sentence english john forced bill eat 
difficulty assign word lattices intermediate nodes parse tree 
assign forced eat way squeeze word bill level 
solution unification pass features parse tree 
store information top level feature called tmp parallel gloss syn syntactic feature structures 
feature structure lowest node looks gloss eat tmp force past complex 
pp rule successfully features words level gloss op forced op bill op op eat efficiently turn bundles features english difficult general problem lying heart natural language generation 
glosser tackles simple instances problem involving features 
binary features require rules spell specify cases 
see decomposition feature spells independently 
cases exponential blowup required number glosser rules 
fragment rules dealing example 
tmp tmp xor syn entry form gloss gloss tmp force syn entry form ta tmp force gloss gloss tmp past syn entry form ta gloss op gloss gloss op past 
gloss gloss tmp tmp pp 
np syn entry form syn entry form gloss gloss 
pp xor syn entry form ga gloss op gloss gloss op gloss syn entry form ni tmp force xor tmp past gloss op forced gloss op force forces gloss op gloss gloss op gloss op gloss sem instance sem go agent subject map sem map subject agent goal verb want sem instance map sem subject person john instance name sem sem person john instance name map sem goal instance agent subject go sem instance want agent person john instance name sem want instance agent goal agent go instance meaning semantics john ga iki tai glossing john want wants go goes op op op op word lattice want wants go goes op op john john ga iki tai john want wants go goes semantic analysis versus glossing 
convert parse trees feature structures unification compositional techniques 
semantics computes conceptual representation glossing computes target language word lattice 
notation symbol borrowed grammar kaplan bresnan means feature sequence exist incoming child constituent xor sets disjunction feature constraints allowed satisfied 
statistical language modeling glosser module proposes number possible translations japanese word sequence words matched syntactic constituent bottom parser 
translation unit represents lexical island knowledge glosser piece text constraints available 
time various renditions translation unit combine leading possible translations sentence level 
order select combinations possibilities need objective function score hopefully ranking correct translation near top 
accomplish task approximate correctness fluency approximate fluency likelihood selecting combination words phrases occur target language 
approach offers advantages ffl measure likelihood sentence level take account interactions words phrases produced different parts japanese input 
example bei japanese may mean american rice sha may mean 
possibilities survive words glosser processes likelihood criterion select american correct translation 
addition ranking potential translations probability target language indirectly handles collocational constraints allows correct choice function words may appear source text articles japanese subject non compositional lexical constraints prepositions english afraid monday versus february 
ffl absence additional lexical constraints originating neighboring target language words phrases individual translations containing common widely words preferred translations contain rare obscure words 
way japanese word translated car tactic optimal disambiguating information available selects translation avoiding rare specialized alternatives 
remainder section discuss measure probability english sentence probabilities short sequences words grams bahl estimate basic probabilities grams handle problems sparse data smoothing estimates search space translation possibilities efficiently translation select best scoring translations 
sentence likelihood model discussed previous paragraph want associate english sentence likelihood measure pr 
number sequences large training text unlimited expect count occurrences corpus classic estimation technique maximum likelihood estimation 
adopt markov assumption probability seeing word depends short history words appearing just sentence 
history previous words stochastic process generates sequences english words approximated second order markov chain bigram trigram model respectively 
reasons numerical accuracy finite precision computations convert probabilities log likelihoods 
log likelihood sequence words wn ll log pr jw gamma bigrams ll log pr jw gamma gamma trigrams unfortunately likelihood model assign smaller smaller probabilities sequence longer 
need compare alternative translations different lengths alleviate problem adding heuristic corrective bonus increasing function sentence length 
experimenting functions function length word sequence gives satisfactory results added log likelihood measure 
equivalent adding exponential function length original probabilities 
estimating gram probabilities estimate conditional bigram trigram probabilities model processed large corpus carefully written english texts measured frequencies word sequences 
aim translation unrestricted japanese newspaper articles selected wall street journal wsj corpus representative available collection english texts output imitate 
processed years wsj corpus giving words training text containing approximately different word types 
large number different word types modeling task significantly complicated previous similar language models 
models usually designed speech recognition tasks vocabulary limited frequent english words 
vocabulary words delta different bigrams delta different trigrams 
handling large numbers grams unlimited text approach feasible practical limitations terms memory hardware speed 
available acl data collection initiative cd rom 
problematic terms storage space retrieval speed 
furthermore estimating probabilities difficult occur training text 
order reduce number grams need estimate probabilities implemented simple schema class smoothing 
developed finite state automata features word position capitalization types characters word separate words classes numbers monetary amounts proper names regular words 
treat words classes word pooling frequencies uniform maximum likelihood estimates words class irrespective particular word seen training corpus 
class smoothing reduces number words need estimate individual probabilities importantly reduces number bigrams factor number trigrams factor 
surviving grams observed training corpus estimate probability zero clearly incorrect compositionality english language 
proposed method addresses problem theoretically optimal general distributional assumptions gram follows marginal binomial distribution resulting turing estimator replaces observed frequency corrected frequency number grams occur times 
corrected frequencies subsequently provide estimates probabilities maximum likelihood formula 
general probability mass stolen observed grams proportionally observed times frequent ones redistributed unseen grams 
turing estimator suffers disadvantage assigns probability grams seen corpus matter grams seen number times corpus 
church gale proposed enhanced version estimator bigrams secondary predictor unigram word probabilities separate bigrams bins turing formula applied separately bin 
rationale approach pairs frequent infrequent words expected frequent infrequent departures norm notable bigrams separated bins likelihood component words differences lead different estimates bigram probabilities bigrams recall contains delta words consequently slightly higher number bigrams trigrams special sentence token taken account 
frequency corpus 
church gale provide empirical evidence indicates enhanced turing estimator outperforms simple estimators mle lesser extent complex estimators enhanced version deleted estimation method jelinek mercer implemented basic turing method single words allowing unseen words 
turing estimates probabilities words compute secondary predictor log pr pr enhanced turing estimator bigram ab 
extended enhanced method trigrams abc secondary predictor log pr ab pr combines estimated probabilities initial bigram final word 
smooth secondary predictor bins order magnitude trigrams bigrams respectively smooth counts ngrams bin dynamically self adjusting local smoother 
searching word lattice space previous subsections discussed sequence words assigned likelihood estimate appropriately modified length 
principle allow ranking various translation alternatives simply computing likelihood 
word lattices produced glosser compactly encode billions possible translations simple linear chain states states arcs leaving state represents paths corresponding potential translation 
consequently method needed efficiently search word lattice select small set highly translations 
adopted best algorithm purpose chow schwartz widely viterbi algorithm viterbi produces single best scoring path algorithm offers advantage producing number highest scoring paths lattice paths extensive expensive method 
offers controlled accuracy extent suboptimality arbitrarily decreased amount memory available search empirical studies nguyen shown performs equally complicated methods 
forward estimates viability partial path required example case stack decoder jelinek bahl algorithm 
perform topological sort states word lattice visit state predecessors processed 
process state keep list best scoring sequences words reaching state start state log pr pr pr predictor believe may outperform log pr ab pr correlated log pr abc 
computing alternative predictor line impractical large number possible trigrams 
lattice 
state extend word sequences predecessors current state recompute scores prune search space keeping prespecified number sequences specified width global search beam 
practice beam hypotheses node gives accurate results reasonable search speed 
sequences stored compactly pointers preceding states information specific arc taken step maintained fast priority queue avoid sorting 
allows simulate hmm order trace number final sentences beam width final state lattice reached 
complexity search algorithm slightly superlinear terms beam width number states gram length model average fan lattice number arcs leaving state 
results glosser currently machine translation system fall back component cases parsing semantic transfer failures 
participated september arpa evaluation machine translation systems see white connell discussion evaluation methodology employed promising results 
sample translations produced glossing module form japanese input followed correct translation translation glosser 
due space limitations showing output small example sentences typically operates longer sentences characteristic newspaper text 
ffl ae rf xik unusual ability english 
holds talent exceeded english language 
ae news got abroad 
information spread 
fl fl jl xi living creatures adaptable environmental change 
animal circumstances accommodation variation enable 
upsilon fl fi xi ffl sigma visitors japan admire mt fuji 
tourists coming japan decided say mt fuji 
ffl sigma thetak adverse violence 
contrary violence 
translations obtained bigram language model heavy computational storage demands delayed deployment precise trigram model 
expect higher quality output trigram model fully operational 
related discussion glosser described type transfer mt follows tradition syntax mt systems 
statistics allowed avoid traditional hand coding produce competitive mt system months 
statistical approaches mt include candide brown syntactic analysis source text yamron probabilistic parsing 
require syntax translate languages radically different word orders 
features syntax glossing semantics gives flexibility correct translation errors capture generalizations rapidly build complete mt system 
features analysis translations improve knowledge 
improvements come better statistical modeling 
directed finding improvements studying interaction knowledge bases statistics 
acknowledgments yolanda gil kenji yamada ijcai reviewers helpful comments draft 
bahl bahl jelinek mercer 
maximum likelihood approach continuous speech recognition 
ieee transactions pattern analysis machine intelligence pami 
brown brown della pietra della pietra mercer 
mathematics statistical machine translation parameter estimation 
computational linguistics june 
chow schwartz chow schwartz 
best algorithm efficient search procedure finding top sentence hypotheses 
proc 
darpa speech natural language workshop pages 
church gale church gale 
comparison enhanced turing deleted estimation methods estimating probabilities english bigrams 
computer speech language 
dowty dowty wall peters 
montague semantics 
reidel dordrecht 

population frequencies species estimation population parameters 
biometrika 
jelinek mercer jelinek mercer 
interpolated estimation markov source parameters sparse data 
pattern recognition practice 
north holland amsterdam 
jelinek jelinek bahl mercer 
design linguistic statistical decoder recognition continuous speech 
ieee transactions information theory 
kaplan bresnan kaplan bresnan 
lexical functional grammar formal system grammatical representation 
mental representation grammatical relations 
mit press cambridge ma 
knight knight 
automated documents 
proc 
aaai 
knight hatzivassiloglou knight hatzivassiloglou 
level paths generation 
proc 
acl 
knight luk knight luk 
building large scale knowledge base machine translation 
proc 
aaai 
knight knight haines hatzivassiloglou hovy iida luk okumura whitney yamada 
integrating knowledge bases statistics mt proc 
conference association machine translation americas 
knight knight haines hatzivassiloglou hovy iida luk whitney yamada 
filling knowledge gaps broad coverage mt system 
proc 
ijcai 
moore moore 
unification semantic interpretation 
proc 
acl 
nguyen nguyen schwartz zhao 
best dead 
proc 
arpa human language technology workshop 
advanced research projects agency 
nmsu crl nmsu crl cmu cmt usc isi 
mark iii machine translation system 
technical report cmu cmt carnegie mellon university 
jointly issued computing research laboratory new mexico state university center machine translation carnegie mellon university information sciences institute university southern california 
edited nirenburg 
okumura hovy okumura hovy 
ontology concept association bilingual dictionary 
proc 
arpa human language technology workshop 
penman penman 
penman documentation 
technical report usc information sciences institute 
shieber shieber 
unification approaches grammar 
university chicago 
csli lecture notes series 
viterbi viterbi 
error bounds convolution codes asymptotically optimal decoding algorithm 
ieee transactions information theory 
white connell white connell 
evaluation arpa machine translation program methodology 
proc 
arpa human language technology workshop 
yamron yamron cant ito 
automatic component machine aided translation system 
proc 
arpa workshop human language technology 
