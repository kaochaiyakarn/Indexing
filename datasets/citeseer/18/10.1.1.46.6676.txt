real world data dirty data cleansing merge purge problem mauricio hern andez salvatore stolfo mauricio cs columbia edu sal cs columbia edu department computer science columbia university new york ny problem merging multiple databases information common entities frequently encountered kdd decision support applications large commercial government organizations 
problem study called merge purge problem difficult solve scale accuracy 
large repositories data typically numerous duplicate information entries entities difficult cull intelligent equational theory identifies equivalent items complex domain dependent matching process 
developed system accomplishing data cleansing task demonstrate cleansing lists names potential customers direct marketing type application 
results statistically generated data shown accurate effective processing data multiple times different keys sorting successive pass 
combing results individual passes transitive closure independent results produces far accurate results lower cost 
system provides rule programming module easy program quite finding duplicates especially environment massive amounts data 
details improvements system reports successful implementation real world database conclusively validates results previously achieved statistically generated data 
keywords data cleaning data cleansing duplicate elimination semantic integration supported part new york state science technology foundation center advanced technology telecommunications polytechnic university nsf iri 
author university illinois 
columbia university supported cooperative research program fellowship 
merging large databases acquired different sources heterogeneous representations information increasingly important difficult problem organizations 
instances problem appearing literature called record linkage semantic integration problem instance identification problem data cleansing problem regarded crucial step kdd dm process 
business organizations call problem merge purge problem 
consider data cleansing large databases information need processed quickly efficiently accurately possible 
instance month typical business cycle certain direct marketing operations 
means sources data need identified acquired conditioned correlated merged small portion month order prepare response analyses 
uncommon large businesses acquire scores databases month total size hundreds millions records need analyzed days 
general setting data mining applications depend conditioned sample data correlated multiple sources information accurate database merging operations highly desirable 
single data set problem crucial accurate statistical analyses 
accurate identification duplicated information frequency distributions various aggregations produce false misleading statistics leading untrustworthy new knowledge 
large organizations problem years dealing lists names addresses identifying information 
credit card companies example need assess financial risk potential new customers may purposely hide true identities history manufacture new ones 
justice department law enforcement agencies seek discover crucial links complex webs financial transactions uncover sophisticated money laundering activities 
errors due data entry mistakes faulty sensor readings malicious activities provide scores erroneous datasets propagate errors successive generation data 
problem merging databases tackled straightforward fashion simple sort concatenated data sets followed duplicate elimination phase sorted list 
databases involved heterogeneous meaning share schema real world entities represented differently datasets problem merging difficult 
issue databases different schema addressed extensively literature known schema integration problem 
primarily interested second problem heterogeneous representations data implication merging joining multiple datasets 
fundamental problem merge purge data supplied various sources typically include identifiers string data different different datasets simply erroneous due variety reasons including typographical transcription errors purposeful fraudulent activity aliases case names 
equality values domain common join attribute specified simple arithmetic predicate set equational axioms define equivalence equational theory 
determining records databases provide information entity highly complex 
rule knowledge base implement equational theory 
problem identifying similar instances real world entity means inexact match studied fuzzy database community 
concentrated problem executing query fuzzy relational database 
answer set tuples satisfying non fuzzy relational database tuples satisfy threshold value 
fuzzy relational databases explicitly store possibility distributions value tuple possibility relations determine strongly records belong fuzzy set defined query 
problem study closely related problem studied fuzzy database community 
fuzzy querying systems concerned accurate efficient fuzzy retrieval tuples query concerned pre processing entire data set ready querying 
process study line involves clustering tuples equivalence classes 
clustering guided equational theory include fuzzy matching techniques 
dealing large databases seek reduce complexity problem partitioning database partitions clusters way potentially matching records assigned cluster 
term cluster line common terminology statistical pattern recognition 
discuss solutions merge purge sorting entire data set bring matching records close bounded neighborhood linear list optimization basic technique seeks eliminate records sorting exact duplicate keys 
treated case clustering sorting replaced single scan process 
clustering resembles hierarchical clustering strategy proposed efficiently perform queries large fuzzy relational databases 
demonstrate may expect basic approaches guarantee high accuracy 
accuracy means actual duplicates appearing data matched merged correctly 
organized follows 
section detail system implemented performs generic merge purge process includes declarative rule language specifying equational theory making easier experiment modify criteria equivalence 
important desideratum commercial organizations strict time constraints precious little time experiment alternative matching criteria 
section demonstrate single pass data particular scheme sorting key performs computing transitive closure independent runs different sorting key ordering data 
moral simply distinct cheap passes data produce accurate results expensive pass data 
result verified independently monge elkan studied problem domain independent matching algorithm equational theory 
section provide detailed treatment real world data set provided child welfare department state washington establish validity results 
statistically generated databases allowed devise controlled studies optimal accuracy results known priori 
real world datasets obviously know best attainable results high precision time consuming expensive human inspection validation process 
cases datasets huge may feasible 
results reported due human inspection small substantial sample data relative entire data set 
results real world data validate previous predictions quite accurate 
may view formal results comparative evaluation browsing site www cs columbia edu sal 
section initial results incremental merge purge algorithm 
basic merge purge procedure section assumes single data set 
new data set arrives concatenated previously processed data set basic merge purge procedure executed entire data set 
incremental algorithm removes restriction information gathered previous merge purge executions 
strategies determining information gather execution incremental algorithm proposed 
initial experimental results showing incremental algorithm reduces time needed execute merge purge procedure compared basic algorithm 
basic data cleansing solutions previous introduced basic sorted neighborhood method solving merge purge variant duplicate elimination method 
describe detail basic approach followed description incremental variant merges new smaller increment data existing previously dataset 
basic sorted neighborhood method collection databases concatenate sequential list records conditioning records apply sorted neighborhood method 
sorted neighborhood method solving merge purge problem summarized phases 
create keys compute key record list extracting relevant fields portions fields 
choice key depends error model may current window records records window window scan data cleansing viewed knowledge intensive domain specific effectiveness method highly depends properly chosen key intent common erroneous data closely matching keys 
discuss effect choice key section 
sort data sort records data list key step 
merge move fixed size window sequential list records limiting comparisons matching records records window 
size window records new record entering window compared previous gamma records find matching records 
record window slides window see 
procedure executed serially main memory process create keys phase operation sorting phase log merging phase wn number records database 
total time complexity method log dlog ne wn 
constants equations differ greatly 
relatively expensive extract relevant key values record create key phase 
sorting requires machine instructions compare keys 
merge phase requires application potentially large number rules compare records potential largest constant factor 
notice parameter window scanning procedure 
legitimate values may range consecutive elements compared element compared 
case pertains full quadratic time process maximal potential accuracy defined equational theory percentage duplicates correctly merging process 
case may viewed small constant relative pertains optimal time performance time minimal accuracy 
fundamental question optimal settings maximize accuracy minimizing computational cost 
note large databases dominant cost disk number passes data set 
case passes needed pass conditioning data preparing keys second pass high speed sort example final pass window processing application rule program record entering sliding window 
depending complexity rule program window size pass may dominant cost 
introduced means speeding phase processing parallel windows sorted list 
note interest sorts optimizations detailed may course fruitfully applied 
concerned alternative process architectures lead higher accuracies computed results reducing time complexity 
consider alternative metrics purposes merge purge include accurately data fixed dollar time constraint specific cost time metrics proposed 
selection keys effectiveness sorted neighborhood method highly depends key selected sort records 
key defined sequence subset attributes substrings attributes chosen record 
example consider records displayed table 
particular application suppose key designer sorting phase determined typical data set keys extracted data provide sufficient discriminating power identifying address id key sal stolfo street sal stolfo street sal street sal stiles forest street table example records keys candidates matching 
key consists concatenation ordered fields attributes data consonants name concatenated letters name field followed address number field consonants street name 
followed digits social security field 
choices key designer determined names typically misspelled due mistakes sounds vowels names typically common prone misunderstood recorded incorrectly 
keys sorting entire dataset intention equivalent matching data appear close final sorted list 
notice second records exact duplicates third person misspelled name 
expect phonetically mistake caught reasonable equational theory 
fourth record having exact key prior records appears person 
equational theory comparison records merge phase determine equivalence complex inferential process considers information compared records keys sorting 
example suppose person names spelled nearly identically exact address 
infer person 
hand suppose records exactly social security numbers names addresses completely different 
assume records represent person changed name moved records represent different persons social security number field incorrect 
information may assume 
information records better inferences 
example michael smith michele smith address names reasonably close 
gender age information available field data infer michael michele married siblings 
need specify inferences equational theory dictates logic domain equivalence simply value string equivalence 
users general purpose data cleansing facility benefit higher level formalisms languages permitting ease experimentation modification 
reasons natural approach specifying equational theory making practical declarative rule language 
rule languages effectively wide range applications requiring inference large data sets 
research conducted provide efficient means compilation evaluation technology exploited purposes data cleansing efficiently 
example simplified rule english exemplifies axiom equational theory relevant idealized employee database records 
name equals name names differ slightly address equals address equivalent 
implementation differ slightly specified english computation distance function applied name fields records comparison results threshold capture obvious typographical errors may occur data 
selection distance function proper threshold knowledge intensive activity demands experimental evaluation 
improperly chosen threshold lead increase number falsely matched records decrease number matching records merged 
number alternative distance functions typographical mistakes implemented tested experiments reported including distances edit distance phonetic distance typewriter distance 
results displayed section edit distance computation outcome program vary different distance functions particular databases study 
notice rules necessarily need compare values attribute domain 
instance detect transposition person name write rule compares name record name second record name record name second record see appendix example rule 
modern object relational databases allow users add complex data types functions manipulate values domain data type database engine 
functions compare complex data types sets images sound rules perform matching complex tuples 
purpose experimental study wrote ops rule program consisting rules particular domain employee records tested repeatedly relatively small databases records 
satisfied performance rules distance functions thresholds recoded rules directly obtain speedup ops implementation 
appendix shows ops version equational theory implemented 
rules encoding knowledge equational theory shown appendix 
inference process encoded rules divided stages 
stage records window compared see similar fields social security field name field street address field 
second stage information gathered stage combined see merge pairs records 
example pair records similar social security numbers similar names rule similar ssn names declares merged 
pair records merged information gathered stage rule program takes closer look fields city name state zipcode see merge done 
third stage precise edit distance functions ssn name initial address lisa boardman wars st lisa brown ward st ramon ward st raymond ward st diana church av 
diana brick church av 
th st apt 
john th st ap 
florida av 
kegan florida st table example matching records detected equational theory rule base 
fields attempt merging pair records 
table demonstrates number actual records rule program correctly deems equivalent 
appendix shows version equational theory 
appendix shows subroutine rule program main code rule implementation comments code show rule ops version implemented 
important note essence approach proposed permits wide range equational theories various data types 
chose string data study names addresses pedagogical reasons gets faulty junk mail 
equally demonstrate concepts alternative databases different typed objects correspondingly different rule sets 
table displays records errors may commonly mailing lists example 
poor implementations merge purge task commercial organizations typically lead pieces mail mailed obviously greater expense household nearly experienced 
records identified rule base equivalent 
process creating equational theory similar process creating knowledge base expert system 
complex problems expert needed describe matching process 
knowledge engineer encode expert knowledge rules 
rules tested results discussed expert 
sessions expert knowledge engineer needed rule set completed 
computing transitive closure results independent runs general single key sufficient catch matching records 
attributes fields appear key higher discriminating power appearing 
error record occurs particular field portion field important part key may little chance record close matching record sorting 
instance employee records database social security number social security number numbers transposed social security number principal field key records fall window records transposed social security numbers far apart sorted list may merged 
show section number matching records missed run sorted neighborhood method large neighborhood grows large 
increase number similar records merged options explored 
simply widening scanning window size increasing clearly increases computational complexity discussed section increase dramatically number similar records merged test cases ran course window spans entire database presumed infeasible strict time cost constraints 
alternative strategy implemented execute independent runs sorted neighborhood method time different key relatively small window 
call strategy multi pass approach 
instance run address principal part key run name employee principal part key 
independent run produce set pairs records merged 
apply transitive closure pairs records 
results union pairs discovered independent runs duplicates plus pairs inferred transitivity equality 
reason approach works test cases explored nature errors data 
transposing digits social security number leads non mergeable records noted 
records variability error appearing field records may large 
social security numbers records grossly error name fields may 
sorting name fields primary key bring records closer lessening negative effects gross error social security field 
notice transitive closure step limited multi pass approach 
improve accuracy single pass computing transitive closure results 
records similar time records similar transitive closure step mark similar relation detected equational theory 
records records marked similar equational theory 
true records transitive closure step need records detected similar 
transitive closure single pass run sorted neighborhood method allow reduce size scanning window detect comparable number similar pairs find final closure phase larger single run results reported section include final closure phase 
utility approach determined nature occurrences errors appearing data 
choice keys sorting order extraction relevant information key field knowledge intensive activity explored carefully evaluated prior running data cleansing process 
section show multi pass approach drastically improve accuracy results run sorted neighborhood method varying large windows 
particular interest observation small search window needed multi pass approach obtain high accuracy individual run single key sorting produced comparable accuracy results large window window sizes approaching size full database 
results consistently variety generated databases variable errors introduced fields systematic fashion 
experimental results generating databases databases test methods generated automatically database generator allows perform controlled studies establish accuracy solution method 
database generator provides user large number parameters may set including size database percentage duplicate records database amount error introduced duplicated records attribute fields 
accuracy measured percentage number duplicates correctly process 
false positives measured percentage records claimed equivalent actual duplicates 
generated database viewed concatenation multiple databases 
merging records resultant single database object study experiments 
record generated consists fields empty social security number name initial name address apartment city state zip code 
names chosen randomly list real names cities states zip codes come publicly available lists data generated intended model processed real world datasets 
errors introduced duplicate records range small typographical mistakes complete change names addresses 
setting parameters typographical errors known frequencies studies spelling correction algorithms 
study generator selected generated records duplication errors error spelling words see ftp ftp dk pub ftp cdrom com cd pub freebsd freebsd current src share misc names cities controlled published statistics common real world datasets 
performance measurement accuracy percentage duplicates captured standard error model plotted varying sized windows may better understand relationship tradeoffs computational complexity accuracy 
believe results substantially different different databases sorts errors duplicated records 
help better establish conjecture widely varying error models afforded database generator 
statistically generated databases may bear direct relationship real data 
believe experiments realistic 
section provides substantial evidence case 
results accuracy purpose experiment determine baseline accuracy method 
ran independent runs sorted neighborhood method database different key sorting phase independent run 
run name principal field key name attribute key 
second run name principal field run street address principal field 
selection attribute ordering keys purely arbitrary 
social security number say street address 
assume fields noisy control data generator matter field ordering select purposes study 
shows effect varying window size records database records additional duplicate records varying errors 
record may duplicated 
notice independent run duplicated pairs 
notice increasing window size help consideration time complexity procedure goes window size increases obviously fruitless point large window 
line marked multi pass keys shows results window size records sorted neighborhood method records duplicates key name key name key st addr 
multi pass keys percent correctly detected duplicated pairs false positives window size records sorted neighborhood method records duplicates key name key name key st addr 
multi pass keys percent incorrectly detected duplicated pairs accuracy results records database program computes transitive closure pairs independent runs 
percent duplicates goes 
manual inspection records equivalent revealed pairs hard human identify information 
equational theory completely trustworthy 
decide records similar equivalent may represent real world entity incorrectly paired records called false positives 
shows percent records incorrectly marked duplicates function window size 
percent false positives insignificant independent run grows slowly window size increases 
percent false positives transitive closure small grows faster individual run 
suggests transitive closure may accurate window size constituent pass large 
number independent runs needed obtain results computation transitive closure depends corrupt data keys selected 
corrupted data runs needed capture matching records 
transitive closure executed pairs tuple id bits fast total number records sorted neighborhood method duplicates duplicates duplicates time performance sorted neighborhood methods different size databases 
solutions compute transitive closure exist 
observing real world scenarios size data set closure computed order magnitude smaller corresponding database records contribute large cost 
note pay heavy price due number sorts clusterings original large data set 
parallel implementation alternatives reduce cost 
scaling demonstrate sorted neighborhood method scales size database increases 
due limitations available disk space grow databases records 
ran independent runs method different key computed transitive closure results 
databases table 
started duplicate databases created duplicates records total twelve distinct databases 
results shown 
relatively large size databases time increase linearly size databases increase independent duplication factor 
original number total records total size mbytes records table database sizes analysis natural question pose multi pass approach superior single pass case 
answer question lies complexity approaches fixed accuracy rate consider percentage correctly matches 
consider question context main memory sequential process 
reason shall see clustering provides opportunity reduce problem sorting entire disk resident database sequence smaller main memory analysis tasks 
serial time complexity multi pass approach passes time create keys time sort times time window scan times window size plus time compute transitive closure 
experiments creation keys integrated sorting phase 
treat phases analysis 
simplifying assumption data memory resident bound multipass sort rn log number passes time transitive closure 
constants depict costs comparison related ffc sort ff 
analyzing experimental program window scanning phase contributes constant ff times large comparisons performed sorting 
replace constants term single constant complexity closure directly related accuracy rate pass depends duplication database 
assume time compute transitive closure database orders magnitude smaller input database time scan input database contributes factor closure 
multipass crn log window size complexity single pass sorted neighborhood method similarly cn log window size fixed accuracy rate question value single pass sorted neighborhood method multi pass approach perform better time cn log crn log gamma ff log rw gamma validate model generated small database records original records selected duplications maximum duplicates selected record 
total size database bytes approximately mbyte 
read database stayed core phases 
ran independent single pass runs different keys multi pass run results single pass runs 
parameters experiment records 
particular case ff theta gamma 
time specified seconds 
multi pass approach dominates single sort approach datasets 
shows time required run independent run processor total time required multi pass approach shows accuracy independent run accuracy multi pass approach please note logarithm scale 
shows multi pass approach needed window size key name key name key street addr multi pass keys time single pass runs multi pass run duplicates window size key name key name key street addr multi pass keys ideal vs real accuracy run time accuracy small database produce accuracy rate 
looking times single pass run total time close slightly higher estimated model 
accuracy single pass runs accuracy level multi pass approach 
single pass run reaches accuracy point shown execution time seconds minutes 
consider issue process bound main memory process 
number disk blocks input data set number memory pages available 
sorted neighborhood method execution access log gamma disk blocks plus disk blocks read window scanning phase 
time sorted neighborhood method expressed snm sort log gamma sort represents cpu cost sorting data block represents cpu cost applying window scan method data block 
comes fact counting read write operations 
sorting divide data buckets hashing records multi dimensional partitioning strategy 
call modification clustering method 
assuming page bucket plus page processing input block need pass entire data partition records buckets blocks read 
writing records buckets requires approximately block writes 
assuming partition algorithm perfect bucket blocks 
sort log gamma block accesses apply window scanning phase bucket independently approximately block accesses 
total clustering method requires approximately log gamma block accesses 
time pass clustering method expressed cluster cluster sort log gamma cluster cpu cost partitioning block data 
cost multi pass approach multiple cost method chose pass plus time needed compute transitive closure step 
instance clustering method passes expect time cluster shows time comparison clustering method sorted neighborhood method 
results gathered generated data set records block size bytes blocks 
notice cases clustering method better sorted neighborhood method 
difference time large 
mainly due fact equational theory involved large number comparisons making lot larger sort cluster time savings initially partitioning data savings small compared time cost 
describe parallel variants basic techniques including clustering show modest amount cheap parallel hardware speed multi pass approach level comparable time single pass approach high accuracy small windows ultimately wins 
time window size records average single pass time naive snm average single pass time clustering snm total multi pass time naive snm total multi pass time clustering vs basic sorted neighborhood method results real world data results achieved wide variety statistically controlled generated data indicate multi pass approach quite may regard definitive validation efficacy techniques 
state washington department social health services maintains large databases transactions years state residents 
march office children administrative research ocar department social health services posted request kdd nuggets asking assistance analyzing databases 
answered request section details results 
ocar analyzes database payments state families businesses provide services children 
ocar goal answer questions children foster care long children stay foster care 
different homes children typically stay 
accurately answer questions computer records payments services identified child 
obviously matching records appropriate individual client frequency distributions services grossly error 
unique identifier individual child exists generated assigned algorithm compares multiple service records database 
fields records help identify child include name birth date case number social security number unreliable containing misspellings typographical errors incomplete information 
unique situation real world databases 
need develop computer processes accurately identified records child spurred ocar seek assistance 
database description ocar data stored relation contains payments service providers 
currently approximately total records relation number grows approximately month 
relation attributes relevant carrying information infer identify individual entities name name birthday social security number case number service id service dates dates gender race provider id amount payment date payment worker id record bytes long 
typical problems ocar data follows 
names frequently misspelled 
nicknames similar sounding names real name 
parent guardian name child name 

social security numbers missing clearly wrong records social security number 
likewise parent guardian information child proper information 

case number uniquely identify family changes child family moves part state referred service second time couple years referral 

records assigned person name entered record child name service provider 
names anonymous male anonymous female 
call type records ghost records 
number records cluster number records child computed ocar graph drawn cluster size 
actuality continues 
private nature data recorded database produce sample records illustrate mentioned cases 
database administrator responsible large corporate agency databases immediately see parallels data 
real world data dirty 
ocar provided sample database conduct study 
sample contains data service office records mbytes 
provided current individual identification number record sample number uniquely identify child database analysis 
ocar assigned identifiers serve basis comparing accuracy varying window sizes 
shows distribution number records individual detected ocar 
individuals database represented average records database approximately individuals represented record database 
note individuals may represented records shown individuals records records 
task apply data cleansing techniques compute new individual identification number record compare accuracy attained ocar provided identification number 
individual clusters window size number individuals detected ocar result st pass nd pass rd pass combined number individuals detected data cleansing ocar data ocar individual identification numbers course perfect 
set identifiers computed single scan clustering technique hashing records letters name letters name birth month year case number hashing keys 
strategy identified individuals sample database 
task create equational theory consultation ocar expert resultant rule base consists rules 
applied equational theory data basic sorted neighborhood method multi pass method rigorous comparative evaluation 
keys independent run 
name name social security number case number 

name name social security number case number 

case number name name social security number 
timothy clark computer information consultant ocar provided necessary expertise define rule base 
ocar analysis fr fr fr fr snm analysis fr fr fr fr separation fr sorted neighborhood method counted possible 
union fr fr counted possible false positive 
example definition possible misses possible false positives displays number individuals detected independent pass basic sorted neighborhood method number individuals closure phase function window size 
constant individuals detected ocar plotted straight line means comparison 
statistically generated data number individuals detected initially goes window size increases stabilizes remains constant 
notice large improvement performance combining results passes transitive closure phase results demonstrated controlled studies validated set real world data 
window size multi pass process detected individuals sample 
question answer individuals closer actual number individuals data ocar individuals 
answer question looked different group individuals detected ocar sample results 
call possible misses groups individuals data cleansing program considered different ocar considered similar 
call possible false positives groups individuals ocar consider similar grouped system 
shows example false positive passes experiments describe passes third key produced similar results 
second key pass marginally improved results 
significant observation may occur real world data indicating multi pass approach may simply pass approach significantly reducing complexity process achieving accurate results 
window size misses st pass nd pass rd pass combined misses respect ocar results false positives window size false positives st pass nd pass rd pass combined false positives respect ocar results accuracy results sorted neighborhood method ocar data definition 
depicts number possible misses possible false positives comparing results ocar 
previous experiments total number misses goes window size goes drops sharply transitive closure phase 
behavior false positives expected contrary misses number grows window size goes transitive closure phase 
multi pass sorted neighborhood method improve ocar results conditions met ffl number possible misses data cleansing program correctly merge larger number real misses ocar correctly merged program 
ffl number possible false positives records correctly merged multi pass sorted neighborhood method larger real false positives cases approach incorrectly merged records ocar 
study results met conditions ocar group manually inspected possible misses possible false positives case window size 
results manual inspection follows ffl possible misses multi pass sorted neighborhood method failed detect individuals ocar detected 
possible misses correctly separated approach real misses 
ocar results wrong 
incorrectly separated approach real misses 
special cases involving ghost records records payments outside agencies 
agreed ocar exclude cases consideration 
ffl possible false positives instances multi pass sorted neighborhood method joining records individuals ocar 
cases manually inspected results incorrectly merged approach 
correctly merged approach 
way summary possible misses real misses correctly classified records estimated possible false positives real false positives 
results lead ocar confident multi pass sorted neighborhood method improve individual detection procedure 
incremental merge purge versions sorted neighborhood method discussed section started procedure concatenating input lists tuples 
concatenation step unavoidable presumably acceptable time set databases received processing 
data merge purge process stored concatenation processed data arrived data re applying merge purge process best strategy follow 
particular situations new increments data available short periods time concatenating data merging purging prove prohibitively expensive time space required 
section describe incremental version sorted neighborhood procedure provide initial time accuracy results statistically generated datasets 
summarizes incremental merge purge algorithm 
algorithm specifies loop repeated increment information received system 
increment concatenated relation prime representatives pre computed previous run incremental merge purge algorithm multi pass sorted neighborhood method applied resulting relation 
prime representatives set records extracted cluster records represent information cluster 
pattern recognition community think prime representatives analogous cluster centroids generally represent clusters information base element equivalence class 
initially previous set prime representatives exists increment just input relation 
concatenation step effect 
execution merge purge procedure record input relation separated clusters similar records 
time algorithm records go new clusters 
starting second time algorithm executed records added previously existing clusters new clusters 
particular importance success incremental procedure terms accuracy results correct selection prime representatives formed cluster 
phases merge purge procedure selection knowledge intensive operation domain application determine set prime representatives 
describing strategies selecting representatives note description step algorithm implies clusters best prime representative representative 
possible practical example strategy true consider ocar data described chapter 
clusters containing records dated years old receive new record 
clusters removed consideration selecting prime representative 
case prime representatives cluster necessary possible strategies selection definitions initial relation 
delta th increment relation 
relation prime representatives clusters identified merge purge procedure 
initially delta incremental algorithm delta 
concatenate delta 

apply merge purge procedure result cluster assignment record 
separate record clusters assigned previous step 

cluster records necessary select records prime representatives cluster 
call relation formed selected prime representatives 
incremental merge purge algorithm ffl random sample select sample records random cluster 
ffl latest data physically ordered time entry relation 
cases elements entered database assumed better represent cluster ocar data example 
strategy latest elements selected prime representatives 
ffl generalization generate prime representatives generalizing data collected positive examples records concept represented cluster 
techniques generalizing concepts known machine learning 
ffl syntactic choose largest complete record 
ffl utility choose record matched frequently 
section initial results comparing time accuracy performance incremental merge purge basic merge purge algorithm 
selected prime representative strategy experiments implementation simplicity 
experiments underway test compare strategies 
results described report 
important assumptions describing incremental merge purge algorithm 
assumed data previously select cluster deleted negative deltas 
second assumed changes rule set occur increment data processed 
discuss briefly implications assumptions 
removing records clustered split clusters 
removed record responsible merging clusters original clusters merged separated 
new prime representatives computed increment data arrives processing 
procedure follow case deletions 
delay deletions step incremental algorithm 

perform deletions 
remember cluster ids clusters affected 

re compute closure clusters affected splitting existing clusters necessary 
incremental algorithm resumes step recomputing new prime representative clusters including new formed deletions 
changes data little difficult 
changes treated deletion followed insertion 
case particular human making change new record belong cluster removed 
user set parameter determine circumstances changes data treated deletion followed insertion evaluated increment evaluation just direct change existing cluster 
changes rule base defining equational theory difficult correctly incorporate incremental algorithm 
minor changes rule base example small changes thresholds defining equality fields deletion rules rarely fired expected little impact contents formed clusters 
depending data major changes rule base large number current clusters erroneous 
unfortunately solution problem run merge purge procedure available data 
hand depending application slight number inconsistencies acceptable avoiding need run entire procedure 
decision highly application dependent requires human intervention resolve 
initial experimental results incremental algorithm conducted number experiments test incremental merge purge algorithm 
experiments interested studying time performance different stages algorithm effect accuracy results 
started ocar sample described section records divided parts delta delta delta delta delta records respectively 
incremental merge purge algorithm implement unix shell script concatenated fed proper parts basic multipass sorted neighborhood method 
awk script combined program implement prime representative selection part algorithm 
strategy tested latest strategy latest record cluster creating clusters select records incremental merge purge total time clustering incremental incremental accumulative time normal multi pass merge purge increment records delta delta delta delta delta time incremental vs normal multi pass merge purge times prime representative 
shows time results part incremental merge purge procedure contrast normal non incremental merge purge 
results obtained pass basic multi pass approach keys described section window size records sun workstation running solaris 
results divided deltas 
bars representing actual time particular measured phase division 
bar corresponds time taken collect prime representatives previous run multi pass approach note bar delta 
second bar represents time executing multi pass approach concatenation current delta prime representatives records 
total time incremental merge purge process addition times represented third bar 
fourth bar shows accumulated total time incremental merge purge procedure 
bar shows time normal merge purge procedure running databases composed concatenation deltas including current 
notice case delta total time incremental merge purge process considerably time normal process 
cases tested number individual clusters delta incremental normal clusters formed number possible misses false positives delta misses incremental misses normal false positives incremental false positives normal possible misses false positives accuracy incremental procedure experiment cumulative time incremental merge purge process larger total time normal merge purge 
due large time cost clustering selecting prime representative records cluster 
current implementation entire dataset concatenation deltas including current sorted find clusters records cluster considered selecting prime representatives 
clearly optimal solution clustering records selection prime representatives 
better implementation incrementally select prime representatives previously computed 
current implementation gives worst case execution time phase 
optimization decrease total incremental merge purge time 
compares accuracy results incremental merge purge procedure normal procedure 
total number individuals clusters detected number possible misses number possible false positives went incremental merge purge procedure 
increase measures negligible arguably acceptable remarkable reduction time provided incremental procedure 
sorted neighborhood method expensive due sorting phase need search large windows high accuracy 
alternative methods data clustering modestly improves process time reported 
achieves high accuracy inspecting large neighborhoods records 
particular interest performing data cleansing process multiple times small windows followed computation transitive closure dominates accuracy method 
multiple passes small windows increases number successful matches small windows favor decreases false positives leading high accuracy merge phase 
alternative view single pass approach far slower achieve comparable accuracy multi pass approach 
results demonstrate statistically generated databases provide means quantifying accuracy alternative methods 
real world data comparable means rigorously evaluating results 
application program real world data provided state washington child welfare department validated claims improved accuracy multi pass method eye significant sample data 
controlled empirical studies shown indicates improved accuracy exhibited real world data sorts errors complexity matching described 
results reported form basis module available informix software 
technology broadly applicable real world data dirty 
acknowledgments grateful timothy clark computer information consultant ocar help provided obtaining results section 
dr diana english ocar chair allowing database 
acm 
sigmod record december 
agrawal jagadish 
multiprocessor transitive closure algorithms 
proc 
int symp 
databases parallel distributed systems pages december 
batini lenzerini navathe 
comparative analysis methodologies database schema integration 
acm computing surverys december 
bitton dewitt 
duplicate record elimination large data files 
acm transactions database systems june 

fuzzy representation data relational databases 
fuzzy sets systems 
generally regarded originated fuzzy databases 
buckley 
hierarchical clustering strategy large fuzzy databases 
proceedings ieee international conference systems man cybernetics pages 
church gale 
probability scoring spelling correction 
statistics computing 
clark 
analyzing foster foster home payments database 
kdd nuggets info gte com kdd nuggets piatetsky shapiro ed 
dietterich michalski 
comparative review selected methods learning examples 
michalski carbonell mitchell editors machine learning volume pages 
morgan kaufmann publishers 
dubes jain 
clustering techniques user 
pattern recognition 
fayyad piatetsky shapiro smyth 
data mining knowledge discovery databases 
ai magazine fall 
fellegi sunter 
theory record linkage 
american statistical association journal pages december 
forgy 
ops user manual 
technical report cmu cs carnegie mellon university july 
george srikanth 
fuzzy database systems challenges opportunities new era 
international journal intelligent systems 
ghandeharizadeh 
physical database design multiprocessor database systems 
phd thesis department computer science university wisconsin madison 
hern andez stolfo 
merge purge problem large databases 
proceedings acm sigmod conference may 
kukich 
techniques automatically correcting words text 
acm computing surveys 
lebowitz 
path utility similarity learning 
proceedings th national conference artificial intelligence pages 
monge elkan 
efficient domain independent algorithm detecting approximate duplicate database records 
proceedings sigmod workshop research issues dmkd pages 
nyberg barclay gray lomet 
risc machine sort 
proceedings acm sigmod conference pages 
pollock zamora 
automatic spelling correction scientific scholarly text 
acm computing surveys 
senator goldberg wong 
artificial intelligence system identifying potential money laundering reports large cash transactions 
proceedings th conference innovative applications ai august 
wang madnick 
inter database instance identification problem integrating autonomous systems 
proceedings sixth international conference data engineering february 
ops version equational theory rule program number tuples tuple window size compare tuples inside window 
match call merge tuples 
void rule program int int start int register int register person person boolean similar ssns similar names similar addrs boolean similar city similar state similar zip boolean similar close aptm close stnum close tuples consideration start person points th tuple person tuples tuples inside window tuples th tuple 
gamma gamma gamma person points th tuple person tuples compare person person rule find similar ssns similar ssns ssn person ssn person ssn rule compare names similar names compare names person name person name person fname person person lname person fname person person lname person fname init person fname init rule ssn name similar ssns similar names merge tuples person person continue rule compare addresses similar addrs compare addresses person person compare fields address similar city city person city person city similar zip zipcode person zipcode person zipcode similar state strcmp person state person state rules closer addresses zips closer address states similar addrs similar addrs similar city similar state jj similar zip rules ssn address name address similar ssns jj similar names similar addrs merge tuples person person continue close close person person person stnum person stnum close stnum close num person stnum person stnum close stnum false person aptm person aptm close aptm close str person aptm person aptm close aptm false rules compare addresses numbers state compare addresses numbers zipcode address city close stnum close close aptm similar city similar state jj similar zip similar addrs jj similar addrs close stnum close aptm similar zip similar addrs true rules ssn address name address similar ssns jj similar names merge tuples person person continue rule close ssn close address similar addrs similar ssns similar names ssn person ssn person ssn merge tuples person person continue rule hard case similar ssns similar addrs similar zip name initial person fname person fname merge tuples person person continue version equational theory rule program number tuples tuple window size compare tuples inside window 
match call merge tuples 
void rule program int int start int register int register person person boolean similar ssns similar names similar addrs boolean similar city similar state similar zip boolean similar close aptm close stnum close tuples consideration start person points th tuple person tuples tuples inside window tuples th tuple 
gamma gamma gamma person points th tuple person tuples compare person person rule find similar ssns similar ssns ssn person ssn person ssn rule compare names similar names compare names person name person name person fname person person lname person fname person person lname person fname init person fname init rule ssn name similar ssns similar names merge tuples person person continue rule compare addresses similar addrs compare addresses person person compare fields address similar city city person city person city similar zip zipcode person zipcode person zipcode similar state strcmp person state person state rules closer addresses zips closer address states similar addrs similar addrs similar city similar state jj similar zip rules ssn address name address similar ssns jj similar names similar addrs merge tuples person person continue close close person person person stnum person stnum close stnum close num person stnum person stnum close stnum false person aptm person aptm close aptm close str person aptm person aptm close aptm false rules compare addresses numbers state compare addresses numbers zipcode address city close stnum close close aptm similar city similar state jj similar zip similar addrs jj similar addrs close stnum close aptm similar zip similar addrs true rules ssn address name address similar ssns jj similar names merge tuples person person continue rule close ssn close address similar addrs similar ssns similar names ssn person ssn person ssn merge tuples person person continue rule hard case similar ssns similar addrs similar zip name initial person fname person fname merge tuples person person continue 
