bayesian approach causal discovery david heckerman microsoft research redmond wa microsoft com christopher meek microsoft research redmond wa meek microsoft com gregory cooper university pittsburgh pittsburgh pa smi med pitt edu february technical report msr tr microsoft research advanced technology division microsoft microsoft way redmond wa examine bayesian approach discovery directed acyclic causal models compare constraint approach 
approaches rely causal markov assumption differ significantly theory practice 
important difference approaches constraint approach uses categorical information conditional independence constraints domain bayesian approach weighs degree constraints hold 
result bayesian approach distinct advantages constraint counterpart 
derived bayesian approach susceptible incorrect categorical decisions independence facts occur data sets finite size 
bayesian approach finer distinctions model structures quantitative qualitative 
information models combined better inferences better account modeling uncertainty 
addition describing general bayesian approach causal discovery review approximation methods missing data hidden variables illustrate differences bayesian constraint methods artificial real examples 
examine bayesian approach discovery causal models family directed acyclic graphs 
bayesian approach related constraint approach discussed chapters collection 
particular methods rely causal markov assumption 
approaches differ significantly theory practice 
important difference constraint approach uses categorical information conditional independence constraints domain bayesian approach weighs degree constraints hold 
result bayesian approach distinct advantages constraint counterpart 
derived bayesian approach susceptible incorrect categorical decisions independence facts occur data sets finite size 
bayesian approach finer distinctions model structures quantitative qualitative 
information models combined better inferences better account modeling uncertainty 
sections review bayesian approach model averaging model selection application discovery directed acyclic causal models 
section discuss methods assigning priors model structures parameters 
section compare bayesian constraint methods causal discovery small domain complete data highlighting advantages bayesian approach 
section note computational difficulties associated bayesian approach data sets incomplete example variables hidden discuss efficient approximation methods including monte carlo asymptotic approximations 
section illustrate bayesian approach data set shah concerning college plans high school students 
example show bayesian approach finer distinctions model structures constraint approach 
bayesian approach constraint approach discovery directed causal models data categorical decisions particular conditional independence constraints hold 
piece decisions looking sets causal structures consistent constraints 
causal markov assumption spirtes link lack cause conditional independence 
bayesian approach causal markov assumption look structures fit conditional independence constraints 
constrast constraint methods data probabilistic inferences conditional independence constraints 
example conclude categorically data variables independent conclude variables independent probability 
probability encodes uncertainty presence absence independence 
furthermore bayesian approach uses probabilistic framework longer need decisions individual independence facts 
compute probability independencies associated entire causal structure true 
probabilities average particular hypothesis interest cause possible causal structures 
examine bayesian approach detail 
suppose problem domain consists variables fx addition suppose data fx xn random sample unknown probability distribution moment assume case consists observation variables assume unknown probability distribution encoded causal model structure spirtes 
assume structure causal model directed acyclic graph encodes conditional independencies causal markov assumption 
uncertain structure parameters model bayesian approach encode uncertainty probabil ity 
particular define discrete variable states correspond possible true models encode uncertainty probability distribution 
addition model structure define continuous vector valued variable theta values correspond possible true parameters 
encode uncertainty theta smooth probability density function jm 
assumption jm probability density function entails assumption faithfulness employed constraint methods causal discovery meek 
random sample compute posterior distributions bayes rule djm djm jd jm dj djm djm dj jm called marginal likelihood 
hypothesis interest determine probability true data averaging possible models parameters hjd hjd hjd hj jd example may event case xn observed configuration xn situation obtain jd jd likelihood model 
example may hypothesis causes 
consider situation detail section 
certain assumptions computations done efficiently closed form 
assumption likelihood term xj factors follows xj jpa local likelihood jpa exponential family 
expression pa denotes configuration variables corresponding parents node denotes set parameters associated local likelihood variable example factorization occurs variable discrete having possible values local likelihood collection multinomial distributions distribution configuration pa jpa ijk pa pa pa denote configurations pa ijk parameters 
parameter ij gamma ijk shall example illustrate concepts 
convenience define vector parameters ij ij ijr second assumption efficient computation parameters mutually independent 
example discrete multinomial likelihoods assume parameter vectors ij mutually independent 
examine consequences assumptions multinomial example 
random sample contains missing observations parameters remain independent jd ij jd update vector parameters ij independently 
assuming vector ij conjugate prior dirichlet distribution dir ij jff ij ff ijr obtain posterior distribution parameters ij jd dir ij jff ij ij ff ijr ijr ijk number cases pa pa note collection counts ijk sufficient statistics data model addition obtain marginal likelihood djm gamma ff ij gamma ff ij ij delta gamma ff ijk ijk gamma ff ijk ff ij ff ijk ij ijk equation equation compute posterior probabilities 
cooper herskovits derive equation 
simple illustration ideas suppose hypothesis interest outcome xn case seen suppose outcome xn xn bernardo smith provide summary likelihoods exponential family conjugate priors 
pa pa depend compute jd average uncertainty parameters 
equations obtain jd ijk jd parameters remain independent get jd ijk ij jd ij integral product expectation dirichlet distribution jd ff ijk ijk ff ij ij average expression jd possible models equation obtain jd 
section show bayesian approach compute probability variable causally influences data 
model selection search full bayesian approach impractical simplifying assumptions described 
computation bottleneck full bayesian approach averaging models equation 
consider causal models variables number possible structure hypotheses exponential consequently situations user exclude hypotheses approach intractable 
statisticians confronted problem decades context types models approaches address problem model selection selective model averaging 
approach select model structure hypothesis possible models model correct model 
approach select manageable number models possible models pretend models exhaustive 
related approaches raise important questions 
particular approaches yield accurate results applied causal structures 
search models 
question accuracy difficult answer theory 
researchers shown experimentally selection single model posteriori yields accurate predictions cooper herskovits aliferis cooper heckerman selective model averaging monte carlo methods efficient yield better predictions herskovits madigan 
chickering shown certain classes prior distributions problem finding model highest posterior np complete 
number researchers demonstrated greedy search methods search space directed acyclic graphs works 
constraint methods step heuristic search causal model singh spirtes meek 
addition performing greedy searches space markov equivalent models see definition represented single model improved performance spirtes meek chickering 
priors compute relative posterior probability model structure assess structure prior parameter priors jm 
unfortunately model structures possible assessments intractable 
certain assumptions derive structure parameter priors model structures manageable number direct assessments 
priors model parameters consider assessment priors parameters model structures 
consider approach heckerman 
address case local likelihoods multinomial distributions assumption parameter independence holds 
approach key concepts markov equivalence distribution equivalence 
say model structures markov equivalent represent set conditional independence assertions verma pearl 
example fx zg model structures represent independence assertion conditionally independent consequently model structures equivalent 
example markov equivalence set complete model structures complete model missing edge encodes assertion conditional independence 
contains variables 
possible complete model structures model structure possible ordering variables 
complete model structures markov equivalent 
general model structures markov equivalent structure ignoring arc directions structures verma pearl 
structure ordered tuple arc arc concept distribution equivalence closely related markov equivalence 
suppose causal models consideration local likelihoods family restriction se large family 
say model structures distribution equivalent respect wrt represent joint probability distributions exists xj xj vice versa 
distribution equivalence wrt implies markov equivalence converse hold 
example family generalized linear regression models complete model structures variables represent sets distributions 
families example multinomial distributions linear regression models gaussian noise markov equivalence implies distribution equivalence wrt heckerman geiger 
notion distribution equivalence important model structures distribution equivalent wrt reasonable expect data help discriminate 
expect djm djm data set heckerman 
call property likelihood equivalence 
note constraint approach discriminate markov equivalent structures 
return main issue section derivation priors manageable number assessments 
geiger heckerman show assumptions parameter independence likelihood equivalence imply parameters complete model structure dirichlet distribution constraints hyperparameters ff ijk ff pa jm ff user equivalent sample size pa jm computed user joint probability distribution xjm 
result remarkable assumptions leading constrained dirichlet solution qualitative 
determine priors parameters incomplete model structures heckerman 
assumption parameter modularity says parents model structures ij jm ij jm call property parameter modularity says discussions equivalent sample size winkler heckerman 

distributions parameters ij depend structure model local variable parents 
assumptions parameter modularity parameter independence simple matter construct priors parameters arbitrary model structure priors complete model structures 
particular parameter independence construct priors parameters node separately 
furthermore node parents pa model structure identify complete model structure parents equation parameter modularity determine priors node 
result terms ff ijk model structures determined equation 
assessments ff xjm derive parameter priors possible model structures 
assess xjm constructing causal model called prior model encodes joint distribution 
heckerman 
discuss construction model 
priors model structures consider assessment priors model structures 
simplest approach assigning priors model structures assume structure equally 
course assumption typically inaccurate sake convenience 
simple refinement approach ask user exclude various structures judgments cause effect impose uniform prior remaining structures 
illustrate approach section 
buntine describes set assumptions leads richer efficient approach assigning priors 
assumption variables ordered knowledge time precedence 
second assumption presence absence possible arcs mutually independent 
assumptions gamma probability assessments possible arc ordering determines prior probability possible model structures 
extension approach allow multiple possible orderings 
simplification assume probability arc absent independent specific arc question 
case probability assessment required 
alternative approach described heckerman 
uses prior model 
basic idea penalize prior probability structure measure deviation structure prior model 
heckerman 
suggest reasonable measure deviation 
construction procedure assumes structure non zero prior probability 
madigan 
give approach imaginary data domain expert 
approach computer program helps user create hypothetical set complete data 
techniques section compute posterior probabilities model structures data assuming prior probabilities structures uniform 
posterior probabilities priors analysis real data 
example section provide simple example applies bayesian model averaging bayesian model selection problem causal discovery 
addition compare methods constraint approach 
consider simple domain containing binary variables denote hypothesis variable causally influences variable brevity state causes 
consider bayesian model averaging 
approach equation compute probability true data models causal expression djm reduces index function true contains arc node node right hand side equation reduces jd sum taken causal models contain arc variable domain possible causal models models containing arc compute apply equation sum taken models just mentioned 
assume uniform prior distribution possible models equation compute marginal likelihood djm 
applying equation prior ff ijk obtain equation uniform distribution xjm equivalent sample ff 
equivalent sample size small data strongly influences posterior probabilities derive 
generate data selected model structure gamma 
gamma randomly sampled probabilities uniform distribution 
resulting model shown 
sampled data model joint distribution 
sampled data kept running total number cases seen possible configuration fx zg 
counts sufficient statistics data causal model statistics shown table cases data set 
false false true true false true false true true true true true true true causal model generate data 
table summary data example 
number sufficient statistics cases yz xy xyz yz xy xyz columns table shows results applying equation assumptions stated cases data set 
data set empty case probability hypothesis just prior probability causes 
table shows number cases database increases probability causes increases monotonically number cases increases 
shown probability increases number cases increases 
column table shows results applying bayesian model selection 
list causal relationship model models highest posterior probability 
example models highest posterior probability 
models cause cause column table shows results applying pc constraint causal discovery algorithm spirtes part tetrad ii system scheines 
pc designed discover causal relationships expressed directed acyclic graphs 
applied pc default settings include statistical signif algorithm assumes hidden variables 
see section discussion hidden table bayesian model averaging bayesian model selection constrain results analysis causes data summarized table 
number output output cases causes jd bayesian model selection pc algorithm unrelated unrelated unrelated causes causes unrelated causes inconsistency causes causes causes causes level 
note pc algorithm detected inconsistency 
particular independence tests yielded dependent dependent independent independent relationships consistent assumption underlying pc algorithm independence facts hold sample entailed causal markov assumption applied generating model 
general inconsistencies may arise due thresholds independence tests 
weaknesses bayesian model selection constraint approaches illustrated results 
output categorical indication strength 
may incorrect disagree generative model 
model averaging column suffer weaknesses indicates strength causal hypothesis 
illustrated weakness constraint approaches output depends threshold independence tests 
causal correct asymptotically threshold adjusted function sample size 
practice unclear function 
note practical problems model averaging 
particular domain large models average 
situations exact probabilities causal hypotheses calculated 
selective model averaging derive approximate posterior probabilities consequently give indication strength causal hypotheses 
variables 
methods incomplete data hidden variables assumptions described section violated assumption observations missing 
section examine bayesian methods relaxing assumption 
note constraint bayesian methods differ significantly way handle missing data 
constraint methods typically throw cases contain variable missing value bayesian methods 
important distinction concerning missing data absence observation dependent actual states variables 
example missing datum drug study may indicate patient sick due side effects drug continue study 
contrast variable hidden observed case absence data independent state 
bayesian methods graphical models suited analysis situations methods handling missing data absence independent state simpler absence state dependent 
concentrate simpler situation 
readers interested complicated case see rubin robins cooper spirtes 

continuing example discrete multinomial likelihoods suppose observe single incomplete case 
ae denote observed unobserved variables case respectively 
assumption parameter independence compute posterior distribution ij model structure follows ij jy zjy ij jy gamma pa jy fp ij jm pa jy ij jx pa see spiegelhalter lauritzen derivation 
term curly brackets equation dirichlet distribution 
variables pa observed case posterior distribution ij linear combination dirichlet distributions dirichlet mixture mixing coefficients gamma pa jy pa jy observe second incomplete case dirichlet components equation split dirichlet mixtures 
posterior distribution ij mixture dirichlet mixtures 
continue observe incomplete cases missing values posterior distribution ij contain number components exponential number cases 
general interesting set local likelihoods priors exact computation posterior distribution intractable 
require approximation incomplete data 
monte carlo methods class approximations monte carlo sampling methods 
approximations extremely accurate provided willing wait long computations converge 
section discuss monte carlo methods known gibbs sampling introduced geman geman 
variables fx joint distribution gibbs sampler approximate expectation function respect follows 
choose initial state variables random 
pick variable current state compute probability distribution states gamma variables 
sample state probability distribution compute 
iterate previous steps keeping track average value 
limit number cases approach infinity average equal provided conditions met 
gibbs sampler irreducible 
probability distribution eventually sample possible configuration possible initial configuration example contains zero probabilities gibbs sampler irreducible 
second chosen infinitely 
practice algorithm deterministically rotating variables typically 
introductions gibbs sampling methods including methods initialization discussion convergence neal madigan york 
illustrate gibbs sampling approximate probability density jd particular configuration incomplete data set fy yn causal model discrete variables independent dirichlet priors 
approximate jd initialize states unobserved variables case 
result complete random sample second choose variable il variable case observed original random sample reassign state probability distribution il jd il il il jm il il il jm il denotes data set observation il removed sum denominator runs states variable il seen terms numerator denominator computed efficiently see equation 
third repeat reassignment unobserved variables producing new complete random sample fourth compute posterior density jd described equations 
iterate previous steps average jd approximation 
monte carlo approximations useful computing marginal likelihood incomplete data 
monte carlo approach described chib raftery uses bayes theorem djm jm dj jd configuration prior term numerator evaluated directly 
addition likelihood term numerator computed causal model inference jensen 
posterior term denominator computed gibbs sampling just described 
sophisticated monte carlo methods described 

gaussian approximation monte carlo methods yield accurate results intractable example sample size large 
approximation efficient methods accurate relatively large samples gaussian approximation kass kass raftery 
idea approximation large amounts data jd dj delta jm approximated multivariate gaussian distribution 
particular log dj delta jm define configuration maximizes 
configuration maximizes jd known maximum posteriori map configuration second degree taylor polynomial approximate obtain gamma gamma gamma gamma transpose row vector gamma negative hessian evaluated raising power equation obtain jd dj jm dj jm expf gamma gamma gamma approximation jd gaussian 
compute gaussian approximation compute negative hessian evaluated section discuss methods finding meng rubin describe numerical technique computing second derivatives 
raftery shows approximate hessian likelihood ratio tests available statistical packages 
thiesson demonstrates multinomial distributions second derivatives computed causal model inference 
gaussian approximation approximate marginal likelihood 
substituting equation equation integrating logarithm result obtain approximation log djm log dj log jm log gamma log jaj dimension 
causal model multinomial distributions dimension typically gamma 
hidden variables dimension lower 
see geiger 
discussion point 
approximation technique integration known laplace method refer equation laplace approximation 
kass 
shown certain regularity conditions relative error approximation number cases laplace approximation extremely accurate 
detailed discussions approximation see example kass 
kass raftery 
laplace approximation efficient relative monte carlo approaches computation jaj intensive large dimension models 
simplification approximate jaj diagonal elements hessian doing incorrectly impose independencies parameters researchers shown approximation accurate circumstances see becker le cun chickering heckerman 
efficient variant laplace approximation described cheeseman stutz chickering heckerman 
obtain efficient accurate approximation retaining terms equation increase log dj increases linearly log jaj increases log large approximated maximum likelihood configuration see section 
obtain log djm log dj gamma log approximation called bayesian information criterion bic derived schwarz 
bic approximation interesting respects 
depend prior 
consequently approximation assessing prior 
second approximation quite intuitive 
contains term measuring parameterized model predicts data log dj term punishes complexity model logn 
third bic approximation exactly minus minimum description length mdl criterion described rissanen 
map ml approximations em algorithm sample size data increases gaussian peak sharper tending delta function map configuration limit replace integral equation hj 
approximation observation sample size increases effect prior jm diminishes 
approximate maximum maximum likelihood ml configuration arg max fp dj class techniques finding ml map gradient optimization 
example gradient ascent follow derivatives likelihood dj local maximum 
russell 
thiesson show compute derivatives likelihood causal model multinomial distributions 
buntine discusses general case likelihood comes exponential family 
course gradient methods find local maxima 
technique finding local ml map expectation maximization em algorithm dempster 
find local map ml assigning configuration random 
compute expected sufficient statistics complete data set expectation taken respect joint distribution conditioned assigned configuration known data discrete example compute xjd ijk pa jy possibly incomplete lth case variables pa observed case term case requires trivial computation zero technical assumptions derive approximation prior non zero 
causal model inference algorithm evaluate term 
computation called expectation step em algorithm 
expected sufficient statistics actual sufficient statistics complete random sample doing ml calculation determine configuration maximizes 
discrete example ijk xjd ijk xjd ijk doing map calculation determine configuration maximizes jd 
discrete example ijk ff ijk xjd ijk ff ijk xjd ijk assignment called maximization step em algorithm 
dempster 
showed certain regularity conditions iteration expectation maximization steps converge local maximum 
em algorithm typically applied sufficient statistics exist local likelihoods exponential family generalizations em complicated local distributions see saul 
case study illustrate bayesian approach differences constraintbased approach consider example 
sewell shah investigated factors influence intention high school students attend college 
measured variables wisconsin high school sex sex male female socioeconomic status ses low lower middle upper middle high intelligence quotient iq low lower middle upper middle high parental encouragement pe low high college plans cp 
goal understand causal relationships variables 
data described sufficient statistics table 
entry denotes number cases variables take particular configuration 
entry corresponds configuration sex male ses low iq low pe low cp 
remaining entries correspond configurations obtained cycling map configuration depends coordinate system parameter variables expressed 
map corresponds canonical coordinate system multinomial distribution see bernardo smith pp 

table sufficient statistics shah study 
reproduced permission university chicago press 
fl university chicago 
rights reserved 
states variable variable cp varies quickly 
example upper lower half table corresponds male female students 
analyze data assumption hidden variables 
generate priors model parameters method described section equivalent sample size prior model xjm uniform 
results sensitive choice parameter priors 
example results reported section change qualitatively equivalent sample sizes ranging 
structure priors assume model structures equally basis prior causal knowledge domain exclude structures sex ses parents cp children 
data set complete equation compute posterior probabilities model structures 
model structures exhaustive search structures shown 
note graph posterior probability extremely close model averaging necessary 
adopt causal markov assumption assume hidden variables arcs graphs interpreted causally 
results surprising example causal influence socioeconomic status iq college plans 
results interesting 
example graph conclude sex influences college plans indirectly parental influence 
graphs differ orientation arc pe iq 
causal relationship plausible 
ses sex pe iq cp log ses sex pe iq cp log posteriori model structures hidden variables 
note second graph selected spirtes 
constraint pc algorithm essentially identical assumptions 
differences independence facts entailed graph second graphs graph entails sex iq independent ses pe second graph entails sex iq marginally independent 
bayesian classical independence tests indicate conditional independence holds strongly data pc algorithm chooses second graph due greedy nature 
particular pc algorithm decides sex iq marginally independent threshold spirtes considers independence sex iq ses pe 
returning analysis suspicious result suggestion socioeconomic status direct influence iq 
question result consider new models obtained models replacing direct influence hidden variable pointing ses iq 
consider models hidden variable points ses iq pe connections ses pe pe iq removed 
structure vary number states hidden variable 
compute posterior probability models cheeseman stutz variant laplace approximation 
find map em algorithm largest local maximum runs different random initializations model highest posterior probability shown 
model delta times best model containing hidden variable 
model containing hidden variable additional arc hidden variable pe delta gamma times best model 
adopt causal markov assumption assume omitted reasonable model male ses sex pe iq cp pe low low high high iq high pe ses high sex male female male female ses low low high high pe high ses sex ses low low low low high high high high pe low high low high low high low high iq low low high high low low high high cp ses iq pe log posteriori model structure hidden variable 
probabilities shown map values 
probabilities omitted lack space 
consideration strong evidence hidden variable influencing socioeconomic status iq population sensible result 
particular probabilities ses iq high takes value 
observation suggests hidden variable represents parent quality 
possible constraint methods independence constraints discriminate models hidden variables indicate presence latent variables see spirtes 
methods distinguish model graph hidden variables network left graphs entail set independence facts 
types constraints considered example tetrad constraints best knowledge constraints distinguish models 
constraint methods bayesian methods case study illustrates determine number classes latent variable 
constraint method independence constraints determine number classes latent variable 
conjecture distinction causal structures constraint methods restricted independence constraints bayesian methods 
addition conjecture asymptotically constraint method chooses model bayesian approach choice provided causal markov assumption assumption faithfulness hold 
open issues bayesian framework gives conceptually simple framework learning causal models 
bayesian solution comes high computational cost 
example learn causal models containing hidden variables exact computation marginal likelihood model averaging selection intractable 
approximations described section applied address difficulties associated computation marginal likelihood model averaging model selection remain difficult 
number possible models hidden variables significantly larger number possible directed acyclic graphs fixed set variables 
constraining set possible models hidden variables instance restricting number hidden variables number possible models infinite 
positive note spirtes 
shown constraint methods suitable assumptions indicate existence hidden common cause variables 
may possible constraint methods suggest initial set plausible models containing hidden variables subjected bayesian analysis 
problem associated learning causal models containing hidden variables assessment parameter priors 
approach section applied situations assessment joint distribution xjm includes hidden variables difficult 
approach may employ property called strong likelihood equivalence heckerman 
property data help discriminate models distribution equivalent respect non hidden variables 
heckerman showed method uses property yield priors differ obtained prior network 
course sample size sufficiently large bic approximation prior assessment longer problem 
models hidden variables interesting issues addressed 
discuss discrete variables having type local likelihood multinomial 
thiesson discusses class local likelihoods discrete variables fewer parameters 
geiger heckerman buntine discuss simple linear local likelihoods continuous nodes continuous discrete variables 
buntine discusses general class local likelihoods exponential family nodes having parents 
alternative likelihoods discrete continuous particular heckerman showed strong likelihood equivalence consistent parameter independence parameter modularity 
variables desired 
local likelihoods fewer parameters allow selection correct models data 
addition local likelihoods express accurately data generating process allow easier interpretation resulting models 
acknowledgments max chickering implementing software analysis shah data 
aliferis cooper aliferis cooper 

evaluation algorithm inductive learning bayesian belief networks simulated data sets 
proceedings tenth conference uncertainty artificial intelligence seattle wa pages 
morgan kaufmann 
becker lecun becker lecun 

improving convergence back propagation learning second order methods 
proceedings connectionist models summer school pages 
morgan kaufmann 
bernardo smith bernardo smith 

bayesian theory 
john wiley sons new york 
buntine buntine 

theory refinement bayesian networks 
proceedings seventh conference uncertainty artificial intelligence los angeles ca pages 
morgan kaufmann 
buntine buntine 

operations learning graphical models 
journal artificial intelligence research 
cheeseman stutz cheeseman stutz 

bayesian classification autoclass theory results 
fayyad shapiro smyth uthurusamy editors advances knowledge discovery data mining page 
aaai press menlo park ca 
chib chib 

marginal likelihood gibbs output 
journal american statistical association 
chickering chickering 

learning bayesian np complete 
fisher lenz editors learning data pages 
springer verlag 
chickering chickering 

learning equivalence classes structures 
proceedings twelth conference uncertainty artificial intelligence portland 
morgan kaufmann 
chickering heckerman chickering heckerman 
revised november 
efficient approximations marginal likelihood incomplete data bayesian network 
technical report msr tr microsoft research redmond wa 
cooper cooper 

causal discovery data presence selection bias 
proceedings fifth international workshop artificial intelligence statistics pages fort lauderdale fl 
cooper herskovits cooper herskovits 

bayesian method induction probabilistic networks data 
machine learning 
dempster dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
kass raftery wasserman 
july 
computing bayes factors combining simulation asymptotic approximations 
technical report department statistics carnegie mellon university pa geiger heckerman geiger heckerman 

learning gaussian networks 
proceedings tenth conference uncertainty artificial intelligence seattle wa pages 
morgan kaufmann 
geiger heckerman geiger heckerman 
revised february 
characterization dirichlet distribution applicable learning bayesian networks 
technical report msr tr microsoft research redmond wa 
geiger geiger heckerman meek 

asymptotic model selection directed networks hidden variables 
proceedings twelth conference uncertainty artificial intelligence portland 
morgan kaufmann 
geman geman geman geman 

stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
heckerman heckerman 

bayesian approach learning causal networks 
proceedings eleventh conference uncertainty artificial intelligence montreal qu pages 
morgan kaufmann 
heckerman geiger heckerman geiger 
revised november 
likelihoods priors bayesian networks 
technical report msr tr microsoft research redmond wa 
heckerman heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
machine learning 
herskovits herskovits 

computer probabilistic network construction 
phd thesis medical information sciences stanford university stanford ca 
jensen jensen lauritzen olesen 

bayesian updating recursive graphical models local computations 
computational quarterly 
kass raftery kass raftery 

bayes factors 
journal american statistical association 
kass kass tierney kadane 

asymptotics bayesian computation 
bernardo degroot lindley smith editors bayesian statistics pages 
oxford university press 
madigan madigan raftery 

eliciting prior information enhance predictive performance bayesian graphical models 
communications statistics theory methods 
madigan madigan raftery 

bayesian model averaging 
proceedings aaai workshop integrating multiple learned models portland 
madigan york madigan york 

bayesian graphical models discrete data 
international statistical review 
meek meek 

strong completeness faithfulness bayesian networks 
proceedings eleventh conference uncertainty artificial intelligence montreal qu pages 
morgan kaufmann 
meng rubin meng rubin 

em obtain asymptotic variance covariance matrices sem algorithm 
journal american statistical association 
neal neal 

probabilistic inference markov chain monte carlo methods 
technical report crg tr department computer science university toronto 
raftery raftery 

bayesian model selection social research 
marsden editor sociological methodology 
cambridge ma 
raftery raftery 

hypothesis testing model selection posterior simulation 
practical markov chain monte carlo 
chapman hall 
appear 
rissanen rissanen 

stochastic complexity discussion 
journal royal statistical society series 
robins robins 

new approach causal mortality studies sustained exposure results 
mathematical modelling 
rubin rubin 

bayesian inference causal effects role randomization 
annals statistics 
russell russell binder koller kanazawa 

local learning probabilistic networks hidden variables 
proceedings fourteenth international joint conference artificial intelligence montreal qu pages 
morgan kaufmann san mateo ca 
saul saul jaakkola jordan 

mean field theory sigmoid belief networks 
journal artificial intelligence research 
scheines scheines spirtes glymour meek 

tetrad ii user manual 
lawrence erlbaum hillsdale schwarz schwarz 

estimating dimension model 
annals statistics 
sewell shah sewell shah 

social class parental encouragement educational aspirations 
american sociology 
singh singh 

algorithm construction bayesian network structures data 
proceedings ninth conference uncertainty artificial intelligence washington dc pages 
morgan kaufmann 
spiegelhalter lauritzen spiegelhalter lauritzen 

sequential updating conditional probabilities directed graphical structures 
networks 
spirtes spirtes glymour scheines 

causation prediction search 
springer verlag new york 
spirtes meek spirtes meek 

learning bayesian networks discrete variables data 
proceedings international conference knowledge discovery data mining montreal qu 
morgan kaufmann 
spirtes spirtes meek richardson 

causal inference presence latent variables selection bias 
proceedings eleventh conference uncertainty artificial intelligence montreal qu pages 
morgan kaufmann 
thiesson thiesson 

score information recursive exponential models incomplete data 
technical report institute electronic systems aalborg university aalborg denmark 
verma pearl verma pearl 

equivalence synthesis causal models 
proceedings sixth conference uncertainty artificial intelligence boston ma pages 
morgan kaufmann 
winkler winkler 

assessment prior distributions bayesian analysis 
american statistical association journal 
