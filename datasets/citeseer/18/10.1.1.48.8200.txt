large margin classification perceptron algorithm yoav freund robert schapire labs park avenue florham park nj usa research att com december introduce analyze new algorithm linear classification combines rosenblatt perceptron algorithm helmbold warmuth leave method 
vapnik maximal margin classifier algorithm takes advantage data linearly separable large margins 
compared vapnik algorithm simpler implement efficient terms computation time 
show algorithm efficiently high dimensional spaces kernel functions 
performed experiments algorithm variants classifying images handwritten digits 
performance algorithm close performance maximal margin classifiers problem saving significantly computation time programming effort 
influential developments theory machine learning years vapnik support vector machines svm 
vapnik analysis suggests simple method learning complex binary classifiers 
fixed mapping phi map instances high dimensional space classes linearly separable 
quadratic programming find vector classifies data correctly maximizes margin minimal distance separating hyperplane instances 
main contributions 
proof new bound difference training error test error linear classifier maximizes preliminary version appeared proceedings eleventh annual conference computational learning theory 
current draft submitted journal publication 
margin 
significance bound depends size margin number support vectors dimension 
superior bounds arbitrary consistent linear classifiers 
second contribution method computing maximal margin classifier efficiently specific high dimensional mappings 
method idea kernel functions described detail section 
main part algorithms finding maximal margin classifier computation solution large quadratic program 
constraints program correspond training examples number large 
practical support vector machines centered finding efficient ways solving quadratic programming problems 
introduce new simpler algorithm linear classification takes advantage data linearly separable large margins 
named new algorithm voted perceptron algorithm 
algorithm known perceptron algorithm rosenblatt transformation online learning algorithms batch learning algorithms developed helmbold warmuth 
aizerman braverman show kernel functions algorithm run algorithm efficiently high dimensional spaces 
algorithm analysis involve little combining known methods 
hand resulting algorithm simple easy implement theoretical bounds expected generalization error new algorithm identical bounds svm vapnik chervonenkis linearly separable case 
repeated experiments performed cortes vapnik svm problem classifying handwritten digits 
tested voted perceptron algorithm variant averaging voting 
experiments indicate kernel functions perceptron algorithm yields dramatic improvement performance test accuracy computation time 
addition training time limited voted perceptron algorithm performs better traditional way perceptron algorithm methods converge eventually roughly level performance 
cristianini campbell experimented different online learning algorithm called 
algorithm suggested method calculating largest margin classifier called maximally stable perceptron 
proved algorithm converges asymptotically correct solution 
organized follows 
section describe voted perceptron algorithm 
section derive upper bounds expected generalization error linearly separable inseparable cases 
section review method kernels describe algorithm 
section summarize results experiments handwritten digit recognition problem 
conclude section summarize observations relations theory experiments suggest new open problems 
algorithm assume instances points jjxjj denote euclidean length assume labels gamma 
basis study classical perceptron algorithm invented rosenblatt 
simple algorithm naturally studied online learning model 
online perceptron algorithm starts initial zero prediction vector 
predicts label new instance sign delta 
prediction differs label updates prediction vector yx 
prediction correct changed 
process repeats example 
common way perceptron algorithm learning batch training examples run algorithm repeatedly training set finds prediction vector correct training set 
prediction rule predicting labels test set 
block minsky papert shown data linearly separable perceptron algorithm finite number mistakes repeatedly cycled training set converge vector correctly classifies examples 
number mistakes upper bounded function gap positive negative examples fact central analysis 
propose sophisticated method applying online perceptron algorithm batch learning variation leave method helmbold warmuth 
voted perceptron algorithm store information training elaborate information generate better predictions test data 
algorithm detailed 
information maintain training list prediction vectors generated mistake 
vector count number iterations survives mistake refer count weight prediction vector 
calculate prediction compute binary prediction prediction vectors combine predictions weighted majority vote 
weights survival times described 
intuitive sense prediction vectors tend survive long time larger weight majority vote 
analysis section give analysis voted perceptron algorithm case algorithm runs exactly training data 
quote theorem vapnik chervonenkis linearly separable case 
theorem bounds generalization error consistent perceptron perceptron algorithm run convergence 
interestingly linearly separable case theorems yield similar bounds 
shall see experiments algorithm continues improve performance 
theoretical explanation improvement 
data linearly separable perceptron algorithm eventually converge storing vectors excessive waste memory 
shall see perceptrons kernels excess memory really quite minimal 
training input labeled training set ym number epochs output list weighted perceptrons ffl initialize 
ffl repeat times compute prediction sign delta 

prediction list weighted perceptrons unlabeled instance compute predicted label follows sign delta sign voted perceptron algorithm 
consistent hypothesis prediction vector correct training examples 
prediction vector mistakes eventually dominate weighted vote voted perceptron algorithm 
linearly separable data algorithm converges regular perceptron algorithm predict final prediction vector 
learned performance final prediction vector analyzed vapnik chervonenkis 
discuss bound section 
give analysis case 
analysis parts combines known material 
review classical analysis online perceptron algorithm linearly separable case extension inseparable case 
second review analysis leave conversion online learning algorithm batch learning algorithm 
online perceptron algorithm separable case analysis known result proved block 
significance result number mistakes depend dimension instances 
gives reason believe perceptron algorithm perform high dimensional spaces 
theorem block ym sequence labeled examples jjx jj suppose exists vector jjujj delta fl examples sequence 
number mistakes online perceptron algorithm sequence fl proof proof known repeat completeness 
denote prediction vector prior kth mistake 
kth mistake occurs delta delta delta delta delta fl delta 
similarly jjv jj jjv jj delta jjx jj jjv jj jjv jj kr combining gives kr jjv jj delta implies fl proving theorem 
analysis inseparable case data linearly separable theorem directly 
give generalized version theorem allows mistakes training set 
far know theorem new proof technique similar simon theorem 
theorem ym sequence labeled examples jjx jj vector jjujj fl 
define deviation example maxf fl gamma delta define number mistakes online perceptron algorithm sequence bounded fl proof case follows theorem assume 
proof reduction inseparable case separable case higher dimensional space 
see reduction change algorithm 
extend instance space adding new dimensions example 
denote extension instance set coordinates equal set th coordinate delta delta positive real constant value specified 
rest coordinates set zero 
extend comparison vector constant calculate shortly ensure length 
set coordinates equal set th coordinate delta 
easy check appropriate normalization delta consider value delta delta delta delta delta delta delta fl gamma delta fl extended prediction vector achieves margin fl delta extended examples 
order apply theorem need bound length instances 
jjx jj additional non zero coordinate value delta get jjx jj delta values theorem get number mistakes online perceptron algorithm run extended space delta delta fl setting delta rd minimizes bound yields bound statement theorem 
finish proof show predictions perceptron algorithm extended space equal predictions perceptron original space 
denote prediction vector predicting instance original space denote prediction vector predicting corresponding instance extended space 
claim follows induction claims 
coordinates equal 
th coordinate equal zero 

sign delta sign delta 
converting online batch algorithm mistakes examples 
setup interested batch setup training set generate hypothesis tested seperate test set 
data linearly separable perceptron algorithm eventually converges final prediction rule hypothesis 
data separable want wait till convergence achieved 
case decide best prediction rule sequence different classifiers online algorithm 
solution problem prediction rule survived longest time changed 
prediction rule survived long time better survived iterations 
method suggested gallant called pocket method 
littlestone suggested phase method performance rules tested seperate test set rule error 
different method converting online perceptron algorithm batch learning algorithm method combines rules generated online algorithm run just single time training data 
describe helmbold warmuth simple leave method converting online learning algorithm batch learning algorithm 
voted perceptron algorithm simple application general method 
start randomized version 
training set ym unlabeled instance 
select number mg uniformly random 
take examples training sequence append unlabeled instance subsequence 
run online algorithm sequence length prediction online algorithm unlabeled instance 
deterministic leave conversion modify randomized leave conversion deterministic obvious way choosing prediction 
compute prediction result possible choices mg take majority vote predictions 
straightforward show majority vote runs risk doubling probability mistake potential significantly decreasing 
decided focus primarily deterministic voting randomization 
theorem follows directly helmbold warmuth 
see kivinen warmuth cesa bianchi 
theorem assume examples generated expected number mistakes online algorithm randomly generated sequence examples 
random training examples expected probability randomized leave oneout conversion mistake randomly generated test instance 
deterministic leave conversion expected probability 
putting verified deterministic leave conversion online perceptron algorithm exactly equivalent voted perceptron algorithm 
combining theorems corollary assume examples generated random 
ym sequence training examples ym test example 
max im jjx jj 
jjujj fl fl maxf fl gamma delta probability choice examples voted perceptron algorithm predict ym test instance xm inf jjujj fl fl fl expectation choice examples 
fact proof yields slightly stronger statement depends examples mistakes occur 
formally stated follows corollary assume examples generated random 
suppose run online perceptron algorithm sequence ym mistakes occur examples indices redefine max jk jjx jj redefine fl maxf fl gamma delta suppose run voted perceptron algorithm training examples ym single epoch 
probability choice examples algorithm predict ym test instance xm inf jjujj fl fl fl expectation choice examples 
similar theorem proved vapnik chervonenkis theorem training perceptron algorithm convergence predicting final perceptron vector 
theorem vapnik chervonenkis assume examples generated random 
suppose run online perceptron algorithm sequence ym repeatedly convergence mistakes occur total examples indices max jk jjx jj fl max jjujj min jk delta assume fl probability 
suppose run perceptron algorithm convergence training examples ym probability choice examples final perceptron predict ym test instance xm min fl expectation choice examples 
separable case fl set zero corollary identical theorem 
difference lose factor 
deterministic algorithm randomized 
important difference number mistakes perceptron certainly larger perceptron run convergence run just single epoch 
gives indication running voted perceptron algorithm better running convergence experiments support prediction 
vapnik gives similar bound expected error support vector machines 
differences bounds 
set vectors perceptron mistake replaced set essential support vectors 
second radius maximal distance support vector optimally chosen vector origin 
support vectors training examples fall closest decision boundary 
kernel classification seen voted perceptron algorithm guaranteed performance bounds data linearly separable 
linear separability strict condition 
way method powerful adding dimensions features input space 
new coordinates nonlinear functions original coordinates 
usually add coordinates data linearly separable 
separation sufficiently senses theorems expected generalization error small provided increase complexity instances moving higher dimensional space 
computational point view computing values additional coordinates prohibitively hard 
problem solved elegant method kernel functions 
kernel functions classification problems proposed suggested aizerman braverman specifically described method combining functions perceptron algorithm 
continuing boser guyon vapnik suggested kernel functions svm 
kernel functions functions variables represented inner product phi delta phi function phi 
words calculate mapping vectors phi phi inner product 
instance important kernel function polynomial expansion delta exist general conditions checking function kernel function 
particular case straightforward construct phi witnessing kernel function 
instance choose phi general define phi coordinate cm monomial degree variables appropriately chosen constant 
aizerman braverman observed perceptron algorithm formulated way computations involving instances fact terms inner products delta pairs instances 
want map instance vector phi high dimensional space need able compute inner products phi delta phi exactly computed kernel function 
conceptually kernel method vectors high dimensional space algorithm performance depends linear separability expanded space 
computationally need modify algorithm replacing inner product computation delta kernel function computation 
similar observations boser guyon vapnik vapnik svm algorithm 
observe computations voted perceptron learning algorithm involving instances written terms inner products means apply kernel method voted perceptron algorithm 
referring see training prediction involve inner products instances prediction vectors order perform operation efficiently store prediction vector implicit form sum instances added subtracted order create 
written stored sum gamma appropriate indices calculate inner product delta gamma delta kernel function merely replace delta 
computing prediction final vector test instance requires kernel calculations number mistakes algorithm training 
naively prediction voted perceptron require kernel calculations need compute delta involves sum gamma instances 
advantage recurrence delta delta delta clear compute prediction voted perceptron kernel calculations 
calculating prediction voted perceptron kernels marginally expensive calculating prediction final prediction vector assuming methods trained number epochs 
epoch random unnorm unnorm avg unnorm vote epoch random unnorm unnorm avg unnorm vote epoch random unnorm unnorm avg unnorm vote epoch random unnorm unnorm avg unnorm vote epoch random unnorm unnorm avg unnorm vote epoch random unnorm unnorm avg unnorm vote learning curves algorithms tested nist data 
experiments experiments followed closely experimental setup cortes vapnik experiments nist ocr database 
chose setup dataset widely available lecun published detailed comparison performance best digit classification systems setup :10.1.1.21.4023
examples nist database consist labeled digital images individual handwritten digits 
instance theta matrix entry bit representation grey value labels set 
dataset consists training examples test examples 
treat image vector cortes vapnik polynomial kernels eq 
expand vector high dimensions 
handle multiclass data essentially reduced binary problems 
trained voted perceptron algorithm classes 
training class replaced labeled example binary labeled example gamma 
sequence weighted prediction vectors result training class 
predictions new instance tried different methods 
method compute score predict label receiving national institute standards technology special database 
see www research att com yann ocr information obtaining dataset list relevant publications 
vote avg 
unnorm norm unnorm norm rand 
unnorm norm mistake vote avg 
unnorm norm unnorm norm rand 
unnorm norm mistake vote avg 
unnorm norm unnorm norm rand 
unnorm norm mistake table results experiments nist class ocr data 
rows marked mistake give average number support vectors average number mistakes 
rows give test error rate percent various methods 
highest score arg max method compute score respective final prediction vector delta method denoted unnormalized results 
variant method compute scores normalizing final prediction vectors delta jjv jj vote avg 
unnorm norm unnorm norm rand 
unnorm norm mistake vote avg 
unnorm norm unnorm norm rand 
unnorm norm mistake vote avg 
unnorm norm unnorm norm rand 
unnorm norm mistake table results experiments nist class ocr data 
rows marked mistake give average number support vectors average number mistakes 
rows give test error rate percent various methods 
method denoted normalized results 
note normalizing vectors effect binary problems plausibly important multiclass case 
method denoted vote uses analog deterministic leave conversion 
set sign delta third method denoted average unnormalized uses average predictions prediction vectors delta label vote avg 
unnorm norm rand 
mistake vote avg 
unnorm norm rand 
mistake vote avg 
unnorm norm rand 
mistake vote avg 
unnorm norm rand 
mistake cortes vapnik table results experiments individual classes polynomial kernels 
rows marked mistake give average number support vectors average number mistakes 
rows give test error rate percent various methods 
method tried variant denoted average normalized normalized prediction vectors delta jjv jj final method denoted random unnormalized possible analog randomized leave method predict prediction vectors exist randomly chosen time slice 
number rounds executed number examples vote avg 
unnorm norm rand 
mistake vote avg 
unnorm norm rand 
mistake vote avg 
unnorm norm rand 
mistake vote avg 
unnorm norm rand 
mistake vote avg 
unnorm norm rand 
mistake vote avg 
unnorm norm rand 
mistake table results experiments nist data distinguishing digits 
rows marked mistake give average number support vectors average number mistakes 
rows give test error rate percent various methods 
processed inner loop algorithm 
classify choose time slice tg uniformly random 
set delta index final vector existed time label 
formally largest number satisfying gamma analogous normalized method random normalized uses delta jjv jj analysis applicable cases voted randomly chosen predictions 
experiments ran algorithm 
polynomial kernels degree data linearly separable 
iterations perceptron algorithm converges consistent prediction vector mistakes 
happens final perceptron gains weight vote average 
tends effect causing variants converge eventually solution 
reaching limit compare voted perceptron algorithm standard way perceptron algorithm find consistent prediction rule 
performed experiments polynomial kernels dimensions corresponds expansion 
preprocessed data experiment randomly permuting training sequence 
experiment repeated times time different random permutation training examples 
able run experiment epochs reasons described 
shows plots test error function number epochs prediction methods vote unnormalized versions average random omitted normalized versions sake readability 
test errors averaged multiple runs algorithm plotted point tenth epoch 
results summarized numerically tables show average test error values different methods rows marked vote avg 
unnorm rows marked show number support vectors total number instances computing scores 
words size union instances mistake occured training 
rows marked mistake show total number mistakes training different labels 
case averaged multiple runs algorithm 
column corresponding helpful getting idea algorithms perform smaller datasets case algorithm tenth available data training examples 
ironically algorithm runs slowest small values larger values move higher dimensional space data linearly separable 
small values especially data linearly separable means perceptron algorithm tends mistakes slows algorithm significantly 
complete run epochs days computation 
comparison run epochs hours complete run takes hours 
running times single sgi mips processor running mhz 
significant improvement performance clearly 
migration higher dimensional space tremendous difference compared running algorithm space 
improvements nearly dramatic 
results indicate voting averaging perform better vector 
especially true prior convergence perceptron updates 
data highly inseparable case improvement persists long able run algorithm 
higher dimensions data separable perceptron update rule converges converges case performance prediction methods similar 
case advantage voting averaging relatively small number epochs 
significant difference voting averaging terms performance 
random vectors performs worst cases contrary worst case analysis 
normalized vectors help bit method help hurt performance slightly average method case differences performance normalized unnormalized vectors minor 
lecun give detailed comparison algorithms dataset :10.1.1.21.4023
best algorithms tested old version boosting top neural net lenet achieves error rate 
version optimal margin classifier algorithm kernel function performs significantly better achieving test error rate 
table shows variants perceptron algorithm perform binary problems corresponding class labels 
table fix compare performance reported cortes vapnik svm 
table gives details perceptron methods perform single binary problem distinguishing images 
note binary problems come closest theory discussed earlier 
interesting perceptron algorithm generally ends fewer support vectors svm algorithm 
summary significant result experiments running perceptron algorithm higher dimensional space kernel functions produces significant improvements performance yielding accuracy levels comparable inferior obtainable machines 
hand algorithm faster easier implement method 
addition theoretical analysis expected error perceptron algorithm yields similar bounds support vector machines 
open problem develop better theoretical understanding empirical superiority support vector machines 
find significant voting averaging better just final hypothesis 
indicates theoretical analysis suggests voting capturing truth 
hand theoretical explanation improvement performance epoch 
acknowledgments vladimir vapnik helpful discussions pointing theorem 
aizerman braverman 
theoretical foundations potential function method pattern recognition learning 
automation remote control 

adaptive perceptron algorithm 
letters dec 
block 
perceptron model brain functioning 
reviews modern physics 
reprinted neurocomputing anderson rosenfeld 
bernhard boser isabelle guyon vladimir vapnik 
training algorithm optimal margin classifiers 
proceedings fifth annual acm workshop computational learning theory pages 
cesa bianchi yoav freund david haussler david helmbold robert schapire manfred warmuth 
expert advice 
journal association computing machinery may 
corinna cortes vladimir vapnik 
support vector networks 
machine learning september 
nello cristianini colin campbell 
kernel fast simple learning procedure support vector machines 
machine learning proceedings fifteenth international conference 
gallant 
optimal linear discriminants 
eighth international conference pattern recognition pages 
ieee 
david helmbold manfred warmuth 
weak learning 
journal computer system sciences 
kivinen manfred warmuth 
additive versus exponentiated gradient updates linear prediction 
information computation january 
norbert hans ulrich simon 
noise free noise tolerant online batch learning 
proceedings eighth annual conference computational learning theory pages 
lecun jackel bottou cortes denker drucker guyon muller sackinger simard vapnik :10.1.1.21.4023
comparison learning algorithms handwritten digit recognition 
international conference artificial neural networks pages 
nick littlestone 
line batch learning 
proceedings second annual workshop computational learning theory pages july 
marvin minsky seymour papert 
perceptrons computational geometry 
mit press 

convergence proofs perceptrons 
proceedings symposium mathematical theory automata volume xii pages 
rosenblatt 
perceptron probabilistic model information storage organization brain 
psychological review 
reprinted neurocomputing mit press 
rosenblatt 
principles neurodynamics 
spartan new york 
vapnik 
estimation dependences empirical data 
springer verlag 
vapnik ya 
chervonenkis 
theory pattern recognition 
nauka moscow 
russian 
vladimir vapnik 
statistical learning theory 
wiley appear 
