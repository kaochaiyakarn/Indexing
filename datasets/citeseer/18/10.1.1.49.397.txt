neural networks logistic regression part martin schumacher reinhard ro ner werner institute medical informatics university freiburg stefan meier str 
freiburg im germany center data analysis model building university freiburg 
freiburg im germany feed forward neural networks applied situations analysis logistic regression model standard statistical approach direct comparisons results seldomly attempted 
comparative investigation logistic regression models feed forward neural networks including extensions 
theoretical features properties reviewed illustrated examples discussing practical problems application 
part ii important aspects approximation overfitting model selection investigated detail analytically means simulation studies 
key words feed forward neural networks logistic regression model newton raphson algorithm survival data overfitting model selection 
years successful application neural networks practical problems demonstrated numerous papers 
medical field applications range diagnosis myocardial classification eeg signals reddy pet scans prediction mechanisms action cancer drug development weinstein 
applications analysis logistic regression model cox cox snell techniques discriminant analysis mclachlan standard approach statistical analysis 
typical example study dependence admission decision psychiatric emergency room patient characteristics diagnostic features 
come neural network trained clinical decisions substantial agreement experienced clinicians 
despite fact successful applications reported failures published sold interesting note direct comparisons neural networks statistical analyses logistic regression model discriminant analysis seldomly attempted discovered exceptions gallinari weinstein 
hand review articles monographs statistical aspects neural networks geman white ripley de de wet cheng titterington cherkassky murtagh michie ripley concentrate comparisons modern statistical methods classification regression trees breiman generalized additive models hastie tibshirani multivariate adaptive regression splines friedman nonparametric regression techniques 
logistic regression model equivalence perceptron logistic activation function representing simple neural network usually briefly mentioned 
feel thorough comparative investigation logistic regression neural networks deserves attention 
plan follows brief review logistic regression introduce basic features feed forward neural networks essentially perceptron extensions back propagation algorithm 
concentrating kullback leibler distance kullback leibler euclidean distance energy function show conceptual similarities discrepancies approaches 
discuss practical problems occurring application neural networks data 
points illustrated examples known data literature finney second represents current typical study medical field 
discussion finishes part 
part ii important aspects investigated detail analytically means simulation studies 
aspects investigated include problem regression functions logistic models approximated neural networks role weight decay comparison model selection procedures 
brief review logistic regression sequel consider situation observe binary outcome variable vector regressor variables covariates individuals 
logistic regression model cox cox snell relates assuming jx fi fi exp gammau denotes logistic function 
unknown regression coefficients fi estimated data directly interpretable log odds ratios terms exp fi odds ratios 
logistic model non linear regression model special case generalized linear model mccullagh nelder yjx fi ar yjx fi gamma fi logit link fi fi fi fi setting fi fi fi fi adding vector estimation usually maximum likelihood principle maximizing log likelihood function fi log fi gamma log gamma fi denote observed data th individual maximizing fi equivalent minimization kullback leibler distance kullback leibler convention delta log log fi gamma log gamma gamma fi written gamma log gamma jy gamma fi interpretation distance fi obvious 
maximum likelihood estimate fi obtained solving likelihood equations case logistic regression model read fi fi gamma fi second derivatives log likelihood function obtained fi fi fi gamma fi gamma fi denoting fi vector derivatives fi matrix second derivatives fi newton raphson method commonly finding maximum likelihood estimate fi written fi fi gamma fi gamma fi fi appropriate starting value 
noted logistic regression model newton raphson method method scoring known gauss newton method fi replaced expected value coincide cf 
schumacher 
individual covariate vector probability jx predicted fi fi fi terms simply referred predictions feed forward neural networks logistic perceptron back propagation simplest feed forward neural network called logistic perceptron displayed 
perceptron consists input units delta delta delta delta delta delta delta delta delta theta theta theta theta theta theta theta theta theta theta theta theta theta theta delta delta delta delta delta delta inputs weights output logistic perceptron constant input equal output unit 
input values weighted weights sum weighted inputs usually transformed logistic function 
output value written function input values weights denotes vector inputs vector weights respectively 
learning framework means observations individuals weights determined called energy learning function gamma minimized 
simply squares criterion finding weights minimizing obtain estimating equations gamma gamma solving called back propagation method proposed rumelhart 
method precisely referred squares back propagation ls bp defined gamma suitable starting value 
positive constant called learning rate easily recognized known method steepest descent constant step lengths 
nonlinear model underlying logistic perceptron identical logistic regression model fi weights equal regression coefficients fi interpreted way 
alternative energy function kullback leibler distance proposed barron barron hinton fogel reads terms logistic perceptron log gamma log gamma gamma due connection maximum likelihood principle back propagation method called maximum likelihood back propagation written gamma appropriate starting value learning rate 
vector derivatives denoted components gamma leads estimating equations identical likelihood equations maximum likelihood back propagation method method known steepest descent different iteration procedure newton raphson method giving differences gamma equal weights 
having obtained weights minimize prediction individual covariate vector similar logistic regression model 
estimating ls bp difference observed target value predicted value weighted variance gamma inverse variance 
statistical theory fahrmeir kaufmann chamberlain tells solving equation optimal interested estimating fi respectively highest accuracy 
weighting deviance done may considered questionable statistical point view 
argue suitable classification decision greater 
case sensible give deviations gamma values close weight estimation process 
extension logistic perceptron output units obvious way extend logistic perceptron consider output units 
neural network displayed characterized weights ik leading output values ik statistical terms equivalent logistic regression model bock engel categorical outcome delta delta delta delta delta delta delta delta delta gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma theta theta theta theta theta theta theta theta theta theta theta theta theta theta omega omega omega omega omega omega omega omega omega omega omega omega omega omega omega oe gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma phi phi phi phi phi phi phi phi phi phi phi phi phi phi phi phi phi phi phi phi delta delta delta delta delta delta delta delta delta bm delta delta delta delta delta delta delta delta delta ik inputs weights outputs logistic perceptron output units variable values kg 
model defined kjx weights directly interpreted regression coefficients ml bp leads maximum likelihood estimates coefficients 
noted impose ordering kg 
intended reached constraints weights 
model known proportional odds model ordered responses mccullagh anderson 
networks hidden layer extension logistic perceptron consists introducing additional layer units usually called hidden layer input output units 
units hidden layer called hidden units consider case hidden unit output unit schematically displayed 
input units connected hidden unit having weights hidden unit connected output unit having weight additional connection skip layer weight drawn constant input output unit 
output obtained function input values delta summarizes network seen rescaled version logistic regression model shown part ii logistic regression model approximated 
function gamma 
delta monotone essential extension logistic perceptron reached adding hidden unit 
ml bp straightforward generalization minimizing kullback leibler distance explicit formulae ripley 
unit hidden layer consider units layer 
output obtained function input values delta ij course combined allow output unit 
extensions grouped survival data articles application feed forward neural nets survival data discussed kappen delta delta delta delta delta delta delta delta delta gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma theta theta theta theta theta theta theta theta theta theta theta theta theta theta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta inputs hidden unit output feed forward neural network hidden unit 
fairly straightforward consider grouped version cox regression model cox framework 
denoting survival time random variable time interval gamma model specified conditional probabilities jt gamma fi fi ik 
original proposal cox models grouped survival data obtained link functions prentice 
neural network corresponding logistic perceptron output units displayed output th output unit corresponding conditional probability dying th time interval data th individual consists vector regressor variables inputs vector kn indicator individual dying interval number intervals individual observed 
kn gamma zero kn equal individual died kn equal censored 
situation implies network randomly varying number output nodes time intervals individual risk 
standard problem survival analysis cf 
prentice easily accomplished slight modification energy function equivalently minus likelihood function 
particular setting fi fi ik summarizes fi fi fi fi ik obtain kn gamma log gamma jy gamma written sum output units introducing indicator individual risk th time interval usually done survival analysis andersen 
proportional hazards assumption implemented constraints fi ik fi parameters fi allowed differ proposals analysing grouped survival data means feed forward neural net 

clark network output unit number time interval additional input 
consider unconditional survival probability dying conditional output 
underlying model reads jx fi fi fi delta parametrization ensures monotonicity survival probabilities implies stringent unusual shape survival distribution case covariates considered reduces fi fi delta exp gamma fi fi delta obviously survival probabilities depend length time intervals strange undesirable feature 
additional hidden layer included straightforward extension having features summarized 
applications neural nets survival data discussed 
kappen simon 
practical problems neural networks learning rate local minima inherent back propagation algorithm choice learning rate section show practical applications fine tuning necessary 
different algorithms newton raphson gauss newton method learning rate automatically implemented changed local behaviour energy learning function 
constant learning rate implies difficulties 
having small learning rate chance finding minima escaping local minimum poor despite iterations 
large learning rate fair chance escaping local minima going direction global minimum algorithm unstable global minimum 
course relevant neural networks hidden layer energy function generally expected unimodal 
situations careful alteration learning rate certain range repetition minimization process different starting values required avoid occurrence local minima 
network selection weight decay application neural networks data requires specification network 
statistical terms known model selection term concentrating non linear regression model implicitly defined neural net 
aspect model selection concerned choice variables entered model terms neural networks input nodes 
principle done similar way different strategies variable selection cf 
miller adding removing input nodes network comparing values learning function obtained different settings 
important question units hidden layer needed hidden layer needed 
practically investigated adding removing units hidden layer 
aspects differences respect complexity networks number parameters regression coefficients weights taken account 
kullback leibler distance energy learning function proposals complexity regularization barron optimal network selection fogel variants akaike information criterion akaike 
practice add number parameters kullback leibler distance comparison networks different number input nodes different number units hidden layer 
theoretical discussion problem see amari 
network selected danger substantial overfitting data especially networks hidden layer 
proposals avoid overfitting popular weight decay ripley 
precisely corresponds adding term delta ij energy function neural network specified 
course specifications weight decay possible see weigend 
ripley discussion proposals relation ridge regression bayesian statistical methods 
framework standard form weight decay requires choice decay parameter sequel see choice fine tuning necessary 
software aspects intend give overview existing software usable neural nets 
examine wide area available tools simply picked packages purpose 
special software neural nets gives flexibility user friendly interface allowing specifications network involved calculations necessary refinements done interactive mode 
usually back propagation algorithm variants network aspects placed foreground 
modifications necessary statistical point view may difficult implement systems coming experts informatics 
second type relies existing statistical packages plus statistical sciences sas sas institute concentrating non linear regression models implicitly defined neural net 
general minimization algorithms newton raphson gauss newton method implemented 
algorithms perform efficiently hand possibilities building handling neural nets poor 
stuttgart neural network simulator version representative type software products modifications logistic activation function bias means modeling intercept explicitly standard squares learning function ls bp user written learning function kullback leibler distance added ml bp 
representative second type ripley nnet ripley available statlib library 
plus software situations network specifications contained unit hidden layer 
calculations done sun sparc station solaris 
examples deep inspiration constriction fingers known example finney table original data finney 
data consist binary responses denoting presence absence constriction skin fingers inspiration volume air average inspiration rate data extensively illustrative purposes see pregibon fahrmeir 
step start analysing data considering covariate log consider model jx fi fi results logistic regression analysis sas proc logistic sas institute upper part table 
show marginally significant effect logarithm inspiration rate probability presence constriction skin fingers 
perceptron input units log regression model neural net 
ml bp know maximizing log likelihood function equivalent minimizing 
reach identical results numerical problems occur 
setting learning rate iterations ml bp delivers weights gamma kullback leibler distance 
figures sufficient agreement parameter estimates minus log likelihood gammal smaller learning rate needed 
expected ls bp produce similar result data set observations minimization squares energy function minimization kullback leibler distance 
weights calculated ls bp gamma far away values obtained maximumlikelihood method 
regression curves displayed show clearly methods lead predicted response probabilities 
variable estimated standard value regression error coefficient intercept log intercept log log table results logistic regression analyses constriction data model upper part model lower part adequate analysis data requires second covariate consider logistic regression model jx fi fi fi log log respectively 
results analysis lower part table covariates show significant influence probability constriction skin fingers 
estimated response probabilities obtained ls bp observed responses constriction data model scatter plot displayed shows data represent extreme situation observations prevent groups defined respectively separated nearly straight line 
perceptron input units log log give similar results ml bp 
nearly achieved gamma back propagation algorithm required changes learning rate initial value involving approximately iterations get value kullback leibler distance comparable minus log likelihood gamma 
learning process graphically displayed ml bp ls bp 
algorithm continues reduce quadratic error 
absolute values weights increase continuously leading steep regression function border observations 
observations sitting marked circle add amount maximally squares energy function misclassification ml bp increase kullback leibler distance infinity 
weights obtained iterations gamma leading squares energy function 
adding hidden layer unit net absolute values weights drastically increased leading kullback leibler distance 
achieved minimizing kullback leibler distance gauss newton type algorithm essentially maximum likelihood estimation nnet ripley iterations 
ml bp needed delta iterations reach comparable value kullback leibler distance 
network predictions observations set observations exception marked observations see predictions set see 
comparing kullback leibler distances value added parameters involved logistic perceptron 
units hidden layer possible trials predict observations correctly observations observations 
maximum likelihood estimation nnet performed iterations implemented needed iterations 
scatter plot constriction data observed responses influential observations marked phi learning process logistic perceptron model ls bp ml bp estimated response probabilities constriction data feed forward neural net hidden unit weight decay different choices decay parameter predictions logistic regression model baseline 
trials algorithms stopped local minimum leading qualitatively similar results obtained unit hidden layer 
tool preventing overfitting weight decay outlined section 
added term proportional sum squared weights kullback leibler distance penalizing large weights 
neural net unit hidden layer tried choices decay parameter shows influence predicted response probabilities results logistic regression analysis 
choice corresponds case weight decay described 
choice decay parameter associated tendency overfitting 
value leads kullback leibler distance obtained perceptron hidden layer gives similar predictions 
choice leads value kullback leibler distance indicating poor fit model seen 
measurements diagnosis breast cancer study performed examine role noninvasive measurements colour doppler flow signals differentiation benign malignant breast tumors 
approximately years women enrolled study measurements patient characteristics recorded 
verification yielded benign tumors malignant tumors final diagnosis 
preliminary logistic regression analysis covariates identified marginally significant 
strong collinearity lead grossly inflated variances standard errors 
known difficulties added numerical deficiencies back propagation algorithm admit get similar results ml bp logistic perceptron 
second digits weights estimated regression coefficients logistic regression model 
decided consider covariates age number arteries tumor number arteries breast ac respectively 
results reduced logistic regression model obtained backward elimination procedure table 
interesting note log likelihood nearly identical obtained model covariates included problems collinearity longer 
variable estimated standard value regression error coefficient intercept age log log ac table results logistic regression analysis breast tumor data logistic perceptron input units age log log ac ml bp yielded similar results carefully altering learning rate iterations 
kullback leibler distance weights obtained gamma gamma 
behaved situation covariates sample size large insure consistency weights obtained ls bp weights gamma gamma squares energy function qualitatively similar results achieved ls bp compared ml bp maximum likelihood estimation 
results logistic regression compared classification developed 
cart method breiman feedforward neural networks hidden layer increasing number hidden units 
comparison sensitivity specificity percentage correct classifications knowing quantities optimistic estimated data 
results summarized table classification rules cutpoint percentage method sensitivity specificity correct classifications logistic regression cart nn nn nn nn nn nn nn nn nn nn table results classification rules logistic regression cart feed forward neural networks hidden units nn breast tumor data predictions 
seen diagnostic ability classification rules logistic regression cart method nearly 
hand increasing number hidden units leads perfect classification benign malignant tumors indication inherently substantial overfitting observed data 
discussion compared logistic regression model feedforward neural networks 
seen logistic perceptron simple neural net equivalent logistic regression model weights neural net interpreted regression coefficients corresponding logistic model 
method estimation different long squares criterion energy function 
statistical point view desirable kullback leibler distance entropy criterion energy function 
case estimating equations identical likelihood equations lead estimates established properties 
neural networks back propagation method standard algorithm minimizing energy function equivalently solving estimating equations 
shown method known steepest descent certain drawbacks see cheng titterington 
understandable having neural net simplified model human brain mind 
prepared accept neural net special non linear regression model back propagation longer seen essential ingredient methods newton raphson gauss newton algorithm preferred 
avoids considerations choice learning rate inherent back propagation method 
obvious extensions logistic perceptron allowing output unit having units hidden layer 
shown feed forward neural nets non linear regression models natural counterparts principle handled way logistic perceptron 
difference far weights neural net generally interpreted simple way logistic perceptron 
extensions proposed grouped survival data deserve special 
danger naive applications neural nets survival data ignore developments concerning statistical methods survival analysis past years 
sign posting developments area 
neural nets unit hidden layer correspond large number parameters non linear regression models flexible carry danger substantial overfitting 
shown examples partially compensated weight decay 
implies proper choice decay parameter 
problems comparison neural nets logistic regression models involving quadratic interaction terms investigated detail part ii 
concentrated comparison neural nets logistic regression models comparison modern non parametric regression methods 
reasons applications medical field analysis logistic regression model standard statistical approach 
secondly felt modern non parametric techniques deficit respect full understanding statistical properties 
second example seen standard logistic regression techniques yield classification rule high sensitivity specificity diagnosis breast cancer results cart analysis similar 
diagnostic accuracy slightly improved neural net number units hidden layer substantially increased 
number additional parameters corresponding non linear regression model implies risk substantial overfitting observed data taken account 
case single reporting neural net successfully applied decision making diagnosis breast cancer wu interesting comparison results obtained standard statistical techniques 
sufficient mention need comparisons tu done results explicitly 
question circumstances neural nets preferred statistical techniques 
tremendous discussion point ranging statements main advantage neurocomputing statistics delta delta delta neurocomputing ability draw sources inspiration delta delta delta methods exploited workers statistics hecht nielsen neural networks statistics 
white types artificial neural networks merely re inventions known statistical methods implemented inefficient algorithms sas institute 
intended give definite answer question extreme statements 
hope jointly help clarify role neural networks developments 
colleagues rudolf jurgen schulte fruitful discussions series seminars statistical aspects neural networks 
dr helmut permission data illustrative purposes 
akaike 
fitting autoregressive models prediction 
annals institute statistical mathematics 
amari 
mathematical methods neurocomputing 

eds chaos networks statistical probabilistic aspects 
london chapman hall 
andersen gill 
statistical methods counting processes springer series statistics new york springer verlag 
anderson 
regression ordered categorical variables discussion 
journal royal statistical society 
barron 
complexity regularization application artificial neural networks 

ed nonparametric functional estimation related topics 
amsterdam kluwer academic publishers 
barron barron 
statistical learning networks unifying view 
wegman 
ed computing science statistics proceedings th symposium interface 
washington american statistical association 

artificial neural network diagnosis myocardial 
annals internal medicine bock 
multivariate statistical methods behavioral research new york mcgraw hill 
breiman friedman olshen stone 
classification regression trees monterey wadsworth brooks cole 
chamberlain 
asymptotic efficiency estimation conditional moment restrictions 
journal econometrics 
cheng titterington 
neural networks review statistical perspective discussion 
statistical science 
cherkassky 
statistical neural network techniques nonparametric regression 
cheeseman selecting models data ai statistics iv 
new york springer 
cox 
analysis binary data london methuen 
cox 
regression models life tables discussion 
journal royal statistical society 
cox snell 
analysis binary data nd edition london chapman hall 
de de wet 
neural networks 
south african statistical journal 

correspondence models binary regression analysis survival analysis 
international statistical review 
engel 
logistic regression 
statistica 
fahrmeir kaufmann 
consistency asymptotic normality maximum likelihood estimator generalized linear models 
annals statistics 
fahrmeir 
multivariate statistical modelling generalized linear models new york springer 
simon 
neural network survival data 
statistics medicine 
finney 
estimation individual records relationship dose response 
biometrika 
finney 
probit analysis third edition cambridge cambridge university press 
fogel 
information criterion optimal neural network selection 
ieee transactions neural networks september 
friedman 
multivariate adaptive regression splines discussion annals statistics 
gallinari fogelman soulie 
relations discriminant analysis multilayer perceptrons 
neural networks 
geman bienenstock doursat 
neural networks bias variance dilemma 
neural computation 
hastie tibshirani 
generalized additive models london chapman hall 
hecht nielsen 
neurocomputing reading ma addison wesley 
hertz krogh palmer 
theory neural computation redwood city addison wesley publishing 
hinton 
connectionist learning procedures 
artificial intelligence 

applied logistic regression new york john wiley 
prentice 
statistical analysis failure time data new york wiley 
kappen 
neural network analysis predict treatment outcome 
annals oncology suppl 
barker pascal nagel 
evaluation neural network classifier pet scans normal alzheimer disease subjects 
journal nuclear medicine 

neural networks architectures learning performance 
bock richter 
eds information systems data analysis 
berlin heidelberg springer 
kullback leibler 
information sufficiency 
annals mathematical statistics 
andersen andersen 
survival analysis neural nets 
statistics medicine 
mccullagh 
regression models ordinal data discussion 
journal royal statistical society 
mccullagh nelder 
generalized linear models second edition 
new york chapman hall 
mclachlan 
discriminant analysis statistical pattern recognition new york wiley 
michie spiegelhalter taylor 
machine learning neural statistical classification new york ellis horwood 
miller 
subset selection regression london chapman hall 


absolute deviations fits generalized linear models 
biometrika 
murtagh 
neural networks related massively parallel methods statistics short overview 
international statistical review 
martin 
neural network approach assess myocardial 
lun 
eds medinfo 
amsterdam elsevier pregibon 
resistant fits commonly logistic models medical applications 
biometrics 
prentice 
regression analysis grouped survival data application breast cancer data 
biometrics 
clark 
practical application neural network analysis predicting outcome individual breast cancer patients 
breast cancer research treatment 
clark owens 
demonstration breast cancer recurrence predicted neural network analysis 
breast cancer research treatment 
reddy 
neural networks classification eeg signals 
lun 
eds medinfo 
amsterdam elsevier ripley 
software neural networks technical report university oxford 
ripley 
statistical aspects neural networks 

eds chaos networks statistical probabilistic aspects 
london chapman hall 
ripley 
neural networks flexible regression determination 
mardia 
ed statistics images 

ripley 
neural networks related methods classification discussion 
journal royal statistical society 
rumelhart hinton williams 
learning representations back propagation errors 
nature 
sas institute 
sas stat user guide version fourth edition 
cary sas institute sas institute 
information neural network macros 

development diagnostic rules color doppler flow signals differentiation benign malignant breast tumors preprint 
schumacher 
point estimation response models 
journal 

neural network approach predicting admission psychiatric emergency room 
medical decision making 

maximum likelihood training connectionist models comparison squares back propagation logistic sion proceedings th annual symposium computer applications medical care 
statistical sciences 
plus data analysis software version seattle statistical sciences 

elements statistical computing numerical computation new york london chapman hall 
tu 
neural network predictive instrument length stay intensive care unit cardiac surgery 
computers biomedical research 
ro ner schumacher 
neural networks logistic regression part ii volume 
weigend rumelhart huberman 
generalization weight elimination application forecasting advances neural information processing iii morgan kaufman 
weinstein kohn rubinstein 
neural computing cancer drug development predicting mechanism action 
science 
white 
artificial neural networks approximation learning theory oxford basil blackwell 
wu doi schmidt metz 
artificial neural networks mammography application decision making diagnosis breast cancer 
radiology 
vogt 
stuttgart neural network simulator user manual 
version report 
institute parallel distributed high performance systems university stuttgart 
