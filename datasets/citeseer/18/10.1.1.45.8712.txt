mixtures principal component analyzers michael tipping christopher bishop neural computing research group dept computer science applied mathematics aston university birmingham uk principal component analysis pca ubiquitous technique data analysis effective application restricted global linear character 
global nonlinear variants pca proposed alternative paradigm capture data nonlinearity mixture local pca models 
existing techniques limited absence probabilistic formalism appropriate likelihood measure require arbitrary choice implementation strategy 
shows pca derived maximum likelihood procedure specialisation factor analysis 
extended develop defined mixture model principal component analyzers expectation maximisation algorithm estimating model parameters 
principal component analysis pca popular technique dimension reduction applications data compression image analysis visualization time series prediction example 
set observed dimensional data points ft ng principal axes qg orthonormal axes retained variance projection maximal 
shown dimensional orthonormal vectors dominant eigenvectors largest associated eigenvalues covariance matrix gamma gamma sw vector gamma reduced dimensional representation observed vector optimal reconstruction observed vector terms squared error wx 
limiting disadvantage pca defined absence associated probability generative model 
deriving pca perspective density estimation offers number important advantages ffl log likelihood measure permits comparison density estimation techniques 
ffl bayesian inference methods may applied model comparison combining likelihood prior parameters 
ffl pca model class conditional densities classification problem example posterior probabilities class membership may computed 
ffl probability density function gives indication novelty data vector 
ffl single pca model may extended mixture models 
final advantage considerable significance 
pca defines linear projection data scope application necessarily somewhat limited 
realistic datasets components may required capture salient structure 
naturally motivated various developments global nonlinear principal component analysis effort retain greater proportion variance fewer components examples include principal curves multilayer autoassociative neural networks generative topographic mapping 
alternative paradigm model nonlinear structure collection mixture local linear sub models 
philosophy attractive motivating example mixture experts technique regression problems approach adopted context pca 
local model implementation pca implies integration procedures partitioning data space distinct regions estimation principal axes region 
question combine elements problematic optimal partitioning depends current set axes optimal axes determined current partition 
choice method interleaving stages necessity somewhat arbitrary 
expected reconstruction error distortion measure batch vector quantisation vq step followed local pca cell defined vq 
similar approach followed interestingly soft version algorithm data point assigned ex region space principal component analyzer responsibility shared analyzers 
nj responsibility jth analyzer reconstructing data point nj exp gammae oe exp gammae oe reconstruction cost jth analyzer 
pseudo em expectation maximisation algorithm derived fitting model unfortunately requires noise parameter oe value chosen arbitrarily 
major benefit deriving probabilistic model pca possible formulate model comprising mixture principal component analyzers principled manner 
shown principal subspace set data vectors obtained maximisation likelihood function 
extend key result mixture modelling context outline efficient em algorithm estimating model parameters 
necessary combination partitioning principal axes estimation occurs automatically 
furthermore seen covariance responsibility weighting soft version hinton implicit element algorithm arbitrary parameter oe defined ingredient model re estimated em procedure 
section describe concept latent variable models indicate particular choice noise model statistical technique factor analysis adapted extract principal components dataset 
efficient em algorithm determining parameters mixture principal component analyzers example operation illustrated synthetic data set 
latent variable models latent variable model desired describe set dimensional observed data vectors ft terms set dimensional latent variables fx model delta delta function random variable parameters independent noise process 
generally latent variables offer parsimonious description data 
model may termed generative data vectors may generated sampling distributions applying 
standard factor analysis model standard normal factor analysis model linear wx dimensional common factors psi psi diagonal specific unique factors 
theta parameter matrix contains factor loadings constant represents mean data general data assumed ignored formulation normally distributed model covariance psi ww key motivation model observed variables conditionally independent underlying factors latent variables reduced dimensional distribution intended model dependencies observed variables specific factors model independent noise 
contrast pca treats inter variable dependencies independent noise identically 
equation implies probability distribution space tjx gammad gamma exp ae gamma gamma wx psi gamma gamma wx oe gaussian prior latent variables defined gammaq exp ae gamma oe gammad jcj gamma exp ae gamma gamma oe tjx dx 
bayes rule posterior distribution latent variables observed may calculated xjt gammaq jpj gamma exp ae gamma gamma vt gamma gamma vt oe posterior mean covariance determined gamma gamma gamma log likelihood data standard factor model ln gamma nd ln gamma ln jcj gamma tr theta gamma sample covariance matrix observed em algorithm exists maximising measure 
isotropic noise model key understanding bridge factor analysis pca realise distribution specific factors assumed isotropic oe may substituted psi covariance model observed data oe ww maximum likelihood solution columns span principal subspace data 
importantly enables derive pca perspective density model 
prove central result considering derivative respect nc gamma gamma gamma may obtained standard matrix results 
considering singular value decomposition weight matrix lv theta matrix columns orthonormal diagonal orthogonal matrix may shown non zero solutions gamma oe columns eigenvectors diagonal matrix elements corresponding eigenvalues arbitrary orthogonal matrix 
details derivation shown global maximum likelihood occurs comprises principal eigenvectors furthermore combinations eigenvectors saddle points likelihood surface 
maximum likelihood pca result properties simple extension em formulation parameter estimation standard factor model obtain principal component projection maximising likelihood function 
algorithm converged determined principal subspace may simply calculated seen application bayes rule effectively performs analogue dimension reducing projection pca posterior mean projection hx gamma oe gamma orthogonal projection standard pca recovered 
density model singular undefined 
conversely oe projection manifold skewed origin result prior orthogonal projection data point may optimally reconstructed latent variable computation skewing account 
section develop algorithm fitting mixtures principal component analyzers data probabilistic pca model 
mixtures analyzers adopting isotropic noise model possible develop maximum likelihood formulation mixture principal component analyzers 
model log likelihood observing data set assumed ln ln ji ji single pca model mixing proportion 
note separate mean vector associated component mixture parameters oe addition independent latent variables associated component 
related model exploited data visualization similar approach standard factor analysis diagonal psi noise model employed handwritten digit recognition implement pca 
em algorithm maximise likelihood employ iterative expectation maximisation em ap proach 
consider latent variables fx ni missing data introduce missing binary variables fz ni indicate mixture component responsible generating data point values missing variables known estimation model parameters straightforward standard techniques 
know mixture component value ni responsible generation 
know joint distribution observed latent variables fx ni fz ni calculate expectation corresponding complete data log likelihood 
step em algorithm expectation calculated respect posterior distributions ni ni observed current parameter values computed 
step new parameter values new new new oe new determined maximize expected complete data log likelihood 
procedure guaranteed increase likelihood interested local maximum 
fact adopt stage procedure corresponding generalised em algorithm stage comprises step step 
firstly calculate expectation respect posterior distribution ni update parameters new new 
second stage calculate expectation respect posterior distribution ni update remaining parameters new oe new approach advantage expectation step computed updated values new results increase convergence speed simplification algorithm 
prescription implementation em algorithm 
mathematical details concerning derivation available 
stage step mixture component expectation respect ni results calculation responsibility ni component generating data point ni ji step new ni new ni ni stage mixture component data point posterior mean projection characterised statistics hx ni oe gamma gamma new hx ni ni oe oe gamma hx ni ni statistics calculated step stage exploiting updated values new computed stage 
leads step updates new ni gamma new hx theta ni hx gamma oe new new theta ni tr gamma new gamma new gamma gamma new hx new new hx new implementation sight determination necessary step updates oe equations appears computationally demanding 
expanding hx hx algebraic manipulation reveals necessary updates expressed terms previously calculated responsibilities ni quantities new ni gamma new gamma new oe matrix equation may clearly interpreted responsibility weighted covariance matrix intrinsic quantity computed formal step update equations 
parameter updates oe simplify new new oe gamma gamma oe new tr theta gamma gamma new new note instance equation old value weights second instance value calculated equation 
necessary parameters mixture pca model may determined equations followed equations 
iteration sequence guaranteed find local maximum likelihood 
maximum likelihood solution model may interpreted combination local pca models show similar fashion single pca model stationary values new occur new gamma oe 
case eigenvectors values local responsibility weighted covariance matrix note quantity emerges inherent quantity em algorithm ad hoc feature 
result new oe new may calculated analytically possible fixed responsibilities ni find maximum complete data likelihood respect oe step iterative re estimation scheme 
approach allow earlier algorithms viewed approximation maximum likelihood procedure 
procedure requires determination eigenvectors theta matrix step em scheme necessitates inversion theta matrix 
computations scale cube matrix dimension eigenvector decomposition may give better convergence step step update may require significantly computations em updates particularly furthermore computation wasted early stages determining model clustering data set determination sub optimal 
experiment demonstration operation algorithm generated synthetic dataset comprising data points sampled uniformly surface hemisphere additive gaussian noise 
shows data 
synthetic data 
mixture principal component analyzers fitted data em algorithm 
probabilistic formalism exists generative model data emphasise generating set data points time sampled generative model 
data points illustrated may seen occupy region true data manifold 
shows alternative projection sampled data vectors generated individual component model highlighted darker plotting 
data sampled trained generative model 
single component highlighted 
modelling complexity data mixture simple linear models attractive paradigm offers computational algorithmic advantages increased ease interpretability 
mixtures principal component analyzers developed applications algorithms earlier may effective require arbitrary implementational decisions selection parameters 
formulated mixture pca models terms solution maximum likelihood problem derived em algorithm estimating parameters 
addition concomitant advantages defining density function modelling process potential computational advantages eigenvector decomposition covariance matrix required 
furthermore partitioning principal axis estimation steps integrated automatically features earlier algorithms specifically soft clustering responsibility weighted covariance matrices inherent components em update equations 
supported epsrc contract gr neural networks visualization high dimensional data 
bishop 
neural networks pattern recognition 
clarendon press oxford 
bishop williams 
gtm generative topographic mapping 
accepted publication neural computation 
bregler omohundro 
nonlinear image interpolation manifold learning 
tesauro touretzky leen editors advances neural information processing systems pages 
cambridge mass mit press 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
hastie stuetzle 
principal curves 
journal american statistical association 
hinton dayan revow 
modelling manifolds images handwritten digits 
ieee transactions neural networks 
hinton revow dayan 
recognizing handwritten digits mixtures linear models 
tesauro touretzky leen editors advances neural information processing systems pages 
cambridge mass mit press 
jolliffe 
principal component analysis 
springer verlag new york 
jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 
leen 
fast nonlinear dimension reduction 
cowan tesauro alspector editors advances neural information processing systems pages 
san francisco ca morgan kaufmann 
kramer 
nonlinear principal component analysis autoassociative neural networks 
aiche journal 
rubin thayer 
em algorithms ml factor analysis 
psychometrika 
tipping bishop 
hierarchical latent variable model data visualization 
technical report ncrg neural computing research group aston university aston street birmingham uk 
tipping bishop 
mixtures principal component analyzers 
technical report ncrg neural computing research group aston university aston street birmingham uk 
