incremental learning support vector machines ahmed huan liu kah kay sung program research intelligent systems pris school computing national university singapore singapore comp nus edu sg real world databases increase size need scale inductive learning algorithms handle training data 
incremental learning techniques possible solution scalability problem data processed parts result combined memory 
support vector machines svms worked batch mode learning shown impressive performance practical applications 
nice properties summarizing data form support vectors 
suggests able incorporate certain standard frameworks incremental learning 
proposes framework incremental learning svms evaluates ectiveness set proposed goodness criteria standard machine learning benchmark datasets 
example learning attractive framework extracting knowledge empirical data goal generalizing new input patterns 
realworld processes formulated way classi cation task may solved example learning methods 
developing classi ers learning methods training data reduce prediction error learning process get computationally intractable 
issue evident today complex classi cation problems waiting solved domains scienti elds sales logs social economic nancial studies large amounts data available fayyad researchers machine learning data mining community catlett provost kolluri trying scale classical inductive learning algorithms handle extremely large data sets 
ideally desirable able consider examples simultaneously get best possible estimate class distribution 
hand training set large examples loaded memory go 
approach overcome constraint train classi er incremental learning technique subsets data considered time results subsequently combined 
support vector machines shown results batch learning training batch data 
nice properties summarizing data achieve preserving support vectors 
suggests extend incremental learning framework shall elaborate section 
aim 
incremental learning procedure svms 

propose criteria evaluating goodness incremental learning procedure 

empirically demonstrate proposed incremental svm procedure performs criteria standard benchmarking datasets 
svm incremental learning brie review key ideas support vector machines svms appealing adapted incremental form learning 
support vector algorithm support vector machines svms general class statistical learning architectures perform structural risk minimization nested set structure separating hyper planes vapnik classi cation problem data points svm training involves solving quadratic programming problem optimal solution gives rise decision function form sgn 
allow decision surfaces hyper planes rst non linearly transform set input subset subset subset subset 
support vectors support vectors target concept 
initial empty concept concept target support vectors final concept target concept incremental training procedure training vectors higher dimensional feature space map 
linear separation 
leads decision functions form sgn 
sgn 
known kernel 
notice training classi cation procedures involve computing high dimensional dot products form 

avoid computationally expensive operations consider classes reduce simple kernel functions small fraction coecients non zero 
corresponding pairs entries known support vectors output labels fully de ne decision function 
coecients preserved classi cation procedure 
remaining training examples contribute decision function may regarded redundant 
incremental training small fraction training examples support vectors vapnik support vector algorithm able summarize data space concise manner 
suggests partition huge database partition ts main memory incrementally train svm classi er partitions 
training preserve support vectors incremental step add training set step 
model obtained method similar obtained data train 
reason svm algorithm preserve essential class boundary information seen far compact form support vectors contribute appropriately deriving new concept iteration 
evaluate ectiveness incremental training compare performance incrementally trained model model trained data far batch 
speci cally need compare cases case rst case learned model previously seen data preserved form support vectors new information comes classi cation algorithm 
performance deteriorate keep discarding redundant examples successive incremental step new examples come 
case second save previously seen data new information arrives classi cation algorithm svm fare unknown data 
ectively batch learning data seen far 
evaluation discussion analyze situations experiment designed follows 
call training set tr test set te iteration training set split parts tr part containing mutually exclusive data tr 
types incremental learning carried 
trained svm tr took support vectors chosen tr call sv added tr ran svm training algorithm sv tr step trained machine classify te svs chosen training sv tr taken call sv tr added 
training testing done 
incremental step repeated tr 

svm trained tr trained machine classify examples te classi cation rate correct classi cation recorded 
tr added tr tr tr training 
trained machine classify te results recorded 
tr added tr tr data original training set training 
arbitrarily data partition tr incrementally learning large dataset data step bu er hold 
partition dataset dataset suits conditions 
steps repeated times 
time mutually exclusive examples test set te rest training set tr lines fold cross validation 
problem handled experiments class classi cation problem 
dataset examples 
attr 
australian diabetes german heart ionosphere liver disorders monks monks monks mushroom promoter gene sonar table datasets experiments 
dataset batch method incr 
method australian diabetes german heart ionosphere liver disorders monk monk monk mushroom promoter gene sonar average table final prediction accuracy batch incremental methods datasets carrying empirical study summarized table 
datasets standard machine learning community benchmarking purposes 
datasets obtained uci machine learning repository obtained sv light experiment 
rst step gives model obtained incremental training 
get model trained examples far tr tr tr batch 
incremental step comparing cases gives insight incremental method fares compared batch method 
shows plot cases various datasets learning progresses 
axis incremental step axis height bar denotes prediction accuracy models obtained corresponding incremental step 
kernel parameter setting brackets name dataset www ics uci edu mlearn mlrepository html www ai informatik ni dortmund de forschung verfahren svm light svm light eng html clear plots prediction accuracy incrementally trained svm method closely follows performance svm trained batch data 
observation important provides empirical evidence support vectors chosen svm algorithm able remember relevant parts data seen concise ective form 
table shows nal prediction accuracy incremental batch training methods training completed incremental steps 
table clearly shows negligible drop prediction accuracy support vector machines incrementally trained 
idea support vectors suf cient represent data space previously demonstrated clearly vapnik results shown batch training testing 
novelty result rst empirical evidence aware successive steps discarding redundant examples context incremental learning successively collected set support vectors remain representative set 
succinctness support vector representation aim incrementally train svm large dataset desirable selected support support vector set small possible 
investigate selected set support vectors really necessary set conducted set experiments 
experiments aim observe ect removing examples support vector set representativeness remaining support vectors 
achieve experiment conducted follows 
svm algorithm run dataset support vector set sv obtained 

svm trained obtained support vector set sv fold cross validation fold cross validation training set tr test set te obtained support vector set sv average prediction accuracy trained machine test set recorded 
ect second step examples support vector set sv form testing set te rest form training set tr 
folds cross validation di erent th support vectors missing training set 
support vectors necessary non redundant 
reasonable expect training svm part tr sv testing part te yield poor prediction accuracy 
table provides prediction accuracy various datasets experiment 
rst column shows percentage examples chosen support vectors step australian gaussian diabetes gaussian german gaussian incremental learning step australian data svs incremental learning step classification prediction accuracy diabetes data svs incremental learning step classification prediction accuracy german data svs heart gaussian gaussian liver disorders gaussian incremental learning step heart data svs incremental learning step classification prediction accuracy ionosphere data svs incremental learning step classification prediction accuracy liver disorder data svs monk gaussian monk gaussian monk gaussian incremental learning step monk data svs incremental learning step classification prediction accuracy monk data svs incremental learning step classification prediction accuracy monk data svs mushroom linear promoter gene linear sonar gaussian incremental learning step mushroom data svs incremental learning step classification prediction accuracy promoter gene data svs incremental learning step classification prediction accuracy sonar data svs concept drift results benchmark dataset exs 
accuracy accuracy selected data selected exs 
australian diabetes german heart ion 
liver 
monk monk monk 
gene sonar avg 
table results illustrating necessity selected samples experiment second column shows average prediction accuracy classi er fold cross validation carried dataset column shows prediction accuracy obtained step 
seen accuracy drops drastically training set created subset selected support vectors 
means removing small portion support vectors set sv ects representation capability remaining set 
turn implies support vector set chosen svm algorithm minimal set removing samples result loss vital information class distribution 
results clearly demonstrate set chosen support vectors forms necessary set retain structure original data space 
results combined results previous section provide clear evidence support vector machine summarizing data compact form selected support vectors form minimal set 
result properties svms readily incremental framework perform just incrementally trained 
related method considered similar spirit decomposition chunking techniques employed train svms osuna method di ers methods technique looks examples determine support vectors 
discarded vectors considered 
hand example osuna decomposition algorithm iterative method cycles training set times select support vectors 
method considered kind lossy approximation chunking methods 
consequently positive results show approximation performed signi cantly deteriorating performance algorithm 
started suggesting support vector machines adaptable incremental training provide reasons suggestions 
investigate demonstrate validity suggestion empirically shown support machines perform incremental training 
provide empirical evidence chosen support vector sets form minimal set 
positive results svms set study support vector sets eciently train kinds learning algorithms decision tree induction quinlan neural networks rumelhart aleksander morton instance learning aha cover hart attain goal scalable data mining 
aha aha albert 
instance learning algorithms 
machine learning 
aleksander morton aleksander morton 
neural computing 
chapman hall 
catlett catlett 
machine learning large databases 
phd thesis department computer science university sydney australia 
catlett catlett 
test ight 
proceedings eighth international workshop machine learning pages 
morgan kaufmann 
cover hart cover hart 
nearest neighbour pattern classi cation 
institute electrical electronics engineers transactions information theory 
fayyad fayyad smyth 
data mining knowledge discovery overview 
advances knowledge discovery data mining 
mit press 
osuna osuna freund girosi 
improved training algorithm support vector machines 
proceedings ieee amelia island fl 
provost kolluri provost kolluri 
survey methods scaling inductive learning algorithms 
technical report isl intelligent systems lab department computer science university pittsburgh 
quinlan quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
rumelhart rumelhart hinton williams 
learning internal representations error propagation 
rumelhart pdp research editors parallel distributed processing explorations microstructure cognition volume foundations pages 
mit press cambridge ma 
vapnik vapnik 
nature statistical learning theory 
springer verlag new york 
