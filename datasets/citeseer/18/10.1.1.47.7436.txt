ellard thomas freeze free algorithm process migration ellard thomas university washington stanford university campbell university thesis submitted partial fulfillment requirements degree doctor philosophy computer science graduate college university illinois urbana champaign urbana illinois freeze free algorithm process migration ellard thomas ph department computer science university illinois urbana champaign roy campbell advisor dynamic process migration moves running process new machine supports load sharing processor fault tolerance 
thesis introduces freeze free process migration algorithm uses techniques dramatically reduce overhead complexity dynamic process migration 
existing systems request response messages initiate process migrations transfer elements process state 
freeze free algorithm eliminates request response messages process migration latency period 
combined process control execution state message implicitly signals start process migration 
current stack page message implicitly tells new host resume execution 
old host combined process control execution state current code page current heap page current stack page new host delay latency period 
information reduced support process migration broad class processes 
second freeze free algorithm delivers critical pages page faults 
program counter identifies current code page stack pointer identifies current stack page 
heuristic identifies current heap page examining instruction stream 
system truncates top stack page portion currently 
third freeze free design separates process control communication state allows process migration message receipt proceed parallel 
new design effectively eliminates message freeze time plagued prior systems 
fourth freeze free design separates process control file state allows process resume execution new host system flushes data file server 
fifth freeze free algorithm partially initializes set data structures process migration time moves expensive operations critical path 
sixth freeze free design reorganizes data structures information object appears object 
drastically reduces cost extracting inserting state 
net result techniques reduction process migration latency time order magnitude simultaneously supporting processor fault tolerance effectively eliminating message freeze times 
furthermore latency cost change process size 
latency time ms kb page system ms iii kb page system ms kb system 
comparison sprite dynamic process migration takes ms 
thesis shows process migration latency costs small fraction demand page operations network 
analysis reveals potentially large savings process migration latency cross network demand paging 
thesis demonstrates negative impact increasing overhead system load sharing speedup experiments different overhead costs 
small overhead essential speedup 
combined process migration latency operations cross network demand pages add overhead test process execution time 
iv professor roy campbell asked better done earlier 
thesis answers question 
extend appreciation support research unconventional area 
professor serving prelim thesis committees 
professor michael loui serving prelim committee 
professor michael serving thesis committee helping avoid major scheduling problem 
members extended choices group provided useful support exchanged ideas willy liao see tan amitabh dave david lun xiao sane chris tin qian liu chen ashish david 
anda bonnie howard administrative support 
table contents chapter process migration problem scope thesis statement thesis outline background system availability system model autonomous system integrated system massively parallel processor system summary load indicators processor load measures obtaining load measures static vs dynamic load balancing time constraints task characteristics system loading migration delay load balancing strategies summary related programming language systems user level process migration rsh zhou ferrari butler remote execution monitor process server vi condor sunderam chang pms utopia summary kernel level process migration locus demos mp mos process suspend resume nest alonso emerald aix tcf mosix mess clouds galaxy emps birlix amoeba mach mdx charlotte accent sprite summary summary mechanism process migration protocol process control execution state address space messages files summary existing process migration algorithms total copy algorithm pre copy algorithm vii demand page algorithm file server algorithm summary performance improvement opportunities protocol address space messages file information object allocation process modularity summary freeze free algorithm algorithm steps process migration latency message handling page faults flushing summary performance measurements test system hardware operating system communication subsystem file system process subsystem application operating system interface process migration subsystem process migration performance process migration latency migrate steps migrate steps process migration latency period analysis latency period critical path extraction insertion virtual memory data copying communication cost demand page analysis load sharing summary viii algorithm comparison normalized performance demand page algorithm file server algorithm distributed virtual memory summary process migration latency time message freeze time modularity load sharing vs overhead fault tolerance opportunities improvement higher bandwidth network page transfer protocol communication protocol handling virtual memory opportunities added capabilities related areas summary bibliography vita ix list tables process migration systems example times message transfer performance process subsystem performance migrate kb page migrate kb page migrate kb page migrate messages kb page migrate messages kb page migrate messages kb page migrate object pool major activities critical path page fault overhead page faults second major activities demand page load sharing experiment results normalized comparison process migration latency comparative process migration latency message freeze times list figures comparison process migration latency times total copy algorithm pre copy algorithm demand page algorithm file server algorithm freeze free algorithm process migration resource usage time page size impact performance added overhead versus performance kb pages added overhead versus performance kb pages added overhead versus performance kb pages combined overhead versus performance xi chapter process migration moves process source machine destination machine provides ability share machines potentially improve performance 
process migration enhances processor fault tolerance providing capability move away failing machine 
unfortunately benefits process migration come price 
prior process migration systems add overhead computation migrated process overhead drastically limits load sharing benefits 
thesis presents new approach dramatically reduces overhead order magnitude 
research concentrates implement process migration mechanism low cost manner 
thesis uses new process migration system range additional artificial overhead demonstrate negative impact overhead load sharing system speedup 
chapter begins section delimits scope process migration problem tackled thesis 
second section declares thesis statement highlights major accomplishments thesis research 
final section outlines organization remainder thesis 
process migration problem scope sharing machines encompasses vast range issues far coverage thesis 
section defines scope thesis 
research targets improvement dynamic process migration transfer running processes homogeneous machines distributed system 
section defines component dynamic process migration issue environment 
thesis deals process migration distributed system distributed system defined collection multiple computer systems connected communication network shared memory 
collections workstations server machines represent typical distributed system 
mechanism moving processes processors shared memory multiprocessor differs process migration distributed system considered 
single network simultaneously supports wide variety computers 
transfer running program dissimilar machine encounters translation problems 
common differences character set integer format floating point format byte order word size instruction set register architecture 
dynamic process migration heterogeneous machines involves costly translations 
process migration heterogeneous machines proposed th high translation costs severely limit utility 
thesis deals solely process migration source destination machines having machine architecture 
purposes thesis homogeneous machines differ clock speed amount memory number type peripherals 
furthermore homogeneous machines differ portions machine hidden application processes kernel privileged instructions presence absence cache memory 
important element process environment operating system 
process uses operating system provided resources services 
dynamic process migration requires recreation state process operating system interface destination machine 
translating state process operating system interface costly possible operating systems different abstractions provide comparable services 
thesis deals solely process migration machines running operating system 
major existing process migration algorithms new process migration algorithm independent application programming language 
application process migrated regardless source programming language 
process migration algorithms deal strictly executable version process 
approaches executing process new host involve parts load balancing policy system determine transfer process transfer machine 
process migration mechanism system migrates selected process source machine destination machine 
remote execution process executes new machine 
load balancing policy issue large separate topic covered part process migration mechanism 
process migration mechanism add unnecessary overhead execution process remote host 
thesis cover issues involving remote process execution 
process migration occurs distinctly different situations static process migration system migrates process machine prior execution 
static process migration migrates relatively limited process environment information process execution request 
static process migration effective means moving distributed system 
dynamic process migration system migrates process machine process begun execution 
system migrate process environment plus large amount process state 
thesis concentrates solely dynamic process migration henceforth term process migration refers dynamic process migration 
important issues process migration minimization process transfer time 
time period issuance process migration request process ready execute called process migration latency time 
primary focus thesis minimizing process migration latency time 
message exchange important frequent activity distributed systems 
process migration successfully deal active messages arriving process migration 
prior process migration systems stopped message processing extended time period process migration period called message freeze time 
thesis demonstrates alternative 
current operating systems scatter information single process component subsystems 
term excision refers process extracting process related information operating system kernel term insertion refers reverse operation 
thesis statement mts theoretical study shows short process migration latency essential performance gains provides quantifiable target measure 
best current systems achieve needed speed 
thesis holds needed process migration latency time achievable 
thesis research demonstrates order magnitude improvement process migration latency time best existing systems sprite ms 
achieves ms process migration latency milliseconds thesis see section 
thesis presents new freeze free algorithm process migration 
freeze free algorithm achieves large performance improvement improvements areas 
freeze free algorithm reduces process migration latency effectively eliminates message freeze time supports processor fault tolerance 
freeze free algorithm minimizes information transfer process migration latency period shipping just combined process control execution state current code page current heap page current stack page 
process migration system transfer migrate broad class processes 
freeze free algorithm eliminates request response messages process migration latency period 
freeze free algorithm eliminates signal messages piggybacking signals data transfer messages 
program counter identifies current code page stack pointer identifies current stack page 
heuristic obtains current heap page scan instruction stream 
freeze free algorithm delivers critical pages page faults 
process control communication states permits process migration message receipt proceed parallel 
process control file state permits migrating process resume execution new host old host flushes file cache blocks 
freeze free algorithm places information object strictly object 
change greatly reduces cost extracting inserting state information 
freeze free algorithm partially initializes set data structures 
change moves costly operations critical path 
thesis demonstrates value low process migration overhead experiment measuring effects overhead load sharing system speedup 
low process migration overhead essential system speedup 
thesis outline thesis presents detailed study mechanics process migration demonstrates new effective process migration algorithm working implementation 
chapter presents benefits process migration examines principle constraints process migration explains process migration performance important 
chapter describes wide range process migration systems major contributions 
chapter provides overview principle issues involved process migration 
chapter describes major process migration algorithms milestones development field 
chapter identifies major outstanding limitations existing process migration systems freeze free algorithm overcomes limitations 
chapter presents new freeze free algorithm process migration shows overcomes existing bottlenecks 
chapter reports performance measurements new freeze free algorithm process migration 
chapter describes load sharing experiment dynamic process migration demonstrates utility dynamic process migration effects overhead system speedup 
chapter compares performance prior algorithms new freeze free algorithm speed algorithmic step comes implementation 
chapter summarizes accomplishments thesis research identifies related research areas 
chapter background original motivation desire improve distributed operating systems effectively harnessing processor power available distributed system 
research began examination load balancing policies 
review dozens load balancing strategies revealed process migration mechanism foundation load balancing existing process migration mechanisms costly 
consequence research focus shifted process migration mechanism 
major limitations existed 
theoretical study explored fast process migration latency order process migration effective results indicated current process migration latency times greatly exceeded time needed system load sharing speedup 
chapter examines environment process migration operates 
chapter begins section examining availability machines 
subsequent sections cover system models process migration load indicators static versus dynamic load balancing versus process migration time constraints load balancing strategies 
migration delay time constraint provided initial justification seeking major reduction process migration latency 
system availability distributed systems spreading 
steady reduction hardware costs led organizations placing machines desk 
humans constantly machines results considerable unused processor capacity 
theimer tlc reported third stations idle busiest times day 
nichols nic stated stations idle day idle night 
mutka livny studied microvax ii stations month period stations available outside conservative criteria time ml 
machine utilization rates continue drop 
litzkow livny ll reported machines idle 
douglis ousterhout stated machines idle 
krueger chawla kc reported average station capacity unused 
anderson report workstations available time 
latest results indicate machines available user 
net result lot computer power left unused wasted computing power expected 
process migration provides means harness idle processor power 
price pressures driving increases workstations 
larger sales volume smaller systems spreads costs units 
reduces unit costs drives profits motivates investments 
result price performance ratio smaller systems increasing faster larger systems 
example workstation price performance increasing year supercomputer price performance increasing year 
result increasing amounts shift expensive workstations distributed systems 
system model review process migration systems load balancing strategies revealed conceptual model computer system 
section identifies system models guide policy process migration load balancing 
autonomous system autonomous system model treats distributed system collection independent computer systems secondary characteristic having communications links 
autonomous system belongs user 
autonomous system available owner 
context owner person primary control machine necessarily legal title 
owner resumes machine system evicts processes originating outside machine process migration 
processes normally execute originating machine 
special command causes processes executed remote machines 
condor ll llm prototype model autonomous system load balancing 
condor default settings consider machine idle machine cpu load average tasks keyboard mouse activity minutes 
conservative measures report machines idle condor 
condor manages queue jobs remote execution idle machines algorithm allocate cpu resources fair way 
course days condor provided cpu days completed jobs 
integrated system integrated system model treats distributed system unified system 
model closer time sharing model uni processor system 
integrated system model expands time sharing model include multiple cpu 
integrated system views cpu storage devices devices set resources utilization maximized accomplishing greatest amount useful 
integrated system hardware may set stations autonomous system policy different 
integrated system concept individual owns station 
system migrates processes bring processor loads closer balance 
amoeba tan chapter prototype integrated system model 
amoeba manages machines processor pool 
amoeba run server monitors load processors 
new program executed run server uses heuristics select processor causes program executed 
run server balances load processors processor pool 
massively parallel processor system massively parallel processor mpp system model views massively parallel processor machine primarily vehicle running large application multiple threads 
mpp model application controls items systems controlled operating system 
specifically application control load balancing processing units mpp differs general operating system level approach distributed systems 
mpp community typically tackles problems having enormous computational requirements 
provides justification expending large programmer efforts optimizing particular program 
high cost application specific programmer optimization generally renders approach 
summary systems currently machines differently trends bring closer 
existing process migration systems require period time large noticed human 
example condor takes minutes migrate large process 
process migration accomplished time human notice issue handled simple scheduling policy option 
freeze free process migration algorithm distributed file system practical 
manufacturers building mpp supercomputers processors memory operating systems stations adv 
mpp supercomputers resemble distributed system collection processors connected communications network shared memory 
closely resemble distributed system software issues similar including process migration 
load indicators primary benefits process migration load balancing 
general impossible predict run time process load balancing uses measure current load predict load 
requires useful measure processor load efficient way collect processor loads 
processor load measures wide variety system load measures available 
different system load measurements unix system appear mw table 
determination measures 
kunz kun attempted experimentally determine best measures 
experiment created artificial load containing cpu bound bound balanced tasks network unix stations 
controlling migration number tasks run queue yields better performance improvements size free available memory rate cpu context switches rate system calls minute load average amount free cpu time 
indicators improves system performance 
kunz combined indicators measured system improvement 
surprisingly number tasks run queue performs better combined measure 
result repeated studies cast doubt usefulness complex load criteria 
study shows useful easy obtain load measure available 
obtaining load measures systems messages bring load information site load balancing decisions 
messages relatively slow computer terms 
motivated search ways minimize message traffic 
eager analyzed performance improvement distributed control strategy increases number probes nodes load information 
results show noticeable improvement probes lesser improvement probes improvement higher numbers 
wong rw investigated adequate results obtained querying full set nodes 
achieved satisfactory performance improvement querying subset half example 
shows reasonable load balancing decisions small number messages 
static vs dynamic load balancing isb studied relative merits static dynamic load balancing 
jobs require small amount computation static outperforms dynamic load balancing 
length jobs increases dynamic load balancing important provides better performance 
related result static method load balancing important longer jobs 
combining static dynamic load balancing produces superior results method results fewer job migrations 
krueger livny kl conclude dynamic process migration achieve considerable additional improvement just static process migration 
zhou ferrari zf experimented load balancing various conditions including situations tasks migrated 
jobs moved interact heavily user terminal text editors command line interpreters 
programs tied interfaces mail message programs 
intuitively obvious point load balancing impractical percentage immobile increases 
zhou ferrari systems achieve benefits load balancing load immobile 
study shows small part load mobile process migration effectively support load balancing 
time constraints theoretical empirical studies investigated time properties task execution system loading effects delay process migration 
task characteristics leland ott analyzed processes vax 
identified categories processes 
short little cpu disk usage vast majority tasks 
heavy cpu little disk usage 
heavy disk little cpu usage 
heavy disk cpu tasks 
smallest processes cpu time 
largest processes cpu time 
additionally average longer process executes longer remaining cpu time lo 
cabrera examined processes runs vax machines 
processes consume second cpu time cab 
sub second tasks require seconds cpu time 
process life times examined processes lived time units live life time units time cab 
zhou examined tasks berkeley vax installation tasks bell vax installation tasks lawrence berkeley laboratory vax cluster installation 
zhou total cpu time consumed jobs requiring second jobs 
jobs consume half second cpu time jobs require seconds traces 
study probability load balancing success rom concludes presence cpu hogs ordinary size processes yields high probability load balancing improve performance 
studies task characteristics confirm situation 
important drawn studies 
long running tasks recognized simple expedient examining task done 
second median life time process study seconds cab process migration quicker 
system loading zhou number tasks ready execution load indicator 
studied system average number tasks queued execution time net change 
average cpu queue changed second interval 
system loads change wide swings 
migration delay towsley stankovic mts theoretical models examine effects migration delays system performance process service time 
short delays produced substantial gains 
high delays performance improvements disappear 
best current implementations approach small delays needed better performance improvements 
impact process migration latency closely related processor speed 
faster processors probably require corresponding improvements migration times 
cabrera study processor speeds probably improved factor 
service time cabrera study ms adjusting processor improvement adjust factor yields modern service time ms 
applying formula half service time yields target process migration latency time ms 
existing systems achieve process migration latency time close 
new freeze free design achieves smaller process migration latency time 
arpaci report parallel programs node cluster high migration costs suffer slowdown times systems lower migration costs adv 
confirms process migration costs impact system performance 
load balancing strategies investigators produced enormous number studies related diverse set load balancing strategies 
ck wang morris wm produced taxonomies load balancing strategies extensive annotated bibliographies 
theimer tl find centralized scheduler scaled thousands machines adequate today needs 
systems including sprite centralized schedulers quite satisfactory 
summary studies show current distributed systems usually stations idle 
plenty computer power available migrating process 
number tasks run queue provides adequate load measure easily obtained 
studies show adding dynamic process migration improves performance just static process migration 
processes immobile process migration provide effective load balancing 
empirical evidence shows past execution history fair predictor activity 
centralized schedulers handle distributed systems thousands machines 
sufficient machine resources knowledge capabilities implement useful dynamic process migration 
currently systems dynamic process migration 
obstacle exist systems dynamic process migration load balancing fault tolerance benefits 
theoretical study points overhead dynamic process migration 
chapter shows actual performance numbers dynamic process migration overhead major barrier 
chapter related chapter examines existing process migration systems related 
chapter places emphasis migration mechanisms effectiveness design 
section examines programming language system supporting migration shows important similarities differences operating system process migration 
second section examines existing user level process migration systems 
fundamental performance limitations user level process migration systems led decision concentrate kernel level process migration systems 
third section surveys process migration mechanisms kernel level systems performance 
results previous process migration systems provided useful guidance designing improved process migration system 
programming language systems moving performed process level 
programming language systems successfully move smaller units machines 
brief examination programming language system serves highlight differences similarities 
charm parallel programming system kal supports migration tasks dr charm calls 
communicate messages 
charm forwards messages migrating updates address information source machine 
identifiers location dependent charm assigns new identifiers migration time 
charm migrates just private persistent data 
charm migrates persistent data programming language system deals problems managing identifiers communication links 
big difference lies smaller state programming language system migrates compared process migration 
example migration involve code transfers 
user level process migration section surveys process migration systems implemented user level 
rsh unix rsh facility supports execution subset shell commands remote machine 
rsh options execution program remote machine 
rsh supports static process migration 
limitations rsh preserve environment current working directory 
zhou ferrari zhou ferrari implemented static process migration unix bsd running dec vax machines 
modified shell execute remotely selected types user commands 
load information manager lim constantly monitors system loads performs job placement 
load balancing manager lbm exists host support remote execution 
butler butler system nic carnegie mellon university collection programs run andrew stations 
butler system manages collection idle stations automatically selects station request 
butler supports static process migration 
butler ships environment information destination host machine improvement rsh 
butler follows autonomous system model 
butler provides minute warning owner reclaims machine kills remaining programs 
remote execution monitor remote execution monitor rem developed berkeley unix 
rem system gives application processes ability manage remote processes means application create kill remote processes 
rem supports varying levels fault tolerance replication processes multiple machines 
rem adds message protocol supports location independence 
rem achieved speedups st 
process server process server system runs top cedar system hag 
process server supports static process migration processes strings streams files communication 
condor condor ll llm supports process migration networks unix stations 
condor essentially batch facility allocates processes idle stations 
idle criteria quite strict unix load keyboard mouse inactive minutes 
owner machine begins station condor migrates processes away 
condor targets primarily long lived processes supports just processes files perform internal processing 
condor forwards file requests submitting machine 
condor uses algorithm ml schedule machines fairly 
condor performs process migration checkpointing process file transferring file restoring process checkpoint file 
checkpointing placement consume approximately seconds megabyte checkpoint file average cost checkpointing placement approximately seconds 
sunderam sunderam implemented unnamed process migration system unix 
system requires migrating process perform actions communicate pipes sockets access non nfs files spawn processes 
system uses pair programs support application terminal interface migration 
new host program pretending terminal interface exchanges data program old host turn exchanges data real terminal interface 
system supports dynamic process migration moving entire process state large operation 
user process migration level system requires approximately seconds kb core image ms 
chang implemented unnamed process migration system unix designed support long running processes 
design collects process state file transfers information file transfer protocol reconstructs process new host machine 
transfer time order minutes dca 
pms process migration system pms fre approaches process migration totally different perspective 
currently systems provide universal location transparent access computer resources cost difficulty 
application processes cpu bound little operating system resources 
pms targets applications 
pms provides process migration facility migrates processes resources location access transparency 
pms delegates responsibility application correctly handling access resources lacking location transparent access 
pms demonstrates approach compute bound ray tracing program 
utopia utopia supports static process migration 
system migrate applications utopia support features migrated processes process groups 
utopia supports file access migrated processes systems uniform shared file name space 
andrew athena hcs meet requirement nfs 
system unusual utopia functions multiple operating systems ultrix sun os hp ux aix irix osf derive unix 
utopia provides automated load balancing 
scalability distributed systems large numbers machines important criteria utopia run sites contain stations 
utopia supports clusters stations larger distributed system provides form hierarchical control 
utopia includes utilities command interpreter batch facility take advantage parallel processing power 
parallel facility demonstrates speedups machine system 
summary user level implementations process migration systems principle problems 
user level implementation access kernel state arbitrary process 
user level implementations migrate certain classes processes 
secondly user level implementation cross kernel application protection boundary time requests kernel service including accessing kernel state 
boundary crossing expensive user level implementation process migration compete kernel level implementation terms overhead 
principle reason user level implementations relatively easier software development maintenance environment 
kernel level process migration section surveys process migration systems implemented kernel level concentrates process migration mechanism 
locus locus distributed operating system unix provides single processor unix semantics distributed multi processor environment 
locus provides network wide location independent name structure distributed file system pwc 
locus follows unix design placing system resources file system network transparent file system facilitates process migration 
locus supports static process migration modified fork command supports dynamic process migration migrate command smi 
locus supports dynamic process migration total copy algorithm refer section 
origin site process keeps track process pw 
demos mp demos mp pm supports dynamic process migration total copy algorithm refer section 
demos mp message distributed operating system 
demos mp places process state various functional modules operating system 
demos mp servers maintain resource state 
universal messages coupled high degree locality respect state greatly facilitates implementation process migration demos mp 
demos mp sends messages buffered way channels called links 
demos mp maintains links process migration forwarding messages new machine 
routing messages old host costly 
forwarding machine informs message sender machine correct new destination machine address updates link 
mos mos bsw distributed operating system compatible unix version application level 
mos attempts provide illusion network computers single system 
mos supports dynamic process migration 
mos reports able transfer memory ms kb ms kb process 
authors report process migration time 
mos runs pdp computers connected bit second local area network lan 
process suspend resume unix system operating system cag implemented ability suspend process file time resume process file 
resume command works operating system rebooted process suspended 
chen che implemented essentially capabilities unix bsd operating system 
store information file names form complete file path names 
perform process migration network file system coupled file transfer capability principle allow processes migrated slowly 
systems find need kernel level modifications obtain necessary information 
nest nest ae system runs collection computers connected compatible bit second ethernet 
computers operate permanently compute servers machines available idle 
nest manages machine resource pool 
rexec command submits remote execution 
nest automatically selects host user specify host 
nest supports static process migration 
nest preserves migrated process view file system parent child relationship process groups process signals 
nest switch determines file activity redirected home machine 
migrated process accesses system utilities system libraries temporary files locally input output files redirected migrated process suffers execution performance penalty versus penalty system redirects accesses home machine 
alonso alonso modified version sun unix operating system support dynamic process migration ak 
process migration facility supports processes communicate processes pipes sockets processes depend environment information process identification 
process migration facility uses variant total copy algorithm see section 
process migration time system checkpoints process files 
file contains dump text data regions 
second file contains name host current working directory file information terminal flags 
third file contains stack registers process control information 
system transfers data uses data restore checkpointed process 
system requires seconds migrate process new host 
system runs sun stations connected bit second ethernet lan 
emerald emerald object programming language distributed system 
emerald runs top ultrix operating system 
emerald complete operating system 
emerald processes section covers emerald process migration systems 
emerald supports migration objects including objects possessing process 
emerald object address space linked node address space 
means object migration involve transfer address space 
emerald migrates data object 
disadvantage translated migration time 
emerald possesses additional information support translation 
emerald process carries actions invoking methods various objects 
invocation involves activation record 
emerald place activation records stack emerald collect activation records process migration time 
emerald code change code readily replicated 
emerald migration avoids migrating code copy code destination machine 
emerald migrates process containing variables ms jul 
emerald transfers process message bytes includes object immediate data stack segment general process control information 
emerald runs microvax ii machines connected ethernet 
emerald process migration fast compared systems described section emerald process migration clearly migrating quite different 
aix tcf aix transparent computing facility tcf wm distributed operating system gives illusion single machine unix system 
aix tcf supports static process migration rexec run commands dynamic process migration migrate commands 
aix tcf process migration initially sends remote tasking message containing part proc user structures open file information 
kernel server process forks child process obtains environment argument stack data information 
child process demand pages program code pure code child process copies code 
child process sends fork done message source machine 
source machine sends message contains signals confirms process destroyed source machine 
machine keeps track processes created process may migrated 
aix tcf process migration support fault tolerance 
mosix mosix bw distributed operating system compatible unix version 
mosix gives application illusion application running single processor system 
mosix descendant mos 
mosix supports dynamic process migration bsw 
mosix runs national semiconductor ns processors connected bit second token ring lan 
memory transfer rate ms kb 
report process migration times 
mosix achieves speedups processors automatic process migration 
mess mess distributed operating system premises 
mess treats memory global virtual memory 
mess provides universal transparent access resources including processors 
presence multiple processors network invisible applications 
process migration occurs byproduct unified distributed virtual memory dvm 
mach uses dvm address space transfers 
studied load balancing simulation models process migration process control transfer followed memory transfer dvm ma 
mess publication clear mess implementation status 
research oriented distributed operating system ggi experimental message distributed operating system 
provides access location transparency design goal hide distributed nature physical system 
supports preemptive load balancing process migration nut 
load balancing server decides migrate process machine 
load balancing server manages collection load information supports variety migration policies 
process migration manager provides process migration mechanism uses variant total copy algorithm see section 
process migration time process migration manager freezes migrating process 
process migration manager transfers entire state 
inter process communication manager freezes message ports transfers messages redirects messages 
old host process migration manager destroys local migrating process 
new host process migration manager resumes migrating process 
clouds clouds dcm distributed operating system 
clouds object thread model 
clouds object large grained encapsulation code data 
clouds object passive persistent exists virtual memory space 
clouds thread causes actions occur invoking object methods execute address space called object 
effect threads move object object course normal execution 
objects reside processor 
threads easily migrate processors 
clouds moves objects processors policy demand page mechanism 
clouds transfers kb page ms clouds shows conventional process model just possible system model 
clouds movement data code control processors correlate directly normal process migration clouds performance incomparable 
galaxy galaxy sms distributed operating system 
galaxy views entities objects including files devices nodes 
object access occurs location transparent manner facilitates process migration 
galaxy notion origin machine process 
galaxy supports dynamic process migration spj 
galaxy address space transfer algorithm combines elements pre copy algorithm see section demand page algorithm see section 
process migration time process execution continues galaxy transfers memory resident pages part working set 
galaxy freezes migrating process 
galaxy transfers process state 
galaxy transmits pages working set pre copied page process subsequently modified 
point galaxy resumes process execution new machine 
parallel execution galaxy transfers remaining pages 
galaxy objects including processes unique object id galaxy keeps location information object id table entry 
process migration time updated 
problem similar updating links demos mp 
immediately prior freezing migrating process galaxy informs message server 
message server forwards messages new host 
migrating process ready execution new host galaxy sends message id manager host copy migrating process id protocol reduces amount message forwarding 
emps eindhoven multiprocessor system emps supports dynamic process migration total copy algorithm refer section 
emps delivers messages mailboxes part specific process 
process connects mailbox port 
mailbox records connected processes machine locations 
design provides incomplete separation message processing process migration 
process receiving sending message mailbox message processing continues unaffected process migration 
system suspends mailbox message processing duration process migration 
emps separation process mailbox provides partially effective means separating process migration message processing 
worst case message freeze time remains unchanged 
emps runs motorola mc mhz processors connected bits second lan 
emps process migration time formula milliseconds ime emps fastest reported process migration times small processes 
example emps migrates kb process mailbox ms emps performance scale increasing process size 
approximately kb size process sprite begins outperform emps fig 

emps reports process migration times machine figures comparable process migration times machines 
birlix birlix distributed operating system object model emphasizes persistent objects 
team birlix version process 
team consists collection segments representing memory regions threads access descriptors representing communication bindings 
birlix supports dynamic process migration version total copy algorithm see section 
birlix checkpoints migrating team process migration time 
birlix sends migration request message type manager destination machine 
receipt birlix transfers checkpointed information 
arrival information birlix performs recovery 
destination agent thread sends message source machine team containing destination agent identification 
birlix forwards identification client team initiated process migration destroys old team 
upgrade added ability tailor object basis standard birlix migration mechanisms lux 
goals take advantage specific object properties reduce migration costs support different migration policies 
birlix migration supports migration object types including processes files dictionaries 
birlix takes approximately ms migrate kb team thread rough estimate performance graph lux 
birlix runs network sun machines connected bit second ethernet lan 
amoeba amoeba distributed operating system objects heavily oriented client server architecture 
amoeba supports static process migration 
despite earlier reports dynamic process migration amoeba actual implementation dynamic process migration reported 
amoeba uses total copy algorithm see section 
process migration time amoeba freezes migrating processes 
amoeba rejects incoming messages process migrating reason 
amoeba source migration server sends message containing control information waits acceptance 
destination migration server obtains memory migrating process series rpc requests 
source migration server transmits execution token causes migrating process resume execution 
source machine responds incoming messages reply 
normal message recovery eventually updates address migrating process correctly delivers messages affected process migration 
mach mach abb message operating system 
milojicic implemented dynamic task migration mach 
task migration operates mach level independent operating system unix vms 
task migration independent mach applications 
raises question appropriate migrate mach level tasks migrating higher level process 
milojicic report migrating task migrating process causes performance degradation migration actions forwarded home machine 
residual dependencies occur system simultaneously migrate higher level process prevent task migration supporting fault tolerance 
milojicic implemented task migration systems 
simple migration server sms optimized migration server oms primarily operate user level 
task migration implemented kernel modifications 
kernel modification provides access task thread kernel ports modification provides ability export pager port 
mach task migration suspends task aborts threads clear associated kernel state 
mach task migration obtains task thread kernel ports 
mach logically transfers address space 
mach transfers threads capabilities task state 
mach restores task thread ports just prior resuming task 
mach task migration normally demand pages address space 
oms differs sms fewer messages transfer thread capability task state information supporting additional memory transfer policies pre copy flushing similar sprite eager copy similar mosix 
dropped oms sms robust oms limitations types tasks oms transfer 
developers dropped oms favor sms thesis uses sms standard bearer 
version mach supports remote memory access norma distributed shared memory dsm extended memory management implemented kernel fbs 
supports demand paging 
version mach memory maps files allows dsm provide file access support migrated processes 
performance reasons milojicic investigating adding distributed file system 
mach task migration runs mhz intel processors connected ethernet 
performance measurements show typical size task sms performs task migration ms oms performs task migration ms 
sms task migration time formula milliseconds ime oms task migration time formula milliseconds ime mdx modular distributed computing system mdx extends migration capabilities nearly system objects including processes sch 
mdx uses location transparent naming 
mdx uses variant total copy algorithm see section migrate processes 
mdx runs mhz intel pc computers connected bit second ethernet 
mdx migrates kb process roughly ms sch 
charlotte charlotte message distributed operating system 
charlotte af fa supports dynamic process migration variant total copy algorithm see section 
charlotte processes communicate connection oriented duplex communication channels system represents capabilities called links 
charlotte buffers message sender side destination accepts message caches messages receiver side 
charlotte freezes communications process migration system marshals transfers process context 
charlotte delays message delivery process migration 
prior process migration starter processes collaborate decide migrate process 
process migration time source destination machines exchange pair messages destination accept migrating process 
charlotte transfers address space 
charlotte updates links 
charlotte transfers process control information 
charlotte runs vax machines connected token ring 
charlotte migrates kb process links approximately ms 
charlotte process migration time formula milliseconds ime lag lag process migration approved 
links local 
set links 
che message distributed operating system 
runs collections disk workstations servers 
processes accessed files network migrating process new machine cause significant changes file system 
process uses inter process communication primitives messages access outside process address space 
updating communication links serves reconnect process migration 
process migration relies error recovery re establish communication links migration 
transfer time entire address space processes longer message timeout periods means program failures occur result process migration total copy algorithm 
theimer developed pre copy algorithm tlc overcome message freeze problem 
section describes pre copy algorithm detail 
virtual memory time stu 
milestone process migration showing address space process transferred operation 
runs sun mhz processors connected bit second ethernet lan 
migrates kb process approximately ms 
process migration time formula milliseconds ime accent accent gos sec 
message distributed operating system 
implemented process migration system copy transfer address space 
process migration time accent moves migrating process messages 
core message contains computer kernel stack process control block set port rights 
accent physically transfers contents core message destination machine 
real imaginary memory address space message contains logical representation virtual address space 
process restored destination machine core messages 
memory page requested destination machine obtains source machine 
copy technique averages fewer byte transfers reduction message processing time 
accent process migration algorithm milestone process migration performance 
accent loses ability support fault tolerance residual dependency memory 
accent runs stations connected bit second ethernet lan 
accent migrates kb process approximately ms 
accent process migration time formula milliseconds ime sprite sprite ocd network operating system distributed file system 
sprite process migration dou dou takes advantage sprite network file server gain benefits demand paged memory transfer simultaneously eliminating long term memory residual dependencies 
integration file server support process migration milestone development process migration 
sprite completely eliminate residual dependencies sprite forwards small set kernel service requests home machine migrating process 
sprite kernel service requests serviced current host machine kernel 
process migration time sprite transfers migrating process state cached file blocks address space directly destination machine 
sprite flushes cached file blocks memory pages differ backing store copy sprite file server 
sprite resumes execution migrating process destination machine 
migrating process begins destination machine memory pages cached file blocks 
sprite demand pages memory file data new host 
sprite runs sparcstation stations connected bit second ethernet lan 
average sprite process migration time ms formula corresponds roughly kb process open files 
sprite process migration time formula milliseconds ime summary existing dynamic process migration systems show increased difficulty kernel level implementation serious obstacle 
despite access state migrating process existing kernel level dynamic process migration systems supports migration application processes 
kernel level systems support dynamic process migration wider class application processes 
kernel level implementations perform dynamic process migration far time user level implementations 
table reports process migration time kb process algorithm computer lan year results published systems tlc af lux rough estimate sch 
process migration systems report time migrating program resumed process migration latency time 
algorithms differ considerably amount state transfer 
total copy pre copy algorithms transfer state 
demand page file server algorithms transfer state address space file cache blocks 
freeze free algorithm transfers process control state execution state current code page current stack page current heap page 
demand page file server algorithms demand page current code page current operating computer lan name year sample algorithm system mb time ms sun mhz ethernet pre copy accent workstations ethernet demand page charlotte vax total copy sprite sparcstation ethernet file server emps mc total copy mach sms mhz intel ethernet demand page mach oms mhz intel ethernet demand page sun ethernet total copy mdx mhz intel ethernet total copy choices sparcstation ethernet freeze free table process migration systems example times heap page current stack page vast majority migrating processes get useful execution underway 
example adding page transfers ms requests ms sprite migration time ms yields effective process migration latency time ms 
graphically shows process migration latency times kernel level process migration systems kb process 
times adjusted hardware differences 
shows decade process migration latency times remained quite large despite wide variety hardware 
summary chapter began brief comparison similarities differences programming language migration process migration 
chapter surveyed user level process migration systems 
inability obtain process state performance handicap caused repeated crossings kernel application boundary severely handicap user level process migration systems 
thesis drops user level process migration systems consideration 
chapter surveyed kernel level process migration systems 
fundamental performance advantage user level process migration systems access process state means migrate greater variety processes 
existing kernel level systems suffer overhead complications message freeze time 
process migration systems support processor fault tolerance 
accent charlotte sprite emps mach oms mach sms seconds process migration latency times system mdx seconds choices comparison process migration latency times chapter mechanism mechanism migrates running process new machine focus thesis 
chapter identifies major issues facing process migration mechanisms 
process migration mechanism decomposes protocol controlling information transfer transfer process control state execution state address space communication state file state 
separate section covers major areas 
process migration protocol process migration protocol defines steps taken machines agree process migration 
protocol establishes transfer order state components migrating process 
major factors influence protocol design 
new host may reject process migration 
protocol preclude old host recovering migrating process new host accept migrating process 
improved network communications offer improved data bandwidth limited approximately round trip times par 
performance goals call minimizing round trip messages total number messages 
process control process control state comprises information operating system manage process 
process control information includes priority state process identification parent process identification 
important characteristic system marshals process control information process migration system freeze migrating process 
execution state execution state includes processor state represents current execution process 
execution state includes general purpose registers floating point registers stack pointer program counter status registers 
execution state heavily machine dependent 
address space address space incorporates virtual memory belonging process 
address space largest component state process 
transferring entire address space single unit results large transfer times relatively small applications 
process migration goal support load balancing transferring address space unit desirable 
messages message state process consists buffered messages control information link 
management communication links process migration re establishment communication links process migration major challenges migrating message state process 
files file state process consists file descriptors cached file blocks 
possible existence temporary files automatically deleted file closed forces general purpose process migration mechanism transfer files open state 
large file caches exist reduce load file server 
long delay results process migration mechanism waits entire file cache flush file server new host 
summary chapter covered major issues facing process migration systems areas protocol controlling information transfer transfer process control state address space communication state file state 
chapter examines existing process migration algorithms address issues 
chapter existing process migration algorithms computer scientists developed primary process migration algorithms 
chapter presents process migration algorithms chronological order invention total copy pre copy demand page file server algorithms 
chronological order correlates directly order increasing sophistication 
study existing process migration algorithms serves show accomplished current limitations exist 
actual systems implemented principle process migration algorithms differ terms actual process components 
example sprite application processes communicate file system messages 
published reports vary considerably level detail respective process migration algorithms 
thesis presents generalized ver sions principle process migration algorithms process components standardized level granularity 
generalized versions assume distributed file system 
total copy algorithm total copy algorithm widely process migration algorithm 
locus smi demos mp pm aix tcf wm ggi emps amoeba mdx sch charlotte af variations total copy algorithm 
total copy algorithm suspends migrating process transfers state information new host resumes migrating process 
generalized variant total copy algorithm follows 
old host suspends migrating process 
old host transmits process migration approval request message 
new host responds approval rejection message 
old host marshals transfers process control execution state 
old host ships communication links buffered messages 
old host transfers file descriptors dirty file cache blocks 
old host discards clean file cache blocks 
old host ships code heap stack pages 
completion old host transmits message telling new host resume execution migrated process 
graphically displays data exchange sequence total copy algorithm 
total copy algorithm transfers process state prior resuming execution new host valid orderings transferring components process state 
total copy algorithm major flaw slow transfer time 
relatively small processes address spaces stretch kb large processes extend millions bytes 
transfer time particularly important processes communicate processes 
transfer period migrating process unable accept messages 
communication systems timeout periods detect failed message receivers 
long process migration transfer time appear communication system failed receiver result communication failure 
long transfer times add overhead process migration load balancing reduce utility 
total copy algorithm eliminates residual dependencies support processor fault tolerance 
pre copy algorithm theimer invented pre copy algorithm operating system order overcome communication failures occur total copy algorithm tlc 
important message design 
mach oms process migration system implemented pre copy algorithm transfer strategy options 
pre copy algorithm transfers address space new host parallel continued execution migrating process old host 
migrating process modifies address space pages continued execution 
pre copy algorithm transfers modified pages number modified pages falls small threshold value 
pre copy algorithm suspends migrating process transfers remaining modified pages process state components 
pre copy algorithm freezes message processing suspension migrating process 
approach dramatically reduces message freeze time large reduction process state transferred message freeze time 
generalized version pre copy algorithm follows 
process migration time migrating process continues execution 
old host transmits process migration approval request message 
new host responds approval rejection message 
old host ships code heap stack pages 
old host compares predetermined limit number pages modified continuing execution migrating process 
number modified pages exceeds limit old host ships modified heap stack pages 
code pages normally modified 
migrating process modify code pages old host ship modified code pages 
old host repeats cycle shipping modified pages number modified pages predetermined limit 
time old host suspends migrating process 
old host marshals transfers process control execution states 
old host ships communication links buffered messages 
old host transfers file descriptors dirty file cache blocks 
old host discards clean file cache blocks 
old host ships remaining dirty heap dirty stack pages 
completion old host transmits message telling new host resume execution migrated process 
graphically displays data exchange sequence pre copy algorithm 
pre copy algorithm reduces message freeze time eliminates process migration caused communication failures 
message freeze time substantial 
modified page transfer represents transfer page pre copy algorithm transferred constitutes additional total copy algorithm 
additional pre copy algorithm requires total time total copy algorithm 
increased overhead detracts ability process migration support load balancing 
demand page algorithm implemented process migration accent copy method transfer address space 
lazy copying refers method 
copy applies older time shared operating system concept demand paging process migration 
process migration time address space remains old host 
new host demand pages address space data required 
aix tcf demand pages unmodified code pages 
distributed virtual memory dvm accomplishes essentially form address space transfer 
mach oms sms dvm address space transfer 
mess uses dvm address space transfer 
generalized version demand page algorithm follows 
generalized version differs original accent version accent accesses file system messages mention file cache 
old host suspends migrating process 
old host transmits process migration approval request message 
new host responds approval rejection message 
old host marshals transfers process control execution state information address space 
system transfer actual code heap stack pages time 
old host ships communication links buffered messages 
old host transfers file descriptors dirty file cache blocks 
old host discards clean file cache blocks 
old host transmits message telling new host resume execution migrated process 
migrating process page faults immediately migrating process needed code page 
new host transmits message requesting specific code page old host responds transmitting needed page 
shortly new host page faults stack page heap page 
realistic programs require code heap stack page meaningful execution 
graphically displays data exchange sequence demand page algorithm 
demand page algorithm dramatically reduces amount data shipped process migration latency period 
demand page algorithm generally shorter message freeze time pre copy algorithm 
demand page algorithm eliminates transfer address space pages migration 
average approach transfers fewer bytes reduces message processing times 
reductions process migration latency period amount data transferred process migration contribute reducing process migration overhead boost utility load balancing 
major disadvantage demand page algorithm old host maintain address space migrating process completes execution 
process migrate machine maintain address space information 
worst case scenario page fault occurring process migrates multiple times causes search missing page machine involved past process migrations 
address space residual dependency prevents demand page algorithm supporting fault tolerance 
file server algorithm previous process migration algorithms involved exactly machines old host new host 
douglis expanded process migration possibilities adding file server machine third machine 
goal sprite process migration mechanism achieve efficiency demand page algorithm address space residual dependency 
sprite file system distributed file system manages files including temporary files backing store 
suspending migrating process sprite flushes dirty memory pages dirty file cache blocks file server 
new host demand pages file blocks memory pages file server 
sprite applications dependent sprite file server approach add new machine dependency 
sprite process migration dou dou swaps cost file server flush elimination address space residual dependency 
sprite eliminate residual dependencies old host machine system forwards system calls home machine execution system time call 
douglis publish name process migration algorithm file server represents important contribution thesis uses name file server algorithm generic algorithm 
sprite differs file server algorithm sprite applications communicate file system messages 
generic version file server algorithm follows 
system suspends migrating process 
old host transmits process migration approval request message 
new host responds approval rejection message 
old host marshals transfers process control execution state information address space new host 
actual code heap stack pages transmitted time 
old host ships communication links buffered messages new host 
old host transfers file descriptors new host 
old host flushes dirty stack pages dirty heap pages dirty file cache blocks file server 
old host discards clean pages clean file cache blocks 
old host transmits message telling new host resume execution migrated process 
migrating process page faults immediately migrating process needed code page 
new host transmits message requesting specific code page old host responds transmitting needed page 
shortly new host page faults stack page heap page 
realistic programs require code heap stack page meaningful execution 
graphically displays data exchange sequence file server algorithm 
sprite takes seconds migrate process dirty pages file blocks open files dou 
sprite worry message freeze time sprite applications messages file system communications implemented way time sensitive 
study system developers comparing sprite amoeba system developers report message communications amoeba simpler better performance sprite file communications 
file communication replace message communication 
file server algorithm potentially long message freeze times file server algorithm cause communication failures message system 
file server algorithm transfers dirty memory pages dirty file cache blocks file server new host 
data transfers file server represent overhead algorithms burden process migration support load balancing 
actual process migration latency performance sprite ranks top older systems excluding mach oms kb size process shown table 
summary chapter examined primary process migration algorithms 
despite widely algorithm total copy algorithm unacceptably long process migration latency times message freeze times 
pre copy algorithm reduces message freeze time communication failures occur 
pre copy algorithm achieves goal cost longer process migration latency times greater amount total 
demand page algorithm physically transfer address space part process migration latency operation shortens dramatically process migration latency time message freeze time 
demand page algorithm demand pages needed pages old host 
address space dependency old host exist indefinitely demand page algorithm loses ability support processor fault tolerance 
file server algorithm adds file server support 
process migration time file server algorithm flushes modified data file server acts demand page algorithm 
file server algorithm trades flush operation elimination residual dependencies 
normal case acceptable process migration latency message freeze times worst cases unacceptable 
chapter explores improve existing algorithms 
old host new host request migration approval communication links file info approved migration request resume process command useful execution total copy algorithm code pages stack pages heap pages process control execution state total copy algorithm old host new host request migration approval approved migration request pre copy algorithm code pages stack pages heap pages modified stack pages modified heap pages communication links file info modified heap pages modified stack pages resume process command useful execution process control execution state pre copy algorithm old host new host request migration approval communication links file info resume process command request code page code page request stack page stack page request heap page heap page useful execution demand page algorithm approved migration request process control execution state demand page algorithm file server algorithm new host old host file server file info communication links migration accepted request migration approval dirty heap page dirty file cache page dirty stack page useful execution code page heap page stack page resume process command request code page request stack page request heap page process control execution state file server algorithm chapter performance improvement opportunities chapter identifies performance bottlenecks existing process migration algorithms shows overcome limitations 
faster processor network speeds reduce process migration times process execution times 
thesis places emphasis algorithmic changes improvements process migration 
separate sections identify performance improvement changes protocol controlling information transfer address space transfer message management file management object allocation process modularity 
protocol existing systems send message old host new host requesting approval process migration wait response 
better approach eliminates approval request message 
freeze free algorithm uses process control execution state message transfer state implication signal start process migration new host 
new host responds approval rejection 
old host assumes acceptance continues shipment data 
freeze free algorithm approach eliminates waiting time 
new host occasionally reject process migration simply fail 
freeze free algorithm protects data loss discarding data old host new host confirms receipt 
old host receive positive confirmation old host migrates process simply executes process locally resources available 
newer communication gear steadily appearing increased data bandwidth 
switching circuits reducing switching times laws physics limit time electron flows wire time photon passes optical fiber 
round trip times physically distributed systems decrease bandwidth increases dramatically 
facts justify moving protocol rely request response messages 
freeze free algorithm process migration information uses request messages unit infor mation needed order 
blast protocol require message mul reduces communication overhead 
address space existing process migration algorithms transfer entire address space rely demand paging 
transferring entire address space results unacceptably long process migration latency periods 
demand paging address space requires request response messaging page results waiting period page 
examined transferring working set pages process migration time demand paging remaining pages 
performance measurements show system achieves better performance demand paging pages 
network disk access speeds differ surprising demand paging network differs somewhat demand paging disk 
existing process migration algorithms overlooked possibility 
process migration time system determine immediately needed code stack pages usually determine needed heap page 
moment program counter holds address computer accesses instruction 
similarly stack pointer holds address top stack top current frame 
freeze free algorithm determines heap page examining instruction stream instructions load store registers 
possible pathological program possess complex branching large distances 
rare occasions freeze free algorithm easily determine heap address freeze free algorithm simply resorts demand paging heap pages 
shipping wrong page cause failure incur large performance penalty freeze free algorithm uses heuristic determine heap address 
freeze free algorithm needs determine correct page freeze free algorithm takes advantage program locality 
freeze free algorithm uses heap address ignore branches 
heuristic fast simple effective 
heuristic requires seconds determine heap address test program 
important advantage approach program perform useful execution correct code correct stack correct heap page 
hand program accomplish pages 
approach system transfer remaining address space pages process migration latency period 
freeze free algorithm flushes modified pages file server blast protocol 
blast occurs parallel execution migrated process new host 
distributed file systems standard component distributed systems 
absence file server freeze free algorithm data new host 
approach eliminates address space dependencies old host process migration completes 
machine fault tolerance important goal dynamic process migration 
stack special property memory past current top stack currently process 
freeze free algorithm discards bytes stack page past current top stack 
process migration system reduces current stack page bytes test application process represents savings kb page size system 
messages existing process migration algorithms freeze message processing portion process migration latency period 
message freeze time existing systems lasts process suspension process resumption 
message freeze time complicates communication subsystem message freeze requires communication subsystem buffer messages drop messages freeze rely error recovery 
systems separate process state state communication links 
emps places communication link mailbox separate process 
migrating process accessing communication functions emps migrates process freezing message processing 
emps freezes message processing process migration latency period process active message activity wait message operation 
emps message freeze time worst case extends process suspension resumption emps faces complications communication subsystem systems face 
migrating process generate outbound messages process migration system suspends migrating process period 
programs remain active send messages migrating process 
message receipt communication subsystem modifies process state record message receipt 
message receipt old systems affects just message queue 
process migration system may affected data structures 
process migration system freezes message processing reconstructed entire process state new host 
crux problem entanglement process state communication link state 
freeze free algorithm design removes communication link control state module containing process control state 
conversely freeze free algorithm design removes process control state module containing communication link control state 
freeze free algorithm design allocates memory region address space process solely purpose containing communication state 
steps serve isolate process communication link 
freeze free algorithm design permits message receipt proceed parallel process migration 
communication subsystem buffers incoming message communication memory region records message communication message queue resides communication memory region 
migrating process wait message arrival communication subsystem process message arrives process migration 
message arrives message queue full error occurs regardless process migration underway 
process migration message freeze time longer dependent process migration 
remains issue message processing support migration communication link 
just prior shipping communication link process migration system sets flag causes rejection incoming messages 
process migration system immediately transfers communication link current design fits message 
process migration system queues communication link transfer message transmission communication subsystem reject incoming message 
old host rejects messages arriving communication link transfer 
rejection notice contains reason process migrated provides new location 
communication subsystem originated message updates link information retransmits message new host 
rare situations retransmitted message arrives new host prior communication link transfer message 
new host communication subsystem blocks messages communication link arrives 
test system uses communication subsystem built choices port lia kernel hp 
kernel uses thread process message message protocol stack maintains pool threads 
message thread blocks message processing unaffected 
message originators receive rejection notices new location information transmission communication link message 
communication link transfer message transit new host wait short 
message block time short system treats manner page fault fetches disk page containing message queue 
approach effectively eliminates message freeze time 
migrating process resumes execution waiting communication link state 
migrating process attempt access messages prior arrival communication links new host block migrating process 
process migration system automatically communication link information shipping stack pages 
communication link information arrive shortly need request message 
file information distributed file systems sprite file system nwo cache file blocks enhance performance 
process migration time process may large number dirty file blocks file system cache 
simple approach cached dirty file blocks resuming migrating process 
unfortunately simple approach yields long process migration latency periods 
open files dirty file blocks exist sprite reports process migration times measured seconds dou 
long process migration times hinder effective load balancing 
better approach handling file cache clearly needed 
eliminating file cache viable approach degrade application run time performance 
examination application file access patterns provides valuable clues effective solution 
ousterhout analyzed file system access patterns unix bsd file accesses heavily sequential och tab 

baker investigated access patterns distributed file system sprite 
reported sequentiality access increased years traces show read accesses sequential file transfers versus bsd study data transferred sequentially versus bsd study bhk 
results major significance process migration 
means fully written file blocks written 
process wait vast majority dirty file blocks flushed 
unchanged file blocks file server 
freeze free algorithm design simply discards unmodified file blocks old host process migration time flushes modified file blocks file server 
freeze free algorithm flushes dirty file blocks migrating process resumes execution new host 
new host request file block old host distributed file system orders old host transfer block directly new host 
actual measured file access patterns situation occur overwhelming majority file blocks 
freeze free algorithm design applies separation applied communication link state process control state file descriptor state process control state 
file descriptor information contains information process control state vice versa 
new design places file descriptor information memory region address space process 
migrating process resumes execution waiting file descriptor state 
migrating process attempt access files prior arrival file descriptor information migrating process blocks system automatically ships file descriptor state communication links arrive shortly 
object allocation message communication subsystems allocate message sized chunk memory time message arrives 
message communication subsystems deposit incoming messages allocated buffers 
publications existing process migration systems referenced thesis mention preallocated data structures 
process migration freeze free algorithm set data structures partially initializes data structures 
technique moves memory allocation initialization process migration latency period 
preallocation partial initialization occur system startup times process migration completes 
motivation allows process migration take advantage processor cycles critical time periods 
implemented process migration system data structures process domain memory regions command line arguments code heap user stack kernel stack message queue 
chapter provides measurements showing substantial performance gains 
process migration time freeze free algorithm uses migrated information complete initialization 
process modularity currently operating systems typically scatter process related information different locations operating system data structures 
violates concept modularity 
information dispersal extraction insertion task control information difficult takes longer 
measured time accent expends extracting process sample programs 
dividing measurements migration time host lookup measurements excision time produced percentage time accent spends excision 
accent process migration spends excision average excision 
figures show information dispersal expensive 
typical process migration insertion operation scatters process related information new host resulting costly operation 
freeze free algorithm design reorganizes operating system data structures information object contained strictly object 
example process object contains process control information contains information particular application message queue 
conversely freeze free design takes information objects belong 
new find process objects contain information specific process objects 
result combined excision insertion costs process control state execution state memory region information take process migration latency period see section breakdown category process migration latency period activities 
excision insertion costs smaller percentage far smaller latency period 
summary chapter identified major impediments process migration performance provided techniques overcome existing limitations 
freeze free algorithm takes advantage specific changes protocol address space transfer message management file management object allocation process modularity 
chapter freeze free algorithm chapter describes new freeze free algorithm process migration 
freeze free algorithm starts operating system orders process migration subsystem migrate specific process old host machine specific new host machine 
freeze free algorithm targets distributed system environment containing distributed file system applications communicate messages 
freeze free algorithm supports process migration homogeneous machines running operating system 
freeze free algorithm independent programming language operates executable module level 
basic freeze free algorithm migrates process exactly thread control 
chapter begins section describing freeze free algorithm steps 
chapter follows sections covering process migration latency message handling page faults flush operations 
algorithm steps process migration time freeze free algorithm suspends migrating process 
old host marshals execution process control state message transmits new host 
new host initializes empty process object responds acceptance rejection message 
waiting old host determines current code page ships 
old host searches current heap page heuristic 
old host determines current heap page old host ships 
old host determines current stack page discarding bytes past top stack ships 
new host received processed code page heap page optional stack page new host resumes migrating process 
old host continues shipping remaining stack pages followed communication links file descriptor information new host 
old host flushes file server dirty heap pages dirty file cache pages 
additional physical communication link exists old host file server separate communication link old host new host old host flushes dirty pages file server right process suspension parallel transfers new host 
data transferred old host old host sends flush complete message new host 
completes process migration 
graphically portrays steps freeze free algorithm 
process migration latency process migration latency period extends process suspension till process resumption occurs new host receives stack page 
freeze free algorithm performs steps regardless process address space size number open files state message queue number dirty pages file cache blocks 
factors affect performance number bytes truncated stack page presence floating point heap page 
freeze free algorithm migrates large processes quickly 
message handling old host ships communication links old host receives messages 
new host holds messages arriving communication links transit 
communication links arrive new host new host accepts messages 
freeze free algorithm stops message receipt message receipt delayed communication links transit 
time system suspends migrating process migrating process wait message 
message arriving process migration latency period available migrating process resumes execution new host 
page faults flushing process migration migrating process knows dirty pages reside old host 
page fault occurs process migration distributed file system knows missing page resides old host file server distributed file system requests page directly appropriate machine 
flush complete message tells new host pages available file server 
point dependencies old host exist old host file server 
time flushing directly correlated number dirty pages old host 
summary chapter described new freeze free process migration algorithm 
freeze free algorithm reduces information sent process migration latency period minimum 
general purpose process migration system transfer process control state execution state current code page current heap page current stack page 
freeze free algorithm effectively eliminates message freeze time 
freeze free algorithm supports processor fault tolerance quickly eliminating residual dependencies old host machine 
freeze free algorithm significant advance previous process migration algorithms 
old host code page file info communication links stack page heap page migration accepted remaining stack page flush complete file server new host freeze free algorithm useful execution dirty file cache page dirty heap page process control execution state freeze free algorithm chapter performance measurements chapter describes experiments carried validate process migration ideas reports actual performance measurements 
chapter begins description process migration test system covers series experiments 
experiments prove freeze free algorithm works 
second experiments show freeze free algorithm achieves process migration latency time ms 
time order magnitude shorter previous process migration systems time achieves short process migration latency time identified section 
third chapter reports performance measurements process migration steps analyzes major costs 
fourth chapter reports performance measurements cross network demand page operations analyzes major costs 
fifth experiments demonstrate utility process migration load sharing effects process migration overhead load sharing speedup 
test system general algorithm tailored actual target environment 
section describes environment actual process migration system implementation 
hardware implementation platform pair sparc station tec stations kb cache mb main memory 
lance amd supports connection bit second ethernet lan 
operating system choices ci release host operating system 
choices object oriented operating system built str small amount assembly language 
choices models system resources management entities collection objects represent policies algorithms mechanisms data representations cij 
objects organized frameworks combined produce system 
communication subsystem choices supports quite different communication subsystems 
process migration system uses choices port lia kernel hp system 
environment migrating processes needs message protocol location transparent addressing 
flip protocol provides location transparent addressing amoeba 
hutchinson peterson designed kernel facilitate addition new protocols 
process migration system implementation includes implementation new protocol addresses messages process identifiers top udp kernel 
message protocol works message communications single processor network 
message delivers signals processes 
example system delivers signal messages kernel performs appropriate action blocked process 
communication subsystem copies message data transmission kernel threads 
communication subsystem delivers address message data kernel thread copying 
table reports round trip transfer time needed kernel thread kernel thread message ethernet lan 
various message data sizes correspond actual data sizes messages process migration 
final copy time table shows actual data copy times encountered daemon 
combined process control execution state message bytes daemon copies process control execution state information kernel resident data structure 
messages deliver page data requires addition memory management unit address translation copy operation 
cost adding address translation accounts large time difference copying bytes msg round trip final round trip size final copy copy final copy bytes time sec time sec time sec table message transfer performance bytes 
messages application processes require additional copy transmit receipt 
file system choices file system mad supports variety file formats kernel level implementation 
choices file system distributed file system location transparency 
individual working providing file data network variety flexible caching strategies 
new process migration system support migrating programs open files time reasons 
working distributed file system unified name space location transparency available 
secondly design moved file state transfer critical path process migration latency period 
operation time sec kernel thread context switch lock acquire release semaphore table process subsystem performance process subsystem choices process single thread control executes virtual memory address space 
choices supports multiple processes virtual address space 
process migration implementation migrates process exclusive access virtual memory address space 
freeze free algorithm design process object system objects localizes process related information process object 
implementation effort included upgrading process scheduler support process priorities globally enforce uniform scheduling policy 
implementation effort included upgrading performance reasons process locking single heavyweight lock set lightweight locks appropriate different situations 
result complete replacement major performance improvements reported rlc 
context switch combined acquire release lock combined non blocking semaphore kernel level performance times appear table 
application operating system interface choices provides object oriented application interface operating system uses rus represent kernel objects extend inheritance application programs 
contains kernel address kernel object represented 
specific kernel address may new host 
system existing expensive translations 
extending inheritance kernel application boundary extends compilation dependencies 
reasons process migration system migrate 
system dependencies quite extensive 
process migration system proxies process migration 
areas new design eliminates 
example new design supports access process objects location independent process identifiers 
experience leads recommendation location dependent identifiers distributed systems 
process migration subsystem principal components process migration subsystem daemons 
daemon kernel thread supporting outbound process migration daemon kernel thread supporting inbound process migration 
daemons new message protocol communications 
process object exist complete isolation 
process object requires support ing objects 
example domain object identifies memory regions supporting process object contains execution state process 
keeping object oriented design methodology freeze free algorithm design adds methods supporting objects extracting inserting state data 
approach facilitated process migration implementation 
sprite uses callbacks modules encapsulate data reported approach easier implement maintain port earlier approach performed functions central location dou 
process migration performance section reports process migration latency performance describes costs process migration steps old host new host 
process migration latency experiment demonstrates short process migration latency times application sends receives messages 
process migration system performs process migration latency period ms kb page size 
system design page size option 
process migration system performs process migration latency period ms kb page size ms kb page size 
items cause variance process migration latency time 
floating point registers processes 
occasionally possible heuristic fail find heap page address system demand pages heap pages 
degree system truncate top stack page varies 
times apply processes size time variations factors just identified 
migrate steps step improving process migration learning 
section details steps performed daemon process migration latency period 
timing begins object receives application process migration request 
start daemon step system blocks application process context switches daemon 
daemon initializes init step 
daemon adds application stack kernel domain add stack step 
daemon extracts process control information get process step memory region information get domain step register state get regs step 
daemon processes message communication protocol stack st msg step 
daemon adds application heap kernel domain add heap step finds current application heap address get heap addr step processes heap page message communication protocol stack heap page msg step 
daemon processes stack page communication protocol stack stack msg step removes stack kernel domain remove stack step 
daemon processes message responds acceptance message 
daemon processes code heap stack page information responds timer message 
enqueues migrating application process execution 
daemon waits accept migration message accept msg step 
daemon waits timer message msg wait step 
system uses timer message process migration latency period timing tests signal old host new host resumed migrating process 
timer message part actual process migration system subtracting cost delivering timer message message arrival time yields actual process migration latency time 
process migration performance numbers kb page size appear table kb page size appear table kb page size appear table 
migrate steps messages daemon drive operations daemon new host 
daemon receives messages process migration latency period 
receipt process migration message daemon prepares new process migration 
data copy step copies step time cumulative time start daemon init add stack get process get domain get regs st msg add code code page msg add heap get heap addr heap page msg stack msg remove stack enqueue accept msg msg wait msg cost total time table migrate kb page step time cumulative time start daemon init add stack get process get domain get regs st msg add code code page msg add heap get heap addr heap page msg stack msg remove stack enqueue accept msg msg wait msg cost total time table migrate kb page step time cumulative time start daemon init add stack get process get domain get regs st msg add code code page msg add heap get heap addr heap page msg stack msg remove stack enqueue accept msg msg wait msg cost total time table migrate kb page message data local buffer 
daemon adds application code memory region application domain add code user step application heap memory region add heap user step application command line argument memory region add args step 
daemon initializes process control information process object init process step 
daemon queues accept message transmission accept msg step 
receipt code message daemon adds application code memory region kernel domain add code kernel step 
daemon loads code page copy code page step 
receipt heap message daemon adds application heap memory region kernel domain add heap kernel step 
daemon loads heap page copy heap page step 
receipt stack message daemon adds application stack memory region kernel domain add stack kernel step 
daemon loads stack page copy stack page step 
daemon performance measurements kb page size appear table kb page size appear table kb page size appear table 
daemon maintains pool objects process migration time 
daemon creates empty domain object empty process object backing store objects memory regions empty user stack kernel step time cumulative time control msg data copy add code user add heap user add args init process load regs accept msg code msg add code kernel copy code page heap msg add heap kernel copy heap page stack msg add stack kernel copy stack page remove stack kernel table migrate messages kb page step time cumulative time control msg data copy add code user add heap user add args init process load regs accept msg code msg add code kernel copy code page heap msg add heap kernel copy heap page stack msg add stack kernel copy stack page remove stack kernel table migrate messages kb page step time cumulative time control msg data copy add code user add heap user add args init process load regs accept msg code msg add code kernel copy code page heap msg add heap kernel copy heap page stack msg add stack kernel copy stack page remove stack kernel table migrate messages kb page step time cumulative time domain process back store user stack kernel stack table migrate object pool stack 
daemon adds kernel stack kernel domain 
time required create objects appears table 
process migration latency period analysis section identifies operations process migration latency period assigns operation costs major categories 
section examines impact areas process migration latency period identifies performance improvement possibilities 
latency period critical path process migration uses principle resources old host processor new host processor ethernet lan 
resources potentially bottleneck 
graphically displays resource usage resources time process migration kb page size 
old host busy new host busy ethernet busy process migration latency period 
process migration latency period divides naturally steps process control execution state code page heap page stack page 
time measurements old host come table 
adding appropriate header udp header ip headers ethernet headers trailers ethernet gaps actual data length dividing bits second yields ethernet transmission time 
calculation account possible ethernet packet collisions 
router software periodically transmits routing messages conflict slightly process migration ethernet 
directly measuring time spent kernel processing inbound message possible 
kb message travels ethernet messages 
kernel processes inbound ethernet message separate thread 
kb requires kernel thread context switch cost 
processing inbound message message protocol stack order magnitude processing outbound message 
context switch message protocol costs guide resulted estimate message arriving new host 
subtracting outbound message protocol processing time transmission time round trip time yields measure receive message protocol processing cost close agreement prior estimate 
combination receive message processing cost daemon process migration costs table yield total time cost step new host 
net result rough useful picture process migration system spends time process migration latency period 
old host ethernet new host code heap msg code msg stack msg state state msg time state code heap stack ack msg execution heap stack resource usage process migration latency period process migration resource usage time clearly shows new host principal bottleneck current process migration system ethernet lan close second 
table reports percentage time critical path spent major activity category 
extraction insertion extraction old host insertion new host process control state execution state memory region information takes activity time cent category critical path time state extract insert domain add remove add address translations init process data copy msg protocol transmission time miscellaneous table major activities critical path process migration latency period 
accent excision average takes process migration latency period see section 
clearly localizing information yields noticeable performance improvement 
virtual memory adding removing memory regions address space domains hosts takes process migration latency period 
adding memory management address translations new host takes process migration latency period 
virtual memory management operations take process migration latency period 
process object initialization takes process migration latency period 
biggest part process object initialization preparation zero filled memory region application stack 
primarily virtual memory operation 
categories combined take process migration latency period 
improvements process migration performance require serious examination virtual memory support operations 
choices virtual memory system operational substantial opportunities improvement 
change operation adds memory region address space domain saved ms better search heuristic 
mismatch exists virtual memory subsystem class hierarchy functional dependencies 
choices project added class bottom existing virtual memory class hierarchy support mapping selectable number pages memory region 
parent class represents memory region 
currently created part adding domain destroyed removal domain 
system creates destroys process migration latency period 
single choices object creation needed memory allocation requires changing class hierarchy class subclass eliminate unnecessary object creations class hierarchy functional dependency relationships 
opportunities virtual memory performance enhancements exist 
rewriting virtual memory subsystem crt 
data copying copying data message buffers old new host takes process migration latency time 
current limitations require prior transmission copy contiguous memory various message headers udp ip ethernet message data message trailer 
smarter network interface device eliminate copy transmission directly shipping headers data trailer different memory locations 
process migration related messages contain page data 
currently receipt daemon copy page data page align data strip message protocol wrapper 
leads recommendation development special protocol page size transfers 
smart interface device separate message wrapper deposit page data physical page 
data delivered remapping physical page 
approach eliminate data copy message receipt 
distributed virtual memory file transfers system services benefit protocol 
communication cost system waiting time transmit bits ethernet lan takes process migration latency time 
bulk transmission time hidden higher costs creating initializing objects new host 
interesting implications 
faster network speeds yield large improvements 
replacing bit second ethernet lan bit second atm network leaving unchanged yield reduction process migration latency time 
naturally transmission time important savings occur areas 
message protocol handling takes action kb page kb page kb page page request page transfer page copy total time table page fault overhead process migration latency time 
combined transmission time message protocol handling take percent process migration latency time 
demand page analysis page faults non resident pages incur substantial overhead new host 
message sent requesting missing page takes host spends selecting correct page 
system takes transporting kb page new host 
new host spends copying message kb page data actual kb page 
new host continues page simply missing needed address translations 
kb page fault network adds table reports times kb kb kb page sizes 
performance tests old host idle domain change occurs servicing page request 
domain changes add overhead cross network page fault 
alternative view page fault overhead number page faults add second overhead 
table shows kb page faults kb page faults kb pages faults add roughly second overhead 
action kb page kb page kb page page faults second table page faults second activity time cent category critical path time add address translation data copy msg protocol transmission time table major activities demand page table shows page fault network absolute times percentages spent major activities 
relative importance differs dramatically process migration latency period 
network transmission time dominates faster network speeds effective 
earlier recommendation eliminate data copying see section applies eliminating data copying improve performance 
messages containing page larger maximum size supported ethernet fragmented multiple messages transmit reassembled receipt 
kernel assigns thread process message fragment 
receive side minimum context switch kernel thread plus context switch back daemon 
system requires minimum context switches kb page 
leads design change recommendation kernel 
new design places incoming messages queue 
single thread processes sequentially messages queue 
new design activates new thread currently running thread blocks 
new design reduces number context switches 
traces kernel activity show choices kernel message threads run sequentially rare exceptions 
cost context switch process migration system transfers kb pages example application 
proposed change save approximately ms particular process migration 
communication subsystem performance respectable opportunities performance enhancement exist line code expansion manner unix bsd system lc 
load sharing goal thesis demonstrate better process migration mechanism 
load sharing important process migration 
experiment demonstrates effectiveness process migration 
parent process creates identical child processes measures total execution time 
child process executes variant lawrence livermore loops test seconds performing input output 
processes share 
system allows active process migration time 
old host acts server page data 
choices allow program stopped arbitrary points resource management limitations 
test case time sec speedup migrations migration kb migration kb migration kb migration table load sharing experiment results child processes periodically poll system asking migrate machine 
polling adds scheduling delay 
application process execution takes process migration turned 
remainder experiments turned process migration 
kb page system migrates average processes executes applications speedup 
kb page system migrates average processes executes applications speedup 
kb page system migrates average processes executes applications speedup 
table summarizes results 
kb system yields better speedup kb system turn yields better speedup kb system 
graphs relationship page size speedup 
series experiments show effect additional overhead performance 
experiments ran page size additional artificial delays ranging seconds 
experiment ran buffer copy operation delay period simulate artificial load 
charts effect additional overhead speedup kb systems process migrations 
page size vs performance speedup page size kb page size impact performance kb systems process migrations handles kb systems process migrations 
process migration latency period demand paging network add overhead execution application new host 
formula defines total cost overhead process migration total process migration overhead cost kb page size test system milliseconds kb sample programs generates page faults executed kb page size system 
formula sample program incurs total process migration overhead ms result network page faults 
formula calculates theoretical load sharing system speedup process migration processor system workload split equal numbers identical processes speedup overhead vs performance speedup overhead seconds added overhead versus performance kb pages overhead vs performance speedup overhead seconds added overhead versus performance kb pages overhead vs performance speedup overhead seconds added overhead versus performance kb pages combined process migration overhead additional overhead values appear plotted speedup 
theoretically complete overhead values appear plotted speedup 
clearly shows actual measured performance tracks theoretical curve significant additional overhead 
additional overhead lies scheduler policy 
application processes poll request process migration 
polling introduces delay system migrate system begins process migration 
new host lies idle application waits page faults network 
migrating applications allow processor application waits page transfer 
third delay occurs old host 
choices schedules processes priority scheduler preempt process newly ready process higher priority 
scheduler allows currently running process complete time slice 
process servicing demand page requests higher priority application code waits anyway lack preemption 
priority process preemption needed testing migration single process 
multiple application test shows priority process preemption essential effective load balancing 
delay demand page request repeats times times example delays combine 
shows relationship speedup overhead non linear 
example speedup requires overhead speedup requires overhead second application execution time 
actual process migration overhead ms second application performance vs overhead real values theory values speedup overhead seconds combined overhead versus performance execution time 
small overhead essential effective load balancing constant unit reduction overhead yields increasing units speedup overhead reaches zero 
results agree general direction mts results show overhead far execution time speedups processor system 
summary performance measurements freeze free process migration system confirm freeze free algorithm achieves targetted goals 
freeze free algorithm completes process migration latency period ms kb page system ms kb page system ms kb page system 
performance figures show system accomplishes process migration minimal overhead 
virtual memory management operations dominate cost process migration latency period 
analysis demand page overhead shows demand page cost dominates total overhead process migration cost ms versus ms latency overhead sample program 
communication costs dominate total demand page cost 
experiments measuring load sharing speedup versus increasing overhead provide concrete evidence minimizing overhead absolutely essential effective speedup 
scheduling overhead adds process migration latency demand page overheads 
process migration requires priority scheduling priority process preemption support demand paging load sharing environment 
demand page delays excessive 
chapter algorithm comparison chapter compares demand page file server process migration algorithms freeze free algorithm 
chapter concentrates algorithms seeks factor implementation differences 
comparison uses step performance times chapter algorithms 
total copy pre copy algorithms severe performance limitations compared demand page file server algorithms thesis drops total copy pre copy algorithms coverage 
chapter describes dvm support freeze free algorithm 
normalized performance unfair compare process migration systems running different platforms solely basis absolute time 
processor network speeds important perfor mance factors 
demand page algorithm implementation accent file server algorithm implementation sprite freeze free algorithm implementation choices bit second ethernet lan 
raw communication speed held constant 
processor speeds vary significantly 
chapter analyzes performance demand page file server algorithms performed steps described chapter speed actual freeze free algorithm implementation reported 
original demand page algorithm implementation accent mention file cache flush operation 
comparison assumes demand page algorithm memory maps files 
demand page algorithm flush dirty cache blocks file server algorithm 
process migration step times come experiment measurements 
freeze free process migration system engage request response message pairs 
adding little overhead round trip time null message ms produces estimate ms confirm new host step 
control exec state step lasts daemon begins daemon completes insertion process control execution state 
system overlaps old host new host transmission data lan 
comparison uses time new host completes code heap stack new host freeze free algorithm 
algorithms demand page code heap stack page 
flush operation processes message message protocol stack time dominated actual transmission 
comparison assumes file descriptors fit page 
table shows costs steps involved demand page file server freeze free algorithms 
table effectively provides normalized process migration latency costs formulas algorithms 
step demand page file server freeze free ms ms ms confirm new host control exec state communication link file control current code page current heap page current stack page resume process command dirty stack pages dirty heap pages dirty file cache blocks number communications links number files number additional stack pages number dirty heap pages number dirty file cache blocks table normalized comparison process migration latency comparison process migration latency times sample program demand page file server freeze free algorithms 
sample program dirty heap pages dirty file cache blocks open files communication link stack bytes 
process migration latency time ms demand page algorithm ms file server algorithm ms freeze free algorithm 
message freeze times ms demand page algorithm ms file server algorithm 
freeze free algorithm message freeze period 
table summarizes results 
algorithm total time ms msg freeze time ms demand page file server freeze free example process code kb dirty heap kb dirty file cache blocks kb open files communication link stack bytes table comparative process migration latency message freeze times freeze free algorithm new host communication link information file descriptor information 
migrating process accesses communication link files immediately algorithms virtually execution times process migration latency period 
demand page algorithm demand page algorithm process migration latency time times longer freeze free algorithm 
message freeze time short long encounter messages 
principle disadvantage demand page algorithm long term residual dependency old host eliminates fault tolerance 
file server algorithm file server algorithm eliminates residual dependency cost longer message freeze latency times 
worst case message freeze latency times arbitrarily long stretch seconds 
communication failures occur process migrations long message freeze times 
load sharing suffers potentially long flush times prior process resumption new host 
limited numbers dirty heap dirty file cache pages exist file server algorithm provides acceptable tradeoff fault tolerance performance 
freeze free algorithm effectively eliminates message freeze times short process migration latency period provides fault tolerance quickly eliminating residual dependencies old host 
distributed virtual memory distributed virtual memory dvm offers means transferring address space 
process migration system uses page dvm address space transfers dvm approach precisely performance advantages disadvantages demand page algorithm 
dvm approach simply saves process migration implementor need write code transfer memory 
page dvm combined option transfer selected pages times page faults enable dvm provide page transfer strategy freeze free algorithm 
page dvm transfer kernel data structures suffer granularity mismatch cause transfer unneeded bytes 
object dvm tan chap 
support kernel data structure transfers suffer extra messages 
object dvm combined option ship selected objects single transfer eliminate extra overhead 
object dvm approach feasible relevant objects modular self contained 
sprite nwo demonstrated value sharing physical memory pages virtual memory system file cache 
systems require page transfer mechanism 
leads recommendation process migration system distributed file system dvm system share page transfer mechanism 
summary freeze free algorithm outperforms demand page file server algorithms normalizing step speeds process migration latency times message freeze times 
freeze free algorithm achieves performance sacrificing processor fault tolerance 
chapter thesis research seeks demonstrate ways reduce overhead process migration existing systems order magnitude 
new freeze free algorithm process migration dramatically reduces overhead techniques described chapter 
chapter summarizes major accomplishments process migration identifies promising avenues improvements 
process migration latency time prevalent myth process migration latency large process migration generally cost effective 
mullender states process migration remains expensive operation expensive know distributed systems load balancing purposes mul 
thesis research shatters myth achieving order magnitude improvement prior process migration systems 
freeze free process migration system achieves process migration latency times ms kb page system ms kb page system ms kb page system 
way comparison sprite requires ms 
freeze free algorithm reduces number messages sent process migration latency period messages containing combined process control execution state current code page current heap page current stack page 
process migration system supports migration wide class application processes reduce information 
freeze free algorithm allows variations page size process control execution state information 
changes contribute significantly 
freeze free algorithm eliminates request response message pair dependencies process migration latency period 
freeze free algorithm uses pool partially initialized objects shift expensive object creation operations process migration latency period 
freeze free algorithm design systematically reorganizes system objects self contained means design places state information object object 
reduces excision insertion costs 
message freeze time existing process migration systems freeze message processing part process migration 
root cause extended message freeze time entanglement process state message queue state 
freeze free algorithm design separates process state message queue state 
permits message queue continue receiving messages process migrates 
freeze free algorithm freezes message processing 
message queue accepts messages old new host 
message queue transit messages temporarily held suspension new host 
suspension lasts long required transfer message queue short treated page fault disk 
modularity existing operating systems widely scatter process related information 
approach extracting inserting process related information expensive 
approach state process state module holding process state information 
net result action process module holding information process account effects actions entities 
manifests process migration time form extended freeze times process supporting modules 
example message processing freeze time 
new design receives messages parallel process migration 
file descriptor information provides example 
new design enables old host file cache flush proceed parallel execution migrating process new host 
leads general recommendation system decouple process state supporting modules 
load sharing vs overhead effectiveness load sharing distributed system inversely proportional overhead incurred implementing load sharing 
process migration latency operations multiple demand pages network scheduling activities add overhead 
experiments measuring effects overhead load sharing shown graphically portray negative impact overhead performance 
small overheads needed high speedups 
example overhead results speedup processor system overhead results speedup 
large reduction cost process migration latency period reduces role load sharing overhead 
sample test process process migration latency operations cost ms demand pages network cost ms order magnitude larger 
process migration improvement efforts place greater emphasis cross network demand page operations 
schedules processes priority preempt processes newly ready higher priority processes 
high priority process serving page requests waits time slice expiration low priority compute bound application process 
adds delays page request combines produce enormous overhead 
priority process preemption necessary effective load sharing 
fault tolerance freeze free algorithm supports fault tolerance quickly moving process related information old host 
dependency time period directly linearly dependent number dirty pages file cache blocks old host 
dependency period generally longer process migration latency period 
opportunities improvement freeze free algorithm achieves significant performance improvements performance analysis uncovered promising areas improvement 
higher bandwidth network replacement bit second ethernet bit second atm network dramatically reduce transmission times 
sample program higher bandwidth reduce ms ms reduction 
page transfer protocol development protocol transferring pages coupled smarter network interface devices eliminate data copying 
sample program elimination data copying eliminate ms ms reduction 
communication protocol handling sample program communication protocol handling consumes ms ms process migration overhead 
options exist reducing communication protocol handling costs 
new network supporting larger packet sizes eliminate fragmentation costs 
kernel package deep level procedure nesting protocol 
replacing procedure calls line code expansions option 
option redesigning protocol stack reduce layers 
bottom line small improvements area affect process migration significant contribution overhead 
virtual memory sample program virtual memory process initialization operations consume ms ms process migration overhead 
operations consume process migration latency time 
virtual memory operations important smaller processes 
adding address translations consumes ms improving address translations clear target 
opportunities added capabilities process migration system benefit additional features 
freeze free algorithm readily extended support migration processes multiple threads 
integration dvm process migration enable migration processes shared memory regions 
current system combines system library executable load module 
system changed memory resident system library applications 
process migration system migrate library library machines 
related areas high overheads earlier process migration systems severely handicapped effective load sharing 
freeze free algorithm dramatically reduces overhead process migration time reexamine distributed system process scheduling 
systematically compare effectiveness top dozen scheduling policies distributed systems leutenegger systematically compared scheduling multiprocessors leu 
scheduling overhead measured 
process migration added system 
process migration added conjunction distributed system scheduling automatic task placement monitoring facilities 
parallel similar utilities essential widespread acceptance process migration 
summary new freeze free process migration algorithm reduces process migration latency times order magnitude ms kb page systems ms kb page systems ms kb page systems 
thesis identifies additional changes produce major savings 
freeze free algorithm effectively eliminates message freeze times 
freeze free algorithm supports fault tolerance rapidly eliminating dependencies old host 
thesis experiments graphically show reductions load sharing system speedup caused increasing overhead 
high cost previous process migration systems principal barrier widespread process migration load sharing fault tolerance 
freeze free algorithm improvements process migration open way full utilization computational power distributed systems 
bibliography abb accetta baron bolosky golub rashid tevanian young 
mach new kernel foundation unix development 
usenix summer conference pages 
anderson culler patterson team 
case network workstations 
appear ieee micro principles distributed computing aug 
adv arpaci dusseau vahdat liu anderson patterson 
interaction parallel sequential workloads network workstations 
performance evaluation review may 
ae agrawal 
processor sharing nest network computer stations 
st international conference computer workstations pages 
af finkel 
designing process migration facility charlotte experience 
ieee computer september 
ak alonso 
process migration implementation unix system 
usenix winter conference pages 
bhk baker hartman kupfer shirriff ousterhout 
measurements distributed file system 
operating systems review october 
published th symposium operating systems principles 
bsw barak wheeler 
flood prevention mosix load balancing scheme 
ieee cs tc operating systems newsletter winter 
bw barak wheeler 
mosix integrated multiprocessor unix 
usenix winter conference pages 
cab cabrera 
influence workload load balancing strategies 
usenix summer conference pages 
cag 
process suspension resumption unix operating system 
master thesis university illinois 
technical report uiucdcs 
che chen 
unix bsd implementation process suspension resumption 
master thesis university illinois 
technical report uiucdcs 
che cheriton 
distributed system 
communications acm march 
ci campbell islam 
choices parallel object oriented operating system 
agha wegner yonezawa editors research directions concurrent object oriented programming 
cij roy campbell islam ralph johnson panos peter 
choices frameworks refinement 
luis felipe cabrera vincent russo marc shapiro editor object orientation operating systems pages palo alto ca october 
ieee computer society press 
campbell islam 
designing implementing choices object oriented system 
communications acm september 
ck 
taxonomy scheduling general purpose distributed computing 
ieee transactions software engineering feb 
crt campbell tan 
remote procedure calls degrade microkernel vm performance 
submitted usenix 
dca chang 
heavyweight process migration 
third workshop trends distributed computing systems pages 
dcm dasgupta chen menon pearson ramachandran ahamad leblanc khalidi 
design implementation clouds distributed operating system 
computing systems 

mess distributed operating system universe 
second ieee workshop trends distributed computing systems pages 
dasgupta leblanc ahamad ramachandran 
clouds distributed operating system 
ieee computer november 
douglis ousterhout 
process migration sprite operating system 
th international conference distributed computing pages 
douglis ousterhout 
process migration sprite status report 
ieeecs tc operating systems newsletter winter 
douglis ousterhout 
transparent process migration design alternatives sprite implementation 
software practice experience august 
douglis ousterhout kaashoek tanenbaum 
comparison distributed systems amoeba sprite 
computing systems 
dou douglis 
experience process migration sprite 
usenix distributed multiprocessor systems workshop pages 
dou douglis 
transparent process migration sprite operating system 
phd thesis university california berkeley 
dr 
efficient task migrations message driven parallel execution nonshared memory architectures 
international conference parallel processing pages ii 
eager zahorjan 
adaptive load sharing homogeneous distributed systems 
ieee transactions software engineering may 

load balancing nest network workstations 
fall joint computer conference pages 
fa finkel 
process migration mechanism charlotte 
ieee cs tc operating systems newsletter winter 
fbs forin barrera 
shared memory server 
winter usenix pages 
fre freedman 
experience building process migration subsystem unix 
usenix winter pages 
ggi indulska zhu 
study design issues distributed operating systems generalized way 
symposium experiences distributed multiprocessor systems pages 
gos 
distributed operating systems logical design 
addison wesley 
hag hagmann 
process server sharing processing power workstation environment 
th international conference distributed computing systems pages 
lux 
operating systems top persistent object systems birlix approach 
th hawaii international conference system sciences pages 
hp hutchinson peterson 
kernel architecture implementing network protocols 
ieee transactions software engineering january 
isb saltz 
comparative analysis static dynamic load balancing strategies 
international conference parallel processing pages 
jul levy hutchinson black 
fine grained mobility emerald system 
acm transactions computer systems february 
jul jul migration light weight processes emerald 
ieee cs tc operating systems newsletter winter 
kal 
parallel programming charm overview 
technical report 
kc krueger chawla 
stealth distributed scheduler 
th international conference distributed computing systems pages 
kl krueger livny 
comparison preemptive non preemptive load distributing 
th international conference distributed computing systems pages 
kaashoek van renesse van staveren tanenbaum 
flip internetwork protocol supporting distributed systems 
acm transactions computer systems february 
kun kunz 
influence different workload descriptions heuristic load balancing scheme 
ieee transactions software engineering jul 
lc liao campbell 
overcoming bsd networking network interface framework 
submitted usenix 
leu leutenegger 
issues multiprocessor scheduling 
phd thesis university wisconsin madison 
technical report 
lux 
migrating multi threaded shared objects 
th hawaii international conference system sciences pages 
lia liao 
operating system support embedding network subsystems 
master thesis university illinois urbana champaign 
ll litzkow livny 
experience condor distributed batch system 
ieee workshop experimental distributed systems pages 
llm litzkow livny mutka 
condor hunter idle workstations 
th international conference distributed computer systems pages 
lo leland ott 
load balancing heuristics process behaviour 
performance acm sigmetrics pages 
lux lux 
adaptable object migration concept implementation 
acm sigops april 
ma 
process migration virtual memory multicomputer systems 
th hawaii international conference system sciences pages vol 
mad 
object oriented framework file systems 
phd thesis university illinois 
milojicic 
experiences load distribution top mach microkernel 
usenix distributed multiprocessor systems iv pages 
ml mutka livny 
profiling workstations available capacity remote execution 
performance pages 
ml mutka livny 
scheduling remote processing capacity workstation processor bank network 
th international conference distributed computing systems 
ms sunderam 
process migration unix networks 
usenix winter conference pages 
mts towsley stankovic 
analysis effects delays load sharing 
ieee transactions computers november 
mul mullender 
distributed systems 
addison wesley 
mullender van rossum tanenbaum van renesse van staveren 
amoeba distributed operating system 
ieee computer 
mw wah 
automated learning workload measures load balancing distributed system 
international conference parallel processing pages iii 
milojicic 
task migration top mach microkernel 
usenix mach iii symposium pages 
nic nichols 
idle workstations shared computing environment 
th acm symposium operating systems principles pages 
nut 
brief survey systems providing process object migration facilities 
acm sigops operating systems review october 
nwo nelson welch ousterhout 
caching sprite network file system 
acm transactions computer systems february 
ocd ousterhout douglis nelson welch 
sprite network operating system 
ieee computer pages february 
och ousterhout da costa harrison kunze kupfer thompson 
analysis unix bsd file system 
th acm symposium operating systems principles pages 
par partridge 
gigabit networking 
addison wesley 
pm powell miller 
process migration demos mp 
th acm symposium operating systems principles pages 
pw popek walker 
locus distributed system architecture 
mit press 
pwc popek walker chow edwards kline thiel 
locus network transparent high reliability distributed system 
th symposium operating systems principles pages 
russo johnston campbell 
process management exception handling multiprocessor operating systems object oriented design techniques 
conference object oriented programming systems languages applications pages 
technical report 
uiucdcs department computer science university illinois urbana champaign 
rlc liao campbell 
performance single multi processor operating system process subsystem 
submitted usenix 
rom 
probability load balancing success homogeneous network 
ieee transactions software engineering september 
rus russo 
object oriented operating system 
phd thesis university illinois urbana champaign 
rw wong 
task migration algorithm load balancing distributed system 
proceedings annual hawaii international conference system sciences software track pages 
sch 
migration processes files virtual devices mdx operating system 
acm sigops april 
clarke taylor taylor 
software facility load sharing parallel processing workstation environments 
annual hawaii international conference system sciences pages vol ii 
smi smith 
survey process migration mechanisms 
acm operating systems review july 
sms sinha jia park nakano 
galaxy distributed operating system 
ieee computer august 
spj sinha park jia 
process migration galaxy distributed operating system 
fifth international parallel processing symposium pages 
st taylor 
extending computational bandwidth engineering workstations 
th annual international phoenix conference computers communications pages 
str bjarne stroustrup 
programming language 
addison wesley reading massachusetts 
stu stumm 
design implementation decentralized scheduling facility workstation cluster 
nd ieee conference computer workstations pages 
zhu 
implementation process migration amoeba 
th international conference distributed computing systems pages 
tan tanenbaum 
modern operating systems 
prentice hall 
tan tanenbaum 
distributed operating systems 
prentice hall 
tec ross technology 
sparc risc user guide 
semiconductor 
th theimer hayes 
heterogeneous process migration recompilation 
th international conference distributed computing pages 
theimer 
preemptable remote execution facilities loosely coupled distributed systems 
phd thesis stanford university 
tl theimer 
finding idle machines workstation distributed system 
th international conference distributed computing systems pages 
tlc theimer cheriton 
preemptable remote execution facilities system 
th acm symposium operating systems principles pages 
van dijk van 
efficient process migration emps multiprocessor system 
sixth international parallel processing symposium 
wm wang morris 
load sharing distributed systems 
ieee transactions computers march 
wm walker mathews 
process migration aix transparent computing facility 
ieee cs tc operating systems newsletter winter 
walker popek english kline thiel 
locus distributed operating system 
th acm symposium operating systems principles pages 

attacking process migration bottleneck 
eleventh acm symposium operating system principles pages november 

copy process migration system 
phd thesis carnegie mellon university 
technical report cmu cs 
zf zhou ferrari 
measurement study load balancing performance 
th international conference distributed computing pages 
zhou 
experimental assessment resource queue lengths load indices 
usenix winter pages 
zhou 
performance studies dynamic load balancing distributed systems 
phd thesis university california berkeley 
technical report ucb csd 
zhou zheng wang delisle 
utopia load sharing facility large heterogeneous distributed computer systems 
software practice experience december 
vita ellard born december san mateo california 
earned mathematics university washington graduated cum laude 
earned computer science computer engineering stanford university 
earned campbell university 
nearing completion ph computer science university illinois urbanachampaign 
specializing area distributed operating systems evidenced thesis 
ph graduate student university illinois created new algorithm overcomes major obstacles effective process migration moving running programs computers network environment improved performance reliability 
implemented new algorithm performs times faster world previous best effectively eliminates message freeze time 
replaced operating system process subsystem resulting significant kernel performance improvement process locking times faster virtual choices times faster ss choices 
tcp communications application improved data rates transmission reception 
choices multi threaded single multi processor operating system implemented 
project technical advisor sterling software performed technical consulting european site personnel central software team maxi project 
designed developed software maintained large fielded systems lines code installed systems 
maxi assembly language software package world wide sites ties multiple vax pdp utilizing icc 
system connects communication networks transmit receive process information time sensitive 
initiated efforts developed plan standard window gui technologies station access maxi client server architecture 
government contract resulted windows motif package fielded sun dec station access maxi 
regularly field tested new software releases developed changes needed operational sites 
real world hour missions required near constant availability 
site software staff encountered difficulty provided 
solutions spanned operating system kernel code inter task communication device handlers local remote communications real time applications user interfaces microprocessor devices 
trouble shooting example cured erratic terminal reliability identifying overcoming microprocessor code slight physical device timing mismatch 
project delivered reliable software involving tcp ip communications ethernet lan connecting macro fortran software ias pdp software mvs ibm software unix stations 
senior system consultant incorporated managed entire maxi system hq osc germany 
initially delivered system suffered times exceeding 
quickly provided numerous software fixes significantly reduced outages period exceeding year hour operations 
delivered new software profile maintenance subsystem performed system tuning maintenance training 
previous project developed security plan system involving numerous computer systems communication links 
developed unix software packages including utilizing ingres th mi group augsburg germany 
captain army service managed section responsible operating systems dec pdp ibm computers 
led group delivering fortran army standard intelligence plotting system version graphics package pdp ibm honeywell series computers 
provided technical guidance army computer software projects 
implemented macro device handler supporting terminals printers su connected br line interface pdp 
forms mode graphics supported 
