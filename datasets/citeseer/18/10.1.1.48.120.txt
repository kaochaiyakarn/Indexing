maximum likelihood covariant algorithms independent component analysis david mackay university cambridge cavendish laboratory road cambridge cb mackay cam ac uk december draft bell sejnowski derived blind signal processing algorithm non linear feedforward network information maximization viewpoint 
shows algorithm viewed maximum likelihood algorithm optimization linear generative model 
second covariant version algorithm derived 
algorithm simpler somewhat biologically plausible involving matrix inversions converges smaller number iterations 
third gives partial proof folk theorem mixture sources high kurtosis histograms separable classic ica algorithm 
fourth collection formulae may useful adaptation non linearity ica algorithm 
blind separation algorithms blind separation jutten herault comon bell sejnowski attempt recover source signals observations linear mixtures unknown coefficients source signals vs algorithms attempt create inverse post multiplicative factor set examples fxg 
bell sejnowski derived blind separation algorithm information maximization viewpoint 
algorithm may summarised linear mapping wx followed non linear map oe example oe gamma tanh learning rule deltaw gamma zx non linear function mentioned bell sejnowski needed 
parts 
shown bell sejnowski algorithm may derived maximum likelihood algorithm 
independently pointed pearlmutter parra give exciting generalization ica algorithm 
second pointed algorithm covariant covariant algorithm described simpler faster somewhat biologically plausible 
covariant algorithm independently suggested amari 
pearlmutter parra 
third gives partial proof folk theorem mixture sources high kurtosis histograms separable classic ica algorithm 
fourth collection formulae may useful adaptation nonlinearity ica algorithm 
maximum likelihood derivation ica latent variable models statistical models generative models latent variables describe probability distribution observables everitt 
examples latent variable models include mixture models model observables coming superposed mixture simple probability distributions hanson latent variables unknown class labels examples hidden markov models rabiner juang factor analysis helmholtz machines hinton dayan density networks mackay mackay 
note usual latent variables simple distribution separable distribution 
learn latent variable model finding description data terms independent components 
expect independent component analysis algorithm description terms generative latent variable model 
case 
independent component analysis latent variable modelling 
generative model model observable vector fx generated latent variables fs linear mapping simplest derivation results assume number sources equal number observations 
data obtain set observations fx assume latent variables independently distributed marginal distributions jh 
denotes assumed form model assumed probability distributions latent variables 
probability observables hidden variables fx fs jv js jh ffi gamma ji assumed vector generated noise assumption leads bell algorithm 
straightforward give derivation term ffi gamma ji replaced probability distribution mean ji noise distribution sufficiently small standard deviation identical algorithm results 
likelihood function learning data relevant quantity likelihood function jv product factors obtained marginalizing latent variables 
adopt summation convention point example ji ji single factor likelihood jv js ffi gamma ji det gamma ij log jv gamma log det log gamma ij obtain maximum likelihood algorithm find gradient log likelihood 
introduce gamma log likelihood single example may written log jv log det log ij need identities ji log det gamma ij ij ji gamma lm gammav gamma lj gamma im gammaw lj im ij gammav jm lm li define ij oe log da oe indicates direction needs change probability data greater 
may obtain gradient respect ji equations ji log jv gammaw ij gamma alternatively derivative respect ij ij log jv ji choose change ascend gradient obtain precisely learning algorithm bell sejnowski equation 
recall scalars ds ffi gamma vs examples help explain generative modelling viewpoint figures illustrate typical distributions generated independent components model components cosh cauchy distributions 
shows samples cauchy model 
cauchy distribution heavy tailed gives clearest picture predictive distribution depends assumed generative parameters best known special cases 
nonlinearity 
oe gammaa implicitly assuming gaussian distribution latent variables 
known algorithm works non linearities algorithm linear output obtained second order decorrelation 
equivalently gaussian distribution latent variables invariant rotation latent variables evidence favouring particular alignment latent variable space 

tanh nonlinearity 
oe gamma tanh implicitly assuming cosh gammas heavier tailed distribution latent variables gaussian distribution 
heavier tails possibility 

tanh nonlinearity gain fi oe gamma tanh fia 
fi varies implied probabilistic model changes cosh fis fi limit large fi non linearity step function probability distribution distribution exp 
limit fi approaches gaussian mean zero variance fi 
covariant simpler faster learning algorithm derived learning algorithm performs steepest likelihood function 
designers learning algorithms advocate principle covariance says colloquially consistent algorithm give results independent units quantities measured knuth 
prime example non covariant algorithm popular steepest rule 
dimensionless objective function defined derivative respect parameters computed changed rule deltaw popular equation dimensionally inconsistent left hand side equation dimensions right hand side dimensions 
defense dimensional constant untenable parameters dimensions 
behaviour learning algorithm covariant respect linear rescaling vector algorithm covariant form deltaw ii matrix element dimensions 
obtain matrix 
sources matrices metrics curvatures 
illustration generative models implicit learning algorithm 
distributions observables generated cosh distributions latent variables compact distribution gamma gamma broader distribution 
contours generative distributions latent variables cauchy distributions 
learning algorithm fits object empirical data way maximize likelihood 
contour plot adequately represent heavy tailed distribution 
part tails cauchy distribution giving contours times density origin 
data generative distributions illustrated 
tell 
samples created fell plotted region 
metrics curvatures natural metric defines distances parameter space matrix obtained metric 
natural choice 
special case known quadratic metric defining length vector matrix obtained quadratic form 
example length natural matrix steepest appropriate 
way finding metric look curvature objective function defining 
matrix gamma give covariant algorithm algorithm newton algorithm recognize alleviate principle difficulties steepest slow convergence minimum objective function ill conditioned 
newton algorithm converges minimum single step quadratic 
problems may curvature consists data dependent terms data independent terms case choose define metric terms gull 
resulting algorithm covariant implement exact newton step 
obviously covariant algorithms unique choice 
covariant algorithms small subset set algorithms 
maximum likelihood problem evaluated gradient respect gradient respect gamma bell sejnowski chose perform steepest procedure covariant 
construct alternative algorithm covariant help curvature log likelihood 
second derivative log likelihood respect obtain terms data independent ji kl gammav jk li second data dependent kl ffi ik sum derivative tempting drop data dependent term define matrix gamma ij kl jk li 
matrix positive definite nonpositive eigenvalue poor approximation curvature log likelihood positive definite neighbourhood maximum likelihood solution 
consult data dependent term inspiration 
aim find convenient approximation curvature obtain covariant algorithm necessarily implement exact newton step 
average value ffi ik true value omega ffi ik ff jm ln ffi ik severe approximations replace value replace correlated average hs hs hz sigma mnd sigma variance covariance matrix latent variables assumed exist typical value curvature log da sources assumed independent sigma diagonal matrices 
approximations motivate matrix gamma ij kl jm sigma ln ffi ik ij kl mj sigma gamma mn nl ffi ik gamma simplicity assume sources similar sigma homogeneous 
algorithm covariant respect linear rescaling data respect linear rescaling latent variables 
problems assumptions hold straightforward retain inhomogeneous sigma ij kl mj ml ffi ik multiplying matrix gradient equation obtain covariant learning algorithm deltaw ij gamma ij delta notice expression require inversion matrix additional computation computed single backward pass weights compute quantity terms covariant algorithm reads deltaw ij ij comment regarding value dimensionless quantity assuming implement covariant algorithm line 
held constant value implicitly solving weighted maximum likelihood problem exponential weighting data 
wants data points receive equal weight parameters converge limiting value go asymptotically number current data point 
comments covariant algorithm simpler bell sejnowski algorithm dependence requires matrix inversion 
algorithm dimensionally consistent somewhat closer biological plausibility 
key points 
bell sejnowski algorithm non local involves matrix inversion 
covariant algorithm local involving single extra back propagation 
bell sejnowski algorithm robust parameter matrix value invertible gradient diverge 
contrast covariant algorithm singular behaviour 

algorithm behaved bell sejnowski modifications data transforming representation data homogenous second order properties batches training 
modifications algorithm poorly conditioned 
steps introduce lag learning process 
preprocessing step interpreted way making steepest algorithm covariant 
quicker data covariant algorithm 

note want solve real application continuously adapting time preprocessing requirement kind cumbersome 
demonstration simple comparison time taken algorithms solve theta decorrelation problem data 
data generated distribution sources source twice loud source microphone times loud microphone 
practical purposes wish momentum batching data 
simplicity momentum batches size 
batch size bell sejnowski 
parameter optimized trial error single value giving fastest convergence set go results shown figures 
notice faster covariant algorithm note bell sejnowski algorithm 
note scatterplots show algorithm converged perfect solution 
easily detected mixing sources 
robustness choice nonlinear function literature independent component analysis contains folk theorem algorithm robust details sources separated particular gamma tanh function give results sources large kurtosis sources sufficiently heavy tails 
give partial proof folk theorem 
theorem general folk theorem proven sources limited set high kurtosis distributions sources having distribution 
slightly general folk theorem shows tanh function gain fi including step function robustness property 
loss generality assume true weights question ica find alternative achieves greater likelihood 
theorem asserts apart trivial cases achieves lesser expected log likelihood 
theorem data generated sources having symmetric density gammas 
define 
data modelled det wq wx log ds gamma tanh fis 
true source density continuous density satisfies heavy tail condition ds log ds matrix determinant theta identity matrix log xji log equality holding trivial cases rotation reflection 
comment constraint matrix determinant weaken theorem 
claims find non trivial solution det delta stretch axes space factor delta theorem says replacing expected value likelihood function increased 
likelihood maximized proportional identity matrix 
iterations iterations source source time course weights covariant algorithm 

scatter plot outputs versus true source signals example weights 
iterations source source iterations source output source time course weights bell sejnowski algorithm batch size 
faster learning rates proved unstable 
scatter plot outputs versus true source signals weights 
note comparing horizontal time axis times longer 
batch size 
faster learning rates proved unstable 
proof proof proceeds steps decomposition theta transformation determinant degrees freedom represented terms pure gammai pure rotations cos gamma sin sin cos gamma gamma zero 
ignore optional inversion 
step show probability distribution rotations biggest likelihood achieved shear transformation 
second step show rotations rotation gives greatest likelihood 
zero shear best fixed 
show values achieves greatest likelihood 
transforming basis shear diagonal gammai density transforms cos sin cos gamma sin transforms cosh cos gamma sin gammai cosh cos gammai sin cosh ce gamma se gammai cosh ce gammai se cos sin independent prove maximum likelihood shear property need density invariant rotation degrees 
gammay 
integral interest log rewritten log log gamma log cosh ce gamma se gammai cosh ce gammai se cosh ce se gammai cosh gammai se ji gamma log hi cosh cy sy gamma gammai cosh cy gamma sy gammai jj cosh cy sy gammai cosh sy gamma cy gamma gammai jji cosh cosh cosh cosh gamma 
examining product inside logarithm notice cosh cy sy gamma gammai achieves minimum value cy sy case particular term independent cosh cy gamma sy gammai achieves minimum value gammai minimized cy gamma sy case particular term independent similarly terms minimized 
non zero argument logarithm minimized value 
maximum likelihood 
zero rotation best having proved maximum likelihood need prove maximum likelihood form multiple 
restrict attention cases cases reduced interval 
parameterize space oe cos oe sin oe notation oe write oej cos oe sin oe cosh 
crucial property required oe cos oe sin oe decreasing function oe oe satisfies condition theorem 
proof cos oe sin oe oe log cos oe sin oe gamma dx log sin oe dx log cos oe ae gamma dx log dx log oe ds log ds example cosh satisfies heavy tail condition oej decreasing function oe oe 
notice transformation point xa oe mapped ya oe point xb gamma oe mapped yb gammaoe points satisfy oej gamma gamma oej oej 
proof gamma gamma oej cos gammaoe sin gammaoe cos oe sin oe oej symmetries obtain gamma dr gamma oe gamma doe oe gamma oe log oej gamma log oe range integral terms oe gamma oe log oej gamma log oe positive monotonic proved equation 
completes proof 
general theorem proved relies moment properties heavy defined terms smoothness properties 
learning nonlinearity conclude discussing learn density latent variables 
bell sejnowski discuss concept learning nonlinearity don give explicit algorithm doing 
construct family parameterized distributions explicitly normalized differentiate log likelihood 
example parameterization non linearity student distribution xj gamma gamma derivative oe log xj dx gamma learning achieved gradient log xj psi gamma psi gamma ln gamma digamma function psi dx log gamma 
approximation accurate mackay peto psi gamma psi log learning tanh non linearity distribution fi cosh fix fi fi increases distribution tends distribution 
quantity learning algorithm log dx gammad dx fi log cosh fix gamma tanh fix normalizing constant fi numerical approximation log fi log fi 
formula get log log fi fi learn fi need derivative log dfi fi log cosh fix fi gamma fi tanh fix non equal dimensionalities real cocktail party problem challenge addressed 
learning algorithm case fewer sources measurements define generative model pseudoinverse gamma replace ffi function narrow gaussian distribution 
assume simplicity noise component variance oe fi 
likelihood function js assume noise level oe sufficiently small term js sharp peak dominates integral 
term product gaussian integral log likelihood single term 
mackay bretthorst green mackay log jv log fi gamma fi gamma gamma log det log mp mp wx expression may differentiated obtain learning rule may interesting pursue alternative algorithm assume ability compute pseudoinverse progress robert mackay graeme mitchison helpful discussions 
supported gatsby charitable foundation 
amari cichocki yang new learning algorithm blind signal separation 
appear nips 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neural computation 
bretthorst 
bayesian spectrum analysis parameter estimation 
springer 
comon jutten herault 
blind separation sources 
problems statement 
signal processing 
dayan hinton neal zemel 
helmholtz machine 
neural computation 
everitt 
latent variable models london chapman hall 
green mackay 
bayesian analysis linear phased array radar 
maximum entropy bayesian methods santa barbara ed 
pp 
dordrecht 
kluwer 
gull 
developments maximum entropy data analysis 
maximum entropy bayesian methods cambridge ed 
skilling pp 
dordrecht 
kluwer 
hanson stutz cheeseman 
bayesian classification correlation inheritance 
proceedings th international joint conference artificial intelligence sydney australia volume pp 

morgan kaufmann 
horn hopfield 
decomposition mixture signals model olfactory bulb 
proceedings national academy sciences united states america 
hinton dayan frey neal 
wake sleep algorithm unsupervised neural networks 
science 
jutten herault 
blind separation sources 
adaptive algorithm neuromimetic architecture 
signal processing 
knuth 
art computer programming 
volume fundamental algorithms reading mass addison wesley 
mackay 
bayesian interpolation 
neural computation 
mackay 
bayesian neural networks density networks 
nuclear instruments methods physics research section 
mackay 
density networks application protein modelling 
maximum entropy bayesian methods cambridge ed 
skilling pp 
dordrecht 
kluwer 
mackay peto 
hierarchical dirichlet language model 
natural language engineering 
pearlmutter parra context sensitive generalization ica 
appear iconip 
available www cnl salk edu bap papers iconip cica ps gz 
rabiner juang 
hidden markov models 
ieee assp magazine pp 

