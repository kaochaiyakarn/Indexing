platt smo algorithm svm classi er design keerthi bhattacharyya murthy ssk guppy mpe nus edu sg csa ernet csa ernet murthy csa ernet technical report cd control division dept mechanical production engineering national university singapore singapore ph revised version report preparation submission journal 
welcome comments suggestions improving report 
points important source confusion ineciency platt sequential minimal optimization smo algorithm caused single threshold value 
clues kkt conditions dual problem threshold parameters employed derive modi cations smo 
modi ed algorithms perform signi cantly faster original smo benchmark datasets tried 
past years lot excitement interest support vector machines yielded excellent generalization performance wide range problems 
fast iterative algorithms easy implement suggested 
platt sequential minimization algorithm smo important example 
remarkable feature smo extremely easy implement 
comparative testing algorithms done platt shown smo faster better scaling properties 
enhance value smo 
particular point important source confusion ineciency caused way smo maintains updates single threshold value 
getting clues optimality criteria associated kkt conditions dual suggest threshold parameters devise modi ed versions smo remove confusion associated smo ecient original smo 
computational comparison number benchmark datasets shows modi cations perform signi cantly faster original smo situations 
ideas mentioned applied smo regression algorithm 
report results extension 
organized follows 
section brie discuss svm problem formulation dual problem associated kkt optimality conditions 
point conditions lead proper criteria terminating algorithms designing svm classi ers 
section gives short summary platt smo algorithm 
section point problem associated way smo uses single threshold value describe modi ed algorithms section 
computational comparison done section 
appendix gives pseudo codes smo modi cations 
pseudo codes similar smo platt 
short easy develop working code svm design 
svm problem optimality conditions 
basic problem addressed category classi cation problem 
tutorial burges gives overview solution problem svms 
denote input vector support vector machine denote feature space vector related transformation 
svm designs assume known computations done kernel function 

denotes inner product space 
denote training set th input pattern corresponding target value means class means class 

optimization problem solved support vector machine min kwk subject 
problem referred primal problem 
lagrangian problem kwk 
kkt optimality conditions 
refer lagrange multipliers 
de ne wolfe duality theory shown obtained solving dual problem maxw 
subject obtained primal variables easily determined kkt conditions mentioned earlier 
possible solution non unique instance take boundary values possible unique 
numerical approach svm design solve dual primal nite dimensional optimization problem 
note 

derive proper stopping conditions algorithms solve dual important write optimality conditions dual 
lagrangian dual 
de ne 
kkt conditions dual problem conditions simpli ed considering cases 
case 
case 
case 
de ne index sets fi cg fi fi cg fi cg fi 
note index sets depend necessary conditions rewritten de ne minff low optimality conditions hold low easy see close relationship threshold parameter primal problem multiplier particular optimality identical 
rest denote quantity 
say index pair de nes violation sets conditions holds note optimality conditions hold exist index pair de nes violation 
numerical solution usually possible achieve optimality exactly need de ne approximate optimality conditions 
condition replaced low positive tolerance parameter 
pseudo codes appendix parameter referred tol 
correspondingly de nition violation altered replacing optimality mentioned mean approximate optimality 
placed halfway low approximate optimality conditions hold exists satis ed margin approximate optimality conditions employed platt joachims 
argued soundness approximate conditions stopping criterion dual algorithms 
platt smo algorithm 
number algorithms suggested solving dual problem 
traditional quadratic programming algorithms active set method interior point algorithms suitable large size problems reasons 
require kernel matrix computed stored memory 
requires extremely large memory 
second methods involve expensive matrix operations cholesky decomposition large submatrix kernel matrix 
third practitioners develop implementation svm classi er coding algorithms dicult 
attempts develop methods overcome problems 
vapnik observation number support vectors small known directly solve reduced problem involving support vectors deal signi cantly larger datasets 
support vectors known set vectors chosen chunked memory resulting problem solved 
remaining vectors tested optimality violate included 
process repeated solution obtained 
referred chunking algorithm 
number support vectors large chunking algorithm unsuitable 
osuna suggested subset vectors working subset optimize corresponding freezing 
arguments osuna convergence algorithm incorrect expected algorithm converge asymptotically number steps goes nity 
joachims developed ecient algorithm svm building basic idea 
platt suggested algorithm sequential minimal optimization smo puts subset selection osuna algorithm extreme iteratively selecting subsets size 
note presence equality constraint see variables need chosen optimization take step 
platt computational experiments shown smo faster chunking algorithm scales better problem size grows 
smo algorithm fares better joachim algorithm 
give brief description smo algorithm 
working set size equality constraint eliminate lagrange multipliers optimization problem step quadratic minimization just variable 
straightforward write analytic solution 
complete details derived 
procedure part pseudocode gives clear description implementation 
need recall details 
important comment role threshold parameter de ne output error th pattern consistent pseudocode call indices multipliers chosen optimization step look details shows take step varying need know knowledge value needed take step 
method followed choose step crucial ecient solution problem 
number experiments platt came set heuristics 
employs loop approach outer loop chooses chosen inner loop chooses outer loop iterates patterns violating optimality conditions rst lagrange multipliers upper lower boundary satis ed patterns violating optimality conditions ensure problem solved 
clearly algorithm spends large fraction time adjusting multipliers take non boundary values small amount time multipliers take boundary values 
appropriately platt maintains updates cache values indices corresponding non boundary multipliers 
remaining computed needed 
see smo algorithm chooses aim large increase objective function 
expensive try possible choices choose gives best increase objective function index chosen maximize je 
de ne real parameter denotes change values corresponding lagrangian multiplier vector je 
available cache non boundary multiplier indices indices initially choice choice yield sucient progress steps taken 
starting randomly chosen index indices corresponding non bound multipliers tried choices 
sucient progress possible indices tried choices starting randomly chosen index 
choice random seed ects running time smo see example computational costs mentioned section 
value needed take step needed employed checking optimality 
smo algorithm updated step 
step involving takes non boundary value exploited update value rare case happen exists interval say low admissible thresholds situation smo simply chooses low 
section see problems caused choice 
problems smo algorithm 
smo carefully organized algorithm excellent computational eciency 
way computing single threshold value get confused state inecient 
illustrate rst issue numerical example 
example 
consider example patterns kernel matrix suppose start usual point smo starts 
calculating get 
indices violate optimality conditions 
note low smo uses check optimality conditions 
suppose smo chooses indices optimization keeping xed 
easy check leads point 
new point 
note low optimality conditions satis ed 
smo chooses 
value check optimality third training pattern shows violation optimality criterion employed platt violation 
note chosen interval ensured veri cation 
example clearly sums rst concern 
smo constrains unnecessarily particular single choice threshold gets trouble especially termination 
issue raised appears somewhat pathological presence single index forces unique really serious problem 
note takes certain extreme values little possibility having index 
point practical problem ineciency 
instant smo algorithm xes current indices optimized 
checking remaining examples violate optimality quite possible di erent shifted choice may better job 
smo algorithm quite possible reached value optimality satis ed smo hasn detected identi ed correct choice quite possible particular index may appear violate optimality conditions employed incorrect value index may able pair de ne violation 
situation smo algorithm expensive wasteful search looking second index take step 
believe major source ineciency smo algorithm 
modi cations smo algorithm 
section suggest modi ed versions smo algorithm overcomes problems mentioned section 
see computational evaluation section modi cations better original smo algorithm situations give quite remarkable improvement eciency tested benchmark problems 
short modi cations avoid single threshold value checking optimality 
threshold parameters low maintained employed checking optimality 
modi cations adequately described pseudo codes appendix 
give additional pointers help give easy understanding pseudo codes 
assume reader familiar pseudo codes 

suppose instant available low indices low low minff checking particular optimality easy 
example suppose check low condition holds violation case smo procedure applied index pair low 
similar steps indices sets 
approach checking optimality choice second index go hand hand original smo algorithm 
see compute low low ecient updating process 

ecient smo algorithm spend ort altering cache maintained updated eciently 
optimality holds examine indices optimality 

extra steps added procedure 
successful step pair indices fi compute partially low low 
note extra steps inexpensive cache ff available updates easily done 
careful look shows just involved successful step sets non empty partially computed low low null elements 
low take values fi choices subsequent step see item keep values cache 

working loop note holds point implies optimality holds far concerned 
mentioned item choice low uenced indices gives easy way exiting loop 

ways implementing loop involving indices 
method 
line done smo 
loop check optimality violated choose appropriately 
example low violation case choose low 
method 
worst violating pair choose low 
depending methods call resulting modi cation smo smo modi cation smo modi cation 
optimality holds said come back check optimality indices 
loop indices 
low low partially computed update quantities examined 
computed rst optimality checked current low low 
example low violation case take step low 
hand violation modi ed 
suppose described 
happens violation loop having 
conclude optimality holds 
answer 
easy see argument 
suppose contradiction exist pair de ne violation satisfy 
say satis ed optimality check described implementation earlier seen ected calculation low settings 
words mistakenly taken having satis ed optimality earlier loop detected violating optimality analysed 
holds possible indices satisfy optimality checks 
furthermore holds loop indices completed true values low de ned computed indices encountered 
computational comparison 
section compare performance modi cations original smo algorithm 
implemented methods fortran ran mhz pentium machine 
value experiments 
standard problems testing wisconsin breast cancer data spirals data checkers data set wisconsin breast cancer spirals checkers adult adult adult web web web table data set properties 
data uci adult data web page classi cation data 
checkers data created random set points checkers grid see data sets downloaded sites mentioned full training 
division training validation test sets 
case adult data set inputs represented special binary format platt testing smo 
study scaling properties training data grows platt staged experiments adult web data 
data rst fourth seventh stages 
gaussian kernel exp kx experiments 
values employed dimension input number training points table 
values table chosen follows 
adult web data values platt experiments smo data chose suitably get generalization 
particular method svm design value usually unknown chosen trying number values validation set 
performance method range values important 
problem tested algorithms appropriate range values 
cost updating cache dominant part computational cost 
total number kernel evaluations indicator computing cost 
measure pretty independent computing environment easy developing new algorithms compare methods ones studied running methods 
tables total number kernel evaluations various problems tried 
point ect choice random seed cost associated original smo algorithm reported costs random seeds 
haven done web data data change random seed ect computational cost 
smo modi cations require random seed 
clear modi cations outperform original smo algorithm 
situations improvement eciency remarkable 
modi cations second fares better 

pointed important source ineciency platt smo algorithm caused operation single threshold value 
suggested modi cations smo algorithm overcome problem eciently maintaining updating threshold parameters 
computational experiments show modi cations speed smo algorithm considerably situations 
platt established smo algorithm fastest algorithms svm design 
modi ed versions smo enhance value smo algorithm 
ideas mentioned svm classi cation extended smo regression algorithm 
report results extension 
smo smo smo smo random seed random seed modi cation modi cation table wisconsin breast cancer data number kernel evaluations bennett mangasarian robust linear programming discrimination linearly inseparable sets optimization methods software vol pp 
burges tutorial support vector machines pattern recognition data mining knowledge discovery vol number 
support vector networks kernel bias soft margin tech 
report university dept automatic control systems engineering england 
joachims making large scale support vector machine learning practical sch olkopf burges smola 
advances kernel methods support vector machines mit press cambridge ma december 
kaufman solving quadratic programming problem arising support vector classi cation sch olkopf burges smola 
advances kernel methods support vector smo smo smo smo random seed random seed modi cation modi cation table spirals data number kernel evaluations smo smo smo smo random seed random seed modi cation modi cation table checkers data number kernel evaluations smo smo smo smo random seed random seed modi cation modi cation table adult data number kernel evaluations smo smo smo smo random seed random seed modi cation modi cation table adult data number kernel evaluations smo smo smo smo random seed random seed modi cation modi cation table adult data number kernel evaluations smo smo smo modi cation modi cation table web data number kernel evaluations smo smo smo modi cation modi cation table web data number kernel evaluations smo smo smo modi cation modi cation table web data number kernel evaluations machines mit press cambridge ma december 
keerthi bhattacharyya murthy fast iterative nearest point algorithm support vector machine classi er design tech 
report tr isl intelligent systems lab dept computer science automation indian institute science bangalore india march 
see guppy mpe nus edu sg mangasarian successive overrelaxation support vector machines tech 
report computer sciences dept university wisconsin madison wi usa 
osuna freund girosi improved training algorithm support vector machines principe giles morgan wilson editors neural networks signal processing vii proceedings ieee workshop pp new york ieee 
platt fast training support vector machines sequential minimal optimization sch olkopf burges smola 
advances kernel methods support vector machines mit press cambridge ma december 
platt adult web datasets 
www research microsoft com platt sparseness analytic qp speed training support vector machines advances neural information processing systems kearns solla cohn eds mit press 
keerthi bhattacharyya murthy improved versions smo algorithm svm regression tech 
rept dept mech 
prod 
engrg national university singapore singapore aug preparation 
smola sch olkopf tutorial support vector regression neurocolt technical report tr royal holloway college london uk 
spirals data 
ftp ftp cs cmu edu pub neural bench bench spirals tar gz vapnik estimation dependences empirical data 
springer verlag berlin 
vapnik nature statistical learning theory 
springer verlag new york 
wisconsin breast cancer data 
ftp pub machine learning databases breast cancer wisconsin appendix 
pseudo codes modi ed smo algorithms 
pseudo codes improved smo algorithms 
statements starting denote comments 
target desired output vector point training point matrix cache vector fi values note definition fi different ei platt smo algorithm 
fi subtract threshold 
procedure procedure platt smo pseudo code 
return alph lagrange multiplier target compute return kernel point point kernel point point kernel point point eta eta alph eta objective function objective function eps alph alph return alph alph update weight vector reflect change linear svm update new lagrange multipliers store alpha array update simply achieved keeping updating information alpha 
target gives information index set belongs 
update compute updated values alph alph alph alph compute low low applying equations indices see item section 
return procedure target alph lagrange multiplier compute set update low low low low low check optimality current low violated find index joint optimization optimality low tol optimality low tol optimality optimality return choose better low low return return main routine modification initialize alpha array zero initialize index class initialize low low index class set low loop training examples loop easy check optimality attained blow tol exit loop setting main routine modification initialize alpha array zero initialize index class initialize low low index class set low loop training examples loop difference smo modifications 
modification inner loop selects sequentially set current low set current clearly corresponds choosing worst violating pair members indices 
inner loop success blow tol inner loop success low target alph lagrange multiplier inner loop success low inner loop success 
