international computer science institute center st ffl suite ffl berkeley california ffl ffl fax unsupervised learning dyadic data thomas hofmann international computer science institute berkeley ca computer science division uc berkeley hofmann cs berkeley edu jan puzicha institut fur informatik iii university bonn germany jan cs uni bonn de tr december dyadic data refers domain finite sets objects observations dyads pairs element set 
includes event occurrences histogram data single stimulus preference data special cases 
dyadic data arises naturally applications ranging computational linguistics information retrieval preference analysis computer vision 
systematic domain independent framework unsupervised learning dyadic data statistical mixture models 
approach covers different models flat hierarchical latent class structures unifies probabilistic modeling structure discovery 
mixture models provide parsimonious flexible parameterization probability distributions generalization performance sparse data structural information data inherent grouping structure 
propose annealed version standard expectation maximization algorithm model fitting empirically evaluated variety data sets different domains 
past decade learning data highly active field research distributed disciplines ranging pattern recognition neural computation statistics machine learning data mining 
domain independent learning architectures underlying theories learning focusing feature data representation vectors points euclidean space 
restricted case substantial progress achieved 
focus metric data disregarded variety important problems fit setting 
examples received attention proximity data hb replace metric distances weaker notion pairwise similarities ranked preference data css 
variety types non metrical data example psychometric literature coo kru ca particular context multidimensional scaling correspondence analysis 
dyadic data introduce general framework unsupervised learning dyadic data 
notion dyadic refers domain sets objects fx fy observations dyads 
simplest case focus sequel elementary observation consists occurrence cases may provide scalar value strength preference association rating 
cases additional observations features objects may available restrict attention case purely dyadic observations 
exemplary application areas dyadic data ffl computer vision particular context image segmentation corresponds image locations discretized categorical feature values dyad denotes occurrence feature particular image location 
ffl text information retrieval corresponds document collection vocabulary dyad represents occurrence token content document 
ffl computational linguistics corpus statistical analysis word occurences applications probabilistic language modeling word clustering ptl word sense disambiguation hin dlp discrimination sch automated thesaurus construction sp 
ffl preference analysis consumption behavior identifying individuals objects 
dyads correspond single stimulus preferences 
type data starting point machine learning technique known collaborative filtering 
modeling goals principles different domains main tasks play fundamental role unsupervised learning dyadic data probabilistic modeling learning joint conditional probability model theta ii structure discovery identifying clusters data hierarchies 
glance may statistical models dyadic data trivial 
object sets nominal scale empirical occurrence frequencies sufficient statistics capturing know data 
intrinsic difficulty modeling dyadic data data sparseness known zero frequency problem goo goo kat wb 
product set theta large majority pairs small probability observed sample 
empirical frequencies typically zero significantly corrupted sampling noise 
sparseness problem urgent case higher order occurrences triplets quadruples observed gram language modeling 
typical state art techniques natural language processing apply smoothing deal zero frequencies unobserved events 
prominent techniques example katz back method kat model interpolation held data jm jel similarity smoothing techniques es dlp 
contrast propose model statistical approach family latent class finite mixture models tsm mb principled approach deal data sparseness problem 
mixture clustering models dyadic data investigated titles class gram models bdm distributional clustering ptl aggregate markov models sp natural language processing 
approaches recovered special cases general learning framework 
close relation clustering methods qualitative data information clustering approach cf 

modeling principle latent variable models specification joint probability distribution latent observable variables 
unifies statistical modeling structure detection probabilistic model observables obtained marginalization bayes rule induces posterior probabilities latent space structures observations 
crucial exploratory data analysis extraction natural structures data groups data hierarchies essential goals 
latent classes clusters common trait mixture models assumption underlying latent class cluster membership observations sets observations 
formally starting point observation sequence nn realization underlying sequence random variables nn different possibilities define flat latent class models dyadic data ffl direct way introduce unobserved latent class observation dyad associated latent variable finite set fa ak realization nn effectively partitions observations classes 
type model called aspect observations sharing class simply referred aspect 
ffl alternatively latent classes introduced objects spaces resulting model referred sided asymmetric clustering model 
loss generality consider latent variables fc denote realization partitions reversing role results different model 
ffl latent classes defined sets simultaneously 
latent variables fc partition variables fd dlg partition type model called sided symmetric clustering model 
difference aspect clustering models emphasized 
latent class structure aspect model partitions observations clustering models provide group structure object spaces 
consequence identical observations may different latent classes aspect models latent variables shared sets observations clustering models 
underlying latent class structures different case show sequel clustering models derived constrained aspect models 
rest organized follows section give overview different mixture models dyadic data including extensions hierarchical structured latent class follow convention utilize subscript indices unique names members sets superscript indices number observations 
graphical model representation aspect model symmetric parameterization asymmetric parameterization 
circles denote random variables bold circles correspond observed quantities ellipses represent parameters rectangular frames utilized multiple instances 
directed graph structure represents conditional independence properties standard way 
models derive expectation maximization em algorithms maximum likelihood model fitting 
section discusses generalizations modifications pure maximum likelihood framework annealed em cross validation number acceleration techniques 
variety experimental results different application domains section 
mixture models dyadic data aspect model model specification aspect model built assumption occurrences sample pairs random variables conditionally independent respective latent class randomized data generation process described follows 
choose aspect probability 
select object probability xja 
select object probability yja 
corresponding complete data probability joint probability data hypothetical instantiation latent variables ja ja graphical model representation aspect model depicted 
summing possible realizations latent variables grouping identical dyads obtains usual mixture probability distribution observables xja yja jf represents empirical occurrence frequencies 
corresponding marginal counts objects denoted respectively 
expectation maximization algorithm maximum likelihood estimation requires maximize log likelihood log respect model parameters concatenated vector notational convenience 
overcome difficulties maximizing log sum apply expectation maximization em framework dlr mk alternate reestimation steps ffl expectation step estimating posterior probabilities unobserved mapping ajs parameter estimate ffl maximization step involves maximization expected complete data log likelihood ajs log posterior probabilities respect 
em algorithm known increase observed likelihood step converges local maximum mild assumptions mk 
aspect model contains continuous parameters xja yja 
step equations class posterior probabilities aspect model derived bayes rule pfa ajx xja yja xja yja sequel simply denoted ajx 
straightforward derive step re estimation formulae differentiating different components 
introducing appropriate lagrange multipliers ensure correct normalization obtains ajx xja ajx ajx yja ajx ajx notice unnecessary store delta posteriors step efficiently interleaved 
asymmetric parameterization cross entropy minimization achieve better understanding aspect model elucidating switch equivalent asymmetric parameterization cf 
yjx ajx yja arrives interpretation aspect model terms low dimensional representation conditional probabilities yjx xjy symmetry 
class conditionals yja interpreted non negative dimensional vectors span subspace dimensionality precisely span sub simplex simplex multinomial distributions 
determination ajx equivalent optimal approximation yjx subspace 
view parameters ajx correspond coordinates subspace spanned fp kk reversing role equivalent dual formulation obtained 
fact shows aspect model conditional distributions yjx fixed obtained convex combination prototypical class conditionals yja 
notice estimation decoupled parameters 
maximizing joint predictive likelihood equivalent 
known general maximum likelihood estimation equivalently stated minimization cross entropy kullback leibler divergence empirical distribution model distribution 
case occurrence data empirical distribution asymmetric parameterization may rewrite log likelihood log ajx yja log yjx log ajx yja estimation carried independently maximum likelihood estimation remaining parameters equivalent minimizing sum cross entropies empirical conditional distributions yjx model distributions yjx ajx yjx weighted 
symmetry model equivalent decomposition obtained interchanging role sets probabilistic factor analysis discrete data dimensionality reduction obtained aspect model similar spirit factor analysis bar 
factor analysis occurrence data known latent semantic analysis lsa ls 
lsa occurrence matrix counts decomposed singular value decomposition svd sigmav orthogonal matrices diagonal matrix sigma containing singular values approximation obtained thresholding largest singular values zero rank optimal sense matrix norm 
similarly aspect model represented matrix decomposition ja ja sigma diag joint probability model matrix notation written product sigma weighted sum outer products rows reflects conditional independence assumption factors correspond mixture components 
mixing proportions substitute singular values svd 
important advantages aspect model compared lsa aspect model utilizes statistically meaningful divergence measure cross entropy probability distributions measure quality low dimensional approximation 
contrast lsa squares principle approximation occurrence counts 
equivalent additive gaussian noise assumption adequate frequency tables 
ii mixture approximation yields defined probability distribution factors clear probabilistic meaning terms component distributions 
contrast lsa model define properly normalized probability distribution worse may contain negative entries 
regard aspect model understood novel factor analysis approach count data utilizes likelihood objective function additional assumptions non intrinsic loss functions association measures 
sided clustering model model specification asymmetric sided clustering model differs aspect model latent class variables associated object single observations 
clustering model strict sense assumes definite partition space groups 
connection aspect model explicit modify introducing additional latent clustering variables fc latent variable graphical model representation sided clustering model hierarchical clustering model 
states cluster aspects identified clustering variables impose consistency constraints aspect variables ajx pfa ajx cg ffi ac notation ffi ac identified ffi ac 
constraints restrict latent variable space guarantee observations particular share latent aspect identified 
corresponding graphical model representation shown 
notice compared asymmetric parameterization aspect model parameters conditional distributions ajx replaced ajx free parameters determined consistency constraints omitted graphical model representation 
directly read complete data distribution parameterization sided clustering model aspect variables replaced respective clustering variable imposed constraints 
summing latent space clustering variables effectively defines mixture denotes set observations involving notice occurrences independent parameters aspect model coupled shared latent variable 
expectation maximization algorithm straightforward derive em algorithm sided clustering model 
update equations posterior probabilities step pfc cjs contrast step equation aspect model likelihood contributions observations combined calculation posterior probabilities shared variable 
parameter update equation step obtains pfc cjs pfc cjs pfc cjs alternating posterior calculations parameter re estimations defines convergent maximum likelihood procedure 
cross entropy clustering naive bayes classification illuminating rewrite class posterior probabilities pfc cjs exp gamman gamma yjx log comparing standard models gaussian mixture model mb probabilistic vector quantization demonstrates cross entropy empirical conditional probability yjx class conditional serves distortion measure sided clustering model 
notice increasing number observations posteriors surely approach extremal values asymptotic behavior differs aspect model obvious comparing 
eq 
allows give sided clustering model interpretation terms information theory identifying objects sources alphabet conditional probabilities correspond source codes asymptotic expected codeword length gamma log data encoded stages specifying code utilize representing occurring code 
asymptotic average codeword length code exactly cross entropy governs latent class posterior probabilities cf 
ct 
sided clustering model viewed unsupervised version naive bayes classifier give interpretation feature space sample set correspond set feature values assumed conditionally independent 
supervised setting step estimation performed training data class posteriors new data unknown labels computed step equations 
possibilities weaken conditional independence assumption aspect clustering model substituting class conditional multinomial distributions log linear markov models wl whi tree dependency models mj 
direction pursued 
sided clustering model model specification sided clustering model latent structure consists cluster partitionings defined respectively 
conditional model propose defined pfx oe oe ir delta cluster association parameters 
intuitively weights increase decrease probability observing dyad associated cluster pair relative unconditional independence model 
order define proper probabilistic model ensure correct global normalization constrains choice possible cluster association graphical model representation sided clustering model 
parameters 
prior probabilities delta complete model specification 
sided clustering model enforces strict partition provides principled approach infer clustering structures simultaneously sets 
level single occurrence observations simultaneous clustering space implies observation associated latent aspect pair second component identified latent class object respectively 
similar sided clustering model may interpret sided clustering model terms constrained aspect model identifying imposing sets compatibility conditions ajx pfa ajx cg ffi ac dg ffi bd graphical model representation sided clustering model obtain yja xja bayes rule conditional independences implied graph may rewrite xja ajx ajc ffi ac bjd ffi bd results ffi ac ffi bd comparing suggests cluster association parameters oe correspond ratio aspect probabilities product marginal probabilities corresponding aspect pairs 
equivalence formulations sided clustering model maximum likelihood approach cluster associations oe hand constrained aspect model hand proven paragraph 
order correct graphical model demand technical condition cluster empty 
happen xja notice similarly conceptually different 
prior probability object latent class probability latent aspect observation approximate expectation maximization algorithm main difficulty sided clustering model latent mappings coupled cluster association strengths oe 
explicit performing maximization augmented complete data likelihood constitutes step em procedure 
inserting maximum likelihood estimates maximize pfc djs oe log oe pfc djs oe goe gamma respect oe 
solving lagrange multiplier reveals gamman differentiation yields oe pfc djs oe pfc cjs oe ih djs oe straightforward verify expression numerator equivalent step re estimation equation obtain parameters graphical model corresponding formulation starting point em 
terms denominator correspond exactly marginals 
remaining update equations simply pfc cjs oe djs oe follows posterior joint probabilities pfc djs oe computed step order perform exact em likelihood maximization 
preserve tractability model fitting procedure approximate step adequate 
propose perform variational approximation known mean field approximation replace exact computation posteriors step factorial approximation pfc djs oeg cjs djs oeg utilize approximation parameter re estimation step 
approximating probability distribution chosen order minimize kl divergence true posterior distribution cf 
nh 
approximate step equations cf 
appendix cjs oeg exp djs oeg log oe djs oeg exp cjs oeg log oe notice mean field conditions form highly non linear coupled system equations 
solution fixed point iteration alternates computation latent variables space precisely approximate posterior probabilities intermediate solution space vice versa 
alternating computation interleaved re computation oe parameters updating set cjs oeg djs oeg certain term exploited derivation cf 
appendix 
resulting alternation scheme optimizes common objective function step maintains valid probability distribution 
alternatively markov chain monte carlo mcmc methods applied approximate required posterior probabilities 
mean field approximation advantage efficient due deterministic nature 
mutual information clustering elucidating consider hard clustering limit sided clustering model 
substitute continuous parameters step re estimation formula obtain goodness fit measure defined instantiations log oe log identified expected mutual information random variables hard clustering case sided clustering model reduces maximizing mutual information part aspects variables 
sloppy formulation may state mutual information partitions space maximized 
hierarchical clustering model model specification combining aspects clusters aspect model clustering models define non hierarchical flat latent class structure 
especially context structure discovery important find hierarchical data organization 
known learning architectures hierarchical mixtures experts jj fit hierarchical models data 
case dyadic data alternative way define hierarchical model combining aspects clusters 
hierarchical clustering proposed aspects identified nodes inner terminal hierarchy complete binary tree clusters identified terminal nodes 
latent aspect structure nn latent clustering structure introduced 
order enforce hierarchical organization aspects intended tree structures related compatibility constraints ajx pfa ajx cg symbolic expression reads path root 
comparing constraints ones imposed sided clustering model immediately follows reduces degenerated hierarchy inner nodes 
sided clustering model observations involving particular aspect hierarchical model condition weakened observations aspects associated single path tree 
graphical model representation hierarchical clustering model identical sided clustering model difference constraints weakened clustering structure completely determine aspect variables 
freedom introduce additional parameters ajx cf 

addition fully parameterized model parsimonious model ajx ajc entropy maximizing model additional parameters ajx depth provide interesting alternatives 
em approach sided clustering model converges inferior local minimum started random initialization posteriors typically approach uniform distributions 
remedy solution sided clustering model utilized initialize set latent variables 
sequel implicitly assume ajx parameterization ajc model non identifiable degenerates sided clustering model suggest fit mixing proportions weight different abstraction levels hierarchy held data 
summing hidden variables obtains nested mixture formulation hierarchical model yja ajx comparing flat clustering model may notice class conditional probability yja replaced inner mixture mixture vertical axis hierarchy 
expectation maximization algorithm step hierarchical model best performed stage manner 
mixture formulation straightforward generalize posterior probability sided clustering model pfc cjs yja ajx constraints imposed clustering variables posteriors aspect variables conditioned values compute pfa ajx ajx yja jx yja derivation step equations follows standard procedure yields yja pfa ajx ajx pfa ajx pfc cjs completing derivation em algorithm 
discussion point helpful give systematic overview models 
stressed different models parameterization derived imposing constraints basic aspect model 
consequence different models systematically arranged simple venn diagram cf 
partial order distinguishes constrained models 
obviously aspect model constrained model sided clustering model possesses strongly restricted latent aspect space 
way compare different models predictive probabilities new data yjx summarized table 
regard table comments ffl essential difference parameters delta posterior probabilities pf deltag latent variables 
example sided clustering model specific distribution ajx aspects replaced posterior probability pfc having latent class asymptotically approach boolean values necessarily true parameters aspect model 
mixing weights ajx free sided sided sided hierarchical hierarchical clustering model clustering model clustering model clustering model clustering model aspect model venn diagram family occurrence latent variable models 
parameters aspect model corresponding weights clustering models induced posterior uncertainty vanish values latent variables known 
ffl consequences interpreting class conditional distributions yja 
aspect model prototypical particular group objects yjx obtained convex combinations class conditionals 
contrast sided clustering model attempts identify class conditionals characteristic group objects ffl hierarchical generalization flat clustering model class conditional replaced flexible weighted combination 
hand class conditionals sided clustering model restricted sided clustering model proportional marginal probability times cluster specific modulation factor depends associated class 
reflects additional coarsening induced grouping objects related models previous brown bdm proposed information criterion class gram models language modeling 
word classes formed mutual information classes adjacent words maximized 
model closely related sided clustering model main difference bdm word classes predicting predicted word identified 
brown proposed non probabilistic hard clustering variant introduced greedy cluster merging algorithm 
ideas pioneering pereira tishby lee ptl distributional clustering 
ptl aspect model mixture distribution starting point latent aspect variables introduced corresponding em algorithm derived 
pereira propose clustering method similar sided clustering model different principles posterior class distributions cjx obtained minimizing cross entropy criterion equivalently kullback leibler divergence entropic regularization parameter fi results maximum entropy cluster memberships cjx exp fi yjx log fi identification advantageous terms predictive log likelihood doubtful 
algorithmically identification poses additional problems alternation scheme proposed sided clustering model longer applies 
model yjx aspect ajx yja sided clustering pfc hierarchical clustering pfc ajx yja sided clustering pfc oe table systematic overview dyadic mixture models 
differ step equations sided clustering model missing factor exponent class prior probabilities 
reason objects treated equally particular irrespective number observations sample set ptl centroid condition derived maximum likelihood principle inserting maximum entropy cluster membership probabilities 
principles considered complementary re estimation equations iterated pseudo em scheme 
ptl remains unresolved underlying common objective function exists 
new formulation tp types equations derived rigorously rate theoretic ansatz mutual information principle 
offer perspectives foundation unsupervised learning maximum likelihood principle basis 
saul pereira sp independently proposed model called aggregate markov model utilized back model kat context language modeling 
approach equivalent aspect model asymmetric parameterization restricted class bigram model 
derived em algorithm employing annealing methods cf 
section 
generalizations extensions annealed em annealed em important generalization em model fitting pursues main goals ffl avoiding overfitting controlling effective model complexity ffl reducing sensitivity em local maxima ffl generating tree topologies hierarchical clustering model 
annealed em closely related deterministic annealing technique applied clustering problems including vectorial clustering bk pairwise clustering hb phb context occurrence data distributional clustering ptl 
key idea introduce temperature parameter replace minimization combinatorial objective function substitute known free energy 
annealing methods statistical physics 
consider general case maximum likelihood estimation em algorithm 
step definition computes posterior average complete data log likelihood maximized step 
annealed step temperature performs average distribution obtained generalizing bayes formula likelihood contribution taken power mnemonic notation annealed prior theta likelihood amounts increasing influence prior turn results larger entropy annealed posteriors 
example case aspect model sided clustering model annealed steps generalizing ajx xja yja cjs respectively 
fixed annealed step performs regularization entropy 
reason annealed em reduces sensitivity local minima controls effective model complexity 
annealing potential improve generalization overfitting models supervised learning problems cf 
pkm 
theoretical investigations emphasize benefits annealing avoid overfitting phenomena 
advantages deterministic annealing investigated experimentally cf 
section 
statistical learning deterministic annealing fin limit stopping temperature fin data 
hard clustering applications usually perform annealing limit 
theoretical guarantee deterministic annealing finds global minimum general independent empirical studies indicate typical solutions obtained significantly better corresponding optimization 
explained fact annealing homotopy method original objective function log likelihood smoothed large hierarchical models annealed em algorithm offers natural way generate tree topologies 
known adaptive vector quantization starting high value successively lowering typically leads sequence phase transitions 
phase transition effective number distinguishable clusters grows maximal number reached annealing stopped 
suggests heuristic procedure start single cluster recursively split clusters 
course annealing sequence splits tracked 
respective phase diagram tree topology 
note merely tree topology successively grown parameters posterior probabilities latent classes may drastically change annealing process 
expectation maximization cross validation data sparseness pervasive analysis dyadic data 
neglected problem maximum likelihood estimation shares empirical risk minimization methods maximization likelihood training data automatically guarantee equivalent performance unseen new data 
effect known overfitting severe sparse data 
cross validation conceptually simplest criterion model assessment model trained part available data tested remaining observations 
procedure repeated different data split 
average predictive log likelihood may serve model score 
cross validation great model selection directly indicate modify model fitting procedure order obtain better generalization performance 
early stopping common way utilize cross validation model fitting iterative fitting method aborted test error performance starts degrade 
course specific assumptions nature fitting method 
indication early stopping optimal sense circumvent overfitting phenomena 
context discussed em procedures simple modification leads efficient onestep approximation leave cross validation 
propose modify step parameter estimates computing posterior probabilities corrected subtracting influence observations related latent variable question 
precise consider step basic aspect model 
neglecting th observation associated latent variable leads modified equations ajx gamman gamman gamman ja gamman ja gamman denote modified step estimates obtained gamman delta gamma ajx gamman gamman yja delta delta yja gamma ajx gamman gamman delta delta xja gamma ajx gamman occurring posterior probabilities estimates computed preceding step 
essentially amounts removing direct influence th observation step parameter estimation 
em iterative procedure course remaining indirect influence may lead fitting problems 
sided clustering model observations removed estimating posterior gammax gamma pfc cjs gammax gammax pfc cjs gammax gn efficient computation corrections additional bookkeeping quantities straightforward 
interpretation leave step intuitive example clustering model posterior probabilities pfc cjs gammax calculated averaged statistics objects assigned particular cluster predict observations involve order preserve strict optimization principles modification step carried slightly careful just eliminating certain observations cf 
appendix 
empirically naive corrections discussed typically lead convergence stability problems 
simpler equations experiments 
accelerated em em algorithms important advantages gradient methods 
problems convergence speed em restricts applicability large data sets 
simple way accelerate em algorithms relaxation step 
discussed context mixture models pw rediscovered title em bks 
method useful accelerating fitting procedure discussed models 
essentially estimator generic parameter step modified gamma step estimate usual step 
choosing guarantees convergence typically choice speed convergence 
case positivity normalization constraint violated performing relaxed step parameter vector projected back admissible parameter space replacing negative probabilities small positive constant 
overview elaborated acceleration methods em refer mk 
aspect cluster hierarchical cluster table comparative results context dependent unigram modelling discussed models cranfield ir test collection cranfield 
results fold cross validation 
multiscale em multiscale optimization pb approach accelerating clustering algorithms neighborhood structure exists object space 
image segmentation example natural assumption adjacent image sites belong high probability cluster image segment 
fact exploited significantly accelerate estimation process maximizing suitable nested sequence variable subspaces coarse fine manner 
achieved temporarily tying adjacent sites joint assignment variable 
notational convenience restrict presentation sided clustering model extensions sided clustering straightforward 
formally coarsening hierarchy nested sequence equivalence relations ae xg 
context image analysis equivalence relations typically correspond multi resolution pixel grids obtained subsampling 
log likelihood minimized coarsening level imposing constraints form coarse scale optimization level simply yields constrained aspect model 
computational advantage reduced number posterior computations step performed equivalence class 
maximization procedure resolution level converged optimization proceeds level gamma solution subset defined gamma initializing optimization level gamma solution level emphasize contrast multiresolution optimization schemes multiscale optimization advantage maximize original log likelihood resolution levels 
set hidden variables effectively reduced imposing constraints set hidden variables applied multiscale optimization image analysis experiments resulting typical accelerations factors compared single level optimization 
applications experimental results section presents different application domains discusses applicability statistical models dyadic data information retrieval ii data mining text databases iii computational linguistics iv image segmentation computer vision 
conception largely domain independent theory unsupervised learning dyadic data attempt perform thorough detailed performance comparison state art techniques problems accompanying forthcoming publications deal different applications separately 
show sufficient detail different models applied realistic problems report representative results real world multiscale optimization current form applicable aspect model 
diff diff diff diff diff diff annealing experiments tdt dataset model components different subsampling factors upper left lower right 
denotes number documents training held data set numbers indicate inverse utilized respective training curves 
data 
addition different models evaluated data different domains variants extensions em discussed section systematically compared 
main goal demonstrate relevance dyadic data models important applications investigate questions modeling model fitting domains 
information retrieval intelligent information retrieval databases key topics data mining 
problem severe cases query formulated precisely natural language interfaces document collections digital libraries image multi media databases 
typically obtain entries documents images database best match query similarity measure 
difficult reliably estimate similarities query may contain information possibly relevant keywords occur query documents 
context dependent unigram models series experiments investigated general question different mixture models perform predicting occurrences words context particular document thought iterations iterations iterations iterations iterations annealing experiments tdt documents subsampling different number components different inverse temperatures context dependent unigram language model 
standard procedure statistical learning set word occurrences divided training validation test set 
statistical point view canonical goodness fit measure average log likelihood test set 
context natural language processing customary report perplexity related average pattern test set log likelihood exp gammal 
validation set determine optimal choice computational temperature annealed em algorithm 
standard ir collection documents mechanical engineering cranfield evaluate perplexity results 
preprocessing step infrequent common words word list eliminated porter stemmer generate word stem 
results different maximal number components summarized table 
main ffl lowest perplexity perplexity reduction compared unigram model obtained aspect model 
hierarchical clustering model performs better constrained flat clustering models 
terms perplexity constraints mixture models preferred clustering models 
ffl aspect model consistently standard em algorithm 
clustering models optimal generalization performance requires higher temperature 
recall method medline queries tf tfidf ml ml aml aml abs 
impr 
vs baseline rel 
impr 
vs baseline cranfield queries tf tfidf aml aml tfidf abs 
impr 
vs baseline rel 
impr 
vs baseline table retrieval precision results percent evaluated medline cranfield collection macro averaging 
tdf denote different term weighting schemes ml aml refer aspect model trained em maximum likelihood annealed em annealed maximum likelihood 
ffl temperature complexity control clearly better restricting number components 
aspect model components suffers overfitting trained non annealed em 
stress advantages annealed em investigated effect temperature regularization detail 
order take sample set size account utilized topic detection tracking tdt corpus ldc consists approximately words documents large subsampling experiments 
want focus control overfitting problem local maxima models trained fixed temperature 
shows perplexity curves different inverse temperatures function number annealed em iterations 
temperatures performed early stopping perplexity held data increased 
subsampling experiments documents repeated runs times order evaluate solution variability different randomized initial conditions 
effect model size varied number components subsampling level 
resulting curves depicted 
observations ffl temperature yields significant consistent improvement sample sizes cf 
models largely varying number parameters cf 

ffl expected benefits annealing diminishing larger data sets may vanish infinite data limit effect temperature considerable full word tdt dataset 
overfitting important problem large scale data sets 
ffl early stopping quite successful prevent overfitting 
experiments indicate may perplexity model performance cranfield collection terms perplexity precision absolute gain vs baseline different inverse temperatures fact take advantage models huge number parameters 
observes overfitting gets severe increasing training curves get steeper perplexity respective minima decreasing 
compared annealed training needs approximately times parameters achieve performance 
example annealed solution quality obtained early stopping em 
ffl computational complexity annealed em fixed slightly higher compared standard em typically twice iterations necessary converge 
experiments achieves prespecified performance roughly computing time memory requirements standard em 
iteration model components roughly twice expensive iteration components 
compared effect annealing predictive em variant showed slight consistent improvement aspect model 
additional computations necessary calculate corrections practical predictive em somewhat limited aspect model 
predictive em shown stronger effect combination clustering models expected correction terms significant case 
em variant proven valuable tool simulations typical acceleration factor gamma 
overrelaxation helps particular accelerate convergence process annealed em course important combination early stopped em training 
probabilistic latent semantic indexing controversy popular family techniques utilized information retrieval called vector space model vsm introduced salton sm sb bas smart system 
vsm document represented term vector transformed frequency counts term occurrences components 
important ingredients vsm function measure similarity documents typically cosine vector representations ii term weighting scheme re weight cluster result model data model observ distribu studi model neuron process author inform cluster state defect function author scale cluster object problem algorithm design data algorithm classif feature ion increas atom system test gener faint blue sub nonlinear simul approach point genet init process model applic cluster learn input vector segment object estim approach base atom format size target grain couple dynamic network lattice matrix control case complex redshift alpha event time seismic region distribu univers power scale spectrum map imag feature spatial allow partition method algorithm size processor graph partition system learn cell format center function estim equal vector network neural learn control kohonen speaker speech train continu recogni image region segment iter color 
map scale backprop 
activ adapt codebook design compress channel model chip phone hmm ellipsoid concept shell pitch phrase brain patient reson pixel texture contour motion spatial filter glvq init mountain cost membership bayesian upper levels cluster hierarchy 
node described probable terms conditional probabilities yja 
influence different terms 
successfully applied weighting scheme tfidf term frequency inverse document frequency transforms counts gamma log delta denotes fraction documents contain term similarity document query second document computed ji angular similarity measure scale invariant may replace counts empirical probabilities yjx 
key idea probabilistic latent semantic indexing plsi replace empirical conditionals multinomial word distributions derived aspect model 
consider linear interpolations empirical probabilities conditional distributions derived model yjx gamma yjx yja ajx intuitively aims exploiting semantic relations extracted aspect model synonyms words similar meanings get improved matches 
example word query may occur document indirect conditional probability aspect model high 
similar dimension reduction approach pursued standard lsi 
tested plsi method number medium sized standard document test collection relevance assessments computing precision recall curves 
precision recall curve reported sequel obtained called macro averaging cf 
vr chapter 
query determine best lists precision recall pairs computed single precision recall pair obtained follows total number relevant documents query number relevant documents best list precision network neural function result gener network algorithm learn train problem model neuron process author inform function weight learn error example perform network process data model dynamic time model synaptic neuron learn algorithm rule network map net state neuron continu method error gener learn perform classif train target classifi control neural design inform network neuron activ spike burst network neural learn algorithm dynamic visual imag motion cell process associ pattern learn function number hidden bound algorithm minima update rate delta valu associ model regular term imag classifi constraint map decision function model nonlinear statist optim linear time robot domain level algorithm fire cell spike rate record circuit local spatial electr control motor problem robot inhibit cell exten field orient map object recogni perform segment pattern node new rule optim size hebbian optim pyramid rat threshold center ltp conduct phase connec shape human region pattern line transform recogn filter local digit detect gabor continu art univers threshold circuit node theta transfer 
upper levels neural hierarchy 
node described probable terms conditional probabilities yja 
recall defined prec rq rec rq rq table summarized results medline collection cranfield collection inverse document frequency term weighting models obtained em annealed em training 
main draw aspect model estimate document specific word distribution yields substantial improvements terms retrieval precision 
particular regime high recall relative gain average number irrelevant documents returned rec level halved 
ii experiments observed improvements word perplexity retrieval precision strongly correlated 
stressed table comparing overfitted models trained likelihood criterion perplexity optimized models trained annealed likelihood 
exemplary run annealed em optimal stopping temperature compares simultaneous development word perplexity precision curves shown 
seen crucial control generalization performance model precision inversely correlated perplexity 
particular notice model obtained maximum likelihood estimation deteriorates retrieval performance 
data mining text databases structuring visualizing large databases key topics data mining 
applications range interactive coarse fine information access efficient data management representation 
paragraph focus application hierarchical clustering model structuring large text repositories 
clustering documents provides popular way database applied mixed success context query retrieval cf 
wil overview great importance interactive retrieval 
frequently methods context linkage algorithms single linkage complete linkage wards method cf 
jd hybrid combinations agglomerative centroid methods cluster robust segment channel glass decision exemplary relative word distributions nodes cluster dataset keywords cluster decision glass robust segment channel 
probabilistic interpretation number disadvantages 
contrast hierarchical mixture model provide sound statistical basis additional features suitable candidate context 
performed experiments information retrieval different collections abstracts 
facilitate assessment extracted structure investigated dataset documents containing abstracts papers clustering title word cluster second dataset documents abstracts journals neural computation neural networks neural 
data presumably amenable interpretation reader standard text collections 
top levels cluster hierarchy cluster neural generated hierarchical clustering model visualized respectively 
hierarchical organization documents satisfying topological relations clusters capture important aspects inter document similarities 
contrast multi resolution approaches distributions inner nodes hierarchy obtained coarsening procedure typically performs sort averaging respective subtree hierarchy 
abstraction mechanism fact leads specialization inner nodes 
specialization effect probabilities yja suitable cluster summarization 
notice low level nodes capture specific vocabulary documents associated clusters subtree 
specific terms automatically probable words component distribution higher level nodes account general terms 
demonstrate ability hierarchical clustering model identify abstraction levels hierarchy visualized distribution responsibility observations involving index word particularly interesting examples 
tree word cluster shows expected occurrences cluster documents explained common feature documents occurrences assigned root 
word decision level node indicating typical word algorithmically oriented documents assigned nodes subtree left branch papers physics astronomy 
index term robust occurs different meanings approach point genet process model applic cluster learn input vector segment object estim approach base algorithm method cluster propos model observ distrib studi cluster object problem algorithm design fuzzy data algorithm classif feature function author scale cluster state defect tissue brain patient reson pixel texture contour motion spatial center function estim equal vector network neural learn control kohonen image region segment iter color speaker speech train continu recogni example run interactive retrieval session documents texture image segmentation level look ahead cluster hierarchy 
highly specific meaning context stability analysis aspect characterized terms plane perturb eigenvalue root broad meaning sense robust methods algorithms 
word segment occurs mainly documents computer vision language processing significant larger extend field 
glass specific term solid state physics lowest level hierarchy 
channel ambivalent context physics communication theory 
bimodal distribution clearly captures fact 
examples demonstrate extracted hierarchical organization reflects interesting structure inherent occurrence data 
demonstrate usefulness example depicted virtual interactive expansion cluster hierarchy retrieval session texture segmentation 
computational linguistics computational linguistics statistical analysis word occurrences lexical structures adjective noun verb direct object received considerable degree attention hin ptl dlp dlp 
potential applications methods word sense disambiguation problem occurs different linguistic tasks ranging parsing tagging machine translation 
data utilized test different models consists adjective noun pairs extracted tagged version penn treebank corpus lob corpus perplexity results penn dataset reported table 
results qualitatively similar ones obtained cranfield document singular plural forms identified 
aspect cluster hierarchical cluster fi fi fi fi table comparative perplexity results adjective noun pairs treebank corpus 
collection application quite different information retrieval 
supports drawn 
result simultaneous hard clustering lob data scm reported 
visualization matrix reveals groups space preferably combined mainly group complementary space 
example adjective group holy divine human occurrences exclusively nouns cluster life nature 
groups indifferent respect groups corresponding set adjective group headed small big suitable 
unsupervised texture segmentation unsupervised segmentation textured images challenging 
numerous approaches texture segmentation proposed past decades obey stage scheme 
modeling stage characteristic features extracted textured input image spatial frequencies jf mrf models mj 
clustering stage features grouped homogeneous segments homogeneity features typically formalized clustering optimization criterion 
widely features interpreted vectors euclidean space jf mj ph segmentation obtained minimizing means criterion sums square distances feature vectors assigned group specific prototype feature vectors 
occasionally grouping process pairwise similarity measurements image sites similarity measured non parametric statistical test applied feature distribution surrounding neighborhood 
pairwise similarity clustering provides indirect way group discrete feature distributions reducing information distribution mean 
mixture models dyadic data especially sided clustering model formalize grouping feature distribution direct manner 
contrast pairwise similarity clustering offer sound generative model texture class description utilized subsequent processing stages edge localization sb 
furthermore need compute large matrix pairwise similarity scores image sites greatly reduces processing time memory requirements 
compared means techniques provides significantly flexibility distribution modeling 
especially texture segmentation application class features exhibit non gaussian multi modal distribution main reason success pairwise similarity clustering approaches sided clustering model compared standard means 
applied sided clustering model unsupervised segmentation textured images objects correspond image locations 
number observed features identical sites simply set experiments adopted framework jf pb utilized image representation modulus complex gabor filters 
site empirical distribution coefficients surrounding filter specific window determined 
reported segmentation results filter bank twelve gabor filters orientations scales 
filter output discretized equally voice face body head light idea book job manner word line impression statement style power war size test weapon behaviour floor trip leave companion place field person section study life nature spirit name worker demand informant acid glance party country system sense market society family court child community experience reason result knowledge feature position course unit trade control expression desire atmosphere space note world town method movement view water eye hair fish milk attitude thought tradition programme division deal friend example night bird land music metal blow stone way thing hand area side authority government council committee piece shape sound wind soil spot iron ship drink number amount scale report majority change term property action effort man time people year day value level rate price cost smile frame crowd baby phrase effect quality solution picture colour interest condition development meeting activity service school purpose class agreement part form problem case type feeling love letter silence heart complete natural official separate growing great fine bad close strange thin soft bitter plain sudden sharp powerful friendly curious true hard excellent ancient famous right wide left opposite old young little early short high final low ordinary average nuclear western apparent atomic potential human divine holy continued special important particular various basic large certain considerable total annual common foreign technical open medical small big suitable light tiny perfect native distant violent empty brilliant endless clever slight vertical severe awful similar single strong regular obvious electric massive gentle cool cheap double huge tremendous occasional firm national general public social private different real original possible simple new modern serious existing royal extended closed reciprocal long poor happy golden beautiful red cold black dark hot heavy literary familiar pure solid full military free professional musical deep daily quick mixed square local white central legal eastern interesting usual successful remarkable historical political personal economic individual industrial clustering lob scm visualization oe matrix characterization clusters probable words 
sized bins 
consequence conditional independence assumptions results feature space size 
channel gabor coefficients sampled local window size proportional scale filter 
finest scale rectangular theta window utilized 
simple spatial prior utilized class distribution suppress small regions 
prior distribution multiscale em essential compute high quality solutions 
excellent segmentation quality obtained acm histogram clustering algorithm illustrated results fig 

mixture different brodatz textures partitioned accurately error rate 
errors basically correspond boundary sites 
result obtained mondrian aerial images satisfactory due missing ground truth quality quantified 
disconnected texture regions type identified correctly problems occur texture boundaries 
segmentation quality achieved outdoor images fig 
visually semantically satisfying 
detailed evaluation sided clustering model unsupervised texture segmentation scope published ph 
main contribution novel class statistical models analysis cooccurrence data proposed evaluated 
introduced discussed different models 
distinguished systematic point view way hidden variables introduced effectively imposes constraints component distributions mixture 
proposed statistical models turned special cases 
models sound statistical foundation define generative distribution fitted approximate em algorithm 
proper selection method choice problem crucially depends modeling goal 
argued required detect groups structure hierarchical representations 
situations may sacrifice precision terms statistical accuracy perplexity reduction order extract structure interest 
proposed left mixture image containing different textures brodatz errors depicted black 
right mixture images containing textures extracted aerial images 
framework models derived extract group structure simultaneously object spaces model hierarchical dependencies clusters 
strongly believe proposed framework flexible adapted different tasks 
generality developed methods stressed revealing benefits context broad range potential applications 
addition modeling problem addressed computational issues particular focusing improved variants basic em algorithm 
importantly experiments underline possible advantages annealed version em fruitful combination ideas methods statistics statistical physics 
acknowledgment authors wish michael jordan peter dayan tishby joachim buhmann helpful comments suggestions 
authors grateful carl de marcken joshua goodman sharing expertise data natural language processing du buf providing image data depicted fig 

appendix em annealed em free energy minimization nh shown step step em algorithm generalized free energy criterion 
fact importance particular deriving approximate steps 
denote generic latent variable realizations consider family objective functions zjs log log zj zjs sample set parameter vector corresponds computational temperature 
sum expected complete data log likelihood entropy posterior distribution 
derivative identical 
maximizing probability distribution zjs treating variational parameters intended meaning get zj zj typical segmentation result real world image left right segments obtained acm 
recovers posterior probabilities 
proves maximized step zj step free energy gammaf lyapunov function em algorithm 
arbitrary motivates annealed em optimization principle 
approximate em mean field approximation optimization principle offers systematic approach deriving approximate steps restricting optimization zj particular sub family distributions 
approximating sub family chosen tractability considerations typically simplified factorial form 
general formulation approximate step probability distribution chosen maximizes shown variational principle equivalent minimizing kl divergence approximation true posterior distribution zjs 
occurrence modeling framework utilized general principle sided clustering model parameterizing delta set constraints 
inserting expression making factorial form obtains relevant dependent part augmented lagrange multiplier term enforce normalization log oe log log oe gamma performing maximization set variables yields exp log oe provided oe theta delta eq 
simply step equation approximated posterior probabilities cf 
assure variables identical order get simple expression 
leave step optimization framework utilized derive leave modified step strict optimization principle 
demonstrate idea aspect model introduce variational parameters corrected posterior probabilities 
substituting corrected step equations parameters obtains log gamma gamma log log maximize derivation stationary conditions straightforward involves technical algebraic manipulation 
anderson 
statistical analysis categorial data 
springer 
bar 
latent variable models factor analysis 
number griffin statistical monographs courses 
oxford university press new york 
bas buckley allan salton 
automatic routing retrieval smart trec 
information processing managment 
bdm brown desouza mercer della pietra lai 
class gram models natural language 
computational linguistics 
bk buhmann 
vector quantisation complexity costs 
ieee transactions information theory 
bks bauer koller singer 
update rules parameter estimation bayesian networks 
proceedings th annual conference uncertainty ai uai providence rhode island august 
bock 
theoretische und praktische methoden zur und von daten cluster analyse 
number studia mathematica mathematische 
und ruprecht 
buhmann 
empirical risk approximation induction principle unsupervised learning 
technical report iai tr institut fur informatik iii university bonn 
domany 
data clustering model granular magnet 
neural computation 
ca carroll arabie 
multidimensional scaling 
annual review psychology 
cutting karger pedersen 
scatter gather cluster approach browsing large document collections 
seventeenth annual international acm sigir conference research development information retrieval 
coo coombs 
theory data 
john wiley son 
css cohen shapire singer 
learning order things 
advances neural information processing systems 
ct cover thomas 
elements information theory 
john wiley sons new york 
dlp dagan lee pereira 
similarity estimation word probabilities 
proceedings association computational linguistics 
dlp dagan lee pereira 
similarity methods word sense disambiguation 
proceedings association computational linguistics 
dlr dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal statist 
soc 

deerwester dumais furnas landauer 
indexing latent semantic analysis 
journal american society information science 
es essen steinbiss 
cooccurrence smoothing stochastic language modeling 
proceedings ieee conference acoustics speech signal processing pages 
geman geman dong 
boundary detection constrained optimization 
ieee transactions pattern analysis machine intelligence 
goldberg nichols oki terry collaborative filtering weave information tapestry 
communications acm 
goo 
population frequencies species estimation population parameters 
biometrika 
goo 
estimation probabilities 
research monograph 
mit press cambridge ma 
hb hofmann buhmann 
pairwise data clustering deterministic annealing 
ieee transactions pattern analysis machine intelligence 
hin hindle 
noun classification predicate argument structures 
proceedings acl pages 
heitz perez bouthemy 
multiscale minimization global energy functions visual recovery problems 
cvgip image understanding 
hofmann puzicha buhmann 
unsupervised texture segmentation deterministic annealing framework 
ieee transactions pattern analysis machine intelligence 
hofmann puzicha jordan 
learning dyadic data 
advances neural information processing systems 
jd jain dubes 
algorithms clustering data 
prentice hall englewood cliffs nj 
jel jelinek 
development experimental discrete dictation recogniser 
proceedings ieee 
jf jain 
unsupervised texture segmentation gabor filters 
pattern recognition 
jordan ghahramani jaakkola saul 
variational methods graphical models 
jordan editor learning graphical models pages 
kluwer academic publishers 
jj jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 
jm jelinek mercer 
interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice 
kat katz 
estimation probabilities sparse data language model component speech recogniser 
ieee transactions acoustics speech signal processing 
konstan miller maltz herlocker 
grouplens applying collaborative filtering usenet news 
communications acm 
kru kruskal 
multidimensional scaling 
sage publications beverly hills ca 
ldc ldc 
linguistic data consortium tdt pilot study corpus documentation 
www ldc upenn edu tdt 
ls lochbaum streeter 
comparing combining effectiveness latent semantic indexing ordinary vector space model information retrieval 
information processing management 
mb mclachlan basford 
mixture models 
marcel dekker new york basel 
mj mao jain 
texture classification segmentation multiresolution simultaneous autoregressive models 
pattern recognition 
mj meila jordan 
estimating dependency structure hidden variable 
advances neural information processing systems 
mk mclachlan krishnan 
em algorithm extensions 
wiley new york 
nh neal hinton 
view em algorithm justifies incremental variants 
jordan editor learning graphical models pages 
kluwer academic publishers 
pb jan puzicha joachim buhmann 
multi scale annealing real time unsupervised texture segmentation 
proceedings international conference computer vision iccv pages 
ph healey 
markov random field models unsupervised segmentation textured color images 
ieee transactions pattern analysis machine intelligence 
ph puzicha hofmann 
histogram clustering unsupervised segmentation image retrieval 
technical report optimierung preprint submitted pattern recognition letters 
phb puzicha hofmann buhmann 
theory proximity clustering structure detection optimization 
pattern recognition 
pkm pawelzik kohlmorgen muller 
annealed competition experts segmentation classification switching dynamics 
neural computation 
ptl pereira tishby lee 
distributional clustering english words 
proceedings acl pages 
pw peters walker 
iterative procedure obtaining maximum likelihood estimates parameters mixture normal distributions 
siam journal applied mathematics 
rose gurewitz fox 
statistical mechanics phase transitions clustering 
physical review letters 
rose gurewitz fox 
vector quantization deterministic annealing 
ieee transactions information theory 
rao miller rose 
gersho 
deterministically annealed mixture experts models statistical regression 
proceedings ieee international conference acoustics speech signal processing volume pages 
ieee comput 
soc 
press 
sb salton buckley 
global text matching information retrieval 
science august 
sb bigun 
hierarchical image segmentation multi dimensional clustering orientation adaptive boundary refinement 
pattern recognition 
sch schutze 
automatic word sense discrimination 
computational linguistics 
sm salton mcgill 
modern information retrieval 
mcgraw hill 
sp saul pereira 
aggregate mixed order markov models statistical language processing 
proceedings nd international conference empirical methods natural language processing 
sp schutze pedersen 
cooccurrence thesaurus applications information retrieval 
information processing managment 
tp tishby pereira 
personal communication manuscript preparation 
tsm titterington smith makov 
statistical analysis finite mixture distributions 
john wiley sons 
vr van rijsbergen 
information retrieval 
butterworths london boston 
wb witten bell 
zero frequency problem estimating probabilities novel events adaptive text compression 
ieee transactions information theory 
whi whittaker 
graphical models applied multivariate statistics 
john wiley son chichester 
wil willett 
trends hierarchical document clustering critical review 
information processing management 
wl wermuth lauritzen 
substantive research hypotheses conditional independence graphs graphical chain models 
journal royal statistical society series methodological 
