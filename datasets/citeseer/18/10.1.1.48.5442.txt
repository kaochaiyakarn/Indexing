beowulf harnessing power parallelism pile pcs daniel ridge donald becker phillip center excellence space data information sciences code nasa goddard space flight center md becker gsfc nasa gov thomas sterling high performance computing systems group jet propulsion laboratory center advanced computing research california institute technology caltech edu rapid increase performance mass market commodity microprocessors significant disparity pricing pcs scientific workstations provided opportunity substantial gains performance cost harnessing pc technology parallel ensembles provide high capability scientific engineering applications 
beowulf project nasa initiative sponsored hpcc program explore potential pcs develop necessary methodologies apply low cost system configurations nasa computational requirements earth space sciences 
processor beowulf costing sustained gravitational simulation particles tree code algorithm standard commodity hardware software components 
describes technologies methodologies employed achieve breakthrough 
opportunities afforded approach challenges confronting application real world problems discussed framework hardware software systems results benchmarking experiments 
near term technology trends directions pile pcs concept considered 
table contents 

background 
beowulf architecture characteristics 
software architecture 
applications scaling performance 
discussion 
remarkable technological advances accelerated growth computational performance 
decade feature size integrated circuits shrunk dimension order magnitude yields die sizes increased significantly 
result order magnitude increase number devices chip similar increase total chip speed 
field revolutionized advances pc greatest beneficiary dramatic success 
years workstation microprocessors experienced rate performance increase year 
extraordinary easily surpassed performance increase microprocessors exceeded factor year past years 
pc market orders magnitude larger workstation market resulting economies scale allowed pc prices decrease sustaining dramatic performance increase 
today pc performance overlaps range workstation performance highest speed workstation microprocessors remaining faster pc processors 
change comes opportunities 
potential harnessing power parallel processing cost pcs identified possible importance nasa mission critical applications consistent agency objective cheaper better faster 
real terms lower cost computing large scale problems means science performed dollar 
early beowulf project initiated sponsorship nasa hpcc earth space sciences project investigate potential clustered pcs performing important computational tasks capabilities contemporary workstations greater cost 
october announced beowulf system exceeded sustained performance space science application total system cost breakthrough performance price may significant implications impact wide range industrial applications including aerospace 
presents findings year beowulf project techniques employed achieve objective results benchmarking experiments demonstrated success project 

background nasa hpcc program nasa hpcc program initiated january far reaching agenda advancing state massively parallel processing mpp applying major computational problems important nasa mission objectives computational cas earth space sciences ess 
ess project represents computational domain includes direct manipulation large data sets user scientists 
context need powerful user terminal capability identified early program definition 
large data set comes simulation large scale physical phenomena evolution galaxy plasma solar corona remote sensing platforms planned eos scientists need acquire examine explore manipulate visualize transform large collections complex data 
may involve numerically compute intensive activities involves large amounts data movement 
component ess project specified title scientific workstation 
clear time goal addressed recognized principle objective function optimized user response time 
workstation requirements involved data access time secondary storage 
usually data reside shared file server common local area network lan 
ironically cases data accessed repeatedly working session typical workstation simply disk capacity hold requisite data 
result long latencies file servers tedious response cycles burdening shared resources 
beowulf parallel workstation project initiated address challenge 
pile pcs pile pcs term today describe loose ensemble cluster pcs applied concert single problem 
similar cow cluster workstations network workstations emphasizes ffl mass market commodity components ffl dedicated processors scavenging cycles idle workstations ffl private system area network san goal achieving best system cost performance ratio 
beowulf adds model emphasizing ffl custom components ffl easy replication multiple vendors ffl scalable ffl freely available software base ffl freely available distribution computing tools minimal changes ffl returning design improvements community 
pile pcs approach exploits components respond widely accepted industry standards benefits prices resulting heavy competition mass production 
pile pcs approach intrinsic advantages serious interest certain niche communities provide complementary computing medium high workstations symmetric multiprocessors scalable distributed memory systems 
advantage single vendor owns rights product 
vendors provide essentially identical subsystem types peripheral controllers devices packaging 
subsystems provide accepted standard interfaces pci bus ide scsi interfaces ethernet communications 
second advantage pile pcs approach permits technology tracking 
rapidly changing industry generation may year pricing varies significantly quarter quarter approach allows computing systems acquired best technology best price 
example beowulf pile pcs number country exactly run software 
leads advantage just place configuration 
user needs vary dramatically 
clear required configuration pile pcs approach permits extreme flexibility user driven decisions system configuration evolve 
systems vendor limited vendor current options lists may months date 
users pick choose wide array sources try things change configuration time 
beowulf exploits readily available usually free software systems sophisticated robust efficient commercial grade software 
software derived community wide collaborations operating systems languages compilers parallel computing libraries 
comparable quality vendor offered software systems cases 
widely operating system class distributed computing linux bsd unix posix systems available net cost 
commercial distributors available commercial support services full windowing popular shells standard compilers programming languages 
major message passing libraries pvm mpi available systems widely community 
addition source code available permitting easy customization redistribution legal constraints typical proprietary software products 
beowulf uses linux operating system better performance better availability source code better device support wide user acceptance 
particular linux pcs academic computer labs sophistication accessibility low cost 
reasons pile pc approach considered relate technology industry trends 
technology convergence workstation pc microprocessors performance growth traditional vector supercomputers 
country true computer built doe asci program provided intel components pentium pro pcs 
generation pc microprocessor expected included scalable high performance computer offered major vendor 
high dec working hard migrate alpha microprocessor pc market offering low cost compatible software 
industry comparatively high performance computing market sustain separate research development path 
clear changes industry 
cray computer closed doors 
convex acquired hp 
cray research acquired sgi 
possible exception tera computer computer dedicated solely production high performance computers 
important convergence underway ironically pile pcs approach may asymptote 
research issues pile pcs proving successful path parallel computing issues realm applied research need addressed 
relate primarily resource management software tools distributed computing 
beowulf project addressing identifying key gaps available tools implementing new user tools fill gaps 
collection software tools coming beowulf project known continuously evolving set 
challenge pile pcs approach exemplified beowulf relatively long latencies modest interconnection bandwidth provided low cost networking fast ethernet 
addressed software performance tuning aggregating networks rich interconnect topologies 
case past distributed memory parallel computing systems applications need written parallel message passing explicating algorithmic parallelism 
addition algorithms need latency tolerant overlapping computation communications greatest efficiency 
significantly difficult sequential programming styles embodied fortran 
approach increasingly acceptable user community systems primarily computational scientists exploited performance benefits previous generations familiar issues methods associated employing class system 
tolerable importance practical 
explores realm pile pcs base experience derived beowulf project 
intent convey potential user community opportunities potential pitfalls harnessing ing class low cost high performance computer 
reasonably complete presentation diverse topics related new challenge 
provides description architectures date basic system software advanced tools developed accomplishments application programming including performance 
targeting major sector applications community hoped technology may prove accelerate process high performance computing applied computational challenges 

beowulf architecture characteristics beowulf represents family systems tracked evolution commodity pc hardware enjoyed increasing range applications 
beowulf project driven need high performance scientific computing earth space sciences ess community community diverse set requirements continues fuel evolution beowulf class machines 
beowulf parallel workstation architecture important testbed emerging ideas clusters original response needs ess scientists 
constraints workstation architecture exclusively commodity hardware beowulf workstation populated disk memory cost highperformance scientific workstation origin system silicon graphics roughly 
earth observing applications developed body codes message passing machines port easily new architecture 
large core classification registration problems usually programmed spmd single program multiple data style owner computes data distribution patterns requirement high performance image processing coupled high aggregate bandwidth disk subsystems take advantage data distributions 
accommodate needs beowulf places disks node distinction compute nodes achieves high network bandwidths aggregating multiple channels commodity fast ethernet hardware 
beowulf class machines costeffective platforms large multi body codes body particle cell dsmc codes 
large typically core problems benefit high floating point performance demand high bandwidth low latency interprocessor communications 
high bandwidth interconnect broad availability small scale multiprocessor processors system boards provide extremely cost effective way increase floating point performance cpu intensive applications 
beowulf clusters assembled new generation commodity cpu mhz intel dx processor 
current price performance point desktop architectures intel mhz pentium pro cpu key new generation beowulf class machines retain user costs perform favorably bracketed factor commercial distributed memory supercomputer architectures node basis ibm sp hp convex spp series cray 
latest beowulf clusters installed caltech los alamos national laboratory nasa goddard space flight center node pentium pro machines configurations reflect specific needs users 
caltech los alamos machines figures built body galactic gravitational simulations feature fastest networks available commodity marketplace 
cal beowulf parallel workstation architecture tech cluster interconnected degree fast ethernet connected port crossbar switch 
los alamos machine loki degree fast ethernet topology connected degree point point hypercube plus degree switched network switched network bypasses long routes hypercube avoids low performance broadcast multicast operations characteristic simple hypercubes 
nasa goddard latest machine pentium pro cluster nodes contains gb distributed disk aggregate disk bandwidth excess gbit second 
nodes connected classic beowulf network dual fast ethernet segments bonded transparently software single logical network 
general purpose machine built applications software development largely testbed development mass storage system 
aggressive beowulf class machine considered time jointly funded darpa nasa project develop network attached secondary storage system 
node processor system terabyte distributed disk space aggregate external bandwidth gigabyte second 
special purpose machine step usual price point beowulf clusters take advantage hybrid network topology achieve better scaling 
nodes collected series node connected internally fast ethernet 
meta nodes attached gbit myrinet crossbar building cluster shallow fat tree 
beowulf spectrum clusters built slightly different set requirements universities country 
number schools drexel gmu clemson university illinois urbana champaign caltech belong academic beowulf consortium assemble machines frequently pedagogical purposes research 
machines tradeoff absolute performance increased parallelism substituting smaller disks memory slower cpus 
machines serve teaching machines application platforms distributed computing built dollar amounts reach university 

software architecture beowulf class machines leverage available software shelf hardware 
beowulf system widely available linux operating system 
linux full featured clone unix operating system originally designed processors extended support common desktop architectures 
addition portability linux kernel features posix compliance tcp ip protocol stack sockets interface broad device support dynamically linked shared libraries interprocess communication efficient virtual memory subsystem unified buffer cache 
linux kernel cpu fast ethernet switch cpu cpu caltech cluster cpu cpu cpu cpu los alamos loki cluster basic supporting software distributed terms free software foundation gnu public license insures source code system available easily share improvements node 
beowulf software environment implemented add commercially available royalty free base linux distributions 
distributions include software needed networked workstation kernel linux utilities gnu software suite add packages 
initially popular distribution 
migrating redhat distribution better package management upgrade system 
beowulf distribution includes programming environments development libraries individually packages 
pvm mpi bsp available 
style ipc threads supported 
considerable amount gone improving network subsystem kernel implementing device support 
changes incorporated kernel source code tree 
beowulf scheme common clusters node responsible running copy kernel nodes generally autonomous kernel level 
interests presenting uniform system image users applications extended linux kernel allow loose ensemble nodes participate number global namespaces 
guiding principle extensions little increase kernel size complexity importantly negligible impact individual processor performance 
global process id space normal unix processes belong kernel running unique identifier context 
parallel distributed scheme convenient unix processes gb disk mb ram ram mb fast ethernet myrinet fast ethernet crossbar myrinet switch ess testbed backbone beowulf mass storage system process id unique entire cluster spanning kernels 
unix systems notably linux fujitsu ap multicomputer support directly notion spmd context execution multiple copies code run collection nodes share unix process id generally useful scheme available library layer pvm 
pvm provides task running virtual machine task id unique hosts participating virtual machine 
pvm api incorporates library calls provide functionality unix kill getpid 
ideally mechanism transparently available processes just written compiled specific library 
implemented global process id schemes 
independent external libraries 
second pvm designed compatible pvm task id format pvm signal transport 
traditional unix calls kill getpid transparently schemes 
pvm undefined range pvm task id space 
pvm reserves pid range node max procs local processes processes init necessity replicated cluster clutter global process id space 
scheme children inherit type global local parent clone system call explicitly instructed process creation time 
example system node right run pvm daemon master dispatch mechanism 
clone immediately migrate global pid space children spawned daemon exist global space 
definition includes fields inherited pvm provide special ids pvm dispatch daemons 
performance impact runtime pid assignment requires internode communication kernels assign pids static non overlapping range 
added rarely variable process creation 
remote delivery signals significantly complicated require internode communication done error path signals invalid pids 
guiding principle system services separate policy mechanism embodying mechanism kernel implementing policy outside 
setting process id range reliable delivery signals recovering failures handled user level process 
additional impetus combined user level implementation paradigms kernel expected endpoint communication merely handling communications behalf user processes 
presents set conceptual challenges apply broadly global name spaces require communication coherence 
kernel level user level communication required stateless kernel sake reliability happens interface linux vfs virtual filesystem switch 
unified proc filesystem extension sufficient control processes little global view processes 
underway mechanism allow unmodified versions standard unix process utilities ps top beowulf cluster 
linux advanced implementation proc pseudo filesystem 
system process information form filesystem generated real time kernel 
scheme originated interface debuggers popularized plan operating system 
basic proc presents subdirectory process local processor 
linux implementation extends proc system information format 
proc filesystem common system monitoring tools linux ps top 
tools unchanged conceptually simple step combining proc directories cluster existing nfs capabilities 
implementation complexity comes handling pseudo files length read performance impact gathering fresh directory information request 
programming models distributed application programming environments available beowulf 
commonly pvm mpi environments bsp available 
distributed shared memory package planned 
pvm mpi multiple disparate programming paradigms widely message passing 
hardware systems support shared memory mechanisms message passing application programmers portability 
beowulf support popular pvm mpi programming models slightly modified oak ridge pvm package unchanged ohio state lam mpi package 
bsp traditional unix kernels fundamental distinction namespaces unix kernels manage casting explicit system call filesystem interface implicit memory interface mmap 
far simpler systems programmer provide coherency namespaces managed explicitly sorts mechanism bane application programmers 
explicit message passing techniques parallelize serial applications port parallel codes developed shared memory architectures tedious 
time port develop considered part net productivity gain platform 
cases bsp bulk synchronous parallel libraries provide active messaging eliminate server side message passing application code 
clientside remote memory explicit 
removes local remote address space transparency frequently convenient complicates referencing indirect distributed objects 
distributed shared memory linux kernel provides vfs interface virtual memory system 
simpler add transparent distributed backends implicitly managed namespaces 
systems created allow entire memory cluster accessed completely transparently 
additional environment added beowulf packages page network virtual memory known distributed shared memory dsm 
initial implementation zero overhead unified network dsm system system sarnoff 
page distributed shared memory uses virtual memory hardware processor software enforced ownership consistency policy give illusion memory region shared processes running application 
conventional dsm implementation planned support network memory server 
parallel filesystem beowulf systems take advantage number libraries written provide parallel filesystem interfaces networks workstations 
university maryland college park parallel virtual file system pvfs developed clemson concert nasa regional data centers run beowulf 
portable parallel file system ppfs uiuc runs systems similar beowulf 
mpi io expected core software new applications 
pvfs automatically enable key nasa applications 
ppfs developed alongside scalable io initiative support provides contact multi agency multi vendor initiative address increasing demands io subsystems high performance computing community 
support additional legacy applications pathfinder available beowulf users 

applications scaling performance beowulf architecture provides sites flexibility build machines tuned particular demands application 
clusters built date retained balance original delivering high performance application 
number applications port beowulf little recompile mpi pvm bsp common libraries available 
important example application gravitational body codes 
body body codes target application beowulf class machines 
past beowulf machines nasa goddard larger machines 
codes developed debugged beowulf migrated large distributed memory machines paragon cm 
code migrated way 
michael warren john salmon recipients gordon bell prize performance large scale scientific computing migrated highly optimized body codes thinking machines cm clusters installed institutions 
port reported take man minutes simple recompile existing code worked modification 
cluster sustains body problem 
direct solution equations force system interacting particles requires calculation 
tree codes collection algorithms find approximate solutions exploiting naturally locality system 
particle information sorted tree spatial hierarchy intermediate node tree responsible storing average quantities mass center mass high order moments mass distribution particles stored leaf nodes underneath node 
approximate force body algorithm searches tree uses information stored intermediate nodes satisfies certain approximation criteria 
results log scaling presents difficulties parallel programmer tree search known priori particle tree unstructured frequent indirect addressing required 
warren salmon algorithm builds tree data structure differently standard algorithm decomposing space morton order space 
doing able control shape tree match tree induced communication patterns granularity particular constraints 

discussion advantages beowulf system success fronts 
beowulf systems constructed sites academic scientific uses 
price performance compares favorably modern parallel architectures beowulf pile pcs equaled performance ibm sp comparable nodes tenth price user important problems 
absolute performance surpassed gflops non trivial application goal established project 
system software developed part project widely distributed 
site retain low cost beowulf tradeoff absolute performance increased parallelism substituting larger numbers expensive processors pedagogical instructional uses 
limitations pile pc methodology experimental match valuable services provided computer vendors 

emerging opportunity high performance computing field complements competes hpc industry commercial products 
opportunities available approach provides lower entry level price parallel computing increasing parallel processing user community earlier helping build market 
initial entry level users acquire higher grade vendor supplied systems experienced advantages overcome psychological barriers parallel computing 
vendors traditional high performance computers provide full support maintenance 
users piles pcs provide 
places easy academic national laboratories markets business banking industries 
accomplished remains done 
projects mentioned progress 
goal assembling cluster easy afternoon activity expert intervention required 
case 
remains done writing instructions sufficiently clear preparing robust packaged distribution avoids pitfalls 
cheng clustered workstations potential role high speed compute processors nas computational services technical report rns nas systems division nasa ames research center april 
linux documentation project accessible internet world wide web url sunsite unc edu linux html 
gnu general public license version june free software foundation massachusetts ave cambridge ma 
sterling becker savarese beowulf parallel workstation scientific computation proceedings international conference parallel processing icpp august vol 
pp 

sterling savarese becker olson communication overhead space science applications beowulf parallel workstation proceedings fourth ieee symposium high performance distributed computing hpdc august pp 

sterling ridge savarese becker 
design study alternative network topologies beowulf parallel workstation proceedings fifth ieee symposium high performance distributed computing hpdc august 
red hat software homepage accessible internet world wide web url www redhat com 
tridgell walsh ap linux modern os ap national university technical report cap anu edu au cap projects linux index html 
killian processes files usenix summer conference proceedings june 
pike thompson trickey plan bell labs proceedings summer uk unix user group conference july pp 

lam mpi parallel computing homepage accessible internet world wide web url www osc edu lam lam html 
sunderam pvm framework parallel distributed computing concurrency practice experience december pp 

snir otto walker dongarra mpi complete mit press cambridge massachusetts 
zero overhead network dsm system sarnoff technical report ftp ftp sarnoff com pub www docs cluster html 
bulk synchronous parallel bsp computing model worldwide homepage www bsp worldwide org 
barnes hut nature 
arvo scalable monte carlo image synthesis appear parallel computing 
bennett bryant sussman das saltz framework optimizing parallel proceedings scalable parallel libraries conference 
