reducing run time complexity support vector machines appear icpr brisbane australia august edgar osuna federico girosi center biological computational learning massachusetts institute technology cambridge ma usa mail ai mit edu support vector machine svm new promising technique classification regression developed vapnik group bell labs :10.1.1.15.9362
technique seen new training algorithm polynomial radial basis function multi layer perceptron networks 
svms currently considered slower runtime techniques similar generalization performance 
focus svm classification investigate problem reducing runtime complexity 
relevant results svm regression tool approximate decision surface user specified accuracy reformulation training problem yields exact decision surface smaller number basis functions 
believe reformulation offers great potential improvements technique 
selected problems approaches give reductions run time range system degradation 
keywords support vector machines machine learning optimization 
support vector machines svms seen approximate implementation vapnik defined structural risk minimization srm inductive principle aims minimizing upper bound generalization error model minimizing mean square error data set 
details technique 
simple intuition technique obtained case linear classifier linearly separable classes 
case svm separate data hyperplane described delta leaves maximum margin classes see fig 

shown maximizing margin equivalent minimizing upper bound generalization error classifier providing strong theoretical motivation technique 
vector maximizes margin shown form margin hyperplane defined sum distances hyperplane closest points classes 
separating hyperplane small margin 
separating hyperplane large margin 
better generalization capability expected 
parameters solving quadratic programming qp problem 
extension technique non linearly separable classes simple discussed 
extension case non linear decision surfaces achieved mapping original input variables high possibly infinite dimensional space features oe oe computing linear separating surface new space 
turns approach leads decision surfaces form heaviside function data set gamma set parameters computed solving qp problem kernel function represents scalar product operation feature space delta 
notice definition eq 
form delta represents linear separating surface feature space 
parameters solving qp problem maximize gamma subject ij positive constant sets cost willing pay misclassified data point 
kernel function defines kernel function type classifier exp gamma gaussian rbf polynomial degree tanh gamma theta multi layer perceptron fixed theta table possible kernel functions type decision surface define type decision surface machine build 
table list choices kernel allowed theory svm notice lead known classifiers decision surfaces known approximation properties 
nature qp problem turns subset different zero corresponding data points called support vectors data points contribute determine optimal separating hyperplane 
intuitively support vectors data points lie border classes feature space 
vapnik showed generalization error upper bound ratio number support vectors total number data points 
easy problems classes overlap feature space number support vectors small respect number data points 
case data set large say number support easily order require computation eq 
time pattern classified 
reduction run time complexity svms defined series heuristics techniques reduce computational effort spent evaluation eq 

outline follows section specify problem want solve comment possible approaches solution section offer possible solutions section experimental results section comment limitations suggested approaches concluding remarks 
motivation statement problem number operations required compute solution see eq 
easily bottleneck system needs performs massive number classifications 
examples issue arise face people detection systems svms distinguish object background checks zip code readers system required spend fractions second check envelope 
different ways obtain speedup 
approximating solution fl synthetic vectors necessarily data points anymore fl weights 
approach radial kernels resembles technique moving centers pursued burges procedure hard implement lacks principled way controlling approximation accuracy 

approximating solution fl support vector weight fl 
approach described detail section 
possible rewriting solution fl data points necessarily support vectors traditional definition weight fl 
hope approach find alternative efficient representation separating hyperplane problems expansion unique 
second approach described detail section 
class problems approach characterizing explicitly kind problems trying solve want example training ripley data set data points dimensions linearly separable linear svm yields roughly support vectors 
means separating hyperplane defined non zero coefficients 
presents data set support vectors separating hyperplane 
representation strikingly inefficient linear combination vectors define vector lives example simple linear classifier representative problems encountered complex examples involving nonlinear decision surfaces input spaces higher dimensionality 
happening 
available ftp markov stats ox ac uk pub neural papers ripley data set 
optimal separating hyperplane support vectors 
dotted lines hyperplane depict sigma range separating surface 

karush kuhn tucker optimality conditions training problem mandate misclassified training point points falling margin correctly classified 

non zero contributes expansion removed training set affecting solution 

data set non separable training errors expansion 
approaches ffl removing known errors data set training ffl removing errors expansion training ffl training removing errors obtained set support vectors retraining general give separating hyperplane errors needed expansion need taken account penalizing 
want approach problems number support vectors excessively high 
problem appears particular data set non separable relevant increase noise degree non separability size training set 
approach solution svm consists linear combination kernels support vector 
way simplify solution approximate linear combination subset kernels 
accomplished support vector regression machines 
natural extension svm problem function approximation seen problem finding minimum functional form jy gamma ffl oe oe smoothness functional jxj ffl vapnik ffl insensitive cost function defined ffl 
ffl gammaffl effect cost function jxj ffl technique robust outliers insensitive error certain threshold ffl 
minimum functional form ff kernel depends particular choice smoothness functional oe coefficients ff solving qp problem similar svm classification 
solution typically sparse coefficients ff different zero number critically controlled parameter ffl controls accuracy approximation 
relationship sparse approximation studied 
approach consists approximation scheme approximate function obtained solving svm classification obtain sparse version uses smaller number support vectors 
different choices kernels available choose kernel classification stage reasons 
ffl accuracy chosen arbitrarily small perfect approximation achieved provided parameter big 
important property consider approximation quality important speed 

properties kernel functions estimate distance kw gamma wk true separating hyperplane feature space sparse approximation 
kernel initially training built linear hyperplane feature space kernel takes advantage linearization 
algorithm formal algorithm stated 
train sv classifier kernel parameters choice 
step defines 
run data points support vector obtained step 
provide function form smaller number support vectors step high value parameter kernel desired ffl accuracy 
results obtained approach seen section 
second approach reformulating problem traditionally svm derived deriving constrained optimization problem finding optimal separating hyperplane maximum margin primal problem studying dual problem form 
common reason studying problem dual setting way avoids definition computation parameters hyperplane possibly infinite dimensional feature space solution depends scalar products feature space easily computed kernel possible solve primal formulation problem kernel fact show primal problem dual problem currently eq 
written minimize xi subject gamma free problem formulation refer primal reformulation initial structure original primal formulation incorporates kernel natural clear interpretation ffl shown proportional inverse margin feature space want minimize ffl constraint gamma models data point classified 
ffl captures data point degree separability pays linear penalty cost function 
show minimum problem gives separating surface classical approach uses possibly different expression lines done 
guaranteed new representation hyperplane sparse classical solution 
experimentally approach works reasons coefficients forced upper bound misclassifications lagrange multipliers anymore second starting initial solution helps keeping level unnecessary points expansion 
possible improvements clear advantage primal reformulation compared training problem typically solved include cost function certain attractor terms order benefit certain types expansion 
example ffl include small penalty form techniques enforce sparse representations 
ffl include small penalty points meet separability constraints exactly 
bring set coefficients subset support vectors obtained current training problem 
coefficients lagrange multipliers anymore values drastically different 
ffl include small penalty errors want penalties small essence cost function altered minimal way small penalties just coding preference possible linear combinations 
scheme gradually reduces penalties zero possible 
results obtained approach seen section 
experimental results datasets selected experiments different runs performed 
run corresponds different kernel parameter setting 
problems selected information regarding kernel parameters number support vectors training performance table 
training performance give reader idea separability data 
results obtained sv regression approach table 
notice reduction percentages runs electrons 
notice relationship ffl approximation quality number vectors obtained 
database data points kernel tr 
perf 
svs kwk skin pol skin pol electrons pol electrons rbf oe ripley linear ripley rbf oe table problems selected experiments 
database regression primal ref 
ffl gamma ffl gamma kw gamma wk kwk vec 
red 
kw gamma wk kwk vec 
red 
vec 
red 
skin theta gamma theta gamma skin theta gamma theta gamma electrons theta gamma theta gamma electrons theta gamma theta gamma ripley theta gamma theta gamma ripley theta gamma theta gamma table results obtained sv regression primal reformulation approaches 
column vec reflects number vectors expansion 
number corresponds percentage reduction indicated column red 
results obtained primal reformulation shown table 
case sv regression approach reduction percentages runs electrons 
limitations final remarks think solutions successful kind problem decided approach clear limitations discussed 
solution reduce run time complexity classifier coefficients strictly bounds 
situation tends occur data highly totally separable worsens dimensionality feature space grows 
date best cases reduced set method burges 

delayed column generation algorithm devised decomposing solving primal reformulation memory limitations prohibitive large data sets 
alternatives problem open area research 
feel approaches attacked solved problem excessive number support vectors active upper bound 
sv regression approach useful applied training filter limited memory requirements offers accuracy control approximation parameter ffl 
primal reformulation offers step training approach approximation fact gives exactly hyperplane obtained training current qp problem 
possible improvements suggested section technique enforce certain properties points linear combination fact give subset original support vectors loss accuracy approximation error 
primal reformulation natural structure suitable improvements technique 
comment considered reducing run time complexity sv classifier 
proposed methods easily adapted reducing complexity sv regression machines 
burges 
simplified support vector decision rules 
procedings th international conference machine learning pages 
cortes vapnik 
support vector networks 
machine learning 
girosi 
equivalence sparse approximation support vector machines 
neural computation 
press 
moody darken 
fast learning networks locally tuned processing units 
neural computation 
oren papageorgiou sinha osuna poggio 
pedestrian detection wavelet templates 
proc 
computer vision pattern recognition pages puerto rico june 
osuna freund girosi 
training support vector machines application face detection 
proc 
computer vision pattern recognition puerto rico june 
poggio girosi 
regularization algorithms learning equivalent multilayer networks 
science 
pontil verri 
properties support vector machines 
neural computation 
press 
vapnik 
nature statistical learning theory 
springer verlag 
