efficient top jacobian evaluation tree structured neural networks alois heinz institut fur informatik universitat freiburg am freiburg germany heinz informatik uni freiburg de tree structured neural networks tsnn known universal approximators easily derivable equivalent implementations feed forward neural networks hidden layers 
particularly interesting large scale real time applications adaptive control ability support efficient lazy evaluation exploiting hierarchical structure special kind sigmoid function 
new highly time space efficient algorithm evaluation jacobian matrix tsnn function extremely helpful kind search input space function inversion 
proposed algorithm differs earlier approaches performs top fashion avoiding bottom propagation larger intermediate results 
average case time worst case space requirements estimated shown superior predecessors magnitudes 
application areas neural networks adaptable models functions inputs outputs kind ir ir running application implies frequently repeated training re evaluation network function 
large scale real time applications strong demands efficient sublinear network size adaptation evaluation algorithms 
furthermore applications require line inversion network function derived object function inverse kinematics problem robotics inverse plant modeling control 
numerical inversion techniques involve iterated evaluations functions jacobian ir ir thetad th entry consists partial derivative th component respect th component input 
evaluation jacobian considered expensive operation ordinary feed forward neural networks dimensional row vectors forward propagated dimensional column vectors backward propagated neuron network 
different mixed propagation modes slightly enhanced efficiency may applicable inspect single neuron perform number operations 
tree structured neural networks tsnn support related different hierarchical models constructive learning algorithms 
known representational complexity ordinary feed forward networks particular capable simultaneous uniform approximation piecewise continuous function derivatives 
linearly transformed feed forward network hidden layers 
importantly tsnn possess strongly sublinear adaptation evaluation algorithms 
due hierarchical architecture special kind sigmoid function 
hitherto known bottom jacobian evaluation procedures tsnn take profit facts impeded necessity evaluate intermediate matrices 
remainder organized follows 
section give short tsnn functions 
section describe new top jacobian evaluation algorithm tsnn 
average case time worst case space requirements new algorithm estimated compared requirements predecessors section 
section provides 
tree structured neural networks tsnn input dimension output dimension defined labeled binary tree 
inner node equipped normalized weight vector ir threshold ir positive radius ir leaf labeled vector ir decision function ir associated inner node oe gamma lazy sigmoid function oe ir defined oe ae max gamma oe gammax derivative oe max gamma jxj 
evaluation respect input vector defined evaluation root root evaluation leaf defined evaluation inner node recursively defined gamma kr denote left right descendants respectively 
note recursive definition tsnn evaluation implies flow vector valued information bottom tree top 
applying known automatic differentiation modes evaluation procedure yields program computes jacobian propagates theta matrices bottom 
top jacobian evaluation order develop faster jacobian evaluation algorithm recursive equations defining reformulated 
responsibility oe node tsnn respect defined oe root oe gamma oe suppose leafs responsibility vector phi ir respect oe lm th component leaf weight matrix ir theta lm th column 
easy verify phi note independent components phi non negative sum unity 
phi usually sparse vector 
jacobian described phi components phi computed recursive scheme oe root oe gamma gammaoe oe oe important notice th row phi ir thetad consists zeros lm belongs left subtree ancestor decision function right subtree 
case responsibility lm zero contribution lm jacobian 
complete recursive top jacobian evaluation algorithm fig 

visits nodes non disappearing responsibilities respect actual vector responsibilities gradients respect propagated top bottom 
leaf reached determines capacity tree 
easy see number training examples represented exactly grows linearly 
top proc node vector oe real roe vector matrix local vector integer integer leaf vk theta roe od od gamma ck rk oe roe theta gamma gamma oe theta theta wk top oe theta gamma fi roe theta oe theta theta wk top kr oe theta fi fi recursive top procedure top computes jacobian tsnn function 
global matrix initialized zeros procedure called arguments root tree vector root responsibility gradient dimensional vector zeros non contributing subtrees visited 
contribution added global matrix initialized zeros procedure called root vector root responsibility gradient matrix parameters 
inner node reached recursive initiated descendants depending value decision function 
time space requirements number inner nodes leafs tsnn visited certain call top depends critically input vector order estimate algorithm average case resource requirements assume know upper bound inner node side left right conditional probability event visited visited bounded 
note conditional probability visiting descendants node upper bounded density distribution input vectors known doe ks doe assumptions probability node height visited equal non increasing function trees nodes including leafs trees show worst case behavior levels smaller blog filled nodes remaining nodes level 
computing reasonable values observed experiments ranged 
functions ratio plotted values 
relative numbers nodes leaf nodes tsnn nodes visited procedure top average bounded respectively 
parameter depends distribution input vectors 
sum nodes probabilities dividing yields bound relative number nodes visited average case np log gamma proportion number leaf nodes visited average case number nodes similarly bounded log gamma displays plots functions ratio interesting practical values shown lim considerations possible determine bounds time space requirements top average case time consumption cdn theta dn theta gamma cd operations matrix elements performed responsible leaf operations involving vector elements responsible inner node 
compared running time earlier bottom algorithm cdn theta great improvement equation 
worst case space requirement top additional global tree structure global matrix height tree log tree suitably balanced 
improvement compared space requirement bottom algorithm cd 
notice time space requirements strongly sublinear model size long tree balanced great improvement compared requirements ordinary neural networks 
bounds number matrix operations large trees reduced factor slightly larger see right part fig 

new top jacobian evaluation algorithm treestructured neural networks 
time space requirements shown minor earlier known algorithms magnitudes 
new algorithm strongly supports large scale real time applications heavy demand function inversion fields robotics control 
kindermann linden 
inversion neural networks gradient descent 
journal parallel computing 
bertsekas 
nonlinear programming 
athena scientific belmont ca 
demers kreutz delgado 
solving inverse kinematics redundant manipulators 
van editors neural systems robotics pages 
academic press new york 
brown harris 
adaptive modelling control 
systems control engineering 
prentice hall international london uk 
dennis schnabel 
numerical methods unconstrained optimization nonlinear equations 
prentice hall englewood cliffs nj 

simple automatic derivative evaluation program 
comm 
acm 
bryson 
ho 
applied optimal control 
new york 
revised printing new york hemisphere 
baur strassen 
complexity partial derivatives 
theoretical computer science 
rumelhart hinton williams 
learning representations back propagating errors 
nature 
yoshida 
derivation computational process partial derivatives functions transformations graph 
transactions information processing society japan 
griewank reese 
calculation jacobian matrices markowitz rule 
pages 
siam 
yoshida 
node elimination rule calculation jacobian matrices 
proceedings second international siam workshop computational differentiation santa fe nm philadelphia 
siam 
heinz 
tree structured neural network real time adaptive control 
amari xu 
chan king 
leung editors progress neural information processing proceedings international conference neural information processing hong kong volume pages berlin september 
springer 
ripley 
pattern recognition neural networks chapter treestructured classifiers pages 
cambridge university press cambridge 
griewank corliss editors 
automatic differentiation algorithms theory implementation application 
siam philadelphia penn 
