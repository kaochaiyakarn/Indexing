memory approaches reinforcement learning non markovian domains long ji lin tom mitchell may cmu cs school computer science carnegie mellon university pittsburgh pa reinforcement learning type unsupervised learning sequential decision making 
qlearning probably best understood reinforcement learning algorithm 
learning agent learns mapping states actions utilities 
important assumption learning markovian environment assumption meaning information needed determine optimal actions reflected agent state representation 
consider agent state representation solely immediate perceptual sensations 
sensors able essential distinctions world states markov assumption violated causing problem called perceptual aliasing 
example facing closed box agent current visual sensation act optimally optimal action depends contents box 
basic approaches addressing problem sensors history current world state 
studies connectionist approaches learn history handle perceptual aliasing window recurrent recurrent model architectures 
empirical study architectures 
relative strengths weaknesses discussed 
research supported part fujitsu laboratories part avionics lab wright research development center aeronautical systems division afsc air force wright patterson afb oh contract arpa order 
views contained document authors interpreted representing official policies expressed implied fujitsu laboratories government 
keywords reinforcement learning markov non markov decision task action model neural network recurrent neural network contents perceptual aliasing memory architectures architectures experimental designs results task cup collection task task random features task task control errors task pole balancing discussion architecture characteristics related experience replay algorithm parameter settings experiments pole balancing problem reinforcement learning paradigm learning agent continually receives inputs sensors determines action inputs control policy executes action receives scalar reinforcement payoff 
goal agent construct optimal policy maximize performance measured discounted cumulative reinforcement short utility 
best understood reinforcement learning algorithm probably learning watkins lin 
idea learning construct function state action utility function predict discounted cumulative reinforcement called utility value state action pair agent state executes action 
function control policy simply choose action maximal 
function incrementally constructed temporal difference td methods sutton related dynamic programming barto 
state transition meaning action response state results new state immediate payoff function updated way delta fl delta max gamma fl discount factor learning rate 
note update rule td 
update rule sophisticated 
detailed description algorithm see appendix lin lin 
watkins shown learning converge find optimal policy primary conditions weak ones look table representation environment markovian 
means time state environment determined current state action taken 
environments information needed determine current optimal action reflected current state representation 
condition disallows generalization state space large look table method unacceptable generalization method modeling function 
example lin lin lin successfully combined connectionist error back propagation algorithm learning solve nontrivial learning problems 
consider reinforcement learning agent state representation immediate sensation 
sensors able essential distinctions world states markov assumption mentioned violated causing problem called perceptual aliasing whitehead ballard 
example consider packing task involves steps open box put gift close seal 
agent driven current visual percepts accomplish task facing closed box agent know gift box decide seal open box 
solutions problem 
solution actively choose perceptual action measuring weight box resolve ambiguity 
second solution history information gift put box help determine current world state 
previous studying kind solutions whitehead ballard tan 
focus second kind solutions 
rest organized follows 
distinguish types perceptual aliasing followed connectionist architectures problem 
network architecture proposed model learning learning 
empirical studies architectures relative strengths weaknesses discussed 
section discusses related 
perceptual aliasing distinguish types perceptual aliasing voluntary involuntary chrisman personal communication 
consider agent set sensing operations allow agent complete distinctions world states 
sensing operations costly due time constraints physical resource constraints agent choose apply subset available sensing operations 
choice limited sensing operations tends perceptual aliasing occur available operations call voluntary perceptual aliasing 
whitehead ballard proposed lion algorithm learns focus perceptual attention order collect necessary sensory information optimal control 
tan proposed cost sensitive algorithm learns choose minimal sensing operations disambiguate world states 
algorithms called sensor approaches voluntary perceptual aliasing 
contrast approaches may characterized memory 
involuntary perceptual aliasing due limitations sensors available agent 
example color object closed box identified vision 
optimal action depends color agent rely memory sensors 
memory history information control non markovian environment referred memory approach perceptual aliasing 
note voluntary perceptual aliasing handled memory approaches 
hand situations memory approaches fail 
consider example paragraph 
agent got chance see color object packed box memory approaches certainly fail situation 
deal complex real world problems efficiently sensor memory approaches needed 
integration types approaches remains investigated 
memory architectures depicts memory architectures reinforcement learning non markovian domains 
type indirect control direct control 
architectures utility net action utility model sensation action history features memory utility net sensation action sensation payoff net sensation action history features memory current sensation sensations actions memory architectures reinforcement learning non markovian domains window architecture recurrent architecture recurrent model architecture 
temporal difference methods incrementally learn function represented neural networks 
just current sensation state representation window architecture uses current sensation sensations actions taken state representation 
words window architecture allows direct access information past sliding window 
called window size 
window architecture simple straightforward problem may know right window size advance 
window size chosen large history information construct optimal function 
window size needs large large number units input layer require lot training patterns successful learning generalization 
spite problems worth study kind time delay neural networks quite successful speech recognition waibel domains 
window architecture sort brute force approach history information 
alternative distill small set history features large amount information past 
set features current sensation agent state output units hidden units input units context units feedback elman network 
representation 
optimal control actions determined just new state representation set history features reflects information needed optimal control 
recurrent recurrent model architectures illustrated idea ways construct history features different 
architecture architectures principle discover utilize history features depend sensations arbitrarily deep past practice difficult achieve 
recurrent neural networks elman networks elman provide way construct history features 
illustrated input layer elman network divided parts true input units context units 
context units hold feedback signals coming network state previous time step 
context units function memory remember aggregate previous network states output network depends past current input 
recurrent architecture uses recurrent network model function 
predict utility values correctly recurrent network called recurrent net forced learn history features enable network properly assign different utility values states sensation 
recurrent model architecture consists concurrent learning components learning action model learning 
action model function maps sensation action sensation immediate payoff 
predict behavior environment recurrent action model forced learn set history features 
current sensation set history features state representation turn non markovian task markovian solve conventional learning perfect action model available 
simply time state environment completely determined new state representation action taken 
recurrent recurrent model architectures learn history features gradient descent method error back propagation differ important way 
model learning goal minimize errors actual predicted sensations payoffs 
essence environment provides needed training information consistent time long environment change 
recurrent learning goal minimize errors temporally successive predictions utility values sutton lin 
error signals computed partly information environment partly current approximation true function 
changes time carries information learning 
words error signals general weak noisy inconsistent time 
recurrent architecture practice may questioned 
general action model trained predict new sensation immediate payoff 
consider packing task involves steps put gift open box seal box opened place box proper bucket depending color gift box 
reward box placed right bucket 
note agent required know gift color order predict sensations box opened sealed 
model predicts sensations right features needed accomplish task 
tasks described section action model need predict immediate payoff order discover history features needed optimal control 
worthwhile note combinations architectures possible 
example combine architectures inputs recurrent net include just current sensation sensations 
combine architectures memory shared recurrent model recurrent net history features developed error signals coming model net 
concerned basic architectures 
investigation needed see kinds combination result better performance basic versions 
architectures function action model take kinds inputs sensation action 
alternative network structures realize 
monolithic network input units encode sensation action 
domains discrete actions structure undesirable monolithic network trained model highly nonlinear function sensation history features different actions may demand different outputs network 
alternative call action network architectures 
multiple networks action represent function model 
illustrates types recurrent architectures linear nonlinear 
linear consists single layer perceptrons nonlinear consists multilayer networks units nonlinear squashing function 
time network corresponding selected action compute values output context units ignored 
important note ensure multiple networks distributed representation history features 
achieved output history features shared input units context units output units output units input units context units output units output input units context units hidden units input units context units hidden units output units sensation history features sensation architectures recurrent models recurrent nets linear nonlinear 
linear consists multiple perceptrons nonlinear consists modified elman networks 
cases time network corresponding selected action compute values output context units 
having networks share activations context units reinforced case nonlinear having networks share connections emitting hidden layer 
empirically sharing connections necessary tends help 
shown input layer network linear nonlinear divided parts true input units context units seen elman network 
note recurrent networks modified elman network 
differences standard elman network modified version 
elman network back propagation time 
see 
second hidden layer elman network fed back input layer portion hidden units fed back 
consider environment little perceptual aliasing words feed forward networks sufficient model aspects environment just history feature needs discovered order model environment 
case sense hidden units just context unit 
limited experience elman network tended need context units modified 
consequence normally required connections 
furthermore context units part inputs net context units require training patterns training time part learning 
third difference constant feedback weight elman network uses slightly greater say tends network converge sooner 
reason help early phase training activations context units normally close zero symmetric squashing function 
context units appear unimportant compared inputs 
magnifying activations back propagation algorithm pay attention context units 
linear architecture shown fact architecture proposed mozer bachrach applied learn model finite state automata fsa demonstrated success domains 
model function 
conventional recurrent networks elman network spectacularly unsuccessful modeling fsa 
may explained fact experiments took monolithic approach network actions 
contrast experiments nonlinear architecture quite successful 
learned perfectly fsa able learn training data needed acquire perfect models due fact nonlinear networks degrees freedom connections linear ones 
model fsa probably need nonlinear linear may suffice outperform nonlinear 
applications nonlinear necessary replaced linear 
example functions general nonlinear 
pole balancer discussed action model nonlinear 
sake comparing results various experiments nonlinear recurrent nets models experiments 
hidden units output units nonlinear case symmetric sigmoid squashing function gammax gamma 
slightly modified version back propagation algorithm rumelhart adjust network weights 
back propagation time bptt rumelhart significant improvement back propagation bptt 
apply bptt recurrent networks completely unfolded time errors back propagated chain networks weights modified cumulative gradients 
restrict influence output errors time gradients computed times earlier applied decay errors propagated context units time hidden layer time gamma 
better performance decay 
decay 
context units initialized start task instance 
clarify mean able learn perfect model 
finite set input output patterns generated unknown finite state automaton fsa infinite number fsa fit perfectly training data 
moment model learning algorithm sure learned exactly fsa producing training data characteristic example upper bound size target fsa known advance 
say architectures learn fsa perfectly mean predict environment perfectly long period time say steps 
actions walk left walk right pick binary inputs left cup right cup left collision right collision reward cup picked possible initial states task cup collection task 
experimental designs results section describe study architectures solving various learning problems different characteristics 
study expect gain insights architectures architectures may best types problems 
parameters experiments discount factor fl recency factor td methods learning rate number context units number hidden units window size fixed experiments 
appendix gives detailed description parameter settings chosen give roughly best performance 
experience replay algorithm described appendix significantly reduce number trials needed learn optimal control policy 
experience replay learning agent remembers past action sequences repeatedly applies learning algorithm sequence chronologically backward order 
experiments experience trials replayed train function action model trained experience past 
experiment consisted interleaved phases learning phase test phase 
learning phase agent chose actions stochastically boltzmann distribution lin 
randomness action selection ensured sufficient exploration agent learning 
test phase agent took best actions current policy 
learning curves shown describe performance test phase 
task cup collection started simple cup collection task 
task requires learning agent pick cups located space 
agent actions walking right cell walking left cell pick 
agent executes pick action pick cup cup located agent current cell 
agent sensation includes binary bits bits indicating cup immediate left right cell bits indicating previous action results collision left right 
action attempting move agent space cause collision 
cups placed far apart agent picks cup see 
act optimally agent remember location second cup 
task trivial reasons agent sense cup front agent gets reward cups picked agent operates cup sight especially picking cup 
note restrict problem possible initial states shown 
reason restriction simplify task avoid perceptual aliasing history information available 
optimal policy requires steps pick cups situation 
shows learning curves architectures 
curves show mean performance runs different seeds random number generator 
axis indicates number steps pick cups task instances shown time steps 
axis indicates number trials agent attempted far 
trial agent started possible initial states stopped cups picked time 
experimented different values parameter things obvious learning curves 
architectures successfully learned optimal policy 
second gave better performance 
consider situation consecutive states bad state displays sensation state td method estimates utility state example just immediate payoff rc utility state state displays sensation bad state utility state underestimated features learned distinguish states features learned utilities state preceding states underestimated 
problem mitigated td method estimates utility state rc state rewards rd re states state 
additional observations shown window size performance obtained 
matter fact optimal policy learned occasionally 
imagine agent picked cup walking second cup 
steps sensations including current fact provide agent information sensation bits 
agent learn optimal policy 
way worked picking cup agent determines right direction move 
agent simply follows general direction previous actions headed 
words agent action choices medium passing information past 
performance task window architecture recurrent architecture recurrent model architecture 
rc rd re difficult situation td 
consecutive states bad state 
rc rd re payoffs received state transitions 
states display sensation 
robustness 
policy learned window architecture robust fool manner suppose agent learned optimal policy 
suddenly interrupt agent force move opposite direction steps 
agent resume cup collection task optimally policy choose continue new direction opposite optimal direction see paragraph 
contrast policy learned recurrent model architecture robust appeared choose actions actual world state actions taken 
examined policy learned recurrent architecture 
appeared recurrent architecture learned policy appeared choose actions actual world state action choices 
instructive see history features learned model versus recurrent net 
plan study 
imperfect model 
agent learned perfect model trials 
instance agent seen cup steps model normally able predict appearance cup 
imperfect model prevent learning learning optimal policy 
reveals recurrent model architecture need learn perfect model order learn optimal policy 
needs learn important aspects environment 
know advance important task attempting learn including details environment relevant predicting sensation unnecessary optimal control 
computation time 
recurrent recurrent model architectures optimal policy number trials actual cpu time taken different 
took minutes complete run took minutes 
recurrent model architecture learn model net model network bigger recurrent net 
experiment revealed lessons ffl architectures worked simple problem 
ffl recurrent model architecture just partially correct model may provide sufficient history features optimal control 
news perfect model difficult obtain 
task task random features task simply task random bits agent sensation 
random bits simulate difficult predict irrelevant features accessible learning agent 
real world features difficult predict fortunately relevant task solved 
example predicting going rain outside difficult matter task pick cups inside 
ability handle difficult predict irrelevant features important learning system practical 
shows learning curves architectures task 
curves mean performance runs 
see random features gave little impact performance window architecture recurrent architecture negative impact recurrent model architecture noticeable 
recurrent model find optimal policy times trials 
just optimal policy optimal policy sub optimal policies 
observed model tried vain reduce prediction errors random bits 
possible explanations poorer performance compared obtained random sensation bits 
model fail learn history features needed solve task effort wasted random bits 
second activations context units shared model network net change representation history features model part simply destabilize trained net change significant 
explanation ruled optimal policy times 
test second explanation fixed model point learning allowed changes net 
setup agent optimal policy stuck 
experiment reveals lessons ffl recurrent architecture economic recurrent model architecture sense try learn history feature appear relevant predicting utilities 
ffl potential problem recurrent model architecture changes representation history features model part may cause instability net part 
task task control errors noise uncertainty prevail real world 
study capability architectures handle noise added control errors agent actuators time executed action effect environment 
random bits removed 
shows mean performance architectures runs 
note performance task window architecture recurrent architecture recurrent model architecture 
turned control errors tested performance agent optimal number steps 
runs window architecture successfully optimal policy runs suboptimal policies 
spite control errors recurrent architecture learned optimal policy little instability 
recurrent model architecture optimal policy trials policy optimal sub optimal ones due changing representation history features happened task 
find way model example gradually decreasing learning rate able obtain stable optimal policy 
short learned lessons experiment ffl architectures handle small control errors degree 
ffl architectures recurrent scale best presence control errors 
task pole balancing traditional pole balancing problem balance pole cart cart position pole angle cart velocity pole angular velocity 
see appendix detailed description problem 
problem studied times anderson nontrivial control problem due sparse reinforcement signals gamma pole falls past degrees 
task problem cart position pole angle learning agent 
balance pole agent learn features velocity 
experiment policy considered satisfactory pole balanced steps test trials pole starts angle sigma sigma sigma degrees 
maximum initial pole angle pole balanced indefinitely degrees 
training phase pole angles cart positions generated randomly 
initial cart velocity pole velocity set 
experiment 
input representation straightforward real valued input unit pole angle cart position 
table shows number trials taken architecture satisfactory policy learned 
numbers average results best runs 
satisfactory policy trials 
contrast memoryless learning took trials solve traditional pole balancing problem agent cart position pole angle cart velocity pole angular velocity 
lesson learned experiment experiment sensing errors 
pointed bachrach train recurrent model properly noisy environments may need alter learning procedure slightly 
precisely current sensations trusted predicting sensations 
trust model predicted sensations model gets better 
performance task window architecture recurrent architecture recurrent model architecture 
table performance pole balancing task 
method window recurrent recurrent model trials ffl recurrent architecture suitable architecture cup collection tasks outperformed architectures pole balancing task 
discussion experiments provide insight performance memory architectures 
section consider problem characteristics determine architecture appropriate task environments 
architectures exhibit different advantages relative importance varies task parameters ffl memory depth 
important problem parameter length time agent remember previous inputs order represent optimal control policy 
example memory depth task evidenced fact agent able obtain optimal control window size 
memory depth pole balancing task 
note learning optimal policy may requires larger memory depth needed represent policy 
ffl payoff delay 
cases payoff zero goal state define payoff delay problem length optimal action sequence leading goal 
parameter important influences difficulty learning 
payoff delay increases learning accurate function increasingly difficult due increasing difficulty credit assignment 
ffl number history features learned 
general perceptual aliasing agent faces history features agent discover difficult task 
general predicting sensations model requires history features predicting utilities net turn requires history features representing optimal policies 
consider task example 
binary history features required determine optimal actions cup front second cup right hand side left hand side 
perfect function requires features cups picked far far second cup 
perfect model task requires features perfect function 
perfect model task requires features current state random number generator perfect function task requires extra features 
important note need perfect function perfect model order obtain optimal policy 
function just needs assign value action response situation relative values right order model just needs provide sufficient features constructing function 
architecture characteristics problem parameters understand architectures best suited types problems 
consider key advantages disadvantages architecture problem parameters influence importance characteristics 
ffl recurrent model architecture 
key difference architecture recurrent architecture learning history features driven learning action model function 
strength approach agent obtain better training data action model function making learning reliable efficient 
particular training examples action model sensation action sensation payoff 
quadruples directly observable step agent takes environment 
contrast training examples function sensation action utility 
triples directly observable agent estimate training utility values changing approximation true function 
second strength approach learned features dependent environment independent reward function action model may trained predict rewards sensations reused agent different reward functions goals learn achieve 
ffl recurrent architecture 
architecture suffers relative disadvantage learn indirectly observable training examples offsetting advantage need learn history features relevant control problem 
history features needed represent optimal action model superset needed represent optimal function 
easily seen noticing optimal control action principle computed action model look ahead search 
cases features necessary predicting utilities needed predict completely state number history features learned recurrent architecture smaller number needed recurrent model architecture 
ffl window architecture 
primary advantage architecture learn state representation recursively recurrent network architectures 
recurrent networks typically take longer train non recurrent networks 
advantage offset disadvantage history information limited features directly observable fixed window captures bounded history 
contrast recurrent network approaches principle represent history features depend sensations arbitrarily deep agent history 
competing advantages architectures imagine preferred architecture different types problems ffl expect advantage window architecture greatest tasks memory depths smallest example pole balancing task 
ffl expect recurrent model architecture advantage directly available training examples important tasks payoff delay longest example pole balancing task 
situations indirect estimation training values problematic recurrent architecture 
ffl expect advantage recurrent architecture need learn features relevant control pronounced tasks ratio relevant irrelevant history features lowest example cup collection task random features 
recurrent model architecture acquire optimal policy long just relevant features learned drive learning irrelevant features may cause problems 
representing irrelevant features may limited context units sacrifice learning relevant features 
secondly seen experiments recurrent model architecture subject instability due changing representation history features change improves model deteriorate function needs re learned 
expect window architecture general fail disambiguate world states window size chosen large represent optimal policy 
consider cup collection task 
picking cup agent walks back forth steps 
note kind random walks occur early learning 
window agent know world state currently knowing requires history information available window 
window agent follows optimal path bounded history suffices identify world state path 
contrast trained recurrent model keep track state transitions able know actual world state situation 
tapped delay line scheme window architecture uses widely applied speech recognition waibel turned quite useful technique 
mentioned expect control tasks speech recognition important difference tasks 
major task speech recognition find temporal structure exists sequence speech phonemes 
learning control agent look temporal structure generated actions 
actions generated randomly case early learning find sensible temporal structures action sequence improve action selection policy 
related mentioned section basic approaches addressing problem perceptual aliasing sensor approaches memory approaches 
whitehead ballard tan examples sensor approaches 
sensor approaches assume existence sensory operations actual world state unambiguously identified need look back past 
assume deterministic environments lookup table representations function 
years multilayer neural network recurrent network emerged important components controlling nonlinear systems hidden states perceptual aliasing 
illustrates control architectures literature 
architectures recurrent networks just feedforward networks 
principle architectures modified control systems hidden states introducing time delay networks recurrent networks architectures 
note types critic type type 
type critic evaluation function state action pairs equivalent function critic may multiple outputs example output discounted cumulative pain output discounted cumulative pleasure 
nets modified multiple outputs 
type critic evaluation function states 
temporal difference td methods employed learn types critic 
adaptive heuristic critic ahc architecture proposed sutton studied researchers anderson lin 
time action taken action rewarded punished leads better worse results measured critic 
time critic trained td methods 
architecture assumes discrete actions 
back propagated adaptive critic bac architecture proposed werbos werbos 
architecture assumes availability desired utility time 
example desired utility pole balancing system time pole falls 
assumption assumption critic action model learned control policy trained back propagating difference desired utility actual critic output critic model policy network networks formed large feed forward network 
gradients obtained policy network indicate change policy maximize utility 
note architecture handle continuous actions 
architecture shown described werbos schmidhuber 
architecture assumes availability desired outputs system controlled 
assumption errors actual outputs desired outputs back propagated model policy network 
gradients obtained policy network indicate change policy obtain desired outputs system 
note architecture handle continuous actions 
jordan jacobs proposed control architecture known net utility critic action state policy model utility critic action state policy new state utility critic action state policy model utility critic new sensation sensation action model mem action sensation new sensation reinforcements policy action model action state policy new state reinforcements model mem action sensation new sensation reinforcements critic utility critic mem action sensation utility control architectures 
architecture somewhat similar bac architecture model type critic uses type critic 
architecture assumes availability desired utility handle continuous actions 
architecture shown described thrun moller 
trained model architecture performs multiple step look ahead planning find best action situation 
input situation action best training pattern policy network trained mimic multiple step planning process step 
architecture handle continuous actions 
architecture shown proposed bachrach 
assuming action model learned architecture trains type critic td methods 
control policy simply choose action critic output maximal 
architecture assumes discrete actions 
proposed recurrent model architecture shown replicated similar bachrach architecture 
assume discrete actions require availability desired utility 
differences architecture uses type critic type critic type critic uses actual current sensation inputs type critic uses predicted sensation inputs 
correct predictions type critic strongly relies correct predictions model expect recurrent model architecture outperform bachrach architecture learning agent rich sensations unable learn action model 
recurrent model architecture similar control architecture proposed chrisman 
main difference model learning gradient descent maximum likelihood estimation 
architectures discussed section far consist components 
recurrent architecture hand consists component critic 
critic directly choose actions assuming actions enumerable number actions finite 
presents memory architectures reinforcement learning nonmarkovian domains window recurrent recurrent model architectures 
architectures idea history information discriminate situations indistinguishable immediate sensations 
shown architectures capable learning non markovian tasks 
able handle irrelevant features small control errors degree 
discussed strengths weaknesses architectures solving tasks various characteristics 
performance reported obtainable techniques experience replay large values back propagation time 
general summary architectures 
surprisingly recurrent architecture promising thought study fact expect architecture 
long memory depth payoff delay required task large architecture appears effectively 

tasks small window size sufficient remove perceptual aliasing window architecture 
unable represent optimal control policy memory depth problem greater 
recurrent model approach quite costly apply model learning takes lot effort necessary 
difficulty general problem model learning recognized 
methods truly successful 
example diversity inference procedure proposed rivest schapire restricted deterministic discrete domains 
instance approaches moore suitable nondeterministic continuous domains learn history features 
chrisman studies model learning method statistical test 
method handle nondeterminism history features scale 
think architectures proposed scale problems large memory depths 
hand model learned task may re usable tasks 
mentioned combinations architectures possible may give better performance basic versions 
study remains done 
chris atkeson ronald williams rich sutton sebastian thrun fruitful discussions issues related 
chrisman sebastian thrun helpful comments draft 
research supported part fujitsu laboratories part avionics lab wright research development center aeronautical systems division afsc air force wright patterson afb oh contract arpa order 
experience replay algorithm shows experience replay algorithm train functions 
define terms 
experience quadruple meaning time action response state results state reinforcement lesson sequence experiences starting initial state final state goal achieved 
experience replay agent remembers lesson repeatedly presents experiences lesson chronologically backward order algorithm depicted fl fl discount factor recency parameter td methods sutton 
replay 



fl gamma 
adjust network implementing back propagating error gamma 
exit gamma go experience replay algorithm idea algorithm follows 
roughly speaking expected return called td return written recursively watkins fl gamma note term brackets equation expected return time weighted sum return predicted current function return received replayed lesson 
backward replay exactly discounted cumulative reinforcements time lesson 
learning function usually far correct actual return better estimate expected return predicted function 
function accurate function provides better prediction 
particular replayed lesson far back past involves bad choices actions actual return lesson smaller received current policy case better 
ideally want hold true 
difference sides error temporally successive predictions expected return 
reduce error networks implement function adjusted back propagation algorithm step 
set step undefined 
discussions see lin 
experience replay algorithm batch mode incremental mode 
incremental mode nets adjusted replaying experience batch mode nets adjusted replaying lesson 
advantage backward replay greatest algorithm incremental mode 
non recurrent nets trained incremental mode recurrent nets batch mode avoid instability 
table parameter settings task task task task window task task recurrent task recurrent model cm cm cm cm hm hm hm hm jm jm jm jm parameter settings experiments parameters experiments included ffl discount factor fl fixed ffl recency factor ffl range random initial weights networks fixed ffl momentum factor fixed perceptrons multilayer networks ffl window size ffl learning rate nets ffl learning rate action models ffl number hidden units nets ffl number hidden units action models hm ffl number context units nets ffl number context units action models cm ffl temperature controlling randomness action selection ffl see 
table shows parameter settings generate mean performance described 
note means perceptrons nets 
parameter values empirically chosen give roughly best performance 
experience replay algorithm replay experience past 
replayed experience trials replayed actions probabilities pole balancing problem 
chosen stochastic action selector greater 
see lin lin discussions 
value cup collection tasks pole balancing task depending recency experience replayed 
pole balancing problem dynamics cart pole system equations motion sutton anderson sin gamma cos gamma sgn sin cos gammaf gammam sin mc mp gamma mp gamma mp cos mc mp acceleration due gravity kg mass cart kg mass pole half pole length coefficient friction cart track coefficient friction pole cart sigma force applied cart center mass time time seconds corresponding simulation step sgn reinforcement function defined radian jx 
anderson anderson strategy learning multilayer connectionist representations 
proceedings fourth international workshop machine learning pages 
bachrach bachrach connectionist modeling control finite state environments 
ph thesis department computer information sciences university massachusetts 
barto barto bradtke singh real time learning control asynchronous dynamic programming 
technical report computer science department university massachusetts 
chrisman chrisman reinforcement learning perceptual aliasing predictive distinctions approach 
appear aaai 
elman elman finding structure time 
cognitive science 
jordan jacobs jordan jacobs learning control unstable system forward modeling 
touretzky ed advances neural information processing systems pages morgan kaufmann 
lin lin long ji 
programming robots reinforcement learning teaching 
proceedings aaai pages 
lin lin long ji 
self improving reactive agents reinforcement learning planning teaching 
machine learning press 
moore moore efficient memory learning robot control 
ph thesis technical report computer laboratory university cambridge 
mozer bachrach mozer bachrach connectionist architecture inferring structure finite state environments 
machine learning 
rivest schapire rivest schapire diversity inference finite automata 
proceedings eighth annual symposium foundations computer science pages 
rumelhart rumelhart hinton williams learning internal representations error propagation 
parallel distributed processing explorations microstructure cognition vol bradford books mit press 
schmidhuber schmidhuber reinforcement learning markovian nonmarkovian environments 
touretzky ed advances neural information processing systems pages morgan kaufmann 
sutton sutton temporal credit assignment reinforcement learning 
phd thesis department computer information science university massachusetts 
sutton sutton learning predict methods temporal differences 
machine learning 
tan tan ming 
learning cost sensitive internal representation reinforcement learning 
proceedings eighth international workshop machine learning pages 
thrun moller sebastian thrun knut moller 
active exploration dynamic environments 
touretzky ed advances neural information processing systems morgan kaufmann 
waibel waibel modular construction time delay neural networks speech recognition 
neural computation 
watkins watkins learning delayed rewards 
phd thesis king college cambridge 
werbos werbos building understanding adaptive systems statistical numerical approach factory automation brain research 
ieee transactions systems man cybernetics 
werbos werbos generalization back propagation applications recurrent gas market model 
neural networks 
werbos werbos menu designs reinforcement learning time 
miller sutton werbos editors neural networks control 
mit press cambridge ma 
whitehead ballard whitehead ballard learning perceive act trial error 
machine learning 
