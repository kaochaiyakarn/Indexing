revised version appear advances neural information processing systems 
infinite gaussian mixture model carl edward rasmussen department mathematical modelling technical university denmark building dk lyngby denmark carl imm dtu dk bayesian mixture model necessary priori limit number components finite 
infinite gaussian mixture model neatly difficult problem finding right number mixture components 
inference model done efficient parameter free markov chain relies entirely gibbs sampling 
major advantages bayesian methodology overfitting avoided difficult task adjusting model complexity vanishes 
neural networks demonstrated neal infinite networks led gaussian process models williams rasmussen 
markov chain monte carlo mcmc implementation infinite hierarchical gaussian mixture model 
surprisingly inference models possible finite amounts computation 
similar models known statistics dirichlet process mixture models go back ferguson antoniak 
usually expositions start dirichlet process west derive model limiting case wellknown finite mixtures 
bayesian methods mixtures unknown finite number components explored richardson green methods easily extended multivariate observations 
finite hierarchical mixture finite gaussian mixture model components may written yj means precisions inverse variances mixing proportions positive sum normalised gaussian specified mean variance 
simplicity exposition initially assume scalar observations comprise training data fy yn consider models fixed value explore properties limit 
gibbs sampling known technique generating samples complicated multivariate distributions monte carlo procedures 
simplest form gibbs sampling update variable turn conditional distribution variables system 
shown gibbs sampling generates samples joint distribution entire distribution explored number gibbs sweeps grows large 
introduce stochastic indicator variables observation role encode class generated observation indicators take values indicators referred missing data mixture model context 
sections priors component parameters hyperparameters specified conditional distributions needed gibbs sampling derived 
general form priors chosen hopefully reasonable modelling properties eye mathematical convenience conjugate priors 
component parameters component means gaussian priors 
mean precision hyperparameters common components 
hyperparameters vague normal gamma priors exp mean variance observations shape parameter gamma prior set unity corresponding broad distribution 
conditional posterior distributions means obtained multiplying likelihood eq 
conditioned indicators prior eq 
jc occupation number number observations belonging class mean observations 
hyperparameters eq 
plays role likelihood priors eq 
give conditional posteriors standard form kr kr rj component precisions gamma priors 
shape mean hyperparameters common components priors inverse gamma gamma form exp strictly speaking priors ought depend observations 
current procedure equivalent normalising observations unit priors 
wide variety reasonable priors lead similar results 
conditional posterior precisions obtained multiplying likelihood eq 
conditioned indicators prior eq 
jc 
hyperparameters eq 
plays role likelihood priors eq 
give js 
exp 
exp density standard form shown log js log concave may generate independent samples distribution log adaptive rejection sampling ars technique gilks wild transform get values mixing proportions symmetric dirichlet known multivariate beta prior concentration parameter dirichlet mixing proportions positive sum 
mixing proportions prior occupation numbers multinomial joint distribution indicators kronecker standard dirichlet integral may integrate mixing proportions write prior directly terms indicators 
order able gibbs sampling discrete indicators need conditional prior single indicator easily obtained eq 
keeping single indicator fixed jjc subscript indicates indexes number observations excluding associated component posterior indicators derived section 
lastly vague prior inverse gamma shape put concentration parameter exp likelihood may derived eq 
prior eq 
gives jk exp notice conditional posterior depends number observations number components observations distributed components 
distribution log jk log concave may efficiently generate independent samples distribution ars 
infinite limit far considered fixed finite quantity 
section explore limit final derivations regarding conditional posteriors indicators 
model variables indicators conditional posteriors infinite limit obtained substituting number classes data associated rep equations previously derived finite model 
indicators letting eq 
conditional prior reaches limits components jjc components combined shows conditional class prior components associated observations proportional number observations combined prior classes depends notice analytical tractability integral eq 
essential allows directly finite number indicator variables infinite number mixing proportions 
may combine likelihood eq 
conditioned indicators prior eq 
obtain conditional posteriors indicators components jjc jjc exp components combined jc jc ds likelihood components observations currently associated gaussian component parameters likelihood pertaining currently unrepresented classes parameters associated obtained integration prior distribution 
note need differentiate infinitely unrepresented classes parameter distributions identical 
unfortunately integral analytically tractable follow neal suggests sample priors gaussian gamma shaped order generate monte carlo estimate probability generating new class 
notice approach effectively generates parameters sampling prior classes unrepresented 
monte carlo estimate unbiased resulting chain sample exactly desired distribution matter samples approximate integral single sample works fairly applications 
detail possibilities computing conditional posterior class probabilities depending number observations associated class observations associated class posterior class probability top line eq 

observation currently observation associated class odd situation observations associated class class parameters 
turns situation handled unrepresented class sampling parameters simply uses class parameters consult neal detailed derivation 
unrepresented classes values mixture parameters picked random prior parameters gaussian gamma shaped classes parameters associated easily evaluate likelihoods gaussian priors take form components observations associated remaining class 
hitherto unrepresented classes chosen new class introduced model classes removed empty 
inference spirals example illustrate model dimensional spirals dataset ueda containing data point plotted 
data points generated isotropic gaussians means follow spiral pattern 
cases dimensional spirals data 
crosses represent single random sample posterior mixture model 
rep represented classes account mass lines indicate std 
dev 
gaussian mixture components thickness lines represent mass class 
multivariate generalisation generalisation multivariate observations straightforward 
means precisions vectors matrices respectively prior posterior distributions multivariate gaussian wishart 
similarly hyperparameter vector multivariate gaussian prior matrices wishart priors 
parameter stays scalar prior gamma mean dimension dataset 
specifications stay 
setting recovers scalar case discussed detail 
inference mixture model started single component large number gibbs sweeps performed updating parameters hyperparameters turn sampling conditional distributions derived previous sections 
auto covariance quantities plotted reveals maximum correlation length 
iterations performed modelling purposes minutes cpu time pentium pc steps initially burn followed generate approximately independent samples posterior spaced evenly apart 
represented components sample posterior visualised data 
see posterior number represented classes concentrated concentration parameter takes values corresponding mass predictive distribution belonging unrepresented classes 
predictive distribution particular state markov chain predictive distribution parts represented classes gaussian unrepresented classes 
updating indicators may chose approximate unrepresented classes finite mixture gaussians parameters drawn prior 
final predictive distribution average 
samples posterior 
spirals data density roughly components represented classes plus represent remaining mass attempted show distribution 
imagine smoothed version single sample shown averaging models slightly varying numbers classes parameters 
small mass unrepresented classes spreads entire observation range 
lag time iterations markov chain auto covariance coefficient log rep log log log det log det log rep log log log det log det concentration frequency represented classes frequency left plot shows auto covariance length various parameters markov chain iterations 
number represented classes rep significant correlation effective correlation length approximately computed sum covariance coefficients lag 
histograms shows distribution concentration parameter number represented classes samples posterior 
infinite hierarchical bayesian mixture model introduced shown performance overfitting achieved multidimensional data 
efficient practical mcmc algorithm free parameters derived demonstrated example 
model fully automatic needing specification parameters vague prior 
corroborates falsity common misconception difference bayesian non bayesian methods prior arbitrary anyway 
tests variety problems reveals infinite mixture model produces densities generalisation highly competitive commonly methods 
current undertaken explore performance high dimensional problems terms computational efficiency generalisation 
infinite mixture model advantages finite counterpart applications may appropriate limit number classes number represented classes automatically determined mcmc effectively avoids local minima plague mixtures trained optimisation methods 
em ueda simpler handle infinite limit finite models unknown sizes richardson green traditional approaches extensive crossvalidation 
bayesian infinite mixture model solves simultaneously long standing problems mixture models density estimation 
acknowledgments radford neal helpful comments ueda making spirals data available 
funded danish research councils computational neural network center connect thor center 
antoniak 

mixtures dirichlet processes applications bayesian nonparametric problems 
annals statistics 
ferguson 

bayesian analysis nonparametric problems 
annals statistics 
gilks wild 
adaptive rejection sampling gibbs sampling 
applied statistics 
neal 
bayesian learning neural networks lecture notes statistics new york springer verlag 
neal 

markov chain sampling methods dirichlet process mixture models 
technical report department statistics university toronto 
www cs toronto edu radford html 
richardson green 
bayesian analysis mixtures unknown number components 
journal royal statistical society 
ueda nakano ghahramani hinton 
algorithm mixture models nips mit press 
west muller escobar 
hierarchical priors mixture models applications regression density estimation 
freeman smith editors aspects uncertainty pp 

john wiley 
williams rasmussen 
gaussian processes regression touretzky mozer hasselmo editors nips mit press 
