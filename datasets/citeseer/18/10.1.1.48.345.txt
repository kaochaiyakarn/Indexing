policy invariance reward transformations theory application reward shaping andrew ng harada stuart russell computer science division university california berkeley berkeley ca fang cs berkeley edu investigates conditions modifications reward function markov decision process preserve optimal policy 
shown positive linear transformation familiar utility theory add reward transitions states expressible difference value arbitrary potential function applied states 
furthermore shown necessary condition invariance sense transformation may yield suboptimal policies assumptions underlying mdp 
results shed light practice reward shaping method reinforcement learning additional training rewards guide learning agent 
particular known bugs reward shaping procedures shown arise non potential rewards methods constructing shaping potentials corresponding distance heuristics 
show potentials lead substantial reductions learning time 
sequential decision problems studied dynamic programming reinforcement learning literatures task represented reward function 
reward function model domain optimal policy determined 
elementary theoretical question arises freedom specifying reward function optimal policy remains unchanged 
field utility theory studies primarily single step decisions corresponding question utility function answered simply 
single step decisions uncertainty monotonic transformation utilities leaves optimal decision unchanged uncertainty positive linear transformations allowed von neumann morgenstern 
results important implications designing evaluation functions games eliciting utility functions humans areas 
knowledge question policy invariance reward function transformations fully explored sequential decision problems 
transformations important areas ffl task structural estimation mdps rust involves recovering model reward function observed optimal behavior 
see discussion inverse reinforcement learning russell 
transformations determine extent reward function recovered 
ffl practice reward shaping reinforcement learning consists supplying additional rewards learning agent guide learning process supplied underlying mdp 
important understand impact shaping learned policy 
results known approximate invariance rewards perturbed new policy value gamma fl original optimal policy singh yee williams baird 
focuses primarily reward shaping potential powerful technique scaling reinforcement learning methods handle complex problems dorigo colombetti mataric 
similar ideas arisen animal training literature see discussion 
simple pattern extra rewards suffices render straightforward completely intractable problem 
see policy invariance important shaping consider examples bugs arise describes system learns ride simulated bicycle particular location 
speed learning provided positive rewards agent progress goal 
agent learned ride tiny circles near start state penalty incurred riding away goal 
similar problem occurred soccer playing robot trained david andre astro teller personal communication 
possession soccer important provided reward touching ball 
agent learned policy remained ball touching ball frequently possible 
policies clearly optimal original mdp 
examples suggest shaping rewards obey certain conditions mislead agent learning suboptimal policies 
difficulty positive reward cycles leads consider rewards derived conservative potential reward executing transition states essentially difference value potential function applied state 
turns sufficient condition guaranteeing policy invariance reward transformations assuming prior knowledge mdp necessary condition able guarantee 
section gives definitions needed state claim precisely section states proves claim 
section shows construct shaping potentials various kinds demonstrates efficacy speeding learning simple domains 
section connects results existing algorithms advantage learning baird policy iteration bertsekas tsitsiklis closes discussion 
preliminaries definitions section provide definitions focusing case finitestate markov decision processes mdps 
shaping interest finite state infinite state problems underlying mdp theory infinite state case significantly difficult absence shaping 
analysis methodology easily generalized finite infinite state space case underlying mdp theory laid mention start definitions explicitly considering finite state domains 
finite state markov decision process mdp tuple fl finite set states fa set actions fp sa delta js ag state transition probabilities sa giving probability transitioning state action state fl discount factor specifies reward distributions 
simplicity assume rewards deterministic case bounded real function called reward function 
literature reward functions typically written theta 
reward received action state write reward functions form allow general form theta theta 
reward received action state transitioning state fixed set actions policy set states function 
note policies defined states mdps policy may applied different mdps long mdps states actions 
policy states mdp fl states actions may define value function evaluated state gives flr fl reward received ith step executing policy state expectation state transitions taken executing 
define optimal value function sup function evaluated psa delta notation sa delta means drawn distribution sa delta optimal function sup 
define optimal policy mdp arg max 
optimal policy may unique generally say policy optimal arg max lastly context mdp clear may drop subscript write need largely standard regularity conditions sure definitions sense 
undiscounted fl mdps assume contains distinguished state called absorbing state mdp stops transition rewards 
undiscounted mdps assume policies proper meaning executing policy starting state probability eventually transition really condition say transition probabilities proper condition holds 
discounted mdps corresponding absorbing state infinite horizon note write gamma fs standard regularity conditions needed mdps finite state spaces see sutton barto case explicitly said focus 
mdps infinite state spaces needed example undiscounted case expectation definition flr fl may exist 
issues need properly addressed define things optimal policies excellent sources material include bertsekas hern andez bertsekas shreve 
unfortunately explaining infinite jsj case full generality require measure theory wish delve comment appropriate generalizations required regularity conditions mdp results easily generalized infinite jsj case 
continually draw links results may proved infinite jsj defer general proofs infinite jsj full 
note infinite state case important useful condition reinforcements bounded absolute value mentioned 
similar sense mean cauchy distribution existing 
shaping rewards section introduce formal framework shaping rewards 
intuitively trying learn policy mdp fl wish help learning algorithm giving additional shaping rewards hopefully guide learning optimal policy faster 
formalize assume running reinforcement learning algorithm fl run transformed mdp fl reward function transformed mdp theta theta 
bounded real valued function called shaping reward function 
similar domain undiscounted case strictly gamma fs theta theta overly point 
original mdp received reward transitioning action new mdp receive reward event 
fixed mdp assuming additive memoryless shaping reward functions general possible form shaping rewards 
cover fairly large range possible shaping rewards come 
example encourage moving goal shaping reward function choose closer appropriate sense goal positive reward 
encourage action set states set 
elementary important property form reward transformation generally implemented reinforcement learning applications explicitly tuple fl allowed learn actions mdp observing resulting state transitions rewards 
access simulate having type access simply actions full consider general necessarily additive form arbitrary reward received original mdp appropriate conditions turns give optimality guarantees similar give additional freedom gives choosing shaping rewards allows rescale rewards fixed positive factor 
add interesting richness defer result full 
pretending observed reward observed reward naturally simple reason works actions states transition probabilities 
online offline model model free algorithms may applied may general readily applied way 
learning policy hope question hand forms shaping reward functions guarantee optimal policy optimal 
section answer fair degree generality 
main results practical applications exactly know priori may may know 
goal possibly come shaping reward function theta theta 
optimal section give form guarantee optimal provide weak converse showing knowledge type shaping function give guarantee 
focusing undiscounted case fl try gain intuition give rise shaping bug pointed 
bicycle task agent rewarded riding goal punished riding away learned ride tiny circle obtain positive reward happened moving goal 
generally sequence states agent travel cycle delta delta delta delta delta delta gain net positive shaping reward doing delta delta delta gamma gamma agent may distracted really trying ride goal try repeatedly go round cycle 
address difficulty cycles form immediately comes mind difference potentials phi gamma phi phi function states 
way delta delta delta gamma gamma eliminated problem cycles distract agent 
ways choose aside cycles problems shaping need address 
turns prior knowledge potential shaping functions guarantee consistency optimal policy turns essentially need order guarantee 
formal theorem theorem fl shaping reward function theta theta 

say potential shaping function exists real valued function phi 
gamma fs fl phi gamma phi gamma fs fl 
potential shaping function necessary sufficient condition guarantee consistency optimal policy learning fl fl sense ffl sufficiency potential shaping function optimal policy optimal policy vice versa 
ffl necessity potential shaping function phi exists satisfying equation exist proper transition functions reward function theta 
optimal policy optimal note infinite state case choose phi construct shaping function formal results go really demand phi bounded shaping rewards bounded similar condition bounded section issue discussed 
note finite state case vacuous condition phi having range finite cardinality automatically bounded 
necessity sufficiency conditions little complicated usual multiple optimal policies clear quantifications strongest possible theorem form 
sufficiency condition says long potential guaranteed trying learn optimal necessity condition says knowledge choose potential learning want guarantee consistency learning optimal policy 
intimate knowledge necessity condition say possible able shaping functions 
proof necessity appendix prove equation sufficient condition form may guarantee optimal policy optimal prove result fully rigorously case finite jsj proof infinite jsj nearly identical requires little care justifying bellman equations 
proof sufficiency form 
fl replacing phi phi phi gamma constant change shaping rewards difference potentials may replacing phi phi gamma phi necessary assume loss generality phi express satisfies phi 
original mdp know optimal function satisfies bellman equations see sutton barto psa delta fl max simple algebraic manipulation gives gamma phi fl phi gamma phi fl max gamma phi define qm gamma phi substitute fl phi gamma phi back previous equation get qm fl max qm fl max qm exactly bellman equation undiscounted case qm gamma phi gamma 
qm satisfies bellman equations fact unique optimal function 
qm gamma phi optimal policy satisfies arg max arg max gamma phi arg max optimal show optimal policy optimal simply apply proof roles interchanged shaping function gammaf 
completes proof 
corollary conditions theorem suppose take form fl phi gamma phi 
suppose phi fl 
gamma phi gamma phi proof proved sufficiency proof follows immediately identity max 
robustness learning proved identities corollary hold arbitrary policies just optimal policy gamma phi similarly functions 
consequence shaping robust sense near optimal policies preserved learn near optimal policy say jv gamma potential shaping nearoptimal jv gamma 
see apply identity just pointed policies subtract 
policies optimal phi better understand potential preserve optimal policies worth noting mdp potential reinforcement function fl phi gamma phi policy optimal potential shaping functions indifferent policies sense give reason prefer policy intuitive level accounts give reason prefer policy switch theorem suggests choose shaping rewards form fl phi gamma phi 
applications grid world trial number grid world trial number experiment grid world 
plot steps taken goal vs trial number 
dot shaping dot dash phi phi solid phi phi 
experiment grid world 
phi course chosen expert knowledge domain 
may corollary suggests particularly nice form phi know domain try choosing 
see phi phi undiscounted case equation tells value function particularly easy value function learn lacking model world remain done learn non zero values 
avoid misconception stress way choosing useful phi shaping rewards help significantly phi far say sup norm guiding exploration see examples section 
case long choose potential guarantee near optimal policy learn near optimal turn attention small experiments demonstrate potential shaping applied practice 
experiments empirical convincingly justified shaping mataric bother try justify 
goal show potential style shaping functions fit picture demonstrate shaping functions derived practice 
goals chose simplicity clarity simple grid world domains showcase interesting aspects potential shaping 
domain shortest path goal gridworld start goal states opposite corners discounting step reinforcement 
actions compass directions move step intended direction time random direction time agent stays place tries walk grid 
shaping potential phi 
pointed earlier equation suggests phi shaping potential 
go type reasoning suggest crude estimate doing hope demonstrate little expert knowledge distances location goal similar reasoning may similarly derive phi minimum cost problems 
trying take step goal chance desired step goal chance random action 
take random action border gridworld move away goal 
states expect optimal policy steps manhattan distance progress goal timestep crude estimate expected number steps needed get goal manhattan goal 
set grid world flags subgoals trial number grid world subgoals including goal state visited order 
experiment grid world subgoals 
plot steps taken goal vs trial number 
dot shaping dot dash phi phi solid phi phi estimate value function phi phi vm goal 
guess shaping function 
shaping reward quite far sup norm tried phi phi 
results experiment shown 
experiments reported section averages independent runs 
readily seen shaping functions significantly helped speed learning 
worth re stressing phi quite far significantly helped initial stages learning 
larger grid world results dramatic shows result experiment repeated larger grid 
plots phi phi low graph barely seen learning shaping clearly losing hopelessly potential shaping algorithm 
goal experiments try justify shaping done far convincingly 
demonstrated style simple reasoning putting distance goal heuristic enabled pick sensible phi dramatically sarsa sutton barto greedy exploration learning rate 
experiments sarsa gave analogous results showing shaping significantly speeding learning 
sped learning 
class problems similar style reasoning domains assign subgoals 
consider grid world start lower left hand corner pick set flags sequence going final goal state 
actions rewards previous grid world state space expanded keep track collected flags 
flag subgoal tempting choose rewarded visiting subgoals 
see potential function style reasoning lead choose equation suggests magnitudes subgoal rewards 
knowledge subgoal locations reasoning analogous suggested earlier steps progress timestep may estimate expected number timesteps say needed reach goal 
imagine subgoal equally hard reach previous having reached th subgoal gamma steps go 
slightly refined argument changes gamma gamma steps comes typical case halfway th st subgoals choice phi phi gamma gamma gamma denotes number subgoals achieved form function see phi phi jumps reach subgoal final goal state shaping reward function phi gamma phi giving reward reaching subgoals 
exactly intuition suggested shaping reward 
comparison carried experiment fine tuned shaping reward similar previous grid world experiments explicitly estimated remaining time goal state constructed corresponding phi vm potential function 
result experiments shown see crude shaping function phi allowed significantly speed learning shaping fine tuned phi unsurprisingly gave better performance repeating experiment larger domains subgoals results reported dramatic 
discussion shown necessary sufficient conditions shaping function leave optimal policies invariant 
easy generalizations worth mentioning aside guaranteeing consistency trying learn optimal policy easy show argument similar section potential trying learn policy restricted class policies framework studied kearns example includes task finding best weights neural network mapping states actions 
semi markov decision processes smdps actions take varying amounts time complete equation unsurprisingly generalizes gammafi phi gamma phi time action took complete fi discount rate 
fl phi gamma phi form surface reminiscent terms equations advantage learning baird policy iteration bertsekas tsitsiklis 
crude level turns may thought trying modify phi gain computational representational advantage 
consider problem modifying phi trying learn rough shaping function lead quite naturally algorithm multi scale value function approximation may initially unusual try learn shaping function multiscale rough vs fine approximation aspect leads possibly powerful subject 
shown potential shaping rewards fl phi gamma phi leave near optimal policies unchanged 
proved type shaping guarantee invariance assumptions mdp 
just practitioners discounting undiscounted problems improve convergence algorithms believe experience potential style shaping rewards may lead occasionally try shaping rewards inspired potentials strictly form 
example analogy discounting undiscounted problems conceivable certain problems may easier expert propose potential phi undiscounted shaping function phi gamma phi fl 
theorem may longer guarantee optimality case shaping function may purely engineering point view worth trying judiciously care 
spirit regularity conditions demanded bounded phi plausible practitioners want try certain unbounded phi 
naturally expert knowledge domain available non potential shaping functions fully appropriate 
guidelines choosing shaping functions suggested distance heuristic heuristic choosing potentials shaping crucial making learning tractable believe task finding shaping functions problem increasing importance 
acknowledgments ng supported berkeley fellowship 
supported part aro muri daah onr nsf ecs 
baird baird 

reinforcement learning continuous time advantage updating 
relates observation learned shaping reward operating psychologically capture piece chess operates reward underlying mdp rewards 
proceedings international conference neural networks 
bertsekas bertsekas 

dynamic programming optimal control volume ii 
athena scientific 
bertsekas shreve bertsekas shreve 

stochastic optimal control discrete time case 
academic press 
bertsekas tsitsiklis bertsekas tsitsiklis 

neuro dynamic programming 
athena scientific 
dorigo colombetti dorigo colombetti 

robot shaping developing autonomous agents learning 
artificial intelligence 
hern andez hern 

adaptive markov control processes 
springer verlag 
kearns kearns mansour ng 

approximate planning large pomdps reusable trajectories 
preprint 
mataric mataric 

reward functions accelerated learning 
proceedings eleventh international conference machine learning 
morgan kaufmann 


learning drive bicycle reinforcement learning shaping 
proceedings fifteenth international conference machine learning 
morgan kaufmann 
russell russell 

learning agents uncertain environments extended 
proceedings eleventh annual acm workshop computational learning theory colt madison wisconsin 
acm press 
rust rust 

people behave bellman principal optimality 
submitted journal economic perspectives 
raymond touretzky 

shaping robot behaviour principles instrumental conditioning 
robotics autonomous systems 
singh yee singh yee 

upper bound loss approximate optimal value functions 
machine learning 
sutton barto sutton barto 

reinforcement learning 
mit press 
von neumann morgenstern von neumann morgenstern 

theory games economic behavior 
princeton university press princeton new jersey edition 
williams baird williams baird 

tight performance bounds greedy policies imperfect value functions 
proceedings tenth yale workshop adaptive learning systems 
appendix proof necessity appendix sketch proof necessity part theorem 
brevity give proof case jaj generalization obvious tedious 
lemma 
lemma exists gamma fs exists proper transition functions reward function optimal policy optimal proof sketch lemma assume loss generality delta gamma 
undiscounted case assume simplicity 
fl proof nearly having ensure just tedious 
construct follows sa sa delta 
clearly hand delta gamma delta ready show main necessity result 
proof necessity 
assume 
need show construct optimal policy optimal lemma depends done need consider shaping functions unlabeled thick edges correspond actions 
edges probability 
edge carries reward delta edges zero reward 
form depend 
fl distinguished absorbing state fixed state 
noting constant offsets reward affect optimal policy fl may replacing gamma necessary assume loss generality 
define phi gammaf assumption potential exists fl phi gamma phi assume distinct cases impossible handled similarly 
construct way assuming jaj 
state states actions lead probability 
define delta flf gamma delta delta delta delta 
model illustrated 
delta delta flf gamma delta flf relied fact construction 
ae delta ae delta 
