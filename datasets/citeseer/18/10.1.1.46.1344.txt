technical report department statistics university toronto annealed importance sampling radford neal department statistics department computer science university toronto toronto ontario canada www cs utoronto ca radford radford stat utoronto ca february 
simulated annealing moving tractable distribution distribution interest sequence intermediate distributions traditionally inexact method handling isolated modes markov chain samplers 
shown markov chain transitions annealing sequence define importance sampler 
markov chain aspect allows method perform acceptably high dimensional problems finding importance sampling distributions difficult importance weights ensures estimates converge correct values number annealing runs increases 
annealed importance sampling procedure resembles second half previously studied tempered transitions seen generalization proposed variant sequential importance sampling 
related thermodynamic integration methods estimating ratios normalizing constants 
annealed importance sampling attractive isolated modes estimates normalizing constants required may generally useful independent sampling allows bypass problems assessing convergence autocorrelation markov chain samplers 
bayesian statistics statistical physics expectations various quantities respect complex distributions computed 
simple distributions estimate expectations sample averages points drawn independently distribution interest 
simple monte carlo approach distribution complex allow easy generation independent points 
generate independent points simpler approximating distribution importance sampling estimate points weighted compensate wrong distribution 
alternatively sample dependent points obtained simulating markov chain converges correct distribution 
show approaches combined importance sampling distribution defined series markov chains 
method inspired idea annealing way coping isolated modes may attractive multimodality problem 
importance sampling works follows see example geweke 
suppose interested distribution quantity probabilities probability densities proportional function 
suppose computing feasible able directly sample distribution defines 
able sample distribution approximates defined probabilities probability densities proportional function able evaluate 
base estimates sample independent points generated distribution defined 
compute importance weight follows estimate expectation respect distribution defined provided easy see gamma converge dx dx normalizing constants 
see converge expectation respect distribution defined 
accuracy depends variability importance weights 
weights vary widely estimate effectively points largest weights 
importance sampling distribution defined fairly approximation defined ratio vary wildly 
high dimensional complex multimodal finding importance sampling distribution difficult limiting applicability method 
alternative obtain sample dependent points simulating markov chain converges distribution interest metropolis hastings algorithm metropolis hastings 
markov chain methods long statistical physics widely applied statistical problems illustrated papers book edited gilks richardson spiegelhalter 
markov chains sample complex distributions usually proceed making small changes state variables 
causes problems distribution contains widely separated modes nearly isolated respect transitions 
chain move modes rarely take long time reach equilibrium exhibit high autocorrelations functions state variables long time lags 
method simulated annealing introduced kirkpatrick gelatt vecchi way handling multiple modes optimization context 
employs sequence distributions probabilities probability densities differs slightly distribution interest 
distribution designed markov chain sample allows movement regions state space 
traditional scheme set fi fi fi delta delta delta fi annealing run started initial state simulate markov chain designed converge number iterations necessarily approach equilibrium 
simulate number iterations markov chain designed converge gamma starting final state previous simulation 
continue fashion final state simulation initial state simulation gamma simulate chain designed converge hope distribution final state produced process close note contains isolated modes simply simulating markov chain designed converge starting arbitrary point give poor results stuck mode closest starting point mode little total probability mass annealing process heuristic avoiding advantage movement possible distributions gradually approaching desired unfortunately reason think annealing give precisely correct result mode exactly right probability 
little consequence optimization context final distribution degenerate maximum serious flaw applications statistics statistical physics require sample non degenerate distribution 
annealed importance sampling method essentially way assigning weights states multiple simulated annealing runs produce estimates converge correct value number runs increases 
done viewing annealing process defining importance sampling distribution explained section 
discussing accuracy importance sampling general section analyse efficiency annealed importance sampling section find results obtained sufficient number interpolating distributions provided vary smoothly 
demonstrations simple distributions section confirm 
annealed importance sampling related tempered transitions neal way modifying annealing procedure produce correct results 
discussed section annealed importance sampling preferable tempered transitions 
tempered transitions relationship annealed importance sampling allows find estimates ratios normalizing constants previously unavailable 
section shows view form sequential importance sampling due maceachern clyde liu instance annealed importance sampling 
section discuss general utility annealed importance sampling way handling multimodal distributions way calculating normalizing constants way combining adaptivity markov chains advantages independent sampling 
annealed importance sampling procedure suppose wish find expectation function respect distribution probabilities probability densities 
available sequence distributions hope assist sampling distribution able compute function proportional 
method sampling preferably produces independent points 
gamma able simulate markov chain transition leaves invariant 
sequence distributions specially constructed suit problem scheme may generally useful 
fix give distribution interest fix give simple distribution sample fi gammafi fi fi fi 
note traditional simulated annealing scheme fi usually suitable usually leads independent sampling easy 
applications bayesian statistics unnormalized posterior distribution product prior density likelihood prior density computed normalized form 
markov chain transitions represented functions giving probability probability density moving current state necessary compute generate transitions may constructed usual ways metropolis gibbs sampling updates may involve scans iterations 
annealed importance sampling scheme valid leave corresponding invariant necessarily essential produce ergodic markov chain usually desirable 
annealed importance sampling produces sample points corresponding weights estimate expectation function equation 
generate point associated weight generate sequence points gamma follows generate gamma generate gamma gamma gamma 
generate generate 
set gamma gamma gamma gamma gamma gamma gamma delta delta delta avoid overflow problems may best computations terms log 
see annealed importance sampling valid consider extended state space points gamma 
identify original state function original state considered function extended state just looking component 
define distribution gamma function proportional joint probability probability density gamma delta delta delta gamma gamma gamma reversal transition defined invariance respect ensures valid transition probabilities dx 
turn guarantees marginal distribution original distribution interest joint probability product marginal probability conditional probabilities components earlier components 
apply equation rewrite function follows gamma delta delta delta gamma gamma gamma gamma gamma gamma gamma delta delta delta gamma gamma gamma gamma gamma gamma gamma gamma gamma look joint distribution gamma defined annealed importance sampling procedure 
proportional function gamma gamma gamma gamma gamma delta delta delta regard importance sampler distribution extended state space 
appropriate importance weights equations 
dropping superscript right side simplify notation gamma gamma delta delta delta gamma gamma gamma gamma gamma gamma gamma weights equation showing annealed importance sampling procedure valid 
procedure produces sample single independent points estimating expectations equation 
practice better estimates obtained point initial state markov chain leaves invariant simulate pre determined number iterations 
states simulation estimating expectation importance weight validity procedure seen including extra states extended state space conditional distributions reversed markov chain transitions noting expectation average states expectation respect annealed importance sampling provides estimate ratio normalizing constants normalizing constants important statistical physics statistical problems bayesian model comparison 
normalizing constant defined equation normalizing constant equation average importance weights converges ratio normalizing constants dx dx 
bayesian application normalized prior unnormalized posterior ratio marginal likelihood model 
data collected annealed importance sampling runs estimate expectations respect intermediate distributions simply uses states application gamma weights omitting factors equation pertain states 
similarly estimate ratio normalizing constants averaging weights 
usually prefer start annealing runs distribution generate independent points annealed importance sampling valid points gamma generated start run independent 
particular points generated markov chain samples annealed importance sampling estimates converge correct values provided markov chain sample ergodic 
accuracy importance sampling estimates discussing annealed importance sampling necessary consider accuracy importance sampling estimates general 
results needed demonstration section 
importance sampling estimate points drawn independently density proportional gamma gamma importance weights 
accuracy importance sampling estimator discussed geweke 
estimator form regenerative markov chain methods mykland tierney yu ripley weights lengths tours regeneration points 
determining accuracy estimator assume loss generality normalizing constant multiplying constant effect assume adding constant simply shifts amount changing variance 
large numerator denominator right side equation converge expectations assumptions gives gamma delta delta delta differences averages expectations 
large discard term judge accuracy variance assuming finite approximate var var gamma hi return actual situation may may zero modifying equation suitably var gamma hi gamma geweke estimates data compute follows var gamma equivalent estimate discussed ripley section context regenerative simulation 
small ripley recommends estimate 
independent equation simplifies var gamma gamma gamma var var step uses var gamma gamma gamma equation shows independent cost points drawn plus variance normalized importance weights 
estimate sample variance gamma gives rough indication factor sample size effectively reduced particular function expectation estimated 
note applications expectations functions estimated sample variance intuitively attractive indicator accurate estimates large points largest importance weights dominate estimates 
trust estimate adjusted sample size var small equation gives small estimate variance estimator 
note possible sample variance small estimates wildly inaccurate sample variance bad estimate true variance normalized importance weights 
happen example important mode seen sampling independently drawn generated markov chain sampler assessing accuracy estimates difficult depend variance normalized importance weights autocorrelations produced markov chain 
reason preferring generate points independently start annealed importance sampling run 
efficiency annealed importance sampling efficiency annealed importance sampling depends normalized importance weights having large variance 
sources variability importance weights 
different annealing runs may different modes assigned different weights 
variation weights due large important modes rarely 
general guarantee happen hope find effective scheme defining annealing distributions radically different markov chain eliminates isolated modes altogether 
high variability importance weights result transitions distributions bring distribution close equilibrium 
extreme case case annealed importance sampling reduces simple importance sampling inefficient close variability source reduced increasing number iterations basic markov chain update 
example consists metropolis updates variance importance weights reduced increasing brings state closer equilibrium distribution local mode 
variability importance weights come finite number distributions interpolate analyse affects variance sequence distributions comes parameter family equation 
analysis assume produces state drawn independent previous state 
assumption course unrealistic especially isolated modes purpose understand effects unrelated markov chain convergence 
convenient look log 
discussed section measure inefficiency estimation plus variance normalized importance weights 
fact exp oe exp gaussian mean variance oe see log gaussian mean variance oe sample size effectively reduced factor var exp oe exp oe exp oe equation log log gamma gamma gamma log gamma distributions defined equation log fi gamma gamma fi log gamma gamma log gamma assume fi equally spaced log log gamma gamma log gamma assumption produces state drawn independently provided log gamma gamma log gamma finite variance gamma drawn central limit theorem applied conclude log approximately gaussian distribution large keeping fixed increases 
variance log asymptotically form oe constant oe plus variance normalized weights form exp oe 
assume transition takes fixed amount time regardless time required produce estimate degree accuracy proportional exp oe minimized oe point variance normalized importance weights gamma 
behaviour occur fi equally spaced long chosen scheme leads fi gamma gamma fi going approximately inverse proportion range fi values close gaussian approximately constant regions high density argument similar tempered transitions neal section shows best scheme uses uniform spacing log fi geometric spacing fi 
results hold generally annealing schemes families distributions density varies smoothly parameter analogous fi 
get idea efficiency annealed importance sampling affected dimensionality problem supposing components independent identically distributed 
assuming produces independent state drawn quantities log gamma gamma log gamma composed identically distributed independent terms 
variance quantity increase proportion variance log asymptotically form koe optimal choice koe variance normalized importance weights gamma 
assuming behaviour similar interesting distributions components independent analysis shows increasing dimensionality problem slow annealed importance sampling 
linear slowdown severe simple importance sampling efficiency goes exponentially analysis assumes generates state nearly independent previous state presumably require metropolis gibbs sampling iterations 
probably better practice transitions come close producing independent state take time increasing number interpolating distributions produce total computation time 
states generated come close equilibrium distributions distributions change annealing step increased number distributions may help reduce variance importance weights analysis terms equation longer independent 
see variance importance weights reduced needed increasing number distributions annealing scheme provided transitions distribution establishing equilibrium 
isolated modes provision true global sense transitions sample local mode 
performance annealed importance sampling adequate depend annealing heuristic fact capable finding modes distribution 
absence theoretical information pointing modes located reliance heuristic inevitable 
demonstrations simple distributions illustrate behaviour annealed importance sampling show works simple distribution single mode markov chain transitions sample intermediate distributions distribution modes isolated respect markov chain transitions distribution interest 
distributions unimodal distribution components state independent distribution gaussian mean standard deviation 
distribution defined gamma normalizing constant 
sequence annealing distributions defined scheme equation 
distribution chosen components independent gaussian mean zero standard deviation 
function define distribution chosen corresponding gaussian probability density normalized 
estimate normalizing constant average importance weights 
annealed importance sampling choose sequence fi define intermediate distributions 
number spacing fi appropriate problem 
mentioned previous section gaussian diffuse expect geometric spacing appropriate fi far 
spaced fi near zero 
detail test fi spaced uniformly followed fi spaced geometrically total distributions 
tests annealing sequences twice half distributions spaced scheme 
define markov chain transitions distributions 
general different schemes different distributions tests metropolis updates proposal distributions transition probabilities course different metropolis acceptance criterion changes 
detail sequences metropolis updates gaussian proposal distributions centred current state having covariances proposal distributions lead adequate mixing intermediate distributions 
test sequence updates repeated times give test repeated times 
test annealing runs done 
test states produced run result applying succession starting point generated independently saved twentieth state applying note applied run tests required occurs naturally program 
state applying estimates valid state 
shows results test 
upper graphs show variance log importance weights increases course run 
importance weights run defined equation factors distributions omitted 
transitions distributions expected mix best strategy minimizing variance final weights space fi variance log weights increases equal amount annealing step 
plot upper right shows spacing chosen test close optimal 
variance final normalized importance weights 
analysis section number intermediate distributions slightly greater optimal number variance normalized weights gamma 
lower graphs show distribution value component state test 
seen lower left distribution narrows distribution fi approaches 
plot lower right shows values component importance weights states ends runs 
case values weights appear independent 
estimate expectation component state test standard error estimated equation 
compatible 
beta log weight index distribution variance log weights 
beta component state component state importance weight 
results test unimodal distribution 
upper left logs importance weights values fi runs 
upper right variance log weights function index fi 
lower left distribution component state fi values 
lower right joint distribution component importance weight ends runs 
random jitter added fi values plots left improve presentation 
beta log weight 
index distribution variance log weights beta component state 
component state importance weight 
results test distribution modes 
plots correspond 
true value 
case error estimate equation close arrive estimated standard deviation adjusted sample size var expected values weights independent 
average importance weights test standard error estimated simply sample variance weights divided compatible true normalizing constant 
tests done run half computer time test 
annealing sequence identical test number repetitions metropolis updates reduced 
increased variance normalized importance weights corresponding increase standard errors estimates 
test number distributions annealing sequence cut half spaced scheme number metropolis repetitions kept 
increased variance normalized importance weights 
expected spreading number updates intermediate distributions appears better updates try produce nearly independent points fewer stages 
final test unimodal distribution twice intermediate distributions spaced scheme 
reduced variance normalized importance weights corresponding reduction standard errors benefit case worth factor increase computer time 
test confirm mixes variance importance weights reduced desired spacing fi closely 
tests done distribution modes mixture gaussians components independent means standard deviations 
gaussians mixing proportion means standard deviations distribution unimodal tests 
gaussian mixing proportion means gamma standard deviations 
mixture distribution defined exp gamma gamma exp gamma normalizing constant 
means components respect gamma 
tests independent standard gaussian distributions component normalized 
transitions metropolis updates scheme spacing fi test number distributions test unimodal distribution 
results shown 
seen lower left distributions fi near zero cover modes fi increased modes separated 
metropolis updates able move modes fi near larger proposals standard deviation probability proposing movement mode simultaneously components small 
modes seen annealing mode gamma seen rarely times runs despite fact twice probability mode final distribution fi 
unweighted average final states annealing runs give inaccurate results 
plot lower right shows importance weights compensate unrepresentative sampling 
runs ended rarely sampled mode received higher weights sampled mode 
estimate expectation component runs gamma estimated standard error equation compatible true value gamma 
standard error estimate expect estimated standard deviation adjusted sample size var 
difference arises values importance weights independent case 
average importance weights runs estimated standard error compatible true value normalizing constant see annealed importance sampling produces valid estimates example 
procedure efficient hope runs mode gamma 
symptom problem variance normalized importance weights test quite high compared variance seen similar test unimodal distribution 
see comes upper plots 
small values fi plots quite similar presumably mode gamma influence distributions 
mode important fi approaches producing high variance weights 
hope reduce variance importance weights increasing number intermediate distributions spacing fi closely 
ran tests twice distributions times distributions cases number metropolis updates distribution 
results differed little test 
variance importance weights runs mode reduced difference importance weights modes reduced number runs mode gamma increase 
little difference standard errors estimates 
example annealing heuristic marginally adequate 
expect obtain better results finding better initial distribution better scheme interpolating equation 
example illustrates dangers reliance empirical estimates accuracy 
runs done probability runs mode gamma 
result simulated runs ended mode runs actual test 
runs estimate expectation component estimated standard error estimate normalizing constant estimated standard error 
estimates differ true values times estimated standard error 
unrecognized inaccuracies course possible importance sampling markov chain method theoretically derived guarantees accuracy available 
relationship tempered transitions ways modifying simulated annealing procedure order produce asymptotically correct estimates developed past including simulated tempering parisi geyer thompson metropolis coupled markov chains geyer 
method tempered transitions neal closely related annealed importance method 
tempered transition method samples distribution interest markov chain transitions defined terms elaborate proposal procedure involving sequence distributions proposed state simulating sequence base transitions leave invariant distributions followed second sequence base transitions leave invariant reversals corresponding respect decision accept reject final state product ratios probabilities various distributions proposed state rejected new state old state 
detail tempered transition operates follows starting state generate generate 
generate gamma generate gamma 
generate generate 
state accepted state markov chain probability min delta delta delta gamma gamma gamma delta gamma gamma gamma delta delta delta second half tempered transition procedure identical annealed importance sampling procedure provided fact generates point independent recognize annealed importance sampling weight equation essentially second half product defining tempered transition acceptance probability 
due similarities characteristics annealed importance sampling quite similar corresponding tempered transitions 
particular comparison neal tempered transitions simulated tempering relevant annealed importance sampling 
major difference annealed importance sampling tempered transitions tempered transition requires twice computation corresponding annealing run tempered transition involves upward sequence transitions downward sequence methods 
reason prefer annealed importance sampling easy generate independent points distribution easy tempered transitions preferred annealed importance sampling conjunction markov chain sampler produces dependent points tempered transitions possibility sequence annealing distributions sequence chosen randomly tempered transition fixed order 
potentially lead sampling annealing sequence adequate 
appears way employing multiple annealing sequences annealed importance sampling adding equivalent upward sequence tempered transitions 
tempered transitions idea annealed importance sampling applied order estimate ratios normalizing constants previously unavailable tempered transitions 
see note half tempered transition generation gamma gamma gamma annealed importance sampling run sequence distributions reversed exchange roles state run current state comes general corresponds gamma gammaj 
importance weights backwards annealed importance sampling delta delta delta gamma gamma gamma gamma gamma gamma gamma average weights tempered transitions accepted rejected converge dx dx ratio normalizing constants similar estimate imagining reversal markov chain defined tempered transitions 
chain states visited reverse order accepted transitions original chain accepted transitions reversed chain reversed sequence states rejected transitions original chain remain unchanged 
importance sampling estimate ratio normalizing constants obtained reversed chain manner 
importance weights accepted transitions follows terms original chain delta delta delta gamma gamma gamma gamma gamma gamma gamma importance weights rejected transitions equation 
estimates averaged producing estimate uses states accepted transitions plus states rejected transitions double weight 
estimate ratio normalizing constant similar fashion intermediate distributions simply averaging weights obtained truncating products equations appropriate point 
weights estimate expectations functions respect intermediate distributions 
note error assessment importance sampling estimates take account variance importance weights autocorrelations produced markov chain tempered transitions 
relationship sequential importance sampling variant sequential importance sampling developed maceachern clyde liu viewed instance annealed importance sampling sequence distributions obtained looking successively data points 
method maceachern call sequential importance sampler applies model joint distribution observable variables associated latent variables finite range 
able compute joint probabilities marginal probabilities subset indexes 
wish estimate expectations respect conditional distribution known values apply gibbs sampling problem possible slow converge due isolated modes 
method maceachern viewed annealed importance sampling sequence distributions related distribution conditional gamma observed variables distribution interest conditional detail distributions probabilities proportional gammaj gammaj gammaj gamma apply annealed importance sampling sequence distributions transitions defined follows 
begins number gibbs sampling updates gammaj gammaj gammaj 
ignore gammaj generate values afterward conditional distribution gammaj independently previous values 
done forward simulation conditional probabilities 
need generate values gamma values effect subsequent computations anyway 
easily seen equivalent sampling done procedure maceachern importance weights equation products factors form gamma gammaj gammaj gammaj gammaj gammaj gammaj gammaj gammaj gammaj gammaj gammaj gammaj gammaj gammaj gammaj gammaj gammaj product factors produces weights maceachern sequential importance sampler maceachern equivalent annealed importance sampling annealing distributions defined equation 
family distributions equation distributions form fixed discrete family 
consequently variance importance weights decreased increasing number distributions 
method inefficient practical 
possible sequence distributions defined equation extended continuous family partially conditioning way adjusting variance gaussian likelihood 
forms annealed importance sampling family equation applied problem 
discussion annealed importance sampling potentially useful way dealing isolated modes means calculating ratios normalizing constants general monte carlo method combines independent sampling adaptivity markov chain methods 
idea annealed importance sampling applied conjunction tempered transitions method may best viewed variation annealed importance sampling useful hard generate independent points handling isolated modes original motivation annealing primary motivation developing methods related annealing produce asymptotically correct results 
annealed importance sampling method characteristics similar tempered transitions 
discussed neal methods best may depend sequence annealing distributions deceptive certain ways 
possible say annealed importance sampling better methods simulated tempering probably easily implemented methods 
annealing methods closely related methods estimating ratios normalizing constants simulations distributions discussed gelman meng 
surprising methods simulated tempering parisi geyer thompson metropolis coupled markov chains geyer easily yield estimates ratios normalizing constants byproduct 
tempered transitions previously seen deficient respect neal see estimates fact obtained annealed importance sampling estimators conjunction tempered transitions 
estimate expectations respect intermediate distributions way possible simulated tempering metropolis coupled markov chains 
ratios normalizing constants obtained annealed importance sampling perspective seen form thermodynamic integration see gelman meng 
expect thermodynamic integration estimate finite number points suffer systematic error results show annealed importance sampling estimate ratio normalizing constants fact unbiased converge correct value number annealing runs increases 
note procedure averages estimates multiple runs ratio normalizing constants log ratio natural point view statistical physics 
simulated tempering related method umbrella sampling preliminary estimates ratios normalizing constants required annealed importance sampling 
metropolis coupled markov chains share advantage disadvantage require storage states intermediate distributions 
annealed importance sampling circumstances tempered transitions may convenient general method estimating normalizing constants 
addition particular uses annealed importance sampling may attractive combines independent sampling ability markov chain sampler adapt characteristics distribution 
evans devised adaptive importance sampling method sequence intermediate distributions similar annealing 
method requires class tractable importance sampling densities defined contains density appropriate distributions sequence 
annealed importance sampling uses sampling distribution implicitly defined operation markov chain transitions density generally tractable compute making simple importance sampling infeasible 
perspective idea annealed importance sampling find appropriate importance weights sampling distribution looking ratios densities sequence intermediate distributions 
annoyance markov chain monte carlo need estimate autocorrelations order assess accuracy estimates obtained 
provided points start annealing runs generated independently need annealed importance sampling 
estimate variance normalized importance weights 
may easier nightmare scenarios drastically wrong results obtained indication problem possible methods sort 
annealed importance sampling occur distribution importance weights heavy upward tail apparent data collected 
annoyance markov chain monte carlo need decide run discard burn coming close equilibrium distribution 
long run simulated exact amount discarded may crucial shorter runs done desirable order diagnose possible non convergence decision may harder 
discarding little lead biased estimates discarding waste data 
annealed importance sampling analogous decision computation time spend annealing runs determine importance weights spend simulating chain samples starting final state annealing run usually desirable see section 
decision affects variance estimates results asymptotically correct regardless close final state equilibrium 
regenerative methods mykland tierney yu eliminate problems dealing sequential dependence replace possible problems due heavy tailed distributions 
regenerative methods appropriate splitting scheme devised markov chain sampler 
high dimensional problems may harder defining appropriate sequence intermediate distributions annealed importance sampling 
discussed section time required annealed importance sampling expected increase direct proportion dimensionality problem addition increase due markov chain samplers slower higher dimensions 
consider human computer time required select appropriate sequence intermediate distributions appropriate markov chain transitions 
reasons annealed importance sampling probably useful allows find needed ratios normalizing constants serves avoid problems isolated modes 
note potential problems multiple modes exists theoretical guarantee distribution unimodal 
david mackay helpful comments 
research supported natural sciences engineering research council canada 
evans 
chaining annealing annals statistics vol 
pp 

gelman meng 
simulating normalizing constants importance sampling bridge sampling path sampling appear statistical science 
geyer 
markov chain monte carlo maximum likelihood editor computing science statistics proceedings rd symposium interface pp 
interface foundation 
geyer thompson 
annealing markov chain monte carlo applications ancestral inference journal american statistical association vol 
pp 

geweke 
bayesian inference econometric models monte carlo integration econometrica vol 
pp 

gilks richardson spiegelhalter 
markov chain monte carlo practice london chapman hall 
hastings 
monte carlo sampling methods markov chains applications biometrika vol 
pp 

kirkpatrick gelatt vecchi 
optimization simulated annealing science vol 
pp 

parisi 
simulated tempering new monte carlo scheme letters vol 
pp 

maceachern clyde liu 
sequential importance sampling nonparametric bayes models generation appear canadian journal statistics 
metropolis rosenbluth rosenbluth teller teller 
equation state calculations fast computing machines journal chemical physics vol 
pp 

mykland tierney yu 

regeneration markov chain samplers journal american statistical association vol 
pp 

neal 
sampling multimodal distributions tempered transitions statistics computing vol 
pp 

ripley 
stochastic simulation new york john wiley 

sampling distributions monte carlo free energy estimation umbrella sampling journal computational physics vol 
pp 

