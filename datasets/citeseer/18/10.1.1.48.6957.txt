proceedings aaai planning deadlines stochastic domains thomas dean leslie pack kaelbling jak kirman ann nicholson department computer science brown university providence ri cs brown edu provide method theory markov decision problems efficient planning stochastic domains 
goals encoded reward functions expressing desirability world state planner find policy mapping states actions maximizes rewards 
standard goals achievement goals maintenance prioritized combinations goals specified way 
optimal policy existing methods methods best polynomial number states domain number states exponential number propositions state variables information starting state reward function transition probabilities domain restrict planner attention set world states encountered satisfying goal 
furthermore planner generate complete plans depending time available 
describe experiments involving mobile robotics application consider problem scheduling different phases planning algorithm time constraints 
completely deterministic world possible planner simply generate sequence actions knowing executed proper order goal necessarily result 
nondeterministic worlds planners address question things go expected 
method triangle tables fikes plans executed robustly circumstance nominal trajectory world states allowing certain classes failures serendipitous events 
case execution error move world situation previously considered planner 
systems sipe example wilkins monitor plan failures initiate replanning 
replanning slow useful time critical domains 
schoppers universal plans schoppers gives method generating reaction possible situation plan execution plans robust fast execute large expensive generate 
inherent contradiction approaches 
world assumed deterministic purpose planning nondeterminism accounted performing execution monitoring generating reactions world states nominal planned trajectory 
address problem planning nondeterministic domains nondeterminism account start 
explored body theory algorithms addressing question finding optimal policies universal plans nondeterministic domains 
unfortunately methods impractical large state spaces 
know start state model nature world nondeterminism restrict planner attention set world states encountered way goal 
furthermore planner generate complete plans depending time available 
way provide efficient methods existing techniques finding optimal strategies planning time constraints non deterministic domains 
approach addresses uncertainty resulting control error sensor error assume certainty observations 
assume environment modeled stochastic automaton set states set actions matrix transition probabilities 
simplest cases achieving goal corresponds performing sequence actions results state satisfying proposition 
guarantee length sequence needed achieve goal stochastic domain interested building planning systems minimize expected number actions needed reach goal 
approach constructing plan achieve goal corresponds finding policy mapping states actions maximizes expected performance 
performance expected accumulated reward sequences state transitions determined underlying stochastic automaton 
rewards determined reward function mapping states real numbers specially formulated goal 
policy framework corresponds universal plan achieving goals quickly average 
refer automaton modeling environment system automaton 
generating optimal policy system automaton formulate simpler restricted stochastic automaton search optimal policy restricted automaton 
state space restricted automaton called envelope subset states system automaton augmented special state represents state outside envelope 
algorithm developed consists basic subroutines 
envelope extension adds states restricted automaton making approximate system automaton closely 
policy generation computes optimal policy restricted automaton complete policy system automaton constructed augmenting policy restricted automaton set default actions reflexes executed states outside envelope 
algorithm implemented anytime algorithm dean boddy interrupted point execution return answer value certain classes stochastic processes improves expectation function computation time 
gather statistics envelope extension policy generation improve performance statistics compile expectations allocating computing resources time critical situations 
focus primarily details algorithm results series computational experiments provide indication merit 
subsequent papers expand representation goals deal complicated models interaction require sophisticated methods allocating computational resources 
planning algorithm definitions model entire environment stochastic automaton 
finite set world states assume reliably identified agent 
finite set actions action taken state 
transition model environment function mapping elements theta discrete probability distributions write pr probability world transition state state action taken 
policy mapping specifying action taken situation 
environment combined policy choosing actions environment yields markov chain kemeny snell reward function mapping specifying instantaneous reward agent derives state 
policy reward function value state sum expected values rewards received time step discounted far occur 
fl reward received tth step executing policy starting state discounting factor fl controls influence rewards distant 
due properties exponential definition rewritten fl pr say policy dominates better 
policy optimal dominated policy 
common goals achieve certain condition soon possible 
define reward function holds state gamma represent goal states absorbing optimal policy result agent reaching state satisfying soon possible 
absorbing means actions result state probability pr 
making goal states absorbing ensures go nearest state holds independent states follow 
language reward functions quite rich allowing specify complex goals including maintenance properties world prioritized combinations primitive goals 
partial policy mapping subset actions domain partial policy called envelope fringe partial policy set states envelope policy may reached step policy execution state envelope 
fs pr construct restricted automaton take envelope states add distinguished state 
states action transition probabilities remain 
define probability going envelope pr gamma pr state absorbing 
cost falling envelope parameter depends domain 
possible re invoke planner agent falls envelope approach assign estimated value state agent fell minus function time construct new partial policy 
reward function described earlier value state negative magnitude expected number steps goal time spent planning penalized simply added magnitude value state suitable weighting function 
structure assume initially separate phases operation planning execution 
planner constructs policy followed agent new goal pursued agent falls current envelope 
sophisticated models interaction planning execution possible including planner runs concurrently execution sending new expanded strategies developed 
questions schedule deliberation discussed section see dean 
execution explicit policy trivial describe algorithm generating policies 
high level planning algorithm description environment start state follows 
generate initial envelope 
deadline extend envelope generate optimal policy restricted automaton state set 
return algorithm finds small subset world states calculates optimal policy states 
gradually adds new states order policy robust decreasing chance falling envelope 
new states added optimal policy new envelope calculated 
note interdependence steps choice states add envelope extension may depend current policy policy generated result optimization may quite different depending states added envelope 
algorithm terminates deadline reached envelope expanded include entire state space 
sections consider subcomponent algorithm detail 
generating initial envelope high level algorithm works matter initial envelope chosen done intelligence early policies useful 
examples consider goal state satisfying soon possible 
simple goals achievement initial envelope containing chain states initial state state satisfying state action non zero probability moving state chain 
implemented system generate initial envelope doing depth search considering probable outcome action decreasing order probability 
yields set states traversed fairly high probability goal state 
sophisticated techniques generate initial envelope strategy spend little time possible doing plausible policy available soon possible 
generating optimal policy howard policy iteration algorithm guaranteed generate optimal policy restricted automaton 
algorithm works follows 
policy 
loop calculate solving set linear equations unknowns equation action fl pr 
return algorithm iterates generating step policy strictly dominates previous policy terminates policy longer improved yielding optimal policy 
iteration values states current policy computed 
done solving system equations potentially operation realistic environments transition state transition matrix sparse allowing efficient solution equations 
algorithm improves policy looking states doing action step continuing result higher expected reward simply executing 
state policy changed chooses action state 
algorithm requires number iterations polynomial number states practice instance domain world states taken iterations 
subroutine planning algorithm generate random policy step fixed state absorbing need explicitly included policy calculations 
subsequent steps old policy starting point policy iteration 
general policy change radically envelope extended requires iterations typically policy iteration algorithm generate optimal policy extended envelope 
occasionally dire consequence exceptional new path discovered policy changed 
extending envelope number possible strategies extending envelope appropriate depends domain 
aim envelope extension judiciously broaden subset world states including states outside envelope current policy may reached executing policy 
simple strategy add entire fringe current policy result adding states uniformly current envelope 
case states fringe current policy 
reasonable strategy similar advocated drummond bresina drummond bresina look fringe states 
simulating restricted automaton accumulating probabilities falling fringe state 
choice strategies 
add fringe states 
alternatively goals achievement take element subset fringe states find chain states leads back state envelope 
experiments described sections fringe states added paths back envelope 
example approach drummond bresina extending current policy coupled tightly naturally changing policy required keep optimal respect restricted view world 
example illustrates changes algorithm described 
example domain mobile robot path planning 
floor plan divided grid locations directional states associated location fn wg corresponding direction robot facing resulting total world states 
actions available robot go turn right turn left turn 
transition probabilities outcome action may obtained empirically 
experimental simulation stay action guaranteed succeed 
probability success go turning actions locations remainder probability mass divided undesired results overshooting rotating slipping sideways world contains sinks locations difficult impossible leave 
average state successors 
shows subset domain locations surrounding complete sink non zero transitions accessible direction north 
small squares associated location possible heading small square corresponds state direction arrow shows policy robot location heading 
shows optimal policy small early envelope figures show subsequent envelopes policy changes direct robot circumvent reflecting aversion risk involved shortest path 
deliberation scheduling stage algorithm generating policies provided previous section agent allocate processor time stages faced time critical situation 
determining allocations called deliberation scheduling dean boddy consider situations agent deadline initial state deadline produce policy adjustments policy allowed 
interval time current time deadline called deliberation interval 
address complicated situations dean deliberation scheduling relies compiling statistics produce expectations regarding performance improvements guide scheduling 
general guarantee algorithm produce sequence policies increase value delta delta delta complete policies constructed adding reflexes partial policies generated algorithm 
best hope algorithm produces sequence policies values increase expectation delta delta delta initial state considered random variable 
allocating processor time concerned expected improvement gamma relative allocation processor time 
envelope extension current policy just partition deliberation interval subintervals spent envelope extension second policy generation 
stages mutually dependent consider performing multiple rounds round involves amount envelope extension followed amount policy generation 
ee pg time allocated envelope extension policy generation ith round algorithm ei envelope ith round envelope extension 
obtain optimal deliberation schedule consider expected value goal location env env env env env env env env env env env env env start best path turn left turn right actions go turn example policy change different envelopes near complete sink 
direction arrow indicates current policy state 
sink envelope policy chooses straightforward shortest path 
sink included policy north 
states surrounding included barriers south east west sides allow policy take longer safer path 
run fl 
final policy 
rounds possible allocations ee pg suspect finding optimal deliberation schedule np hard 
expedite deliberation scheduling greedy algorithm statistics 

expected improvement starting envelope size adding states gamma 

expected time required extend states envelope size compute optimal policy resulting restricted automaton ee pg 
round envelope extension followed policy generation envelope size find maximizing ratio add states perform round time permitting 
deadline occurs envelope extension algorithm returns policy round 
deadline occurs policy generation algorithm returns policy iteration policy iteration 
results section results iterative refinement algorithm table lookup deliberation scheduling strategy statistics described previous sections 
generated data points compute required statistics planning domain 
start goal states chosen randomly executions planning algorithm greedy deliberation strategy number fringe states added phase envelope extension determined deliberation scheduling statistics 
compared performance planning algorithm greedy deliberation strategy policy iteration optimizing policy domain 
results show planning algorithm greedy deliberation strategy supplies policy early typically converges policy close optimal domain policy iteration method 
shows average results runs single run involves particular start state goal state 
graph shows average improvement start state policy available time function time 
order compare results different start goal runs show average ratio value current policy value optimal policy domain plotted ratio actual time time opt policy iteration takes reach optimal value 
greedy deliberation strategy performs significantly better standard optimization method 
considered simple strategies adding small fixed iteration adding value time complete greedy comparison planning algorithm greedy deliberation strategy dashed line policy iteration optimization method solid line 
fringe iteration performed fairly domain greedy policy 
experimentation required draw definitive comparative performance deliberation strategies particular domains 
related primary interest applying sequential decision making techniques bellman bellman howard howard time critical applications 
initial motivation methods discussed came anytime synthetic projection drummond bresina 
drummond bresina improve drummond bresina providing coherent semantics goals stochastic domains ii theoretically sound probabilistic foundations iii decision theoretic methods controlling inference 
approach described represents particular instance time dependent planning dean boddy borrows horvitz horvitz approach flexible computation 
boddy boddy describes solutions related problems involving dynamic programming 
hansson mayer bps bayesian problem solver hansson mayer supports general statespace search decision theoretic control inference may bps basis envelope extension providing finegrained decision theoretic control 
christiansen goldberg christiansen goldberg address problem planning stochastic domains 
approach applicable stochastic domains certain characteristics typically multiple paths goal domain relatively benign 
path goal done procedure finding initial envelope extending envelope improves policy new states recovered 
research plans involve extending approach directions allowing complex goals performing complicated deliberation scheduling integrating online deliberation parallel execution policies relaxing assumption observation certainty handle sensor error 

thomas dean supported part national science foundation presidential young investigator award iri advanced research projects agency department defense monitored air force contract 
national science foundation conjunction advanced research projects agency department defense contract 
iri 
leslie kaelbling supported part national science foundation national young investigator award iri part onr contract arpa order 
bellman 
dynamic programming 
princeton university press 
boddy 
anytime problem solving dynamic programming 
proceedings aaai 
aaai 

christiansen goldberg 
robotic manipulation planning stochastic actions 
darpa workshop innovative approaches planning scheduling control 
san diego california 
dean boddy 
analysis timedependent planning 
proceedings aaai 
aaai 

dean kaelbling kirman nicholson 
deliberation scheduling time critical sequential decision making 
submitted ninth conference uncertainty artificial intelligence 
drummond bresina 
anytime synthetic projection maximizing probability goal satisfaction 
proceedings aaai 
aaai 

fikes hart nilsson 
learning executing generalized robot plans 
artificial intelligence 
hansson mayer 
heuristic search evidential reasoning 
proceedings fifth workshop uncertainty ai 

horvitz 
reasoning varying uncertain resource constraints 
proceedings aaai 
aaai 

howard 
dynamic programming markov processes 
mit press cambridge massachusetts 
kemeny snell 
finite markov chains 
van nostrand new york 
schoppers 
universal plans reactive robots unpredictable environments 
proceedings ijcai 


wilkins 
practical planning extending classical ai planning paradigm 
morgan kaufmann los altos california 
