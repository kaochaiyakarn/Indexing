power amnesia dana ron yoram singer naftali tishby institute computer science center neural computation hebrew university jerusalem israel propose learning algorithm variable memory length markov process 
human communication text handwriting speech multi characteristic time scales 
short scales characterized dynamics generate process large scales syntactic semantic information carried 
reason conventionally fixed memory markov models capture effectively complexity structures 
hand long memory models uniformly practical short memory 
algorithm propose minimizing statistical prediction error extending memory state length adaptively total prediction error sufficiently small 
demonstrate algorithm learning structure natural english text applying learned model correction corrupted text 
states model performance far superior fixed memory models similar number states 
show algorithm applied intergenic coli dna base prediction results comparable hmm methods 
methods automatically acquiring structure human language attracting increasing attention 
main difficulties modeling natural language multiple temporal scales 
known years language far complex finite memory markov source 
markov models powerful tools capture short scale statistical behavior language long memory models generally impossible estimate 
obvious desired solution markov source deep memory just really needed 
variable memory length markov models language modeling speech recognition time systematic derivation rigorous analysis learning mechanism proposed 
markov models natural candidate language modeling temporal pattern recognition due mathematical simplicity 
obvious finite memory markov models way capture recursive nature language trained effectively long memory 
notion variable length memory appear naturally context universal coding 
information theoretic notion known closely related efficient modeling 
natural measure appears information theory description length measured statistical predictability kullback liebler kl divergence 
algorithm propose optimizing statistical prediction markov model measured instantaneous kl divergence symbols current statistical surprise model 
memory extended precisely surprise significant statistical prediction stochastic model sufficiently 
apply algorithm successfully statistical language modeling 
demonstrate ability spelling correction corrupted english text 
show algorithm applied intergenic coli dna base prediction results comparable hmm methods 
prediction suffix trees finite state automata definitions notations sigma finite alphabet 
denote sigma set strings sigma 
string sigma length denoted denote empty string 
length string denoted jsj size alphabet sigma denoted sigmaj 
gamma denote longest prefix string denote set prefixes including empty string 
similarly suffix suffix set suffixes set strings called prefix free set phi psi 
call probability measure strings sigma proper string oe sigma soe 
prefix free set specifically integer sigma 
prediction suffix trees prediction suffix tree sigma tree degree sigmaj 
edges tree labeled symbols sigma internal node outgoing edge labeled symbol 
nodes tree labeled pairs fl string associated walk starting node root tree fl sigma output probability function related satisfying oe sigma fl oe 
prediction suffix tree induces probabilities arbitrary long strings manner 
probability generates string wn sigma denoted pi fl gamma gamma string labeling deepest node reached walk corresponding starting root definition prediction suffix tree induces proper measure sigma prefix free set strings fw specifically sigma pt 
example prediction suffix tree depicted fig 
left nodes tree labeled corresponding suffix 
left prediction suffix tree sigma 
strings written nodes suffixes nodes 
node probability vector possible symbols 
example probability observing observing string 
right equivalent probabilistic finite automaton 
bold edges denote transitions symbol dashed edges denote transitions 
states automaton leaves tree leaf denoted string replaced prefixes strings 
finite state automata markov processes probabilistic finite automaton pfa tuple sigma fl finite set states sigma alphabet size theta sigma transition function fl theta sigma output probability function probability distribution starting states 
functions fl satisfy requirements oe sigma fl oe 
probability generates string sigma pa fl gamma 
interested learning sub class finite state machines property 
state machine belonging sub class labeled string length sigma 
set strings labeling states suffix free 
require states symbol oe sigma oe labeled string labeled string suffix delta oe 
set strings labeling states suffix free exists string having property unique 
order defined set string set suffix free property string set symbol oe exists string suffix soe 
convenience point state denote string labeling state 
special case automata case includes strings length automata known markov processes order interested learning automata number states smaller means states long memory states short 
refer automata markov processes bounded memory case markov processes order identity states strings labeling states known learning process reduces approximating output probability function 
learning markov processes bounded memory task learning algorithm involved reveal identity states 
shown slightly complicated definition prediction suffix trees assuming initial distribution states stationary distribution models equivalent grow size linear proof equivalence scope transformation prediction suffix tree finite state automaton simple 
roughly speaking order implement prediction suffix tree finite state automaton define leaves tree states automaton 
transition function automaton delta delta defined set strings need slightly expand tree leaves expanded tree 
output probability function automaton fl delta delta defined prediction values leaves tree 
state leaf symbol oe fl oe fl oe 
outgoing edges states defined follows oe suffix oe 
example finite state automaton corresponds prediction tree depicted fig 
left depicted right part 
learning prediction suffix trees sample consisting sequence length sequences lengths find prediction suffix tree statistical properties sample predict outcome sequences generated source 
stage transform tree markov process bounded memory 
sequence created markov process algorithm find structure estimate probabilities process 
key idea iteratively build prediction tree probability measure equals empirical probability measure calculated sample 
start tree consisting single node labeled empty string add nodes reason believe tree 
node oes added tree statistically differs parent node natural measure check statistical difference relative entropy known kl divergence conditional probabilities 
observation space probability measures kl divergence dkl jjp log note distance symmetric absolutely continuous respect problem kl divergence measures additional information gained suffix oes prediction predicting shorter suffix cases statistical difference large probability observing suffix oes small neglect cases 
weigh statistical error prior probability observing oes 
statistical error measure case err oes oes dkl jjp oes oe sigma oe log oe oe js oe sigma log oe js oes node oes added tree statistical difference defined err oes node larger predetermined accuracy ffl 
tree grown level level adding son leaf tree statistical surprise large 
problem requirement node statistically differs parent node necessary condition belonging tree sufficient 
leaves prediction suffix tree differ parents redundant internal nodes property 
continue testing potential descendants leaves tree depth order avoid exponential grow number strings tested test strings belong branches reached small probability 
set strings tested step denoted viewed kind potential frontier growing tree stage construction completed produce equivalent markov process bounded memory 
learning algorithm prediction suffix tree depicted fig 

algorithm gets parameters accuracy parameter ffl maximal order process maximal depth tree true source probabilities known estimated empirical counts appearances observation sequences 
denote number time string appeared observation sequences oejs number time symbol oe appeared string laplace rule succession empirical estimation probabilities delta sigma jsj sigmaj oejs oejs delta oejs oe sigma oe js sigmaj toy learning example algorithm applied symbols long sequence produced automaton depicted top left fig 

alphabet binary 
bold lines represent transition symbol dashed lines represent symbol 
prediction suffix tree plotted stage algorithm 
ffl initialize tree candidate strings consists single root node foe oe sigma oe fflg 
ffl 
pick remove 
err suffix ffl add node corresponding nodes path deepest node deepest ancestor 
jsj oe sigma oes ffl add oes algorithm learning prediction suffix tree 
run automaton plotted bottom right 
note original automaton learned automaton small transition probabilities 
original automaton top left instantaneous automata built run algorithm left right top bottom final automaton bottom left 
applications applied algorithm bible ffl resulted automaton having states 
alphabet english letters blank character 
final automaton constitutes states length qu xe hand symbols long states shall 
indicates algorithm really captures notion variable context length prediction resulted compact accurate model 
building full markov model case impossible requires sigmaj states 
demonstrate algorithm cleaning corrupted text 
test text taken training sequence modified different ways 
stationary noise altered letter probability text modified changing blank random letter 
probable state sequence dynamic programming 
cleaned observation sequence probable outcome knowledge error rate 
example decoding types noise shown fig 

applied algorithm intergenic original text god called dry land earth gathering waters called seas god saw god said earth bring forth grass herb yielding seed fruit tree yielding fruit kind corrupted text god earth ibd oj waters re seas aed god saw ann god said tae earth bring forth tse tree kind corrected text god caused dry land earth gathering waters called sees god saw god said earth bring forth grass memb yielding fruit tree fielding fruit kind corrupted text corrected text god called dry land earth gathering called god saw took god said forth grass herb yielding seed thy fruit fruit kind cleaning corrupted text markov process bounded memory 
regions coli dna ffl 
alphabet result algorithm automaton having states 
names states final automaton depicted fig 

performance model compared models hmm model calculating normalized log likelihood nll unseen data 
nll empirical measure entropy source induced model 
nll bounded memory markov model obtained hmm model 
markov model contain length distribution intergenic segments hmm model slightly better 
hand hmm model complicated requires manual tuning architecture 
aa ac ca cc ct cg ta tc tt tg ga gc gt gg aac aat tag tca tct tta ttg gaa gac gat gag gca gta gga states constitute automaton predicting base intergenic regions coli dna 
research new efficient algorithm estimating structure transition probabilities markov processes bounded variable memory 
algorithm applied natural language modeling result compact accurate model captures short term correlations 
theoretical properties algorithm described 
fact prove slightly different algorithm constructs bounded memory markov process arbitrary high probability induces distributions sigma close induced true markovian source sense kl divergence 
algorithm uses polynomial size sample runs polynomial time parameters problem 
investigating hierarchical models automata able capture multi scale correlations model large scale structure natural language 
acknowledgment lee giles providing software plotting finite state machines anders krogh david haussler letting coli dna data helpful discussions 
clore foundation support 
kemeny snell finite markov chains springer verlag 
freund kearns ron rubinfeld schapire sellie efficient learning typical finite automata random walks stoc 
jelinek self organized language modeling speech recognition 
nadas estimation probabilities language model ibm speech recognition system ieee trans 
assp vol 
pp 

kullback information theory statistics new york wiley 
rissanen langdon universal modeling coding ieee trans 
info 
theory pp 

rissanen stochastic complexity modeling ann 
stat 
krogh mian haussler hidden markov model finds genes coli dna ucsc tech 
rep ucsc crl 
