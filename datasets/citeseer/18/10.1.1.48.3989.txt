information theoretic analysis hard soft assignment methods clustering michael kearns labs research florham park new jersey mansour tel aviv university tel aviv israel andrew ng carnegie mellon university pittsburgh pennsylvania assignment methods heart algorithms unsupervised learning clustering particular known means expectation maximization em algorithms 
study different methods assignment including hard assignments means soft assignments em 
known means minimizes distortion data em maximizes likelihood little known systematic differences behavior algorithms 
shed light differences information theoretic analysis 
cornerstone results simple decomposition expected distortion showing means extension inferring general parametric densities unlabeled sample data implicitly manage trade similar data assigned cluster data balanced clusters 
data balanced measured entropy partition defined hard assignments 
addition letting predict verify systematic differences means em specific examples decomposition allows give general argument showing means consistently find densities overlap em 
study third natural assignment method call posterior assignment close spirit soft assignments em leads surprisingly different algorithm 
algorithms density estimation clustering unsupervised learning important tool machine learning 
classical algorithms means algorithm expectation maximization em algorithm 
algorithms applied wide variety settings including parameter estimation hidden markov models speech recognition estimation conditional probability tables belief networks probabilistic inference various clustering problems 
high level means em appear similar perform step iterative optimization performed repeatedly convergence 
step assignment data points clusters density models second step reestimation clusters density models current assignments 
means em algorithms differ manner assign data points step 
loosely speaking case clusters density models clusters means assigns assigned call hard winner take wta assignment 
contrast em assigns fractionally assigning weight assigning rest call soft fractional assignment 
third natural alternative assign means randomly assign assigning probability 
call posterior assignment 
assignment methods interpreted classifying points belonging distinct populations solely basis probabilistic models densities populations 
alternative interpretation different ways inferring value hidden unobserved variable value indicate sources generated observed data point 
assignment methods differ context unsupervised learning subject 
context unsupervised learning em typically viewed algorithm mixture density estimation 
classical density estimation finite training set unlabeled data derive hypothesis density 
goal hypothesis density concentrate case just clusters densities simplicity development 
results hold general case clusters densities 
model true sampling density accurately possible typically measured kl divergence 
em algorithm find mixture density model form ff gamma ff known mixture model em local minimum log loss equivalent local maximum likelihood empirical analogue kl divergence 
means algorithm viewed vector quantization algorithm referred lloyd max algorithm vector quantization literature 
known means find local minimum distortion quantization error data discuss length 
fractional wta assignment methods natural widely iterative optimization heuristic em means respectively known loss function locally minimized algorithm log loss distortion respectively 
relatively little known precise relationship loss functions attendant heuristics 
structural similarity em means leads considered closely related roughly equivalent 
duda hart go far saying means viewed approximate way obtain maximum likelihood estimates means goal density estimation general em particular 
furthermore means formally equivalent em mixture gaussians covariance matrices ffli identity matrix limit ffl 
practice conflation algorithms means density estimation applications due rapid convergence obtain initial parameter values subsequent execution em 
simple examples means em converge different solutions preceding remarks tell entire story 
quantitative statements systematic differences algorithms loss functions 
answer question giving new interpretation classical distortion locally minimized means algorithm 
give simple information theoretic decomposition expected distortion shows means algorithm seeking minimize distortion manage trade data balanced distributed clusters hard assignments accuracy density models sides assignment 
degree data balanced clusters measured entropy partition defined assignments 
refer trade information modeling trade 
information modeling trade identifies significant ways means em differ 
em seeks model entire sampling density mixture model ff gamma ff means concerned explicitly identifying distinct subpopulations sampling density finding models separately second choice subpopulations identified means may strongly influenced entropy partition define em influence entirely absent 
differences intuitive result differing assignment methods formalize second obvious determine behavior means simple examples shall see 
addition letting predict explain behavior means specific examples new decomposition allows derive general prediction means em differ means tend find density models overlap compared em 
certain simple examples bias means apparent argue general bias depends little sampling density form density models algorithms 
mathematical framework allows analyze variant means maintains unequal weightings density models show weighting interesting effect loss function essentially erasing incentive finding partition high entropy 
study posterior assignment method mentioned show despite resulting loss function algebraic similarity iterative optimization performed em differs dramatically 
results interest applying em means variants problems unsupervised learning 
loss decomposition hard assignments suppose densities possibly randomized mapping maps refer partition think assigning points exactly think density model points assigned 
may flip coins determine assignment output value words hard assignments 
call triple fp partitioned density section propose measure goodness partitioned densities explore interpretation consequences 
settings consider partition determined additional parameters suppress dependency quantities notational brevity 
simple examples hard assignment methods methods discussed wta assignment means assigned call posterior assignment assigned probability 
soft fractional assignment method em fall framework fractionally assigned development assume unclassified data drawn fixed unknown density distribution call sampling density 
partitioned density fp reasonable way measure partitioned density models sampling density 
far concerned mentioned ask density model sampling density conditioned event words imagine partitions distinct subpopulations demand separately model subpopulations 
immediately clear criteria ask meet defer question moment 
fix partitioned density fp define partition loss theta gamma log expectation possible randomization suppressed dependence partitioned density consideration notational brevity logarithm base 
ask partition loss minimized capture informal measure goodness proposed assignment method assign penalize assigned density log loss gamma log 
define training partition loss finite set points expected partition loss respect natural ways 
digress briefly show special case multivariate gaussian normal densities means identity covariance matrices partition wta assignment method partition loss set points equivalent known distortion quantization error set points modulo additive multiplicative constants 
distortion respect simply min jjx gamma jj jjx gamma jj jjx gamma jj assigns nearer euclidean distance wta assignment 
dimensional gaussian gamma jjx gamma jj wta assignment respect partition loss gamma log log jjx gamma jj jjx gamma jj log log term equation distortion times constant second term additive constant depend minimization partition loss equivalent minimization distortion 
generally equal dimensioned real vectors measure distortion distance metric expressed function gamma distortion smaller distances distortion special case partition loss density gammad wta assignment 
property function gamma sufficient condition ensure normalization factor independent depends partition loss include additional dependent term distortion guarantee general minimizations equivalent 
returning development turns expectation partition loss respect sampling density interesting decomposition interpretation 
step shall require basic important definitions 
fixed mapping value define pr 
define delta pr probability taken randomization mapping simply distribution conditioned event splits note definitions depend partition determined 
write expectation partition loss respect gamma log gamma log log gamma log log gamma log kl jjp kl jjp kl jjp kl jjp kl jjp denotes kullback leibler divergence denotes entropy random variable distributed possibly randomized assignment 
decomposition form cornerstone subsequent arguments take moment examine interpret detail 
remember term equation depends coupled way depends assignment method 
caveat note quantity kl jjp natural measure models respective side partition defined discussed informally 
furthermore weighting terms equation natural 
instance approaches approaches important kl jjp small partition assigns negligible fraction population category important model category especially important accurately model dominant category 
isolation terms kl jjp kl jjp encourage choose sides split defined fact modeled terms isolation 
term equation measures informativeness partition defined reduces entropy precisely appealing symmetry mutual information may write distributed gamma gamma gamma jx gamma gamma jx gammap log gamma gamma log gamma binary entropy function 
term independent partition see equation reduces uncertainty amount gamma jx 
note deterministic mapping wta assignment jx simply maximizes 
particular deterministic optimal respect regardless resulting general case jx measure randomness trade competing quantities example maximized flips coin gammah jx minimized 
important expect may competition modeling terms kl jjp kl jjp partition information term 
chosen parametric class densities limited complexity instance multivariate gaussian distributions demand kl jjp small interpreted demand partition yield simple virtue approximated kl divergence sense densities lying 
demand may tension demand informative equation prescription manage competition refer sequel trade 
view implicitly defining hard partition case wta assignment partition loss provides particular way evaluating goodness models sampling density course ways evaluating evaluate mixture kl divergence kl discuss general case mixture coefficients shortly 
expression locally minimized standard density estimation approaches em particularly call attention ways equation differs expression 
equation differ incorporating penalty partition asking mixture model entire population asked credit modeling respective return differences considerably detail section 
close section observing chosen class densities constrain wta assignment method simple familiar iterative optimization algorithm locally minimizing partition loss set points choices simply repeat steps convergence ffl wta assignment set set points set gamma ffl reestimation replace argmin gamma log noted case restricted gaussian densities identity covariance matrices means parameters algorithm reduces classical means algorithm 
natural extension estimating general parametric class may parameters just means 
abuse terminology simply refer generalized version means 
reader familiar em algorithm choosing recognize algorithm simply hard wta assignment variant unweighted em mixture coefficients equal 
easy verify means result local minimum partition loss chosen wta assignment method 
rename special case partition loss means loss convenience 
fact means locally minimizes means loss combined equation implies means implicitly manage trade 
note means increase means loss iteration mean terms equation increase see examples case 
observed vector quantization literature iteration means estimated means fact true means points assigned imply instance terms kl jjp nonincreasing example change iteration 
note easily generalize equation cluster case eq kl jjp note equation gamma gamma jx distributed general log quantity 
weighted means noted means hard assignment variant unweighted em algorithm mixture coefficients forced general case densities 
natural generalization means thought hard assignment variant weighted em 
class densities space weighted means takes input set data points outputs pair densities weight ff 
generalization case densities weights straightforward 
algorithm begins random choices ff repeatedly executes steps ffl wta assignment set set points ff gamma ff set gamma ffl reestimation replace argmin gamma log ffl reweighting replace ff js jsj 
ask question loss function algorithm locally minimizing 
fix weighted wta partition ff gamma ff 
note deterministic general ff adjustable parameter weighted means algorithm necessarily defined current weighted wta partition depends 
turns weighted means find give local minimum unweighted means loss slightly different loss function expectation differs unweighted means loss interesting way 
define weighted means loss gamma log ff gammaf gamma ff weighted wta partition determined ff data set define fx bg 
show weighted means fact increase weighted means loss iteration 
gamma log ff gammaf gamma ff gamma log ff gamma log gamma ff gamma log gamma log log ff gamma js log gamma ff log ff gamma js log gamma ff js jsj log ff js jsj log gamma ff entropic expression minimized choice ff js jsj 
exactly new value ff computed weighted means current assignments furthermore summations equation clearly reduced obtain densities minimize log loss respectively exactly new densities computed weighted means 
weighted means decreases weighted means loss equation fp iteration justifying naming loss 
fixed expected weighted means loss respect sampling density 
gamma log ff gammaf gamma ff ji theta gamma log gammaw log ff gamma log gamma ff grateful nir friedman pointing derivation 
pr 
term right hand side just expected partition loss fp 
terms give binary distributions gamma ff gamma ff 
fixed fp say cross entropy weighted means know convergence ff js jsj weighted means ff iteration js jsj simply empirical estimate limit large samples expect gamma log gamma log combining equation equation general decomposition partition loss equation gives ff weighted means gamma log ff gammaf gamma ff ji kl jjp kl jjp gammaw log gamma log kl jjp kl jjp gamma gammaw log gamma log kl jjp kl jjp depend ff may think generalization goal weighted means finding fp minimizes sum kl jjp kl jjp 
differs goal unweighted means ways 
weight ff changed definition partition changed definition fixed unweighted means corresponds fixing ff 
weight ff removed bias finding informative partition information modeling trade weighted means algorithm try minimize modeling terms kl jjp kl jjp 
note quite different mixture kl divergence minimized em 
means vs em examples section consider different sampling densities compare solutions means unweighted weighted em 
example significant differences error surfaces defined parameter space means losses kl divergence 
main tool understanding differences loss decompositions unweighted means loss equation weighted means loss equation 
important remember solutions algorithms considered better algorithms simply different loss functions justifiable terms choice loss function minimize algorithm determines solution find 
examples instance space simply 
compare solutions unweighted weighted em unweighted weighted means output pair fp gaussians oe oe oe oe parameters adjusted algorithms 
weighted versions algorithms output weight parameter ff 
case em output interpreted representing mixture distribution evaluated kl divergence sampling density 
case unweighted weighted means output interpreted partitioned density evaluated expected unweighted weighted means loss respect sampling density 
note generalization classical vector quantization case simply allowing gaussians non unit variance 
example various algorithms run examples sampling density dimensional problems sample size sufficient ensure observed behavior close running directly sampling density 
example 
sampling density symmetric gaussian mixture gamma see 
suppose initialized parameters algorithms gamma oe oe 
algorithm begins search true parameter values sampling density 
behavior unweighted em clear starting em global minimum expected loss function kl divergence staying begins em enjoy solution perfectly models sampling density kl divergence 
true weighted em presence absence weighting parameter ff essentially irrelevant optimal value parameter ff choice unweighted means 
examine terms decomposition expected partition loss equation 
term minimized initial choice parameters wta partition simply yields 
terms kl jjp kl jjp different story 
notice conditioned event gamma 
gamma chopped tail sampling density example 
sampling density example 
iteration evolution means loss top plot decomposition example kl divergences kl jjp kl jjp bottom plot partition information gain middle plot function iteration unweighted means running examples 
plot sampling mixture density example 
low added 
equivalently gamma tail reflected back 
clearly tail reflection operation gamma results moves mean left gamma tail reflection moved mass left reduces variance tail moved final mean 
respect term kl jjp best choice smaller initial value gamma best choice oe smaller initial value 
symmetric remarks apply term kl jjp 
furthermore long movements oe oe symmetric wta partition remain unchanged movements possible improve terms kl jjp initial conditions degrading initially optimal value term 
essentially prediction weighted means optimal performance achieved ff 
performing experiment finite sample find iterations means converged solution gamma oe oe yields 
predicted means pushed origin variances reduced 
naturally kl divergence sampling density mixture model inferior starting parameters expected means loss superior 
simple example easy predict behavior means directly 
point decomposition equation provides justification behavior provided regarding means coarse approximation em 
move examples behavior various algorithms subtle 
example 
examine example term directly competes kl divergences 
sampling density single unit variance gaussian see 
consider initial choice parameters oe distant location say oe 
examine behavior unweighted means 
wta partition defined settings 
little mass partition informative 
term kl jjp equation negligible 
furthermore tail reflection described example occurs tail negligible part density 
kl jjp kl jjp kl jjp 
words cared kl divergence terms settings near optimal 
information modeling trade moving closer origin kl divergences may degrade obtain informative partition 
iterations unweighted means converges gamma oe oe yields 
information modeling tradeoff illustrated nicely simultaneously plot unweighted means loss terms kl jjp kl jjp function number iterations run 
plot clearly shows increase meaning decrease increase kl jjp kl jjp 
fact gain partition information worth increase kl divergences shown resulting decrease unweighted means loss 
note especially difficult justify solution unweighted means viewpoint density estimation 
predicted equation behavior weighted means dramatically different algorithm incentive find informative partition concerned kl divergence terms 
find iterations converged oe oe ff 
expected weighted means chosen completely uninformative partition exchange making kl jjp 
values oe simply reflect fact convergence assigned rightmost points examples 
note behavior means algorithms different em prefer resulting mixture 
solution weighted means closer em sense weighted means effectively eliminates densities fits sampling density single gaussian 
example 
slight modification sampling distribution example results interesting subtle difference behavior algorithms 
essentially example addition small distant spike density see 
starting unweighted means initial conditions oe oe kl jjp kl jjp obtain convergence solution gamma oe oe shown kl jjp kl jjp 
example unweighted means starts solution better kl divergences worse partition information elects degrade exchange improvement 
interesting note bounded significantly away presumably improvement partition information worth degradation kl divergences 
words solution minimum means loss truly balance terms movement parameters direction causes loss increase due decrease partition information movement parameters direction causes loss increase due increase modeling error 
example local minimum unweighted means loss sampling density oe oe suboptimal unweighted means loss 
clearly local minimum kl divergence terms minimized expense uninformative partition 
essentially solution chosen weighted means regardless initial conditions easily predicted equation 
surprisingly example weighted means converges solution close equation 
example 
examine case sampling density mixture gaussians gamma see 
distinct subpopulations sampling density 
run unweighted means examples initial conditions gamma unweighted means sampling density example 
sampling density example 
distance means variation distance variation distance function distance sampling means em bottom grey line unweighted means lowest top grey lines posterior loss gradient descent middle top grey lines weighted means top grey line 
dark line plots 
plot equation vertical axis function horizontal axis 
line plotted 
oe oe obtain convergence gamma oe oe 
unweighted means sacrifices initial optimally informative partition exchange better kl divergences 
weighted means converges approximately solution predicted fact unweighted algorithm choose maximize partition information 
furthermore note modeled subpopulations gamma modeled natural clustering behavior algorithm prefers group middle subpopulation left right subpopulation splitting 
contrast unweighted em initial conditions converges approximately symmetric solution gamma oe oe unweighted em chooses split middle population difference means unweighted em example simple illustration difference quantities kl jjp kl jjp kl gamma ff shows natural case behavior means preferable clustering point view 
interestingly example solution weighted em quite close means 
means forces different populations partition loss decomposition equation better understanding loss function minimized means allowed explain differences means em specific simple examples 
general differences identify 
section give derivation strongly suggests bias inherent means algorithm bias finding component densities different possible sense precise 
denote variation distance densities jp gamma note 
notice due triangle inequality partitioned density fp assume loss generality pr 
case unweighted weighted means case deterministic partition chosen equation may write gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma examine equation detail 
assume case gamma 
equation lower bounds quantity approaches maximum value approaches 
extent succeed approximating differ 
partition loss decomposition equation includes terms kl jjp directly encouraging approximate true different technical senses approximation variation distance kl divergence 
rigorously kl jjq holds may write gamma kl jjp kl jjp gamma gamma gamma kl jjp kl jjp gamma gamma expression kl jjp kl jjp directly appears equation see means attempting minimize loss function encourages large case algorithm finds roughly equal weight clusters ensuing argument holds distance metric densities 
expect case unweighted means entropic term gammah equation 
weighted means entropic term eliminated 
show results simple experiment supporting suggestion means tends find densities overlap em 
experiment sampling density mixture dimensional unit variance gaussians varying distance means horizontal axis 
vertical axis shows variation distance target gaussians dark line variation distance solutions em grey line near solid line unweighted means lowest top grey lines posterior loss gradient descent discussed section middle top grey lines weighted means top grey line 
new algorithm posterior partition wta assignment method way making hard assignments basis natural hard assignment method natural 
suppose randomly assign fixed probability 
assign posterior probability generated prior assumption sampling density course may true 
call posterior partition 
nice property posterior partition compared wta assignment avoids potential truncation resulting wta assignment mentioned example form true sampling mixture components terms kl jjp zero 
recall occurred sampling density gaussian mixture gaussian wta assignment resulted gaussian tail reflected back posterior partition delta pr derivation kl jjp 
kl divergence terms expected partition loss equation encourage model sampling density definition reason tempting think posterior partition lead closer density estimation wta assignments 
situation subtle competing constraint informative partition 
see example moment 
note posterior partition partition loss fp fixed point theta gamma log gamma log gamma log expectation taken randomization call special case partition loss posterior loss 
posterior loss sample simply summation right hand side equation example revisited 
recall sampling density example gamma start gamma means weighted unweighted move means away origin symmetrically maximally informative partition preserved doing kl divergences improved 
posterior partition definition kl divergences improved initial conditions informativeness partition 
general expression gamma gamma jx distributed 
means choice term jx deterministic 
posterior partition stated initial conditions holds jx probabilistic 
possible better solution instance reducing variances moving means symmetrically away origin may able preserve reducing jx 
case starting stated initial parameter values steps gradient descent training posterior loss see discussion algorithmic issues arising finding local posterior loss results solution gamma oe oe point gradients respect parameters smaller absolute value 
solution expected posterior loss opposed initial conditions 
course kl divergence sampling density increased initial conditions 
algorithm order minimize expected posterior loss sample 
worth commenting algebraic similarity equation iterative minimization performed em 
unweighted em current solution sample data solution minimize gamma log log summand equation righthand side equation appear quite similar crucial difference 
equation decoupling posterior log losses gamma log current guesses fix posterior minimize resulting weighted log losses gamma log respect giving guess 
equation decoupling order evaluate potential solution log losses posteriors determined informal way explaining difference em current guess generate random labels posteriors minimize log losses labels get posterior loss evaluate generate labels 
obvious iterative algorithm minimize expected posterior loss 
alternative smoothly parameterized class densities resort gradient descent parameters minimize posterior loss 
intriguing difference posterior loss standard mixture log loss revealed examining derivatives 
fix densities point think representing mixture define log gamma log mixture logloss log ln gamma derivative expected behavior 
negative meaning mixture log loss decreased increasing give weight mixture 
second derivative goes gamma 
contrast define posterior loss post gamma log gamma log obtain post gamma log log log gamma ln derivative shows curious differences mixture log loss posterior loss 
notice sign derivative determined bracketed expression equation 
define bracketed expression rewritten gamma log gamma gamma ln function 
shows plot expression equation value horizontal axis 
plot see post positive point exhibit repulsive force occurs ratio falls certain critical value approximately 
explanation phenomenon straightforward equation long models somewhat poorly gives small probability preferable modeled poorly possibly assignment deterministic possible 
interesting note clustering algorithms data points explicit repulsive effects distant centroids proposed literature means selforganizing maps 
preceding discussion natural expect means minimizing posterior loss density class lead different say classical density estimation intuition derives fact repel sense 
means shown fairly general manner details omitted 
cover thomas 
elements information theory 
wiley interscience 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
duda hart 
pattern classification scene analysis 
john wiley sons 
gersho 
structure vector quantizers 
ieee transactions information theory 
hertz krogh palmer 
theory neural computation 
addisonwesley 
lauritzen 
em algorithm graphical association models missing data 
computational statistics data analysis 
macqueen 
methods classification analysis multivariate observations 
proceedings fifth berkeley symposium mathematics statistics probability volume pages 
rabiner juang 
fundamentals speech recognition 
prentice hall 
