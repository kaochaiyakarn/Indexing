title page parallel learning belief networks large difficult domains xiang chu contact author xiang associate professor department computer science university regina regina saskatchewan canada phone fax mail cs ca parallel learning belief networks large difficult domains xiang department computer science university regina regina saskatchewan canada chu avant sunnyvale ca usa learning belief networks large domains expensive single link lookahead search slls 
slls learn correctly class problem domains multi link lookahead search needed increases computational complexity 
experiment learning difficult domains dozen variables took days 
study parallelism speed slls learning large domains tackle increased complexity learning difficult domains 
propose natural decomposition learning task parallel processing 
investigate strategies job allocation processors improve load balancing efficiency parallel system 
learning large datasets regrouping available processors slow data access file system replaced fast memory access 
experimental results distributed memory mimd computer demonstrate effectiveness proposed algorithms 
keywords belief networks parallel implementation data mining probabilistic belief networks widely inference uncertain knowledge artificial intelligence 
alternative elicitation domain experts learning belief networks data actively studied 
task np hard general justified heuristics learning 
algorithms developed scoring metric combined single link lookahead search slls alternative network structures differing current structure link evaluated exhaustively adopted 
complexity polynomial number variables problem domain computation expensive large domains 
furthermore class domain models termed pseudo independent pi models learned correctly slls 
alternative multi link lookahead search consecutive structures differ multiple links 
complexity higher 
experiment section learning variable pi domain model containing small pi submodels took half days learning variable pi domain model containing slightly larger pi submodel took days 
study parallel learning speed computation slls large domains tackle increased complexity potential pi domains 
focus learning decomposable markov networks show lessons learned applicable learning bayesian networks bns 
best knowledge investigation parallel learning belief networks 
learning graphical probabilistic models important subarea data mining knowledge discovery extends parallel data mining learning models 
focus multiple instruction multiple data mimd distributed memory architecture available discuss generalization lessons architectures 
organized follows self contained briefly introduce pi models sections 
sections propose parallel algorithms learning refinements 
experimental results sections 
graph theoretic terms unfamiliar readers list frequently acronyms included appendix 
pseudo independent models set discrete variables problem domain 
tuple assignment values variable probabilistic domain model pdm determines probability tuple disjoint sets variables conditionally independent xjy xjz shall denote 
oe marginally independent denoted oe 
table pi model table shows pdm binary variables 
pdm satisfies fv xg 
subset fv yg pair marginally dependent dependent third 
subset fu xg pair dependent third oe oe 
said collectively dependent marginally independent 
pdm pi model 
general pi model pdm proper subsets set collectively dependent variables display marginal independence 
example pi models include parity modulus addition problems 
pi models real datasets 
analysis data general social survey conducted statistics canada personal risk discovered pi models harmful drinking accident prevention 
disjoint subsets nodes undirected graph denote nodes intercept paths graph map pdm correspondence nodes variables disjoint subsets 
minimal map link removed resultant graph map 
minimal map pdm shown 
minimal map pdm table 
network structure learned slls 
algorithms learning belief networks shown unable learn correctly underlying pdm pi 
suppose learning starts empty graph nodes link 
slls connect oe 
connected 
results learned structure incorrect 
hand perform double link search single link search effectively test holds answer negative links added 
structure learned 
sequential algorithm parallel learning algorithms sequential algorithm seq learns structure chordal graph dmn cross entropy scoring metric 
structure learned numerical parameters easily estimated dataset 
search organized levels outer loop number lookahead links identical level 
level consists multiple passes repeat loop 
pass level alternative structures differ current structure number links evaluated 
search pass selects links decrease cross entropy maximally evaluating distinct valid combinations links 
corresponding entropy decrement significant links adopted pass level starts 
pass higher level starts 
survey variables 
analysis performed data subtopics due limited time 
pi models may analysis applied entire data 
algorithm seq input dataset set variables maximum size clique maximum number gamma lookahead links threshold ffih 
initialize empty graph repeat initialize entropy decrement dh set links oe chordal implied single clique size compute entropy decrement dh dh dh dh dh dh ffih done false done true done true return note intermediate graph chordal indicated statement innermost loop 
condition implied single clique means links contained subgraph induced helps reduce search space 
note algorithm greedy learning problem np hard 
link committed early search necessarily contained corresponding minimal map 
illustrates seq dataset variables fu yg 
slls performed simplicity 
search starts empty graph 
alternative graphs evaluated say selected 
pass starts current redrawn graphs evaluated 
repeating process suppose eventually graph obtained 
pass suppose graphs decreases cross entropy significantly 
graph final result 
task decomposition parallel learning algorithm seq pass level jn structures evaluated link added 
jn structures evaluated links added pass level tackle complexity speed slls large domains explore parallelism 
decompose learning task observation pass search exploration alternative structures coupled current structure 
current structure evaluation alternative structures independent evaluation performed parallel 
mentioned earlier study performed architecture processors communicate message passing vs shared memory 
partition pro example sequential learning cessors follows processor designated search manager structure explorers 
manager executes mgr algorithm 
pass generates alternative graphs current graph 
partitions sets distributes set explorer 
algorithm mgr input ffih algorithm seq total number explorers 
send explorer initialize empty graph repeat initialize cross entropy decrement dh partition graphs differ links sets send set graphs explorer explorer receive dh dh dh dh dh dh ffih done false done true done true send halt signal explorer return explorer executes epr 
checks graph received computes dh chordal graph 
chooses best graph reports dh manager 
manager collects reported graphs explorers selects global best starts pass search 
illustrates parallel learning explorers dataset variables fu yg 
slls performed simplicity 
manager starts empty graph 
algorithm epr receive manager repeat receive set graphs manager initialize dh received graph chordal implied single clique size compute dh dh dh dh dh send dh manager halt signal received sends alternative graphs explorers 
explorer checks graphs 
suppose selected reported manager 
suppose explorer reports 
collecting graphs manager chooses new current graph 
sends graphs 
repeating process manager gets graph sends graphs explorers 
suppose decreases cross entropy significantly 
manager chooses graph final result terminates explorers 
job allocation report report report report report job allocation explorer explorer manager halt signal job allocation report example parallel learning issue load balancing algorithm mgr alternative graphs evenly allocated explorers 
amount computation evaluating graph tends swing extremes 
graph non chordal discarded immediately computation 
hand graph chordal cross entropy decrement computed 
shows example graph 
graphs links differ link 
dotted links added resultant graph non chordal 
dashed links added resultant graph chordal 
complexity checking jn jej jej number links graph amount computation small 
complexity computing cross entropy decrement jdj log jdj number distinct tuples appearing dataset amount computation greater 
result job allocation may cause significant fluctuation explorers amount computation 
manager collect reports explorers new current graph selected explorers idle completing jobs 
chordal alternative structures shows time taken explorers particular pass learning dataset variables distributed memory mimd computer 
explorer took longer 
explorers sec job completion time explorers analysis implies sophisticated job allocation strategy needed improve efficiency parallel system 
sections propose strategies multi batch allocation stage allocation 
multi batch allocation multi batch allocation idea keeping jobs unallocated initial allocation allocating explorers finish early 
multi batch allocation problem abstracted follows total number job units corresponds graph evaluated 
job unit type non chordal type chordal 
takes time process unit type job type 
explorer finished batch job units takes time send batch job units message explorer 
shall refer batch sent explorer batch additional batch 
goal find proper size batch sum idle time explorers reduced completion job units 
deriving batch sizes assumptions assumption constants pass 
computation time test graph 
complexity checking jn jej graph pass identical number nodes links treated constant 
time manager send additional batch explorer 
additional batch smaller seen batch 
message additional batch short 
messages sent communication channels 
bps parallel computer actual data transfer fast 
consequently consists mainly handshaking time varies slightly message message 
assumption constant pass larger computation time process unit type job involves checking graph computing cross entropy decrement chordal graph 
larger example learning database variables seconds seconds parallel computing environment 
seconds 
assumption constant accurate 
variation clique sizes chordal graph small tends close constant 
variation large tends vary depending specific job unit 
assumption useful approximation deriving simple method determine batch size 
suppose batch allocated explorer 
units 
denote number type units batch assigned explorer denote total number type units batches 
fi percentage type units batch explorer fi percentage type units batches 
losing generality suppose fi max fi alternatively denote fi fi max time taken explorer process batch fi gamma fi fi gamma sum idle time explorers explorer processing batch 
derive gamma fi max gamma gamma fi gamma fi max gamma gamma fi gamma substituting fi gamma nj fi gamma fi max equation gamma fi max gamma gamma nj fi gamma fi max gamma nj fi max gamma fi gamma idle time allocate gamma nj denoted reserved job units additional batches explorers finish batches explorer 
denote percentage type jobs units fi ideally units allocated explorers fully engaged time period units completed time result equation condition expressed fi gamma mt total number additional batches allocate units 
value depends actual size batch including estimation discussed shortly 
equations imply gamma nj fi gamma mt nj fi max gamma fi gamma solving equation expressed fi gamma mt fi max gamma fi fi gamma compute need values fi fi max fi unknown search pass computed 
estimation values discussed values fi fi max estimated assumption assumption difference values fi fi max successive search passes small 
assumption usually holds graphs involved successive passes differ links 
shows values fi fi max search pass learning dataset variables provides empirical justification assumption 
value fi usually varies fi min min fi fi max approximate fi equation average fi avg fi min fi max 
equation estimation errors fi fi max fi smaller larger ideal value 
smaller units reserved resulting additional batches 
hand larger units reserved explorers idle units allocated 
consider estimation numerator equation effect estimation error small fi gamma larger larger max passes search fi fi max values obtained explorers assumption analysis manager collect values fi fi avg fi max previous pass search calculate value follows fi avg gamma fi max fi avg gamma fi gamma determined size batch explorer 
determine size additional batches 
example consider situation illustrated 
suppose histogram depicts computation time batch explorer 
explorer finishes 
size second batch allocated explorer 
conservative batch size gamma effectively assumes explorer explorer finishes moment 
usually explorers finish size allocate explorer 
allocation slightly increases number additional batches 
small additional batches affect efficiency significantly 
adopted conservative batch size 
general remaining job units allocation batch units explorer finishes remaining job units allocation batch units explorer finishes second 
batch size allocated explorer finishes ith place gamma gamma gamma gamma note number remaining units drops gamma jobs allocated unit unit achieve high degree load balancing 
equations modify mgr epr algorithms mgr epr 
manager executes mgr 
pass computes equation sends current graph batch graphs explorer 
explorer executes epr 
checks graph received computes entropy decrement chordal graph 
explorer sends signal manager indicating algorithm mgr input ffih send explorer initialize empty graph set initial values fi fi max fi avg repeat initialize cross entropy decrement dh send current graph graphs explorer repeat receive completion signal explorer send graphs explorer send report signal explorer report signal sent explorer explorer receive dh fi dh dh dh dh dh ffih done false done true done true update fi fi max fi avg done true send halt signal explorer return completion batch 
receiving signal manager computes size additional batch sends batch explorer 
job units left pass manager signal explorer report 
reports collected explorers manager updates relevant search parameters starts pass 
note updated account inaccuracy assumptions 
stage allocation stage allocation fact chordal structure non chordal require significantly different amount computation evaluation difference major source unbalanced load amount processors allocation 
improve load balancing modify job allocation mgr epr allocating jobs stages shown algorithms mgr epr 
stage manager see mgr partitions alternative graphs evenly distributes set explorer 
explorer see epr checks graph received reports manager valid candidates chordal graphs 
complexity checking jn jej graph identical number nodes links computation algorithm epr receive manager repeat receive manager initialize dh repeat receive set graphs manager received graph chordal implied single clique size compute entropy decrement dh dh dh dh dh send completion signal manager report signal received send dh fi manager halt signal received explorers 
second stage manager partitions received graphs evenly distributes set explorer 
explorer computes entropy decrement graph received 
chooses best graph reports entropy decrement manager 
manager collects reported graphs selects best starts pass 
graphs chordal second stage degree load balance mainly depends variability sizes largest cliques 
comparison allocation strategies compared multi batch allocation stage allocation simpler 
needs partition distribute job units twice 
multi batch allocation multiple batches sent explorer resulting higher communication overhead 
example learning database variables explorers average batches sent explorer 
data collection computation involved multi batch allocation expensive 
stage allocation suffers variation amount computation calculating entropy decrements set new links forms new cliques sizes may vary significantly 
hand multi batch allocation resistance variation clique size allocation dynamically adapted actual amount computation batch 
experimental comparison strategies section 
algorithm mgr input ffih send explorer initialize empty graph repeat initialize dh partition graphs differ links sets send set graphs explorer receive set valid graphs explorer partition received graphs sets send set graphs explorer explorer receive dh dh dh dh dh dh ffih done false done true done true send halt signal explorer return algorithm epr receive manager repeat receive current graph set graphs manager initialize dh received graph chordal implied single clique size mark valid send valid graphs manager receive set graphs manager received graph compute entropy decrement dh dh dh dh dh send dh manager halt signal received marginal servers order learn belief network satisfactory accuracy dataset large number cases preferred 
learning data frequently accessed explorer obtain marginal probability distributions marginals subsets variables computing entropy decrements 
distributed memory architecture available local memory processor limited 
dataset proper compression fit local memory processor copy dataset data accessed effectively learning 
special measure taken data access 
obvious solution access data file system 
file access slower memory access 
worse parallel computers limited channels file access making bottleneck 
example computer available file access processors performed single host computer 
achieve efficient data access propose alternative called marginal servers avoid file access completely learning 
idea split dataset subset stored local memory processor 
group say processors task serving explorers computing partial marginals local data 
particular servers connected logically pipeline 
dataset partitioned sets size set may identical discuss shortly 
server stores distinct set data explorer duplicates copy remaining set 
example consider computation marginal binary variables fx yg ae suppose jdj explorer marginal servers 
store tuples explorer server 
table shows possible scenario tuples distributed fx yg 
tuples explorer tuples server tuples server table data storage servers explorer needs compute marginal fx yg sends fx yg servers computes locally potential non normalized distribution 
requested explorer server computes local potential sends server 
server computes local potential adds result server obtain sum sends sum explorer 
explorer adds sum local potential obtain normalizes get marginal 
stage allocation enhanced marginal servers implemented mgr epr svr 
multi batch allocation enhanced accordingly 
algorithm mgr input ffih partition sets send set server broadcast set explorers initialize empty graph repeat initialize dh partition graphs differ links sets send set graphs explorer server receive set valid graphs explorer server partition received graphs sets send set graphs explorer explorer receive dh dh dh dh dh dh ffih done false done true send pass signal server done true send halt signal explorer server return manager executes mgr 
partitions data sets distributes explorers servers starts search process 
stage pass manager generates alternative graphs current graph 
partitions sets distributes explorers servers receives reported valid graphs 
second stage manager partitions valid graphs sets sends set explorer 
explorer executes epr 
stage pass checks received graph reports valid graphs manager 
second stage explorer receives set valid graphs manager 
graph received identifies marginals subset ae necessary computing entropy decrement 
marginal sends request servers computes local potential receives potential server specified sums obtains marginal 
evaluating valid graphs received explorer chooses best graph reports manager 
manager collects reported graphs explorers selects best new current graph sends signal server notify current pass starts pass 
marginal server executes svr 
stage pass server functions explorer testing 
second stage server processes requests repeatedly receives signal current pass 
request marginal subset ae server computes local potential adds potential algorithm epr receive subset data repeat receive set graphs manager initialize dh received graph chordal implied clique size mark valid send valid graphs manager receive set graphs manager received graph set variables involved computing dh send marginal server compute local potential receive potential server compute marginal compute entropy decrement dh dh dh dh dh send dh manager halt signal received predecessor head pipeline sends sum server requesting explorer depending pipeline 
keep processors fully engaged dataset properly partitioned explorers servers 
server serves explorers processing request server times fast local processing requesting explorer 
implies nt time process marginal request server explorer respectively 
jd jd number tuples stored locally server explorer respectively 
expressed jd jn jd coefficients jn computation time identify marginals necessary computing entropy decrement 
nk jd jn jd algorithm mgr partitioned sets jdj jd denoting solving equations obtain jd jdj jdj gamma jd jdj jd jdj ff value experimental environment 
practice jd jd rounded integers jd upper bounded algorithm svr receive subset data repeat receive set graphs manager received graph chordal implied clique size mark valid send valid graphs manager repeat receive set variables explorer compute local potential server head server pipeline receive potential predecessor server sum local potential received potential server tail server pipeline send sum server send sum requesting explorer pass signal received halt signal received available local memory data storage 
example suppose jdj jd jn ff 
jd 
experimental environment parallel algorithms implemented alex series distributed memory mimd computer 
contains root nodes compute nodes may partitioned multiple users time 
root node processor control topology compute nodes 
compute node consists processor mhz computation processor message passing nodes channels node 
data transmission rate mbps simplex mode mbps duplex mode 
processors node share mb memory additional mb memory 
access file system root node host computer 
configure available processors ternary tree reduce length message passing path 
root manager non root nodes explorers servers 
servers cooperate logically pipeline 
tested implementation alarm network randomly generated networks imi pi model 
alarm variables 
im variables contains embedded pi submodel variables 
im variables contains embedded pi submodels variables 
im variables contains embedded pi submodels similar servers marginal host explorers manager computer ternary tree topology im 
im variables contains embedded pi submodel variables 
datasets generated sampling control networks cases respectively 
measure performance programs speed efficiency 
task execution time sequential program parallel program processors 
experimental results demonstrate performance multi batch stage allocation strategies benefit marginal servers 
learned data obtained alarm left pim right dmn learned alarm dataset shown left 
task decomposition parallelism introduce errors learning outcome identical obtained seq learning parameters 
right shows dmn learned im dataset 
nodes labeled form pi submodel im nodes labeled 
learning alarm dataset compared mgr epr multi batch mgr epr stage mgr epr allocations 
dataset compression loaded local memory explorer 
table shows experimental results stage allocations number explorers increases 
allocation stage allocation time speed efficiency time speed efficiency table experimental results stage allocations columns show increases speed increases allocation strategy 
demonstrates parallel algorithms effectively reduce learning time provides positive evidence parallelism alternative tackle computational complexity learning belief networks 
comparing column column seen stage allocation speeds learning improves efficiency allocation 
example explorers speed efficiency allocation stage 
plots speed efficiency strategies comparison 
strategies allocation lowest speed efficiency especially increases 
significant difference multi batch stage allocations 
multi batch allocation slightly better stage allocation 
increase stage performs better multi batch 
overhead multi batch job allocation significant number explorers increases 
multi batch multi batch stage stage speed left efficiency right learning alarm dataset results show gradual decrease efficiency increases 
decrease due allocation overhead 
start pass manager allocates jobs explorers sequence 
explorer idle submission report previous pass receipt batch jobs 
efficiency decrease significant learning performed large pi domains proportion message passing time pass smaller computation time 
illustrated learning results pi domains follows pim pim pim pim time min time min speed efficiency time min speed efficiency time min speed efficiency table experimental results learning pi models table shows experimental results learning models imi triple link lookahead learning imi link lookahead learning im 
column indicates number explorers 
expected speed shown increase third column shows results learning im 
explorers speedup efficiency 
table shows rapid decrease efficiency explorers 
similar trend seen column learning im 
domains relatively small variables respectively complex sparse small pi submodels respectively 
message passing time significant compared computation time cases 
column shows results learning im 
domain contains variables pi submodels control network densely connected 
significantly longer computation time min sequential program 
column shows results learning im 
domain large variables fact contains sub model variables link lookahead needed identify sub model computation expensive 
took sequential program days min 
compared im im speed efficiency learning models better larger number explorers 
note explorers time learn im reduced days day min 
demonstrate marginal servers learning alarm network 
alarm large dataset loaded entirely local memory explorer choose reasons domain size hinder demonstration correctness server method 
second decrease available local memory point large hold alarm dataset 
case data access file system necessary server method 
generality compromised alarm demonstrate effect servers assume dataset loaded local memory explorers 
data access file system took sec sequential program complete learning alarm table shows results learning alarm servers 
number explorers ranges 
data size stored explorer twice large server 
note marginal servers replace slow file access fast memory access efficiency larger shown table 
time speed efficiency table experimental results marginal servers looking distributed memory mimd flynn taxonomy classifies hardware simd mimd 
mimd computers classified shared distributed memory 
discussion extends lessons distributed memory mimd suitability architectures parallel learning belief networks 
incapable true parallelism discuss simd mimd 
computer applies multiple instructions single data stream 
example perform matrix operations gamma simultaneously 
task learning belief networks decomposes naturally evaluation alternative network structures multiple data streams investigated study 
architecture appears unsuitable task 
simd computers consist multiple arithmetic logic units alus supervision single control unit cu 
cu synchronizes alus broadcasting control signals 
alus perform instruction different data fetches memory 
instance cm connect machine processors bit cpu bit memory 
normal instructions executed host computer vector instructions broadcast host computer executed processors 
learning belief networks alternative network structure unique graphical topology requires unique stream instructions evaluation 
simd computers appear suitable learning task decomposed level network structures 
words appears necessary decompose task lower abstraction level 
alternative partition dataset small subsets loaded memory processor 
marginal computed cooperation multiple processors requested host computer 
host computer carry major steps evaluating alternative structure 
essentially sequential learning algorithm seq parallelism applied marginal computation 
degree parallelism reduced compared 
simd computers appear better architecture mimd 
mimd computer processor execute program data 
cooperation processors achieved shared memory message passing distributed memory architectures 
mimd computer shared memory programs data stored memories accessible processors restriction memory accessed processor time 
restriction tends put upper bound number processors effectively incorporated 
shared memory systems efficient small medium number processors 
parallel learning belief networks shared memory mimd computer manager explorer partition processors 
manager generates alternative structures stores memory 
explorer fetch structures evaluation time controlled accessing critical section 
job allocation performed similarly multi batch stage strategies 
hand dataset access bottleneck large number processors want access memory data time 
problem may alleviated duplicating dataset multiple memories 
may practical large datasets due limited total memory 
investigation distributed memory mimd computer analysis believe architecture suited parallel learning belief networks architectures considered 
investigated parallel learning belief networks way tackle computational complexity learning large difficult pi problem domains 
proposed parallel algorithms decompose learning task naturally parallelism introduce errors compared corresponding sequential learning algorithm 
studies multi batch stage job allocations improve efficiency parallel system straightforward allocation strategy 
multi batch effective number processors small stage effective number large 
proposed marginal server configuration replace slow data access file system fast memory access 
allows parallel learning large datasets performed effectively 
implemented algorithms distributed memory mimd computer experimental results confirmed analysis 
study focused learning 
results easily extended learning bayesian networks bns 
known algorithms learning belief networks bns evaluation alternative network structures local computations relative dataset 
results task decomposition job allocation strategies marginal servers applicable learning type belief networks 
extended lessons learned distributed memory mimd system architectures flynn taxonomy 
analysis features architecture features learning belief networks believe distributed memory mimd architecture suited task 
supported research natural sciences engineering research council canada institute robotics intelligent systems networks centres excellence program canada 
appendix graph theoretic terminology undirected graph 
set nodes complete pair nodes adjacent 
set nodes clique complete superset complete 
chord link connects nonadjacent nodes 
chordal cycle length chord 
decomposable markov network dmn set variables pair chordal graph probability distribution node labeled element link signifies direct dependence nodes 
disjoint subsets nodes signifies factorized marginal distributions cliques appendix frequently acronyms bn bayesian network dmn decomposable markov network pdm probabilistic domain model pi pseudo independent mimd multiple instruction multiple data multiple instruction single data multi link lookahead search single instruction single data simd single instruction multiple data slls single link lookahead search beinlich suermondt chavez cooper 
alarm monitoring system case study probabilistic inference techniques belief networks 
technical report ksl knowledge systems lab medical computer science stanford university 
chickering geiger heckerman 
learning bayesian networks methods experimental results 
proc 
th conf 
artificial intelligence statistics pages ft lauderdale 
society ai statistics 
cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
herskovits cooper 
entropy driven system construction probabilistic expert systems database 
proc 
th conf 
uncertainty artificial intelligence pages cambridge 
hu 
learning belief networks pseudo domains 
master thesis university regina 
jensen 
bayesian networks 
ucl press 
kullback leibler 
information sufficiency 
annals mathematical statistics 
lam bacchus 
learning bayesian networks approach mdl principle 
computational intelligence 
lewis el 
parallel computing 
prentice hall 
moldovan 
parallel processing applications systems 
morgan kaufman 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
spirtes glymour 
algorithm fast recovery sparse causal graphs 
social science computer review 
xiang 
understanding pseudo independent domains 
poster proc 
th inter 
symposium methodologies intelligent systems oct 
xiang wong cercone 
critical remarks single link search learning belief networks 
proc 
th conf 
uncertainty artificial intelligence pages portland 
xiang wong cercone 
microscopic study search learning decomposable markov networks 
machine learning 

