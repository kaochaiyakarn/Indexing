speech communication inference variable length linguistic acoustic units multigrams sabine deligne frederic bimbot enst dept signal cnrs ura rue paris cedex france efficiency pattern recognition algorithms highly conditioned proper definition patterns assumed structure data 
multigram model provides statistical tool retrieve sequential variable length regularities streams data 
general formulation model applicable single multiple parallel strings data having discrete continuous values 
model assessed derive inventory variable length sequences letters text data spaces words removed 
turns sequences letters inferred fully unsupervised procedure clearly relate morphological structure text 
model infer set variable length acoustic units directly speech data 
speech files containing examples acoustic units provided order illustrate consistency auditory point view 
report experiments acoustically defined units continuous speech recognition 
resume des algorithmes de reconnaissance des formes est par la definition adequate des formes les 
le un statistique de des de variable dans des flux de 
dans cet article nous une formulation de ce applicable des flux ou multiples de des ou continues 
la evaluation du extraction partir de un repertoire de sequences de de variable dans tous les entre mots ont ete 
il que les sequences de de cette procedure non sont la structure du 
le est utilise pour partir des de parole un ensemble unites de variable 
des audio des unites sont avec cet article de leur un point de vue 
cet article rend des experiences ces unites pour la reconnaissance de parole continue 
elsevier science corresponding author 
mail itl atr jp 
speech files available 
see elsevier 
current affiliation atr interpreting telecommunication research laboratories dep 
seika cho soraku gun kyoto japan 
mail bimbot sig enst fr 
elsevier science rights reserved 
pii deligne communication 
motivations field speech natural language processing domains efficiency pattern recognition algorithms highly conditioned proper definition patterns assumed structure data 
pointed moore nature units underlying speech language things don know speech moore consequently set units speculated risk possible mismatch due lack priori knowledge learned examples data driven approaches 
case risk inadequacy reduced provided large representative set data samples reliable inference procedure available 
accordingly increasing effort dedicated learn structure speech language data acoustic level applications coding chou recognition lee phonetic level text speech synthesis sagisaka lexical level language modelling glass ries suhm waibel context expose general formulation multigram model aims retrieving sequential variable length regularities streams observations 
model especially appropriate description linguistic data subject constraints redundancy give rise recurrent variable length sequences acoustic phonetic syntactic levels 
efficiency multigram model assessed automatic retrieval inventory variable length units text data directly speech data 
section theoretical background original multigram model extension joint multigram model joint streams observations 
section model applied unsupervised segmentation text data 
section dedicated derivation inventory non uniform acoustic units speech data 
consistency acoustic multigrams assessed auditory point view 
continuous speech recognition experiments reported evaluate consistency opposed linguistically defined units 

multigram framework 
definition multigram model originally developed order model variable length regularities streams symbols bimbot deligne bimbot bimbot name multigram opposed grams dependencies supposed fixed length 
multigram model understood production model deligne source emits string units called multigrams drawn limited set multigram gives rise variable length sequence elementary observations 
assume practice observable output process string observations resulting concatenation sequences 
observed string goal retrieve set distinct underlying multigrams identify observation sequences originating common multigram 
locating multigrams string involves finding segmentation illustrated fig 


statistical formulation traditional approaches data description full set patterns underlying data assumed known 
pattern parameters estimated maximizing likelihood data pre defined fixed set 
hand multigram case sets patterns meeting requirements sequential dependency assumed model definition possible candidates account observed data 
hood data set weighted likelihood set 
consequently optimal set multigrams dez fig 

inference underlying multigram structure observed string data 
deligne communication rived maximizing jointly likelihood data set max oz iii 
likelihood data oz measures data fits set second term eq 
evaluates likelihood set 
priori distribution possible sets unknown information theory 
likelihood related inverse number bits required fully specify set inversely related description length result selecting set eq 
equivalent searching optimal trade adequacy observed string complexity measured number bits 
principle related minimum description length mdl approaches rissanen expected advantage reduce risk overlearning noticed ml estimations 
ml criterion focus placed exclusively best fit data mdl criterion balanced lesser complexity model 
generally complex model better fit data worse tion capability fitzgerald specifying multigram probability pz requires minimum pz bits minimization complexity obtained discarding multigrams low probability 
concerns data likelihood computed oz zz ii 
express zz assumptions dependencies sequences deligne theoretical formulations model provided considering various kinds dependencies multigrams 
multigrams conventional approaches set usually fixed predefined set linguistic units 
likelihood value viewed constant 
assumed independent likelihood computed cs zz ps pz ii tt denotes observation sequence corre sponding multigram rank cs number sequences equivalently number multigrams model fully described prior distribution multigrams pz set condi tional distribution functions char variability sequences observed multigram maximization data likelihood expressed eq 
tends favor inference highly recurrent multigrams having low variability 
set segmentation string units assumed underly determined max zz 
deri ation estimates 
em optimization optimization eq 
trivial solu tion 
set searched em expectation maximization procedure dempster data likelihood complexity model alternately optimized iteration 
procedure begins initial set 
associated parameters ties iteration step process kq 
modification set ii reduce complexity 
heuristic applied consists removing multigrams having low prior probability 
reestimation parameters remaining multigrams maximize data likelihood kq 
oz ml reestimation formula parameters derived section 

maximization likelihood reestimation formula multigram parameters obtained ml estimation incomplete data dempster observed data string unknown data underlying structure qk kq deligne communication auxiliary function computed likelihoods iterations kq qk kq log kq 
shown dempster kq 
qk kq qk equality parameters iterations kq identical 
set parameters maximizes iteration kq leads increase corpus likelihood 
reestimation formula parameters iteration kq derived maximizing auxiliary function qk kq 
eq 
log likelihood observation string log cs log pz ps tt ts grouping identical multigrams rewrites log cs log ps ii ts number distinct multigrams kronecker variable th multigram 
number occurrences ii log log pz ii cs log ps ts replacing log likelihood iteration kq eq 
expression eq 
allows rewrite function sum func rest section short 
notation tions depend respectively priors posteriori distribution functions qk kq kq kq qk kq zo log kq 
qk kq zo cs kq 
log 
ts reestimation formula priors posteriori parameters derived independently maximizing respectively 
estimation priors 
reestimation formula priors iteration kq obtained maximizing qk kq respect kq 
set pz constraints kq 
kq 
pz 
ii kq 
function pz attains global maximum single point zo kq 
zo js equals cs sim js kq 
cs zo eq 
shows estimate pz merely weighted average number occurrences possible underlying structure iteration improves model sense increasing data likelihood eventually con critical point possibly local maximum 
deligne communication particular case posteriori distribution functions dirac functions 
section dedicated unsupervised acquisition inventory sequences letters text data observation sequences null variability respect underlying multigrams case distribution function dirac function zz ps consequently segmentation likelihood value zero string ps 
eq 
simplifies zo kq 
cs zo ss 
estimation posteriori distributions 
case posteriori distribution functions dirac functions parametric distributions assumed 
instance section hidden markov model hmm gaussian distributions model state emissions associated multigram reestimation formula parameters derived maximizing func tion results formula possible contribute reestimation parameters 
decision oriented way doing consists parameters exclusively underlying structure iteration case parametric distribution characterizes variability multigram reestimated observation sequences identified originating variability modeled hmm instance simply consists applying baum welch algorithm 
implementation 
decision oriented implementation setting zero likelihood functions highest value eq 
kq 
cs prior probability simply reestimated relative frequency lying segmentation iteration determined means viterbi procedure 

forward backward implementation reestimation formula implemented forward backward algorithm 
relies forward variable backward variable denote substring ranks variable defined likelihood string truncated rank sl 
segmentation ends sequence 
length 
sequences assumed independent variable recursively calculated 
indicated box 
box 
recursion formula 
ppp ls zt 
similarly backward variable defined likelihood ty observations sl tq 
recursively computed 
qn indicated box 
box recursion formula variable po ppp ls tq 
parameter reestimation formula eq 
rewrites function variables iteration kq 
tn zk zk zb ts ls zk ts wx ifo sz zt 
deligne communication fig 

inference underlying joint multigram structure parallel strings data priors initialized relative frequencies training set multigrams defined 
iteratively reestimated eq 
training set likelihood significantly increase fixed number iterations 

extension joint multigram model 
definition joint multigram model deligne extension multigram model defined section multigram unit emitted source gives rise parallel sequences observations single sequence 
section assume translates pair observation sequences concatenation results strings zt refers joint segmentation strings projection respectively denoted observed strings goal retrieve set underlying multigrams identify observation sequences originating common multigram illustrated fig 

sequences matched common pair may different length finding joint segmentation equivalent finding mapping strings 
procedure deriving set joint multigrams formally section single multigrams max iii set structure assumed underlie max zz likelihood computed zz ii cs ii tt ts reestimation formula parameters directly derived analogy single multigram case 
joint multigrams dirac distribution functions considered defines mapping sequence formed symbols alphabet sequence formed symbols alphabet prior probability pz represents proba bility occur 

application decoding occurrence probabilities estimated training corpus model perform automatic decoding test input string output string sequence sequence transcription process 
transcription stated standard maximum posteriori decoding prob lem consisting finding string stream assuming joint segmentation accounts 
short notation deligne communication fig 

viterbi search segmentation sequences letters sentence 
likelihood maximize approximated likelihood bayes rule eq 
rewritten vv measures likelihood association best joint segmentation 
computed ps zt conditional probabilities deduced probabilities ps estimated learning phase 
likelihood output stream estimated language model lead ing alternative decoding vv instance computed bigram model estimates ss 
deduced frequency counts segmented training corpus iteration mapping process 

unsupervised segmentation text data assessment multigram approach applied model language level 
due lexical morphological constraints combinations letters text equally frequent 
words hypothesis held strings orthographic characters result concatenation limited set variable length se quences letters 
previous bimbot deligne phonetic transcription utterances observed string multigram model expected capture language 
current study illustrate ability multigram model segmenting text data boundaries words unknown 
task studied field cognitive sciences help understand cues allow children identify words flow speech 
cartwright brent cartwright brent brent cartwright instance propose segment text data mdl approach 
apply heuristic aims minimizing total number bits required describe data inventory sequences derived segmentation 
de marcken uses multigram model perform similar task 
derived set non uniform phonetic sequences intended serve set units concatenative synthesis 
deligne communication fig 

segmentation iterations multigram viterbi learning procedure thresholds original sentence delight law lord law day night sequences modified indicated bold characters application database obtained removing spaces words text data instance sentence extracted king james version bible experiments run 
set sentences forms database letters drawn letter alphabet covers vocabulary words 
application multigram procedure aims retrieving recurrent patterns consisting variable length sequences letters 
part posteriori distribution sequences assumed dirac function correspondence multigrams sequences letters 
case multigram model size multigram limited letters viterbi learning procedure initialization probabilities initialized relative frequencies combinations text corrupted letters deleted inserted substituted assumption longer hold 
case variability sequences letters characterized discrete density function 
letters occurring pre specified number times text data 
iteration probabilities reestimated relative frequencies sequences letters occur pre specified number times segmentation text data 
segmentation searched viterbi algorithm best path lattice shown fig 

iterations segmentation text data evolves recurrent combinations letters tend emerge increased number occurrences 
illustration segmentations example delight law lord law day night shown fig 
initialization iteration converged segmentation sentence reached 
experiment combinations letters occurring times initialization iterations discarded 
note threshold values final segmentation obtained second iteration 
segmentation function words occur quite frequently english grouped single sequence 
word delight parsed size limited deligne communication letters 
decomposed de light independently word delight correspond english 
similarly forms sequence probably allows isolate sequence preposition highly frequent combination letters 
experiments convergence set letters reached iterations represents minutes sparc workstation 
experiment sequences limited letters threshold values inventory sequences letters derived final segmentation set distinct sequences twice number distinct lexical items 
size final inventory sequences depends values thresholds pruning lesser extent shown fig 
maximal length sequence 
curve fig 
plots pair zz threshold values 
number sequences final inventory maximum number letters sequence increased 
shows threshold values size inventory tends stable soon maximum length sequence exceeds letters 
pruning low threshold 
values size inventory keep increasing longer sequences allowed 
relevant fact lexical items english letters longer sequences letters frequent retrieved 
instance sequences letters allowed thresh old values final inventory contains sequences letters 
sequences listed fig 
usually correspond long words merging frequently occurring short words 
follow quantitative evaluation quality decompositions provided arbitrary decide instance word decomposed components admissible frequently occurring words merged single unit 
examination segmentation resulting iteration see fig 

size final inventory sequences derived corpus maximum number letters sequence increased various threshold values 

deligne communication example fig 
tends show regularities detected model strongly correlated morphological structure sentences 

acquisition subword units speech data 
deri ation acoustic units section attempt generalise multigram model description acoustic speech data text data 
goal derive inventory variable length acoustic units obtained observed stream continuous valued spectral vectors 
purpose speech signal parametrized temporal decomposition atal bimbot td applied resulting string vectors order reduce variability 
td targets quantized ergodic hmm state sequence derive initial set acoustic multigrams modeled fig 

sequences letters inventory obtained multigram model threshold values 
deligne communication fig 

segmentation sentences iterations learning multigram model threshold values hmm 
iterative process refine estimation initial units 

acoustic obser ations td model spectral evolution describes speech segment linear combination limited set vectors called targets 
temporal contribution target expressed inter function see fig 
spectral characterizations acoustic events target vectors expected show variability original frames multigrams searched variable length sequences td target vectors 

deri ation procedure characterization multigram observations continuous values models defined account variability sequences dirac functions assumed section 
purpose multigram associated hidden markov model hmm consequently inference set multigrams viewed inference set hmm 
note altogether number distinct hmm number states need inferred 
definition initial set hmm see fig 
deligne communication fig 

graphic illustration temporal decomposition td target vector quantized replaced symbol called acoustic symbol denoting quantization class 
resulting string acoustic sym combination nn symbols oc max pre specified number times threshold leads define left right state hmm 
state parameters initialized mean covariance values corresponding quantization class 
reduction set hmm underlying structure iteration iteration kq consists discarding hmm occurring pre specified number times parameters remaining models priori probability reestimated relative frequency hmm parameters applying baum welch algorithm 
iterations stopped set units stable 
note major drawback inference process offer possibility create new hmm improve accuracy acoustic modelling dramatically increasing complexity 

experiments database 
experimental protocol 
inventory acoustic multigrams derived applying procedure described section database min continuous speech 
database consists sailing weather forecasts uttered french male professional speaker 
speech digitally recorded radio khz coded bits linear scale example sentence database listened signal td target vectors computed ms frames parameters 
number target vectors number frames average number targets phoneme 
derivation procedure applied distinct acoustic symbols quantize target vectors 
hmm defined combination acoustic symbols occurring times 
consequently hmm states 
iteration hmm having number occurrences discarded 
means set acoustic multigrams considered stabilized units occur times speech database 
fig 

definition initial set hmm 
deligne communication fig 

evolution iteration number hmm values normalized equal initialization log likelihood values training set prior log likelihood posteriori log likelihood global log likelihood 
resulting acoustic units 
analysis inventory acoustic units resulting iteration derivation process curves plotted fig 

upper curve represents evolution number models starting initialization stage iteration iterations stable zz 
iteration initialization set contains hmm reduces stabilization 
checked average frequency occurrence model increases start ing iteration reach ceiling iteration fig 
priori posteriori log likelihood values speech data plotted computed respectively pz ps tit tz iteration priori log likelihood measures frequent derived acoustic units posteriori log likelihood evaluates corresponding accuracy speech data description 
log likelihood curves keep increasing iteration concatenation models underlying data predictable average model concatenation gives better account variability data especially variability speech segments respect underlying acoustic units keeps decreasing speech data partitioned reduced number models 
set speech files attached article order allow reader evaluate perceptual subjective point view consistency acoustic units inventory 
speech file contains examples speech segments corresponding specific acoustic unit inventory acoustic units selected basis priori probability occurrence acoustic units speech files provided correspond recurrent hmm having states see elsevier 
deligne communication temporal information needed extract segments speech signal provided td interpolation functions 
speech segments tend show acoustic units longest frequently occurring ones perceptually consistent 
speech files linked provide examples acoustic units corresponding word unit signal rf rsr corresponds french word force subword unit signal ra iter occurs word concatenation breath noise small word signal ra var article un noun vent merged concatenation subword units overlapping words signal overlaps words nord 

speech recognition units 
acoustic recognition units continuous speech recognition performance affected choice appropriate unit acoustic modelling 
traditional approaches recognition units chosen priori depending phonetic identity located speech signal database estimation models 
alternative linguistically defined units effort put definition recognition units acoustic criterion instance lee purpose acoustic criterion linguistic criterion define set units acoustically consistent possibly reliable speech recognition 
sub section propose evaluate reliability acoustic inventory derived section recognition task 
usually case recognition units acoustically derived number length models set advance 
sequence frames extracted td target vector comprised crossing points interpolation function interpolation functions adjacent target vectors 
instance lee propose partition speech data pre specified number hmm exactly states 
hand maximal number states hmm pre specified 
number distinct models length result derivation process 
acoustic inventory usable recognition framework phonetic content unit determined 
lee report experiments boundaries words speech signal known 
lattice derived combinations models observed word training database 
controlled number multiword units added lexicon acoustic units overlap word boundaries 
propose find probabilistic mapping variable length acoustic sequences variable length sequences phonemes fully unsupervised way prior knowledge word boundaries speech signal 
approach requires recognition process reformulated proposed section 
section explains mapping acoustic units sequences phonemes performed decoding test data 
experiments continuous speech recognition corpus reported section 

reformulation recognition process recognition task usually formulated determination linguistic string maxi te mum likelihood acoustic string te max lo max te te te ll set defines level intermediate representation acoustic linguistic levels max te te reduce computational costs assumption exists single optimal intermediate representation accounts te data likelihood searched indepen te deligne communication dently decoding test string te step procedure firstly acoustic decoding retrieve underlying structure te secondly linguistic decoding retrieve te phonetic string matches te te max te te stage decoding linguistic component evaluate likelihood just usually done classical recognition system max te te 
mapping decoding 
posteriori mapping phonetic se quences 
compute eq 
te eq 
necessary define te probabilistic model models mapping intermediate representation linguistic representation estimate parameters model 
experiments mapping relies joint multigram model defined section 
hmm resulting iteration inference process produce transcription target vectors acoustic symbols 
obtained acoustic sequence symbols corresponding states visited viterbi recognition procedure 
transcription acoustic symbols aligned transcription phonetic symbols probabilistic mapping provided joint multigram model 
procedure jointly parses strings matching variable length sequences acoustic symbols variable length sequences phonemes 
results dictionary pairs acoustic phonetic sequences assigned probability occurrence 
mapping performed regardless boundaries words utterances known 

acoustic linguistic decoding 
acoustic decoding transcription acoustic symbols test stream td target vectors retrieved succession visited states hmm issuing derivation pro table speech database training set test set speaker population male speaker vocabulary words words nb sentences duration min 
nb words nb phonemes nb frames nb td targets word bigram perplexity vocabulary distinct words vocabulary occurrences 
cess 
linguistic decoding consists producing string phonemes sequence sequence zz 

transcription process eq 
eq 
relying dictionary joint multigrams 
note concatenation phonetic sequences output process may result string necessarily correspond succession lexical items 

recognition experiments database 
experimental protocol 
training test database 
recognition units derived protocol defined section min continuous speech database 
additional set weather forecasts uttered speaker corresponding total min speech test database 
details regarding sets training testing table 
acoustic component multigrams 
protocol defined section applied distinct acoustic symbols quantize td targets 
process deriving set hmm stopped iterations empirically noticed roughly stabilized set 
transcription acoustic symbols sequences symbols paired sequences phonemes joint multigram mapping procedure 
deligne communication table comparison systems multigrams number distinct acoustic symbols number hmm convergence average number acoustic sequences sequence phonemes phonetic accuracy language model word accuracy bigram model acoustic component linguistic units 
systems recognition units phonemes triphones words build td target vectors training observations 
acoustic components comprise respectively hmm modelling french phonemes hmm modelling triphones occurring times training utterances models phoneme system added set ensure exhaustive phonetic coverage hmm modelling words training vocabulary 
state hmm modelling silence state hmm modelling breath noise added system 
due td target vectors state emissions hmm systems state phoneme 
database segmented phonemes words initial models aligned training stream target vectors proportionally number states 
step embedded reestimation model parameters step re alignment models training set alternately repeated converged alignment reached 
linguistic component 
apart training acoustic components systems bigram language model estimated assigns uniform probability distribution unknown combinations words 
perplexity values computed model training test sets respectively 
bigrams words usable multigram recognition frame number td target vectors number initial frames average number target vectors phoneme approximately equals 
zz 
decoding criterion eq 
concatenation decoded phonetic sequences comply constraints 
step decoding process set possible candidates restricted sequences phonemes concatenation preserves resulting string 
case system phonemes triphones hmm concatenated form models words 

results 
number quantization classes 
results obtained distinct acoustic symbols reported table 
number quantization classes higher number distinct acoustic units increases 
interpreted partitioning speech data average number acoustic sequences matched common phonetic sequence joint multigram mapping process increases 
recognition scores deteriorate accordingly word accuracy number quantization classes increases 
speaker database limited vocabulary clustering relatively small number classes sufficient 
temporal decomposition 
experiments conducted conventional frames evaluate suitable spectral representation td inference acoustic units 
quantization frames td targets produced highly unstable succession acoustic symbols repetitions symbols instance initialize set hmm initial set hmm defined string quantized td targets inference process pursued initial frames state emissions 
experiments results increased number deligne communication table comparison systems multigrams acoustic symbols linguistic units acoustic modeling phonemes triphones words multigrams number hmm convergence average frequency model training set average number states model phonetic accuracy language model word accuracy bigram model insertions distinct acoustic symbols phonetic accuracy test set deteriorates percentage phonemes correctly identified remains unchanged comparison systems 
results obtained systems multigram system acoustic symbols table 
language model recognition scores evaluate reliability acoustic modeling system multigram case reliability probabilistic mapping sequences acoustic symbols phonemes 
phonetic accuracy obtained multigram units intermediate tween scores obtained triphones words motivation inferring acoustic multigrams find set units altogether recurrent variable possible 
assuming average variability recognition units directly related average length words better comply requirements multigrams derived experiments 
average length words measured average number states model average frequency training data higher multigram units 
true multigrams respect triphones tending corre experiments model interpolation tied estimation techniques warrant reliability estimates models triphones 
course related number distinct units speech data partitioned context dependent phonemes length context independent phonemes provide refined partitioning data 
longer units equally frequent units 
scores tend ranked accordingly 
bigram language model compensates relative lack reliability phoneme triphone models case outperform models trained words word accuracies respectively 
integration language model acoustic component multigrams quite profitable allow exceed word accuracy 
fact conversely multigrams language model give better results phonemes triphones interpretations may drawn 
explanation linguistic information encoded models acoustic multigrams case phoneme triphone models 
models having states multigram approach captures language lesser extent flexible way word models 
explanation complete word models combined language model perform triphones 
relative lack efficiency linguistic component multigram case illustrates difficulty integrating top information bottom derived models multigram models 
way multigram level linguistic level interfaced experiments far optimal particularly levels optimised separately 
identify possible majors tracks improvement 
firstly decoding performed eq 
eq 
sentence words output recognizer determined summing likelihood deligne communication values multiple acoustic transcriptions derived single best transcription te secondly joint multigram mapping associates closed list phonetic sequences sequence acoustic symbols lacks flexibility especially lexical constraints applied 
instance may happen sentence retrieved sequences mapped acoustic sequence non zero probability 
flexible approach instance production model phonetic sequences associated acoustic sequence closed list 

multigram model described intended provide new framework description single multiple streams data having discrete continuous values concatenation variable length sequences 
initial assessment model segment text data boundaries words known 
application illustrates ability discrete version model extract strongest sequential dependencies underlying string symbols 
general continuous version model explore possibility deriving inventory acoustically consistent units speech data 
listening units perceptual point view gives hint consistency second stage acoustic units evaluated speech recognition units 
purpose acoustic units matched variable length phonetic sequences fully unsupervised way probabilistic mapping procedure joint multigram model 
experiments acoustic multigrams outperform context independent context dependent phone models phoneme recognition efficient word models 
word recognition performed bigram language model conventional units outperform time multigram units 
main weakness new approach certainly non optimal mapping acoustic units linguistic level 
effort put question representation lexical entries posteriori acoustic units 
understood step data driven acoustic modelling need deciding priori nature recognition units eliminated 
automatic inference set acoustic units resulting compromise fit data yielding reliable robust estimates opens large field perspectives 
approach sub optimal ways 
improved procedures need imagined order attain accurate reliable set inferred acoustic speech units model correspondence linguistic level 
multigrams appear relevant starting point reach objective 
grateful stephanie nave development systems section 
atal 
efficient coding lpc parameters temporal decomposition 
proc 
internat 
conf 
acoust 
speech signal process 
ostendorf sagisaka paliwal 
design speech recognition system acoustically derived segmental units 
proc 
internat 
conf 
acoust 
speech signal process bimbot 
synthese de la parole des segments aux avec utilisation de la decomposition 
phd thesis telecom paris rue paris france 
bimbot deligne 
unsupervised decomposition phoneme strings variable length sequences multigrams 
proc 
august 
bimbot pieraccini levin atal 
de sequences horizon variable 
proc 
france june 
bimbot pieraccini levin atal 
variablelength sequence modeling multigrams 
ieee signal proc 
lett 
brent cartwright 
distributional regularity useful segmentation 
cognition 
cartwright brent 
segmenting speech lexicon roles speech 
proc 
st meeting association computational linguistics 
deligne communication chou 
variable dimension vector quantization linear predictive coefficients speech 
proc 
internat 
conf 
acoust 
speech signal process 
vol 
pp 

de marcken 
unsupervised acquisition lexicon continuous speech 
technical report mit artificial intelligence lab 
center biological computational learning dep 
brain cognitive sciences 
deligne 
de sequences de variables application au traitement du langage de la parole 
phd thesis telecom paris rue paris france 
deligne bimbot 
language modeling variable length sequences theoretical formulation evaluation multigrams 
proc 
internat 
conf 
acoust 
speech signal process 
deligne bimbot 
variable length sequence matching phonetic transcription joint multigrams 
proc 
eurospeech vol 
pp 

deligne bimbot 
introducing statistical dependencies structural constraints variable length se quence models 
de la eds 
grammatical inference learning syntax sentences lecture notes artificial intelligence 
springer berlin pp 

dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal statis 
soc 

lee juang 
segment model approach speech recognition 
proc 
internat 
conf 
acoust 
speech signal process pp 

lee juang rabiner 
word recognition word subword models 
proc 
internat 
conf 
acoust 
speech signal process pp 

glass 
empirical acquisition word phrase classes atis domain 
proc 
eurospeech 
moore 
things don know speech 
niemann eds 
progress prospects speech research technology 
infix germany 
ries wang 
improved language modeling unsupervised acquisition structure 
proc 
internat 
conf 
acoust 
speech signal process 
rissanen 
stochastic complexity statistical inquiry chapter 
world scientific series computer science vol 
singapore pp 

fitzgerald 
numerical bayesian methods applied signal processing chapter springer new york pp 

sagisaka 
objective optimization algorithms text speech synthesis chapter 
paliwal eds 
speech coding synthesis elsevier amsterdam pp 

suhm waibel 
better language models spontaneous speech 
proc 
icslp 
