journal artificial intelligence research submitted published ai access foundation morgan kaufmann publishers 
rights reserved 
improved heterogeneous distance functions randall wilson randy axon cs byu edu tony martinez martinez cs byu edu computer science department brigham young university provo ut usa instance learning techniques typically handle continuous linear input values handle nominal input attributes appropriately 
value difference metric vdm designed find reasonable distance values nominal attribute values largely ignores continuous attributes requiring discretization map continuous values nominal values 
proposes new heterogeneous distance functions called heterogeneous value difference metric hvdm interpolated value difference metric ivdm windowed value difference metric wvdm 
new distance functions designed handle applications nominal attributes continuous attributes 
experiments applications new distance metrics achieve higher classification accuracy average previous distance functions datasets nominal continuous attributes 

instance learning ibl aha kibler albert aha wilson martinez wettschereck aha mohri domingos paradigm learning algorithms typically store available training examples instances training set learning 
instance input vector output class generalization systems distance function determine close new input vector stored instance nearest instance instances predict output class classify 
instance learning algorithms referred nearest neighbor techniques cover hart hart dasarathy memorybased reasoning methods stanfill waltz cost salzberg overlap significantly instance paradigm 
algorithms success wide variety applications real world classification tasks 
neural network models distance functions including radial basis function networks broomhead lowe renals rohwer wasserman networks hecht nielsen art carpenter grossberg selforganizing maps kohonen competitive learning rumelhart mcclelland 
distance functions fields machine learning neural networks including statistics atkeson moore schaal pattern recognition diday michalski stepp diday cognitive psychology tversky nosofsky 
wilson martinez distance functions proposed decide instance closest input vector michalski stepp diday diday 
metrics numerical attributes appropriately handle nominal discrete unordered attributes 
value difference metric vdm stanfill waltz introduced define appropriate distance function nominal called symbolic attributes 
modified value difference metric uses different weighting scheme vdm pebls system cost salzberg 
distance metrics nominal domains handle continuous attributes directly 
rely discretization lebowitz schlimmer degrade generalization accuracy ventura martinez 
real world applications nominal linear attributes including example half datasets uci machine learning database repository merz murphy 
introduces new distance functions appropriate previous functions applications nominal continuous attributes 
new distance functions incorporated learning systems areas study augmented weighting schemes wettschereck aha mohri atkeson moore schaal enhancements system provides 
choice distance function influences bias learning algorithm 
bias rule method causes algorithm choose generalized output mitchell 
learning algorithm bias order generalize shown learning algorithm generalize accurately summed possible problems schaffer information problem training data available 
follows distance function strictly better terms generalization ability considering possible problems equal probability 
higher probability class problems occurring learning algorithms generalize accurately wolpert 
better summed problems problems perform occur 
sense algorithm distance function improvement higher probability generalization better matched kinds problems occur 
learning algorithms bias simplicity mitchell wolpert generalize bias appropriate meaning leads generalization accuracy wide variety real world applications meaning simplicity varies depending representational language learning algorithm 
biases decisions basis additional domain knowledge particular problem mitchell improve generalization 
light distance functions appropriate comparison average yield improved generalization accuracy collection applications 
results theoretically limited set datasets hope datasets representative problems interest occur frequently real world distance functions useful cases especially involving continuous nominal input attributes 
section provides background information distance functions previously 
section improved heterogeneous distance functions introduces distance function combines euclidean distance vdm handle continuous nominal attributes 
sections extensions value difference metric allow direct continuous attributes 
section introduces interpolated value difference metric ivdm uses interpolation probabilities avoid problems related discretization 
section presents windowed value difference metric wvdm uses detailed probability density function similar interpolation process 
section presents empirical results comparing commonly distance functions new functions 
results obtained distance functions instance learning system datasets 
results indicate new heterogeneous distance functions appropriate previously functions datasets nominal linear attributes achieve higher average generalization accuracy datasets 
section discusses related section provides research directions 

previous distance functions mentioned learning systems depend distance function successful 
variety distance functions available uses including mahalanobis smith chebychev quadratic correlation chi square distance metrics michalski stepp diday diday context similarity measure contrast model tversky hyperrectangle distance functions salzberg domingos 
functions defined 
distance functions proposed far commonly euclidean distance function defined input vectors typically stored instance input vector classified number input variables attributes application 
square root computed practice closest instance closest regardless square root taken 
alternative function city block manhattan distance function requires computation defined euclidean manhattan distance functions equivalent distance function respectively 
wilson martinez 
equations selected distance functions 
vectors attribute values 
max ji sum size size sign sign euclidean manhattan city block chebychev quadratic mahalanobis correlation chi square kendall rank correlation problem specific positive definite weight matrix covariance matrix vector values attribute occuring training set instances average value attribute occuring training set 
sign respectively 
sum sum values attribute occuring training set size sum values vector det 
normalization weakness basic euclidean distance function input attributes relatively large range attributes 
example application just attributes values values influence distance function usually influence 
distances normalized dividing distance attribute range maximum minimum attribute distance attribute approximate range 
order avoid outliers common divide standard deviation range trim range removing highest lowest percent data consideration defining range 
possible map value outside range minimum maximum value avoid normalized values outside range 
domain knowledge decide method appropriate 
related idea normalization attribute weights weighting improved heterogeneous distance functions schemes 
learning systems distance functions incorporate various weighting schemes distance calculations wettschereck aha mohri atkeson moore schaal 
improvements independent schemes various weighting schemes enhancements instance pruning techniques conjunction new distance functions 

attribute types distance functions shown including euclidean distance appropriately handle non continuous input attributes 
attribute linear nominal linear attribute continuous discrete 
continuous continuously valued attribute uses real values mass planet velocity object 
linear discrete integer attribute discrete set linear values number children 
argued value stored computer discrete level 
reason continuous attributes treated differently different values value may appear rarely particular application 
causes problems algorithms vdm described section depend testing values equality continuous values rarely equal may quite close 
nominal symbolic attribute discrete attribute values necessarily linear order 
example variable representing color values red green blue brown black white represented integers respectively 
linear distance measurement values little sense case 

heterogeneous euclidean overlap metric way handle applications continuous nominal attributes heterogeneous distance function uses different attribute distance functions different kinds attributes 
approach overlap metric nominal attributes normalized euclidean distance linear attributes 
purposes comparison testing define heterogeneous distance function similar ib ib ib aha kibler albert aha giraud carrier martinez 
function defines distance values attribute unknown overlap nominal rn diff unknown attribute values handled returning attribute distance maximal distance attribute values unknown 
function overlap difference rn diff defined overlap wilson martinez rn diff range value range normalize attributes defined range max min max min maximum minimum values respectively observed training set attribute means possible new input vector value outside range produce difference value greater 
cases rare occur large difference may acceptable anyway 
normalization serves scale attribute point differences 
definition returns value typically range attribute nominal linear 
distance possibly heterogeneous input vectors heterogeneous euclidean overlap metric function distance function removes effects arbitrary ordering nominal values overly simplistic approach handling nominal attributes fails additional information provided nominal attribute values aid generalization 

value difference metric vdm value difference metric vdm introduced stanfill waltz provide appropriate distance function nominal attributes 
simplified version vdm weighting schemes defines distance values attribute vdm number instances training set value attribute number instances value attribute output class number output classes problem domain constant usually conditional probability output class attribute value 
seen defined sum classes improved heterogeneous distance functions sum classes fixed value distance measure vdm values considered closer similar classifications similar correlations output classes regardless order values may 
fact linear discrete attributes values remapped randomly changing resultant distance measurements 
example attribute color values red green blue application identify object apple red green considered closer red blue similar correlations output class apple 
original vdm algorithm stanfill waltz feature weights included equations variants vdm cost salzberg domingos alternate weighting schemes 
discussed earlier new distance functions independent schemes cases similar enhancements 
problem formulas define done value appears new input vector appeared training set 
attribute value instance training set sum classes 
cases undefined 
nominal attributes way know probability value inherent ordering values 
assign default value cases possible number output classes sum 
distance function directly continuous attributes values potentially unique case value value value addition new vectors unique values resulting division zero problem 
value substituted resulting distance measurement nearly useless 
values unique different values continuous attribute statistical sample small value distance measure untrustworthy 
problems inappropriate vdm directly continuous attributes 

discretization approach problem vdm continuous attributes discretization lebowitz schlimmer ventura 
models vdm variants cost salzberg mohri tanaka discretized continuous attributes somewhat arbitrary number discrete ranges treated values nominal discrete unordered values 
method advantage generating large statistical sample nominal value values significance 
discretization lose important information available continuous values 
example values discretized range considered equal opposite ends range 
effects reduce generalization accuracy ventura martinez 
propose new alternatives sections 
section presents heterogeneous distance function uses euclidean distance linear attributes vdm nominal attributes 
method requires careful attention wilson martinez problem normalization nominal linear attributes regularly weight 
sections distance functions interpolated value difference metric ivdm windowed value difference metric wvdm discretization collect statistics determine values continuous values occurring training set instances retain continuous values 
generalization value continuous value interpolated values ivdm wvdm essentially different techniques doing nonparametric probability density estimation tapia thompson determine values class 
generic version vdm algorithm called discretized value difference metric dvdm comparisons new algorithms 

heterogeneous value difference metric hvdm discussed previous section euclidean distance function inappropriate nominal attributes vdm inappropriate continuous attributes sufficient heterogeneous application nominal continuous attributes 
section define heterogeneous distance function hvdm returns distance input vectors defined follows hvdm number attributes 
function returns distance values attribute defined unknown normalized vdm nominal normalized diff linear function uses functions defined section depending attribute nominal linear 
note practice square root typically performed distance positive nearest neighbor nearest distance squared 
models distance weighted nearest neighbor require square root evaluated 
applications contain unknown input values handled appropriately practical system quinlan 
function da returns distance unknown done aha kibler albert giraud carrier martinez 
complicated methods tried wilson martinez little effect accuracy 
function hvdm similar function section improved heterogeneous distance functions uses vdm overlap metric nominal values normalizes differently 
similar distance function rise domingos important differences noted section 
section presents alternatives normalizing nominal linear attributes 
section presents experimental results show schemes provides better normalization set datasets 
section gives empirical results comparing hvdm commonly distance functions 

normalization discussed section distances normalized dividing distance variable range attribute distance input variable range 
policy section 
dividing range allows outliers extreme values profound effect contribution attribute 
example variable values range case exceptional possibly erroneous value dividing range result value 
robust alternative presence outliers divide values standard deviation reduce effect extreme values typical cases 
new heterogeneous distance metric hvdm situation complicated nominal numeric distance values come different types measurements numeric distances computed difference linear values normalized standard deviation nominal attributes computed sum differences probability values number output classes 
necessary find way scale different kinds measurements approximately range give variable similar influence distance measurement 
values normal distribution fall standard deviations mean difference numeric values divided standard deviations scale value range usually width 
function normalized diff defined shown equation normalized diff sa standard deviation numeric values attribute alternatives function normalized vdm considered heterogeneous distance function 
labeled definitions normalized vdm normalized vdm wilson martinez normalized vdm function equation 
similar formula pebls rise domingos nominal attributes 
uses squaring individual differences 
analogous euclidean distance manhattan distance 
slightly expensive computationally formula hypothesized robust favors having class correlations fairly similar having close different 
able distinguish 
practice square root taken individual attribute distances squared hvdm function 
function heterogeneous radial basis function networks wilson martinez hvdm introduced 

normalization experiments order determine normalization scheme gave unfair weight nominal linear attributes experiments run databases machine learning database repository university california irvine merz murphy 
datasets experiment nominal linear attributes require heterogeneous distance function 
experiment fold cross validation 
trials distance instance test set instance training set computed 
computing distance attribute normalized diff function linear attributes normalized vdm function respective experiments nominal attributes 
average distance sum distances divided number comparisons computed attribute 
average linear attributes database computed averages listed heading table 
table 
average attribute distance linear nominal attributes 
database nom 
lin 
anneal australian bridges crx echocardiogram flag heart heart cleveland heart hungarian heart long beach va heart heart swiss hepatitis horse colic soybean large average improved heterogeneous distance functions 
average distances 
nominal linear average distance number output classes avg 
average distances 
nominal linear average distance avg number output classes 
average distances 
nominal linear average distance number output classes avg averages nominal attributes normalization schemes listed headings table 
average distance linear variables exactly regardless average 
table lists number nominal nom number linear lin attributes database number output classes 
seen averages columns row table closer 
important understand reasons difference order know normalization scheme robust general 
figures graphically display averages shown table headings respectively ordered left right number output classes 
hypothesized number output classes grows normalization get worse appropriate add scaling factor sum 
length line indicates difference average distance nominal attributes linear attributes 
ideal normalization scheme difference zero longer lines indicate worse normalization 
number output classes grows difference linear distances nominal distances grows wider cases 
hand remain quite close independent number output classes 
interestingly poorly scaling factor apparently squaring factor provides rounded distance metric nominal attributes similar provided euclidean distance manhattan distance linear attributes 
underlying hypothesis performing normalization proper normalization typically improve generalization accuracy 
nearest neighbor classifier implemented hvdm distance metric 
system tested heterogeneous datasets appearing table different normalization schemes discussed fold cross validation schaffer results summarized table 
normalization schemes training sets test sets trial 
bold entries indicate scheme highest accuracy 
asterisk indicates difference greater highest scheme 
seen table normalization scheme highest accuracy wilson martinez table 
generalization accuracy 
database anneal australian bridges crx echocardiogram flag heart cleveland heart hungarian heart long beach va heart heart heart swiss hepatitis horse colic soybean large average substantially lower 
highest accuracy domains 
significantly higher times compared higher just dataset 
higher just dataset lower average accuracy 
results support hypothesis normalization scheme achieves higher generalization accuracy datasets due robust normalization accuracy 
note proper normalization necessarily improve generalization accuracy 
attribute important classification giving higher weight may improve classification 
important attribute higher weight accidentally poor normalization may improve generalization accuracy 
random improvement typically case 
proper normalization improve generalization cases typical applications 
consequence results normalization scheme hvdm function normalized vdm defined 

empirical results hvdm vs euclidean nearest neighbor classifier distance functions listed table tested datasets uci machine learning database repository 
datasets results obtained datasets nominal attributes shown table 
results approximately equivalent datasets linear attributes results remaining datasets shown section 
fold crossvalidation distance metrics training sets test sets trial 
results experiments shown table 
column lists name database test means database originally meant test set entirety separate database 
second column shows results obtained euclidean distance function normalized standard deviation attributes including nominal attributes 
column shows generalization accuracy obtained metric uses range normalized euclidean distance linear attributes overlap metric nominal attributes 
final column shows accuracy obtained hvdm distance function uses standard deviation normalized euclidean distance normalized diff defined equation linear attributes normalized vdm function nominal attributes 
highest accuracy obtained database shown bold 
entries euclid 
improved heterogeneous distance functions table 
generalization accuracy euclidean hvdm distance functions 
database anneal audiology audiology test australian bridges crx echocardiogram flag heart cleveland heart hungarian heart long beach va heart heart swiss hepatitis horse colic house votes image segmentation led led creator monks test monks test monks test mushroom promoters soybean large soybean small thyroid thyroid thyroid thyroid thyroid dis thyroid hypothyroid thyroid sick euthyroid thyroid sick zoo average euclid 
hvdm columns significantly higher hvdm higher confidence level tailed paired test marked asterisk 
entries significantly lower hvdm marked sign seen table hvdm distance function average accuracy higher metrics 
hvdm achieved high higher generalization accuracy distance functions datasets 
euclidean distance function highest datasets highest datasets 
hvdm significantly higher euclidean distance function datasets significantly lower 
similarly hvdm higher datasets significantly lower 
results support hypothesis hvdm handles nominal attributes appropriately euclidean distance heterogeneous euclidean overlap metric tends achieve higher generalization accuracy typical applications 

interpolated value difference metric ivdm section section introduce distance functions allow vdm applied directly continuous attributes 
alleviates need normalization attributes 
cases provides better measure distance continuous attributes linear distance 
example consider application input attribute height output class indicates person candidate fighter pilot particular airplane 
individuals heights significantly preferred height considered poor candidates beneficial consider heights similar preferred height farther apart linear sense 
wilson martinez hand linear attributes linearly distant values tend indicate different classifications handled appropriately 
interpolated value difference metric ivdm handles situations handles heterogeneous applications robustly 
generic version vdm distance function called discretized value difference metric dvdm comparisons extensions vdm 

ivdm learning algorithm original value difference metric vdm uses statistics derived training set instances determine probability output class input value attribute ivdm continuous values discretized equal width intervals continuous values retained integer supplied user 
unfortunately currently little guidance value 
value large reduce statistical strength values value small allow discrimination classes 
purposes heuristic determine automatically whichever greatest number output classes problem domain 
current research examining sophisticated techniques determining values cross validation statistical methods tapia thompson 
early experimental results indicate value may critical long number instances training set 
width discretized interval attribute max min max min maximum minimum value respectively occurring training set attribute example consider iris database uci machine learning databases 
iris database continuous input attributes length 
training set consisting available training instances test set consisting remaining 
division training set values length attribute ranged 
output classes database resulting width 
note discretization part learning process unfair instances test set help determine discretize values 
discretized value continuous value attribute integer discretize discrete max min deciding finding discretized values continuous attributes improved heterogeneous distance functions just discrete values nominal attributes finding lists pseudo code done 

pseudo code finding training set attribute instance input value attribute instance discretize just discrete output class instance increment 
increment 
discrete value attribute class return array attribute iris database values displayed 
discretized ranges probability corresponding output classes shown bar heights 
note heights bars sum discretized range 
bold integers indicate discretized value range 
example length greater equal discretized value 

iris setosa 
iris 
iris length cm probability bold discretized range number 
output class 
attribute iris database 

ivdm dvdm generalization far dvdm ivdm algorithms learn identically 
point dvdm algorithm need retain original continuous values discretized values generalization 
hand ivdm continuous values 
generalization algorithm nearest neighbor classifier distance function dvdm defined follows dvdm vdm discretize discretize wilson martinez discretize defined equation vdm defined equation 
repeat convenience vdm unknown input values quinlan treated simply discrete value done domingos 
table 
example iris database 
input attributes output class iris setosa iris example consider training instances shown table new input vector classified 
attribute discretized values respectively 
values distance attribute distance discretized value 
note values different ends range nearly close 
spite fact discretized distance function says equal happen fall discretized range 
ivdm uses interpolation alleviate problems 
ivdm assumes values hold true midpoint range interpolates midpoints find attribute values 
shows values second output class iris function attribute value length 
dashed line indicates value dvdm solid line shows ivdm uses 
length cm center points bold discretized range number 
dvdm ivdm probability class 
values dvdm ivdm attribute class iris database 
improved heterogeneous distance functions distance function interpolated value difference metric defined ivdm ivdm ivdm defined ivdm vdm discrete formula determining interpolated probability value continuous value attribute class mid mid mid equation mid mid midpoints consecutive discretized ranges mid probability value discretized range taken probability value midpoint range similarly 
value setting discretize subtracting value mid follows mid min width shows values attribute iris database output classes 
data points outside range min max probability value taken seen visually diagonal lines sloping zero outer edges graph 
note sum probabilities output classes sum point midpoint range midpoint range 
bold discretized range number 
length cm 
iris setosa 
iris 
iris output class probability class 
interpolated probability values attribute iris database 
wilson martinez table 
generalization dvdm vs ivdm 
database annealing australian bridges credit screening echocardiogram flag glass heart disease heart cleveland heart hungarian heart long beach va heart heart swiss hepatitis horse colic image segmentation ionosphere iris liver disorders pima indians diabetes satellite image shuttle sonar thyroid thyroid thyroid thyroid thyroid dis thyroid hypothyroid thyroid sick thyroid sick euthyroid vehicle vowel wine average dvdm ivdm table 
example ivdm vs vdm 
value ivdm vdm ivdm example instances table values attribute discretized dvdm find interpolated probability values 
example value interpolates midpoints returning values shown table classes 
instance value falls midpoints instance value falls midpoints 
seen table ivdm single attribute distance function ivdm returns distance indicates closer attribute certainly case 
dvdm discretized vdm hand returns distance indicates value equal quite far illustrating problems involved discretization 
ivdm dvdm algorithms implemented tested datasets uci machine learning databases 
results datasets contain continuous attributes shown table 
ivdm dvdm equivalent domains discrete attributes results remaining datasets deferred section 
fold cross validation average accuracy database trials shown table 
bold values indicate value highest dataset 
asterisks indicates difference statistically significant confidence level higher tailed paired test 
set datasets ivdm higher average generalization accuracy discretized algorithm 
ivdm obtained higher generalization accuracy dvdm cases significant level 
dvdm higher improved heterogeneous distance functions define instance list instances sorted ascending order attribute instance val value attribute instance 
center value current window instance val 
probability output class input value attribute note index value 
number instances current window output class total number instances current window 
instance instance window 
instance instance outside window 
window contains instances instance 
window width attribute training set continuous attribute sort instance ascending order attribute quicksort 
initialize start empty window 
instance val 
expand window include instances range instance val increment class instance 
increment increment 
shrink window exclude instances longer range instance val decrement class instance 
decrement increment 
compute probability value class current window class 

return array 

pseudo code wvdm learning algorithm 
accuracy cases difference statistically significant 
results indicate interpolated distance function typically appropriate discretized value difference metric applications continuous attributes 
section contains comparisons ivdm distance functions 

windowed value difference metric wvdm ivdm algorithm thought sampling value midpoint mid discretized range sampled finding instances value attribute range mid 
incremented instance incremented instance output class computed 
ivdm interpolates sampled points provide continuous rough approximation function 
possible sample points provide closer approximation function may turn provide accurate distance measurements values 
shows pseudo code windowed value difference metric wvdm 
wvdm samples value value occurring training set wilson martinez attribute midpoints range 
fact discretized ranges wvdm continuous attributes determine appropriate window width range width dvdm ivdm 
pseudo code learning algorithm determine attribute value 
value occurring training set attribute sampled finding instances value attribute range computing 
having fixed number sampling points window instances centered training instance determining probability point 
technique similar concept shifted histogram estimators rosenblatt parzen window techniques parzen 
attribute values sorted nlogn sorting algorithm allow sliding window collect needed statistics time attribute 
sorted order retained attribute binary search performed log time generalization 
values occurring sampled points interpolated just ivdm points available new value interpolated closer precise values ivdm 
wvdm find attribute continuous value find value attribute find instance val instance val binary search 
instance val min instance val case max class case return array 
pseudo code wvdm probability interpolation see definitions 
pseudo code interpolation algorithm 
algorithm takes value attribute returns vector probability values binary search find consecutive instances sorted list instances attribute surround probability class interpolated stored surrounding instances 
exceptions noted parenthesis handle outlying values interpolating done ivdm 
probability values input vector attribute values computed vdm function just discrete probability values 
wvdm distance function defined wvdm wvdm wvdm defined wvdm vdm discrete improved heterogeneous distance functions table 
generalization wvdm vs dvdm 
database annealing australian bridges credit screening echocardiogram flag glass heart disease heart cleveland heart hungarian heart long beach va heart heart swiss hepatitis horse colic image segmentation ionosphere iris liver disorders pima indians diabetes satellite image shuttle sonar thyroid thyroid thyroid thyroid thyroid dis thyroid hypothyroid thyroid sick thyroid sick euthyroid vehicle vowel wine average dvdm wvdm 
example wvdm probability landscape 
probability class 
iris setosa 
iris 
iris output class length cm interpolated probability value continuous value computed 
note typically finding distance new input vector instance training set 
instances training set define probability attribute values binary search interpolation unnecessary training instances immediately recall stored probability values pruning techniques 
drawback approach increased storage needed retain probability values attribute value training set 
execution time significantly increased ivdm dvdm 
see section discussion efficiency considerations 
shows probability values classes attribute iris database time windowed sampling technique 
comparing reveals attribute ivdm provides approximately shape misses detail 
example peak occurring output class approximately length 
flat line misses peak entirely due somewhat arbitrary position midpoints probability values sampled 
table summarizes results testing wilson martinez wvdm algorithm datasets dvdm ivdm 
bold entry indicates highest accuracy measurements asterisk indicates difference statistically significant confidence level tailed paired test 
set databases wvdm average accurate dvdm 
wvdm higher average accuracy dvdm databases significantly higher dvdm higher databases differences statistically significant 
section provides comparisons wvdm distance functions including ivdm 

empirical comparisons analysis distance functions section compares distance functions discussed 
nearest neighbor classifier implemented different distance functions euclidean normalized standard deviation discussed section hvdm discussed section dvdm ivdm discussed section wvdm discussed section 
summarizes definition distance function 

summary distance function definitions 
continuous distance function linear discrete nominal range vdm disc disc ivdm interpolate probabilities range midpoints 
wvdm interpolate probabilities adjacent values 
wvdm functions distance function euclidean hvdm ivdm dvdm definition attribute type vdm range max min vdm vdm vdm vdm distance function tested datasets uci machine learning databases improved heterogeneous distance functions fold cross validation 
average accuracy trials reported test table 
highest accuracy achieved dataset shown bold 
names new distance functions hvdm ivdm wvdm shown bold identify 
table lists number instances database inst number continuous con integer int linear discrete nominal nom input attributes 
inputs database euclid hvdm dvdm ivdm wvdm inst 
con int nom annealing audiology audiology test australian breast cancer bridges credit screening echocardiogram flag glass heart disease heart cleveland heart hungarian heart long beach va heart heart swiss hepatitis horse colic house votes image segmentation ionosphere iris led noise led liver disorders monks monks monks mushroom pima indians diabetes promoters satellite image shuttle sonar soybean large soybean small thyroid thyroid thyroid thyroid thyroid dis thyroid hypothyroid thyroid sick euthyroid thyroid sick vehicle vowel wine zoo average table 
summary generalization accuracy wilson martinez set datasets new distance functions hvdm ivdm wvdm substantially better euclidean distance 
ivdm highest average accuracy higher average euclidean distance indicating robust distance function datasets especially nominal attributes 
wvdm slightly lower ivdm accuracy 
somewhat surprisingly dvdm slightly higher hvdm datasets uses discretization linear distance continuous attributes 
vdm distance functions outperformed euclidean distance 
datasets euclidean distance highest accuracy times highest times hvdm dvdm ivdm wvdm 
datasets continuous attributes vdm distance functions hvdm dvdm ivdm wvdm equivalent 
datasets vdm distance functions achieve average accuracy compared euclidean indicating substantial superiority problems 
datasets nominal attributes euclidean hvdm equivalent distance functions perform average dvdm averages indicating detrimental effects discretization 
euclidean similar definitions applications nominal attributes euclidean normalized standard deviation normalized range attribute 
interesting average accuracy datasets slightly higher euclidean indicating standard deviation may provide better normalization datasets 
difference small datasets contain outliers difference probably negligible case 
disadvantage scaling attributes standard deviation attributes value boolean attribute large weight due scale relative frequencies attribute values 
related problem occur hvdm 
skewed class distribution instances classes values quite small classes quite large case difference correspondingly small nominal attributes get little weight compared linear attributes 
phenomenon noted ting recognized problems hypothyroid dataset 
research address normalization problems look automated solutions 
fortunately dvdm ivdm wvdm suffer problem attributes scaled amount cases may part account success hvdm experiments 
datasets nominal continuous attributes hvdm slightly higher euclidean distance datasets turn slightly higher indicating overlap metric may improvement heterogeneous databases 
dvdm ivdm wvdm higher euclidean distance datasets ivdm lead 

effects sparse data distance functions vdm require statistics determine distance 
hypothesized generalization accuracy lower vdm distance functions improved heterogeneous distance functions euclidean distance little data available functions increase accuracy slowly instances available sufficient number instances allowed reasonable sample size determine probability values 

average accuracy amount data increases 
instances average generalization accuracy euclidean hvdm dvdm ivdm wvdm test hypothesis experiments obtain results shown table repeated part available training data 
shows generalization accuracy test set improves percentage available training instances learning generalization increased 
generalization accuracy values shown averages datasets table 
surprisingly vdm distance functions increased accuracy fast faster euclidean little data available 
may little data available random positioning sample data input space greater detrimental affect accuracy error statistical sampling vdm functions 
interesting note distance functions pair distinct pairs 
interpolated vdm distance functions ivdm wvdm maintain highest accuracy vdm functions functions linear overlap distance remain lowest early graph 
wilson martinez 
efficiency considerations section considers storage requirements learning speed generalization speed algorithms 

storage distance functions store entire training set requiring nm storage number instances training set number input attributes application instance pruning technique 
euclidean functions necessary amount storage restrictive grows large 
hvdm dvdm ivdm probabilities attributes discrete attributes hvdm stored requiring mvc storage average number attribute values discrete discretized attributes number output classes application 
possible store array vdm hvdm dvdm storage mv savings wvdm probability values stored continuous attribute value resulting nmc storage typically larger mvc usually larger 
necessary store list pointers instances attribute requiring additional mn storage 
total storage wvdm nm cnm 
storage mn mn mn mvc mn mvc mn mvc cmn learning time mn mn mn mvc mn mvc mn mvc mvc generalization time mn mn mnc mn mnc mn mnc mn mnc distance function euclidean hvdm dvdm ivdm wvdm table 
summary efficiency distance metrics 
table summarizes storage requirements system 
wvdm distance functions requires significantly storage 
applications critical factor distance functions conjunction instance pruning techniques reduce storage requirements 
see section list techniques reduce number instances retained training set subsequent generalization 

learning speed takes nm time read training set 
takes additional nm time find standard deviation attributes euclidean distance just nm time find ranges 
computing vdm statistics hvdm dvdm ivdm takes mn mvc time approximately mn 
computing wvdm statistics takes mnc time approximately 
general learning time quite acceptable distance functions 
improved heterogeneous distance functions 
generalization speed assuming distance function compare new input vector training instances euclidean take mn time 
hvdm ivdm dvdm take mnc stored hvdm case search done mn time 
wvdm takes logn mnc mnc time 
typically fairly small generalization process require significant amount time computational resources grows large 
techniques trees deng moore wess althoff sproull projection papadimitriou bentley reduce time required locate nearest neighbors training set algorithms may require modification handle continuous nominal attributes 
pruning techniques reduce storage section reduce number instances searched generalization 

related distance functions variety fields including instance learning neural networks statistics pattern recognition cognitive psychology see section 
section lists commonly distance functions involving numeric attributes 
normalization desirable linear distance function euclidean distance attributes arbitrarily get weight 
dividing range standard deviation normalize numerical attributes common practice 
turney turney halasz investigated contextual normalization standard deviation mean normalization continuous attributes depend context input vector obtained 
attempt contextual normalization simpler methods normalizing continuous attributes focus normalize appropriately continuous nominal attributes 
value distance metric vdm introduced stanfill waltz 
uses attribute weights functions 
modified value difference metric cost salzberg attribute weights uses instance weights 
assumed systems discretization lebowitz schlimmer handle continuous attributes 
ventura ventura martinez explored variety discretization methods systems discrete input attributes 
discretization preprocess data degraded accuracy recommended machine learning algorithms designed handle continuous attributes directly 
ting different discretization techniques conjunction ib aha kibler albert 
results showed improved generalization accuracy discretization 
discretization allowed algorithm attributes linear distance continuous attributes avoided normalization problems discussed sections 
similar results seen slightly higher results dvdm discretizes continuous attributes uses vdm compared hvdm uses linear distance continuous attributes 
dvdm uses equal width intervals discretization wilson martinez ting algorithms advanced discretization techniques 
domingos uses heterogeneous distance function similar hvdm rise system hybrid rule instance learning system 
rise uses normalization scheme similar sections square individual attribute distances 
mohri tanaka statistical technique called quantification method ii qm derive attribute weights distance functions handle nominal continuous attributes 
transform nominal attributes values boolean attributes time weights attribute correspond individual attribute values original data 
turney addresses cross validation error voting values instance learning systems explores issues related selecting parameter number neighbors decide classification 
order focus attention distance functions accuracy improved applications 
ivdm wvdm nonparametric density estimation techniques tapia thompson determining values computing distances 
parzen windows parzen shifting histograms rosenblatt similar concept techniques especially wvdm 
techniques gaussian kernels advanced techniques fixed sized sliding window 
experimented kernels results slightly worse wvdm ivdm increased overfitting 
applies distance function problem classification input vector mapped discrete output class 
distance functions systems perform regression atkeson moore schaal atkeson cleveland loader output real value interpolated nearby points kernel regression deng moore 
mentioned section pruning techniques reduce storage requirements instance systems improve classification speed 
techniques introduced including ib aha kibler albert aha condensed nearest neighbor rule hart reduced nearest neighbor rule gates selective nearest neighbor rule typical instance learning algorithm zhang prototype methods chang hyperrectangle techniques salzberg wettschereck dietterich rule techniques domingos random mutation hill climbing skalak cameron jones kibler aha tomek wilson 

research areas learning systems depend reliable distance function achieve accurate generalization 
euclidean distance function distance functions inappropriate nominal attributes function throws away information achieve better accuracy euclidean function 
value difference metric vdm designed provide appropriate measure improved heterogeneous distance functions distance nominal attribute values 
current systems vdm discretize continuous data discrete ranges causes loss information corresponding loss generalization accuracy 
introduced new distance functions 
heterogeneous value difference function hvdm uses euclidean distance linear attributes vdm nominal attributes uses appropriate normalization 
interpolated value difference metric ivdm windowed value difference metric wvdm handle continuous attributes paradigm vdm 
ivdm wvdm provide classification accuracy higher average discretized version algorithm dvdm datasets continuous attributes examined equivalent dvdm applications continuous attributes 
experiments datasets ivdm wvdm achieved higher average accuracy hvdm better dvdm euclidean distance 
ivdm slightly accurate wvdm requires time storage desirable distance function heterogeneous applications similar 
properly normalized euclidean distance achieves comparable generalization accuracy nominal attributes situations appropriate distance function 
learning system obtain generalization accuracy results nearest neighbor classifier hvdm ivdm wvdm distance functions nearest neighbor classifier incorporated wide variety systems allow handle continuous values including instance learning algorithms pebls radial basis function networks distance neural networks 
new distance metrics areas statistics cognitive psychology pattern recognition areas distance heterogeneous input vectors interest 
distance functions conjunction weighting schemes improvements system provides 
new distance functions show improved average generalization datasets experimentation 
hoped datasets representative kinds applications face real world new distance functions continue provide improved generalization accuracy cases 
research look determining conditions distance function appropriate particular application 
look closely problem selecting window width look possibility smoothing wvdm probability landscape avoid overfitting 
new distance functions conjunction variety weighting schemes provide robust generalization presence noise irrelevant attributes increase generalization accuracy wide variety applications 
aha david 
tolerating noisy irrelevant novel attributes instance learning algorithms 
international journal man machine studies vol 
pp 

aha david dennis kibler marc albert 
instance learning algorithms 
machine learning vol 
pp 

wilson martinez atkeson chris 
local models control movement 
touretzky ed advances neural information processing systems 
san mateo ca morgan kaufmann 
atkeson chris andrew moore stefan schaal 
locally weighted learning 
appear artificial intelligence review 
bruce 
pattern recognition ideas practice 
new york plenum press pp 

yoram 
context similarity measure 
proceedings european conference machine learning ecml 
italy springer verlag pp 

broomhead lowe 
multi variable functional interpolation adaptive networks 
complex systems vol 
pp 

cameron jones 
instance selection encoding length heuristic random mutation hill climbing 
proceedings eighth australian joint conference artificial intelligence pp 

carpenter gail stephen grossberg 
massively parallel architecture self organizing neural pattern recognition machine 
computer vision graphics image processing vol 
pp 

chang chin liang 
finding prototypes nearest neighbor classifiers 
ieee transactions computers vol 
pp 

cleveland loader 
computational methods local regression 
technical report murray hill nj bell laboratories statistics department 
cost scott steven salzberg 
weighted nearest neighbor algorithm learning symbolic features 
machine learning vol 
pp 

cover hart 
nearest neighbor pattern classification 
institute electrical electronics engineers transactions information theory vol 
pp 

dasarathy 
nearest neighbor nn norms nn pattern classification techniques 
los alamitos ca ieee computer society press 
deng kan andrew moore 
multiresolution instance learning 
appear proceedings international joint conference artificial intelligence ijcai 
diday edwin 
progress distance similarity measures pattern recognition 
second international joint conference pattern recognition pp 

domingos pedro 
rule induction instance learning unified approach 
appear international joint conference artificial intelligence ijcai 

distance weighted nearest neighbor rule 
ieee transactions systems man cybernetics vol 
april pp 

improved heterogeneous distance functions gates 
reduced nearest neighbor rule 
ieee transactions information theory vol 
pp 

giraud carrier christophe tony martinez 
efficient metric heterogeneous inductive learning applications attribute value language 
intelligent systems pp 

hart 
condensed nearest neighbor rule 
institute electrical electronics engineers transactions information theory vol 
pp 

hecht nielsen 
networks 
applied optics vol 
pp 

kibler david aha 
learning representative exemplars concepts initial case study 
proceedings fourth international workshop machine learning 
irvine ca morgan kaufmann pp 

kohonen teuvo 
self organizing map 
proceedings ieee vol 
pp 

lebowitz michael 
categorizing numeric information generalization 
cognitive science vol 
pp 

merz murphy 
uci repository machine learning databases 
irvine ca university california irvine department information computer science 
internet www ics uci edu mlearn mlrepository html 
michalski robert stepp edwin diday 
advance data analysis clustering objects classes characterized conjunctive concepts 
progress pattern recognition vol 
kanal rosenfeld eds 
new york north holland pp 

mitchell tom 
need biases learning generalizations 
shavlik dietterich eds readings machine learning 
san mateo ca morgan kaufmann pp 

mohri tanaka 
optimal weighting criterion case indexing numeric symbolic attributes 
aha ed case reasoning papers workshop technical report ws 
menlo park ca press pp 

morton eric smith 
pattern recognition engineering 
new york wiley pp 

nosofsky robert 
attention similarity identification categorization relationship 
journal experimental psychology general vol 
pp 

papadimitriou christos jon louis bentley 
worst case analysis nearest neighbor searching projection 
lecture notes computer science vol 
automata languages programming pp 

wilson martinez parzen 
estimation probability density function mode 
annals mathematical statistics 
vol 
pp 

quinlan 
unknown attribute values induction 
proceedings th international workshop machine learning 
san mateo ca morgan kaufmann pp 

john simon kasif steven salzberg david aha 
better understanding memory bayesian classifiers 
proceedings eleventh international machine learning conference 
new brunswick nj morgan kaufmann pp 

renals steve richard rohwer 
phoneme classification experiments radial basis functions 
proceedings ieee international joint conference neural networks ijcnn vol 
pp 

woodruff lowry 
algorithm selective nearest neighbor decision rule 
ieee transactions information theory vol 
pp 

rosenblatt murray 
remarks nonparametric estimates density function 
annals mathematical statistics 
vol 
pp 

rumelhart mcclelland 
parallel distributed processing mit press ch 
pp 

salzberg steven 
nearest hyperrectangle learning method 
machine learning vol 
pp 

schaffer cullen 
selecting classification method cross validation 
machine learning vol 

schaffer cullen 
conservation law generalization performance 
proceedings eleventh international conference machine learning ml morgan kaufmann 
schlimmer jeffrey 
learning representation change 
proceedings sixth national conference artificial intelligence aaai vol 
pp 

skalak 
prototype feature selection sampling random mutation hill climbing 
proceedings eleventh international conference machine learning ml 
morgan kaufman pp 

sproull robert 
refinements nearest neighbor searching dimensional trees 
algorithmica vol 
pp 

stanfill waltz 
memory reasoning 
communications acm vol 
december pp 

improved heterogeneous distance functions tapia richard james thompson 
nonparametric probability density estimation 
baltimore md johns hopkins university press 
ting kai ming 
discretization continuous valued attributes instance learning 
technical report basser department computer science university sydney australia 
ting kai ming 
discretisation lazy learning 
appear special issue lazy learning artificial intelligence review 
tomek ivan 
experiment edited nearest neighbor rule 
ieee transactions systems man cybernetics vol 
june pp 

turney peter 
theoretical analyses cross validation error voting instancebased learning 
journal experimental theoretical artificial intelligence pp 

turney peter 
exploiting context learning classify 
proceedings european conference machine learning 
vienna austria springer verlag pp 

turney peter michael halasz 
contextual normalization applied aircraft gas turbine engine diagnosis 
journal applied intelligence vol 
pp 

tversky amos 
features similarity 
psychological review vol 
pp 

ventura dan 
discretization preprocessing step supervised learning models master thesis department computer science brigham young university 
ventura dan tony martinez 
empirical comparison discretization methods 
proceedings tenth international symposium computer information sciences pp 

wasserman philip 
advanced methods neural computing 
new york ny van nostrand reinhold pp 

wess stefan klaus dieter althoff guido 
trees improve retrieval step case reasoning 
stefan wess klaus dieter althoff richter eds topics case reasoning 
berlin springer verlag pp 

wettschereck dietrich thomas dietterich 
experimental comparison nearest neighbor nearest hyperrectangle algorithms 
machine learning vol 
pp 

wettschereck dietrich david aha mohri 
review comparative evaluation feature weighting methods lazy learning algorithms 
technical report aic 
washington naval research laboratory navy center applied research artificial intelligence 
wilson martinez wilson randall tony martinez 
potential prototype styles generalization 
proceedings sixth australian joint conference artifical intelligence ai pp 

wilson randall tony martinez 
heterogeneous radial basis functions 
proceedings international conference neural networks icnn vol 
pp 

wilson dennis 
asymptotic properties nearest neighbor rules edited data 
ieee transactions systems man cybernetics vol 
pp 

wolpert david 
overfitting avoidance bias 
technical report sfi tr 
santa fe nm santa fe institute 
zhang 
selecting typical instances instance learning 
proceedings ninth international conference machine learning 
