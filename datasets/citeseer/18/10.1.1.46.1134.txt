mdl categorical theories continued quinlan basser department computer science university sydney sydney australia quinlan cs su oz au continues reported ml minimum description length principle non probabilistic theories 
new encoding scheme developed similar benefits adhoc penalty function previously 
scheme implemented rules empirical trials real world datasets reveal small useful improvement classification accuracy 
classifiers induced data resulting theories commonly interpreted functions attribute values classes class distributions 
example talk accuracy learned classifier unseen cases measured percentage cases classifier predicts actual class 
theories interpretation described categorical synonym deterministic common 
concern learning real world domains theory overfit data overly complex theories lower accuracy new cases 
techniques overfitting avoidance cost complexity pruning breiman friedman olshen stone reduced error pruning quinlan minimum description length principle rissanen analogous minimum message length principle georgeff wallace particularly attractive intuitive interpretation strong theoretical base 
mdl approach possible theories ft derived data characterized description length number bits schaffer points overfitting avoidance form bias lead worse performance situations 
needed encode theory data learned 
choosing theory minimum description length equivalent maximizing probability jd data 
raises immediate problem best theory learned noisy data expected fit data exactly 
interpreted categorically fit jd zero 
pednault puts deterministic case theory absolutely agree observations ruled 
situations mdl sense theories interpreted probabilistically objective sought mml correct classification maximum number unseen cases minimization amount information needed determine class category known 
wallace patrick despite mdl situations learned theory assessed categorical accuracy predictions quinlan rivest 
examples tasks mdl leads poor choices competing categorical theories quinlan 
tasks theories larger categorical error rates tend assign unexpectedly high low prior probability described class 
recommended additional bias favor theories predicted class distribution matches observed data 
offer theoretical justification preference argued philosophical perspective theory learned data accurately summarize data 
theory intended interpreted categorically prior probabilities classes 
limited empirical trials showed bias effective selecting theories lower categorical error rate unseen cases implementation penalty function ad hoc concluded new encoding scheme reflected categorical performance reasonable prior assumptions satisfying 
encoding scheme lines envisaged developed 
section defines kind theories considered mdl 
summarizing problem previous approach introduces new encoding scheme incorporated learning program rules quinlan 
experiments realworld domains demonstrate benefit scheme 
class description theories mdl symbolic classifiers come forms including decision trees hunt marin stone decision lists rivest cnf dnf expressions pagallo haussler concepts described special purpose logics michalski 
quinlan concerns class tasks learned theory description classes called target class formalism description expressed important 
theory covers case case matches description cases covered predicted belong target class cases assigned nontarget class 
mdl principle best explained terms communication model sender transmits receiver description consisting theory data derived quinlan rivest 
description length associated consists cost message encoding theory cost data intuitively length component measures theory complexity degree theory fails account data description length represents balancing model fit complexity 
choice theories mdl principle states theory associated shortest description length preferred 
assume agreed language theories expressed theory cost number bits needed transmit particular sentence representing cost encoding theory broken bits needed transmit attribute values case plus bits required cases classes 
theories ignored description lengths compare possible theories 
identifying case class theory comes identifying cases misclassified theory classes inverted assumption 
number bits needed identify errors theory referred exceptions cost 
methods encoding exceptions discussed quinlan 
specifying schemes detail follows wallace patrick adopting perspective 
messages fm occur probabilities fp postulate encoding scheme message requires gammalog bits logarithms taken base 
course assumes probability message occurring independent previous messages receiver knows relevant probabilities fp instance suppose misclassifies cases errors identified sending messages case probabilities jdj gamma jdj respectively 
receiver know probabilities order decode messages transmit ranges jdj 
total number bits transmitted log jdj theta gammalog jdj jdj gamma theta gammalog gamma jdj called uniform coding strategy errors identified single group 
alternative divided strategy identifies separately errors cases covered theory false positives remaining cases false negatives fp fn respectively numbers cases covered covered theory respectively exceptions cost log fp theta gammalog fp gamma fp theta gammalog gamma fp log fn theta gammalog fn gamma fn theta gammalog gamma fn divided strategy requires bits uniform strategy approach identifying errors subsets data quinlan rivest wallace patrick 
table exceptions costs competing theories theory false false cases uniform divided biased pos neg covered encoding encoding encoding anomaly previous solution discussed quinlan mdl lead poor choices candidate categorical theories 
hypothetical illustration supposes dataset cases belong target class candidate theories give rise various numbers false positive false negative errors shown table 
theories presumed theory cost mdl choose theory lowest exceptions cost 
situation uniform strategy find exact tie errors training data 
divided approach chose errors equally complex theory far fewer errors 
choices mdl admittedly contrived example clearly odds intuition 
number cases covered theory tp fp gamma fn tp number true positive cases belonging target class 
categorical context proportion cases covered theory interpreted predicted prior probability target class 
theories cover cases respectively marked variance data prior probability target class 
attempt force categorical theories agree training data respect quinlan penalizes atypical theories 
details unimportant idea multiply description length theory factor discrepancy predicted proportion target cases observed data 
new solution resorting ad hoc penalty function inherently unsatisfying particularly principal attraction mdl methods clean theoretical base 
justification inability find method coding theories favors predicted class distribution similar observed data 
realized concentrating wrong component description length method encoding exceptions adapted prefer theories 
proportions target class cases predicted theory observed training data numbers false positives false negatives equal 
suggests new biased coding scheme follows just uniform scheme total number errors sent receiver 
transmitting error messages data sender transmits errors cases covered theory uncovered cases 
assumption false positives false negatives balanced probability error covered cases probability encode error messages covered cases 
false positives identified receiver calculate true number false negatives fp probability error uncovered cases known fn total exceptions cost log jdj fp theta gammalog gamma fp theta gammalog gamma fn theta gammalog fn gamma fn theta gammalog gamma fn slight complication number covered cases small may greater 
overcome problem retaining symmetry scheme followed half cases covered theory half covered false negative errors uncovered cases transmitted probability followed false positives fp final column table shows biased exceptions costs theories section 
smaller uniform divided encoding costs fp close fn larger assumption balanced errors grossly incorrect 
example mdl place ahead theories intuitively sensible outcome 
applying scheme rules rules program generates rule classifiers decision trees quinlan 
algorithm proceeds phases 
rule class formulated leaf decision tree majority class leaf 
left hand side initially contains condition appears path root tree leaf rules usually generalized dropping conditions 
result rules longer mutually disjoint 

class turn rules class examined subset selected 

order class rule subsets determined default class chosen 
second phase subset rules selected class guided mdl 
learning task may number classes subset selection essentially class problem goal cover cases class question covering cases belonging class 
description length candidate subset determined calculating theory cost encode constituent rules exceptions cost identify misclassified cases 
subset lowest description length chosen 
mdl rules fits squarely context addressed rule subset categorical theory characterizes class classes 
new encoding doing job lead better choice rules class ultimately accurate classifier 
test hypothesis versions rules prepared differ method calculate exceptions costs 
version uses uniform strategy set generally robust divided strategy quinlan 
biased version employs rules consideration subsets exhaustive 
release carries series greedy searches starting rules randomly chosen rules search attempts improve current subset adding deleting single rule improvement possible 
best subset searches retained 
differs release described quinlan simulated annealing search best subset 
new strategy uniform strategy transmits single global error count uses initial assumption equal numbers false positive false negative errors derive separate error probabilities covered uncovered cases 
comprehensive collection containing real world datasets assembled uci repository 
intention cover spectrum properties size attribute numbers types number classes class distribution attempt favor coding strategy 
summary main characteristics appendix 
trials carried dataset 
trial data split randomly training set test set 
rule classifiers learned training data versions rules classifiers evaluated test data 
table shows dataset average trials respective error rates test data numbers rules retained 
final columns record numbers trials biased uniform exceptions costs led accurate classifier 
ways results compare coding strategies ffl biased strategy gives lower average error uniform approach domains error rate domains higher error rate domains credit approval horse colic sonar 
ffl performance strategy dataset judged number trials superior biased coding wins domains ties loses domains 
ffl biased approach gives accurate classifier trials versus trials uniform strategy comes ahead 
ffl particular domain ratio average error rate biased strategy obtained uniform approach measures extent benefit values loss values greater associated 
values ratio range splice junction sonar average domains 
new domain biased strategy rules expected lead lower error rate uniform strategy adopted 
ffl ratio computed just trials strategies give different numbers errors test data average ratio 
coding strategy matters trial biased coding approach give error rate considerably lower obtained alternative 
table comparison biased uniform exceptions coding strategies implemented rules 
dataset biased coding uniform coding trials superior error rules error rules biased uniform audiology auto insurance breast cancer wi chess endgame congress voting credit approval glass identification heart disease cl hepatitis horse colic hypothyroid image regions iris led digits lymphography nettalk phoneme nettalk stress pima diabetes primary tumor promoters sick euthyroid sonar soybean disease splice junction tic tac toe ffl number rules retained rough indicator complexity final theory 
respect systematic difference strategies biased coding approach leads fewer rules domains number rules domains rules domains 
accuracy metrics biased strategy defined emerges clearly preferable uniform strategy trials 
related research anonymous reviewers drew attention alternative approaches selecting categorical theories resemble mdl trading accuracy theory complexity 
consider families loss functions criteria judge appropriateness selected theory 
selecting theory minimize categorical error rate title pattern recognition problem tasks considered vapnik 
derives upper bound error rate selected theory confidence true error rate theory exceed bound 
factors amount training data jdj observed error rate theory bound depends capacity set candidate theories roughly largest amount data partitioned subsets possible ways theories 
basis structural risk minimization candidate theories grouped sequence subsets increasing capacity placing theories similar complexity subset best candidate subset final theory selected choosing subsets minimizing upper bound error rate estimating value loss function subset leave cross validation 
barron concerned problems arising mdl general loss functions develops alternative strategy complexity regularization 
theory chosen minimize sum error rate complexity component categorical loss functions jdj jdj cost encoding theory number errors training data long constant value greater log approximately barron shows expected penalty choosing theory approaches zero jdj increases 
criterion tried rules results quite poor datasets error rate component dominated complexity component rules selected 
reviewer pointed exceptions coding costs reduced quantizing transmitted number errors expressed units jdj rounded nearest integer number bits needed encode error count approximately halved 
gain offset fact message probabilities known lower accuracy 
quantization appear advantageous application discussed representing benefit values message probabilities change appreciably 
theories table lowest biased encoding cost quantization employed 
quantization scheme implemented rules performance degraded domains 
predecessor focuses common learning scenario theory induced training set classify unseen case predicting class determining posterior probabilities classes 
straightforward application minimum description length principle situations lead anomalous choices contending theories 
better choices obtained addition bias theories probability predicting class similar relative frequency class training data 
relying artificial penalty function implement bias case quinlan biased exceptions coding strategy achieves effect manner tune mdl principle 
new scheme tested rule learning program rules shown lead greater predictive accuracy domains investigated 
improvement dramatic described useful 
biased scheme involves additional computation incorporated release software 
release published morgan kaufmann obtain update latest version anonymous ftp ftp cs su oz au file pub ml patch tar compressed tar file contains replacements source code files changed release 
releases incorporate changes affect system performance biased exceptions cost tested independently william cohen domains include datasets reported 
ripper rule induction system cohen previously uniform coding strategy altered biased strategy proved superior domains inferior 
average ratio error rate biased encoding uniform encoding domain error rate dropped zero undue impact average 
excluding highest lowest value ratio obtain average remaining datasets modest gain 
particular strategy described way exploit expected balance false positive false negative errors 
instance transmit number false positive errors estimate probability false negatives assumption number errors uncovered cases 
interesting see alternative biased encoding schemes beneficial 
supported australian research council assisted research agreements digital equipment 
anonymous reviewers helped greatly identifying relevant research statistics 
am grateful william cohen providing experimental results ripper 
patrick murphy david aha maintaining uci repository ljubljana oncology institute slovenia providing lymphography primary tumor data 
breiman friedman olshen stone 

classification regression trees 
belmont ca wadsworth 
barron 

complexity regularization application artificial neural networks 
nonparametric functional estimation related topics ed boston kluwer academic publishers 
cohen 

fast effective rule induction 
proceedings th international conference machine learning tahoe city volume 
georgeff wallace 

general selection criterion inductive inference 
technical note sri international menlo park 
hunt marin stone 

experiments induction 
new york academic press 
retaining copy old files recommended 
michalski 

pattern recognition inductive inference 
ieee transactions pattern analysis machine intelligence 
pagallo haussler 

algorithms learn dnf discovering relevant features 
proceedings th international workshop machine learning ithaca 
san mateo morgan kaufmann 
pednault 

minimal length encoding inductive inference 
knowledge discovery databases piatetsky shapiro frawley eds menlo park aaai press 
quinlan 

simplifying decision trees 
international journal man machine studies 
quinlan 

programs machine learning 
san mateo morgan kaufmann 
quinlan 

minimum description length principle categorical theories 
proceedings th international conference machine learning new brunswick 
san francisco morgan kaufmann 
quinlan rivest 

inferring decision trees minimum description length principle 
information computation 
rissanen 

universal prior integers estimation minimum description length 
annals statistics 
rivest 

learning decision lists 
machine learning 
schaffer 

overfitting avoidance bias 
machine learning 
vapnik 

estimation dependences empirical data 
new york springer verlag 
wallace patrick 

coding decision trees 
machine learning 
appendix summary datasets provides brief description datasets experiments terms ffl size number instances dataset ffl attributes number types attributes involved continuous valued binary nominal ffl number distinct classes 
dataset size attributes classes audiology auto insurance breast cancer wi chess endgame congress voting credit approval glass identification heart disease cl hepatitis cc horse colic hypothyroid image regions iris led digits lymphography nettalk phoneme nettalk stress pima diabetes primary tumor promoters sick euthyroid sonar soybean disease splice junction tic tac toe 
